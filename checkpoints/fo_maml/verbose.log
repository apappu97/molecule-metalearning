Fold 0
Command line
python chemprop/meta_train.py --meta_learning --FO_MAML --data_path /home/apappu/thesis/molecule-metalearning/filtered_chembl/chembl_less_1024_more_128_645_tasks.csv --dataset_type classification --split_type scaffold_balanced --chembl_assay_metadata_pickle_path filtered_chembl/ --save_dir checkpoints/fo_maml/ --results_save_dir results/fo_maml/ --experiment_name fo_maml
Args
{'ANIL': False,
 'FO_MAML': True,
 'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 32,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'chembl_assay_metadata_pickle_path': 'filtered_chembl/',
 'class_balance': False,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'data_path': '/home/apappu/thesis/molecule-metalearning/filtered_chembl/chembl_less_1024_more_128_645_tasks.csv',
 'dataset_type': 'classification',
 'depth': 3,
 'dropout': 0.2,
 'dummy': False,
 'ensemble_size': 1,
 'epochs': 1000,
 'experiment_name': 'fo_maml',
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_size': None,
 'ffn_hidden_size': 400,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 300,
 'init_lr': 0.0001,
 'inner_loop_lr': 0.05,
 'kaiming': True,
 'log_frequency': 10,
 'loss_queue_window': 500,
 'max_data_size': None,
 'max_lr': 0.001,
 'meta_batch_size': 32,
 'meta_learning': True,
 'meta_test_epochs': 30,
 'meta_test_lr': 0.0001,
 'meta_test_split_sizes': (0.8, 0.1, 0.1),
 'meta_train_split_sizes': (0.8, 0.2, 0),
 'metric': 'prc-auc',
 'minimize_score': False,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': False,
 'num_folds': 1,
 'num_inner_gradient_steps': 2,
 'num_lrs': 1,
 'num_tasks': None,
 'num_workers': 0,
 'outer_loop_lr': 0.0015,
 'pytorch_seed': 0,
 'quiet': False,
 'results_save_dir': 'results/fo_maml/',
 'save_dir': 'checkpoints/fo_maml/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'scaffold_balanced',
 'target_columns': None,
 'task_names': None,
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 645
Class sizes
CHEMBL1033994 0: 41.98%, 1: 58.02%
CHEMBL1119333 0: 9.59%, 1: 90.41%
CHEMBL1217000 0: 52.41%, 1: 47.59%
CHEMBL1243965 0: 17.11%, 1: 82.89%
CHEMBL1243966 0: 41.96%, 1: 58.04%
CHEMBL1243967 0: 10.17%, 1: 89.83%
CHEMBL1243968 0: 15.06%, 1: 84.94%
CHEMBL1243970 0: 6.91%, 1: 93.09%
CHEMBL1243972 0: 9.92%, 1: 90.08%
CHEMBL1243976 0: 28.99%, 1: 71.01%
CHEMBL1246087 0: 33.88%, 1: 66.12%
CHEMBL1246088 0: 48.37%, 1: 51.63%
CHEMBL1613762 0: 2.45%, 1: 97.55%
CHEMBL1613779 0: 58.47%, 1: 41.53%
CHEMBL1613785 0: 43.22%, 1: 56.78%
CHEMBL1613787 0: 69.08%, 1: 30.92%
CHEMBL1613807 0: 95.99%, 1: 4.01%
CHEMBL1613813 0: 44.20%, 1: 55.80%
CHEMBL1613814 0: 53.85%, 1: 46.15%
CHEMBL1613817 0: 84.86%, 1: 15.14%
CHEMBL1613853 0: 41.86%, 1: 58.14%
CHEMBL1613861 0: 14.47%, 1: 85.53%
CHEMBL1613864 0: 5.19%, 1: 94.81%
CHEMBL1613867 0: 97.13%, 1: 2.87%
CHEMBL1613870 0: 15.80%, 1: 84.20%
CHEMBL1613871 0: 6.15%, 1: 93.85%
CHEMBL1613874 0: 65.78%, 1: 34.22%
CHEMBL1613876 0: 54.35%, 1: 45.65%
CHEMBL1613884 0: 21.51%, 1: 78.49%
CHEMBL1613890 0: 34.85%, 1: 65.15%
CHEMBL1613897 0: 46.64%, 1: 53.36%
CHEMBL1613898 0: 75.45%, 1: 24.55%
CHEMBL1613904 0: 50.77%, 1: 49.23%
CHEMBL1613907 0: 5.34%, 1: 94.66%
CHEMBL1613926 0: 30.15%, 1: 69.85%
CHEMBL1613928 0: 45.90%, 1: 54.10%
CHEMBL1613929 0: 81.25%, 1: 18.75%
CHEMBL1613941 0: 32.58%, 1: 67.42%
CHEMBL1613942 0: 41.43%, 1: 58.57%
CHEMBL1613947 0: 46.01%, 1: 53.99%
CHEMBL1613949 0: 60.13%, 1: 39.87%
CHEMBL1613950 0: 43.05%, 1: 56.95%
CHEMBL1613955 0: 54.04%, 1: 45.96%
CHEMBL1613962 0: 24.26%, 1: 75.74%
CHEMBL1613967 0: 44.59%, 1: 55.41%
CHEMBL1613981 0: 58.85%, 1: 41.15%
CHEMBL1613991 0: 14.98%, 1: 85.02%
CHEMBL1613997 0: 7.73%, 1: 92.27%
CHEMBL1614001 0: 98.28%, 1: 1.72%
CHEMBL1614004 0: 14.66%, 1: 85.34%
CHEMBL1614016 0: 12.22%, 1: 87.78%
CHEMBL1614030 0: 19.96%, 1: 80.04%
CHEMBL1614034 0: 2.63%, 1: 97.37%
CHEMBL1614035 0: 77.14%, 1: 22.86%
CHEMBL1614049 0: 39.63%, 1: 60.37%
CHEMBL1614053 0: 42.48%, 1: 57.52%
CHEMBL1614063 0: 95.18%, 1: 4.82%
CHEMBL1614065 0: 9.71%, 1: 90.29%
CHEMBL1614066 0: 94.74%, 1: 5.26%
CHEMBL1614069 0: 56.82%, 1: 43.18%
CHEMBL1614072 0: 50.36%, 1: 49.64%
CHEMBL1614084 0: 13.26%, 1: 86.74%
CHEMBL1614091 0: 4.43%, 1: 95.57%
CHEMBL1614092 0: 47.50%, 1: 52.50%
CHEMBL1614097 0: 46.99%, 1: 53.01%
CHEMBL1614098 0: 97.25%, 1: 2.75%
CHEMBL1614104 0: 7.74%, 1: 92.26%
CHEMBL1614105 0: 6.12%, 1: 93.88%
CHEMBL1614109 0: 59.87%, 1: 40.13%
CHEMBL1614128 0: 10.35%, 1: 89.65%
CHEMBL1614131 0: 97.14%, 1: 2.86%
CHEMBL1614132 0: 42.07%, 1: 57.93%
CHEMBL1614138 0: 67.83%, 1: 32.17%
CHEMBL1614155 0: 25.40%, 1: 74.60%
CHEMBL1614158 0: 74.52%, 1: 25.48%
CHEMBL1614167 0: 16.17%, 1: 83.83%
CHEMBL1614170 0: 68.86%, 1: 31.14%
CHEMBL1614171 0: 93.77%, 1: 6.23%
CHEMBL1614175 0: 80.66%, 1: 19.34%
CHEMBL1614185 0: 26.17%, 1: 73.83%
CHEMBL1614197 0: 5.26%, 1: 94.74%
CHEMBL1614199 0: 49.66%, 1: 50.34%
CHEMBL1614202 0: 7.32%, 1: 92.68%
CHEMBL1614215 0: 52.76%, 1: 47.24%
CHEMBL1614216 0: 48.52%, 1: 51.48%
CHEMBL1614218 0: 22.36%, 1: 77.64%
CHEMBL1614225 0: 62.50%, 1: 37.50%
CHEMBL1614244 0: 38.85%, 1: 61.15%
CHEMBL1614247 0: 44.34%, 1: 55.66%
CHEMBL1614252 0: 60.11%, 1: 39.89%
CHEMBL1614255 0: 61.26%, 1: 38.74%
CHEMBL1614259 0: 32.82%, 1: 67.18%
CHEMBL1614272 0: 67.71%, 1: 32.29%
CHEMBL1614276 0: 51.74%, 1: 48.26%
CHEMBL1614287 0: 85.53%, 1: 14.47%
CHEMBL1614288 0: 72.51%, 1: 27.49%
CHEMBL1614290 0: 4.51%, 1: 95.49%
CHEMBL1614295 0: 2.29%, 1: 97.71%
CHEMBL1614301 0: 19.74%, 1: 80.26%
CHEMBL1614304 0: 63.76%, 1: 36.24%
CHEMBL1614309 0: 27.66%, 1: 72.34%
CHEMBL1614311 0: 53.76%, 1: 46.24%
CHEMBL1614314 0: 94.70%, 1: 5.30%
CHEMBL1614319 0: 72.16%, 1: 27.84%
CHEMBL1614320 0: 29.79%, 1: 70.21%
CHEMBL1614321 0: 60.16%, 1: 39.84%
CHEMBL1614328 0: 46.64%, 1: 53.36%
CHEMBL1614329 0: 40.69%, 1: 59.31%
CHEMBL1614336 0: 47.01%, 1: 52.99%
CHEMBL1614344 0: 67.72%, 1: 32.28%
CHEMBL1614356 0: 43.90%, 1: 56.10%
CHEMBL1614359 0: 50.26%, 1: 49.74%
CHEMBL1614363 0: 53.19%, 1: 46.81%
CHEMBL1614385 0: 93.27%, 1: 6.73%
CHEMBL1614388 0: 56.19%, 1: 43.81%
CHEMBL1614393 0: 45.00%, 1: 55.00%
CHEMBL1614395 0: 11.32%, 1: 88.68%
CHEMBL1614403 0: 88.15%, 1: 11.85%
CHEMBL1614423 0: 57.84%, 1: 42.16%
CHEMBL1614425 0: 38.24%, 1: 61.76%
CHEMBL1614433 0: 60.77%, 1: 39.23%
CHEMBL1614434 0: 80.25%, 1: 19.75%
CHEMBL1614456 0: 3.36%, 1: 96.64%
CHEMBL1614466 0: 10.32%, 1: 89.68%
CHEMBL1614469 0: 24.90%, 1: 75.10%
CHEMBL1614477 0: 43.75%, 1: 56.25%
CHEMBL1614478 0: 12.50%, 1: 87.50%
CHEMBL1614480 0: 85.71%, 1: 14.29%
CHEMBL1614484 0: 9.01%, 1: 90.99%
CHEMBL1614492 0: 40.54%, 1: 59.46%
CHEMBL1614499 0: 44.47%, 1: 55.53%
CHEMBL1614503 0: 40.30%, 1: 59.70%
CHEMBL1614504 0: 17.80%, 1: 82.20%
CHEMBL1614509 0: 79.43%, 1: 20.57%
CHEMBL1614512 0: 62.79%, 1: 37.21%
CHEMBL1614514 0: 83.07%, 1: 16.93%
CHEMBL1614515 0: 19.61%, 1: 80.39%
CHEMBL1614516 0: 96.81%, 1: 3.19%
CHEMBL1614522 0: 28.55%, 1: 71.45%
CHEMBL1614524 0: 12.98%, 1: 87.02%
CHEMBL1614528 0: 40.91%, 1: 59.09%
CHEMBL1614547 0: 72.51%, 1: 27.49%
CHEMBL1614548 0: 17.95%, 1: 82.05%
CHEMBL1614549 0: 84.25%, 1: 15.75%
CHEMBL1614550 0: 9.30%, 1: 90.70%
CHEMBL1614554 0: 71.64%, 1: 28.36%
CHEMBL1676103 0: 39.85%, 1: 60.15%
CHEMBL1737860 0: 3.60%, 1: 96.40%
CHEMBL1737863 0: 36.27%, 1: 63.73%
CHEMBL1737865 0: 61.01%, 1: 38.99%
CHEMBL1737868 0: 79.41%, 1: 20.59%
CHEMBL1737910 0: 45.71%, 1: 54.29%
CHEMBL1737912 0: 26.51%, 1: 73.49%
CHEMBL1737942 0: 44.52%, 1: 55.48%
CHEMBL1737951 0: 51.83%, 1: 48.17%
CHEMBL1737961 0: 93.34%, 1: 6.66%
CHEMBL1737966 0: 21.36%, 1: 78.64%
CHEMBL1737967 0: 89.17%, 1: 10.83%
CHEMBL1737977 0: 13.87%, 1: 86.13%
CHEMBL1737978 0: 18.18%, 1: 81.82%
CHEMBL1737979 0: 8.94%, 1: 91.06%
CHEMBL1738019 0: 24.85%, 1: 75.15%
CHEMBL1738021 0: 10.87%, 1: 89.13%
CHEMBL1738025 0: 33.80%, 1: 66.20%
CHEMBL1738040 0: 73.48%, 1: 26.52%
CHEMBL1738043 0: 46.43%, 1: 53.57%
CHEMBL1738079 0: 3.88%, 1: 96.12%
CHEMBL1738080 0: 76.43%, 1: 23.57%
CHEMBL1738091 0: 89.77%, 1: 10.23%
CHEMBL1738097 0: 72.41%, 1: 27.59%
CHEMBL1738131 0: 68.59%, 1: 31.41%
CHEMBL1738164 0: 79.79%, 1: 20.21%
CHEMBL1738171 0: 85.45%, 1: 14.55%
CHEMBL1738183 0: 59.62%, 1: 40.38%
CHEMBL1738197 0: 36.90%, 1: 63.10%
CHEMBL1738202 0: 3.47%, 1: 96.53%
CHEMBL1738242 0: 16.55%, 1: 83.45%
CHEMBL1738249 0: 86.53%, 1: 13.47%
CHEMBL1738253 0: 7.33%, 1: 92.67%
CHEMBL1738319 0: 59.17%, 1: 40.83%
CHEMBL1738325 0: 96.86%, 1: 3.14%
CHEMBL1738362 0: 21.43%, 1: 78.57%
CHEMBL1738369 0: 56.34%, 1: 43.66%
CHEMBL1738371 0: 81.97%, 1: 18.03%
CHEMBL1738391 0: 68.38%, 1: 31.62%
CHEMBL1738400 0: 77.86%, 1: 22.14%
CHEMBL1738402 0: 54.19%, 1: 45.81%
CHEMBL1738407 0: 85.58%, 1: 14.42%
CHEMBL1738408 0: 2.18%, 1: 97.82%
CHEMBL1738414 0: 5.11%, 1: 94.89%
CHEMBL1738418 0: 68.85%, 1: 31.15%
CHEMBL1738422 0: 4.29%, 1: 95.71%
CHEMBL1738424 0: 77.50%, 1: 22.50%
CHEMBL1738430 0: 25.54%, 1: 74.46%
CHEMBL1738438 0: 2.76%, 1: 97.24%
CHEMBL1738482 0: 7.69%, 1: 92.31%
CHEMBL1738485 0: 17.65%, 1: 82.35%
CHEMBL1738494 0: 97.26%, 1: 2.74%
CHEMBL1738495 0: 1.70%, 1: 98.30%
CHEMBL1738497 0: 64.50%, 1: 35.50%
CHEMBL1738502 0: 15.20%, 1: 84.80%
CHEMBL1738510 0: 54.41%, 1: 45.59%
CHEMBL1738512 0: 3.05%, 1: 96.95%
CHEMBL1738513 0: 28.64%, 1: 71.36%
CHEMBL1738552 0: 64.70%, 1: 35.30%
CHEMBL1738575 0: 65.65%, 1: 34.35%
CHEMBL1738578 0: 23.30%, 1: 76.70%
CHEMBL1738579 0: 22.79%, 1: 77.21%
CHEMBL1738593 0: 17.65%, 1: 82.35%
CHEMBL1738599 0: 16.07%, 1: 83.93%
CHEMBL1738602 0: 42.70%, 1: 57.30%
CHEMBL1738610 0: 42.07%, 1: 57.93%
CHEMBL1738611 0: 2.23%, 1: 97.77%
CHEMBL1738632 0: 86.32%, 1: 13.68%
CHEMBL1738633 0: 4.05%, 1: 95.95%
CHEMBL1738639 0: 64.47%, 1: 35.53%
CHEMBL1738642 0: 34.82%, 1: 65.18%
CHEMBL1738670 0: 9.18%, 1: 90.82%
CHEMBL1738673 0: 12.98%, 1: 87.02%
CHEMBL1738679 0: 63.95%, 1: 36.05%
CHEMBL1738682 0: 62.45%, 1: 37.55%
CHEMBL1794296 0: 44.02%, 1: 55.98%
CHEMBL1794303 0: 2.71%, 1: 97.29%
CHEMBL1794320 0: 96.68%, 1: 3.32%
CHEMBL1794327 0: 91.59%, 1: 8.41%
CHEMBL1794336 0: 53.07%, 1: 46.93%
CHEMBL1794350 0: 3.44%, 1: 96.56%
CHEMBL1794355 0: 5.26%, 1: 94.74%
CHEMBL1794356 0: 13.92%, 1: 86.08%
CHEMBL1794358 0: 94.14%, 1: 5.86%
CHEMBL1794365 0: 7.12%, 1: 92.88%
CHEMBL1794383 0: 6.90%, 1: 93.10%
CHEMBL1794387 0: 65.04%, 1: 34.96%
CHEMBL1794393 0: 67.55%, 1: 32.45%
CHEMBL1794396 0: 3.96%, 1: 96.04%
CHEMBL1794410 0: 23.98%, 1: 76.02%
CHEMBL1794413 0: 6.02%, 1: 93.98%
CHEMBL1794438 0: 40.17%, 1: 59.83%
CHEMBL1794445 0: 87.84%, 1: 12.16%
CHEMBL1794452 0: 94.30%, 1: 5.70%
CHEMBL1794457 0: 92.98%, 1: 7.02%
CHEMBL1794460 0: 28.63%, 1: 71.37%
CHEMBL1794467 0: 18.39%, 1: 81.61%
CHEMBL1794475 0: 56.02%, 1: 43.98%
CHEMBL1794484 0: 44.93%, 1: 55.07%
CHEMBL1794494 0: 5.33%, 1: 94.67%
CHEMBL1794497 0: 8.47%, 1: 91.53%
CHEMBL1794499 0: 73.90%, 1: 26.10%
CHEMBL1794508 0: 94.63%, 1: 5.37%
CHEMBL1794516 0: 3.21%, 1: 96.79%
CHEMBL1794522 0: 58.65%, 1: 41.35%
CHEMBL1794528 0: 79.86%, 1: 20.14%
CHEMBL1794531 0: 92.03%, 1: 7.97%
CHEMBL1794548 0: 88.74%, 1: 11.26%
CHEMBL1794566 0: 6.05%, 1: 93.95%
CHEMBL1794567 0: 9.61%, 1: 90.39%
CHEMBL1794570 0: 92.79%, 1: 7.21%
CHEMBL1794571 0: 53.55%, 1: 46.45%
CHEMBL1794573 0: 10.54%, 1: 89.46%
CHEMBL1794574 0: 1.09%, 1: 98.91%
CHEMBL1794578 0: 46.99%, 1: 53.01%
CHEMBL1794581 0: 19.65%, 1: 80.35%
CHEMBL1863510 0: 2.14%, 1: 97.86%
CHEMBL1863512 0: 2.79%, 1: 97.21%
CHEMBL1909084 0: 99.28%, 1: 0.72%
CHEMBL1909085 0: 92.10%, 1: 7.90%
CHEMBL1909086 0: 91.52%, 1: 8.48%
CHEMBL1909087 0: 91.86%, 1: 8.14%
CHEMBL1909088 0: 87.97%, 1: 12.03%
CHEMBL1909089 0: 88.69%, 1: 11.31%
CHEMBL1909090 0: 90.81%, 1: 9.19%
CHEMBL1909091 0: 97.24%, 1: 2.76%
CHEMBL1909092 0: 97.14%, 1: 2.86%
CHEMBL1909093 0: 98.42%, 1: 1.58%
CHEMBL1909094 0: 90.85%, 1: 9.15%
CHEMBL1909095 0: 98.56%, 1: 1.44%
CHEMBL1909097 0: 99.28%, 1: 0.72%
CHEMBL1909102 0: 92.94%, 1: 7.06%
CHEMBL1909103 0: 97.23%, 1: 2.77%
CHEMBL1909104 0: 87.06%, 1: 12.94%
CHEMBL1909105 0: 88.70%, 1: 11.30%
CHEMBL1909106 0: 98.57%, 1: 1.43%
CHEMBL1909107 0: 97.86%, 1: 2.14%
CHEMBL1909108 0: 93.04%, 1: 6.96%
CHEMBL1909109 0: 90.35%, 1: 9.65%
CHEMBL1909110 0: 92.47%, 1: 7.53%
CHEMBL1909111 0: 94.03%, 1: 5.97%
CHEMBL1909112 0: 90.91%, 1: 9.09%
CHEMBL1909114 0: 96.50%, 1: 3.50%
CHEMBL1909115 0: 96.00%, 1: 4.00%
CHEMBL1909116 0: 96.98%, 1: 3.02%
CHEMBL1909121 0: 92.80%, 1: 7.20%
CHEMBL1909123 0: 99.16%, 1: 0.84%
CHEMBL1909124 0: 99.40%, 1: 0.60%
CHEMBL1909130 0: 95.56%, 1: 4.44%
CHEMBL1909131 0: 97.01%, 1: 2.99%
CHEMBL1909132 0: 96.15%, 1: 3.85%
CHEMBL1909134 0: 95.19%, 1: 4.81%
CHEMBL1909135 0: 95.76%, 1: 4.24%
CHEMBL1909136 0: 93.20%, 1: 6.80%
CHEMBL1909138 0: 96.16%, 1: 3.84%
CHEMBL1909139 0: 93.84%, 1: 6.16%
CHEMBL1909140 0: 93.87%, 1: 6.13%
CHEMBL1909141 0: 89.73%, 1: 10.27%
CHEMBL1909142 0: 99.04%, 1: 0.96%
CHEMBL1909143 0: 93.01%, 1: 6.99%
CHEMBL1909145 0: 97.97%, 1: 2.03%
CHEMBL1909148 0: 98.57%, 1: 1.43%
CHEMBL1909150 0: 95.10%, 1: 4.90%
CHEMBL1909156 0: 94.28%, 1: 5.72%
CHEMBL1909157 0: 96.38%, 1: 3.62%
CHEMBL1909158 0: 99.05%, 1: 0.95%
CHEMBL1909159 0: 93.18%, 1: 6.82%
CHEMBL1909165 0: 97.79%, 1: 2.21%
CHEMBL1909169 0: 98.32%, 1: 1.68%
CHEMBL1909170 0: 92.35%, 1: 7.65%
CHEMBL1909171 0: 93.29%, 1: 6.71%
CHEMBL1909172 0: 92.82%, 1: 7.18%
CHEMBL1909173 0: 92.38%, 1: 7.62%
CHEMBL1909174 0: 92.71%, 1: 7.29%
CHEMBL1909180 0: 98.20%, 1: 1.80%
CHEMBL1909181 0: 97.71%, 1: 2.29%
CHEMBL1909182 0: 96.99%, 1: 3.01%
CHEMBL1909184 0: 98.93%, 1: 1.07%
CHEMBL1909186 0: 99.52%, 1: 0.48%
CHEMBL1909190 0: 98.66%, 1: 1.34%
CHEMBL1909191 0: 94.99%, 1: 5.01%
CHEMBL1909192 0: 99.52%, 1: 0.48%
CHEMBL1909200 0: 97.97%, 1: 2.03%
CHEMBL1909201 0: 97.72%, 1: 2.28%
CHEMBL1909203 0: 98.05%, 1: 1.95%
CHEMBL1909204 0: 97.17%, 1: 2.83%
CHEMBL1909205 0: 98.67%, 1: 1.33%
CHEMBL1909206 0: 99.16%, 1: 0.84%
CHEMBL1909209 0: 92.34%, 1: 7.66%
CHEMBL1909210 0: 96.05%, 1: 3.95%
CHEMBL1909211 0: 88.80%, 1: 11.20%
CHEMBL1909212 0: 98.30%, 1: 1.70%
CHEMBL1909213 0: 99.28%, 1: 0.72%
CHEMBL1909214 0: 98.79%, 1: 1.21%
CHEMBL1909215 0: 95.61%, 1: 4.39%
CHEMBL1963686 0: 61.95%, 1: 38.05%
CHEMBL1963687 0: 64.42%, 1: 35.58%
CHEMBL1963688 0: 62.52%, 1: 37.48%
CHEMBL1963689 0: 89.69%, 1: 10.31%
CHEMBL1963690 0: 57.36%, 1: 42.64%
CHEMBL1963691 0: 49.33%, 1: 50.67%
CHEMBL1963692 0: 53.73%, 1: 46.27%
CHEMBL1963693 0: 71.00%, 1: 29.00%
CHEMBL1963694 0: 91.05%, 1: 8.95%
CHEMBL1963695 0: 74.90%, 1: 25.10%
CHEMBL1963696 0: 77.85%, 1: 22.15%
CHEMBL1963697 0: 74.06%, 1: 25.94%
CHEMBL1963698 0: 74.60%, 1: 25.40%
CHEMBL1963699 0: 83.18%, 1: 16.82%
CHEMBL1963701 0: 67.32%, 1: 32.68%
CHEMBL1963702 0: 83.82%, 1: 16.18%
CHEMBL1963703 0: 68.65%, 1: 31.35%
CHEMBL1963704 0: 65.28%, 1: 34.72%
CHEMBL1963705 0: 55.92%, 1: 44.08%
CHEMBL1963706 0: 53.38%, 1: 46.62%
CHEMBL1963707 0: 53.25%, 1: 46.75%
CHEMBL1963708 0: 48.77%, 1: 51.23%
CHEMBL1963710 0: 63.21%, 1: 36.79%
CHEMBL1963711 0: 94.72%, 1: 5.28%
CHEMBL1963712 0: 75.76%, 1: 24.24%
CHEMBL1963714 0: 74.21%, 1: 25.79%
CHEMBL1963715 0: 50.28%, 1: 49.72%
CHEMBL1963716 0: 91.91%, 1: 8.09%
CHEMBL1963717 0: 55.50%, 1: 44.50%
CHEMBL1963718 0: 65.45%, 1: 34.55%
CHEMBL1963719 0: 63.39%, 1: 36.61%
CHEMBL1963720 0: 69.73%, 1: 30.27%
CHEMBL1963721 0: 56.94%, 1: 43.06%
CHEMBL1963722 0: 49.40%, 1: 50.60%
CHEMBL1963723 0: 53.82%, 1: 46.18%
CHEMBL1963724 0: 67.32%, 1: 32.68%
CHEMBL1963725 0: 69.97%, 1: 30.03%
CHEMBL1963727 0: 56.59%, 1: 43.41%
CHEMBL1963728 0: 92.02%, 1: 7.98%
CHEMBL1963729 0: 84.64%, 1: 15.36%
CHEMBL1963731 0: 61.35%, 1: 38.65%
CHEMBL1963733 0: 79.77%, 1: 20.23%
CHEMBL1963734 0: 79.51%, 1: 20.49%
CHEMBL1963735 0: 71.66%, 1: 28.34%
CHEMBL1963736 0: 78.32%, 1: 21.68%
CHEMBL1963737 0: 86.79%, 1: 13.21%
CHEMBL1963738 0: 61.18%, 1: 38.82%
CHEMBL1963739 0: 84.32%, 1: 15.68%
CHEMBL1963740 0: 64.30%, 1: 35.70%
CHEMBL1963741 0: 67.03%, 1: 32.97%
CHEMBL1963742 0: 73.47%, 1: 26.53%
CHEMBL1963743 0: 66.24%, 1: 33.76%
CHEMBL1963744 0: 71.97%, 1: 28.03%
CHEMBL1963745 0: 67.16%, 1: 32.84%
CHEMBL1963746 0: 66.34%, 1: 33.66%
CHEMBL1963747 0: 81.33%, 1: 18.67%
CHEMBL1963748 0: 58.75%, 1: 41.25%
CHEMBL1963749 0: 55.02%, 1: 44.98%
CHEMBL1963750 0: 58.61%, 1: 41.39%
CHEMBL1963751 0: 70.90%, 1: 29.10%
CHEMBL1963752 0: 64.78%, 1: 35.22%
CHEMBL1963753 0: 88.02%, 1: 11.98%
CHEMBL1963754 0: 59.12%, 1: 40.88%
CHEMBL1963756 0: 61.66%, 1: 38.34%
CHEMBL1963757 0: 73.28%, 1: 26.72%
CHEMBL1963758 0: 82.56%, 1: 17.44%
CHEMBL1963759 0: 81.38%, 1: 18.62%
CHEMBL1963760 0: 86.36%, 1: 13.64%
CHEMBL1963761 0: 83.63%, 1: 16.37%
CHEMBL1963763 0: 72.14%, 1: 27.86%
CHEMBL1963764 0: 62.50%, 1: 37.50%
CHEMBL1963765 0: 86.41%, 1: 13.59%
CHEMBL1963766 0: 81.38%, 1: 18.62%
CHEMBL1963767 0: 77.76%, 1: 22.24%
CHEMBL1963768 0: 77.63%, 1: 22.37%
CHEMBL1963770 0: 89.44%, 1: 10.56%
CHEMBL1963771 0: 53.53%, 1: 46.47%
CHEMBL1963772 0: 48.99%, 1: 51.01%
CHEMBL1963773 0: 59.00%, 1: 41.00%
CHEMBL1963775 0: 76.92%, 1: 23.08%
CHEMBL1963776 0: 87.71%, 1: 12.29%
CHEMBL1963777 0: 63.92%, 1: 36.08%
CHEMBL1963778 0: 51.70%, 1: 48.30%
CHEMBL1963779 0: 54.69%, 1: 45.31%
CHEMBL1963780 0: 89.06%, 1: 10.94%
CHEMBL1963781 0: 94.31%, 1: 5.69%
CHEMBL1963782 0: 75.86%, 1: 24.14%
CHEMBL1963783 0: 57.36%, 1: 42.64%
CHEMBL1963785 0: 70.10%, 1: 29.90%
CHEMBL1963786 0: 50.08%, 1: 49.92%
CHEMBL1963787 0: 66.39%, 1: 33.61%
CHEMBL1963789 0: 67.34%, 1: 32.66%
CHEMBL1963790 0: 57.93%, 1: 42.07%
CHEMBL1963791 0: 82.08%, 1: 17.92%
CHEMBL1963792 0: 85.73%, 1: 14.27%
CHEMBL1963793 0: 65.48%, 1: 34.52%
CHEMBL1963794 0: 77.53%, 1: 22.47%
CHEMBL1963795 0: 46.98%, 1: 53.02%
CHEMBL1963796 0: 69.76%, 1: 30.24%
CHEMBL1963797 0: 83.64%, 1: 16.36%
CHEMBL1963798 0: 89.45%, 1: 10.55%
CHEMBL1963799 0: 43.54%, 1: 56.46%
CHEMBL1963800 0: 64.88%, 1: 35.12%
CHEMBL1963801 0: 75.90%, 1: 24.10%
CHEMBL1963802 0: 61.41%, 1: 38.59%
CHEMBL1963803 0: 87.44%, 1: 12.56%
CHEMBL1963804 0: 65.90%, 1: 34.10%
CHEMBL1963805 0: 53.77%, 1: 46.23%
CHEMBL1963806 0: 41.19%, 1: 58.81%
CHEMBL1963807 0: 36.83%, 1: 63.17%
CHEMBL1963808 0: 68.82%, 1: 31.18%
CHEMBL1963809 0: 79.97%, 1: 20.03%
CHEMBL1963810 0: 43.69%, 1: 56.31%
CHEMBL1963811 0: 73.98%, 1: 26.02%
CHEMBL1963812 0: 53.08%, 1: 46.92%
CHEMBL1963813 0: 79.19%, 1: 20.81%
CHEMBL1963814 0: 51.10%, 1: 48.90%
CHEMBL1963815 0: 71.80%, 1: 28.20%
CHEMBL1963816 0: 89.64%, 1: 10.36%
CHEMBL1963817 0: 71.90%, 1: 28.10%
CHEMBL1963818 0: 55.13%, 1: 44.87%
CHEMBL1963819 0: 62.64%, 1: 37.36%
CHEMBL1963820 0: 84.46%, 1: 15.54%
CHEMBL1963821 0: 79.31%, 1: 20.69%
CHEMBL1963822 0: 90.91%, 1: 9.09%
CHEMBL1963823 0: 76.61%, 1: 23.39%
CHEMBL1963824 0: 39.57%, 1: 60.43%
CHEMBL1963825 0: 51.12%, 1: 48.88%
CHEMBL1963826 0: 59.25%, 1: 40.75%
CHEMBL1963827 0: 50.74%, 1: 49.26%
CHEMBL1963828 0: 80.86%, 1: 19.14%
CHEMBL1963829 0: 86.41%, 1: 13.59%
CHEMBL1963831 0: 52.56%, 1: 47.44%
CHEMBL1963832 0: 72.98%, 1: 27.02%
CHEMBL1963833 0: 87.13%, 1: 12.87%
CHEMBL1963834 0: 59.69%, 1: 40.31%
CHEMBL1963836 0: 85.18%, 1: 14.82%
CHEMBL1963837 0: 26.01%, 1: 73.99%
CHEMBL1963838 0: 75.83%, 1: 24.17%
CHEMBL1963846 0: 85.07%, 1: 14.93%
CHEMBL1963867 0: 73.13%, 1: 26.87%
CHEMBL1963893 0: 85.61%, 1: 14.39%
CHEMBL1963898 0: 75.62%, 1: 24.38%
CHEMBL1963907 0: 64.35%, 1: 35.65%
CHEMBL1963910 0: 10.50%, 1: 89.50%
CHEMBL1963915 0: 15.00%, 1: 85.00%
CHEMBL1963916 0: 40.48%, 1: 59.52%
CHEMBL1963918 0: 73.35%, 1: 26.65%
CHEMBL1963930 0: 3.64%, 1: 96.36%
CHEMBL1963933 0: 93.91%, 1: 6.09%
CHEMBL1963934 0: 3.64%, 1: 96.36%
CHEMBL1963937 0: 39.27%, 1: 60.73%
CHEMBL1963938 0: 65.85%, 1: 34.15%
CHEMBL1963940 0: 33.79%, 1: 66.21%
CHEMBL1963947 0: 6.25%, 1: 93.75%
CHEMBL1963966 0: 3.29%, 1: 96.71%
CHEMBL1963968 0: 51.52%, 1: 48.48%
CHEMBL1963969 0: 25.83%, 1: 74.17%
CHEMBL1963971 0: 10.20%, 1: 89.80%
CHEMBL1963974 0: 88.26%, 1: 11.74%
CHEMBL1963983 0: 76.39%, 1: 23.61%
CHEMBL1964000 0: 76.77%, 1: 23.23%
CHEMBL1964005 0: 6.67%, 1: 93.33%
CHEMBL1964010 0: 76.06%, 1: 23.94%
CHEMBL1964015 0: 73.77%, 1: 26.23%
CHEMBL1964022 0: 76.61%, 1: 23.39%
CHEMBL1964023 0: 73.75%, 1: 26.25%
CHEMBL1964081 0: 47.98%, 1: 52.02%
CHEMBL1964095 0: 65.41%, 1: 34.59%
CHEMBL1964096 0: 11.36%, 1: 88.64%
CHEMBL1964100 0: 79.62%, 1: 20.38%
CHEMBL1964101 0: 34.62%, 1: 65.38%
CHEMBL1964102 0: 71.54%, 1: 28.46%
CHEMBL1964103 0: 51.58%, 1: 48.42%
CHEMBL1964104 0: 63.88%, 1: 36.12%
CHEMBL1964105 0: 64.53%, 1: 35.47%
CHEMBL1964106 0: 57.96%, 1: 42.04%
CHEMBL1964108 0: 69.62%, 1: 30.38%
CHEMBL1964111 0: 73.41%, 1: 26.59%
CHEMBL1964112 0: 87.59%, 1: 12.41%
CHEMBL1964114 0: 61.11%, 1: 38.89%
CHEMBL1964115 0: 43.36%, 1: 56.64%
CHEMBL1964116 0: 70.55%, 1: 29.45%
CHEMBL1964117 0: 63.66%, 1: 36.34%
CHEMBL1964118 0: 60.33%, 1: 39.67%
CHEMBL1964119 0: 57.23%, 1: 42.77%
CHEMBL2028073 0: 76.99%, 1: 23.01%
CHEMBL2028074 0: 74.88%, 1: 25.12%
CHEMBL2028075 0: 56.46%, 1: 43.54%
CHEMBL2028076 0: 85.04%, 1: 14.96%
CHEMBL2028077 0: 96.19%, 1: 3.81%
CHEMBL2095143 0: 93.77%, 1: 6.23%
CHEMBL2098499 0: 74.45%, 1: 25.55%
CHEMBL2114715 0: 60.31%, 1: 39.69%
CHEMBL2114716 0: 12.27%, 1: 87.73%
CHEMBL2114719 0: 77.41%, 1: 22.59%
CHEMBL2114725 0: 51.74%, 1: 48.26%
CHEMBL2114727 0: 16.96%, 1: 83.04%
CHEMBL2114728 0: 9.82%, 1: 90.18%
CHEMBL2114737 0: 87.93%, 1: 12.07%
CHEMBL2114742 0: 33.90%, 1: 66.10%
CHEMBL2114748 0: 59.44%, 1: 40.56%
CHEMBL2114752 0: 15.02%, 1: 84.98%
CHEMBL2114753 0: 8.36%, 1: 91.64%
CHEMBL2114761 0: 78.40%, 1: 21.60%
CHEMBL2114764 0: 42.92%, 1: 57.08%
CHEMBL2114771 0: 64.02%, 1: 35.98%
CHEMBL2114791 0: 1.99%, 1: 98.01%
CHEMBL2114797 0: 42.41%, 1: 57.59%
CHEMBL2114811 0: 10.16%, 1: 89.84%
CHEMBL2114814 0: 44.98%, 1: 55.02%
CHEMBL2114816 0: 37.94%, 1: 62.06%
CHEMBL2114818 0: 4.15%, 1: 95.85%
CHEMBL2114820 0: 16.31%, 1: 83.69%
CHEMBL2114821 0: 18.77%, 1: 81.23%
CHEMBL2114823 0: 20.90%, 1: 79.10%
CHEMBL2114825 0: 18.62%, 1: 81.38%
CHEMBL2114827 0: 93.80%, 1: 6.20%
CHEMBL2114829 0: 25.17%, 1: 74.83%
CHEMBL2114830 0: 20.17%, 1: 79.83%
CHEMBL2114839 0: 28.69%, 1: 71.31%
CHEMBL2114842 0: 62.00%, 1: 38.00%
CHEMBL2114844 0: 32.59%, 1: 67.41%
CHEMBL2114847 0: 13.84%, 1: 86.16%
CHEMBL2114850 0: 8.62%, 1: 91.38%
CHEMBL2114852 0: 3.84%, 1: 96.16%
CHEMBL2114857 0: 50.00%, 1: 50.00%
CHEMBL2114858 0: 32.36%, 1: 67.64%
CHEMBL2114863 0: 33.55%, 1: 66.45%
CHEMBL2114865 0: 3.43%, 1: 96.57%
CHEMBL2114872 0: 9.51%, 1: 90.49%
CHEMBL2114874 0: 28.03%, 1: 71.97%
CHEMBL2114882 0: 92.25%, 1: 7.75%
CHEMBL2114896 0: 51.22%, 1: 48.78%
CHEMBL2114899 0: 87.76%, 1: 12.24%
CHEMBL2114909 0: 1.94%, 1: 98.06%
CHEMBL2114916 0: 42.50%, 1: 57.50%
CHEMBL2114926 0: 41.57%, 1: 58.43%
CHEMBL2114928 0: 86.06%, 1: 13.94%
CHEMBL2114930 0: 4.68%, 1: 95.32%
CHEMBL2114931 0: 97.44%, 1: 2.56%
CHEMBL2114932 0: 4.35%, 1: 95.65%
CHEMBL2354206 0: 93.86%, 1: 6.14%
CHEMBL2354207 0: 15.46%, 1: 84.54%
CHEMBL2354217 0: 20.69%, 1: 79.31%
CHEMBL2354227 0: 21.70%, 1: 78.30%
CHEMBL2354228 0: 59.88%, 1: 40.12%
CHEMBL2354248 0: 96.55%, 1: 3.45%
CHEMBL2354256 0: 7.85%, 1: 92.15%
CHEMBL2354269 0: 9.42%, 1: 90.58%
CHEMBL2354274 0: 5.26%, 1: 94.74%
CHEMBL2354276 0: 75.30%, 1: 24.70%
CHEMBL2354289 0: 34.38%, 1: 65.62%
CHEMBL2354292 0: 77.91%, 1: 22.09%
CHEMBL2354303 0: 86.76%, 1: 13.24%
CHEMBL2354305 0: 14.37%, 1: 85.63%
CHEMBL2354308 0: 9.09%, 1: 90.91%
CHEMBL2378059 0: 2.84%, 1: 97.16%
CHEMBL2449559 0: 17.37%, 1: 82.63%
CHEMBL3214794 0: 64.60%, 1: 35.40%
CHEMBL3214801 0: 28.46%, 1: 71.54%
CHEMBL3214812 0: 33.19%, 1: 66.81%
CHEMBL3214816 0: 25.43%, 1: 74.57%
CHEMBL3214851 0: 77.02%, 1: 22.98%
CHEMBL3214906 0: 61.72%, 1: 38.28%
CHEMBL3214907 0: 48.12%, 1: 51.88%
CHEMBL3214929 0: 44.25%, 1: 55.75%
CHEMBL3214930 0: 79.15%, 1: 20.85%
CHEMBL3214944 0: 32.70%, 1: 67.30%
CHEMBL3214958 0: 69.31%, 1: 30.69%
CHEMBL3214959 0: 31.09%, 1: 68.91%
CHEMBL3214970 0: 20.09%, 1: 79.91%
CHEMBL3214992 0: 27.76%, 1: 72.24%
CHEMBL3214993 0: 32.48%, 1: 67.52%
CHEMBL3214997 0: 27.27%, 1: 72.73%
CHEMBL3215006 0: 73.18%, 1: 26.82%
CHEMBL3215013 0: 10.05%, 1: 89.95%
CHEMBL3215025 0: 7.49%, 1: 92.51%
CHEMBL3215034 0: 39.87%, 1: 60.13%
CHEMBL3215078 0: 41.84%, 1: 58.16%
CHEMBL3215092 0: 57.98%, 1: 42.02%
CHEMBL3215096 0: 30.30%, 1: 69.70%
CHEMBL3215112 0: 7.48%, 1: 92.52%
CHEMBL3215116 0: 83.87%, 1: 16.13%
CHEMBL3215128 0: 14.05%, 1: 85.95%
CHEMBL3215154 0: 44.44%, 1: 55.56%
CHEMBL3215157 0: 5.96%, 1: 94.04%
CHEMBL3215158 0: 12.59%, 1: 87.41%
CHEMBL3215171 0: 97.98%, 1: 2.02%
CHEMBL3215176 0: 91.72%, 1: 8.28%
CHEMBL3215185 0: 91.95%, 1: 8.05%
CHEMBL3215187 0: 7.98%, 1: 92.02%
CHEMBL3215216 0: 15.79%, 1: 84.21%
CHEMBL3215220 0: 91.18%, 1: 8.82%
CHEMBL3215227 0: 32.66%, 1: 67.34%
CHEMBL3215228 0: 88.71%, 1: 11.29%
CHEMBL3215276 0: 7.46%, 1: 92.54%
CHEMBL3215277 0: 90.53%, 1: 9.47%
CHEMBL3215288 0: 27.05%, 1: 72.95%
CHEMBL829401 0: 10.53%, 1: 89.47%
CHEMBL830839 0: 3.07%, 1: 96.93%
CHEMBL830842 0: 10.53%, 1: 89.47%
CHEMBL914418 0: 14.93%, 1: 85.07%
CHEMBL918058 0: 93.33%, 1: 6.67%
Total scaffolds = 77 | train scaffolds = 66 | val scaffolds = 11 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([0.75,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 133 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 99 | train scaffolds = 64 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan, 0.32608696,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0, 46,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([       nan,        nan, 0.33333333,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.4, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0, 31,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 60 | train scaffolds = 48 | val scaffolds = 12 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.90566038, 0.78787879,
       0.94117647, 0.91304348, 0.95918367, 0.8974359 , 0.65853659,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 53, 33, 51, 46, 49, 39, 41,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 3, 3, 3, 3, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 0.66666667, 0.5       ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 3, 2, 3, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, 1. , 1. , 1. , 1. , 1. , 0.6, 0.6, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 7, 4, 7, 6, 7, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 61 | train scaffolds = 42 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.84848485, 0.5625    ,
       0.87179487, 0.83333333, 0.97297297, 0.96153846, 0.71428571,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 33, 48, 39, 36, 37, 26, 28,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan, nan,  0.,  1.,  0.,  1.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  0., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0.,  1., nan,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  1.,  0., nan,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 1.        , 1.        ,
       1.        , 1.        , 1.        , 0.66666667, 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 64 | train scaffolds = 42 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.92156863, 0.66666667,
       0.92307692, 0.89285714, 0.98305085, 0.9047619 , 0.625     ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 51, 39, 65, 56, 59, 42, 48,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  1.,  0., nan,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, 1. , 0. , 1. , 0.5, 1. , 1. , 0. , nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 65 | train scaffolds = 48 | val scaffolds = 17 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.91304348, 0.72222222,
       0.92857143, 0.9       , 0.98113208, 0.95      , 0.67391304,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 46, 36, 56, 60, 53, 40, 46,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0., nan,  0.,  0., nan,  0.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  0.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 71 | train scaffolds = 49 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.95918367, 0.72972973,
       0.94915254, 0.96226415, 0.96969697, 0.93023256, 0.65306122,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 49, 37, 59, 53, 66, 43, 49,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 0.66666667, 0.5       ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 3, 2, 3, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 48 | train scaffolds = 26 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.97435897, 0.80769231,
       1.        , 0.975     , 0.97674419, 0.91304348, 0.80487805,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 39, 26, 42, 40, 43, 46, 41,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0., nan, nan,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 3, 3, 3, 3, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 1.        , 1.        ,
       1.        , 1.        , 1.        , 0.6       , 0.33333333,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 5, 3, 5, 4, 5, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 51 | train scaffolds = 30 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.97560976, 0.78571429,
       1.        , 0.95652174, 0.97959184, 0.95121951, 0.65384615,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 41, 28, 48, 46, 49, 41, 52,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 0, 2, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  0., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0., nan, nan,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 32 | train scaffolds = 21 | val scaffolds = 11 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.63492063, 0.44444444,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 63, 45,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([     nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan,      nan,      nan, 0.8     , 0.640625,
            nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan]), array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 105,  64,   0,
         0,   0,   0,   0,   0,   0,   0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.53846154, 0.47058824,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 39, 34,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 0.8 ,
       0.75,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0., nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 31 | train scaffolds = 15 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.53333333, 0.42      ,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 45, 50,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([     nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan,      nan,      nan, 0.703125, 0.625   ,
            nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 64, 72,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.47058824, 0.44736842,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34, 38,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 0.75,
       0.6 ,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10, 10,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0. , 0.5, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 245 | train scaffolds = 196 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 409 | train scaffolds = 339 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 98 | train scaffolds = 67 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.52173913,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 46,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.66666667,
       1.        ,        nan, 0.76      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 33,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.33333333,
              nan,        nan, 1.        ,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 213 | train scaffolds = 168 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
Total scaffolds = 94 | train scaffolds = 64 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.64705882,
              nan,        nan, 0.56410256,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34,  0,  0,
       39,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.84      ,
       1.        ,        nan, 0.65517241,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  1,  0,
       29,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 139 | train scaffolds = 111 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
Total scaffolds = 183 | train scaffolds = 146 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]))]
Total scaffolds = 71 | train scaffolds = 46 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86363636,
              nan,        nan, 0.81818182,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  0,  0,
       22,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.89473684,
       1.        ,        nan, 0.85      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19,  1,  0,
       20,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 188 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
Total scaffolds = 626 | train scaffolds = 501 | val scaffolds = 125 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 179 | train scaffolds = 142 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 84 | train scaffolds = 56 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.85714286,
              nan,        nan, 0.77272727,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  0,  0,
       22,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.90909091,
       1.        ,        nan, 0.9047619 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 446 | train scaffolds = 358 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 67 | train scaffolds = 44 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.84      ,
              nan,        nan, 0.83333333,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  0,  0,
       24,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95      ,
       1.        ,        nan, 0.85714286,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 1. , nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 115 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 74 | train scaffolds = 52 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.9 ,  nan,  nan, 0.75,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  0,  0,
       20,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.94736842,
       1.        ,        nan, 0.85714286,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0]))]
Total scaffolds = 31 | train scaffolds = 16 | val scaffolds = 15 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 53 | train scaffolds = 47 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 262 | train scaffolds = 209 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 119 | train scaffolds = 92 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 76 | train scaffolds = 50 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan, 0.  , 0.8 ,  nan,  nan, 0.72,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 25,  0,  0,
       25,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 52 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.73333333,
              nan,        nan, 0.67741935,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,  0,  0,
       31,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 147 | train scaffolds = 116 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 77 | train scaffolds = 50 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.71428571,
              nan,        nan, 0.65517241,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95652174,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 161 | train scaffolds = 128 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 53 | train scaffolds = 47 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 82 | train scaffolds = 54 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.75      ,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 208 | train scaffolds = 164 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 160 | train scaffolds = 123 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 257 | train scaffolds = 189 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
Total scaffolds = 744 | train scaffolds = 602 | val scaffolds = 142 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 448 | train scaffolds = 357 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 145 | train scaffolds = 110 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 85 | train scaffolds = 59 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.63636364,
              nan,        nan, 0.625     ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 33,  0,  0,
       32,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95652174,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0]))]
Total scaffolds = 77 | train scaffolds = 53 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.74193548,
              nan,        nan, 0.68965517,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91304348,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 232 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 224 | train scaffolds = 175 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 75 | train scaffolds = 50 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.72,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  0,  0,
       25,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86956522,
       1.        ,        nan, 0.7826087 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 240 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 113 | train scaffolds = 88 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       0.5, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 57 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.71428571,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86956522,
       1.        ,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       24,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]))]
Total scaffolds = 91 | train scaffolds = 59 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.63888889,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 36,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.78571429,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 1. , nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 273 | train scaffolds = 222 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
Total scaffolds = 283 | train scaffolds = 223 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 298 | train scaffolds = 247 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 204 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 81 | train scaffolds = 53 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.67647059,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0]))]
Total scaffolds = 122 | train scaffolds = 97 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 752 | train scaffolds = 591 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 656 | train scaffolds = 506 | val scaffolds = 150 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 188 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 297 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 506 | train scaffolds = 408 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 342 | train scaffolds = 270 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 120 | train scaffolds = 97 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 404 | train scaffolds = 318 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 78 | train scaffolds = 52 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.79166667,
              nan,        nan, 0.73913043,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 24,  0,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , 1. , nan, 0.9, nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  1,  0,
       20,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0]))]
Total scaffolds = 87 | train scaffolds = 58 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.75      ,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.79166667,
       1.        ,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  1,  0,
       24,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 81 | train scaffolds = 50 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.9       ,
              nan,        nan, 0.77777778,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,
        9,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11,  0,  0,
       11,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 115 | train scaffolds = 90 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 87 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 276 | train scaffolds = 220 | val scaffolds = 56 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 685 | train scaffolds = 555 | val scaffolds = 130 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 159 | train scaffolds = 127 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 147 | train scaffolds = 126 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 447 | train scaffolds = 355 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 270 | train scaffolds = 213 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 208 | train scaffolds = 165 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 57 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 298 | train scaffolds = 247 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 113 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 63 | train scaffolds = 47 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 272 | train scaffolds = 218 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 163 | train scaffolds = 129 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 342 | train scaffolds = 270 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 120 | train scaffolds = 95 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 86 | train scaffolds = 65 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.73913043,
              nan,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 23,  0,  0,
       20,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.92857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14,  1,  0,
       14,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 224 | train scaffolds = 179 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 83 | train scaffolds = 59 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([    nan,     nan,     nan,     nan,     nan,     nan,     nan,
           nan,     nan,     nan,     nan,     nan,     nan,     nan,
       0.6875 ,     nan,     nan, 0.65625,     nan,     nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 32,  0,  0,
       32,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91666667,
       1.        ,        nan, 0.76      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 37 | train scaffolds = 24 | val scaffolds = 13 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 270 | train scaffolds = 213 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 146 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 84 | train scaffolds = 58 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.76923077,
              nan,        nan, 0.7037037 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  0,  0,
       27,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.84, 1.  ,  nan, 0.76,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
Total scaffolds = 156 | train scaffolds = 119 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 426 | train scaffolds = 335 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 73 | train scaffolds = 44 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.85714286,
       1.        ,        nan, 0.92307692,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14,  1,  0,
       13,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.76470588,
              nan,        nan, 0.82352941,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 17,  0,  0,
       17,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 111 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, nan, nan, 0.5]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 644 | train scaffolds = 506 | val scaffolds = 138 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 346 | train scaffolds = 268 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 206 | train scaffolds = 159 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 75 | train scaffolds = 50 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.77419355,
              nan,        nan, 0.72413793,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 98 | train scaffolds = 76 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 154 | train scaffolds = 123 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 233 | train scaffolds = 186 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 77 | train scaffolds = 51 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.76666667,
              nan,        nan, 0.7       ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 30,  0,  0,
       30,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91304348,
       1.        ,        nan, 0.82608696,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 693 | train scaffolds = 558 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 98 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 177 | train scaffolds = 138 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 154 | train scaffolds = 123 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 112 | train scaffolds = 90 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 224 | train scaffolds = 179 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 76 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 113 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 262 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan, 1.        ,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 761 | train scaffolds = 597 | val scaffolds = 164 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 82 | train scaffolds = 57 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.73333333,
              nan,        nan, 0.67741935,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,  0,  0,
       31,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.90909091,
       1.        ,        nan, 0.7826087 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 344 | train scaffolds = 272 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 139 | train scaffolds = 111 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
Total scaffolds = 297 | train scaffolds = 232 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 124 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 726 | train scaffolds = 579 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 103 | train scaffolds = 79 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 577 | train scaffolds = 469 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 70 | train scaffolds = 48 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 417 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 151 | train scaffolds = 120 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 176 | train scaffolds = 136 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 57 | train scaffolds = 29 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 717 | train scaffolds = 571 | val scaffolds = 146 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 502 | train scaffolds = 404 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 113 | train scaffolds = 89 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 112 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 130 | train scaffolds = 103 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 103 | train scaffolds = 84 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 213 | train scaffolds = 168 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 151 | train scaffolds = 118 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 63 | train scaffolds = 41 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 223 | train scaffolds = 177 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 227 | train scaffolds = 177 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 417 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 265 | train scaffolds = 214 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 155 | train scaffolds = 125 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 169 | train scaffolds = 131 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 120 | train scaffolds = 98 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 236 | train scaffolds = 181 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan,        nan,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 464 | train scaffolds = 373 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 293 | train scaffolds = 238 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 163 | train scaffolds = 127 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 545 | train scaffolds = 441 | val scaffolds = 104 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 156 | train scaffolds = 126 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 118 | train scaffolds = 98 | val scaffolds = 20 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 218 | train scaffolds = 175 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 227 | train scaffolds = 177 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 150 | train scaffolds = 121 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 196 | train scaffolds = 170 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 260 | train scaffolds = 209 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 502 | train scaffolds = 404 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 65 | val scaffolds = 15 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 152 | train scaffolds = 124 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 452 | train scaffolds = 366 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 131 | train scaffolds = 102 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 219 | train scaffolds = 164 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan, 1.        ,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 367 | train scaffolds = 299 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 132 | train scaffolds = 105 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 772 | train scaffolds = 625 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 218 | train scaffolds = 175 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 228 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 399 | train scaffolds = 320 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 111 | train scaffolds = 90 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 772 | train scaffolds = 625 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 550 | train scaffolds = 434 | val scaffolds = 116 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 216 | train scaffolds = 172 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 372 | train scaffolds = 301 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 228 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 257 | train scaffolds = 200 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 318 | train scaffolds = 263 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 84 | train scaffolds = 61 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 407 | train scaffolds = 320 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 338 | train scaffolds = 268 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 279 | train scaffolds = 218 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 159 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 159 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 258 | train scaffolds = 196 | val scaffolds = 62 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 192 | train scaffolds = 154 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 346 | train scaffolds = 279 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 174 | train scaffolds = 141 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 327 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 229 | train scaffolds = 181 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 224 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 372 | train scaffolds = 301 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 196 | train scaffolds = 156 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 269 | train scaffolds = 214 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 159 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 780 | train scaffolds = 619 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 195 | train scaffolds = 158 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 191 | train scaffolds = 148 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 179 | train scaffolds = 137 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 413 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 405 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 400 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 485 | train scaffolds = 401 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 486 | train scaffolds = 404 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 406 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 481 | train scaffolds = 397 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 473 | train scaffolds = 392 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 402 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.75      ,
              nan,        nan, 0.66666667,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 481 | train scaffolds = 395 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 405 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 484 | train scaffolds = 402 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 400 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 414 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 486 | train scaffolds = 398 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 401 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 485 | train scaffolds = 401 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 477 | train scaffolds = 394 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 404 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 405 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 407 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 406 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 365 | val scaffolds = 123 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 409 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 360 | val scaffolds = 123 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 482 | train scaffolds = 404 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 400 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 401 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 407 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 402 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 407 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 403 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 485 | train scaffolds = 373 | val scaffolds = 112 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 405 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 402 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 400 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 400 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 408 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 411 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 409 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 405 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 409 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 403 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 404 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 479 | train scaffolds = 362 | val scaffolds = 117 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 408 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 405 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 375 | val scaffolds = 113 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 396 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 477 | train scaffolds = 391 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 399 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 401 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 482 | train scaffolds = 396 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 359 | train scaffolds = 282 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 253 | train scaffolds = 192 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 440 | train scaffolds = 371 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 294 | train scaffolds = 237 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 588 | train scaffolds = 455 | val scaffolds = 133 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 421 | val scaffolds = 105 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 252 | train scaffolds = 202 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 502 | train scaffolds = 385 | val scaffolds = 117 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 424 | train scaffolds = 332 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 311 | train scaffolds = 249 | val scaffolds = 62 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 349 | train scaffolds = 273 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 329 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 451 | train scaffolds = 369 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 542 | train scaffolds = 445 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 309 | train scaffolds = 246 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 437 | train scaffolds = 345 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 467 | train scaffolds = 374 | val scaffolds = 93 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 465 | train scaffolds = 374 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 433 | train scaffolds = 363 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 415 | train scaffolds = 328 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 353 | train scaffolds = 277 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 256 | train scaffolds = 199 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 301 | train scaffolds = 234 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 477 | train scaffolds = 388 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 436 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 303 | train scaffolds = 228 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 337 | train scaffolds = 272 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 532 | train scaffolds = 428 | val scaffolds = 104 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 536 | train scaffolds = 434 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 400 | train scaffolds = 327 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 254 | train scaffolds = 202 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 461 | train scaffolds = 384 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 465 | train scaffolds = 368 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 286 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 551 | train scaffolds = 448 | val scaffolds = 103 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 564 | train scaffolds = 454 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 308 | train scaffolds = 230 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 349 | train scaffolds = 276 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 432 | train scaffolds = 340 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 301 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 432 | train scaffolds = 348 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 569 | train scaffolds = 445 | val scaffolds = 124 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 434 | train scaffolds = 357 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 364 | train scaffolds = 298 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 445 | train scaffolds = 370 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 582 | train scaffolds = 460 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 508 | train scaffolds = 382 | val scaffolds = 126 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 340 | train scaffolds = 251 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 227 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 343 | train scaffolds = 278 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 539 | train scaffolds = 439 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 359 | train scaffolds = 293 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 388 | train scaffolds = 316 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 495 | train scaffolds = 392 | val scaffolds = 103 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 524 | train scaffolds = 429 | val scaffolds = 95 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 302 | train scaffolds = 224 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 356 | train scaffolds = 278 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 376 | train scaffolds = 296 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 441 | train scaffolds = 352 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 529 | train scaffolds = 427 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 256 | train scaffolds = 214 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 428 | train scaffolds = 355 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 249 | train scaffolds = 196 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 422 | train scaffolds = 326 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 286 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 362 | train scaffolds = 287 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 302 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 278 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 439 | train scaffolds = 343 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 379 | train scaffolds = 293 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 233 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 568 | train scaffolds = 453 | val scaffolds = 115 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 523 | train scaffolds = 401 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 314 | train scaffolds = 232 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 543 | train scaffolds = 444 | val scaffolds = 99 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 209 | train scaffolds = 163 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 234 | train scaffolds = 201 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 230 | train scaffolds = 182 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 538 | train scaffolds = 433 | val scaffolds = 105 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 347 | train scaffolds = 277 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 594 | train scaffolds = 466 | val scaffolds = 128 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 429 | train scaffolds = 353 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 439 | train scaffolds = 352 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 367 | train scaffolds = 299 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 362 | train scaffolds = 282 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 256 | train scaffolds = 205 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 589 | train scaffolds = 471 | val scaffolds = 118 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 404 | train scaffolds = 333 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 480 | train scaffolds = 379 | val scaffolds = 101 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 431 | train scaffolds = 356 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 339 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 370 | train scaffolds = 287 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 358 | train scaffolds = 281 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 331 | train scaffolds = 261 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 293 | train scaffolds = 239 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 273 | train scaffolds = 216 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 454 | train scaffolds = 372 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 436 | train scaffolds = 361 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 351 | train scaffolds = 261 | val scaffolds = 90 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 265 | train scaffolds = 217 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 330 | train scaffolds = 264 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 237 | train scaffolds = 186 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 250 | train scaffolds = 196 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 594 | train scaffolds = 474 | val scaffolds = 120 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 431 | train scaffolds = 353 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 382 | train scaffolds = 310 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 259 | train scaffolds = 207 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 398 | train scaffolds = 315 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 132 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 588 | train scaffolds = 457 | val scaffolds = 131 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 285 | train scaffolds = 237 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 410 | train scaffolds = 325 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 460 | train scaffolds = 350 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 367 | train scaffolds = 289 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 426 | train scaffolds = 355 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 369 | train scaffolds = 288 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 408 | train scaffolds = 343 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 550 | train scaffolds = 439 | val scaffolds = 111 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 288 | train scaffolds = 242 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 273 | train scaffolds = 220 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 448 | train scaffolds = 356 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 369 | train scaffolds = 283 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 317 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 442 | train scaffolds = 365 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 267 | train scaffolds = 209 | val scaffolds = 58 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 544 | train scaffolds = 442 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 395 | train scaffolds = 322 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 376 | train scaffolds = 301 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 422 | train scaffolds = 328 | val scaffolds = 94 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 603 | train scaffolds = 491 | val scaffolds = 112 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 200 | train scaffolds = 156 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 158 | train scaffolds = 120 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 442 | train scaffolds = 360 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 158 | train scaffolds = 120 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 155 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 171 | train scaffolds = 136 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 154 | train scaffolds = 127 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 383 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 571 | train scaffolds = 423 | val scaffolds = 148 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 162 | train scaffolds = 124 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 242 | train scaffolds = 188 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 535 | train scaffolds = 428 | val scaffolds = 107 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 110 | train scaffolds = 85 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 289 | train scaffolds = 228 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 630 | train scaffolds = 498 | val scaffolds = 132 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 189 | train scaffolds = 147 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 131 | train scaffolds = 104 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 343 | train scaffolds = 268 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 189 | train scaffolds = 147 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 596 | train scaffolds = 466 | val scaffolds = 130 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 358 | train scaffolds = 276 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 586 | train scaffolds = 464 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 343 | train scaffolds = 268 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 279 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 560 | train scaffolds = 451 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 436 | train scaffolds = 369 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 237 | train scaffolds = 189 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 562 | train scaffolds = 426 | val scaffolds = 136 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 368 | train scaffolds = 283 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 241 | train scaffolds = 203 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 233 | train scaffolds = 191 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 242 | train scaffolds = 189 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 198 | train scaffolds = 152 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 265 | train scaffolds = 218 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 194 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 324 | train scaffolds = 247 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 231 | train scaffolds = 174 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 300 | train scaffolds = 229 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 450 | train scaffolds = 359 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 176 | train scaffolds = 141 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 169 | train scaffolds = 135 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 172 | train scaffolds = 139 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 184 | train scaffolds = 145 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 98 | train scaffolds = 79 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 780 | train scaffolds = 623 | val scaffolds = 157 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 260 | train scaffolds = 199 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 318 | train scaffolds = 255 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 157 | train scaffolds = 124 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 160 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 120 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 192 | train scaffolds = 153 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 294 | train scaffolds = 236 | val scaffolds = 58 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 269 | train scaffolds = 212 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 155 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 214 | train scaffolds = 175 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 143 | train scaffolds = 111 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 192 | train scaffolds = 153 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 302 | train scaffolds = 253 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 713 | train scaffolds = 578 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 408 | train scaffolds = 319 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 120 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 232 | train scaffolds = 180 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 108 | train scaffolds = 85 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 152 | train scaffolds = 127 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 135 | train scaffolds = 80 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.65517241,
              nan,        nan, 0.66666667,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 29,  0,  0,
       27,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.82352941,
       1.        ,        nan, 0.8125    ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 17,  1,  0,
       16,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 1.  ,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 160 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 115 | train scaffolds = 87 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 314 | train scaffolds = 255 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 269 | train scaffolds = 212 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 121 | train scaffolds = 93 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 260 | train scaffolds = 208 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 351 | train scaffolds = 278 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 155 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 333 | train scaffolds = 270 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 619 | train scaffolds = 497 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 306 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 625 | train scaffolds = 487 | val scaffolds = 138 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 619 | train scaffolds = 497 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 128 | train scaffolds = 96 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 789 | train scaffolds = 628 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 713 | train scaffolds = 578 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 154 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 144 | train scaffolds = 110 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 164 | train scaffolds = 130 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 200 | train scaffolds = 156 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 116 | train scaffolds = 86 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 168 | train scaffolds = 137 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 298 | train scaffolds = 228 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 221 | train scaffolds = 174 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 221 | train scaffolds = 174 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 214 | train scaffolds = 168 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 179 | train scaffolds = 143 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 164 | train scaffolds = 130 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 570 | train scaffolds = 452 | val scaffolds = 118 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 56 | train scaffolds = 50 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 80 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
Total scaffolds = 104 | train scaffolds = 83 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
Total scaffolds = 197 | train scaffolds = 158 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 217 | train scaffolds = 182 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 612 | train scaffolds = 492 | val scaffolds = 120 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 113 | train scaffolds = 91 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 498 | train scaffolds = 397 | val scaffolds = 101 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 203 | train scaffolds = 160 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 176 | train scaffolds = 143 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 190 | train scaffolds = 152 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 353 | train scaffolds = 274 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 443 | train scaffolds = 352 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 396 | train scaffolds = 310 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 153 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 122 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 221 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 417 | train scaffolds = 336 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 104 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 190 | train scaffolds = 152 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 237 | train scaffolds = 184 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 130 | train scaffolds = 94 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.75      ,
              nan,        nan, 0.83333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 0, 0, 6, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 162 | train scaffolds = 132 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]))]
Total scaffolds = 193 | train scaffolds = 154 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 309 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 124 | train scaffolds = 99 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 239 | train scaffolds = 189 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 249 | train scaffolds = 184 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 1. , nan, 0.5]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 282 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 105 | train scaffolds = 81 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 207 | train scaffolds = 165 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 174 | train scaffolds = 129 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 239 | train scaffolds = 189 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 91 | train scaffolds = 73 | val scaffolds = 18 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 377 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 350 | train scaffolds = 276 | val scaffolds = 74 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 36 | train scaffolds = 20 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 41 | train scaffolds = 30 | val scaffolds = 11 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 36 | train scaffolds = 20 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 75 | train scaffolds = 57 | val scaffolds = 18 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 85 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 204 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 622 | train scaffolds = 493 | val scaffolds = 129 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 377 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 480 | train scaffolds = 356 | val scaffolds = 124 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 486 | train scaffolds = 403 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 406 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 480 | train scaffolds = 398 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 406 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 404 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 435 | train scaffolds = 355 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 431 | train scaffolds = 334 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 428 | train scaffolds = 342 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 279 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 383 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 232 | train scaffolds = 180 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 329 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 228 | train scaffolds = 183 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 448 | train scaffolds = 357 | val scaffolds = 40 | test scaffolds = 51
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 147 | train scaffolds = 125 | val scaffolds = 5 | test scaffolds = 17
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 364 | train scaffolds = 293 | val scaffolds = 32 | test scaffolds = 39
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 117 | train scaffolds = 91 | val scaffolds = 15 | test scaffolds = 11
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 104 | train scaffolds = 80 | val scaffolds = 12 | test scaffolds = 12
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 32 | test scaffolds = 40
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 117 | train scaffolds = 94 | val scaffolds = 9 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 198 | val scaffolds = 28 | test scaffolds = 25
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 20 | test scaffolds = 20
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 294 | val scaffolds = 34 | test scaffolds = 38
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 364 | val scaffolds = 59 | test scaffolds = 67
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
        1.,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 376 | val scaffolds = 53 | test scaffolds = 62
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 491 | train scaffolds = 375 | val scaffolds = 57 | test scaffolds = 59
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 489 | train scaffolds = 362 | val scaffolds = 63 | test scaffolds = 64
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 366 | val scaffolds = 68 | test scaffolds = 57
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 482 | train scaffolds = 354 | val scaffolds = 59 | test scaffolds = 69
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 410 | train scaffolds = 346 | val scaffolds = 31 | test scaffolds = 33
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 546 | train scaffolds = 443 | val scaffolds = 45 | test scaffolds = 58
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 111 | val scaffolds = 9 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 25 | test scaffolds = 24
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 240 | train scaffolds = 197 | val scaffolds = 18 | test scaffolds = 25
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 122 | train scaffolds = 95 | val scaffolds = 13 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 139 | train scaffolds = 105 | val scaffolds = 19 | test scaffolds = 15
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1.,  1., nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 6, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.6       ,
              nan,        nan, 0.66666667,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,
        9,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 217 | train scaffolds = 182 | val scaffolds = 21 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 131 | train scaffolds = 104 | val scaffolds = 13 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 69 | train scaffolds = 53 | val scaffolds = 7 | test scaffolds = 9
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Building model 
Number of parameters = 385,101
MAML(
  (module): MoleculeModel(
    (sigmoid): Sigmoid()
    (encoder): MPN(
      (encoder): MPNEncoder(
        (dropout_layer): Dropout(p=0.2, inplace=False)
        (act_func): ReLU()
        (W_i): Linear(in_features=147, out_features=300, bias=False)
        (W_h): Linear(in_features=300, out_features=300, bias=False)
        (W_o): Linear(in_features=433, out_features=300, bias=True)
      )
    )
    (ffn): Sequential(
      (0): Dropout(p=0.2, inplace=False)
      (1): Linear(in_features=300, out_features=400, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.2, inplace=False)
      (4): Linear(in_features=400, out_features=1, bias=True)
    )
  )
)
Moving maml model to cuda
Epoch 0
Meta loss on this task batch = 1.1081e+00, Meta loss averaged over last 500 steps = 1.1081e+00, PNorm = 51.0095, GNorm = 1.0000
Meta loss on this task batch = 7.5157e-01, Meta loss averaged over last 500 steps = 9.2982e-01, PNorm = 51.0155, GNorm = 1.0000
Meta loss on this task batch = 5.4619e-01, Meta loss averaged over last 500 steps = 8.0194e-01, PNorm = 51.0255, GNorm = 0.2439
Meta loss on this task batch = 5.3284e-01, Meta loss averaged over last 500 steps = 7.3466e-01, PNorm = 51.0363, GNorm = 0.5089
Meta loss on this task batch = 5.5608e-01, Meta loss averaged over last 500 steps = 6.9895e-01, PNorm = 51.0481, GNorm = 0.2646
Meta loss on this task batch = 5.1616e-01, Meta loss averaged over last 500 steps = 6.6848e-01, PNorm = 51.0608, GNorm = 0.1576
Meta loss on this task batch = 4.5796e-01, Meta loss averaged over last 500 steps = 6.3841e-01, PNorm = 51.0743, GNorm = 0.2435
Meta loss on this task batch = 5.6026e-01, Meta loss averaged over last 500 steps = 6.2864e-01, PNorm = 51.0869, GNorm = 0.2848
Meta loss on this task batch = 4.8351e-01, Meta loss averaged over last 500 steps = 6.1251e-01, PNorm = 51.0993, GNorm = 0.2195
Meta loss on this task batch = 4.9013e-01, Meta loss averaged over last 500 steps = 6.0028e-01, PNorm = 51.1111, GNorm = 0.1620
Meta loss on this task batch = 4.5050e-01, Meta loss averaged over last 500 steps = 5.8666e-01, PNorm = 51.1219, GNorm = 0.3140
Meta loss on this task batch = 4.7299e-01, Meta loss averaged over last 500 steps = 5.7719e-01, PNorm = 51.1325, GNorm = 0.1310
Meta loss on this task batch = 4.5978e-01, Meta loss averaged over last 500 steps = 5.6816e-01, PNorm = 51.1434, GNorm = 0.1888
Meta loss on this task batch = 3.9751e-01, Meta loss averaged over last 500 steps = 5.5597e-01, PNorm = 51.1543, GNorm = 0.3263
Meta loss on this task batch = 4.3421e-01, Meta loss averaged over last 500 steps = 5.4785e-01, PNorm = 51.1654, GNorm = 0.1455
Meta loss on this task batch = 4.6228e-01, Meta loss averaged over last 500 steps = 5.4250e-01, PNorm = 51.1763, GNorm = 0.1220
Meta loss on this task batch = 5.6922e-01, Meta loss averaged over last 500 steps = 5.4407e-01, PNorm = 51.1853, GNorm = 0.2553
Meta loss on this task batch = 5.2071e-01, Meta loss averaged over last 500 steps = 5.4278e-01, PNorm = 51.1929, GNorm = 0.3644
Meta loss on this task batch = 5.2793e-01, Meta loss averaged over last 500 steps = 5.4199e-01, PNorm = 51.1997, GNorm = 0.1837
Took 123.18651247024536 seconds to complete one epoch of meta training
Took 130.51938366889954 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.308330
Found better MAML checkpoint after meta validation, saving now
Epoch 1
Meta loss on this task batch = 5.0261e-01, Meta loss averaged over last 500 steps = 5.4003e-01, PNorm = 51.2065, GNorm = 0.3705
Meta loss on this task batch = 5.1327e-01, Meta loss averaged over last 500 steps = 5.3875e-01, PNorm = 51.2130, GNorm = 0.2042
Meta loss on this task batch = 4.9931e-01, Meta loss averaged over last 500 steps = 5.3696e-01, PNorm = 51.2195, GNorm = 0.1671
Meta loss on this task batch = 4.3244e-01, Meta loss averaged over last 500 steps = 5.3241e-01, PNorm = 51.2265, GNorm = 0.1388
Meta loss on this task batch = 5.0965e-01, Meta loss averaged over last 500 steps = 5.3147e-01, PNorm = 51.2337, GNorm = 0.2560
Meta loss on this task batch = 5.1028e-01, Meta loss averaged over last 500 steps = 5.3062e-01, PNorm = 51.2405, GNorm = 0.1746
Meta loss on this task batch = 5.0147e-01, Meta loss averaged over last 500 steps = 5.2950e-01, PNorm = 51.2469, GNorm = 0.1112
Meta loss on this task batch = 5.0381e-01, Meta loss averaged over last 500 steps = 5.2855e-01, PNorm = 51.2537, GNorm = 0.1417
Meta loss on this task batch = 4.3132e-01, Meta loss averaged over last 500 steps = 5.2507e-01, PNorm = 51.2607, GNorm = 0.1315
Meta loss on this task batch = 4.8085e-01, Meta loss averaged over last 500 steps = 5.2355e-01, PNorm = 51.2675, GNorm = 0.1699
Meta loss on this task batch = 4.4508e-01, Meta loss averaged over last 500 steps = 5.2093e-01, PNorm = 51.2748, GNorm = 0.3614
Meta loss on this task batch = 4.8784e-01, Meta loss averaged over last 500 steps = 5.1987e-01, PNorm = 51.2806, GNorm = 0.2567
Meta loss on this task batch = 5.1287e-01, Meta loss averaged over last 500 steps = 5.1965e-01, PNorm = 51.2851, GNorm = 0.2132
Meta loss on this task batch = 4.6012e-01, Meta loss averaged over last 500 steps = 5.1784e-01, PNorm = 51.2894, GNorm = 0.1252
Meta loss on this task batch = 4.7949e-01, Meta loss averaged over last 500 steps = 5.1671e-01, PNorm = 51.2936, GNorm = 0.1141
Meta loss on this task batch = 5.1730e-01, Meta loss averaged over last 500 steps = 5.1673e-01, PNorm = 51.2974, GNorm = 0.1209
Meta loss on this task batch = 4.8072e-01, Meta loss averaged over last 500 steps = 5.1573e-01, PNorm = 51.3013, GNorm = 0.1272
Meta loss on this task batch = 4.2144e-01, Meta loss averaged over last 500 steps = 5.1318e-01, PNorm = 51.3058, GNorm = 0.1218
Meta loss on this task batch = 5.0898e-01, Meta loss averaged over last 500 steps = 5.1307e-01, PNorm = 51.3097, GNorm = 0.1588
Took 120.63620090484619 seconds to complete one epoch of meta training
Took 128.30340051651 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.306352
Epoch 2
Meta loss on this task batch = 4.3994e-01, Meta loss averaged over last 500 steps = 5.1120e-01, PNorm = 51.3129, GNorm = 0.4014
Meta loss on this task batch = 4.6403e-01, Meta loss averaged over last 500 steps = 5.1002e-01, PNorm = 51.3164, GNorm = 0.1545
Meta loss on this task batch = 4.5905e-01, Meta loss averaged over last 500 steps = 5.0877e-01, PNorm = 51.3204, GNorm = 0.1195
Meta loss on this task batch = 4.9821e-01, Meta loss averaged over last 500 steps = 5.0852e-01, PNorm = 51.3246, GNorm = 0.2512
Meta loss on this task batch = 5.1011e-01, Meta loss averaged over last 500 steps = 5.0856e-01, PNorm = 51.3281, GNorm = 0.2520
Meta loss on this task batch = 4.3067e-01, Meta loss averaged over last 500 steps = 5.0679e-01, PNorm = 51.3322, GNorm = 0.2699
Meta loss on this task batch = 4.3366e-01, Meta loss averaged over last 500 steps = 5.0516e-01, PNorm = 51.3372, GNorm = 0.2380
Meta loss on this task batch = 4.9770e-01, Meta loss averaged over last 500 steps = 5.0500e-01, PNorm = 51.3418, GNorm = 0.1361
Meta loss on this task batch = 4.7516e-01, Meta loss averaged over last 500 steps = 5.0437e-01, PNorm = 51.3466, GNorm = 0.1230
Meta loss on this task batch = 5.1692e-01, Meta loss averaged over last 500 steps = 5.0463e-01, PNorm = 51.3511, GNorm = 0.1454
Meta loss on this task batch = 5.3579e-01, Meta loss averaged over last 500 steps = 5.0526e-01, PNorm = 51.3558, GNorm = 0.3893
Meta loss on this task batch = 5.0060e-01, Meta loss averaged over last 500 steps = 5.0517e-01, PNorm = 51.3591, GNorm = 0.2539
Meta loss on this task batch = 5.0616e-01, Meta loss averaged over last 500 steps = 5.0519e-01, PNorm = 51.3622, GNorm = 0.1234
Meta loss on this task batch = 4.8405e-01, Meta loss averaged over last 500 steps = 5.0478e-01, PNorm = 51.3644, GNorm = 0.2175
Meta loss on this task batch = 5.0106e-01, Meta loss averaged over last 500 steps = 5.0471e-01, PNorm = 51.3659, GNorm = 0.1992
Meta loss on this task batch = 5.0023e-01, Meta loss averaged over last 500 steps = 5.0463e-01, PNorm = 51.3672, GNorm = 0.5250
Meta loss on this task batch = 4.5731e-01, Meta loss averaged over last 500 steps = 5.0377e-01, PNorm = 51.3689, GNorm = 0.5314
Meta loss on this task batch = 4.9081e-01, Meta loss averaged over last 500 steps = 5.0354e-01, PNorm = 51.3711, GNorm = 0.5731
Meta loss on this task batch = 5.2275e-01, Meta loss averaged over last 500 steps = 5.0388e-01, PNorm = 51.3738, GNorm = 0.2074
Took 120.1929988861084 seconds to complete one epoch of meta training
Took 129.13676643371582 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.316901
Found better MAML checkpoint after meta validation, saving now
Epoch 3
Meta loss on this task batch = 4.9555e-01, Meta loss averaged over last 500 steps = 5.0373e-01, PNorm = 51.3772, GNorm = 0.1314
Meta loss on this task batch = 4.9436e-01, Meta loss averaged over last 500 steps = 5.0357e-01, PNorm = 51.3805, GNorm = 0.1399
Meta loss on this task batch = 4.9291e-01, Meta loss averaged over last 500 steps = 5.0340e-01, PNorm = 51.3838, GNorm = 0.4600
Meta loss on this task batch = 4.8030e-01, Meta loss averaged over last 500 steps = 5.0302e-01, PNorm = 51.3873, GNorm = 0.1683
Meta loss on this task batch = 4.8761e-01, Meta loss averaged over last 500 steps = 5.0277e-01, PNorm = 51.3912, GNorm = 0.2804
Meta loss on this task batch = 5.0291e-01, Meta loss averaged over last 500 steps = 5.0277e-01, PNorm = 51.3949, GNorm = 0.2250
Meta loss on this task batch = 5.4852e-01, Meta loss averaged over last 500 steps = 5.0349e-01, PNorm = 51.3981, GNorm = 0.1878
Meta loss on this task batch = 4.7754e-01, Meta loss averaged over last 500 steps = 5.0309e-01, PNorm = 51.4018, GNorm = 0.1983
Meta loss on this task batch = 4.1936e-01, Meta loss averaged over last 500 steps = 5.0182e-01, PNorm = 51.4062, GNorm = 0.1081
Meta loss on this task batch = 4.6426e-01, Meta loss averaged over last 500 steps = 5.0126e-01, PNorm = 51.4107, GNorm = 0.1732
Meta loss on this task batch = 4.4474e-01, Meta loss averaged over last 500 steps = 5.0043e-01, PNorm = 51.4153, GNorm = 0.2445
Meta loss on this task batch = 5.7532e-01, Meta loss averaged over last 500 steps = 5.0151e-01, PNorm = 51.4196, GNorm = 0.2294
Meta loss on this task batch = 4.3151e-01, Meta loss averaged over last 500 steps = 5.0051e-01, PNorm = 51.4239, GNorm = 0.0819
Meta loss on this task batch = 4.8240e-01, Meta loss averaged over last 500 steps = 5.0026e-01, PNorm = 51.4280, GNorm = 0.1818
Meta loss on this task batch = 3.7299e-01, Meta loss averaged over last 500 steps = 4.9849e-01, PNorm = 51.4326, GNorm = 0.1415
Meta loss on this task batch = 4.4136e-01, Meta loss averaged over last 500 steps = 4.9771e-01, PNorm = 51.4375, GNorm = 0.1957
Meta loss on this task batch = 5.0824e-01, Meta loss averaged over last 500 steps = 4.9785e-01, PNorm = 51.4418, GNorm = 0.1287
Meta loss on this task batch = 5.6602e-01, Meta loss averaged over last 500 steps = 4.9876e-01, PNorm = 51.4450, GNorm = 0.1668
Meta loss on this task batch = 6.1737e-01, Meta loss averaged over last 500 steps = 5.0032e-01, PNorm = 51.4477, GNorm = 0.5009
Took 117.67849493026733 seconds to complete one epoch of meta training
Took 125.16370511054993 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.302878
Epoch 4
Meta loss on this task batch = 4.7047e-01, Meta loss averaged over last 500 steps = 4.9993e-01, PNorm = 51.4509, GNorm = 0.1495
Meta loss on this task batch = 4.9400e-01, Meta loss averaged over last 500 steps = 4.9985e-01, PNorm = 51.4539, GNorm = 0.1947
Meta loss on this task batch = 4.7907e-01, Meta loss averaged over last 500 steps = 4.9959e-01, PNorm = 51.4573, GNorm = 0.4151
Meta loss on this task batch = 4.6067e-01, Meta loss averaged over last 500 steps = 4.9911e-01, PNorm = 51.4608, GNorm = 0.1099
Meta loss on this task batch = 4.9605e-01, Meta loss averaged over last 500 steps = 4.9907e-01, PNorm = 51.4639, GNorm = 0.1698
Meta loss on this task batch = 4.3108e-01, Meta loss averaged over last 500 steps = 4.9824e-01, PNorm = 51.4674, GNorm = 0.1463
Meta loss on this task batch = 4.4583e-01, Meta loss averaged over last 500 steps = 4.9761e-01, PNorm = 51.4711, GNorm = 0.2197
Meta loss on this task batch = 4.6662e-01, Meta loss averaged over last 500 steps = 4.9724e-01, PNorm = 51.4746, GNorm = 0.0718
Meta loss on this task batch = 4.0196e-01, Meta loss averaged over last 500 steps = 4.9612e-01, PNorm = 51.4784, GNorm = 0.1582
Meta loss on this task batch = 4.3755e-01, Meta loss averaged over last 500 steps = 4.9544e-01, PNorm = 51.4817, GNorm = 0.1700
Meta loss on this task batch = 4.5351e-01, Meta loss averaged over last 500 steps = 4.9495e-01, PNorm = 51.4855, GNorm = 0.1868
Meta loss on this task batch = 4.7371e-01, Meta loss averaged over last 500 steps = 4.9471e-01, PNorm = 51.4883, GNorm = 0.1007
Meta loss on this task batch = 4.4049e-01, Meta loss averaged over last 500 steps = 4.9410e-01, PNorm = 51.4908, GNorm = 0.2952
Meta loss on this task batch = 5.5022e-01, Meta loss averaged over last 500 steps = 4.9473e-01, PNorm = 51.4928, GNorm = 0.0986
Meta loss on this task batch = 5.6789e-01, Meta loss averaged over last 500 steps = 4.9553e-01, PNorm = 51.4933, GNorm = 0.2121
Meta loss on this task batch = 5.7219e-01, Meta loss averaged over last 500 steps = 4.9636e-01, PNorm = 51.4936, GNorm = 0.1288
Meta loss on this task batch = 4.8663e-01, Meta loss averaged over last 500 steps = 4.9626e-01, PNorm = 51.4936, GNorm = 0.1113
Meta loss on this task batch = 5.0765e-01, Meta loss averaged over last 500 steps = 4.9638e-01, PNorm = 51.4945, GNorm = 0.1578
Meta loss on this task batch = 5.4072e-01, Meta loss averaged over last 500 steps = 4.9685e-01, PNorm = 51.4948, GNorm = 0.3041
Took 117.43391442298889 seconds to complete one epoch of meta training
Took 125.22213840484619 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.313392
Epoch 5
Meta loss on this task batch = 4.7556e-01, Meta loss averaged over last 500 steps = 4.9663e-01, PNorm = 51.4956, GNorm = 0.3121
Meta loss on this task batch = 5.1496e-01, Meta loss averaged over last 500 steps = 4.9681e-01, PNorm = 51.4968, GNorm = 0.2524
Meta loss on this task batch = 5.2495e-01, Meta loss averaged over last 500 steps = 4.9710e-01, PNorm = 51.4984, GNorm = 0.4352
Meta loss on this task batch = 5.1144e-01, Meta loss averaged over last 500 steps = 4.9725e-01, PNorm = 51.5005, GNorm = 0.3982
Meta loss on this task batch = 4.7785e-01, Meta loss averaged over last 500 steps = 4.9705e-01, PNorm = 51.5038, GNorm = 0.2242
Meta loss on this task batch = 5.0837e-01, Meta loss averaged over last 500 steps = 4.9716e-01, PNorm = 51.5075, GNorm = 0.1298
Meta loss on this task batch = 4.6452e-01, Meta loss averaged over last 500 steps = 4.9684e-01, PNorm = 51.5112, GNorm = 0.2241
Meta loss on this task batch = 4.7760e-01, Meta loss averaged over last 500 steps = 4.9666e-01, PNorm = 51.5141, GNorm = 0.1877
Meta loss on this task batch = 5.0644e-01, Meta loss averaged over last 500 steps = 4.9675e-01, PNorm = 51.5167, GNorm = 0.1414
Meta loss on this task batch = 4.6902e-01, Meta loss averaged over last 500 steps = 4.9649e-01, PNorm = 51.5190, GNorm = 0.1386
Meta loss on this task batch = 5.3891e-01, Meta loss averaged over last 500 steps = 4.9689e-01, PNorm = 51.5215, GNorm = 0.2269
Meta loss on this task batch = 5.4093e-01, Meta loss averaged over last 500 steps = 4.9730e-01, PNorm = 51.5235, GNorm = 0.4411
Meta loss on this task batch = 5.0350e-01, Meta loss averaged over last 500 steps = 4.9736e-01, PNorm = 51.5254, GNorm = 0.0987
Meta loss on this task batch = 4.0370e-01, Meta loss averaged over last 500 steps = 4.9650e-01, PNorm = 51.5276, GNorm = 0.2075
Meta loss on this task batch = 4.9605e-01, Meta loss averaged over last 500 steps = 4.9649e-01, PNorm = 51.5299, GNorm = 0.0888
Meta loss on this task batch = 3.6980e-01, Meta loss averaged over last 500 steps = 4.9535e-01, PNorm = 51.5337, GNorm = 0.2163
Meta loss on this task batch = 4.8061e-01, Meta loss averaged over last 500 steps = 4.9522e-01, PNorm = 51.5377, GNorm = 0.2048
Meta loss on this task batch = 5.2596e-01, Meta loss averaged over last 500 steps = 4.9549e-01, PNorm = 51.5415, GNorm = 0.1174
Meta loss on this task batch = 4.2876e-01, Meta loss averaged over last 500 steps = 4.9491e-01, PNorm = 51.5462, GNorm = 0.2409
Took 110.04671502113342 seconds to complete one epoch of meta training
Took 117.74788093566895 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.318749
Found better MAML checkpoint after meta validation, saving now
Epoch 6
Meta loss on this task batch = 4.7830e-01, Meta loss averaged over last 500 steps = 4.9476e-01, PNorm = 51.5509, GNorm = 0.1541
Meta loss on this task batch = 5.0075e-01, Meta loss averaged over last 500 steps = 4.9481e-01, PNorm = 51.5545, GNorm = 0.3160
Meta loss on this task batch = 4.7505e-01, Meta loss averaged over last 500 steps = 4.9465e-01, PNorm = 51.5577, GNorm = 0.3606
Meta loss on this task batch = 4.8070e-01, Meta loss averaged over last 500 steps = 4.9453e-01, PNorm = 51.5608, GNorm = 0.0784
Meta loss on this task batch = 5.4394e-01, Meta loss averaged over last 500 steps = 4.9494e-01, PNorm = 51.5633, GNorm = 0.1632
Meta loss on this task batch = 4.5462e-01, Meta loss averaged over last 500 steps = 4.9461e-01, PNorm = 51.5662, GNorm = 0.0840
Meta loss on this task batch = 4.5724e-01, Meta loss averaged over last 500 steps = 4.9430e-01, PNorm = 51.5695, GNorm = 0.1494
Meta loss on this task batch = 4.7748e-01, Meta loss averaged over last 500 steps = 4.9416e-01, PNorm = 51.5734, GNorm = 0.1833
Meta loss on this task batch = 5.1755e-01, Meta loss averaged over last 500 steps = 4.9435e-01, PNorm = 51.5785, GNorm = 0.2308
Meta loss on this task batch = 4.8168e-01, Meta loss averaged over last 500 steps = 4.9425e-01, PNorm = 51.5834, GNorm = 0.1338
Meta loss on this task batch = 4.5949e-01, Meta loss averaged over last 500 steps = 4.9397e-01, PNorm = 51.5879, GNorm = 0.0962
Meta loss on this task batch = 4.9256e-01, Meta loss averaged over last 500 steps = 4.9396e-01, PNorm = 51.5921, GNorm = 0.1022
Meta loss on this task batch = 4.4854e-01, Meta loss averaged over last 500 steps = 4.9360e-01, PNorm = 51.5967, GNorm = 0.0761
Meta loss on this task batch = 5.1410e-01, Meta loss averaged over last 500 steps = 4.9376e-01, PNorm = 51.6005, GNorm = 0.0864
Meta loss on this task batch = 4.4607e-01, Meta loss averaged over last 500 steps = 4.9339e-01, PNorm = 51.6039, GNorm = 0.2947
Meta loss on this task batch = 4.4575e-01, Meta loss averaged over last 500 steps = 4.9302e-01, PNorm = 51.6062, GNorm = 0.1980
Meta loss on this task batch = 4.8037e-01, Meta loss averaged over last 500 steps = 4.9293e-01, PNorm = 51.6078, GNorm = 0.1867
Meta loss on this task batch = 5.1617e-01, Meta loss averaged over last 500 steps = 4.9310e-01, PNorm = 51.6094, GNorm = 0.1243
Meta loss on this task batch = 4.9603e-01, Meta loss averaged over last 500 steps = 4.9313e-01, PNorm = 51.6108, GNorm = 0.1147
Took 115.25952863693237 seconds to complete one epoch of meta training
Took 123.48168659210205 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.317798
Epoch 7
Meta loss on this task batch = 4.5299e-01, Meta loss averaged over last 500 steps = 4.9283e-01, PNorm = 51.6135, GNorm = 0.1367
Meta loss on this task batch = 4.6164e-01, Meta loss averaged over last 500 steps = 4.9260e-01, PNorm = 51.6177, GNorm = 0.1041
Meta loss on this task batch = 5.1180e-01, Meta loss averaged over last 500 steps = 4.9274e-01, PNorm = 51.6215, GNorm = 0.0769
Meta loss on this task batch = 4.6954e-01, Meta loss averaged over last 500 steps = 4.9257e-01, PNorm = 51.6259, GNorm = 0.0982
Meta loss on this task batch = 4.3630e-01, Meta loss averaged over last 500 steps = 4.9216e-01, PNorm = 51.6304, GNorm = 0.1391
Meta loss on this task batch = 4.6620e-01, Meta loss averaged over last 500 steps = 4.9197e-01, PNorm = 51.6344, GNorm = 0.2062
Meta loss on this task batch = 5.0650e-01, Meta loss averaged over last 500 steps = 4.9208e-01, PNorm = 51.6379, GNorm = 0.2925
Meta loss on this task batch = 4.1834e-01, Meta loss averaged over last 500 steps = 4.9155e-01, PNorm = 51.6406, GNorm = 0.1560
Meta loss on this task batch = 4.2152e-01, Meta loss averaged over last 500 steps = 4.9106e-01, PNorm = 51.6432, GNorm = 0.2153
Meta loss on this task batch = 5.7185e-01, Meta loss averaged over last 500 steps = 4.9163e-01, PNorm = 51.6449, GNorm = 0.1818
Meta loss on this task batch = 4.8995e-01, Meta loss averaged over last 500 steps = 4.9161e-01, PNorm = 51.6469, GNorm = 0.0905
Meta loss on this task batch = 4.6416e-01, Meta loss averaged over last 500 steps = 4.9142e-01, PNorm = 51.6499, GNorm = 0.1160
Meta loss on this task batch = 5.4111e-01, Meta loss averaged over last 500 steps = 4.9177e-01, PNorm = 51.6526, GNorm = 0.2430
Meta loss on this task batch = 3.8901e-01, Meta loss averaged over last 500 steps = 4.9107e-01, PNorm = 51.6568, GNorm = 0.1334
Meta loss on this task batch = 5.7073e-01, Meta loss averaged over last 500 steps = 4.9160e-01, PNorm = 51.6605, GNorm = 0.1650
Meta loss on this task batch = 4.6540e-01, Meta loss averaged over last 500 steps = 4.9143e-01, PNorm = 51.6648, GNorm = 0.2029
Meta loss on this task batch = 5.3117e-01, Meta loss averaged over last 500 steps = 4.9169e-01, PNorm = 51.6695, GNorm = 0.0819
Meta loss on this task batch = 4.4367e-01, Meta loss averaged over last 500 steps = 4.9138e-01, PNorm = 51.6749, GNorm = 0.2319
Meta loss on this task batch = 5.2044e-01, Meta loss averaged over last 500 steps = 4.9157e-01, PNorm = 51.6800, GNorm = 0.1924
Took 111.9932906627655 seconds to complete one epoch of meta training
Took 118.67516326904297 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.332860
Found better MAML checkpoint after meta validation, saving now
Epoch 8
Meta loss on this task batch = 4.5932e-01, Meta loss averaged over last 500 steps = 4.9136e-01, PNorm = 51.6858, GNorm = 0.1474
Meta loss on this task batch = 5.3602e-01, Meta loss averaged over last 500 steps = 4.9165e-01, PNorm = 51.6905, GNorm = 0.1522
Meta loss on this task batch = 4.8101e-01, Meta loss averaged over last 500 steps = 4.9158e-01, PNorm = 51.6937, GNorm = 0.0951
Meta loss on this task batch = 4.4505e-01, Meta loss averaged over last 500 steps = 4.9128e-01, PNorm = 51.6976, GNorm = 0.2158
Meta loss on this task batch = 5.1859e-01, Meta loss averaged over last 500 steps = 4.9145e-01, PNorm = 51.7010, GNorm = 0.0999
Meta loss on this task batch = 4.6904e-01, Meta loss averaged over last 500 steps = 4.9131e-01, PNorm = 51.7052, GNorm = 0.1747
Meta loss on this task batch = 4.4626e-01, Meta loss averaged over last 500 steps = 4.9103e-01, PNorm = 51.7087, GNorm = 0.0859
Meta loss on this task batch = 5.5648e-01, Meta loss averaged over last 500 steps = 4.9144e-01, PNorm = 51.7120, GNorm = 0.0942
Meta loss on this task batch = 4.8958e-01, Meta loss averaged over last 500 steps = 4.9143e-01, PNorm = 51.7150, GNorm = 0.2010
Meta loss on this task batch = 4.9397e-01, Meta loss averaged over last 500 steps = 4.9144e-01, PNorm = 51.7170, GNorm = 0.4269
Meta loss on this task batch = 4.9905e-01, Meta loss averaged over last 500 steps = 4.9149e-01, PNorm = 51.7196, GNorm = 0.0870
Meta loss on this task batch = 4.2203e-01, Meta loss averaged over last 500 steps = 4.9106e-01, PNorm = 51.7228, GNorm = 0.2270
Meta loss on this task batch = 5.1627e-01, Meta loss averaged over last 500 steps = 4.9122e-01, PNorm = 51.7256, GNorm = 0.1204
Meta loss on this task batch = 4.2145e-01, Meta loss averaged over last 500 steps = 4.9080e-01, PNorm = 51.7294, GNorm = 0.2631
Meta loss on this task batch = 5.1169e-01, Meta loss averaged over last 500 steps = 4.9092e-01, PNorm = 51.7335, GNorm = 0.0986
Meta loss on this task batch = 4.4507e-01, Meta loss averaged over last 500 steps = 4.9065e-01, PNorm = 51.7383, GNorm = 0.0982
Meta loss on this task batch = 4.6998e-01, Meta loss averaged over last 500 steps = 4.9053e-01, PNorm = 51.7422, GNorm = 0.2269
Meta loss on this task batch = 5.3356e-01, Meta loss averaged over last 500 steps = 4.9078e-01, PNorm = 51.7457, GNorm = 0.1607
Meta loss on this task batch = 4.3638e-01, Meta loss averaged over last 500 steps = 4.9046e-01, PNorm = 51.7495, GNorm = 0.4036
Took 112.93763470649719 seconds to complete one epoch of meta training
Took 120.81534099578857 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.334967
Found better MAML checkpoint after meta validation, saving now
Epoch 9
Meta loss on this task batch = 4.7633e-01, Meta loss averaged over last 500 steps = 4.9038e-01, PNorm = 51.7523, GNorm = 0.2815
Meta loss on this task batch = 5.1935e-01, Meta loss averaged over last 500 steps = 4.9055e-01, PNorm = 51.7553, GNorm = 0.2191
Meta loss on this task batch = 5.0088e-01, Meta loss averaged over last 500 steps = 4.9061e-01, PNorm = 51.7591, GNorm = 0.1999
Meta loss on this task batch = 4.6897e-01, Meta loss averaged over last 500 steps = 4.9048e-01, PNorm = 51.7640, GNorm = 0.2581
Meta loss on this task batch = 5.0067e-01, Meta loss averaged over last 500 steps = 4.9054e-01, PNorm = 51.7694, GNorm = 0.2324
Meta loss on this task batch = 4.9556e-01, Meta loss averaged over last 500 steps = 4.9057e-01, PNorm = 51.7750, GNorm = 0.0846
Meta loss on this task batch = 4.5662e-01, Meta loss averaged over last 500 steps = 4.9038e-01, PNorm = 51.7813, GNorm = 0.4186
Meta loss on this task batch = 5.1055e-01, Meta loss averaged over last 500 steps = 4.9049e-01, PNorm = 51.7872, GNorm = 0.1122
Meta loss on this task batch = 4.1611e-01, Meta loss averaged over last 500 steps = 4.9008e-01, PNorm = 51.7912, GNorm = 0.2726
Meta loss on this task batch = 5.3350e-01, Meta loss averaged over last 500 steps = 4.9032e-01, PNorm = 51.7934, GNorm = 0.3298
Meta loss on this task batch = 5.2787e-01, Meta loss averaged over last 500 steps = 4.9052e-01, PNorm = 51.7948, GNorm = 0.3264
Meta loss on this task batch = 4.5592e-01, Meta loss averaged over last 500 steps = 4.9033e-01, PNorm = 51.7966, GNorm = 0.1259
Meta loss on this task batch = 4.6904e-01, Meta loss averaged over last 500 steps = 4.9022e-01, PNorm = 51.7981, GNorm = 0.2190
Meta loss on this task batch = 4.3423e-01, Meta loss averaged over last 500 steps = 4.8992e-01, PNorm = 51.8000, GNorm = 0.3214
Meta loss on this task batch = 4.4910e-01, Meta loss averaged over last 500 steps = 4.8970e-01, PNorm = 51.8026, GNorm = 0.4049
Meta loss on this task batch = 5.0267e-01, Meta loss averaged over last 500 steps = 4.8977e-01, PNorm = 51.8057, GNorm = 0.4694
Meta loss on this task batch = 4.9054e-01, Meta loss averaged over last 500 steps = 4.8977e-01, PNorm = 51.8099, GNorm = 0.3311
Meta loss on this task batch = 4.4490e-01, Meta loss averaged over last 500 steps = 4.8953e-01, PNorm = 51.8155, GNorm = 0.4091
Meta loss on this task batch = 4.4334e-01, Meta loss averaged over last 500 steps = 4.8929e-01, PNorm = 51.8219, GNorm = 0.0916
Took 115.09485960006714 seconds to complete one epoch of meta training
Took 123.98338675498962 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.337133
Found better MAML checkpoint after meta validation, saving now
Epoch 10
Meta loss on this task batch = 4.5845e-01, Meta loss averaged over last 500 steps = 4.8913e-01, PNorm = 51.8284, GNorm = 0.1696
Meta loss on this task batch = 5.2265e-01, Meta loss averaged over last 500 steps = 4.8930e-01, PNorm = 51.8343, GNorm = 0.1571
Meta loss on this task batch = 4.6431e-01, Meta loss averaged over last 500 steps = 4.8917e-01, PNorm = 51.8385, GNorm = 0.1675
Meta loss on this task batch = 4.6690e-01, Meta loss averaged over last 500 steps = 4.8906e-01, PNorm = 51.8412, GNorm = 0.2009
Meta loss on this task batch = 5.2560e-01, Meta loss averaged over last 500 steps = 4.8925e-01, PNorm = 51.8430, GNorm = 0.1894
Meta loss on this task batch = 4.8483e-01, Meta loss averaged over last 500 steps = 4.8922e-01, PNorm = 51.8446, GNorm = 0.2705
Meta loss on this task batch = 4.0941e-01, Meta loss averaged over last 500 steps = 4.8882e-01, PNorm = 51.8469, GNorm = 0.2593
Meta loss on this task batch = 4.9653e-01, Meta loss averaged over last 500 steps = 4.8886e-01, PNorm = 51.8506, GNorm = 0.2310
Meta loss on this task batch = 4.7130e-01, Meta loss averaged over last 500 steps = 4.8877e-01, PNorm = 51.8555, GNorm = 0.1238
Meta loss on this task batch = 4.5882e-01, Meta loss averaged over last 500 steps = 4.8862e-01, PNorm = 51.8615, GNorm = 0.2195
Meta loss on this task batch = 5.1729e-01, Meta loss averaged over last 500 steps = 4.8876e-01, PNorm = 51.8672, GNorm = 0.2884
Meta loss on this task batch = 4.1473e-01, Meta loss averaged over last 500 steps = 4.8840e-01, PNorm = 51.8736, GNorm = 0.0849
Meta loss on this task batch = 5.2445e-01, Meta loss averaged over last 500 steps = 4.8857e-01, PNorm = 51.8794, GNorm = 0.0924
Meta loss on this task batch = 5.1153e-01, Meta loss averaged over last 500 steps = 4.8869e-01, PNorm = 51.8849, GNorm = 0.1029
Meta loss on this task batch = 4.2777e-01, Meta loss averaged over last 500 steps = 4.8839e-01, PNorm = 51.8899, GNorm = 0.3780
Meta loss on this task batch = 5.6800e-01, Meta loss averaged over last 500 steps = 4.8877e-01, PNorm = 51.8930, GNorm = 0.1882
Meta loss on this task batch = 4.7639e-01, Meta loss averaged over last 500 steps = 4.8872e-01, PNorm = 51.8960, GNorm = 0.1344
Meta loss on this task batch = 5.0008e-01, Meta loss averaged over last 500 steps = 4.8877e-01, PNorm = 51.8984, GNorm = 0.1503
Meta loss on this task batch = 5.0934e-01, Meta loss averaged over last 500 steps = 4.8887e-01, PNorm = 51.9020, GNorm = 0.0977
Took 108.26016211509705 seconds to complete one epoch of meta training
Took 116.0860481262207 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.318605
Epoch 11
Meta loss on this task batch = 4.6832e-01, Meta loss averaged over last 500 steps = 4.8877e-01, PNorm = 51.9065, GNorm = 0.0717
Meta loss on this task batch = 4.8821e-01, Meta loss averaged over last 500 steps = 4.8877e-01, PNorm = 51.9118, GNorm = 0.2414
Meta loss on this task batch = 4.6008e-01, Meta loss averaged over last 500 steps = 4.8863e-01, PNorm = 51.9177, GNorm = 0.1868
Meta loss on this task batch = 5.1206e-01, Meta loss averaged over last 500 steps = 4.8874e-01, PNorm = 51.9246, GNorm = 0.3210
Meta loss on this task batch = 5.2339e-01, Meta loss averaged over last 500 steps = 4.8890e-01, PNorm = 51.9307, GNorm = 0.0935
Meta loss on this task batch = 4.3601e-01, Meta loss averaged over last 500 steps = 4.8866e-01, PNorm = 51.9376, GNorm = 0.3870
Meta loss on this task batch = 4.5656e-01, Meta loss averaged over last 500 steps = 4.8851e-01, PNorm = 51.9454, GNorm = 0.2176
Meta loss on this task batch = 5.3442e-01, Meta loss averaged over last 500 steps = 4.8872e-01, PNorm = 51.9478, GNorm = 0.3530
Meta loss on this task batch = 4.5460e-01, Meta loss averaged over last 500 steps = 4.8856e-01, PNorm = 51.9496, GNorm = 0.1225
Meta loss on this task batch = 5.0914e-01, Meta loss averaged over last 500 steps = 4.8866e-01, PNorm = 51.9522, GNorm = 0.1197
Meta loss on this task batch = 4.4840e-01, Meta loss averaged over last 500 steps = 4.8848e-01, PNorm = 51.9549, GNorm = 0.1541
Meta loss on this task batch = 5.2539e-01, Meta loss averaged over last 500 steps = 4.8864e-01, PNorm = 51.9575, GNorm = 0.0903
Meta loss on this task batch = 4.5952e-01, Meta loss averaged over last 500 steps = 4.8851e-01, PNorm = 51.9606, GNorm = 0.1795
Meta loss on this task batch = 5.0083e-01, Meta loss averaged over last 500 steps = 4.8857e-01, PNorm = 51.9644, GNorm = 0.1833
Meta loss on this task batch = 4.7413e-01, Meta loss averaged over last 500 steps = 4.8850e-01, PNorm = 51.9688, GNorm = 0.2068
Meta loss on this task batch = 4.5098e-01, Meta loss averaged over last 500 steps = 4.8834e-01, PNorm = 51.9747, GNorm = 0.1092
Meta loss on this task batch = 4.8282e-01, Meta loss averaged over last 500 steps = 4.8831e-01, PNorm = 51.9814, GNorm = 0.0694
Meta loss on this task batch = 4.4897e-01, Meta loss averaged over last 500 steps = 4.8814e-01, PNorm = 51.9879, GNorm = 0.2856
Meta loss on this task batch = 4.7693e-01, Meta loss averaged over last 500 steps = 4.8809e-01, PNorm = 51.9947, GNorm = 0.1217
Took 107.85306286811829 seconds to complete one epoch of meta training
Took 115.08422255516052 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.334320
Epoch 12
Meta loss on this task batch = 4.9694e-01, Meta loss averaged over last 500 steps = 4.8813e-01, PNorm = 52.0013, GNorm = 0.2972
Meta loss on this task batch = 5.1047e-01, Meta loss averaged over last 500 steps = 4.8822e-01, PNorm = 52.0080, GNorm = 0.1310
Meta loss on this task batch = 4.8893e-01, Meta loss averaged over last 500 steps = 4.8823e-01, PNorm = 52.0147, GNorm = 0.1247
Meta loss on this task batch = 4.8597e-01, Meta loss averaged over last 500 steps = 4.8822e-01, PNorm = 52.0203, GNorm = 0.4591
Meta loss on this task batch = 4.1613e-01, Meta loss averaged over last 500 steps = 4.8791e-01, PNorm = 52.0260, GNorm = 0.1003
Meta loss on this task batch = 5.7313e-01, Meta loss averaged over last 500 steps = 4.8827e-01, PNorm = 52.0286, GNorm = 0.1362
Meta loss on this task batch = 5.6106e-01, Meta loss averaged over last 500 steps = 4.8858e-01, PNorm = 52.0291, GNorm = 0.1613
Meta loss on this task batch = 5.3995e-01, Meta loss averaged over last 500 steps = 4.8880e-01, PNorm = 52.0297, GNorm = 0.0768
Meta loss on this task batch = 4.0953e-01, Meta loss averaged over last 500 steps = 4.8847e-01, PNorm = 52.0317, GNorm = 0.0761
Meta loss on this task batch = 4.2217e-01, Meta loss averaged over last 500 steps = 4.8819e-01, PNorm = 52.0347, GNorm = 0.0809
Meta loss on this task batch = 4.6630e-01, Meta loss averaged over last 500 steps = 4.8810e-01, PNorm = 52.0393, GNorm = 0.1761
Meta loss on this task batch = 4.6860e-01, Meta loss averaged over last 500 steps = 4.8801e-01, PNorm = 52.0449, GNorm = 0.1735
Meta loss on this task batch = 4.9120e-01, Meta loss averaged over last 500 steps = 4.8803e-01, PNorm = 52.0514, GNorm = 0.1499
Meta loss on this task batch = 4.7675e-01, Meta loss averaged over last 500 steps = 4.8798e-01, PNorm = 52.0579, GNorm = 0.0763
Meta loss on this task batch = 3.6494e-01, Meta loss averaged over last 500 steps = 4.8747e-01, PNorm = 52.0648, GNorm = 0.1322
Meta loss on this task batch = 5.4129e-01, Meta loss averaged over last 500 steps = 4.8770e-01, PNorm = 52.0713, GNorm = 0.0822
Meta loss on this task batch = 5.0249e-01, Meta loss averaged over last 500 steps = 4.8776e-01, PNorm = 52.0773, GNorm = 0.1287
Meta loss on this task batch = 4.6871e-01, Meta loss averaged over last 500 steps = 4.8768e-01, PNorm = 52.0833, GNorm = 0.0814
Meta loss on this task batch = 4.1326e-01, Meta loss averaged over last 500 steps = 4.8738e-01, PNorm = 52.0888, GNorm = 0.2327
Took 109.36071825027466 seconds to complete one epoch of meta training
Took 117.6744647026062 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.317894
Epoch 13
Meta loss on this task batch = 4.4238e-01, Meta loss averaged over last 500 steps = 4.8720e-01, PNorm = 52.0938, GNorm = 0.0928
Meta loss on this task batch = 4.9181e-01, Meta loss averaged over last 500 steps = 4.8721e-01, PNorm = 52.0982, GNorm = 0.1284
Meta loss on this task batch = 4.1836e-01, Meta loss averaged over last 500 steps = 4.8694e-01, PNorm = 52.1049, GNorm = 0.4307
Meta loss on this task batch = 4.8817e-01, Meta loss averaged over last 500 steps = 4.8694e-01, PNorm = 52.1103, GNorm = 0.1718
Meta loss on this task batch = 4.7980e-01, Meta loss averaged over last 500 steps = 4.8691e-01, PNorm = 52.1141, GNorm = 0.1316
Meta loss on this task batch = 5.1529e-01, Meta loss averaged over last 500 steps = 4.8703e-01, PNorm = 52.1163, GNorm = 0.1999
Meta loss on this task batch = 4.3887e-01, Meta loss averaged over last 500 steps = 4.8684e-01, PNorm = 52.1186, GNorm = 0.1091
Meta loss on this task batch = 4.6956e-01, Meta loss averaged over last 500 steps = 4.8677e-01, PNorm = 52.1192, GNorm = 0.3992
Meta loss on this task batch = 4.7249e-01, Meta loss averaged over last 500 steps = 4.8671e-01, PNorm = 52.1212, GNorm = 0.0959
Meta loss on this task batch = 4.8800e-01, Meta loss averaged over last 500 steps = 4.8672e-01, PNorm = 52.1242, GNorm = 0.2636
Meta loss on this task batch = 5.2684e-01, Meta loss averaged over last 500 steps = 4.8687e-01, PNorm = 52.1271, GNorm = 0.0989
Meta loss on this task batch = 4.9414e-01, Meta loss averaged over last 500 steps = 4.8690e-01, PNorm = 52.1308, GNorm = 0.0655
Meta loss on this task batch = 4.7613e-01, Meta loss averaged over last 500 steps = 4.8686e-01, PNorm = 52.1353, GNorm = 0.1742
Meta loss on this task batch = 5.1485e-01, Meta loss averaged over last 500 steps = 4.8697e-01, PNorm = 52.1400, GNorm = 0.1400
Meta loss on this task batch = 4.5685e-01, Meta loss averaged over last 500 steps = 4.8685e-01, PNorm = 52.1449, GNorm = 0.0744
Meta loss on this task batch = 4.5140e-01, Meta loss averaged over last 500 steps = 4.8672e-01, PNorm = 52.1499, GNorm = 0.1463
Meta loss on this task batch = 5.1844e-01, Meta loss averaged over last 500 steps = 4.8684e-01, PNorm = 52.1551, GNorm = 0.1353
Meta loss on this task batch = 4.9829e-01, Meta loss averaged over last 500 steps = 4.8688e-01, PNorm = 52.1601, GNorm = 0.2942
Meta loss on this task batch = 4.4490e-01, Meta loss averaged over last 500 steps = 4.8672e-01, PNorm = 52.1663, GNorm = 0.1238
Took 115.8565878868103 seconds to complete one epoch of meta training
Took 124.24419736862183 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.325471
Epoch 14
Meta loss on this task batch = 4.8722e-01, Meta loss averaged over last 500 steps = 4.8673e-01, PNorm = 52.1726, GNorm = 0.1764
Meta loss on this task batch = 5.1884e-01, Meta loss averaged over last 500 steps = 4.8685e-01, PNorm = 52.1770, GNorm = 0.1377
Meta loss on this task batch = 4.3982e-01, Meta loss averaged over last 500 steps = 4.8667e-01, PNorm = 52.1825, GNorm = 0.0899
Meta loss on this task batch = 4.1840e-01, Meta loss averaged over last 500 steps = 4.8642e-01, PNorm = 52.1885, GNorm = 0.0910
Meta loss on this task batch = 4.2613e-01, Meta loss averaged over last 500 steps = 4.8620e-01, PNorm = 52.1952, GNorm = 0.2397
Meta loss on this task batch = 4.5602e-01, Meta loss averaged over last 500 steps = 4.8608e-01, PNorm = 52.2020, GNorm = 0.2044
Meta loss on this task batch = 5.0249e-01, Meta loss averaged over last 500 steps = 4.8614e-01, PNorm = 52.2074, GNorm = 0.2193
Meta loss on this task batch = 4.9638e-01, Meta loss averaged over last 500 steps = 4.8618e-01, PNorm = 52.2126, GNorm = 0.3148
Meta loss on this task batch = 4.5590e-01, Meta loss averaged over last 500 steps = 4.8607e-01, PNorm = 52.2171, GNorm = 0.4042
Meta loss on this task batch = 5.3278e-01, Meta loss averaged over last 500 steps = 4.8624e-01, PNorm = 52.2204, GNorm = 0.1161
Meta loss on this task batch = 4.6296e-01, Meta loss averaged over last 500 steps = 4.8616e-01, PNorm = 52.2244, GNorm = 0.1279
Meta loss on this task batch = 4.3694e-01, Meta loss averaged over last 500 steps = 4.8598e-01, PNorm = 52.2289, GNorm = 0.3419
Meta loss on this task batch = 4.6274e-01, Meta loss averaged over last 500 steps = 4.8590e-01, PNorm = 52.2332, GNorm = 0.1420
Meta loss on this task batch = 4.9098e-01, Meta loss averaged over last 500 steps = 4.8592e-01, PNorm = 52.2375, GNorm = 0.0992
Meta loss on this task batch = 4.8768e-01, Meta loss averaged over last 500 steps = 4.8592e-01, PNorm = 52.2431, GNorm = 0.6166
Meta loss on this task batch = 5.3342e-01, Meta loss averaged over last 500 steps = 4.8609e-01, PNorm = 52.2493, GNorm = 0.3719
Meta loss on this task batch = 4.6760e-01, Meta loss averaged over last 500 steps = 4.8602e-01, PNorm = 52.2560, GNorm = 0.1039
Meta loss on this task batch = 4.4994e-01, Meta loss averaged over last 500 steps = 4.8590e-01, PNorm = 52.2643, GNorm = 0.0990
Meta loss on this task batch = 5.0659e-01, Meta loss averaged over last 500 steps = 4.8597e-01, PNorm = 52.2707, GNorm = 0.1385
Took 113.58796429634094 seconds to complete one epoch of meta training
Took 121.44594860076904 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.330259
Epoch 15
Meta loss on this task batch = 4.5268e-01, Meta loss averaged over last 500 steps = 4.8585e-01, PNorm = 52.2759, GNorm = 0.0806
Meta loss on this task batch = 4.8552e-01, Meta loss averaged over last 500 steps = 4.8585e-01, PNorm = 52.2812, GNorm = 0.0663
Meta loss on this task batch = 4.5811e-01, Meta loss averaged over last 500 steps = 4.8576e-01, PNorm = 52.2854, GNorm = 0.1342
Meta loss on this task batch = 4.3483e-01, Meta loss averaged over last 500 steps = 4.8558e-01, PNorm = 52.2910, GNorm = 0.1544
Meta loss on this task batch = 5.0320e-01, Meta loss averaged over last 500 steps = 4.8564e-01, PNorm = 52.2956, GNorm = 0.2785
Meta loss on this task batch = 4.9871e-01, Meta loss averaged over last 500 steps = 4.8569e-01, PNorm = 52.2997, GNorm = 0.7280
Meta loss on this task batch = 5.1787e-01, Meta loss averaged over last 500 steps = 4.8580e-01, PNorm = 52.3030, GNorm = 0.1844
Meta loss on this task batch = 5.0822e-01, Meta loss averaged over last 500 steps = 4.8587e-01, PNorm = 52.3059, GNorm = 0.1661
Meta loss on this task batch = 5.0042e-01, Meta loss averaged over last 500 steps = 4.8592e-01, PNorm = 52.3084, GNorm = 0.2147
Meta loss on this task batch = 4.3983e-01, Meta loss averaged over last 500 steps = 4.8577e-01, PNorm = 52.3108, GNorm = 0.2697
Meta loss on this task batch = 4.6090e-01, Meta loss averaged over last 500 steps = 4.8568e-01, PNorm = 52.3130, GNorm = 0.2880
Meta loss on this task batch = 4.8931e-01, Meta loss averaged over last 500 steps = 4.8569e-01, PNorm = 52.3165, GNorm = 0.1162
Meta loss on this task batch = 4.1913e-01, Meta loss averaged over last 500 steps = 4.8547e-01, PNorm = 52.3219, GNorm = 0.3637
Meta loss on this task batch = 5.0577e-01, Meta loss averaged over last 500 steps = 4.8554e-01, PNorm = 52.3276, GNorm = 0.4970
Meta loss on this task batch = 4.6046e-01, Meta loss averaged over last 500 steps = 4.8545e-01, PNorm = 52.3349, GNorm = 0.4234
Meta loss on this task batch = 4.3829e-01, Meta loss averaged over last 500 steps = 4.8530e-01, PNorm = 52.3435, GNorm = 0.1670
Meta loss on this task batch = 4.5963e-01, Meta loss averaged over last 500 steps = 4.8521e-01, PNorm = 52.3534, GNorm = 0.0937
Meta loss on this task batch = 5.0417e-01, Meta loss averaged over last 500 steps = 4.8528e-01, PNorm = 52.3632, GNorm = 0.2342
Meta loss on this task batch = 5.6697e-01, Meta loss averaged over last 500 steps = 4.8554e-01, PNorm = 52.3707, GNorm = 0.3878
Took 140.46309089660645 seconds to complete one epoch of meta training
Took 148.54481053352356 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.326146
Epoch 16
Meta loss on this task batch = 4.6914e-01, Meta loss averaged over last 500 steps = 4.8549e-01, PNorm = 52.3774, GNorm = 0.0994
Meta loss on this task batch = 4.3346e-01, Meta loss averaged over last 500 steps = 4.8532e-01, PNorm = 52.3848, GNorm = 0.0905
Meta loss on this task batch = 4.3778e-01, Meta loss averaged over last 500 steps = 4.8517e-01, PNorm = 52.3928, GNorm = 0.3352
Meta loss on this task batch = 4.5760e-01, Meta loss averaged over last 500 steps = 4.8508e-01, PNorm = 52.4008, GNorm = 0.1113
Meta loss on this task batch = 4.8072e-01, Meta loss averaged over last 500 steps = 4.8506e-01, PNorm = 52.4082, GNorm = 0.1262
Meta loss on this task batch = 4.8888e-01, Meta loss averaged over last 500 steps = 4.8507e-01, PNorm = 52.4144, GNorm = 0.2683
Meta loss on this task batch = 5.4134e-01, Meta loss averaged over last 500 steps = 4.8526e-01, PNorm = 52.4198, GNorm = 0.0926
Meta loss on this task batch = 4.9309e-01, Meta loss averaged over last 500 steps = 4.8528e-01, PNorm = 52.4259, GNorm = 0.2993
Meta loss on this task batch = 4.7400e-01, Meta loss averaged over last 500 steps = 4.8524e-01, PNorm = 52.4316, GNorm = 0.0907
Meta loss on this task batch = 5.0751e-01, Meta loss averaged over last 500 steps = 4.8532e-01, PNorm = 52.4364, GNorm = 0.1509
Meta loss on this task batch = 5.0673e-01, Meta loss averaged over last 500 steps = 4.8538e-01, PNorm = 52.4413, GNorm = 0.2656
Meta loss on this task batch = 4.4911e-01, Meta loss averaged over last 500 steps = 4.8527e-01, PNorm = 52.4463, GNorm = 0.1783
Meta loss on this task batch = 5.1332e-01, Meta loss averaged over last 500 steps = 4.8536e-01, PNorm = 52.4518, GNorm = 0.4523
Meta loss on this task batch = 4.9150e-01, Meta loss averaged over last 500 steps = 4.8538e-01, PNorm = 52.4574, GNorm = 0.1449
Meta loss on this task batch = 5.0950e-01, Meta loss averaged over last 500 steps = 4.8545e-01, PNorm = 52.4640, GNorm = 0.3324
Meta loss on this task batch = 3.9013e-01, Meta loss averaged over last 500 steps = 4.8515e-01, PNorm = 52.4717, GNorm = 0.0790
Meta loss on this task batch = 5.1495e-01, Meta loss averaged over last 500 steps = 4.8525e-01, PNorm = 52.4783, GNorm = 0.0986
Meta loss on this task batch = 4.8385e-01, Meta loss averaged over last 500 steps = 4.8524e-01, PNorm = 52.4846, GNorm = 0.1407
Meta loss on this task batch = 4.6845e-01, Meta loss averaged over last 500 steps = 4.8519e-01, PNorm = 52.4912, GNorm = 0.1207
Took 131.55184197425842 seconds to complete one epoch of meta training
Took 139.23881030082703 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.325471
Epoch 17
Meta loss on this task batch = 4.1751e-01, Meta loss averaged over last 500 steps = 4.8498e-01, PNorm = 52.4985, GNorm = 0.0893
Meta loss on this task batch = 4.6948e-01, Meta loss averaged over last 500 steps = 4.8493e-01, PNorm = 52.5051, GNorm = 0.0745
Meta loss on this task batch = 5.2463e-01, Meta loss averaged over last 500 steps = 4.8506e-01, PNorm = 52.5114, GNorm = 0.0944
Meta loss on this task batch = 4.5653e-01, Meta loss averaged over last 500 steps = 4.8497e-01, PNorm = 52.5177, GNorm = 0.1665
Meta loss on this task batch = 4.9011e-01, Meta loss averaged over last 500 steps = 4.8498e-01, PNorm = 52.5235, GNorm = 0.1390
Meta loss on this task batch = 4.3978e-01, Meta loss averaged over last 500 steps = 4.8485e-01, PNorm = 52.5279, GNorm = 0.1809
Meta loss on this task batch = 5.0971e-01, Meta loss averaged over last 500 steps = 4.8492e-01, PNorm = 52.5323, GNorm = 0.2322
Meta loss on this task batch = 4.9973e-01, Meta loss averaged over last 500 steps = 4.8497e-01, PNorm = 52.5352, GNorm = 0.1244
Meta loss on this task batch = 5.0700e-01, Meta loss averaged over last 500 steps = 4.8503e-01, PNorm = 52.5382, GNorm = 0.0754
Meta loss on this task batch = 5.5253e-01, Meta loss averaged over last 500 steps = 4.8524e-01, PNorm = 52.5418, GNorm = 0.2573
Meta loss on this task batch = 4.3045e-01, Meta loss averaged over last 500 steps = 4.8507e-01, PNorm = 52.5453, GNorm = 0.3161
Meta loss on this task batch = 4.6447e-01, Meta loss averaged over last 500 steps = 4.8501e-01, PNorm = 52.5496, GNorm = 0.1984
Meta loss on this task batch = 3.8799e-01, Meta loss averaged over last 500 steps = 4.8472e-01, PNorm = 52.5554, GNorm = 0.2578
Meta loss on this task batch = 4.9681e-01, Meta loss averaged over last 500 steps = 4.8476e-01, PNorm = 52.5618, GNorm = 0.1341
Meta loss on this task batch = 4.9094e-01, Meta loss averaged over last 500 steps = 4.8478e-01, PNorm = 52.5694, GNorm = 0.3883
Meta loss on this task batch = 3.9395e-01, Meta loss averaged over last 500 steps = 4.8451e-01, PNorm = 52.5766, GNorm = 0.1363
Meta loss on this task batch = 5.2376e-01, Meta loss averaged over last 500 steps = 4.8462e-01, PNorm = 52.5828, GNorm = 0.0905
Meta loss on this task batch = 5.0218e-01, Meta loss averaged over last 500 steps = 4.8467e-01, PNorm = 52.5899, GNorm = 0.2195
Meta loss on this task batch = 5.1938e-01, Meta loss averaged over last 500 steps = 4.8478e-01, PNorm = 52.5963, GNorm = 0.3930
Took 211.87380576133728 seconds to complete one epoch of meta training
Took 220.38535141944885 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.327140
Epoch 18
Meta loss on this task batch = 4.9200e-01, Meta loss averaged over last 500 steps = 4.8480e-01, PNorm = 52.6014, GNorm = 0.1046
Meta loss on this task batch = 4.6754e-01, Meta loss averaged over last 500 steps = 4.8475e-01, PNorm = 52.6059, GNorm = 0.1226
Meta loss on this task batch = 4.7047e-01, Meta loss averaged over last 500 steps = 4.8471e-01, PNorm = 52.6085, GNorm = 0.3002
Meta loss on this task batch = 4.4504e-01, Meta loss averaged over last 500 steps = 4.8459e-01, PNorm = 52.6097, GNorm = 0.3086
Meta loss on this task batch = 5.3369e-01, Meta loss averaged over last 500 steps = 4.8473e-01, PNorm = 52.6107, GNorm = 0.0842
Meta loss on this task batch = 4.9673e-01, Meta loss averaged over last 500 steps = 4.8477e-01, PNorm = 52.6111, GNorm = 0.2454
Meta loss on this task batch = 5.1888e-01, Meta loss averaged over last 500 steps = 4.8486e-01, PNorm = 52.6120, GNorm = 0.0890
Meta loss on this task batch = 4.1593e-01, Meta loss averaged over last 500 steps = 4.8467e-01, PNorm = 52.6154, GNorm = 0.1401
Meta loss on this task batch = 4.9352e-01, Meta loss averaged over last 500 steps = 4.8469e-01, PNorm = 52.6199, GNorm = 0.2314
Meta loss on this task batch = 5.6204e-01, Meta loss averaged over last 500 steps = 4.8491e-01, PNorm = 52.6245, GNorm = 0.0936
Meta loss on this task batch = 4.2011e-01, Meta loss averaged over last 500 steps = 4.8473e-01, PNorm = 52.6308, GNorm = 0.1778
Meta loss on this task batch = 4.8643e-01, Meta loss averaged over last 500 steps = 4.8473e-01, PNorm = 52.6380, GNorm = 0.3077
Meta loss on this task batch = 4.7173e-01, Meta loss averaged over last 500 steps = 4.8470e-01, PNorm = 52.6455, GNorm = 0.1573
Meta loss on this task batch = 5.1457e-01, Meta loss averaged over last 500 steps = 4.8478e-01, PNorm = 52.6541, GNorm = 0.0727
Meta loss on this task batch = 4.8987e-01, Meta loss averaged over last 500 steps = 4.8480e-01, PNorm = 52.6635, GNorm = 0.1481
Meta loss on this task batch = 4.3823e-01, Meta loss averaged over last 500 steps = 4.8467e-01, PNorm = 52.6722, GNorm = 0.0722
Meta loss on this task batch = 4.5498e-01, Meta loss averaged over last 500 steps = 4.8458e-01, PNorm = 52.6806, GNorm = 0.0957
Meta loss on this task batch = 4.2955e-01, Meta loss averaged over last 500 steps = 4.8443e-01, PNorm = 52.6882, GNorm = 0.4149
Meta loss on this task batch = 4.9398e-01, Meta loss averaged over last 500 steps = 4.8446e-01, PNorm = 52.6962, GNorm = 0.1067
Took 108.12201189994812 seconds to complete one epoch of meta training
Took 115.83985233306885 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.373209
Found better MAML checkpoint after meta validation, saving now
Epoch 19
Meta loss on this task batch = 5.4027e-01, Meta loss averaged over last 500 steps = 4.8461e-01, PNorm = 52.7014, GNorm = 0.1942
Meta loss on this task batch = 4.3605e-01, Meta loss averaged over last 500 steps = 4.8448e-01, PNorm = 52.7069, GNorm = 0.0636
Meta loss on this task batch = 5.9833e-01, Meta loss averaged over last 500 steps = 4.8479e-01, PNorm = 52.7115, GNorm = 0.1125
Meta loss on this task batch = 4.3833e-01, Meta loss averaged over last 500 steps = 4.8466e-01, PNorm = 52.7170, GNorm = 0.2892
Meta loss on this task batch = 5.5282e-01, Meta loss averaged over last 500 steps = 4.8485e-01, PNorm = 52.7221, GNorm = 0.1071
Meta loss on this task batch = 3.9604e-01, Meta loss averaged over last 500 steps = 4.8461e-01, PNorm = 52.7282, GNorm = 0.0937
Meta loss on this task batch = 5.3386e-01, Meta loss averaged over last 500 steps = 4.8474e-01, PNorm = 52.7339, GNorm = 0.1926
Meta loss on this task batch = 5.2324e-01, Meta loss averaged over last 500 steps = 4.8484e-01, PNorm = 52.7396, GNorm = 0.0710
Meta loss on this task batch = 4.1627e-01, Meta loss averaged over last 500 steps = 4.8466e-01, PNorm = 52.7453, GNorm = 0.1576
Meta loss on this task batch = 4.6483e-01, Meta loss averaged over last 500 steps = 4.8461e-01, PNorm = 52.7515, GNorm = 0.0932
Meta loss on this task batch = 5.2144e-01, Meta loss averaged over last 500 steps = 4.8470e-01, PNorm = 52.7579, GNorm = 0.0805
Meta loss on this task batch = 4.8987e-01, Meta loss averaged over last 500 steps = 4.8472e-01, PNorm = 52.7655, GNorm = 0.0820
Meta loss on this task batch = 4.6539e-01, Meta loss averaged over last 500 steps = 4.8467e-01, PNorm = 52.7743, GNorm = 0.0886
Meta loss on this task batch = 5.1524e-01, Meta loss averaged over last 500 steps = 4.8475e-01, PNorm = 52.7823, GNorm = 0.3837
Meta loss on this task batch = 4.3963e-01, Meta loss averaged over last 500 steps = 4.8463e-01, PNorm = 52.7910, GNorm = 0.1700
Meta loss on this task batch = 4.9327e-01, Meta loss averaged over last 500 steps = 4.8465e-01, PNorm = 52.7994, GNorm = 0.1850
Meta loss on this task batch = 4.1168e-01, Meta loss averaged over last 500 steps = 4.8446e-01, PNorm = 52.8078, GNorm = 0.0668
Meta loss on this task batch = 4.0163e-01, Meta loss averaged over last 500 steps = 4.8424e-01, PNorm = 52.8158, GNorm = 0.0768
Meta loss on this task batch = 3.9844e-01, Meta loss averaged over last 500 steps = 4.8401e-01, PNorm = 52.8231, GNorm = 0.0919
Took 112.76018190383911 seconds to complete one epoch of meta training
Took 120.78573727607727 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.342854
Epoch 20
Meta loss on this task batch = 4.0898e-01, Meta loss averaged over last 500 steps = 4.8382e-01, PNorm = 52.8311, GNorm = 0.0887
Meta loss on this task batch = 5.0251e-01, Meta loss averaged over last 500 steps = 4.8387e-01, PNorm = 52.8393, GNorm = 0.1539
Meta loss on this task batch = 4.7229e-01, Meta loss averaged over last 500 steps = 4.8384e-01, PNorm = 52.8476, GNorm = 0.3806
Meta loss on this task batch = 4.2853e-01, Meta loss averaged over last 500 steps = 4.8369e-01, PNorm = 52.8567, GNorm = 0.1977
Meta loss on this task batch = 5.1493e-01, Meta loss averaged over last 500 steps = 4.8377e-01, PNorm = 52.8642, GNorm = 0.2480
Meta loss on this task batch = 4.9958e-01, Meta loss averaged over last 500 steps = 4.8381e-01, PNorm = 52.8695, GNorm = 0.1744
Meta loss on this task batch = 5.0568e-01, Meta loss averaged over last 500 steps = 4.8387e-01, PNorm = 52.8731, GNorm = 0.1599
Meta loss on this task batch = 4.6926e-01, Meta loss averaged over last 500 steps = 4.8383e-01, PNorm = 52.8751, GNorm = 0.1398
Meta loss on this task batch = 4.9135e-01, Meta loss averaged over last 500 steps = 4.8385e-01, PNorm = 52.8773, GNorm = 0.5292
Meta loss on this task batch = 4.9609e-01, Meta loss averaged over last 500 steps = 4.8388e-01, PNorm = 52.8819, GNorm = 0.4052
Meta loss on this task batch = 4.2917e-01, Meta loss averaged over last 500 steps = 4.8374e-01, PNorm = 52.8879, GNorm = 0.0974
Meta loss on this task batch = 5.0927e-01, Meta loss averaged over last 500 steps = 4.8381e-01, PNorm = 52.8931, GNorm = 0.2909
Meta loss on this task batch = 4.7842e-01, Meta loss averaged over last 500 steps = 4.8379e-01, PNorm = 52.8996, GNorm = 0.0839
Meta loss on this task batch = 4.7456e-01, Meta loss averaged over last 500 steps = 4.8377e-01, PNorm = 52.9062, GNorm = 0.2161
Meta loss on this task batch = 4.3360e-01, Meta loss averaged over last 500 steps = 4.8364e-01, PNorm = 52.9138, GNorm = 0.2922
Meta loss on this task batch = 5.0176e-01, Meta loss averaged over last 500 steps = 4.8369e-01, PNorm = 52.9219, GNorm = 0.1923
Meta loss on this task batch = 4.4793e-01, Meta loss averaged over last 500 steps = 4.8360e-01, PNorm = 52.9319, GNorm = 0.1042
Meta loss on this task batch = 5.1567e-01, Meta loss averaged over last 500 steps = 4.8368e-01, PNorm = 52.9421, GNorm = 0.2099
Meta loss on this task batch = 4.2417e-01, Meta loss averaged over last 500 steps = 4.8353e-01, PNorm = 52.9539, GNorm = 0.3006
Took 110.28283405303955 seconds to complete one epoch of meta training
Took 118.0703513622284 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.340498
Epoch 21
Meta loss on this task batch = 4.7076e-01, Meta loss averaged over last 500 steps = 4.8350e-01, PNorm = 52.9657, GNorm = 0.1891
Meta loss on this task batch = 4.5399e-01, Meta loss averaged over last 500 steps = 4.8343e-01, PNorm = 52.9778, GNorm = 0.2582
Meta loss on this task batch = 5.1308e-01, Meta loss averaged over last 500 steps = 4.8350e-01, PNorm = 52.9887, GNorm = 0.1571
Meta loss on this task batch = 4.8065e-01, Meta loss averaged over last 500 steps = 4.8349e-01, PNorm = 52.9980, GNorm = 0.1464
Meta loss on this task batch = 5.3867e-01, Meta loss averaged over last 500 steps = 4.8363e-01, PNorm = 53.0059, GNorm = 0.3054
Meta loss on this task batch = 5.0433e-01, Meta loss averaged over last 500 steps = 4.8368e-01, PNorm = 53.0122, GNorm = 0.1599
Meta loss on this task batch = 5.2888e-01, Meta loss averaged over last 500 steps = 4.8379e-01, PNorm = 53.0186, GNorm = 0.1879
Meta loss on this task batch = 4.1045e-01, Meta loss averaged over last 500 steps = 4.8361e-01, PNorm = 53.0255, GNorm = 0.0766
Meta loss on this task batch = 4.9189e-01, Meta loss averaged over last 500 steps = 4.8363e-01, PNorm = 53.0331, GNorm = 0.0826
Meta loss on this task batch = 4.7436e-01, Meta loss averaged over last 500 steps = 4.8361e-01, PNorm = 53.0415, GNorm = 0.0885
Meta loss on this task batch = 4.0911e-01, Meta loss averaged over last 500 steps = 4.8343e-01, PNorm = 53.0510, GNorm = 0.1329
Meta loss on this task batch = 4.6831e-01, Meta loss averaged over last 500 steps = 4.8339e-01, PNorm = 53.0620, GNorm = 0.1041
Meta loss on this task batch = 5.4306e-01, Meta loss averaged over last 500 steps = 4.8354e-01, PNorm = 53.0722, GNorm = 0.2816
Meta loss on this task batch = 3.9690e-01, Meta loss averaged over last 500 steps = 4.8333e-01, PNorm = 53.0843, GNorm = 0.2254
Meta loss on this task batch = 4.6873e-01, Meta loss averaged over last 500 steps = 4.8329e-01, PNorm = 53.0954, GNorm = 0.1097
Meta loss on this task batch = 4.9874e-01, Meta loss averaged over last 500 steps = 4.8333e-01, PNorm = 53.1075, GNorm = 0.3993
Meta loss on this task batch = 4.5427e-01, Meta loss averaged over last 500 steps = 4.8326e-01, PNorm = 53.1202, GNorm = 0.1633
Meta loss on this task batch = 4.6265e-01, Meta loss averaged over last 500 steps = 4.8321e-01, PNorm = 53.1288, GNorm = 0.1463
Meta loss on this task batch = 4.0526e-01, Meta loss averaged over last 500 steps = 4.8302e-01, PNorm = 53.1396, GNorm = 0.2426
Took 104.2706971168518 seconds to complete one epoch of meta training
Took 111.73092722892761 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.344261
Epoch 22
Meta loss on this task batch = 4.2194e-01, Meta loss averaged over last 500 steps = 4.8288e-01, PNorm = 53.1468, GNorm = 0.2755
Meta loss on this task batch = 5.6371e-01, Meta loss averaged over last 500 steps = 4.8307e-01, PNorm = 53.1501, GNorm = 0.2919
Meta loss on this task batch = 4.6021e-01, Meta loss averaged over last 500 steps = 4.8301e-01, PNorm = 53.1550, GNorm = 0.0758
Meta loss on this task batch = 5.3465e-01, Meta loss averaged over last 500 steps = 4.8314e-01, PNorm = 53.1591, GNorm = 0.1769
Meta loss on this task batch = 4.5096e-01, Meta loss averaged over last 500 steps = 4.8306e-01, PNorm = 53.1643, GNorm = 0.5397
Meta loss on this task batch = 4.6234e-01, Meta loss averaged over last 500 steps = 4.8301e-01, PNorm = 53.1717, GNorm = 0.3092
Meta loss on this task batch = 4.6057e-01, Meta loss averaged over last 500 steps = 4.8296e-01, PNorm = 53.1804, GNorm = 0.1084
Meta loss on this task batch = 4.4714e-01, Meta loss averaged over last 500 steps = 4.8287e-01, PNorm = 53.1907, GNorm = 0.1289
Meta loss on this task batch = 4.4728e-01, Meta loss averaged over last 500 steps = 4.8279e-01, PNorm = 53.2020, GNorm = 0.1696
Meta loss on this task batch = 4.1975e-01, Meta loss averaged over last 500 steps = 4.8264e-01, PNorm = 53.2130, GNorm = 0.0921
Meta loss on this task batch = 5.1669e-01, Meta loss averaged over last 500 steps = 4.8272e-01, PNorm = 53.2230, GNorm = 0.1902
Meta loss on this task batch = 5.3284e-01, Meta loss averaged over last 500 steps = 4.8284e-01, PNorm = 53.2324, GNorm = 0.1354
Meta loss on this task batch = 5.3514e-01, Meta loss averaged over last 500 steps = 4.8296e-01, PNorm = 53.2412, GNorm = 0.4046
Meta loss on this task batch = 4.5349e-01, Meta loss averaged over last 500 steps = 4.8289e-01, PNorm = 53.2505, GNorm = 0.1837
Meta loss on this task batch = 4.4964e-01, Meta loss averaged over last 500 steps = 4.8282e-01, PNorm = 53.2613, GNorm = 0.1108
Meta loss on this task batch = 4.4922e-01, Meta loss averaged over last 500 steps = 4.8274e-01, PNorm = 53.2725, GNorm = 0.1433
Meta loss on this task batch = 4.6138e-01, Meta loss averaged over last 500 steps = 4.8269e-01, PNorm = 53.2837, GNorm = 0.0896
Meta loss on this task batch = 4.8650e-01, Meta loss averaged over last 500 steps = 4.8270e-01, PNorm = 53.2961, GNorm = 0.0811
Meta loss on this task batch = 5.1370e-01, Meta loss averaged over last 500 steps = 4.8277e-01, PNorm = 53.3084, GNorm = 0.1312
Took 123.257328748703 seconds to complete one epoch of meta training
Took 131.6345624923706 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.330105
Epoch 23
Meta loss on this task batch = 4.4724e-01, Meta loss averaged over last 500 steps = 4.8269e-01, PNorm = 53.3204, GNorm = 0.1803
Meta loss on this task batch = 4.5790e-01, Meta loss averaged over last 500 steps = 4.8263e-01, PNorm = 53.3321, GNorm = 0.2638
Meta loss on this task batch = 5.1586e-01, Meta loss averaged over last 500 steps = 4.8271e-01, PNorm = 53.3427, GNorm = 0.2255
Meta loss on this task batch = 4.8872e-01, Meta loss averaged over last 500 steps = 4.8272e-01, PNorm = 53.3524, GNorm = 0.3139
Meta loss on this task batch = 4.6078e-01, Meta loss averaged over last 500 steps = 4.8267e-01, PNorm = 53.3631, GNorm = 0.1024
Meta loss on this task batch = 4.9518e-01, Meta loss averaged over last 500 steps = 4.8270e-01, PNorm = 53.3738, GNorm = 0.1489
Meta loss on this task batch = 4.1765e-01, Meta loss averaged over last 500 steps = 4.8255e-01, PNorm = 53.3840, GNorm = 0.0720
Meta loss on this task batch = 4.7397e-01, Meta loss averaged over last 500 steps = 4.8253e-01, PNorm = 53.3937, GNorm = 0.1794
Meta loss on this task batch = 4.4499e-01, Meta loss averaged over last 500 steps = 4.8245e-01, PNorm = 53.4038, GNorm = 0.3790
Meta loss on this task batch = 4.6377e-01, Meta loss averaged over last 500 steps = 4.8241e-01, PNorm = 53.4136, GNorm = 0.0865
Meta loss on this task batch = 5.6731e-01, Meta loss averaged over last 500 steps = 4.8260e-01, PNorm = 53.4208, GNorm = 0.1458
Meta loss on this task batch = 4.6226e-01, Meta loss averaged over last 500 steps = 4.8255e-01, PNorm = 53.4277, GNorm = 0.1703
Meta loss on this task batch = 4.3845e-01, Meta loss averaged over last 500 steps = 4.8245e-01, PNorm = 53.4357, GNorm = 0.1540
Meta loss on this task batch = 4.7612e-01, Meta loss averaged over last 500 steps = 4.8244e-01, PNorm = 53.4431, GNorm = 0.2147
Meta loss on this task batch = 5.1468e-01, Meta loss averaged over last 500 steps = 4.8251e-01, PNorm = 53.4511, GNorm = 0.1450
Meta loss on this task batch = 5.0213e-01, Meta loss averaged over last 500 steps = 4.8255e-01, PNorm = 53.4592, GNorm = 0.3036
Meta loss on this task batch = 4.2181e-01, Meta loss averaged over last 500 steps = 4.8242e-01, PNorm = 53.4677, GNorm = 0.1130
Meta loss on this task batch = 5.0414e-01, Meta loss averaged over last 500 steps = 4.8247e-01, PNorm = 53.4774, GNorm = 0.1084
Meta loss on this task batch = 4.7363e-01, Meta loss averaged over last 500 steps = 4.8245e-01, PNorm = 53.4883, GNorm = 0.0880
Took 117.67022442817688 seconds to complete one epoch of meta training
Took 125.55155968666077 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.356085
Epoch 24
Meta loss on this task batch = 4.4627e-01, Meta loss averaged over last 500 steps = 4.8237e-01, PNorm = 53.4994, GNorm = 0.1764
Meta loss on this task batch = 4.5498e-01, Meta loss averaged over last 500 steps = 4.8231e-01, PNorm = 53.5113, GNorm = 0.0951
Meta loss on this task batch = 4.3086e-01, Meta loss averaged over last 500 steps = 4.8220e-01, PNorm = 53.5236, GNorm = 0.0988
Meta loss on this task batch = 4.9127e-01, Meta loss averaged over last 500 steps = 4.8222e-01, PNorm = 53.5370, GNorm = 0.3135
Meta loss on this task batch = 5.0668e-01, Meta loss averaged over last 500 steps = 4.8227e-01, PNorm = 53.5484, GNorm = 0.1088
Meta loss on this task batch = 4.8453e-01, Meta loss averaged over last 500 steps = 4.8228e-01, PNorm = 53.5584, GNorm = 0.1820
Meta loss on this task batch = 4.0641e-01, Meta loss averaged over last 500 steps = 4.8211e-01, PNorm = 53.5660, GNorm = 0.2195
Meta loss on this task batch = 5.3458e-01, Meta loss averaged over last 500 steps = 4.8223e-01, PNorm = 53.5735, GNorm = 0.2689
Meta loss on this task batch = 4.9965e-01, Meta loss averaged over last 500 steps = 4.8226e-01, PNorm = 53.5791, GNorm = 0.2022
Meta loss on this task batch = 4.9645e-01, Meta loss averaged over last 500 steps = 4.8229e-01, PNorm = 53.5865, GNorm = 0.2271
Meta loss on this task batch = 4.5472e-01, Meta loss averaged over last 500 steps = 4.8223e-01, PNorm = 53.5944, GNorm = 0.1077
Meta loss on this task batch = 4.9873e-01, Meta loss averaged over last 500 steps = 4.8227e-01, PNorm = 53.6008, GNorm = 0.3855
Meta loss on this task batch = 4.6116e-01, Meta loss averaged over last 500 steps = 4.8222e-01, PNorm = 53.6058, GNorm = 0.2993
Meta loss on this task batch = 5.2378e-01, Meta loss averaged over last 500 steps = 4.8231e-01, PNorm = 53.6116, GNorm = 0.2591
Meta loss on this task batch = 4.9737e-01, Meta loss averaged over last 500 steps = 4.8234e-01, PNorm = 53.6175, GNorm = 0.3880
Meta loss on this task batch = 4.5728e-01, Meta loss averaged over last 500 steps = 4.8229e-01, PNorm = 53.6254, GNorm = 0.3372
Meta loss on this task batch = 4.5493e-01, Meta loss averaged over last 500 steps = 4.8223e-01, PNorm = 53.6340, GNorm = 0.2577
Meta loss on this task batch = 4.8040e-01, Meta loss averaged over last 500 steps = 4.8223e-01, PNorm = 53.6437, GNorm = 0.3560
Meta loss on this task batch = 5.0032e-01, Meta loss averaged over last 500 steps = 4.8227e-01, PNorm = 53.6549, GNorm = 0.1443
Took 110.67458009719849 seconds to complete one epoch of meta training
Took 118.26779747009277 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.362961
Epoch 25
Meta loss on this task batch = 4.8673e-01, Meta loss averaged over last 500 steps = 4.8228e-01, PNorm = 53.6672, GNorm = 0.1586
Meta loss on this task batch = 4.4302e-01, Meta loss averaged over last 500 steps = 4.8220e-01, PNorm = 53.6809, GNorm = 0.0970
Meta loss on this task batch = 4.5913e-01, Meta loss averaged over last 500 steps = 4.8215e-01, PNorm = 53.6946, GNorm = 0.2827
Meta loss on this task batch = 4.8501e-01, Meta loss averaged over last 500 steps = 4.8215e-01, PNorm = 53.7086, GNorm = 0.0801
Meta loss on this task batch = 4.8898e-01, Meta loss averaged over last 500 steps = 4.8217e-01, PNorm = 53.7211, GNorm = 0.1152
Meta loss on this task batch = 4.5241e-01, Meta loss averaged over last 500 steps = 4.8211e-01, PNorm = 53.7338, GNorm = 0.2357
Meta loss on this task batch = 4.9565e-01, Meta loss averaged over last 500 steps = 4.8213e-01, PNorm = 53.7442, GNorm = 0.1502
Meta loss on this task batch = 4.7440e-01, Meta loss averaged over last 500 steps = 4.8212e-01, PNorm = 53.7547, GNorm = 0.5149
Meta loss on this task batch = 4.5357e-01, Meta loss averaged over last 500 steps = 4.8206e-01, PNorm = 53.7638, GNorm = 0.1199
Meta loss on this task batch = 4.2307e-01, Meta loss averaged over last 500 steps = 4.8194e-01, PNorm = 53.7735, GNorm = 0.0826
Meta loss on this task batch = 5.0115e-01, Meta loss averaged over last 500 steps = 4.8198e-01, PNorm = 53.7832, GNorm = 0.1817
Meta loss on this task batch = 4.5965e-01, Meta loss averaged over last 500 steps = 4.8193e-01, PNorm = 53.7924, GNorm = 0.2567
Meta loss on this task batch = 5.3704e-01, Meta loss averaged over last 500 steps = 4.8204e-01, PNorm = 53.7998, GNorm = 0.3850
Meta loss on this task batch = 4.6365e-01, Meta loss averaged over last 500 steps = 4.8201e-01, PNorm = 53.8071, GNorm = 0.2272
Meta loss on this task batch = 4.9776e-01, Meta loss averaged over last 500 steps = 4.8204e-01, PNorm = 53.8150, GNorm = 0.0902
Meta loss on this task batch = 4.8613e-01, Meta loss averaged over last 500 steps = 4.8205e-01, PNorm = 53.8225, GNorm = 0.2048
Meta loss on this task batch = 5.1652e-01, Meta loss averaged over last 500 steps = 4.8212e-01, PNorm = 53.8301, GNorm = 0.0844
Meta loss on this task batch = 4.8066e-01, Meta loss averaged over last 500 steps = 4.8211e-01, PNorm = 53.8377, GNorm = 0.1704
Meta loss on this task batch = 4.4192e-01, Meta loss averaged over last 500 steps = 4.8203e-01, PNorm = 53.8455, GNorm = 0.2865
Took 110.02492928504944 seconds to complete one epoch of meta training
Took 117.29730653762817 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.347796
Epoch 26
Meta loss on this task batch = 4.1292e-01, Meta loss averaged over last 500 steps = 4.8189e-01, PNorm = 53.8544, GNorm = 0.3442
Meta loss on this task batch = 5.0299e-01, Meta loss averaged over last 500 steps = 4.8193e-01, PNorm = 53.8629, GNorm = 0.1034
Meta loss on this task batch = 4.2747e-01, Meta loss averaged over last 500 steps = 4.8183e-01, PNorm = 53.8731, GNorm = 0.2648
Meta loss on this task batch = 4.5739e-01, Meta loss averaged over last 500 steps = 4.8178e-01, PNorm = 53.8834, GNorm = 0.1251
Meta loss on this task batch = 4.8126e-01, Meta loss averaged over last 500 steps = 4.8178e-01, PNorm = 53.8937, GNorm = 0.0849
Meta loss on this task batch = 4.2018e-01, Meta loss averaged over last 500 steps = 4.8165e-01, PNorm = 53.9047, GNorm = 0.1687
Meta loss on this task batch = 5.0484e-01, Meta loss averaged over last 500 steps = 4.8045e-01, PNorm = 53.9153, GNorm = 0.2240
Meta loss on this task batch = 4.2391e-01, Meta loss averaged over last 500 steps = 4.7979e-01, PNorm = 53.9268, GNorm = 0.1292
Meta loss on this task batch = 4.8664e-01, Meta loss averaged over last 500 steps = 4.7967e-01, PNorm = 53.9375, GNorm = 0.2220
Meta loss on this task batch = 5.2503e-01, Meta loss averaged over last 500 steps = 4.7966e-01, PNorm = 53.9471, GNorm = 0.1411
Meta loss on this task batch = 4.4407e-01, Meta loss averaged over last 500 steps = 4.7943e-01, PNorm = 53.9556, GNorm = 0.0898
Meta loss on this task batch = 4.5330e-01, Meta loss averaged over last 500 steps = 4.7931e-01, PNorm = 53.9625, GNorm = 0.1977
Meta loss on this task batch = 5.4302e-01, Meta loss averaged over last 500 steps = 4.7948e-01, PNorm = 53.9676, GNorm = 0.2747
Meta loss on this task batch = 4.5543e-01, Meta loss averaged over last 500 steps = 4.7927e-01, PNorm = 53.9741, GNorm = 0.2642
Meta loss on this task batch = 4.4320e-01, Meta loss averaged over last 500 steps = 4.7919e-01, PNorm = 53.9804, GNorm = 0.1700
Meta loss on this task batch = 5.5707e-01, Meta loss averaged over last 500 steps = 4.7932e-01, PNorm = 53.9847, GNorm = 0.1110
Meta loss on this task batch = 4.8703e-01, Meta loss averaged over last 500 steps = 4.7939e-01, PNorm = 53.9905, GNorm = 0.1051
Meta loss on this task batch = 4.3593e-01, Meta loss averaged over last 500 steps = 4.7932e-01, PNorm = 53.9982, GNorm = 0.2138
Meta loss on this task batch = 4.7130e-01, Meta loss averaged over last 500 steps = 4.7934e-01, PNorm = 54.0069, GNorm = 0.1285
Took 116.00557708740234 seconds to complete one epoch of meta training
Took 124.5552875995636 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.353905
Epoch 27
Meta loss on this task batch = 4.9529e-01, Meta loss averaged over last 500 steps = 4.7954e-01, PNorm = 54.0166, GNorm = 0.1721
Meta loss on this task batch = 4.0060e-01, Meta loss averaged over last 500 steps = 4.7947e-01, PNorm = 54.0260, GNorm = 0.2069
Meta loss on this task batch = 5.3323e-01, Meta loss averaged over last 500 steps = 4.7961e-01, PNorm = 54.0345, GNorm = 0.2334
Meta loss on this task batch = 4.1641e-01, Meta loss averaged over last 500 steps = 4.7931e-01, PNorm = 54.0441, GNorm = 0.1114
Meta loss on this task batch = 4.2102e-01, Meta loss averaged over last 500 steps = 4.7911e-01, PNorm = 54.0538, GNorm = 0.2781
Meta loss on this task batch = 5.1992e-01, Meta loss averaged over last 500 steps = 4.7909e-01, PNorm = 54.0623, GNorm = 0.0869
Meta loss on this task batch = 4.9635e-01, Meta loss averaged over last 500 steps = 4.7908e-01, PNorm = 54.0704, GNorm = 0.1432
Meta loss on this task batch = 5.1886e-01, Meta loss averaged over last 500 steps = 4.7909e-01, PNorm = 54.0774, GNorm = 0.1022
Meta loss on this task batch = 4.6113e-01, Meta loss averaged over last 500 steps = 4.7901e-01, PNorm = 54.0836, GNorm = 0.0802
Meta loss on this task batch = 4.7397e-01, Meta loss averaged over last 500 steps = 4.7910e-01, PNorm = 54.0905, GNorm = 0.3857
Meta loss on this task batch = 4.3942e-01, Meta loss averaged over last 500 steps = 4.7896e-01, PNorm = 54.0975, GNorm = 0.1194
Meta loss on this task batch = 4.3090e-01, Meta loss averaged over last 500 steps = 4.7880e-01, PNorm = 54.1054, GNorm = 0.2296
Meta loss on this task batch = 4.2664e-01, Meta loss averaged over last 500 steps = 4.7865e-01, PNorm = 54.1129, GNorm = 0.0780
Meta loss on this task batch = 4.9424e-01, Meta loss averaged over last 500 steps = 4.7863e-01, PNorm = 54.1193, GNorm = 0.1272
Meta loss on this task batch = 4.9450e-01, Meta loss averaged over last 500 steps = 4.7875e-01, PNorm = 54.1259, GNorm = 0.1101
Meta loss on this task batch = 4.9087e-01, Meta loss averaged over last 500 steps = 4.7877e-01, PNorm = 54.1325, GNorm = 0.2087
Meta loss on this task batch = 5.1709e-01, Meta loss averaged over last 500 steps = 4.7892e-01, PNorm = 54.1386, GNorm = 0.1648
Meta loss on this task batch = 4.6919e-01, Meta loss averaged over last 500 steps = 4.7888e-01, PNorm = 54.1455, GNorm = 0.2710
Meta loss on this task batch = 4.7490e-01, Meta loss averaged over last 500 steps = 4.7881e-01, PNorm = 54.1534, GNorm = 0.2479
Took 117.923335313797 seconds to complete one epoch of meta training
Took 126.19694066047668 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.363010
Epoch 28
Meta loss on this task batch = 4.9521e-01, Meta loss averaged over last 500 steps = 4.7888e-01, PNorm = 54.1615, GNorm = 0.3380
Meta loss on this task batch = 4.8620e-01, Meta loss averaged over last 500 steps = 4.7889e-01, PNorm = 54.1700, GNorm = 0.1084
Meta loss on this task batch = 4.8328e-01, Meta loss averaged over last 500 steps = 4.7882e-01, PNorm = 54.1798, GNorm = 0.1050
Meta loss on this task batch = 4.9226e-01, Meta loss averaged over last 500 steps = 4.7884e-01, PNorm = 54.1908, GNorm = 0.3699
Meta loss on this task batch = 4.1088e-01, Meta loss averaged over last 500 steps = 4.7882e-01, PNorm = 54.2031, GNorm = 0.0876
Meta loss on this task batch = 4.7905e-01, Meta loss averaged over last 500 steps = 4.7876e-01, PNorm = 54.2158, GNorm = 0.1078
Meta loss on this task batch = 4.4040e-01, Meta loss averaged over last 500 steps = 4.7876e-01, PNorm = 54.2293, GNorm = 0.1086
Meta loss on this task batch = 4.4596e-01, Meta loss averaged over last 500 steps = 4.7873e-01, PNorm = 54.2432, GNorm = 0.1606
Meta loss on this task batch = 5.1482e-01, Meta loss averaged over last 500 steps = 4.7884e-01, PNorm = 54.2559, GNorm = 0.3299
Meta loss on this task batch = 4.0274e-01, Meta loss averaged over last 500 steps = 4.7865e-01, PNorm = 54.2673, GNorm = 0.2976
Meta loss on this task batch = 4.5406e-01, Meta loss averaged over last 500 steps = 4.7854e-01, PNorm = 54.2771, GNorm = 0.3347
Meta loss on this task batch = 4.7790e-01, Meta loss averaged over last 500 steps = 4.7863e-01, PNorm = 54.2855, GNorm = 0.4895
Meta loss on this task batch = 4.9472e-01, Meta loss averaged over last 500 steps = 4.7875e-01, PNorm = 54.2944, GNorm = 0.1169
Meta loss on this task batch = 4.8113e-01, Meta loss averaged over last 500 steps = 4.7872e-01, PNorm = 54.3016, GNorm = 0.1524
Meta loss on this task batch = 4.8157e-01, Meta loss averaged over last 500 steps = 4.7873e-01, PNorm = 54.3101, GNorm = 0.1146
Meta loss on this task batch = 4.5028e-01, Meta loss averaged over last 500 steps = 4.7860e-01, PNorm = 54.3188, GNorm = 0.0880
Meta loss on this task batch = 4.7809e-01, Meta loss averaged over last 500 steps = 4.7848e-01, PNorm = 54.3263, GNorm = 0.1386
Meta loss on this task batch = 5.4477e-01, Meta loss averaged over last 500 steps = 4.7857e-01, PNorm = 54.3335, GNorm = 0.1078
Meta loss on this task batch = 4.9592e-01, Meta loss averaged over last 500 steps = 4.7855e-01, PNorm = 54.3416, GNorm = 0.1682
Took 125.37186360359192 seconds to complete one epoch of meta training
Took 133.19286155700684 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.358498
Epoch 29
Meta loss on this task batch = 4.6885e-01, Meta loss averaged over last 500 steps = 4.7852e-01, PNorm = 54.3501, GNorm = 0.1142
Meta loss on this task batch = 5.5403e-01, Meta loss averaged over last 500 steps = 4.7863e-01, PNorm = 54.3598, GNorm = 0.2435
Meta loss on this task batch = 4.7655e-01, Meta loss averaged over last 500 steps = 4.7858e-01, PNorm = 54.3706, GNorm = 0.2304
Meta loss on this task batch = 5.0017e-01, Meta loss averaged over last 500 steps = 4.7867e-01, PNorm = 54.3807, GNorm = 0.0827
Meta loss on this task batch = 4.8568e-01, Meta loss averaged over last 500 steps = 4.7866e-01, PNorm = 54.3915, GNorm = 0.2093
Meta loss on this task batch = 4.4151e-01, Meta loss averaged over last 500 steps = 4.7849e-01, PNorm = 54.4021, GNorm = 0.1540
Meta loss on this task batch = 4.8235e-01, Meta loss averaged over last 500 steps = 4.7847e-01, PNorm = 54.4126, GNorm = 0.1447
Meta loss on this task batch = 4.7686e-01, Meta loss averaged over last 500 steps = 4.7843e-01, PNorm = 54.4243, GNorm = 0.0980
Meta loss on this task batch = 5.1148e-01, Meta loss averaged over last 500 steps = 4.7847e-01, PNorm = 54.4356, GNorm = 0.1000
Meta loss on this task batch = 4.6584e-01, Meta loss averaged over last 500 steps = 4.7844e-01, PNorm = 54.4467, GNorm = 0.1042
Meta loss on this task batch = 4.2472e-01, Meta loss averaged over last 500 steps = 4.7831e-01, PNorm = 54.4587, GNorm = 0.1393
Meta loss on this task batch = 4.3584e-01, Meta loss averaged over last 500 steps = 4.7818e-01, PNorm = 54.4699, GNorm = 0.0831
Meta loss on this task batch = 4.3677e-01, Meta loss averaged over last 500 steps = 4.7796e-01, PNorm = 54.4819, GNorm = 0.2340
Meta loss on this task batch = 4.7602e-01, Meta loss averaged over last 500 steps = 4.7795e-01, PNorm = 54.4939, GNorm = 0.2197
Meta loss on this task batch = 4.6970e-01, Meta loss averaged over last 500 steps = 4.7805e-01, PNorm = 54.5050, GNorm = 0.4665
Meta loss on this task batch = 4.3170e-01, Meta loss averaged over last 500 steps = 4.7799e-01, PNorm = 54.5151, GNorm = 0.1841
Meta loss on this task batch = 4.3375e-01, Meta loss averaged over last 500 steps = 4.7797e-01, PNorm = 54.5261, GNorm = 0.3034
Meta loss on this task batch = 4.8016e-01, Meta loss averaged over last 500 steps = 4.7778e-01, PNorm = 54.5372, GNorm = 0.1083
Meta loss on this task batch = 3.9517e-01, Meta loss averaged over last 500 steps = 4.7770e-01, PNorm = 54.5490, GNorm = 0.0836
Took 114.7700731754303 seconds to complete one epoch of meta training
Took 123.4204216003418 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.374223
Found better MAML checkpoint after meta validation, saving now
Epoch 30
Meta loss on this task batch = 4.4879e-01, Meta loss averaged over last 500 steps = 4.7764e-01, PNorm = 54.5610, GNorm = 0.1014
Meta loss on this task batch = 4.9762e-01, Meta loss averaged over last 500 steps = 4.7789e-01, PNorm = 54.5716, GNorm = 0.1601
Meta loss on this task batch = 5.2393e-01, Meta loss averaged over last 500 steps = 4.7805e-01, PNorm = 54.5828, GNorm = 0.3601
Meta loss on this task batch = 3.9146e-01, Meta loss averaged over last 500 steps = 4.7782e-01, PNorm = 54.5936, GNorm = 0.2110
Meta loss on this task batch = 5.1282e-01, Meta loss averaged over last 500 steps = 4.7771e-01, PNorm = 54.6013, GNorm = 0.2352
Meta loss on this task batch = 5.4400e-01, Meta loss averaged over last 500 steps = 4.7756e-01, PNorm = 54.6065, GNorm = 0.2281
Meta loss on this task batch = 4.2316e-01, Meta loss averaged over last 500 steps = 4.7747e-01, PNorm = 54.6144, GNorm = 0.0981
Meta loss on this task batch = 5.1299e-01, Meta loss averaged over last 500 steps = 4.7751e-01, PNorm = 54.6215, GNorm = 0.4425
Meta loss on this task batch = 4.5939e-01, Meta loss averaged over last 500 steps = 4.7747e-01, PNorm = 54.6286, GNorm = 0.1892
Meta loss on this task batch = 4.9193e-01, Meta loss averaged over last 500 steps = 4.7753e-01, PNorm = 54.6373, GNorm = 0.3907
Meta loss on this task batch = 3.4728e-01, Meta loss averaged over last 500 steps = 4.7723e-01, PNorm = 54.6495, GNorm = 0.2830
Meta loss on this task batch = 5.2148e-01, Meta loss averaged over last 500 steps = 4.7741e-01, PNorm = 54.6627, GNorm = 0.1649
Meta loss on this task batch = 4.9208e-01, Meta loss averaged over last 500 steps = 4.7751e-01, PNorm = 54.6761, GNorm = 0.1347
Meta loss on this task batch = 4.1656e-01, Meta loss averaged over last 500 steps = 4.7741e-01, PNorm = 54.6915, GNorm = 0.1035
Meta loss on this task batch = 4.2739e-01, Meta loss averaged over last 500 steps = 4.7746e-01, PNorm = 54.7080, GNorm = 0.2422
Meta loss on this task batch = 4.7558e-01, Meta loss averaged over last 500 steps = 4.7753e-01, PNorm = 54.7226, GNorm = 0.1341
Meta loss on this task batch = 4.6821e-01, Meta loss averaged over last 500 steps = 4.7756e-01, PNorm = 54.7361, GNorm = 0.2817
Meta loss on this task batch = 4.7877e-01, Meta loss averaged over last 500 steps = 4.7757e-01, PNorm = 54.7495, GNorm = 0.1461
Meta loss on this task batch = 5.0358e-01, Meta loss averaged over last 500 steps = 4.7770e-01, PNorm = 54.7605, GNorm = 0.1620
Took 115.6399359703064 seconds to complete one epoch of meta training
Took 124.69415163993835 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.351871
Epoch 31
Meta loss on this task batch = 4.8264e-01, Meta loss averaged over last 500 steps = 4.7756e-01, PNorm = 54.7708, GNorm = 0.5154
Meta loss on this task batch = 4.2950e-01, Meta loss averaged over last 500 steps = 4.7729e-01, PNorm = 54.7808, GNorm = 0.2238
Meta loss on this task batch = 5.1387e-01, Meta loss averaged over last 500 steps = 4.7717e-01, PNorm = 54.7914, GNorm = 0.1201
Meta loss on this task batch = 4.3104e-01, Meta loss averaged over last 500 steps = 4.7706e-01, PNorm = 54.8020, GNorm = 0.1204
Meta loss on this task batch = 5.0625e-01, Meta loss averaged over last 500 steps = 4.7706e-01, PNorm = 54.8123, GNorm = 0.3305
Meta loss on this task batch = 4.7653e-01, Meta loss averaged over last 500 steps = 4.7693e-01, PNorm = 54.8247, GNorm = 0.1991
Meta loss on this task batch = 4.8995e-01, Meta loss averaged over last 500 steps = 4.7696e-01, PNorm = 54.8362, GNorm = 0.1568
Meta loss on this task batch = 3.8501e-01, Meta loss averaged over last 500 steps = 4.7670e-01, PNorm = 54.8489, GNorm = 0.0913
Meta loss on this task batch = 4.4379e-01, Meta loss averaged over last 500 steps = 4.7653e-01, PNorm = 54.8630, GNorm = 0.1893
Meta loss on this task batch = 4.6460e-01, Meta loss averaged over last 500 steps = 4.7644e-01, PNorm = 54.8758, GNorm = 0.1850
Meta loss on this task batch = 4.8531e-01, Meta loss averaged over last 500 steps = 4.7646e-01, PNorm = 54.8879, GNorm = 0.1297
Meta loss on this task batch = 4.9974e-01, Meta loss averaged over last 500 steps = 4.7644e-01, PNorm = 54.8968, GNorm = 0.2135
Meta loss on this task batch = 4.4530e-01, Meta loss averaged over last 500 steps = 4.7640e-01, PNorm = 54.9052, GNorm = 0.1416
Meta loss on this task batch = 5.7013e-01, Meta loss averaged over last 500 steps = 4.7658e-01, PNorm = 54.9114, GNorm = 0.3800
Meta loss on this task batch = 3.7667e-01, Meta loss averaged over last 500 steps = 4.7633e-01, PNorm = 54.9195, GNorm = 0.0920
Meta loss on this task batch = 5.2918e-01, Meta loss averaged over last 500 steps = 4.7645e-01, PNorm = 54.9281, GNorm = 0.2125
Meta loss on this task batch = 4.6076e-01, Meta loss averaged over last 500 steps = 4.7629e-01, PNorm = 54.9370, GNorm = 0.1395
Meta loss on this task batch = 5.1796e-01, Meta loss averaged over last 500 steps = 4.7624e-01, PNorm = 54.9454, GNorm = 0.2034
Meta loss on this task batch = 4.2233e-01, Meta loss averaged over last 500 steps = 4.7608e-01, PNorm = 54.9540, GNorm = 0.0675
Took 114.12212944030762 seconds to complete one epoch of meta training
Took 121.52131795883179 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.394470
Found better MAML checkpoint after meta validation, saving now
Epoch 32
Meta loss on this task batch = 4.3909e-01, Meta loss averaged over last 500 steps = 4.7615e-01, PNorm = 54.9637, GNorm = 0.0977
Meta loss on this task batch = 5.1397e-01, Meta loss averaged over last 500 steps = 4.7619e-01, PNorm = 54.9746, GNorm = 0.1891
Meta loss on this task batch = 4.9591e-01, Meta loss averaged over last 500 steps = 4.7644e-01, PNorm = 54.9846, GNorm = 0.2653
Meta loss on this task batch = 4.9316e-01, Meta loss averaged over last 500 steps = 4.7647e-01, PNorm = 54.9955, GNorm = 0.0737
Meta loss on this task batch = 5.0252e-01, Meta loss averaged over last 500 steps = 4.7642e-01, PNorm = 55.0056, GNorm = 0.5555
Meta loss on this task batch = 5.0933e-01, Meta loss averaged over last 500 steps = 4.7658e-01, PNorm = 55.0169, GNorm = 0.1523
Meta loss on this task batch = 4.9404e-01, Meta loss averaged over last 500 steps = 4.7661e-01, PNorm = 55.0290, GNorm = 0.0892
Meta loss on this task batch = 4.4021e-01, Meta loss averaged over last 500 steps = 4.7649e-01, PNorm = 55.0425, GNorm = 0.0918
Meta loss on this task batch = 4.3150e-01, Meta loss averaged over last 500 steps = 4.7640e-01, PNorm = 55.0569, GNorm = 0.1035
Meta loss on this task batch = 4.7073e-01, Meta loss averaged over last 500 steps = 4.7638e-01, PNorm = 55.0706, GNorm = 0.1016
Meta loss on this task batch = 4.3709e-01, Meta loss averaged over last 500 steps = 4.7617e-01, PNorm = 55.0836, GNorm = 0.4281
Meta loss on this task batch = 4.6535e-01, Meta loss averaged over last 500 steps = 4.7619e-01, PNorm = 55.0965, GNorm = 0.1477
Meta loss on this task batch = 4.5721e-01, Meta loss averaged over last 500 steps = 4.7619e-01, PNorm = 55.1080, GNorm = 0.2299
Meta loss on this task batch = 4.2772e-01, Meta loss averaged over last 500 steps = 4.7609e-01, PNorm = 55.1189, GNorm = 0.0831
Meta loss on this task batch = 4.6637e-01, Meta loss averaged over last 500 steps = 4.7599e-01, PNorm = 55.1295, GNorm = 0.1218
Meta loss on this task batch = 4.9823e-01, Meta loss averaged over last 500 steps = 4.7602e-01, PNorm = 55.1403, GNorm = 0.3084
Meta loss on this task batch = 4.1901e-01, Meta loss averaged over last 500 steps = 4.7594e-01, PNorm = 55.1526, GNorm = 0.2221
Meta loss on this task batch = 4.4520e-01, Meta loss averaged over last 500 steps = 4.7585e-01, PNorm = 55.1668, GNorm = 0.1721
Meta loss on this task batch = 4.6449e-01, Meta loss averaged over last 500 steps = 4.7588e-01, PNorm = 55.1794, GNorm = 0.1184
Took 109.28587341308594 seconds to complete one epoch of meta training
Took 116.59544253349304 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.373154
Epoch 33
Meta loss on this task batch = 5.0623e-01, Meta loss averaged over last 500 steps = 4.7586e-01, PNorm = 55.1923, GNorm = 0.3838
Meta loss on this task batch = 4.9466e-01, Meta loss averaged over last 500 steps = 4.7596e-01, PNorm = 55.2032, GNorm = 0.1650
Meta loss on this task batch = 4.6949e-01, Meta loss averaged over last 500 steps = 4.7601e-01, PNorm = 55.2127, GNorm = 0.1031
Meta loss on this task batch = 4.1044e-01, Meta loss averaged over last 500 steps = 4.7587e-01, PNorm = 55.2218, GNorm = 0.1406
Meta loss on this task batch = 4.8611e-01, Meta loss averaged over last 500 steps = 4.7581e-01, PNorm = 55.2301, GNorm = 0.3080
Meta loss on this task batch = 4.2833e-01, Meta loss averaged over last 500 steps = 4.7567e-01, PNorm = 55.2379, GNorm = 0.0825
Meta loss on this task batch = 5.2595e-01, Meta loss averaged over last 500 steps = 4.7582e-01, PNorm = 55.2455, GNorm = 0.0951
Meta loss on this task batch = 4.5142e-01, Meta loss averaged over last 500 steps = 4.7580e-01, PNorm = 55.2551, GNorm = 0.2019
Meta loss on this task batch = 4.1166e-01, Meta loss averaged over last 500 steps = 4.7560e-01, PNorm = 55.2670, GNorm = 0.2466
Meta loss on this task batch = 4.4781e-01, Meta loss averaged over last 500 steps = 4.7555e-01, PNorm = 55.2793, GNorm = 0.2321
Meta loss on this task batch = 4.7767e-01, Meta loss averaged over last 500 steps = 4.7564e-01, PNorm = 55.2911, GNorm = 0.0877
Meta loss on this task batch = 4.5937e-01, Meta loss averaged over last 500 steps = 4.7562e-01, PNorm = 55.3045, GNorm = 0.3490
Meta loss on this task batch = 4.8167e-01, Meta loss averaged over last 500 steps = 4.7557e-01, PNorm = 55.3183, GNorm = 0.1675
Meta loss on this task batch = 4.3282e-01, Meta loss averaged over last 500 steps = 4.7560e-01, PNorm = 55.3318, GNorm = 0.2305
Meta loss on this task batch = 4.6376e-01, Meta loss averaged over last 500 steps = 4.7569e-01, PNorm = 55.3461, GNorm = 0.0821
Meta loss on this task batch = 4.0531e-01, Meta loss averaged over last 500 steps = 4.7535e-01, PNorm = 55.3596, GNorm = 0.2290
Meta loss on this task batch = 5.3710e-01, Meta loss averaged over last 500 steps = 4.7545e-01, PNorm = 55.3709, GNorm = 0.1389
Meta loss on this task batch = 5.1240e-01, Meta loss averaged over last 500 steps = 4.7554e-01, PNorm = 55.3814, GNorm = 0.1255
Meta loss on this task batch = 4.4074e-01, Meta loss averaged over last 500 steps = 4.7534e-01, PNorm = 55.3912, GNorm = 0.1065
Took 115.08510565757751 seconds to complete one epoch of meta training
Took 122.9535698890686 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.384197
Epoch 34
Meta loss on this task batch = 4.5346e-01, Meta loss averaged over last 500 steps = 4.7547e-01, PNorm = 55.4028, GNorm = 0.2992
Meta loss on this task batch = 4.9241e-01, Meta loss averaged over last 500 steps = 4.7532e-01, PNorm = 55.4148, GNorm = 0.0863
Meta loss on this task batch = 5.3897e-01, Meta loss averaged over last 500 steps = 4.7546e-01, PNorm = 55.4263, GNorm = 0.0836
Meta loss on this task batch = 4.7244e-01, Meta loss averaged over last 500 steps = 4.7534e-01, PNorm = 55.4371, GNorm = 0.1597
Meta loss on this task batch = 4.6781e-01, Meta loss averaged over last 500 steps = 4.7539e-01, PNorm = 55.4482, GNorm = 0.0869
Meta loss on this task batch = 4.8323e-01, Meta loss averaged over last 500 steps = 4.7532e-01, PNorm = 55.4601, GNorm = 0.2263
Meta loss on this task batch = 4.6801e-01, Meta loss averaged over last 500 steps = 4.7534e-01, PNorm = 55.4713, GNorm = 0.1336
Meta loss on this task batch = 4.5154e-01, Meta loss averaged over last 500 steps = 4.7517e-01, PNorm = 55.4833, GNorm = 0.1395
Meta loss on this task batch = 4.9127e-01, Meta loss averaged over last 500 steps = 4.7519e-01, PNorm = 55.4945, GNorm = 0.0972
Meta loss on this task batch = 4.5519e-01, Meta loss averaged over last 500 steps = 4.7521e-01, PNorm = 55.5063, GNorm = 0.0903
Meta loss on this task batch = 5.0247e-01, Meta loss averaged over last 500 steps = 4.7518e-01, PNorm = 55.5166, GNorm = 0.1110
Meta loss on this task batch = 4.5550e-01, Meta loss averaged over last 500 steps = 4.7515e-01, PNorm = 55.5273, GNorm = 0.1181
Meta loss on this task batch = 4.7354e-01, Meta loss averaged over last 500 steps = 4.7520e-01, PNorm = 55.5392, GNorm = 0.2226
Meta loss on this task batch = 4.0729e-01, Meta loss averaged over last 500 steps = 4.7490e-01, PNorm = 55.5515, GNorm = 0.0760
Meta loss on this task batch = 4.2206e-01, Meta loss averaged over last 500 steps = 4.7477e-01, PNorm = 55.5628, GNorm = 0.1809
Meta loss on this task batch = 3.8943e-01, Meta loss averaged over last 500 steps = 4.7456e-01, PNorm = 55.5755, GNorm = 0.1062
Meta loss on this task batch = 4.6209e-01, Meta loss averaged over last 500 steps = 4.7449e-01, PNorm = 55.5868, GNorm = 0.1563
Meta loss on this task batch = 4.2438e-01, Meta loss averaged over last 500 steps = 4.7449e-01, PNorm = 55.5985, GNorm = 0.0958
Meta loss on this task batch = 4.7583e-01, Meta loss averaged over last 500 steps = 4.7441e-01, PNorm = 55.6087, GNorm = 0.4069
Took 110.17098641395569 seconds to complete one epoch of meta training
Took 117.66280555725098 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.410439
Found better MAML checkpoint after meta validation, saving now
Epoch 35
Meta loss on this task batch = 4.6461e-01, Meta loss averaged over last 500 steps = 4.7450e-01, PNorm = 55.6186, GNorm = 0.2231
Meta loss on this task batch = 5.0753e-01, Meta loss averaged over last 500 steps = 4.7449e-01, PNorm = 55.6283, GNorm = 0.1390
Meta loss on this task batch = 4.6537e-01, Meta loss averaged over last 500 steps = 4.7453e-01, PNorm = 55.6367, GNorm = 0.1318
Meta loss on this task batch = 4.7964e-01, Meta loss averaged over last 500 steps = 4.7455e-01, PNorm = 55.6454, GNorm = 0.0948
Meta loss on this task batch = 4.0405e-01, Meta loss averaged over last 500 steps = 4.7429e-01, PNorm = 55.6567, GNorm = 0.1845
Meta loss on this task batch = 4.5014e-01, Meta loss averaged over last 500 steps = 4.7432e-01, PNorm = 55.6698, GNorm = 0.3028
Meta loss on this task batch = 4.4956e-01, Meta loss averaged over last 500 steps = 4.7426e-01, PNorm = 55.6837, GNorm = 0.1737
Meta loss on this task batch = 4.3129e-01, Meta loss averaged over last 500 steps = 4.7409e-01, PNorm = 55.6982, GNorm = 0.1275
Meta loss on this task batch = 5.0058e-01, Meta loss averaged over last 500 steps = 4.7409e-01, PNorm = 55.7110, GNorm = 0.1350
Meta loss on this task batch = 4.5474e-01, Meta loss averaged over last 500 steps = 4.7406e-01, PNorm = 55.7249, GNorm = 0.2966
Meta loss on this task batch = 4.8373e-01, Meta loss averaged over last 500 steps = 4.7402e-01, PNorm = 55.7383, GNorm = 0.0914
Meta loss on this task batch = 4.2527e-01, Meta loss averaged over last 500 steps = 4.7388e-01, PNorm = 55.7505, GNorm = 0.1292
Meta loss on this task batch = 4.8943e-01, Meta loss averaged over last 500 steps = 4.7395e-01, PNorm = 55.7613, GNorm = 0.1519
Meta loss on this task batch = 4.1200e-01, Meta loss averaged over last 500 steps = 4.7375e-01, PNorm = 55.7716, GNorm = 0.1569
Meta loss on this task batch = 4.5474e-01, Meta loss averaged over last 500 steps = 4.7383e-01, PNorm = 55.7817, GNorm = 0.1147
Meta loss on this task batch = 4.2516e-01, Meta loss averaged over last 500 steps = 4.7361e-01, PNorm = 55.7927, GNorm = 0.1187
Meta loss on this task batch = 4.6698e-01, Meta loss averaged over last 500 steps = 4.7349e-01, PNorm = 55.8044, GNorm = 0.1651
Meta loss on this task batch = 4.3457e-01, Meta loss averaged over last 500 steps = 4.7345e-01, PNorm = 55.8157, GNorm = 0.0830
Meta loss on this task batch = 4.7585e-01, Meta loss averaged over last 500 steps = 4.7346e-01, PNorm = 55.8266, GNorm = 0.0987
Took 110.79605984687805 seconds to complete one epoch of meta training
Took 118.35784816741943 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.416069
Found better MAML checkpoint after meta validation, saving now
Epoch 36
Meta loss on this task batch = 4.5112e-01, Meta loss averaged over last 500 steps = 4.7350e-01, PNorm = 55.8357, GNorm = 0.3762
Meta loss on this task batch = 4.5543e-01, Meta loss averaged over last 500 steps = 4.7351e-01, PNorm = 55.8467, GNorm = 0.1036
Meta loss on this task batch = 4.1154e-01, Meta loss averaged over last 500 steps = 4.7333e-01, PNorm = 55.8586, GNorm = 0.3100
Meta loss on this task batch = 4.6119e-01, Meta loss averaged over last 500 steps = 4.7327e-01, PNorm = 55.8710, GNorm = 0.2288
Meta loss on this task batch = 4.3595e-01, Meta loss averaged over last 500 steps = 4.7325e-01, PNorm = 55.8832, GNorm = 0.1562
Meta loss on this task batch = 4.7665e-01, Meta loss averaged over last 500 steps = 4.7332e-01, PNorm = 55.8952, GNorm = 0.1481
Meta loss on this task batch = 4.7055e-01, Meta loss averaged over last 500 steps = 4.7334e-01, PNorm = 55.9065, GNorm = 0.1766
Meta loss on this task batch = 4.3737e-01, Meta loss averaged over last 500 steps = 4.7317e-01, PNorm = 55.9180, GNorm = 0.2235
Meta loss on this task batch = 4.7040e-01, Meta loss averaged over last 500 steps = 4.7318e-01, PNorm = 55.9292, GNorm = 0.1573
Meta loss on this task batch = 5.2206e-01, Meta loss averaged over last 500 steps = 4.7329e-01, PNorm = 55.9419, GNorm = 0.5151
Meta loss on this task batch = 5.0636e-01, Meta loss averaged over last 500 steps = 4.7325e-01, PNorm = 55.9529, GNorm = 0.1461
Meta loss on this task batch = 5.0948e-01, Meta loss averaged over last 500 steps = 4.7330e-01, PNorm = 55.9635, GNorm = 0.0861
Meta loss on this task batch = 3.9175e-01, Meta loss averaged over last 500 steps = 4.7327e-01, PNorm = 55.9756, GNorm = 0.2002
Meta loss on this task batch = 4.7228e-01, Meta loss averaged over last 500 steps = 4.7322e-01, PNorm = 55.9882, GNorm = 0.2134
Meta loss on this task batch = 4.2243e-01, Meta loss averaged over last 500 steps = 4.7312e-01, PNorm = 56.0017, GNorm = 0.1743
Meta loss on this task batch = 4.7349e-01, Meta loss averaged over last 500 steps = 4.7315e-01, PNorm = 56.0163, GNorm = 0.2193
Meta loss on this task batch = 4.8676e-01, Meta loss averaged over last 500 steps = 4.7309e-01, PNorm = 56.0304, GNorm = 0.1566
Meta loss on this task batch = 4.5325e-01, Meta loss averaged over last 500 steps = 4.7317e-01, PNorm = 56.0445, GNorm = 0.1868
Meta loss on this task batch = 3.6157e-01, Meta loss averaged over last 500 steps = 4.7284e-01, PNorm = 56.0580, GNorm = 0.1331
Took 111.65010738372803 seconds to complete one epoch of meta training
Took 119.94263887405396 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.414529
Epoch 37
Meta loss on this task batch = 4.2633e-01, Meta loss averaged over last 500 steps = 4.7267e-01, PNorm = 56.0724, GNorm = 0.0971
Meta loss on this task batch = 4.2673e-01, Meta loss averaged over last 500 steps = 4.7267e-01, PNorm = 56.0868, GNorm = 0.0903
Meta loss on this task batch = 4.5465e-01, Meta loss averaged over last 500 steps = 4.7244e-01, PNorm = 56.1005, GNorm = 0.1482
Meta loss on this task batch = 4.4440e-01, Meta loss averaged over last 500 steps = 4.7238e-01, PNorm = 56.1132, GNorm = 0.1771
Meta loss on this task batch = 4.3572e-01, Meta loss averaged over last 500 steps = 4.7225e-01, PNorm = 56.1253, GNorm = 0.0973
Meta loss on this task batch = 4.1537e-01, Meta loss averaged over last 500 steps = 4.7206e-01, PNorm = 56.1384, GNorm = 0.2856
Meta loss on this task batch = 4.8955e-01, Meta loss averaged over last 500 steps = 4.7210e-01, PNorm = 56.1511, GNorm = 0.1201
Meta loss on this task batch = 4.2749e-01, Meta loss averaged over last 500 steps = 4.7198e-01, PNorm = 56.1638, GNorm = 0.1001
Meta loss on this task batch = 4.2887e-01, Meta loss averaged over last 500 steps = 4.7192e-01, PNorm = 56.1765, GNorm = 0.0984
Meta loss on this task batch = 4.6497e-01, Meta loss averaged over last 500 steps = 4.7183e-01, PNorm = 56.1875, GNorm = 0.2824
Meta loss on this task batch = 3.7305e-01, Meta loss averaged over last 500 steps = 4.7153e-01, PNorm = 56.1978, GNorm = 0.1288
Meta loss on this task batch = 5.3593e-01, Meta loss averaged over last 500 steps = 4.7172e-01, PNorm = 56.2059, GNorm = 0.4848
Meta loss on this task batch = 5.5150e-01, Meta loss averaged over last 500 steps = 4.7191e-01, PNorm = 56.2123, GNorm = 0.5766
Meta loss on this task batch = 4.8948e-01, Meta loss averaged over last 500 steps = 4.7182e-01, PNorm = 56.2203, GNorm = 0.1483
Meta loss on this task batch = 4.6223e-01, Meta loss averaged over last 500 steps = 4.7184e-01, PNorm = 56.2309, GNorm = 0.0913
Meta loss on this task batch = 4.2961e-01, Meta loss averaged over last 500 steps = 4.7168e-01, PNorm = 56.2440, GNorm = 0.1580
Meta loss on this task batch = 4.9784e-01, Meta loss averaged over last 500 steps = 4.7178e-01, PNorm = 56.2587, GNorm = 0.1113
Meta loss on this task batch = 4.6224e-01, Meta loss averaged over last 500 steps = 4.7165e-01, PNorm = 56.2737, GNorm = 0.1001
Meta loss on this task batch = 5.1412e-01, Meta loss averaged over last 500 steps = 4.7176e-01, PNorm = 56.2897, GNorm = 0.1355
Took 113.25520205497742 seconds to complete one epoch of meta training
Took 120.79569911956787 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.407174
Epoch 38
Meta loss on this task batch = 5.4159e-01, Meta loss averaged over last 500 steps = 4.7184e-01, PNorm = 56.3046, GNorm = 0.1465
Meta loss on this task batch = 4.4397e-01, Meta loss averaged over last 500 steps = 4.7178e-01, PNorm = 56.3188, GNorm = 0.1504
Meta loss on this task batch = 4.6105e-01, Meta loss averaged over last 500 steps = 4.7180e-01, PNorm = 56.3317, GNorm = 0.0889
Meta loss on this task batch = 4.7170e-01, Meta loss averaged over last 500 steps = 4.7178e-01, PNorm = 56.3445, GNorm = 0.1283
Meta loss on this task batch = 4.4137e-01, Meta loss averaged over last 500 steps = 4.7177e-01, PNorm = 56.3591, GNorm = 0.1666
Meta loss on this task batch = 4.6348e-01, Meta loss averaged over last 500 steps = 4.7174e-01, PNorm = 56.3756, GNorm = 0.3357
Meta loss on this task batch = 4.5013e-01, Meta loss averaged over last 500 steps = 4.7165e-01, PNorm = 56.3918, GNorm = 0.2271
Meta loss on this task batch = 4.8389e-01, Meta loss averaged over last 500 steps = 4.7159e-01, PNorm = 56.4092, GNorm = 0.3210
Meta loss on this task batch = 5.2280e-01, Meta loss averaged over last 500 steps = 4.7166e-01, PNorm = 56.4265, GNorm = 0.2535
Meta loss on this task batch = 5.0488e-01, Meta loss averaged over last 500 steps = 4.7170e-01, PNorm = 56.4425, GNorm = 0.2836
Meta loss on this task batch = 4.0877e-01, Meta loss averaged over last 500 steps = 4.7168e-01, PNorm = 56.4593, GNorm = 0.1567
Meta loss on this task batch = 4.2664e-01, Meta loss averaged over last 500 steps = 4.7139e-01, PNorm = 56.4780, GNorm = 0.1533
Meta loss on this task batch = 4.7096e-01, Meta loss averaged over last 500 steps = 4.7121e-01, PNorm = 56.4957, GNorm = 0.1104
Meta loss on this task batch = 4.2058e-01, Meta loss averaged over last 500 steps = 4.7097e-01, PNorm = 56.5142, GNorm = 0.1291
Meta loss on this task batch = 5.0365e-01, Meta loss averaged over last 500 steps = 4.7116e-01, PNorm = 56.5315, GNorm = 0.1756
Meta loss on this task batch = 4.7591e-01, Meta loss averaged over last 500 steps = 4.7127e-01, PNorm = 56.5474, GNorm = 0.3228
Meta loss on this task batch = 4.6675e-01, Meta loss averaged over last 500 steps = 4.7127e-01, PNorm = 56.5612, GNorm = 0.1297
Meta loss on this task batch = 4.6877e-01, Meta loss averaged over last 500 steps = 4.7127e-01, PNorm = 56.5737, GNorm = 0.3794
Meta loss on this task batch = 4.6368e-01, Meta loss averaged over last 500 steps = 4.7121e-01, PNorm = 56.5850, GNorm = 0.1703
Took 111.12428140640259 seconds to complete one epoch of meta training
Took 119.4261212348938 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.367448
Epoch 39
Meta loss on this task batch = 4.3564e-01, Meta loss averaged over last 500 steps = 4.7113e-01, PNorm = 56.5966, GNorm = 0.4116
Meta loss on this task batch = 5.1099e-01, Meta loss averaged over last 500 steps = 4.7142e-01, PNorm = 56.6068, GNorm = 0.1915
Meta loss on this task batch = 4.3048e-01, Meta loss averaged over last 500 steps = 4.7120e-01, PNorm = 56.6174, GNorm = 0.1210
Meta loss on this task batch = 4.8189e-01, Meta loss averaged over last 500 steps = 4.7116e-01, PNorm = 56.6279, GNorm = 0.1138
Meta loss on this task batch = 4.5752e-01, Meta loss averaged over last 500 steps = 4.7114e-01, PNorm = 56.6374, GNorm = 0.1425
Meta loss on this task batch = 5.0748e-01, Meta loss averaged over last 500 steps = 4.7133e-01, PNorm = 56.6477, GNorm = 0.3175
Meta loss on this task batch = 4.5595e-01, Meta loss averaged over last 500 steps = 4.7135e-01, PNorm = 56.6593, GNorm = 0.1881
Meta loss on this task batch = 4.6523e-01, Meta loss averaged over last 500 steps = 4.7130e-01, PNorm = 56.6709, GNorm = 0.1338
Meta loss on this task batch = 4.5829e-01, Meta loss averaged over last 500 steps = 4.7138e-01, PNorm = 56.6846, GNorm = 0.2560
Meta loss on this task batch = 5.2513e-01, Meta loss averaged over last 500 steps = 4.7145e-01, PNorm = 56.6981, GNorm = 0.1282
Meta loss on this task batch = 3.6906e-01, Meta loss averaged over last 500 steps = 4.7123e-01, PNorm = 56.7139, GNorm = 0.0897
Meta loss on this task batch = 4.9727e-01, Meta loss averaged over last 500 steps = 4.7120e-01, PNorm = 56.7281, GNorm = 0.1594
Meta loss on this task batch = 4.3846e-01, Meta loss averaged over last 500 steps = 4.7120e-01, PNorm = 56.7417, GNorm = 0.0841
Meta loss on this task batch = 4.3077e-01, Meta loss averaged over last 500 steps = 4.7112e-01, PNorm = 56.7564, GNorm = 0.3112
Meta loss on this task batch = 4.8735e-01, Meta loss averaged over last 500 steps = 4.7115e-01, PNorm = 56.7710, GNorm = 0.2227
Meta loss on this task batch = 4.7587e-01, Meta loss averaged over last 500 steps = 4.7112e-01, PNorm = 56.7849, GNorm = 0.1568
Meta loss on this task batch = 4.9608e-01, Meta loss averaged over last 500 steps = 4.7106e-01, PNorm = 56.7973, GNorm = 0.2193
Meta loss on this task batch = 4.5671e-01, Meta loss averaged over last 500 steps = 4.7099e-01, PNorm = 56.8071, GNorm = 0.2867
Meta loss on this task batch = 4.6733e-01, Meta loss averaged over last 500 steps = 4.7097e-01, PNorm = 56.8194, GNorm = 0.4087
Took 110.56822681427002 seconds to complete one epoch of meta training
Took 117.2609634399414 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.387292
Epoch 40
Meta loss on this task batch = 4.3098e-01, Meta loss averaged over last 500 steps = 4.7080e-01, PNorm = 56.8303, GNorm = 0.1598
Meta loss on this task batch = 4.7271e-01, Meta loss averaged over last 500 steps = 4.7083e-01, PNorm = 56.8415, GNorm = 0.1378
Meta loss on this task batch = 4.8315e-01, Meta loss averaged over last 500 steps = 4.7090e-01, PNorm = 56.8543, GNorm = 0.1174
Meta loss on this task batch = 4.8141e-01, Meta loss averaged over last 500 steps = 4.7082e-01, PNorm = 56.8670, GNorm = 0.1148
Meta loss on this task batch = 4.4208e-01, Meta loss averaged over last 500 steps = 4.7071e-01, PNorm = 56.8806, GNorm = 0.4132
Meta loss on this task batch = 5.0107e-01, Meta loss averaged over last 500 steps = 4.7082e-01, PNorm = 56.8945, GNorm = 0.1361
Meta loss on this task batch = 4.5006e-01, Meta loss averaged over last 500 steps = 4.7075e-01, PNorm = 56.9091, GNorm = 0.2021
Meta loss on this task batch = 4.2653e-01, Meta loss averaged over last 500 steps = 4.7056e-01, PNorm = 56.9230, GNorm = 0.1330
Meta loss on this task batch = 4.8537e-01, Meta loss averaged over last 500 steps = 4.7066e-01, PNorm = 56.9376, GNorm = 0.1262
Meta loss on this task batch = 5.1757e-01, Meta loss averaged over last 500 steps = 4.7085e-01, PNorm = 56.9521, GNorm = 0.1795
Meta loss on this task batch = 4.5226e-01, Meta loss averaged over last 500 steps = 4.7091e-01, PNorm = 56.9676, GNorm = 0.1269
Meta loss on this task batch = 4.5613e-01, Meta loss averaged over last 500 steps = 4.7091e-01, PNorm = 56.9797, GNorm = 0.2171
Meta loss on this task batch = 4.5738e-01, Meta loss averaged over last 500 steps = 4.7082e-01, PNorm = 56.9895, GNorm = 0.3014
Meta loss on this task batch = 4.1233e-01, Meta loss averaged over last 500 steps = 4.7065e-01, PNorm = 56.9991, GNorm = 0.3237
Meta loss on this task batch = 4.6740e-01, Meta loss averaged over last 500 steps = 4.7067e-01, PNorm = 57.0091, GNorm = 0.2723
Meta loss on this task batch = 4.1750e-01, Meta loss averaged over last 500 steps = 4.7044e-01, PNorm = 57.0211, GNorm = 0.1167
Meta loss on this task batch = 4.6380e-01, Meta loss averaged over last 500 steps = 4.7044e-01, PNorm = 57.0320, GNorm = 0.2398
Meta loss on this task batch = 4.4695e-01, Meta loss averaged over last 500 steps = 4.7046e-01, PNorm = 57.0424, GNorm = 0.1794
Meta loss on this task batch = 4.6700e-01, Meta loss averaged over last 500 steps = 4.7047e-01, PNorm = 57.0551, GNorm = 0.4703
Took 112.23479318618774 seconds to complete one epoch of meta training
Took 120.2290985584259 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.432118
Found better MAML checkpoint after meta validation, saving now
Epoch 41
Meta loss on this task batch = 4.5647e-01, Meta loss averaged over last 500 steps = 4.7040e-01, PNorm = 57.0687, GNorm = 0.0917
Meta loss on this task batch = 4.8834e-01, Meta loss averaged over last 500 steps = 4.7040e-01, PNorm = 57.0826, GNorm = 0.1276
Meta loss on this task batch = 4.8285e-01, Meta loss averaged over last 500 steps = 4.7030e-01, PNorm = 57.0970, GNorm = 0.2319
Meta loss on this task batch = 3.6765e-01, Meta loss averaged over last 500 steps = 4.7010e-01, PNorm = 57.1129, GNorm = 0.1282
Meta loss on this task batch = 4.4995e-01, Meta loss averaged over last 500 steps = 4.7010e-01, PNorm = 57.1273, GNorm = 0.1438
Meta loss on this task batch = 4.3616e-01, Meta loss averaged over last 500 steps = 4.6996e-01, PNorm = 57.1425, GNorm = 0.0969
Meta loss on this task batch = 4.6850e-01, Meta loss averaged over last 500 steps = 4.6999e-01, PNorm = 57.1594, GNorm = 0.4600
Meta loss on this task batch = 5.1863e-01, Meta loss averaged over last 500 steps = 4.7006e-01, PNorm = 57.1759, GNorm = 0.1388
Meta loss on this task batch = 4.5999e-01, Meta loss averaged over last 500 steps = 4.7006e-01, PNorm = 57.1922, GNorm = 0.4363
Meta loss on this task batch = 4.1287e-01, Meta loss averaged over last 500 steps = 4.7002e-01, PNorm = 57.2071, GNorm = 0.2563
Meta loss on this task batch = 4.7457e-01, Meta loss averaged over last 500 steps = 4.6996e-01, PNorm = 57.2221, GNorm = 0.1164
Meta loss on this task batch = 4.8844e-01, Meta loss averaged over last 500 steps = 4.6994e-01, PNorm = 57.2360, GNorm = 0.5955
Meta loss on this task batch = 4.2264e-01, Meta loss averaged over last 500 steps = 4.6975e-01, PNorm = 57.2499, GNorm = 0.1172
Meta loss on this task batch = 4.8710e-01, Meta loss averaged over last 500 steps = 4.6971e-01, PNorm = 57.2632, GNorm = 0.3449
Meta loss on this task batch = 5.3607e-01, Meta loss averaged over last 500 steps = 4.6978e-01, PNorm = 57.2762, GNorm = 0.2098
Meta loss on this task batch = 4.2604e-01, Meta loss averaged over last 500 steps = 4.6975e-01, PNorm = 57.2887, GNorm = 0.3243
Meta loss on this task batch = 4.2744e-01, Meta loss averaged over last 500 steps = 4.6969e-01, PNorm = 57.3016, GNorm = 0.1660
Meta loss on this task batch = 4.1477e-01, Meta loss averaged over last 500 steps = 4.6954e-01, PNorm = 57.3155, GNorm = 0.4634
Meta loss on this task batch = 5.4812e-01, Meta loss averaged over last 500 steps = 4.6979e-01, PNorm = 57.3277, GNorm = 0.2963
Took 141.83464574813843 seconds to complete one epoch of meta training
Took 149.22783088684082 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.402832
Epoch 42
Meta loss on this task batch = 4.2748e-01, Meta loss averaged over last 500 steps = 4.6964e-01, PNorm = 57.3411, GNorm = 0.4276
Meta loss on this task batch = 4.9824e-01, Meta loss averaged over last 500 steps = 4.6971e-01, PNorm = 57.3552, GNorm = 0.1574
Meta loss on this task batch = 4.9827e-01, Meta loss averaged over last 500 steps = 4.6983e-01, PNorm = 57.3697, GNorm = 0.2218
Meta loss on this task batch = 3.7797e-01, Meta loss averaged over last 500 steps = 4.6967e-01, PNorm = 57.3857, GNorm = 0.1501
Meta loss on this task batch = 3.9766e-01, Meta loss averaged over last 500 steps = 4.6946e-01, PNorm = 57.4027, GNorm = 0.2917
Meta loss on this task batch = 4.6466e-01, Meta loss averaged over last 500 steps = 4.6925e-01, PNorm = 57.4196, GNorm = 0.1409
Meta loss on this task batch = 3.8172e-01, Meta loss averaged over last 500 steps = 4.6908e-01, PNorm = 57.4369, GNorm = 0.1915
Meta loss on this task batch = 4.7777e-01, Meta loss averaged over last 500 steps = 4.6917e-01, PNorm = 57.4544, GNorm = 0.2822
Meta loss on this task batch = 4.8154e-01, Meta loss averaged over last 500 steps = 4.6925e-01, PNorm = 57.4692, GNorm = 0.3596
Meta loss on this task batch = 4.9963e-01, Meta loss averaged over last 500 steps = 4.6934e-01, PNorm = 57.4822, GNorm = 0.1135
Meta loss on this task batch = 4.3612e-01, Meta loss averaged over last 500 steps = 4.6925e-01, PNorm = 57.4962, GNorm = 0.3409
Meta loss on this task batch = 5.0203e-01, Meta loss averaged over last 500 steps = 4.6927e-01, PNorm = 57.5107, GNorm = 0.2363
Meta loss on this task batch = 4.3885e-01, Meta loss averaged over last 500 steps = 4.6907e-01, PNorm = 57.5256, GNorm = 0.2305
Meta loss on this task batch = 3.7913e-01, Meta loss averaged over last 500 steps = 4.6884e-01, PNorm = 57.5407, GNorm = 0.1061
Meta loss on this task batch = 4.6943e-01, Meta loss averaged over last 500 steps = 4.6883e-01, PNorm = 57.5561, GNorm = 0.3704
Meta loss on this task batch = 4.8747e-01, Meta loss averaged over last 500 steps = 4.6879e-01, PNorm = 57.5688, GNorm = 0.1965
Meta loss on this task batch = 4.9042e-01, Meta loss averaged over last 500 steps = 4.6876e-01, PNorm = 57.5831, GNorm = 0.3152
Meta loss on this task batch = 5.5416e-01, Meta loss averaged over last 500 steps = 4.6897e-01, PNorm = 57.5964, GNorm = 0.1233
Meta loss on this task batch = 4.6420e-01, Meta loss averaged over last 500 steps = 4.6887e-01, PNorm = 57.6090, GNorm = 0.2593
Took 406.2787685394287 seconds to complete one epoch of meta training
Took 413.38989186286926 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.425848
Epoch 43
Meta loss on this task batch = 4.2803e-01, Meta loss averaged over last 500 steps = 4.6874e-01, PNorm = 57.6221, GNorm = 0.1067
Meta loss on this task batch = 4.6487e-01, Meta loss averaged over last 500 steps = 4.6866e-01, PNorm = 57.6360, GNorm = 0.1407
Meta loss on this task batch = 4.9097e-01, Meta loss averaged over last 500 steps = 4.6886e-01, PNorm = 57.6506, GNorm = 0.1798
Meta loss on this task batch = 4.4678e-01, Meta loss averaged over last 500 steps = 4.6872e-01, PNorm = 57.6657, GNorm = 0.0919
Meta loss on this task batch = 4.8165e-01, Meta loss averaged over last 500 steps = 4.6872e-01, PNorm = 57.6804, GNorm = 0.2102
Meta loss on this task batch = 4.6329e-01, Meta loss averaged over last 500 steps = 4.6871e-01, PNorm = 57.6961, GNorm = 0.1387
Meta loss on this task batch = 3.9949e-01, Meta loss averaged over last 500 steps = 4.6867e-01, PNorm = 57.7117, GNorm = 0.1798
Meta loss on this task batch = 4.8383e-01, Meta loss averaged over last 500 steps = 4.6870e-01, PNorm = 57.7263, GNorm = 0.1370
Meta loss on this task batch = 4.6466e-01, Meta loss averaged over last 500 steps = 4.6858e-01, PNorm = 57.7427, GNorm = 0.1354
Meta loss on this task batch = 4.4155e-01, Meta loss averaged over last 500 steps = 4.6855e-01, PNorm = 57.7592, GNorm = 0.1197
Meta loss on this task batch = 4.6048e-01, Meta loss averaged over last 500 steps = 4.6849e-01, PNorm = 57.7743, GNorm = 0.1134
Meta loss on this task batch = 4.2045e-01, Meta loss averaged over last 500 steps = 4.6845e-01, PNorm = 57.7885, GNorm = 0.1770
Meta loss on this task batch = 5.0098e-01, Meta loss averaged over last 500 steps = 4.6843e-01, PNorm = 57.8008, GNorm = 0.1468
Meta loss on this task batch = 4.4138e-01, Meta loss averaged over last 500 steps = 4.6832e-01, PNorm = 57.8130, GNorm = 0.1272
Meta loss on this task batch = 4.1486e-01, Meta loss averaged over last 500 steps = 4.6813e-01, PNorm = 57.8253, GNorm = 0.2884
Meta loss on this task batch = 4.7058e-01, Meta loss averaged over last 500 steps = 4.6797e-01, PNorm = 57.8362, GNorm = 0.3173
Meta loss on this task batch = 4.7353e-01, Meta loss averaged over last 500 steps = 4.6805e-01, PNorm = 57.8473, GNorm = 0.1687
Meta loss on this task batch = 4.2108e-01, Meta loss averaged over last 500 steps = 4.6797e-01, PNorm = 57.8580, GNorm = 0.1138
Meta loss on this task batch = 4.9427e-01, Meta loss averaged over last 500 steps = 4.6818e-01, PNorm = 57.8667, GNorm = 0.2934
Took 122.17393016815186 seconds to complete one epoch of meta training
Took 130.29642939567566 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.436840
Found better MAML checkpoint after meta validation, saving now
Epoch 44
Meta loss on this task batch = 4.2630e-01, Meta loss averaged over last 500 steps = 4.6804e-01, PNorm = 57.8784, GNorm = 0.4431
Meta loss on this task batch = 4.5117e-01, Meta loss averaged over last 500 steps = 4.6796e-01, PNorm = 57.8891, GNorm = 0.2081
Meta loss on this task batch = 3.6734e-01, Meta loss averaged over last 500 steps = 4.6791e-01, PNorm = 57.9002, GNorm = 0.1323
Meta loss on this task batch = 4.6045e-01, Meta loss averaged over last 500 steps = 4.6778e-01, PNorm = 57.9105, GNorm = 0.2282
Meta loss on this task batch = 4.3424e-01, Meta loss averaged over last 500 steps = 4.6764e-01, PNorm = 57.9212, GNorm = 0.1065
Meta loss on this task batch = 4.0586e-01, Meta loss averaged over last 500 steps = 4.6742e-01, PNorm = 57.9341, GNorm = 0.1004
Meta loss on this task batch = 4.2835e-01, Meta loss averaged over last 500 steps = 4.6729e-01, PNorm = 57.9472, GNorm = 0.2354
Meta loss on this task batch = 4.5432e-01, Meta loss averaged over last 500 steps = 4.6726e-01, PNorm = 57.9602, GNorm = 0.2180
Meta loss on this task batch = 3.9185e-01, Meta loss averaged over last 500 steps = 4.6711e-01, PNorm = 57.9748, GNorm = 0.1788
Meta loss on this task batch = 4.6703e-01, Meta loss averaged over last 500 steps = 4.6715e-01, PNorm = 57.9894, GNorm = 0.1145
Meta loss on this task batch = 4.9660e-01, Meta loss averaged over last 500 steps = 4.6708e-01, PNorm = 58.0011, GNorm = 0.2709
Meta loss on this task batch = 4.7191e-01, Meta loss averaged over last 500 steps = 4.6703e-01, PNorm = 58.0125, GNorm = 0.1744
Meta loss on this task batch = 4.2128e-01, Meta loss averaged over last 500 steps = 4.6683e-01, PNorm = 58.0253, GNorm = 0.1203
Meta loss on this task batch = 4.6098e-01, Meta loss averaged over last 500 steps = 4.6692e-01, PNorm = 58.0379, GNorm = 0.1149
Meta loss on this task batch = 5.1688e-01, Meta loss averaged over last 500 steps = 4.6697e-01, PNorm = 58.0501, GNorm = 0.1380
Meta loss on this task batch = 4.5322e-01, Meta loss averaged over last 500 steps = 4.6675e-01, PNorm = 58.0607, GNorm = 0.3067
Meta loss on this task batch = 5.1465e-01, Meta loss averaged over last 500 steps = 4.6694e-01, PNorm = 58.0730, GNorm = 0.1939
Meta loss on this task batch = 5.2549e-01, Meta loss averaged over last 500 steps = 4.6702e-01, PNorm = 58.0833, GNorm = 0.1593
Meta loss on this task batch = 4.5402e-01, Meta loss averaged over last 500 steps = 4.6698e-01, PNorm = 58.0941, GNorm = 0.1860
Took 233.54487204551697 seconds to complete one epoch of meta training
Took 241.14673161506653 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.415317
Epoch 45
Meta loss on this task batch = 5.0298e-01, Meta loss averaged over last 500 steps = 4.6696e-01, PNorm = 58.1058, GNorm = 0.1236
Meta loss on this task batch = 4.7589e-01, Meta loss averaged over last 500 steps = 4.6693e-01, PNorm = 58.1194, GNorm = 0.2044
Meta loss on this task batch = 4.8119e-01, Meta loss averaged over last 500 steps = 4.6702e-01, PNorm = 58.1330, GNorm = 0.1399
Meta loss on this task batch = 5.1138e-01, Meta loss averaged over last 500 steps = 4.6713e-01, PNorm = 58.1462, GNorm = 0.2149
Meta loss on this task batch = 4.2572e-01, Meta loss averaged over last 500 steps = 4.6712e-01, PNorm = 58.1597, GNorm = 0.1851
Meta loss on this task batch = 4.2227e-01, Meta loss averaged over last 500 steps = 4.6698e-01, PNorm = 58.1724, GNorm = 0.1085
Meta loss on this task batch = 4.6271e-01, Meta loss averaged over last 500 steps = 4.6682e-01, PNorm = 58.1839, GNorm = 0.1862
Meta loss on this task batch = 4.9272e-01, Meta loss averaged over last 500 steps = 4.6694e-01, PNorm = 58.1953, GNorm = 0.1494
Meta loss on this task batch = 3.9887e-01, Meta loss averaged over last 500 steps = 4.6654e-01, PNorm = 58.2077, GNorm = 0.1294
Meta loss on this task batch = 4.7356e-01, Meta loss averaged over last 500 steps = 4.6661e-01, PNorm = 58.2177, GNorm = 0.3065
Meta loss on this task batch = 4.9207e-01, Meta loss averaged over last 500 steps = 4.6649e-01, PNorm = 58.2275, GNorm = 0.3652
Meta loss on this task batch = 3.5065e-01, Meta loss averaged over last 500 steps = 4.6640e-01, PNorm = 58.2392, GNorm = 0.3028
Meta loss on this task batch = 4.1600e-01, Meta loss averaged over last 500 steps = 4.6616e-01, PNorm = 58.2525, GNorm = 0.2363
Meta loss on this task batch = 4.6915e-01, Meta loss averaged over last 500 steps = 4.6605e-01, PNorm = 58.2683, GNorm = 0.2522
Meta loss on this task batch = 4.0803e-01, Meta loss averaged over last 500 steps = 4.6604e-01, PNorm = 58.2849, GNorm = 0.2412
Meta loss on this task batch = 4.8433e-01, Meta loss averaged over last 500 steps = 4.6608e-01, PNorm = 58.2996, GNorm = 0.2532
Meta loss on this task batch = 4.9054e-01, Meta loss averaged over last 500 steps = 4.6601e-01, PNorm = 58.3135, GNorm = 0.1444
Meta loss on this task batch = 4.7802e-01, Meta loss averaged over last 500 steps = 4.6599e-01, PNorm = 58.3274, GNorm = 0.1931
Meta loss on this task batch = 4.3257e-01, Meta loss averaged over last 500 steps = 4.6592e-01, PNorm = 58.3422, GNorm = 0.1732
Took 193.75716519355774 seconds to complete one epoch of meta training
Took 202.16482663154602 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.437214
Found better MAML checkpoint after meta validation, saving now
Epoch 46
Meta loss on this task batch = 4.8567e-01, Meta loss averaged over last 500 steps = 4.6586e-01, PNorm = 58.3580, GNorm = 0.3911
Meta loss on this task batch = 4.3740e-01, Meta loss averaged over last 500 steps = 4.6586e-01, PNorm = 58.3725, GNorm = 0.1713
Meta loss on this task batch = 4.6993e-01, Meta loss averaged over last 500 steps = 4.6581e-01, PNorm = 58.3864, GNorm = 0.6544
Meta loss on this task batch = 4.0521e-01, Meta loss averaged over last 500 steps = 4.6580e-01, PNorm = 58.4018, GNorm = 0.2904
Meta loss on this task batch = 4.5105e-01, Meta loss averaged over last 500 steps = 4.6590e-01, PNorm = 58.4154, GNorm = 0.3162
Meta loss on this task batch = 4.5039e-01, Meta loss averaged over last 500 steps = 4.6600e-01, PNorm = 58.4290, GNorm = 0.1986
Meta loss on this task batch = 4.3682e-01, Meta loss averaged over last 500 steps = 4.6606e-01, PNorm = 58.4432, GNorm = 0.2087
Meta loss on this task batch = 5.2475e-01, Meta loss averaged over last 500 steps = 4.6610e-01, PNorm = 58.4555, GNorm = 0.1481
Meta loss on this task batch = 4.4472e-01, Meta loss averaged over last 500 steps = 4.6605e-01, PNorm = 58.4676, GNorm = 0.1349
Meta loss on this task batch = 4.7395e-01, Meta loss averaged over last 500 steps = 4.6614e-01, PNorm = 58.4792, GNorm = 0.2651
Meta loss on this task batch = 5.0511e-01, Meta loss averaged over last 500 steps = 4.6612e-01, PNorm = 58.4895, GNorm = 0.1769
Meta loss on this task batch = 4.5081e-01, Meta loss averaged over last 500 steps = 4.6602e-01, PNorm = 58.5015, GNorm = 0.2281
Meta loss on this task batch = 4.4696e-01, Meta loss averaged over last 500 steps = 4.6590e-01, PNorm = 58.5145, GNorm = 0.1535
Meta loss on this task batch = 4.3622e-01, Meta loss averaged over last 500 steps = 4.6584e-01, PNorm = 58.5286, GNorm = 0.2520
Meta loss on this task batch = 4.6207e-01, Meta loss averaged over last 500 steps = 4.6578e-01, PNorm = 58.5444, GNorm = 0.1563
Meta loss on this task batch = 5.0724e-01, Meta loss averaged over last 500 steps = 4.6580e-01, PNorm = 58.5611, GNorm = 0.2609
Meta loss on this task batch = 4.8503e-01, Meta loss averaged over last 500 steps = 4.6591e-01, PNorm = 58.5769, GNorm = 0.0971
Meta loss on this task batch = 4.4996e-01, Meta loss averaged over last 500 steps = 4.6580e-01, PNorm = 58.5929, GNorm = 0.1448
Meta loss on this task batch = 3.5894e-01, Meta loss averaged over last 500 steps = 4.6556e-01, PNorm = 58.6091, GNorm = 0.1192
Took 124.04130005836487 seconds to complete one epoch of meta training
Took 132.8053092956543 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.408522
Epoch 47
Meta loss on this task batch = 4.6712e-01, Meta loss averaged over last 500 steps = 4.6554e-01, PNorm = 58.6262, GNorm = 0.1668
Meta loss on this task batch = 4.1513e-01, Meta loss averaged over last 500 steps = 4.6550e-01, PNorm = 58.6444, GNorm = 0.1253
Meta loss on this task batch = 4.2049e-01, Meta loss averaged over last 500 steps = 4.6534e-01, PNorm = 58.6629, GNorm = 0.2326
Meta loss on this task batch = 4.2402e-01, Meta loss averaged over last 500 steps = 4.6529e-01, PNorm = 58.6816, GNorm = 0.0842
Meta loss on this task batch = 4.8572e-01, Meta loss averaged over last 500 steps = 4.6523e-01, PNorm = 58.6977, GNorm = 0.3489
Meta loss on this task batch = 4.6698e-01, Meta loss averaged over last 500 steps = 4.6532e-01, PNorm = 58.7122, GNorm = 0.2113
Meta loss on this task batch = 4.7279e-01, Meta loss averaged over last 500 steps = 4.6532e-01, PNorm = 58.7223, GNorm = 0.2360
Meta loss on this task batch = 4.9654e-01, Meta loss averaged over last 500 steps = 4.6541e-01, PNorm = 58.7328, GNorm = 0.2989
Meta loss on this task batch = 4.3430e-01, Meta loss averaged over last 500 steps = 4.6525e-01, PNorm = 58.7438, GNorm = 0.1027
Meta loss on this task batch = 5.2538e-01, Meta loss averaged over last 500 steps = 4.6534e-01, PNorm = 58.7518, GNorm = 0.3763
Meta loss on this task batch = 4.4004e-01, Meta loss averaged over last 500 steps = 4.6514e-01, PNorm = 58.7594, GNorm = 0.3754
Meta loss on this task batch = 4.4284e-01, Meta loss averaged over last 500 steps = 4.6502e-01, PNorm = 58.7686, GNorm = 0.4167
Meta loss on this task batch = 4.5652e-01, Meta loss averaged over last 500 steps = 4.6488e-01, PNorm = 58.7782, GNorm = 0.6030
Meta loss on this task batch = 4.6752e-01, Meta loss averaged over last 500 steps = 4.6499e-01, PNorm = 58.7914, GNorm = 0.2747
Meta loss on this task batch = 4.4983e-01, Meta loss averaged over last 500 steps = 4.6491e-01, PNorm = 58.8068, GNorm = 0.2963
Meta loss on this task batch = 5.4330e-01, Meta loss averaged over last 500 steps = 4.6504e-01, PNorm = 58.8241, GNorm = 0.1924
Meta loss on this task batch = 4.9412e-01, Meta loss averaged over last 500 steps = 4.6521e-01, PNorm = 58.8419, GNorm = 0.1678
Meta loss on this task batch = 4.3353e-01, Meta loss averaged over last 500 steps = 4.6514e-01, PNorm = 58.8610, GNorm = 0.2461
Meta loss on this task batch = 5.2814e-01, Meta loss averaged over last 500 steps = 4.6511e-01, PNorm = 58.8798, GNorm = 0.1496
Took 124.17250728607178 seconds to complete one epoch of meta training
Took 132.56189012527466 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.421165
Epoch 48
Meta loss on this task batch = 4.1769e-01, Meta loss averaged over last 500 steps = 4.6516e-01, PNorm = 58.9000, GNorm = 0.1882
Meta loss on this task batch = 4.6414e-01, Meta loss averaged over last 500 steps = 4.6515e-01, PNorm = 58.9218, GNorm = 0.1769
Meta loss on this task batch = 4.3687e-01, Meta loss averaged over last 500 steps = 4.6502e-01, PNorm = 58.9444, GNorm = 0.1121
Meta loss on this task batch = 4.7054e-01, Meta loss averaged over last 500 steps = 4.6506e-01, PNorm = 58.9662, GNorm = 0.1296
Meta loss on this task batch = 4.3029e-01, Meta loss averaged over last 500 steps = 4.6499e-01, PNorm = 58.9890, GNorm = 0.3442
Meta loss on this task batch = 4.4854e-01, Meta loss averaged over last 500 steps = 4.6508e-01, PNorm = 59.0115, GNorm = 0.1043
Meta loss on this task batch = 4.4963e-01, Meta loss averaged over last 500 steps = 4.6513e-01, PNorm = 59.0340, GNorm = 0.0939
Meta loss on this task batch = 4.6601e-01, Meta loss averaged over last 500 steps = 4.6494e-01, PNorm = 59.0554, GNorm = 0.1410
Meta loss on this task batch = 4.8234e-01, Meta loss averaged over last 500 steps = 4.6498e-01, PNorm = 59.0754, GNorm = 0.1092
Meta loss on this task batch = 4.7434e-01, Meta loss averaged over last 500 steps = 4.6486e-01, PNorm = 59.0938, GNorm = 0.1414
Meta loss on this task batch = 4.4215e-01, Meta loss averaged over last 500 steps = 4.6484e-01, PNorm = 59.1103, GNorm = 0.1159
Meta loss on this task batch = 4.8804e-01, Meta loss averaged over last 500 steps = 4.6489e-01, PNorm = 59.1236, GNorm = 0.1219
Meta loss on this task batch = 5.1484e-01, Meta loss averaged over last 500 steps = 4.6500e-01, PNorm = 59.1349, GNorm = 0.2905
Meta loss on this task batch = 4.3248e-01, Meta loss averaged over last 500 steps = 4.6497e-01, PNorm = 59.1472, GNorm = 0.0958
Meta loss on this task batch = 3.7769e-01, Meta loss averaged over last 500 steps = 4.6484e-01, PNorm = 59.1603, GNorm = 0.1109
Meta loss on this task batch = 4.5318e-01, Meta loss averaged over last 500 steps = 4.6490e-01, PNorm = 59.1738, GNorm = 0.2338
Meta loss on this task batch = 4.6617e-01, Meta loss averaged over last 500 steps = 4.6480e-01, PNorm = 59.1868, GNorm = 0.1573
Meta loss on this task batch = 5.5399e-01, Meta loss averaged over last 500 steps = 4.6484e-01, PNorm = 59.1997, GNorm = 0.1066
Meta loss on this task batch = 4.6976e-01, Meta loss averaged over last 500 steps = 4.6471e-01, PNorm = 59.2108, GNorm = 0.3801
Took 120.44522714614868 seconds to complete one epoch of meta training
Took 127.69631004333496 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.414876
Epoch 49
Meta loss on this task batch = 4.5042e-01, Meta loss averaged over last 500 steps = 4.6471e-01, PNorm = 59.2228, GNorm = 0.1863
Meta loss on this task batch = 4.5456e-01, Meta loss averaged over last 500 steps = 4.6472e-01, PNorm = 59.2344, GNorm = 0.3740
Meta loss on this task batch = 4.7130e-01, Meta loss averaged over last 500 steps = 4.6476e-01, PNorm = 59.2452, GNorm = 0.3698
Meta loss on this task batch = 4.2479e-01, Meta loss averaged over last 500 steps = 4.6469e-01, PNorm = 59.2565, GNorm = 0.1947
Meta loss on this task batch = 4.8351e-01, Meta loss averaged over last 500 steps = 4.6468e-01, PNorm = 59.2675, GNorm = 0.1260
Meta loss on this task batch = 4.9489e-01, Meta loss averaged over last 500 steps = 4.6464e-01, PNorm = 59.2803, GNorm = 0.1020
Meta loss on this task batch = 4.4603e-01, Meta loss averaged over last 500 steps = 4.6464e-01, PNorm = 59.2948, GNorm = 0.1070
Meta loss on this task batch = 4.9963e-01, Meta loss averaged over last 500 steps = 4.6472e-01, PNorm = 59.3087, GNorm = 0.1039
Meta loss on this task batch = 4.1699e-01, Meta loss averaged over last 500 steps = 4.6453e-01, PNorm = 59.3230, GNorm = 0.2770
Meta loss on this task batch = 4.4284e-01, Meta loss averaged over last 500 steps = 4.6444e-01, PNorm = 59.3381, GNorm = 0.1307
Meta loss on this task batch = 4.5707e-01, Meta loss averaged over last 500 steps = 4.6443e-01, PNorm = 59.3547, GNorm = 0.2056
Meta loss on this task batch = 4.0465e-01, Meta loss averaged over last 500 steps = 4.6425e-01, PNorm = 59.3727, GNorm = 0.4251
Meta loss on this task batch = 4.9339e-01, Meta loss averaged over last 500 steps = 4.6440e-01, PNorm = 59.3902, GNorm = 0.1808
Meta loss on this task batch = 5.1020e-01, Meta loss averaged over last 500 steps = 4.6447e-01, PNorm = 59.4067, GNorm = 0.1541
Meta loss on this task batch = 5.0964e-01, Meta loss averaged over last 500 steps = 4.6460e-01, PNorm = 59.4223, GNorm = 0.2726
Meta loss on this task batch = 4.2256e-01, Meta loss averaged over last 500 steps = 4.6452e-01, PNorm = 59.4355, GNorm = 0.1986
Meta loss on this task batch = 4.1857e-01, Meta loss averaged over last 500 steps = 4.6422e-01, PNorm = 59.4471, GNorm = 0.1811
Meta loss on this task batch = 4.5058e-01, Meta loss averaged over last 500 steps = 4.6420e-01, PNorm = 59.4573, GNorm = 0.1512
Meta loss on this task batch = 4.4996e-01, Meta loss averaged over last 500 steps = 4.6422e-01, PNorm = 59.4684, GNorm = 0.1674
Took 121.67773795127869 seconds to complete one epoch of meta training
Took 130.34555506706238 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.445134
Found better MAML checkpoint after meta validation, saving now
Epoch 50
Meta loss on this task batch = 4.5175e-01, Meta loss averaged over last 500 steps = 4.6417e-01, PNorm = 59.4794, GNorm = 0.2444
Meta loss on this task batch = 4.8420e-01, Meta loss averaged over last 500 steps = 4.6411e-01, PNorm = 59.4902, GNorm = 0.1669
Meta loss on this task batch = 4.6226e-01, Meta loss averaged over last 500 steps = 4.6403e-01, PNorm = 59.5010, GNorm = 0.1179
Meta loss on this task batch = 5.1623e-01, Meta loss averaged over last 500 steps = 4.6422e-01, PNorm = 59.5102, GNorm = 0.2596
Meta loss on this task batch = 5.0293e-01, Meta loss averaged over last 500 steps = 4.6422e-01, PNorm = 59.5193, GNorm = 0.1241
Meta loss on this task batch = 4.1484e-01, Meta loss averaged over last 500 steps = 4.6410e-01, PNorm = 59.5305, GNorm = 0.2264
Meta loss on this task batch = 4.1812e-01, Meta loss averaged over last 500 steps = 4.6404e-01, PNorm = 59.5418, GNorm = 0.1018
Meta loss on this task batch = 4.7355e-01, Meta loss averaged over last 500 steps = 4.6408e-01, PNorm = 59.5529, GNorm = 0.4828
Meta loss on this task batch = 4.3207e-01, Meta loss averaged over last 500 steps = 4.6408e-01, PNorm = 59.5651, GNorm = 0.3432
Meta loss on this task batch = 4.4328e-01, Meta loss averaged over last 500 steps = 4.6399e-01, PNorm = 59.5780, GNorm = 0.1084
Meta loss on this task batch = 4.4661e-01, Meta loss averaged over last 500 steps = 4.6387e-01, PNorm = 59.5924, GNorm = 0.1224
Meta loss on this task batch = 4.9905e-01, Meta loss averaged over last 500 steps = 4.6390e-01, PNorm = 59.6066, GNorm = 0.3438
Meta loss on this task batch = 4.0189e-01, Meta loss averaged over last 500 steps = 4.6389e-01, PNorm = 59.6220, GNorm = 0.3248
Meta loss on this task batch = 3.7854e-01, Meta loss averaged over last 500 steps = 4.6357e-01, PNorm = 59.6388, GNorm = 0.1104
Meta loss on this task batch = 4.3747e-01, Meta loss averaged over last 500 steps = 4.6345e-01, PNorm = 59.6555, GNorm = 0.1191
Meta loss on this task batch = 4.7969e-01, Meta loss averaged over last 500 steps = 4.6342e-01, PNorm = 59.6721, GNorm = 0.1218
Meta loss on this task batch = 4.1128e-01, Meta loss averaged over last 500 steps = 4.6333e-01, PNorm = 59.6885, GNorm = 0.1324
Meta loss on this task batch = 4.1388e-01, Meta loss averaged over last 500 steps = 4.6316e-01, PNorm = 59.7043, GNorm = 0.1127
Meta loss on this task batch = 4.8411e-01, Meta loss averaged over last 500 steps = 4.6321e-01, PNorm = 59.7177, GNorm = 0.2318
Took 119.53660750389099 seconds to complete one epoch of meta training
Took 127.14514994621277 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449163
Found better MAML checkpoint after meta validation, saving now
Epoch 51
Meta loss on this task batch = 4.3494e-01, Meta loss averaged over last 500 steps = 4.6303e-01, PNorm = 59.7307, GNorm = 0.2220
Meta loss on this task batch = 5.2205e-01, Meta loss averaged over last 500 steps = 4.6308e-01, PNorm = 59.7404, GNorm = 0.2289
Meta loss on this task batch = 4.2879e-01, Meta loss averaged over last 500 steps = 4.6302e-01, PNorm = 59.7495, GNorm = 0.1063
Meta loss on this task batch = 4.2141e-01, Meta loss averaged over last 500 steps = 4.6295e-01, PNorm = 59.7595, GNorm = 0.1161
Meta loss on this task batch = 4.4200e-01, Meta loss averaged over last 500 steps = 4.6288e-01, PNorm = 59.7697, GNorm = 0.0960
Meta loss on this task batch = 4.8976e-01, Meta loss averaged over last 500 steps = 4.6286e-01, PNorm = 59.7807, GNorm = 0.4923
Meta loss on this task batch = 4.6913e-01, Meta loss averaged over last 500 steps = 4.6282e-01, PNorm = 59.7923, GNorm = 0.3012
Meta loss on this task batch = 4.8261e-01, Meta loss averaged over last 500 steps = 4.6290e-01, PNorm = 59.8050, GNorm = 0.1694
Meta loss on this task batch = 4.2775e-01, Meta loss averaged over last 500 steps = 4.6284e-01, PNorm = 59.8185, GNorm = 0.2267
Meta loss on this task batch = 3.8871e-01, Meta loss averaged over last 500 steps = 4.6264e-01, PNorm = 59.8326, GNorm = 0.1070
Meta loss on this task batch = 4.4270e-01, Meta loss averaged over last 500 steps = 4.6255e-01, PNorm = 59.8490, GNorm = 0.1912
Meta loss on this task batch = 4.8127e-01, Meta loss averaged over last 500 steps = 4.6261e-01, PNorm = 59.8643, GNorm = 0.2033
Meta loss on this task batch = 4.3666e-01, Meta loss averaged over last 500 steps = 4.6249e-01, PNorm = 59.8799, GNorm = 0.1389
Meta loss on this task batch = 5.3036e-01, Meta loss averaged over last 500 steps = 4.6260e-01, PNorm = 59.8955, GNorm = 0.2039
Meta loss on this task batch = 4.7830e-01, Meta loss averaged over last 500 steps = 4.6265e-01, PNorm = 59.9097, GNorm = 0.1292
Meta loss on this task batch = 5.0056e-01, Meta loss averaged over last 500 steps = 4.6281e-01, PNorm = 59.9217, GNorm = 0.2236
Meta loss on this task batch = 4.5184e-01, Meta loss averaged over last 500 steps = 4.6271e-01, PNorm = 59.9330, GNorm = 0.1446
Meta loss on this task batch = 5.3054e-01, Meta loss averaged over last 500 steps = 4.6285e-01, PNorm = 59.9433, GNorm = 0.2036
Meta loss on this task batch = 3.5095e-01, Meta loss averaged over last 500 steps = 4.6248e-01, PNorm = 59.9562, GNorm = 0.1177
Took 116.47152781486511 seconds to complete one epoch of meta training
Took 124.74046564102173 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446444
Epoch 52
Meta loss on this task batch = 5.2733e-01, Meta loss averaged over last 500 steps = 4.6261e-01, PNorm = 59.9694, GNorm = 0.1041
Meta loss on this task batch = 3.8815e-01, Meta loss averaged over last 500 steps = 4.6239e-01, PNorm = 59.9835, GNorm = 0.1037
Meta loss on this task batch = 4.5970e-01, Meta loss averaged over last 500 steps = 4.6233e-01, PNorm = 59.9985, GNorm = 0.1699
Meta loss on this task batch = 4.6622e-01, Meta loss averaged over last 500 steps = 4.6223e-01, PNorm = 60.0123, GNorm = 0.0936
Meta loss on this task batch = 5.1452e-01, Meta loss averaged over last 500 steps = 4.6230e-01, PNorm = 60.0248, GNorm = 0.1731
Meta loss on this task batch = 4.4646e-01, Meta loss averaged over last 500 steps = 4.6231e-01, PNorm = 60.0371, GNorm = 0.2761
Meta loss on this task batch = 4.0569e-01, Meta loss averaged over last 500 steps = 4.6230e-01, PNorm = 60.0506, GNorm = 0.2305
Meta loss on this task batch = 5.0411e-01, Meta loss averaged over last 500 steps = 4.6230e-01, PNorm = 60.0650, GNorm = 0.2575
Meta loss on this task batch = 4.1528e-01, Meta loss averaged over last 500 steps = 4.6227e-01, PNorm = 60.0797, GNorm = 0.1337
Meta loss on this task batch = 3.9822e-01, Meta loss averaged over last 500 steps = 4.6215e-01, PNorm = 60.0963, GNorm = 0.2734
Meta loss on this task batch = 4.8514e-01, Meta loss averaged over last 500 steps = 4.6216e-01, PNorm = 60.1122, GNorm = 0.2601
Meta loss on this task batch = 4.6753e-01, Meta loss averaged over last 500 steps = 4.6226e-01, PNorm = 60.1282, GNorm = 0.1362
Meta loss on this task batch = 4.4156e-01, Meta loss averaged over last 500 steps = 4.6213e-01, PNorm = 60.1423, GNorm = 0.1486
Meta loss on this task batch = 3.9044e-01, Meta loss averaged over last 500 steps = 4.6206e-01, PNorm = 60.1561, GNorm = 0.0916
Meta loss on this task batch = 4.1275e-01, Meta loss averaged over last 500 steps = 4.6192e-01, PNorm = 60.1701, GNorm = 0.4393
Meta loss on this task batch = 3.9041e-01, Meta loss averaged over last 500 steps = 4.6165e-01, PNorm = 60.1832, GNorm = 0.2138
Meta loss on this task batch = 4.8958e-01, Meta loss averaged over last 500 steps = 4.6174e-01, PNorm = 60.1943, GNorm = 0.1919
Meta loss on this task batch = 4.2355e-01, Meta loss averaged over last 500 steps = 4.6168e-01, PNorm = 60.2048, GNorm = 0.1623
Meta loss on this task batch = 4.9999e-01, Meta loss averaged over last 500 steps = 4.6159e-01, PNorm = 60.2138, GNorm = 0.2653
Took 114.02864241600037 seconds to complete one epoch of meta training
Took 122.08874368667603 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446880
Epoch 53
Meta loss on this task batch = 4.5514e-01, Meta loss averaged over last 500 steps = 4.6159e-01, PNorm = 60.2252, GNorm = 0.4472
Meta loss on this task batch = 4.0987e-01, Meta loss averaged over last 500 steps = 4.6153e-01, PNorm = 60.2383, GNorm = 0.2244
Meta loss on this task batch = 3.7471e-01, Meta loss averaged over last 500 steps = 4.6116e-01, PNorm = 60.2519, GNorm = 0.1476
Meta loss on this task batch = 4.8343e-01, Meta loss averaged over last 500 steps = 4.6115e-01, PNorm = 60.2625, GNorm = 0.3236
Meta loss on this task batch = 4.0109e-01, Meta loss averaged over last 500 steps = 4.6108e-01, PNorm = 60.2736, GNorm = 0.1358
Meta loss on this task batch = 5.0152e-01, Meta loss averaged over last 500 steps = 4.6114e-01, PNorm = 60.2848, GNorm = 0.1510
Meta loss on this task batch = 4.0955e-01, Meta loss averaged over last 500 steps = 4.6097e-01, PNorm = 60.2967, GNorm = 0.1343
Meta loss on this task batch = 4.6249e-01, Meta loss averaged over last 500 steps = 4.6110e-01, PNorm = 60.3096, GNorm = 0.1182
Meta loss on this task batch = 3.9559e-01, Meta loss averaged over last 500 steps = 4.6082e-01, PNorm = 60.3233, GNorm = 0.1732
Meta loss on this task batch = 4.9724e-01, Meta loss averaged over last 500 steps = 4.6098e-01, PNorm = 60.3355, GNorm = 0.2653
Meta loss on this task batch = 4.7115e-01, Meta loss averaged over last 500 steps = 4.6108e-01, PNorm = 60.3490, GNorm = 0.1729
Meta loss on this task batch = 4.1280e-01, Meta loss averaged over last 500 steps = 4.6087e-01, PNorm = 60.3625, GNorm = 0.1391
Meta loss on this task batch = 4.5349e-01, Meta loss averaged over last 500 steps = 4.6078e-01, PNorm = 60.3776, GNorm = 0.1147
Meta loss on this task batch = 5.2748e-01, Meta loss averaged over last 500 steps = 4.6080e-01, PNorm = 60.3915, GNorm = 0.1353
Meta loss on this task batch = 4.7495e-01, Meta loss averaged over last 500 steps = 4.6083e-01, PNorm = 60.4045, GNorm = 0.1332
Meta loss on this task batch = 4.4573e-01, Meta loss averaged over last 500 steps = 4.6077e-01, PNorm = 60.4188, GNorm = 0.4082
Meta loss on this task batch = 4.3798e-01, Meta loss averaged over last 500 steps = 4.6077e-01, PNorm = 60.4331, GNorm = 0.2095
Meta loss on this task batch = 3.5347e-01, Meta loss averaged over last 500 steps = 4.6061e-01, PNorm = 60.4492, GNorm = 0.1802
Meta loss on this task batch = 4.8422e-01, Meta loss averaged over last 500 steps = 4.6073e-01, PNorm = 60.4643, GNorm = 0.1882
Took 113.05132007598877 seconds to complete one epoch of meta training
Took 120.28448152542114 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.445666
Epoch 54
Meta loss on this task batch = 3.9021e-01, Meta loss averaged over last 500 steps = 4.6052e-01, PNorm = 60.4803, GNorm = 0.3182
Meta loss on this task batch = 4.5110e-01, Meta loss averaged over last 500 steps = 4.6043e-01, PNorm = 60.4959, GNorm = 0.1257
Meta loss on this task batch = 4.4145e-01, Meta loss averaged over last 500 steps = 4.6034e-01, PNorm = 60.5140, GNorm = 0.3155
Meta loss on this task batch = 4.5050e-01, Meta loss averaged over last 500 steps = 4.6020e-01, PNorm = 60.5322, GNorm = 0.1547
Meta loss on this task batch = 4.7289e-01, Meta loss averaged over last 500 steps = 4.6021e-01, PNorm = 60.5491, GNorm = 0.1954
Meta loss on this task batch = 5.0136e-01, Meta loss averaged over last 500 steps = 4.6026e-01, PNorm = 60.5658, GNorm = 0.2335
Meta loss on this task batch = 4.7310e-01, Meta loss averaged over last 500 steps = 4.6022e-01, PNorm = 60.5805, GNorm = 0.2780
Meta loss on this task batch = 4.2073e-01, Meta loss averaged over last 500 steps = 4.6009e-01, PNorm = 60.5966, GNorm = 0.1191
Meta loss on this task batch = 3.9935e-01, Meta loss averaged over last 500 steps = 4.5992e-01, PNorm = 60.6117, GNorm = 0.2538
Meta loss on this task batch = 4.8131e-01, Meta loss averaged over last 500 steps = 4.5990e-01, PNorm = 60.6273, GNorm = 0.2660
Meta loss on this task batch = 3.8528e-01, Meta loss averaged over last 500 steps = 4.5985e-01, PNorm = 60.6427, GNorm = 0.2737
Meta loss on this task batch = 4.2359e-01, Meta loss averaged over last 500 steps = 4.5974e-01, PNorm = 60.6578, GNorm = 0.1837
Meta loss on this task batch = 4.8132e-01, Meta loss averaged over last 500 steps = 4.5982e-01, PNorm = 60.6728, GNorm = 0.1549
Meta loss on this task batch = 4.3750e-01, Meta loss averaged over last 500 steps = 4.5980e-01, PNorm = 60.6858, GNorm = 0.2451
Meta loss on this task batch = 4.5075e-01, Meta loss averaged over last 500 steps = 4.5967e-01, PNorm = 60.6987, GNorm = 0.2458
Meta loss on this task batch = 4.6260e-01, Meta loss averaged over last 500 steps = 4.5979e-01, PNorm = 60.7130, GNorm = 0.3502
Meta loss on this task batch = 4.5152e-01, Meta loss averaged over last 500 steps = 4.5979e-01, PNorm = 60.7287, GNorm = 0.1573
Meta loss on this task batch = 3.9794e-01, Meta loss averaged over last 500 steps = 4.5963e-01, PNorm = 60.7455, GNorm = 0.0913
Meta loss on this task batch = 4.5693e-01, Meta loss averaged over last 500 steps = 4.5955e-01, PNorm = 60.7629, GNorm = 0.2310
Took 112.10053753852844 seconds to complete one epoch of meta training
Took 119.9435760974884 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453341
Found better MAML checkpoint after meta validation, saving now
Epoch 55
Meta loss on this task batch = 4.5177e-01, Meta loss averaged over last 500 steps = 4.5949e-01, PNorm = 60.7796, GNorm = 0.2686
Meta loss on this task batch = 4.8578e-01, Meta loss averaged over last 500 steps = 4.5950e-01, PNorm = 60.7952, GNorm = 0.2081
Meta loss on this task batch = 4.4268e-01, Meta loss averaged over last 500 steps = 4.5949e-01, PNorm = 60.8108, GNorm = 0.1322
Meta loss on this task batch = 4.0167e-01, Meta loss averaged over last 500 steps = 4.5933e-01, PNorm = 60.8272, GNorm = 0.1506
Meta loss on this task batch = 4.6221e-01, Meta loss averaged over last 500 steps = 4.5917e-01, PNorm = 60.8423, GNorm = 0.1084
Meta loss on this task batch = 4.7957e-01, Meta loss averaged over last 500 steps = 4.5913e-01, PNorm = 60.8566, GNorm = 0.1071
Meta loss on this task batch = 4.4387e-01, Meta loss averaged over last 500 steps = 4.5909e-01, PNorm = 60.8718, GNorm = 0.2709
Meta loss on this task batch = 4.1959e-01, Meta loss averaged over last 500 steps = 4.5882e-01, PNorm = 60.8863, GNorm = 0.1006
Meta loss on this task batch = 4.1451e-01, Meta loss averaged over last 500 steps = 4.5869e-01, PNorm = 60.8997, GNorm = 0.1006
Meta loss on this task batch = 4.2708e-01, Meta loss averaged over last 500 steps = 4.5855e-01, PNorm = 60.9149, GNorm = 0.3713
Meta loss on this task batch = 4.3379e-01, Meta loss averaged over last 500 steps = 4.5844e-01, PNorm = 60.9298, GNorm = 0.3303
Meta loss on this task batch = 4.1833e-01, Meta loss averaged over last 500 steps = 4.5840e-01, PNorm = 60.9448, GNorm = 0.1130
Meta loss on this task batch = 5.5321e-01, Meta loss averaged over last 500 steps = 4.5854e-01, PNorm = 60.9565, GNorm = 0.1980
Meta loss on this task batch = 4.4351e-01, Meta loss averaged over last 500 steps = 4.5847e-01, PNorm = 60.9674, GNorm = 0.1209
Meta loss on this task batch = 4.1646e-01, Meta loss averaged over last 500 steps = 4.5828e-01, PNorm = 60.9782, GNorm = 0.1247
Meta loss on this task batch = 4.1598e-01, Meta loss averaged over last 500 steps = 4.5818e-01, PNorm = 60.9885, GNorm = 0.2492
Meta loss on this task batch = 4.7252e-01, Meta loss averaged over last 500 steps = 4.5828e-01, PNorm = 60.9997, GNorm = 0.1746
Meta loss on this task batch = 4.6462e-01, Meta loss averaged over last 500 steps = 4.5833e-01, PNorm = 61.0109, GNorm = 0.3040
Meta loss on this task batch = 4.2818e-01, Meta loss averaged over last 500 steps = 4.5832e-01, PNorm = 61.0222, GNorm = 0.2592
Took 110.71343970298767 seconds to complete one epoch of meta training
Took 118.22118949890137 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455044
Found better MAML checkpoint after meta validation, saving now
Epoch 56
Meta loss on this task batch = 4.6043e-01, Meta loss averaged over last 500 steps = 4.5829e-01, PNorm = 61.0342, GNorm = 0.0863
Meta loss on this task batch = 3.9333e-01, Meta loss averaged over last 500 steps = 4.5813e-01, PNorm = 61.0485, GNorm = 0.1098
Meta loss on this task batch = 4.5777e-01, Meta loss averaged over last 500 steps = 4.5819e-01, PNorm = 61.0641, GNorm = 0.1509
Meta loss on this task batch = 3.7558e-01, Meta loss averaged over last 500 steps = 4.5807e-01, PNorm = 61.0808, GNorm = 0.1037
Meta loss on this task batch = 4.4717e-01, Meta loss averaged over last 500 steps = 4.5800e-01, PNorm = 61.0974, GNorm = 0.1459
Meta loss on this task batch = 4.7829e-01, Meta loss averaged over last 500 steps = 4.5817e-01, PNorm = 61.1140, GNorm = 0.1797
Meta loss on this task batch = 5.2906e-01, Meta loss averaged over last 500 steps = 4.5833e-01, PNorm = 61.1310, GNorm = 0.1323
Meta loss on this task batch = 4.4097e-01, Meta loss averaged over last 500 steps = 4.5822e-01, PNorm = 61.1489, GNorm = 0.2499
Meta loss on this task batch = 4.9516e-01, Meta loss averaged over last 500 steps = 4.5816e-01, PNorm = 61.1653, GNorm = 0.1204
Meta loss on this task batch = 4.1190e-01, Meta loss averaged over last 500 steps = 4.5820e-01, PNorm = 61.1817, GNorm = 0.3756
Meta loss on this task batch = 4.8617e-01, Meta loss averaged over last 500 steps = 4.5815e-01, PNorm = 61.1971, GNorm = 0.2552
Meta loss on this task batch = 4.4055e-01, Meta loss averaged over last 500 steps = 4.5794e-01, PNorm = 61.2126, GNorm = 0.1629
Meta loss on this task batch = 4.2360e-01, Meta loss averaged over last 500 steps = 4.5794e-01, PNorm = 61.2277, GNorm = 0.1060
Meta loss on this task batch = 4.4667e-01, Meta loss averaged over last 500 steps = 4.5781e-01, PNorm = 61.2430, GNorm = 0.1461
Meta loss on this task batch = 4.0631e-01, Meta loss averaged over last 500 steps = 4.5770e-01, PNorm = 61.2586, GNorm = 0.2527
Meta loss on this task batch = 3.8955e-01, Meta loss averaged over last 500 steps = 4.5750e-01, PNorm = 61.2736, GNorm = 0.2473
Meta loss on this task batch = 4.4106e-01, Meta loss averaged over last 500 steps = 4.5768e-01, PNorm = 61.2880, GNorm = 0.1832
Meta loss on this task batch = 4.3715e-01, Meta loss averaged over last 500 steps = 4.5752e-01, PNorm = 61.3011, GNorm = 0.3707
Meta loss on this task batch = 4.5897e-01, Meta loss averaged over last 500 steps = 4.5745e-01, PNorm = 61.3146, GNorm = 0.4845
Took 110.31715679168701 seconds to complete one epoch of meta training
Took 118.3160331249237 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454682
Epoch 57
Meta loss on this task batch = 4.4325e-01, Meta loss averaged over last 500 steps = 4.5750e-01, PNorm = 61.3275, GNorm = 0.3196
Meta loss on this task batch = 4.0971e-01, Meta loss averaged over last 500 steps = 4.5747e-01, PNorm = 61.3401, GNorm = 0.3664
Meta loss on this task batch = 4.3527e-01, Meta loss averaged over last 500 steps = 4.5739e-01, PNorm = 61.3537, GNorm = 0.1163
Meta loss on this task batch = 4.0838e-01, Meta loss averaged over last 500 steps = 4.5727e-01, PNorm = 61.3667, GNorm = 0.0817
Meta loss on this task batch = 4.3175e-01, Meta loss averaged over last 500 steps = 4.5717e-01, PNorm = 61.3797, GNorm = 0.1130
Meta loss on this task batch = 5.1563e-01, Meta loss averaged over last 500 steps = 4.5720e-01, PNorm = 61.3932, GNorm = 0.2093
Meta loss on this task batch = 4.4490e-01, Meta loss averaged over last 500 steps = 4.5712e-01, PNorm = 61.4065, GNorm = 0.2140
Meta loss on this task batch = 4.8134e-01, Meta loss averaged over last 500 steps = 4.5723e-01, PNorm = 61.4188, GNorm = 0.1141
Meta loss on this task batch = 4.1462e-01, Meta loss averaged over last 500 steps = 4.5703e-01, PNorm = 61.4327, GNorm = 0.1532
Meta loss on this task batch = 4.4156e-01, Meta loss averaged over last 500 steps = 4.5705e-01, PNorm = 61.4468, GNorm = 0.1660
Meta loss on this task batch = 4.4816e-01, Meta loss averaged over last 500 steps = 4.5693e-01, PNorm = 61.4610, GNorm = 0.2242
Meta loss on this task batch = 3.9932e-01, Meta loss averaged over last 500 steps = 4.5678e-01, PNorm = 61.4752, GNorm = 0.1373
Meta loss on this task batch = 3.6618e-01, Meta loss averaged over last 500 steps = 4.5653e-01, PNorm = 61.4903, GNorm = 0.1292
Meta loss on this task batch = 4.1766e-01, Meta loss averaged over last 500 steps = 4.5660e-01, PNorm = 61.5063, GNorm = 0.1452
Meta loss on this task batch = 4.4624e-01, Meta loss averaged over last 500 steps = 4.5660e-01, PNorm = 61.5208, GNorm = 0.2216
Meta loss on this task batch = 5.4729e-01, Meta loss averaged over last 500 steps = 4.5677e-01, PNorm = 61.5329, GNorm = 0.1624
Meta loss on this task batch = 4.1118e-01, Meta loss averaged over last 500 steps = 4.5662e-01, PNorm = 61.5456, GNorm = 0.1303
Meta loss on this task batch = 4.8121e-01, Meta loss averaged over last 500 steps = 4.5658e-01, PNorm = 61.5575, GNorm = 0.3244
Meta loss on this task batch = 4.7903e-01, Meta loss averaged over last 500 steps = 4.5665e-01, PNorm = 61.5718, GNorm = 0.1798
Took 111.7003185749054 seconds to complete one epoch of meta training
Took 119.5441153049469 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464661
Found better MAML checkpoint after meta validation, saving now
Epoch 58
Meta loss on this task batch = 4.3999e-01, Meta loss averaged over last 500 steps = 4.5639e-01, PNorm = 61.5875, GNorm = 0.2005
Meta loss on this task batch = 3.9735e-01, Meta loss averaged over last 500 steps = 4.5643e-01, PNorm = 61.6032, GNorm = 0.1490
Meta loss on this task batch = 4.5333e-01, Meta loss averaged over last 500 steps = 4.5628e-01, PNorm = 61.6192, GNorm = 0.1526
Meta loss on this task batch = 4.5004e-01, Meta loss averaged over last 500 steps = 4.5626e-01, PNorm = 61.6368, GNorm = 0.1637
Meta loss on this task batch = 5.1845e-01, Meta loss averaged over last 500 steps = 4.5626e-01, PNorm = 61.6517, GNorm = 0.1446
Meta loss on this task batch = 4.2569e-01, Meta loss averaged over last 500 steps = 4.5626e-01, PNorm = 61.6670, GNorm = 0.0943
Meta loss on this task batch = 4.5871e-01, Meta loss averaged over last 500 steps = 4.5630e-01, PNorm = 61.6809, GNorm = 0.1446
Meta loss on this task batch = 5.2858e-01, Meta loss averaged over last 500 steps = 4.5633e-01, PNorm = 61.6913, GNorm = 0.1585
Meta loss on this task batch = 3.9811e-01, Meta loss averaged over last 500 steps = 4.5614e-01, PNorm = 61.7037, GNorm = 0.2773
Meta loss on this task batch = 3.8926e-01, Meta loss averaged over last 500 steps = 4.5593e-01, PNorm = 61.7175, GNorm = 0.1217
Meta loss on this task batch = 3.8449e-01, Meta loss averaged over last 500 steps = 4.5569e-01, PNorm = 61.7332, GNorm = 0.1200
Meta loss on this task batch = 4.1478e-01, Meta loss averaged over last 500 steps = 4.5550e-01, PNorm = 61.7501, GNorm = 0.1180
Meta loss on this task batch = 4.3982e-01, Meta loss averaged over last 500 steps = 4.5539e-01, PNorm = 61.7646, GNorm = 0.2908
Meta loss on this task batch = 4.5809e-01, Meta loss averaged over last 500 steps = 4.5543e-01, PNorm = 61.7788, GNorm = 0.1884
Meta loss on this task batch = 4.2531e-01, Meta loss averaged over last 500 steps = 4.5542e-01, PNorm = 61.7939, GNorm = 0.2270
Meta loss on this task batch = 4.8330e-01, Meta loss averaged over last 500 steps = 4.5544e-01, PNorm = 61.8072, GNorm = 0.2488
Meta loss on this task batch = 4.5145e-01, Meta loss averaged over last 500 steps = 4.5547e-01, PNorm = 61.8200, GNorm = 0.3807
Meta loss on this task batch = 3.7272e-01, Meta loss averaged over last 500 steps = 4.5529e-01, PNorm = 61.8338, GNorm = 0.1351
Meta loss on this task batch = 4.7573e-01, Meta loss averaged over last 500 steps = 4.5532e-01, PNorm = 61.8469, GNorm = 0.2362
Took 111.42297601699829 seconds to complete one epoch of meta training
Took 119.41977739334106 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470348
Found better MAML checkpoint after meta validation, saving now
Epoch 59
Meta loss on this task batch = 4.2366e-01, Meta loss averaged over last 500 steps = 4.5532e-01, PNorm = 61.8602, GNorm = 0.2095
Meta loss on this task batch = 4.5389e-01, Meta loss averaged over last 500 steps = 4.5529e-01, PNorm = 61.8735, GNorm = 0.1233
Meta loss on this task batch = 5.1620e-01, Meta loss averaged over last 500 steps = 4.5533e-01, PNorm = 61.8856, GNorm = 0.1486
Meta loss on this task batch = 4.5405e-01, Meta loss averaged over last 500 steps = 4.5540e-01, PNorm = 61.8967, GNorm = 0.1229
Meta loss on this task batch = 4.1028e-01, Meta loss averaged over last 500 steps = 4.5533e-01, PNorm = 61.9083, GNorm = 0.2933
Meta loss on this task batch = 4.8663e-01, Meta loss averaged over last 500 steps = 4.5537e-01, PNorm = 61.9199, GNorm = 0.2927
Meta loss on this task batch = 4.0068e-01, Meta loss averaged over last 500 steps = 4.5516e-01, PNorm = 61.9337, GNorm = 0.1544
Meta loss on this task batch = 4.9783e-01, Meta loss averaged over last 500 steps = 4.5517e-01, PNorm = 61.9486, GNorm = 0.1160
Meta loss on this task batch = 4.3470e-01, Meta loss averaged over last 500 steps = 4.5510e-01, PNorm = 61.9649, GNorm = 0.2968
Meta loss on this task batch = 3.8142e-01, Meta loss averaged over last 500 steps = 4.5504e-01, PNorm = 61.9814, GNorm = 0.1330
Meta loss on this task batch = 4.3766e-01, Meta loss averaged over last 500 steps = 4.5494e-01, PNorm = 61.9988, GNorm = 0.4593
Meta loss on this task batch = 4.2006e-01, Meta loss averaged over last 500 steps = 4.5493e-01, PNorm = 62.0157, GNorm = 0.2592
Meta loss on this task batch = 4.0647e-01, Meta loss averaged over last 500 steps = 4.5469e-01, PNorm = 62.0315, GNorm = 0.2052
Meta loss on this task batch = 4.5097e-01, Meta loss averaged over last 500 steps = 4.5469e-01, PNorm = 62.0453, GNorm = 0.3530
Meta loss on this task batch = 4.3291e-01, Meta loss averaged over last 500 steps = 4.5473e-01, PNorm = 62.0579, GNorm = 0.1188
Meta loss on this task batch = 4.1407e-01, Meta loss averaged over last 500 steps = 4.5466e-01, PNorm = 62.0709, GNorm = 0.1283
Meta loss on this task batch = 4.3853e-01, Meta loss averaged over last 500 steps = 4.5458e-01, PNorm = 62.0839, GNorm = 0.1290
Meta loss on this task batch = 4.8673e-01, Meta loss averaged over last 500 steps = 4.5464e-01, PNorm = 62.0964, GNorm = 0.2846
Meta loss on this task batch = 4.1749e-01, Meta loss averaged over last 500 steps = 4.5451e-01, PNorm = 62.1092, GNorm = 0.1586
Took 108.21690821647644 seconds to complete one epoch of meta training
Took 115.85641551017761 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459166
Epoch 60
Meta loss on this task batch = 4.4760e-01, Meta loss averaged over last 500 steps = 4.5454e-01, PNorm = 62.1210, GNorm = 0.1433
Meta loss on this task batch = 4.4447e-01, Meta loss averaged over last 500 steps = 4.5450e-01, PNorm = 62.1310, GNorm = 0.2068
Meta loss on this task batch = 3.8707e-01, Meta loss averaged over last 500 steps = 4.5446e-01, PNorm = 62.1401, GNorm = 0.2207
Meta loss on this task batch = 3.9782e-01, Meta loss averaged over last 500 steps = 4.5418e-01, PNorm = 62.1491, GNorm = 0.2682
Meta loss on this task batch = 4.0567e-01, Meta loss averaged over last 500 steps = 4.5397e-01, PNorm = 62.1600, GNorm = 0.1278
Meta loss on this task batch = 4.6088e-01, Meta loss averaged over last 500 steps = 4.5401e-01, PNorm = 62.1703, GNorm = 0.2491
Meta loss on this task batch = 3.8301e-01, Meta loss averaged over last 500 steps = 4.5387e-01, PNorm = 62.1819, GNorm = 0.1098
Meta loss on this task batch = 4.3516e-01, Meta loss averaged over last 500 steps = 4.5376e-01, PNorm = 62.1933, GNorm = 0.1477
Meta loss on this task batch = 4.5628e-01, Meta loss averaged over last 500 steps = 4.5359e-01, PNorm = 62.2058, GNorm = 0.1481
Meta loss on this task batch = 4.5990e-01, Meta loss averaged over last 500 steps = 4.5357e-01, PNorm = 62.2196, GNorm = 0.2309
Meta loss on this task batch = 4.4150e-01, Meta loss averaged over last 500 steps = 4.5351e-01, PNorm = 62.2330, GNorm = 0.1703
Meta loss on this task batch = 4.5514e-01, Meta loss averaged over last 500 steps = 4.5346e-01, PNorm = 62.2469, GNorm = 0.1363
Meta loss on this task batch = 4.9194e-01, Meta loss averaged over last 500 steps = 4.5350e-01, PNorm = 62.2613, GNorm = 0.1902
Meta loss on this task batch = 5.2685e-01, Meta loss averaged over last 500 steps = 4.5366e-01, PNorm = 62.2750, GNorm = 0.5665
Meta loss on this task batch = 4.8705e-01, Meta loss averaged over last 500 steps = 4.5365e-01, PNorm = 62.2907, GNorm = 0.5439
Meta loss on this task batch = 4.4011e-01, Meta loss averaged over last 500 steps = 4.5362e-01, PNorm = 62.3080, GNorm = 0.1404
Meta loss on this task batch = 3.9309e-01, Meta loss averaged over last 500 steps = 4.5340e-01, PNorm = 62.3259, GNorm = 0.1441
Meta loss on this task batch = 4.1717e-01, Meta loss averaged over last 500 steps = 4.5332e-01, PNorm = 62.3436, GNorm = 0.2841
Meta loss on this task batch = 4.6949e-01, Meta loss averaged over last 500 steps = 4.5331e-01, PNorm = 62.3598, GNorm = 0.2601
Took 110.25589179992676 seconds to complete one epoch of meta training
Took 117.35557675361633 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447021
Epoch 61
Meta loss on this task batch = 4.8804e-01, Meta loss averaged over last 500 steps = 4.5347e-01, PNorm = 62.3757, GNorm = 0.1852
Meta loss on this task batch = 4.6715e-01, Meta loss averaged over last 500 steps = 4.5356e-01, PNorm = 62.3916, GNorm = 0.2168
Meta loss on this task batch = 4.1744e-01, Meta loss averaged over last 500 steps = 4.5362e-01, PNorm = 62.4080, GNorm = 0.1097
Meta loss on this task batch = 4.8724e-01, Meta loss averaged over last 500 steps = 4.5367e-01, PNorm = 62.4248, GNorm = 0.1177
Meta loss on this task batch = 4.2647e-01, Meta loss averaged over last 500 steps = 4.5368e-01, PNorm = 62.4426, GNorm = 0.1536
Meta loss on this task batch = 4.4177e-01, Meta loss averaged over last 500 steps = 4.5361e-01, PNorm = 62.4596, GNorm = 0.1117
Meta loss on this task batch = 4.5939e-01, Meta loss averaged over last 500 steps = 4.5360e-01, PNorm = 62.4760, GNorm = 0.1410
Meta loss on this task batch = 4.2811e-01, Meta loss averaged over last 500 steps = 4.5344e-01, PNorm = 62.4911, GNorm = 0.2287
Meta loss on this task batch = 4.3241e-01, Meta loss averaged over last 500 steps = 4.5337e-01, PNorm = 62.5053, GNorm = 0.3621
Meta loss on this task batch = 4.2693e-01, Meta loss averaged over last 500 steps = 4.5327e-01, PNorm = 62.5195, GNorm = 0.3363
Meta loss on this task batch = 4.5358e-01, Meta loss averaged over last 500 steps = 4.5337e-01, PNorm = 62.5344, GNorm = 0.2006
Meta loss on this task batch = 3.7813e-01, Meta loss averaged over last 500 steps = 4.5322e-01, PNorm = 62.5506, GNorm = 0.1970
Meta loss on this task batch = 4.7360e-01, Meta loss averaged over last 500 steps = 4.5327e-01, PNorm = 62.5678, GNorm = 0.1482
Meta loss on this task batch = 4.9940e-01, Meta loss averaged over last 500 steps = 4.5341e-01, PNorm = 62.5859, GNorm = 0.2212
Meta loss on this task batch = 4.2872e-01, Meta loss averaged over last 500 steps = 4.5326e-01, PNorm = 62.6042, GNorm = 0.1715
Meta loss on this task batch = 4.5372e-01, Meta loss averaged over last 500 steps = 4.5326e-01, PNorm = 62.6216, GNorm = 0.1563
Meta loss on this task batch = 4.2468e-01, Meta loss averaged over last 500 steps = 4.5314e-01, PNorm = 62.6382, GNorm = 0.1280
Meta loss on this task batch = 4.1729e-01, Meta loss averaged over last 500 steps = 4.5313e-01, PNorm = 62.6549, GNorm = 0.1219
Meta loss on this task batch = 4.8607e-01, Meta loss averaged over last 500 steps = 4.5312e-01, PNorm = 62.6716, GNorm = 0.1580
Took 113.66129803657532 seconds to complete one epoch of meta training
Took 121.96322083473206 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461227
Epoch 62
Meta loss on this task batch = 4.5836e-01, Meta loss averaged over last 500 steps = 4.5321e-01, PNorm = 62.6886, GNorm = 0.1561
Meta loss on this task batch = 4.8088e-01, Meta loss averaged over last 500 steps = 4.5326e-01, PNorm = 62.7048, GNorm = 0.1717
Meta loss on this task batch = 4.1065e-01, Meta loss averaged over last 500 steps = 4.5324e-01, PNorm = 62.7198, GNorm = 0.1379
Meta loss on this task batch = 4.2354e-01, Meta loss averaged over last 500 steps = 4.5315e-01, PNorm = 62.7364, GNorm = 0.5460
Meta loss on this task batch = 4.8451e-01, Meta loss averaged over last 500 steps = 4.5325e-01, PNorm = 62.7547, GNorm = 0.5115
Meta loss on this task batch = 4.6304e-01, Meta loss averaged over last 500 steps = 4.5322e-01, PNorm = 62.7735, GNorm = 0.2231
Meta loss on this task batch = 4.2029e-01, Meta loss averaged over last 500 steps = 4.5316e-01, PNorm = 62.7920, GNorm = 0.1428
Meta loss on this task batch = 4.1076e-01, Meta loss averaged over last 500 steps = 4.5307e-01, PNorm = 62.8110, GNorm = 0.1353
Meta loss on this task batch = 3.9761e-01, Meta loss averaged over last 500 steps = 4.5304e-01, PNorm = 62.8290, GNorm = 0.1896
Meta loss on this task batch = 4.6236e-01, Meta loss averaged over last 500 steps = 4.5305e-01, PNorm = 62.8468, GNorm = 0.1161
Meta loss on this task batch = 4.5750e-01, Meta loss averaged over last 500 steps = 4.5309e-01, PNorm = 62.8640, GNorm = 0.1407
Meta loss on this task batch = 4.8665e-01, Meta loss averaged over last 500 steps = 4.5311e-01, PNorm = 62.8811, GNorm = 0.1430
Meta loss on this task batch = 4.3944e-01, Meta loss averaged over last 500 steps = 4.5305e-01, PNorm = 62.8965, GNorm = 0.1370
Meta loss on this task batch = 4.5325e-01, Meta loss averaged over last 500 steps = 4.5308e-01, PNorm = 62.9109, GNorm = 0.4359
Meta loss on this task batch = 4.8939e-01, Meta loss averaged over last 500 steps = 4.5312e-01, PNorm = 62.9252, GNorm = 0.1030
Meta loss on this task batch = 4.5701e-01, Meta loss averaged over last 500 steps = 4.5299e-01, PNorm = 62.9401, GNorm = 0.2587
Meta loss on this task batch = 3.7957e-01, Meta loss averaged over last 500 steps = 4.5273e-01, PNorm = 62.9554, GNorm = 0.3331
Meta loss on this task batch = 4.9551e-01, Meta loss averaged over last 500 steps = 4.5271e-01, PNorm = 62.9699, GNorm = 0.2384
Meta loss on this task batch = 4.6748e-01, Meta loss averaged over last 500 steps = 4.5286e-01, PNorm = 62.9857, GNorm = 0.1876
Took 112.71982264518738 seconds to complete one epoch of meta training
Took 120.425119638443 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447726
Epoch 63
Meta loss on this task batch = 4.3636e-01, Meta loss averaged over last 500 steps = 4.5278e-01, PNorm = 63.0008, GNorm = 0.2980
Meta loss on this task batch = 3.8222e-01, Meta loss averaged over last 500 steps = 4.5270e-01, PNorm = 63.0183, GNorm = 0.1622
Meta loss on this task batch = 4.2859e-01, Meta loss averaged over last 500 steps = 4.5261e-01, PNorm = 63.0354, GNorm = 0.1674
Meta loss on this task batch = 4.0855e-01, Meta loss averaged over last 500 steps = 4.5246e-01, PNorm = 63.0540, GNorm = 0.1621
Meta loss on this task batch = 4.0900e-01, Meta loss averaged over last 500 steps = 4.5237e-01, PNorm = 63.0739, GNorm = 0.2318
Meta loss on this task batch = 4.4587e-01, Meta loss averaged over last 500 steps = 4.5254e-01, PNorm = 63.0931, GNorm = 0.1781
Meta loss on this task batch = 4.4805e-01, Meta loss averaged over last 500 steps = 4.5258e-01, PNorm = 63.1111, GNorm = 0.1625
Meta loss on this task batch = 4.8220e-01, Meta loss averaged over last 500 steps = 4.5269e-01, PNorm = 63.1280, GNorm = 0.2235
Meta loss on this task batch = 4.9454e-01, Meta loss averaged over last 500 steps = 4.5277e-01, PNorm = 63.1429, GNorm = 0.3093
Meta loss on this task batch = 4.0092e-01, Meta loss averaged over last 500 steps = 4.5269e-01, PNorm = 63.1567, GNorm = 0.3071
Meta loss on this task batch = 4.4494e-01, Meta loss averaged over last 500 steps = 4.5270e-01, PNorm = 63.1698, GNorm = 0.2080
Meta loss on this task batch = 3.9932e-01, Meta loss averaged over last 500 steps = 4.5267e-01, PNorm = 63.1830, GNorm = 0.1756
Meta loss on this task batch = 4.2325e-01, Meta loss averaged over last 500 steps = 4.5254e-01, PNorm = 63.1987, GNorm = 0.4952
Meta loss on this task batch = 4.4524e-01, Meta loss averaged over last 500 steps = 4.5257e-01, PNorm = 63.2136, GNorm = 0.2078
Meta loss on this task batch = 4.8658e-01, Meta loss averaged over last 500 steps = 4.5269e-01, PNorm = 63.2268, GNorm = 0.2449
Meta loss on this task batch = 4.3276e-01, Meta loss averaged over last 500 steps = 4.5263e-01, PNorm = 63.2404, GNorm = 0.1019
Meta loss on this task batch = 4.5715e-01, Meta loss averaged over last 500 steps = 4.5279e-01, PNorm = 63.2536, GNorm = 0.1762
Meta loss on this task batch = 4.5381e-01, Meta loss averaged over last 500 steps = 4.5263e-01, PNorm = 63.2665, GNorm = 0.1340
Meta loss on this task batch = 4.3760e-01, Meta loss averaged over last 500 steps = 4.5240e-01, PNorm = 63.2807, GNorm = 0.2093
Took 112.07033348083496 seconds to complete one epoch of meta training
Took 119.72200274467468 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446806
Epoch 64
Meta loss on this task batch = 4.4034e-01, Meta loss averaged over last 500 steps = 4.5230e-01, PNorm = 63.2951, GNorm = 0.2537
Meta loss on this task batch = 4.3365e-01, Meta loss averaged over last 500 steps = 4.5225e-01, PNorm = 63.3101, GNorm = 0.0985
Meta loss on this task batch = 4.3921e-01, Meta loss averaged over last 500 steps = 4.5227e-01, PNorm = 63.3253, GNorm = 0.1502
Meta loss on this task batch = 4.1452e-01, Meta loss averaged over last 500 steps = 4.5210e-01, PNorm = 63.3415, GNorm = 0.1780
Meta loss on this task batch = 4.1474e-01, Meta loss averaged over last 500 steps = 4.5200e-01, PNorm = 63.3580, GNorm = 0.1693
Meta loss on this task batch = 4.1058e-01, Meta loss averaged over last 500 steps = 4.5180e-01, PNorm = 63.3749, GNorm = 0.2696
Meta loss on this task batch = 4.6944e-01, Meta loss averaged over last 500 steps = 4.5165e-01, PNorm = 63.3914, GNorm = 0.1734
Meta loss on this task batch = 4.1294e-01, Meta loss averaged over last 500 steps = 4.5159e-01, PNorm = 63.4084, GNorm = 0.1264
Meta loss on this task batch = 4.6457e-01, Meta loss averaged over last 500 steps = 4.5160e-01, PNorm = 63.4247, GNorm = 0.2610
Meta loss on this task batch = 3.8664e-01, Meta loss averaged over last 500 steps = 4.5143e-01, PNorm = 63.4400, GNorm = 0.1300
Meta loss on this task batch = 4.6721e-01, Meta loss averaged over last 500 steps = 4.5148e-01, PNorm = 63.4546, GNorm = 0.2533
Meta loss on this task batch = 4.0719e-01, Meta loss averaged over last 500 steps = 4.5137e-01, PNorm = 63.4693, GNorm = 0.1510
Meta loss on this task batch = 4.3297e-01, Meta loss averaged over last 500 steps = 4.5133e-01, PNorm = 63.4838, GNorm = 0.2038
Meta loss on this task batch = 4.7629e-01, Meta loss averaged over last 500 steps = 4.5132e-01, PNorm = 63.4956, GNorm = 0.1865
Meta loss on this task batch = 4.5498e-01, Meta loss averaged over last 500 steps = 4.5118e-01, PNorm = 63.5069, GNorm = 0.1349
Meta loss on this task batch = 3.7042e-01, Meta loss averaged over last 500 steps = 4.5091e-01, PNorm = 63.5189, GNorm = 0.2737
Meta loss on this task batch = 5.3020e-01, Meta loss averaged over last 500 steps = 4.5116e-01, PNorm = 63.5285, GNorm = 0.4825
Meta loss on this task batch = 4.6625e-01, Meta loss averaged over last 500 steps = 4.5123e-01, PNorm = 63.5374, GNorm = 0.2028
Meta loss on this task batch = 4.2084e-01, Meta loss averaged over last 500 steps = 4.5113e-01, PNorm = 63.5483, GNorm = 0.1408
Took 111.35060954093933 seconds to complete one epoch of meta training
Took 118.03998351097107 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470065
Epoch 65
Meta loss on this task batch = 4.5588e-01, Meta loss averaged over last 500 steps = 4.5120e-01, PNorm = 63.5605, GNorm = 0.2017
Meta loss on this task batch = 4.7338e-01, Meta loss averaged over last 500 steps = 4.5114e-01, PNorm = 63.5733, GNorm = 0.2869
Meta loss on this task batch = 3.9080e-01, Meta loss averaged over last 500 steps = 4.5097e-01, PNorm = 63.5877, GNorm = 0.1250
Meta loss on this task batch = 4.3993e-01, Meta loss averaged over last 500 steps = 4.5092e-01, PNorm = 63.6021, GNorm = 0.1350
Meta loss on this task batch = 4.2902e-01, Meta loss averaged over last 500 steps = 4.5084e-01, PNorm = 63.6163, GNorm = 0.0995
Meta loss on this task batch = 4.8283e-01, Meta loss averaged over last 500 steps = 4.5088e-01, PNorm = 63.6301, GNorm = 0.2202
Meta loss on this task batch = 4.1723e-01, Meta loss averaged over last 500 steps = 4.5084e-01, PNorm = 63.6474, GNorm = 0.2923
Meta loss on this task batch = 4.8243e-01, Meta loss averaged over last 500 steps = 4.5079e-01, PNorm = 63.6635, GNorm = 0.1988
Meta loss on this task batch = 3.8887e-01, Meta loss averaged over last 500 steps = 4.5070e-01, PNorm = 63.6784, GNorm = 0.1575
Meta loss on this task batch = 3.7160e-01, Meta loss averaged over last 500 steps = 4.5048e-01, PNorm = 63.6961, GNorm = 0.2209
Meta loss on this task batch = 4.8058e-01, Meta loss averaged over last 500 steps = 4.5053e-01, PNorm = 63.7144, GNorm = 0.3259
Meta loss on this task batch = 4.6165e-01, Meta loss averaged over last 500 steps = 4.5044e-01, PNorm = 63.7325, GNorm = 0.1515
Meta loss on this task batch = 4.3219e-01, Meta loss averaged over last 500 steps = 4.5039e-01, PNorm = 63.7506, GNorm = 0.2840
Meta loss on this task batch = 4.0943e-01, Meta loss averaged over last 500 steps = 4.5028e-01, PNorm = 63.7696, GNorm = 0.3845
Meta loss on this task batch = 4.1632e-01, Meta loss averaged over last 500 steps = 4.5019e-01, PNorm = 63.7877, GNorm = 0.1227
Meta loss on this task batch = 4.1233e-01, Meta loss averaged over last 500 steps = 4.4997e-01, PNorm = 63.8047, GNorm = 0.3768
Meta loss on this task batch = 4.8690e-01, Meta loss averaged over last 500 steps = 4.5020e-01, PNorm = 63.8211, GNorm = 0.2923
Meta loss on this task batch = 4.3759e-01, Meta loss averaged over last 500 steps = 4.5008e-01, PNorm = 63.8378, GNorm = 0.2245
Meta loss on this task batch = 3.6735e-01, Meta loss averaged over last 500 steps = 4.4994e-01, PNorm = 63.8547, GNorm = 0.3300
Took 111.58889651298523 seconds to complete one epoch of meta training
Took 119.26358985900879 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460626
Epoch 66
Meta loss on this task batch = 4.7289e-01, Meta loss averaged over last 500 steps = 4.5003e-01, PNorm = 63.8683, GNorm = 0.2730
Meta loss on this task batch = 4.3638e-01, Meta loss averaged over last 500 steps = 4.4992e-01, PNorm = 63.8809, GNorm = 0.2308
Meta loss on this task batch = 4.3732e-01, Meta loss averaged over last 500 steps = 4.4985e-01, PNorm = 63.8909, GNorm = 0.4384
Meta loss on this task batch = 4.5938e-01, Meta loss averaged over last 500 steps = 4.4977e-01, PNorm = 63.9016, GNorm = 0.4022
Meta loss on this task batch = 5.1770e-01, Meta loss averaged over last 500 steps = 4.4990e-01, PNorm = 63.9079, GNorm = 0.8193
Meta loss on this task batch = 4.9054e-01, Meta loss averaged over last 500 steps = 4.4994e-01, PNorm = 63.9169, GNorm = 0.3995
Meta loss on this task batch = 4.6536e-01, Meta loss averaged over last 500 steps = 4.5001e-01, PNorm = 63.9289, GNorm = 0.2222
Meta loss on this task batch = 4.2647e-01, Meta loss averaged over last 500 steps = 4.4992e-01, PNorm = 63.9445, GNorm = 0.1334
Meta loss on this task batch = 3.8845e-01, Meta loss averaged over last 500 steps = 4.4973e-01, PNorm = 63.9631, GNorm = 0.2126
Meta loss on this task batch = 4.1873e-01, Meta loss averaged over last 500 steps = 4.4960e-01, PNorm = 63.9842, GNorm = 0.2114
Meta loss on this task batch = 4.2589e-01, Meta loss averaged over last 500 steps = 4.4957e-01, PNorm = 64.0072, GNorm = 0.1645
Meta loss on this task batch = 4.7709e-01, Meta loss averaged over last 500 steps = 4.4952e-01, PNorm = 64.0311, GNorm = 0.3321
Meta loss on this task batch = 4.3123e-01, Meta loss averaged over last 500 steps = 4.4949e-01, PNorm = 64.0551, GNorm = 0.1895
Meta loss on this task batch = 4.6464e-01, Meta loss averaged over last 500 steps = 4.4956e-01, PNorm = 64.0786, GNorm = 0.1299
Meta loss on this task batch = 4.8622e-01, Meta loss averaged over last 500 steps = 4.4956e-01, PNorm = 64.1020, GNorm = 0.1940
Meta loss on this task batch = 4.4741e-01, Meta loss averaged over last 500 steps = 4.4942e-01, PNorm = 64.1243, GNorm = 0.1279
Meta loss on this task batch = 4.9757e-01, Meta loss averaged over last 500 steps = 4.4951e-01, PNorm = 64.1439, GNorm = 0.2057
Meta loss on this task batch = 5.0723e-01, Meta loss averaged over last 500 steps = 4.4962e-01, PNorm = 64.1626, GNorm = 0.1843
Meta loss on this task batch = 4.4369e-01, Meta loss averaged over last 500 steps = 4.4959e-01, PNorm = 64.1816, GNorm = 0.5714
Took 110.68307757377625 seconds to complete one epoch of meta training
Took 118.12602162361145 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458725
Epoch 67
Meta loss on this task batch = 3.9575e-01, Meta loss averaged over last 500 steps = 4.4955e-01, PNorm = 64.2006, GNorm = 0.0967
Meta loss on this task batch = 3.9495e-01, Meta loss averaged over last 500 steps = 4.4941e-01, PNorm = 64.2202, GNorm = 0.1526
Meta loss on this task batch = 4.2161e-01, Meta loss averaged over last 500 steps = 4.4942e-01, PNorm = 64.2395, GNorm = 0.1691
Meta loss on this task batch = 4.0675e-01, Meta loss averaged over last 500 steps = 4.4930e-01, PNorm = 64.2601, GNorm = 0.2652
Meta loss on this task batch = 4.4008e-01, Meta loss averaged over last 500 steps = 4.4929e-01, PNorm = 64.2800, GNorm = 0.1138
Meta loss on this task batch = 4.1512e-01, Meta loss averaged over last 500 steps = 4.4919e-01, PNorm = 64.3004, GNorm = 0.2676
Meta loss on this task batch = 4.9985e-01, Meta loss averaged over last 500 steps = 4.4927e-01, PNorm = 64.3197, GNorm = 0.2261
Meta loss on this task batch = 4.8180e-01, Meta loss averaged over last 500 steps = 4.4926e-01, PNorm = 64.3375, GNorm = 0.1678
Meta loss on this task batch = 4.0289e-01, Meta loss averaged over last 500 steps = 4.4910e-01, PNorm = 64.3537, GNorm = 0.0903
Meta loss on this task batch = 4.8463e-01, Meta loss averaged over last 500 steps = 4.4933e-01, PNorm = 64.3690, GNorm = 0.1847
Meta loss on this task batch = 4.4623e-01, Meta loss averaged over last 500 steps = 4.4933e-01, PNorm = 64.3837, GNorm = 0.1293
Meta loss on this task batch = 4.7803e-01, Meta loss averaged over last 500 steps = 4.4941e-01, PNorm = 64.3979, GNorm = 0.1455
Meta loss on this task batch = 4.6904e-01, Meta loss averaged over last 500 steps = 4.4941e-01, PNorm = 64.4125, GNorm = 0.1432
Meta loss on this task batch = 4.6986e-01, Meta loss averaged over last 500 steps = 4.4931e-01, PNorm = 64.4274, GNorm = 0.1947
Meta loss on this task batch = 4.8115e-01, Meta loss averaged over last 500 steps = 4.4936e-01, PNorm = 64.4426, GNorm = 0.1347
Meta loss on this task batch = 4.5465e-01, Meta loss averaged over last 500 steps = 4.4944e-01, PNorm = 64.4580, GNorm = 0.2628
Meta loss on this task batch = 4.9082e-01, Meta loss averaged over last 500 steps = 4.4947e-01, PNorm = 64.4733, GNorm = 0.1044
Meta loss on this task batch = 4.4214e-01, Meta loss averaged over last 500 steps = 4.4938e-01, PNorm = 64.4884, GNorm = 0.1119
Meta loss on this task batch = 4.7666e-01, Meta loss averaged over last 500 steps = 4.4949e-01, PNorm = 64.5035, GNorm = 0.3246
Took 109.31424164772034 seconds to complete one epoch of meta training
Took 117.10253810882568 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449241
Epoch 68
Meta loss on this task batch = 4.5461e-01, Meta loss averaged over last 500 steps = 4.4942e-01, PNorm = 64.5189, GNorm = 0.1110
Meta loss on this task batch = 4.9841e-01, Meta loss averaged over last 500 steps = 4.4935e-01, PNorm = 64.5339, GNorm = 0.4459
Meta loss on this task batch = 4.5183e-01, Meta loss averaged over last 500 steps = 4.4940e-01, PNorm = 64.5490, GNorm = 0.2782
Meta loss on this task batch = 3.8917e-01, Meta loss averaged over last 500 steps = 4.4932e-01, PNorm = 64.5644, GNorm = 0.2067
Meta loss on this task batch = 4.3714e-01, Meta loss averaged over last 500 steps = 4.4937e-01, PNorm = 64.5808, GNorm = 0.1357
Meta loss on this task batch = 4.2951e-01, Meta loss averaged over last 500 steps = 4.4913e-01, PNorm = 64.5990, GNorm = 0.2497
Meta loss on this task batch = 4.3899e-01, Meta loss averaged over last 500 steps = 4.4915e-01, PNorm = 64.6176, GNorm = 0.1373
Meta loss on this task batch = 4.5594e-01, Meta loss averaged over last 500 steps = 4.4907e-01, PNorm = 64.6364, GNorm = 0.1201
Meta loss on this task batch = 4.1328e-01, Meta loss averaged over last 500 steps = 4.4890e-01, PNorm = 64.6546, GNorm = 0.1529
Meta loss on this task batch = 3.7962e-01, Meta loss averaged over last 500 steps = 4.4890e-01, PNorm = 64.6716, GNorm = 0.1617
Meta loss on this task batch = 3.8755e-01, Meta loss averaged over last 500 steps = 4.4888e-01, PNorm = 64.6885, GNorm = 0.0998
Meta loss on this task batch = 4.6087e-01, Meta loss averaged over last 500 steps = 4.4887e-01, PNorm = 64.7070, GNorm = 0.1620
Meta loss on this task batch = 4.7388e-01, Meta loss averaged over last 500 steps = 4.4906e-01, PNorm = 64.7256, GNorm = 0.1340
Meta loss on this task batch = 4.5141e-01, Meta loss averaged over last 500 steps = 4.4901e-01, PNorm = 64.7432, GNorm = 0.1395
Meta loss on this task batch = 4.5440e-01, Meta loss averaged over last 500 steps = 4.4895e-01, PNorm = 64.7557, GNorm = 0.4882
Meta loss on this task batch = 4.5449e-01, Meta loss averaged over last 500 steps = 4.4886e-01, PNorm = 64.7670, GNorm = 0.1645
Meta loss on this task batch = 4.7582e-01, Meta loss averaged over last 500 steps = 4.4894e-01, PNorm = 64.7769, GNorm = 0.1876
Meta loss on this task batch = 4.2611e-01, Meta loss averaged over last 500 steps = 4.4879e-01, PNorm = 64.7885, GNorm = 0.1795
Meta loss on this task batch = 4.8119e-01, Meta loss averaged over last 500 steps = 4.4887e-01, PNorm = 64.8014, GNorm = 0.1860
Took 112.93965148925781 seconds to complete one epoch of meta training
Took 120.70623731613159 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463476
Epoch 69
Meta loss on this task batch = 4.5643e-01, Meta loss averaged over last 500 steps = 4.4903e-01, PNorm = 64.8153, GNorm = 0.2795
Meta loss on this task batch = 3.7229e-01, Meta loss averaged over last 500 steps = 4.4883e-01, PNorm = 64.8309, GNorm = 0.1526
Meta loss on this task batch = 5.1071e-01, Meta loss averaged over last 500 steps = 4.4888e-01, PNorm = 64.8458, GNorm = 0.2346
Meta loss on this task batch = 4.0912e-01, Meta loss averaged over last 500 steps = 4.4872e-01, PNorm = 64.8614, GNorm = 0.1419
Meta loss on this task batch = 4.5266e-01, Meta loss averaged over last 500 steps = 4.4851e-01, PNorm = 64.8775, GNorm = 0.1437
Meta loss on this task batch = 4.4334e-01, Meta loss averaged over last 500 steps = 4.4847e-01, PNorm = 64.8939, GNorm = 0.3232
Meta loss on this task batch = 4.5788e-01, Meta loss averaged over last 500 steps = 4.4853e-01, PNorm = 64.9102, GNorm = 0.2266
Meta loss on this task batch = 4.6024e-01, Meta loss averaged over last 500 steps = 4.4852e-01, PNorm = 64.9255, GNorm = 0.1188
Meta loss on this task batch = 4.8986e-01, Meta loss averaged over last 500 steps = 4.4852e-01, PNorm = 64.9402, GNorm = 0.5073
Meta loss on this task batch = 4.3389e-01, Meta loss averaged over last 500 steps = 4.4850e-01, PNorm = 64.9540, GNorm = 0.1349
Meta loss on this task batch = 4.6266e-01, Meta loss averaged over last 500 steps = 4.4846e-01, PNorm = 64.9661, GNorm = 0.1322
Meta loss on this task batch = 4.0257e-01, Meta loss averaged over last 500 steps = 4.4834e-01, PNorm = 64.9791, GNorm = 0.1967
Meta loss on this task batch = 4.0964e-01, Meta loss averaged over last 500 steps = 4.4836e-01, PNorm = 64.9917, GNorm = 0.1048
Meta loss on this task batch = 3.8926e-01, Meta loss averaged over last 500 steps = 4.4817e-01, PNorm = 65.0054, GNorm = 0.1519
Meta loss on this task batch = 4.1075e-01, Meta loss averaged over last 500 steps = 4.4806e-01, PNorm = 65.0195, GNorm = 0.1473
Meta loss on this task batch = 4.1032e-01, Meta loss averaged over last 500 steps = 4.4800e-01, PNorm = 65.0337, GNorm = 0.1253
Meta loss on this task batch = 4.0343e-01, Meta loss averaged over last 500 steps = 4.4788e-01, PNorm = 65.0474, GNorm = 0.1100
Meta loss on this task batch = 4.4834e-01, Meta loss averaged over last 500 steps = 4.4794e-01, PNorm = 65.0607, GNorm = 0.1353
Meta loss on this task batch = 4.6404e-01, Meta loss averaged over last 500 steps = 4.4786e-01, PNorm = 65.0750, GNorm = 0.1466
Took 113.02998566627502 seconds to complete one epoch of meta training
Took 120.81285357475281 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.439711
Epoch 70
Meta loss on this task batch = 3.9574e-01, Meta loss averaged over last 500 steps = 4.4777e-01, PNorm = 65.0908, GNorm = 0.2737
Meta loss on this task batch = 4.3661e-01, Meta loss averaged over last 500 steps = 4.4782e-01, PNorm = 65.1048, GNorm = 0.1884
Meta loss on this task batch = 4.0786e-01, Meta loss averaged over last 500 steps = 4.4769e-01, PNorm = 65.1189, GNorm = 0.2054
Meta loss on this task batch = 4.1621e-01, Meta loss averaged over last 500 steps = 4.4758e-01, PNorm = 65.1337, GNorm = 0.1081
Meta loss on this task batch = 4.3793e-01, Meta loss averaged over last 500 steps = 4.4761e-01, PNorm = 65.1470, GNorm = 0.1254
Meta loss on this task batch = 4.7055e-01, Meta loss averaged over last 500 steps = 4.4756e-01, PNorm = 65.1596, GNorm = 0.2606
Meta loss on this task batch = 4.8009e-01, Meta loss averaged over last 500 steps = 4.4767e-01, PNorm = 65.1715, GNorm = 0.1451
Meta loss on this task batch = 4.7347e-01, Meta loss averaged over last 500 steps = 4.4772e-01, PNorm = 65.1849, GNorm = 0.1202
Meta loss on this task batch = 4.6246e-01, Meta loss averaged over last 500 steps = 4.4791e-01, PNorm = 65.1966, GNorm = 0.1587
Meta loss on this task batch = 4.4286e-01, Meta loss averaged over last 500 steps = 4.4787e-01, PNorm = 65.2083, GNorm = 0.2112
Meta loss on this task batch = 4.6328e-01, Meta loss averaged over last 500 steps = 4.4793e-01, PNorm = 65.2194, GNorm = 0.1237
Meta loss on this task batch = 4.0917e-01, Meta loss averaged over last 500 steps = 4.4794e-01, PNorm = 65.2317, GNorm = 0.3168
Meta loss on this task batch = 4.0554e-01, Meta loss averaged over last 500 steps = 4.4789e-01, PNorm = 65.2431, GNorm = 0.1725
Meta loss on this task batch = 4.0139e-01, Meta loss averaged over last 500 steps = 4.4778e-01, PNorm = 65.2558, GNorm = 0.1739
Meta loss on this task batch = 4.6718e-01, Meta loss averaged over last 500 steps = 4.4793e-01, PNorm = 65.2694, GNorm = 0.1940
Meta loss on this task batch = 3.6369e-01, Meta loss averaged over last 500 steps = 4.4773e-01, PNorm = 65.2830, GNorm = 0.2399
Meta loss on this task batch = 4.2409e-01, Meta loss averaged over last 500 steps = 4.4758e-01, PNorm = 65.2974, GNorm = 0.2249
Meta loss on this task batch = 4.4587e-01, Meta loss averaged over last 500 steps = 4.4753e-01, PNorm = 65.3115, GNorm = 0.1322
Meta loss on this task batch = 4.7561e-01, Meta loss averaged over last 500 steps = 4.4764e-01, PNorm = 65.3235, GNorm = 0.4241
Took 110.73215293884277 seconds to complete one epoch of meta training
Took 118.34351897239685 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488725
Found better MAML checkpoint after meta validation, saving now
Epoch 71
Meta loss on this task batch = 3.6778e-01, Meta loss averaged over last 500 steps = 4.4745e-01, PNorm = 65.3369, GNorm = 0.1208
Meta loss on this task batch = 4.6419e-01, Meta loss averaged over last 500 steps = 4.4735e-01, PNorm = 65.3510, GNorm = 0.5111
Meta loss on this task batch = 4.6594e-01, Meta loss averaged over last 500 steps = 4.4737e-01, PNorm = 65.3665, GNorm = 0.1549
Meta loss on this task batch = 4.2066e-01, Meta loss averaged over last 500 steps = 4.4718e-01, PNorm = 65.3828, GNorm = 0.3329
Meta loss on this task batch = 4.1579e-01, Meta loss averaged over last 500 steps = 4.4697e-01, PNorm = 65.3997, GNorm = 0.1283
Meta loss on this task batch = 4.5949e-01, Meta loss averaged over last 500 steps = 4.4698e-01, PNorm = 65.4163, GNorm = 0.1718
Meta loss on this task batch = 4.1820e-01, Meta loss averaged over last 500 steps = 4.4681e-01, PNorm = 65.4343, GNorm = 0.3354
Meta loss on this task batch = 4.3835e-01, Meta loss averaged over last 500 steps = 4.4673e-01, PNorm = 65.4518, GNorm = 0.1315
Meta loss on this task batch = 4.0523e-01, Meta loss averaged over last 500 steps = 4.4658e-01, PNorm = 65.4690, GNorm = 0.1218
Meta loss on this task batch = 3.6369e-01, Meta loss averaged over last 500 steps = 4.4628e-01, PNorm = 65.4877, GNorm = 0.1295
Meta loss on this task batch = 5.0545e-01, Meta loss averaged over last 500 steps = 4.4644e-01, PNorm = 65.5055, GNorm = 0.1504
Meta loss on this task batch = 4.2258e-01, Meta loss averaged over last 500 steps = 4.4644e-01, PNorm = 65.5220, GNorm = 0.1584
Meta loss on this task batch = 3.8555e-01, Meta loss averaged over last 500 steps = 4.4629e-01, PNorm = 65.5390, GNorm = 0.1584
Meta loss on this task batch = 4.3035e-01, Meta loss averaged over last 500 steps = 4.4617e-01, PNorm = 65.5553, GNorm = 0.1612
Meta loss on this task batch = 4.3076e-01, Meta loss averaged over last 500 steps = 4.4623e-01, PNorm = 65.5704, GNorm = 0.1671
Meta loss on this task batch = 4.8381e-01, Meta loss averaged over last 500 steps = 4.4625e-01, PNorm = 65.5825, GNorm = 0.2098
Meta loss on this task batch = 4.6561e-01, Meta loss averaged over last 500 steps = 4.4620e-01, PNorm = 65.5937, GNorm = 0.1431
Meta loss on this task batch = 3.5810e-01, Meta loss averaged over last 500 steps = 4.4621e-01, PNorm = 65.6063, GNorm = 0.1699
Meta loss on this task batch = 4.7356e-01, Meta loss averaged over last 500 steps = 4.4633e-01, PNorm = 65.6171, GNorm = 0.3298
Took 110.859872341156 seconds to complete one epoch of meta training
Took 118.85148859024048 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475352
Epoch 72
Meta loss on this task batch = 4.4468e-01, Meta loss averaged over last 500 steps = 4.4628e-01, PNorm = 65.6264, GNorm = 0.4430
Meta loss on this task batch = 4.3837e-01, Meta loss averaged over last 500 steps = 4.4634e-01, PNorm = 65.6371, GNorm = 0.1244
Meta loss on this task batch = 4.8666e-01, Meta loss averaged over last 500 steps = 4.4634e-01, PNorm = 65.6473, GNorm = 0.2329
Meta loss on this task batch = 3.7076e-01, Meta loss averaged over last 500 steps = 4.4610e-01, PNorm = 65.6592, GNorm = 0.1544
Meta loss on this task batch = 4.4488e-01, Meta loss averaged over last 500 steps = 4.4604e-01, PNorm = 65.6723, GNorm = 0.1923
Meta loss on this task batch = 4.6815e-01, Meta loss averaged over last 500 steps = 4.4611e-01, PNorm = 65.6860, GNorm = 0.1428
Meta loss on this task batch = 4.6958e-01, Meta loss averaged over last 500 steps = 4.4608e-01, PNorm = 65.6998, GNorm = 0.1725
Meta loss on this task batch = 4.1275e-01, Meta loss averaged over last 500 steps = 4.4603e-01, PNorm = 65.7144, GNorm = 0.1331
Meta loss on this task batch = 4.4761e-01, Meta loss averaged over last 500 steps = 4.4598e-01, PNorm = 65.7301, GNorm = 0.1383
Meta loss on this task batch = 4.2149e-01, Meta loss averaged over last 500 steps = 4.4602e-01, PNorm = 65.7463, GNorm = 0.1625
Meta loss on this task batch = 4.4118e-01, Meta loss averaged over last 500 steps = 4.4600e-01, PNorm = 65.7638, GNorm = 0.3086
Meta loss on this task batch = 4.5348e-01, Meta loss averaged over last 500 steps = 4.4600e-01, PNorm = 65.7806, GNorm = 0.1579
Meta loss on this task batch = 4.0362e-01, Meta loss averaged over last 500 steps = 4.4594e-01, PNorm = 65.7988, GNorm = 0.2006
Meta loss on this task batch = 4.2698e-01, Meta loss averaged over last 500 steps = 4.4574e-01, PNorm = 65.8185, GNorm = 0.5220
Meta loss on this task batch = 5.1930e-01, Meta loss averaged over last 500 steps = 4.4589e-01, PNorm = 65.8358, GNorm = 0.1736
Meta loss on this task batch = 4.5037e-01, Meta loss averaged over last 500 steps = 4.4584e-01, PNorm = 65.8538, GNorm = 0.4214
Meta loss on this task batch = 4.1735e-01, Meta loss averaged over last 500 steps = 4.4567e-01, PNorm = 65.8723, GNorm = 0.2606
Meta loss on this task batch = 3.6988e-01, Meta loss averaged over last 500 steps = 4.4550e-01, PNorm = 65.8913, GNorm = 0.1047
Meta loss on this task batch = 3.7331e-01, Meta loss averaged over last 500 steps = 4.4536e-01, PNorm = 65.9096, GNorm = 0.1462
Took 110.18915390968323 seconds to complete one epoch of meta training
Took 117.85063290596008 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464384
Epoch 73
Meta loss on this task batch = 4.2779e-01, Meta loss averaged over last 500 steps = 4.4534e-01, PNorm = 65.9266, GNorm = 0.2760
Meta loss on this task batch = 4.7512e-01, Meta loss averaged over last 500 steps = 4.4537e-01, PNorm = 65.9439, GNorm = 0.1281
Meta loss on this task batch = 4.8073e-01, Meta loss averaged over last 500 steps = 4.4531e-01, PNorm = 65.9602, GNorm = 0.1634
Meta loss on this task batch = 4.5826e-01, Meta loss averaged over last 500 steps = 4.4526e-01, PNorm = 65.9761, GNorm = 0.1900
Meta loss on this task batch = 4.0336e-01, Meta loss averaged over last 500 steps = 4.4517e-01, PNorm = 65.9916, GNorm = 0.2093
Meta loss on this task batch = 3.9868e-01, Meta loss averaged over last 500 steps = 4.4525e-01, PNorm = 66.0063, GNorm = 0.1225
Meta loss on this task batch = 4.1580e-01, Meta loss averaged over last 500 steps = 4.4514e-01, PNorm = 66.0199, GNorm = 0.1393
Meta loss on this task batch = 4.5094e-01, Meta loss averaged over last 500 steps = 4.4521e-01, PNorm = 66.0327, GNorm = 0.1659
Meta loss on this task batch = 4.1305e-01, Meta loss averaged over last 500 steps = 4.4520e-01, PNorm = 66.0448, GNorm = 0.1865
Meta loss on this task batch = 4.4754e-01, Meta loss averaged over last 500 steps = 4.4525e-01, PNorm = 66.0568, GNorm = 0.1389
Meta loss on this task batch = 4.6824e-01, Meta loss averaged over last 500 steps = 4.4521e-01, PNorm = 66.0680, GNorm = 0.1579
Meta loss on this task batch = 4.7359e-01, Meta loss averaged over last 500 steps = 4.4523e-01, PNorm = 66.0803, GNorm = 0.1426
Meta loss on this task batch = 4.0556e-01, Meta loss averaged over last 500 steps = 4.4509e-01, PNorm = 66.0937, GNorm = 0.1820
Meta loss on this task batch = 3.7405e-01, Meta loss averaged over last 500 steps = 4.4485e-01, PNorm = 66.1069, GNorm = 0.3247
Meta loss on this task batch = 4.4667e-01, Meta loss averaged over last 500 steps = 4.4487e-01, PNorm = 66.1204, GNorm = 0.1266
Meta loss on this task batch = 4.6027e-01, Meta loss averaged over last 500 steps = 4.4474e-01, PNorm = 66.1334, GNorm = 0.3461
Meta loss on this task batch = 3.7025e-01, Meta loss averaged over last 500 steps = 4.4460e-01, PNorm = 66.1484, GNorm = 0.1447
Meta loss on this task batch = 4.3494e-01, Meta loss averaged over last 500 steps = 4.4458e-01, PNorm = 66.1635, GNorm = 0.1103
Meta loss on this task batch = 4.5315e-01, Meta loss averaged over last 500 steps = 4.4458e-01, PNorm = 66.1809, GNorm = 0.4228
Took 109.56769895553589 seconds to complete one epoch of meta training
Took 117.14196801185608 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465010
Epoch 74
Meta loss on this task batch = 4.2354e-01, Meta loss averaged over last 500 steps = 4.4449e-01, PNorm = 66.1986, GNorm = 0.1351
Meta loss on this task batch = 4.4804e-01, Meta loss averaged over last 500 steps = 4.4449e-01, PNorm = 66.2173, GNorm = 0.2684
Meta loss on this task batch = 4.3826e-01, Meta loss averaged over last 500 steps = 4.4428e-01, PNorm = 66.2358, GNorm = 0.2347
Meta loss on this task batch = 4.4963e-01, Meta loss averaged over last 500 steps = 4.4419e-01, PNorm = 66.2546, GNorm = 0.3802
Meta loss on this task batch = 4.0542e-01, Meta loss averaged over last 500 steps = 4.4413e-01, PNorm = 66.2720, GNorm = 0.1967
Meta loss on this task batch = 4.7673e-01, Meta loss averaged over last 500 steps = 4.4403e-01, PNorm = 66.2887, GNorm = 0.2180
Meta loss on this task batch = 3.9672e-01, Meta loss averaged over last 500 steps = 4.4399e-01, PNorm = 66.3035, GNorm = 0.3275
Meta loss on this task batch = 4.3612e-01, Meta loss averaged over last 500 steps = 4.4393e-01, PNorm = 66.3171, GNorm = 0.1486
Meta loss on this task batch = 4.3915e-01, Meta loss averaged over last 500 steps = 4.4394e-01, PNorm = 66.3297, GNorm = 0.1639
Meta loss on this task batch = 4.4728e-01, Meta loss averaged over last 500 steps = 4.4389e-01, PNorm = 66.3424, GNorm = 0.2185
Meta loss on this task batch = 3.6214e-01, Meta loss averaged over last 500 steps = 4.4375e-01, PNorm = 66.3554, GNorm = 0.2213
Meta loss on this task batch = 3.6861e-01, Meta loss averaged over last 500 steps = 4.4359e-01, PNorm = 66.3685, GNorm = 0.2004
Meta loss on this task batch = 3.6822e-01, Meta loss averaged over last 500 steps = 4.4343e-01, PNorm = 66.3827, GNorm = 0.1774
Meta loss on this task batch = 4.7045e-01, Meta loss averaged over last 500 steps = 4.4344e-01, PNorm = 66.3965, GNorm = 0.1654
Meta loss on this task batch = 5.1326e-01, Meta loss averaged over last 500 steps = 4.4350e-01, PNorm = 66.4099, GNorm = 0.1710
Meta loss on this task batch = 3.8857e-01, Meta loss averaged over last 500 steps = 4.4333e-01, PNorm = 66.4235, GNorm = 0.1630
Meta loss on this task batch = 4.2052e-01, Meta loss averaged over last 500 steps = 4.4329e-01, PNorm = 66.4368, GNorm = 0.1351
Meta loss on this task batch = 4.5637e-01, Meta loss averaged over last 500 steps = 4.4322e-01, PNorm = 66.4510, GNorm = 0.2176
Meta loss on this task batch = 3.8753e-01, Meta loss averaged over last 500 steps = 4.4297e-01, PNorm = 66.4668, GNorm = 0.1411
Took 113.64720320701599 seconds to complete one epoch of meta training
Took 121.8389401435852 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468184
Epoch 75
Meta loss on this task batch = 4.2790e-01, Meta loss averaged over last 500 steps = 4.4296e-01, PNorm = 66.4821, GNorm = 0.1861
Meta loss on this task batch = 4.0692e-01, Meta loss averaged over last 500 steps = 4.4302e-01, PNorm = 66.4985, GNorm = 0.1427
Meta loss on this task batch = 4.3767e-01, Meta loss averaged over last 500 steps = 4.4299e-01, PNorm = 66.5142, GNorm = 0.3290
Meta loss on this task batch = 3.6658e-01, Meta loss averaged over last 500 steps = 4.4279e-01, PNorm = 66.5298, GNorm = 0.1198
Meta loss on this task batch = 4.3897e-01, Meta loss averaged over last 500 steps = 4.4256e-01, PNorm = 66.5448, GNorm = 0.1392
Meta loss on this task batch = 4.3877e-01, Meta loss averaged over last 500 steps = 4.4249e-01, PNorm = 66.5597, GNorm = 0.1345
Meta loss on this task batch = 4.4449e-01, Meta loss averaged over last 500 steps = 4.4248e-01, PNorm = 66.5758, GNorm = 0.2335
Meta loss on this task batch = 4.0722e-01, Meta loss averaged over last 500 steps = 4.4239e-01, PNorm = 66.5925, GNorm = 0.1602
Meta loss on this task batch = 4.9588e-01, Meta loss averaged over last 500 steps = 4.4244e-01, PNorm = 66.6082, GNorm = 0.1469
Meta loss on this task batch = 4.4257e-01, Meta loss averaged over last 500 steps = 4.4247e-01, PNorm = 66.6232, GNorm = 0.1384
Meta loss on this task batch = 4.4207e-01, Meta loss averaged over last 500 steps = 4.4239e-01, PNorm = 66.6378, GNorm = 0.1983
Meta loss on this task batch = 4.1205e-01, Meta loss averaged over last 500 steps = 4.4222e-01, PNorm = 66.6517, GNorm = 0.1610
Meta loss on this task batch = 4.1678e-01, Meta loss averaged over last 500 steps = 4.4217e-01, PNorm = 66.6664, GNorm = 0.1405
Meta loss on this task batch = 4.1234e-01, Meta loss averaged over last 500 steps = 4.4199e-01, PNorm = 66.6816, GNorm = 0.2476
Meta loss on this task batch = 4.1263e-01, Meta loss averaged over last 500 steps = 4.4198e-01, PNorm = 66.6967, GNorm = 0.1480
Meta loss on this task batch = 4.3634e-01, Meta loss averaged over last 500 steps = 4.4197e-01, PNorm = 66.7123, GNorm = 0.1989
Meta loss on this task batch = 3.7775e-01, Meta loss averaged over last 500 steps = 4.4181e-01, PNorm = 66.7292, GNorm = 0.3083
Meta loss on this task batch = 3.7532e-01, Meta loss averaged over last 500 steps = 4.4175e-01, PNorm = 66.7445, GNorm = 0.1987
Meta loss on this task batch = 4.0074e-01, Meta loss averaged over last 500 steps = 4.4157e-01, PNorm = 66.7596, GNorm = 0.1607
Took 113.54665684700012 seconds to complete one epoch of meta training
Took 121.62921929359436 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471433
Epoch 76
Meta loss on this task batch = 4.2421e-01, Meta loss averaged over last 500 steps = 4.4139e-01, PNorm = 66.7739, GNorm = 0.1898
Meta loss on this task batch = 4.0069e-01, Meta loss averaged over last 500 steps = 4.4118e-01, PNorm = 66.7880, GNorm = 0.1812
Meta loss on this task batch = 4.5475e-01, Meta loss averaged over last 500 steps = 4.4124e-01, PNorm = 66.8014, GNorm = 0.1873
Meta loss on this task batch = 4.0199e-01, Meta loss averaged over last 500 steps = 4.4121e-01, PNorm = 66.8135, GNorm = 0.2196
Meta loss on this task batch = 4.3780e-01, Meta loss averaged over last 500 steps = 4.4118e-01, PNorm = 66.8257, GNorm = 0.2494
Meta loss on this task batch = 3.1970e-01, Meta loss averaged over last 500 steps = 4.4092e-01, PNorm = 66.8389, GNorm = 0.3209
Meta loss on this task batch = 4.1960e-01, Meta loss averaged over last 500 steps = 4.4086e-01, PNorm = 66.8536, GNorm = 0.1726
Meta loss on this task batch = 4.6917e-01, Meta loss averaged over last 500 steps = 4.4083e-01, PNorm = 66.8687, GNorm = 0.2885
Meta loss on this task batch = 4.3106e-01, Meta loss averaged over last 500 steps = 4.4077e-01, PNorm = 66.8863, GNorm = 0.2423
Meta loss on this task batch = 4.5977e-01, Meta loss averaged over last 500 steps = 4.4065e-01, PNorm = 66.9030, GNorm = 0.3561
Meta loss on this task batch = 4.8961e-01, Meta loss averaged over last 500 steps = 4.4063e-01, PNorm = 66.9190, GNorm = 0.1491
Meta loss on this task batch = 3.9079e-01, Meta loss averaged over last 500 steps = 4.4058e-01, PNorm = 66.9336, GNorm = 0.2468
Meta loss on this task batch = 4.3730e-01, Meta loss averaged over last 500 steps = 4.4062e-01, PNorm = 66.9479, GNorm = 0.1457
Meta loss on this task batch = 3.8343e-01, Meta loss averaged over last 500 steps = 4.4044e-01, PNorm = 66.9618, GNorm = 0.1715
Meta loss on this task batch = 4.3131e-01, Meta loss averaged over last 500 steps = 4.4043e-01, PNorm = 66.9766, GNorm = 0.2073
Meta loss on this task batch = 5.1962e-01, Meta loss averaged over last 500 steps = 4.4059e-01, PNorm = 66.9910, GNorm = 0.1960
Meta loss on this task batch = 3.9972e-01, Meta loss averaged over last 500 steps = 4.4049e-01, PNorm = 67.0069, GNorm = 0.1534
Meta loss on this task batch = 4.5515e-01, Meta loss averaged over last 500 steps = 4.4041e-01, PNorm = 67.0236, GNorm = 0.1660
Meta loss on this task batch = 4.4797e-01, Meta loss averaged over last 500 steps = 4.4050e-01, PNorm = 67.0387, GNorm = 0.1657
Took 113.62592339515686 seconds to complete one epoch of meta training
Took 121.6430139541626 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450662
Epoch 77
Meta loss on this task batch = 4.3229e-01, Meta loss averaged over last 500 steps = 4.4061e-01, PNorm = 67.0549, GNorm = 0.3293
Meta loss on this task batch = 3.8445e-01, Meta loss averaged over last 500 steps = 4.4050e-01, PNorm = 67.0722, GNorm = 0.1585
Meta loss on this task batch = 4.0522e-01, Meta loss averaged over last 500 steps = 4.4035e-01, PNorm = 67.0891, GNorm = 0.2121
Meta loss on this task batch = 4.3727e-01, Meta loss averaged over last 500 steps = 4.4040e-01, PNorm = 67.1057, GNorm = 0.3623
Meta loss on this task batch = 4.4325e-01, Meta loss averaged over last 500 steps = 4.4046e-01, PNorm = 67.1234, GNorm = 0.2311
Meta loss on this task batch = 4.6628e-01, Meta loss averaged over last 500 steps = 4.4043e-01, PNorm = 67.1411, GNorm = 0.2881
Meta loss on this task batch = 4.1690e-01, Meta loss averaged over last 500 steps = 4.4039e-01, PNorm = 67.1608, GNorm = 0.2365
Meta loss on this task batch = 4.3525e-01, Meta loss averaged over last 500 steps = 4.4022e-01, PNorm = 67.1810, GNorm = 0.1588
Meta loss on this task batch = 4.5244e-01, Meta loss averaged over last 500 steps = 4.4026e-01, PNorm = 67.2006, GNorm = 0.1695
Meta loss on this task batch = 4.0993e-01, Meta loss averaged over last 500 steps = 4.4024e-01, PNorm = 67.2198, GNorm = 0.1923
Meta loss on this task batch = 4.2923e-01, Meta loss averaged over last 500 steps = 4.4021e-01, PNorm = 67.2386, GNorm = 0.2124
Meta loss on this task batch = 5.1577e-01, Meta loss averaged over last 500 steps = 4.4027e-01, PNorm = 67.2573, GNorm = 0.1577
Meta loss on this task batch = 4.5608e-01, Meta loss averaged over last 500 steps = 4.4024e-01, PNorm = 67.2739, GNorm = 0.2035
Meta loss on this task batch = 4.3573e-01, Meta loss averaged over last 500 steps = 4.4015e-01, PNorm = 67.2887, GNorm = 0.1947
Meta loss on this task batch = 4.4271e-01, Meta loss averaged over last 500 steps = 4.4018e-01, PNorm = 67.3021, GNorm = 0.3519
Meta loss on this task batch = 4.7339e-01, Meta loss averaged over last 500 steps = 4.4035e-01, PNorm = 67.3153, GNorm = 0.1653
Meta loss on this task batch = 4.1980e-01, Meta loss averaged over last 500 steps = 4.4030e-01, PNorm = 67.3298, GNorm = 0.4129
Meta loss on this task batch = 4.1368e-01, Meta loss averaged over last 500 steps = 4.4016e-01, PNorm = 67.3441, GNorm = 0.2951
Meta loss on this task batch = 3.7526e-01, Meta loss averaged over last 500 steps = 4.4004e-01, PNorm = 67.3599, GNorm = 0.1239
Took 111.7995433807373 seconds to complete one epoch of meta training
Took 119.55660271644592 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446862
Epoch 78
Meta loss on this task batch = 4.1903e-01, Meta loss averaged over last 500 steps = 4.3982e-01, PNorm = 67.3770, GNorm = 0.1366
Meta loss on this task batch = 4.5289e-01, Meta loss averaged over last 500 steps = 4.3977e-01, PNorm = 67.3947, GNorm = 0.1634
Meta loss on this task batch = 3.6377e-01, Meta loss averaged over last 500 steps = 4.3949e-01, PNorm = 67.4135, GNorm = 0.1344
Meta loss on this task batch = 4.9026e-01, Meta loss averaged over last 500 steps = 4.3957e-01, PNorm = 67.4318, GNorm = 0.1293
Meta loss on this task batch = 3.7803e-01, Meta loss averaged over last 500 steps = 4.3927e-01, PNorm = 67.4505, GNorm = 0.2426
Meta loss on this task batch = 4.6765e-01, Meta loss averaged over last 500 steps = 4.3950e-01, PNorm = 67.4694, GNorm = 0.1797
Meta loss on this task batch = 4.6689e-01, Meta loss averaged over last 500 steps = 4.3938e-01, PNorm = 67.4892, GNorm = 0.3206
Meta loss on this task batch = 3.2787e-01, Meta loss averaged over last 500 steps = 4.3926e-01, PNorm = 67.5094, GNorm = 0.1487
Meta loss on this task batch = 4.7884e-01, Meta loss averaged over last 500 steps = 4.3930e-01, PNorm = 67.5280, GNorm = 0.3115
Meta loss on this task batch = 4.7837e-01, Meta loss averaged over last 500 steps = 4.3932e-01, PNorm = 67.5440, GNorm = 0.2772
Meta loss on this task batch = 3.9984e-01, Meta loss averaged over last 500 steps = 4.3909e-01, PNorm = 67.5589, GNorm = 0.1796
Meta loss on this task batch = 4.5053e-01, Meta loss averaged over last 500 steps = 4.3910e-01, PNorm = 67.5733, GNorm = 0.1844
Meta loss on this task batch = 4.9056e-01, Meta loss averaged over last 500 steps = 4.3927e-01, PNorm = 67.5864, GNorm = 0.1805
Meta loss on this task batch = 4.0497e-01, Meta loss averaged over last 500 steps = 4.3907e-01, PNorm = 67.5991, GNorm = 0.1524
Meta loss on this task batch = 4.0572e-01, Meta loss averaged over last 500 steps = 4.3905e-01, PNorm = 67.6125, GNorm = 0.1687
Meta loss on this task batch = 3.9976e-01, Meta loss averaged over last 500 steps = 4.3906e-01, PNorm = 67.6259, GNorm = 0.1359
Meta loss on this task batch = 3.8596e-01, Meta loss averaged over last 500 steps = 4.3886e-01, PNorm = 67.6396, GNorm = 0.1450
Meta loss on this task batch = 4.5565e-01, Meta loss averaged over last 500 steps = 4.3883e-01, PNorm = 67.6531, GNorm = 0.2184
Meta loss on this task batch = 4.1900e-01, Meta loss averaged over last 500 steps = 4.3879e-01, PNorm = 67.6656, GNorm = 0.4040
Took 112.29234838485718 seconds to complete one epoch of meta training
Took 119.82252597808838 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.443157
Epoch 79
Meta loss on this task batch = 4.1018e-01, Meta loss averaged over last 500 steps = 4.3883e-01, PNorm = 67.6780, GNorm = 0.1387
Meta loss on this task batch = 4.4414e-01, Meta loss averaged over last 500 steps = 4.3889e-01, PNorm = 67.6901, GNorm = 0.1410
Meta loss on this task batch = 4.4781e-01, Meta loss averaged over last 500 steps = 4.3901e-01, PNorm = 67.7024, GNorm = 0.2378
Meta loss on this task batch = 4.0119e-01, Meta loss averaged over last 500 steps = 4.3883e-01, PNorm = 67.7155, GNorm = 0.1662
Meta loss on this task batch = 3.9520e-01, Meta loss averaged over last 500 steps = 4.3877e-01, PNorm = 67.7284, GNorm = 0.2386
Meta loss on this task batch = 3.6792e-01, Meta loss averaged over last 500 steps = 4.3851e-01, PNorm = 67.7419, GNorm = 0.1609
Meta loss on this task batch = 4.6349e-01, Meta loss averaged over last 500 steps = 4.3852e-01, PNorm = 67.7539, GNorm = 0.1716
Meta loss on this task batch = 4.6089e-01, Meta loss averaged over last 500 steps = 4.3863e-01, PNorm = 67.7654, GNorm = 0.1316
Meta loss on this task batch = 4.2300e-01, Meta loss averaged over last 500 steps = 4.3872e-01, PNorm = 67.7773, GNorm = 0.1895
Meta loss on this task batch = 4.5096e-01, Meta loss averaged over last 500 steps = 4.3866e-01, PNorm = 67.7904, GNorm = 0.2092
Meta loss on this task batch = 4.5416e-01, Meta loss averaged over last 500 steps = 4.3876e-01, PNorm = 67.8041, GNorm = 0.1720
Meta loss on this task batch = 3.9793e-01, Meta loss averaged over last 500 steps = 4.3856e-01, PNorm = 67.8182, GNorm = 0.2390
Meta loss on this task batch = 4.3222e-01, Meta loss averaged over last 500 steps = 4.3860e-01, PNorm = 67.8320, GNorm = 0.1552
Meta loss on this task batch = 4.7657e-01, Meta loss averaged over last 500 steps = 4.3863e-01, PNorm = 67.8464, GNorm = 0.2049
Meta loss on this task batch = 4.3346e-01, Meta loss averaged over last 500 steps = 4.3871e-01, PNorm = 67.8611, GNorm = 0.1525
Meta loss on this task batch = 4.7515e-01, Meta loss averaged over last 500 steps = 4.3866e-01, PNorm = 67.8749, GNorm = 0.2152
Meta loss on this task batch = 3.0942e-01, Meta loss averaged over last 500 steps = 4.3834e-01, PNorm = 67.8907, GNorm = 0.2103
Meta loss on this task batch = 4.8063e-01, Meta loss averaged over last 500 steps = 4.3847e-01, PNorm = 67.9053, GNorm = 0.1381
Meta loss on this task batch = 3.8677e-01, Meta loss averaged over last 500 steps = 4.3834e-01, PNorm = 67.9195, GNorm = 0.2635
Took 112.51311564445496 seconds to complete one epoch of meta training
Took 119.97145128250122 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466141
Epoch 80
Meta loss on this task batch = 3.5825e-01, Meta loss averaged over last 500 steps = 4.3800e-01, PNorm = 67.9346, GNorm = 0.1906
Meta loss on this task batch = 4.5136e-01, Meta loss averaged over last 500 steps = 4.3796e-01, PNorm = 67.9498, GNorm = 0.1690
Meta loss on this task batch = 3.9655e-01, Meta loss averaged over last 500 steps = 4.3786e-01, PNorm = 67.9658, GNorm = 0.3056
Meta loss on this task batch = 3.8742e-01, Meta loss averaged over last 500 steps = 4.3776e-01, PNorm = 67.9810, GNorm = 0.1305
Meta loss on this task batch = 4.4449e-01, Meta loss averaged over last 500 steps = 4.3794e-01, PNorm = 67.9955, GNorm = 0.1791
Meta loss on this task batch = 4.6283e-01, Meta loss averaged over last 500 steps = 4.3789e-01, PNorm = 68.0073, GNorm = 0.2957
Meta loss on this task batch = 4.6419e-01, Meta loss averaged over last 500 steps = 4.3804e-01, PNorm = 68.0179, GNorm = 0.2688
Meta loss on this task batch = 4.5016e-01, Meta loss averaged over last 500 steps = 4.3804e-01, PNorm = 68.0274, GNorm = 0.1474
Meta loss on this task batch = 3.8448e-01, Meta loss averaged over last 500 steps = 4.3793e-01, PNorm = 68.0388, GNorm = 0.1268
Meta loss on this task batch = 4.2029e-01, Meta loss averaged over last 500 steps = 4.3787e-01, PNorm = 68.0512, GNorm = 0.3038
Meta loss on this task batch = 4.2731e-01, Meta loss averaged over last 500 steps = 4.3778e-01, PNorm = 68.0651, GNorm = 0.2285
Meta loss on this task batch = 3.9591e-01, Meta loss averaged over last 500 steps = 4.3756e-01, PNorm = 68.0798, GNorm = 0.2662
Meta loss on this task batch = 3.9691e-01, Meta loss averaged over last 500 steps = 4.3741e-01, PNorm = 68.0942, GNorm = 0.1529
Meta loss on this task batch = 4.0517e-01, Meta loss averaged over last 500 steps = 4.3738e-01, PNorm = 68.1090, GNorm = 0.1417
Meta loss on this task batch = 4.8258e-01, Meta loss averaged over last 500 steps = 4.3755e-01, PNorm = 68.1242, GNorm = 0.1411
Meta loss on this task batch = 4.4277e-01, Meta loss averaged over last 500 steps = 4.3747e-01, PNorm = 68.1392, GNorm = 0.4073
Meta loss on this task batch = 3.9340e-01, Meta loss averaged over last 500 steps = 4.3749e-01, PNorm = 68.1547, GNorm = 0.3033
Meta loss on this task batch = 3.8656e-01, Meta loss averaged over last 500 steps = 4.3741e-01, PNorm = 68.1708, GNorm = 0.2276
Meta loss on this task batch = 5.1348e-01, Meta loss averaged over last 500 steps = 4.3748e-01, PNorm = 68.1862, GNorm = 0.3402
Took 111.36904692649841 seconds to complete one epoch of meta training
Took 118.24057412147522 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485392
Epoch 81
Meta loss on this task batch = 4.5170e-01, Meta loss averaged over last 500 steps = 4.3751e-01, PNorm = 68.2014, GNorm = 0.1448
Meta loss on this task batch = 3.9816e-01, Meta loss averaged over last 500 steps = 4.3740e-01, PNorm = 68.2165, GNorm = 0.2122
Meta loss on this task batch = 4.2951e-01, Meta loss averaged over last 500 steps = 4.3733e-01, PNorm = 68.2312, GNorm = 0.1312
Meta loss on this task batch = 3.8802e-01, Meta loss averaged over last 500 steps = 4.3721e-01, PNorm = 68.2452, GNorm = 0.1594
Meta loss on this task batch = 4.5197e-01, Meta loss averaged over last 500 steps = 4.3732e-01, PNorm = 68.2593, GNorm = 0.2812
Meta loss on this task batch = 3.9812e-01, Meta loss averaged over last 500 steps = 4.3720e-01, PNorm = 68.2731, GNorm = 0.1145
Meta loss on this task batch = 3.1012e-01, Meta loss averaged over last 500 steps = 4.3691e-01, PNorm = 68.2869, GNorm = 0.1180
Meta loss on this task batch = 4.0502e-01, Meta loss averaged over last 500 steps = 4.3675e-01, PNorm = 68.2995, GNorm = 0.2079
Meta loss on this task batch = 5.0252e-01, Meta loss averaged over last 500 steps = 4.3687e-01, PNorm = 68.3119, GNorm = 0.1870
Meta loss on this task batch = 4.4880e-01, Meta loss averaged over last 500 steps = 4.3697e-01, PNorm = 68.3235, GNorm = 0.3159
Meta loss on this task batch = 4.1395e-01, Meta loss averaged over last 500 steps = 4.3687e-01, PNorm = 68.3346, GNorm = 0.3213
Meta loss on this task batch = 4.4524e-01, Meta loss averaged over last 500 steps = 4.3680e-01, PNorm = 68.3461, GNorm = 0.1304
Meta loss on this task batch = 3.8433e-01, Meta loss averaged over last 500 steps = 4.3668e-01, PNorm = 68.3585, GNorm = 0.2814
Meta loss on this task batch = 4.3746e-01, Meta loss averaged over last 500 steps = 4.3672e-01, PNorm = 68.3712, GNorm = 0.1659
Meta loss on this task batch = 4.0296e-01, Meta loss averaged over last 500 steps = 4.3669e-01, PNorm = 68.3848, GNorm = 0.1731
Meta loss on this task batch = 4.5869e-01, Meta loss averaged over last 500 steps = 4.3676e-01, PNorm = 68.3995, GNorm = 0.2214
Meta loss on this task batch = 3.8085e-01, Meta loss averaged over last 500 steps = 4.3665e-01, PNorm = 68.4149, GNorm = 0.1329
Meta loss on this task batch = 4.8410e-01, Meta loss averaged over last 500 steps = 4.3678e-01, PNorm = 68.4296, GNorm = 0.1463
Meta loss on this task batch = 4.5067e-01, Meta loss averaged over last 500 steps = 4.3658e-01, PNorm = 68.4439, GNorm = 0.2099
Took 115.72141695022583 seconds to complete one epoch of meta training
Took 123.31124448776245 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488560
Epoch 82
Meta loss on this task batch = 4.3764e-01, Meta loss averaged over last 500 steps = 4.3657e-01, PNorm = 68.4587, GNorm = 0.2134
Meta loss on this task batch = 4.0528e-01, Meta loss averaged over last 500 steps = 4.3654e-01, PNorm = 68.4744, GNorm = 0.2647
Meta loss on this task batch = 4.4611e-01, Meta loss averaged over last 500 steps = 4.3660e-01, PNorm = 68.4905, GNorm = 0.2063
Meta loss on this task batch = 3.7885e-01, Meta loss averaged over last 500 steps = 4.3642e-01, PNorm = 68.5066, GNorm = 0.1511
Meta loss on this task batch = 3.8734e-01, Meta loss averaged over last 500 steps = 4.3626e-01, PNorm = 68.5238, GNorm = 0.1210
Meta loss on this task batch = 4.1093e-01, Meta loss averaged over last 500 steps = 4.3623e-01, PNorm = 68.5412, GNorm = 0.2350
Meta loss on this task batch = 4.3124e-01, Meta loss averaged over last 500 steps = 4.3617e-01, PNorm = 68.5582, GNorm = 0.1781
Meta loss on this task batch = 4.5976e-01, Meta loss averaged over last 500 steps = 4.3630e-01, PNorm = 68.5744, GNorm = 0.2105
Meta loss on this task batch = 4.6678e-01, Meta loss averaged over last 500 steps = 4.3632e-01, PNorm = 68.5900, GNorm = 0.2406
Meta loss on this task batch = 4.6869e-01, Meta loss averaged over last 500 steps = 4.3651e-01, PNorm = 68.6066, GNorm = 0.3318
Meta loss on this task batch = 4.0101e-01, Meta loss averaged over last 500 steps = 4.3641e-01, PNorm = 68.6234, GNorm = 0.5566
Meta loss on this task batch = 3.7728e-01, Meta loss averaged over last 500 steps = 4.3621e-01, PNorm = 68.6386, GNorm = 0.1769
Meta loss on this task batch = 3.9462e-01, Meta loss averaged over last 500 steps = 4.3594e-01, PNorm = 68.6534, GNorm = 0.1635
Meta loss on this task batch = 4.7760e-01, Meta loss averaged over last 500 steps = 4.3602e-01, PNorm = 68.6673, GNorm = 0.3439
Meta loss on this task batch = 3.8636e-01, Meta loss averaged over last 500 steps = 4.3580e-01, PNorm = 68.6796, GNorm = 0.4325
Meta loss on this task batch = 4.0192e-01, Meta loss averaged over last 500 steps = 4.3578e-01, PNorm = 68.6909, GNorm = 0.2886
Meta loss on this task batch = 4.1522e-01, Meta loss averaged over last 500 steps = 4.3564e-01, PNorm = 68.7005, GNorm = 0.3399
Meta loss on this task batch = 4.5368e-01, Meta loss averaged over last 500 steps = 4.3566e-01, PNorm = 68.7107, GNorm = 0.2223
Meta loss on this task batch = 3.7782e-01, Meta loss averaged over last 500 steps = 4.3557e-01, PNorm = 68.7217, GNorm = 0.2516
Took 118.0491681098938 seconds to complete one epoch of meta training
Took 125.74598932266235 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471249
Epoch 83
Meta loss on this task batch = 4.4818e-01, Meta loss averaged over last 500 steps = 4.3558e-01, PNorm = 68.7329, GNorm = 0.1635
Meta loss on this task batch = 4.2232e-01, Meta loss averaged over last 500 steps = 4.3561e-01, PNorm = 68.7456, GNorm = 0.1487
Meta loss on this task batch = 3.4325e-01, Meta loss averaged over last 500 steps = 4.3552e-01, PNorm = 68.7607, GNorm = 0.1560
Meta loss on this task batch = 3.8631e-01, Meta loss averaged over last 500 steps = 4.3541e-01, PNorm = 68.7765, GNorm = 0.1217
Meta loss on this task batch = 4.4165e-01, Meta loss averaged over last 500 steps = 4.3541e-01, PNorm = 68.7935, GNorm = 0.1689
Meta loss on this task batch = 4.5144e-01, Meta loss averaged over last 500 steps = 4.3540e-01, PNorm = 68.8117, GNorm = 0.2400
Meta loss on this task batch = 4.1920e-01, Meta loss averaged over last 500 steps = 4.3535e-01, PNorm = 68.8297, GNorm = 0.1230
Meta loss on this task batch = 3.5863e-01, Meta loss averaged over last 500 steps = 4.3525e-01, PNorm = 68.8485, GNorm = 0.2433
Meta loss on this task batch = 4.6432e-01, Meta loss averaged over last 500 steps = 4.3531e-01, PNorm = 68.8662, GNorm = 0.2563
Meta loss on this task batch = 4.1780e-01, Meta loss averaged over last 500 steps = 4.3533e-01, PNorm = 68.8843, GNorm = 0.1595
Meta loss on this task batch = 4.9920e-01, Meta loss averaged over last 500 steps = 4.3546e-01, PNorm = 68.9018, GNorm = 0.1426
Meta loss on this task batch = 4.2490e-01, Meta loss averaged over last 500 steps = 4.3528e-01, PNorm = 68.9186, GNorm = 0.1836
Meta loss on this task batch = 3.5974e-01, Meta loss averaged over last 500 steps = 4.3511e-01, PNorm = 68.9348, GNorm = 0.1892
Meta loss on this task batch = 4.1310e-01, Meta loss averaged over last 500 steps = 4.3497e-01, PNorm = 68.9517, GNorm = 0.1833
Meta loss on this task batch = 3.6852e-01, Meta loss averaged over last 500 steps = 4.3488e-01, PNorm = 68.9673, GNorm = 0.1604
Meta loss on this task batch = 4.5382e-01, Meta loss averaged over last 500 steps = 4.3491e-01, PNorm = 68.9825, GNorm = 0.2211
Meta loss on this task batch = 4.8309e-01, Meta loss averaged over last 500 steps = 4.3498e-01, PNorm = 68.9959, GNorm = 0.3270
Meta loss on this task batch = 3.9915e-01, Meta loss averaged over last 500 steps = 4.3497e-01, PNorm = 69.0101, GNorm = 0.3581
Meta loss on this task batch = 3.9103e-01, Meta loss averaged over last 500 steps = 4.3502e-01, PNorm = 69.0246, GNorm = 0.2274
Took 117.8915946483612 seconds to complete one epoch of meta training
Took 125.826979637146 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484318
Epoch 84
Meta loss on this task batch = 4.1622e-01, Meta loss averaged over last 500 steps = 4.3502e-01, PNorm = 69.0377, GNorm = 0.2984
Meta loss on this task batch = 4.4810e-01, Meta loss averaged over last 500 steps = 4.3503e-01, PNorm = 69.0504, GNorm = 0.2187
Meta loss on this task batch = 3.6496e-01, Meta loss averaged over last 500 steps = 4.3466e-01, PNorm = 69.0627, GNorm = 0.1469
Meta loss on this task batch = 4.1444e-01, Meta loss averaged over last 500 steps = 4.3467e-01, PNorm = 69.0754, GNorm = 0.3772
Meta loss on this task batch = 3.6322e-01, Meta loss averaged over last 500 steps = 4.3443e-01, PNorm = 69.0887, GNorm = 0.1422
Meta loss on this task batch = 4.0425e-01, Meta loss averaged over last 500 steps = 4.3428e-01, PNorm = 69.1016, GNorm = 0.1401
Meta loss on this task batch = 4.1492e-01, Meta loss averaged over last 500 steps = 4.3423e-01, PNorm = 69.1141, GNorm = 0.1336
Meta loss on this task batch = 3.1801e-01, Meta loss averaged over last 500 steps = 4.3407e-01, PNorm = 69.1278, GNorm = 0.1861
Meta loss on this task batch = 4.5009e-01, Meta loss averaged over last 500 steps = 4.3407e-01, PNorm = 69.1417, GNorm = 0.2064
Meta loss on this task batch = 4.4732e-01, Meta loss averaged over last 500 steps = 4.3406e-01, PNorm = 69.1557, GNorm = 0.1346
Meta loss on this task batch = 4.6212e-01, Meta loss averaged over last 500 steps = 4.3395e-01, PNorm = 69.1695, GNorm = 0.3317
Meta loss on this task batch = 4.3073e-01, Meta loss averaged over last 500 steps = 4.3396e-01, PNorm = 69.1833, GNorm = 0.1607
Meta loss on this task batch = 4.7777e-01, Meta loss averaged over last 500 steps = 4.3400e-01, PNorm = 69.1960, GNorm = 0.1791
Meta loss on this task batch = 3.8690e-01, Meta loss averaged over last 500 steps = 4.3371e-01, PNorm = 69.2084, GNorm = 0.1427
Meta loss on this task batch = 4.3110e-01, Meta loss averaged over last 500 steps = 4.3378e-01, PNorm = 69.2216, GNorm = 0.2320
Meta loss on this task batch = 4.0185e-01, Meta loss averaged over last 500 steps = 4.3380e-01, PNorm = 69.2348, GNorm = 0.1207
Meta loss on this task batch = 4.3658e-01, Meta loss averaged over last 500 steps = 4.3391e-01, PNorm = 69.2478, GNorm = 0.1773
Meta loss on this task batch = 4.5766e-01, Meta loss averaged over last 500 steps = 4.3399e-01, PNorm = 69.2598, GNorm = 0.2382
Meta loss on this task batch = 4.4909e-01, Meta loss averaged over last 500 steps = 4.3401e-01, PNorm = 69.2725, GNorm = 0.3750
Took 116.99723815917969 seconds to complete one epoch of meta training
Took 125.01014041900635 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473755
Epoch 85
Meta loss on this task batch = 4.5674e-01, Meta loss averaged over last 500 steps = 4.3401e-01, PNorm = 69.2839, GNorm = 0.4145
Meta loss on this task batch = 4.0856e-01, Meta loss averaged over last 500 steps = 4.3398e-01, PNorm = 69.2952, GNorm = 0.4065
Meta loss on this task batch = 4.3172e-01, Meta loss averaged over last 500 steps = 4.3387e-01, PNorm = 69.3083, GNorm = 0.1830
Meta loss on this task batch = 3.7775e-01, Meta loss averaged over last 500 steps = 4.3373e-01, PNorm = 69.3239, GNorm = 0.2661
Meta loss on this task batch = 4.0646e-01, Meta loss averaged over last 500 steps = 4.3379e-01, PNorm = 69.3407, GNorm = 0.2605
Meta loss on this task batch = 4.0270e-01, Meta loss averaged over last 500 steps = 4.3365e-01, PNorm = 69.3573, GNorm = 0.1586
Meta loss on this task batch = 4.2192e-01, Meta loss averaged over last 500 steps = 4.3364e-01, PNorm = 69.3726, GNorm = 0.1651
Meta loss on this task batch = 4.0021e-01, Meta loss averaged over last 500 steps = 4.3354e-01, PNorm = 69.3881, GNorm = 0.1876
Meta loss on this task batch = 4.0905e-01, Meta loss averaged over last 500 steps = 4.3332e-01, PNorm = 69.4033, GNorm = 0.1836
Meta loss on this task batch = 4.4512e-01, Meta loss averaged over last 500 steps = 4.3330e-01, PNorm = 69.4187, GNorm = 0.4324
Meta loss on this task batch = 4.3450e-01, Meta loss averaged over last 500 steps = 4.3335e-01, PNorm = 69.4319, GNorm = 0.2471
Meta loss on this task batch = 4.3820e-01, Meta loss averaged over last 500 steps = 4.3326e-01, PNorm = 69.4444, GNorm = 0.1445
Meta loss on this task batch = 4.4284e-01, Meta loss averaged over last 500 steps = 4.3334e-01, PNorm = 69.4568, GNorm = 0.2923
Meta loss on this task batch = 4.2232e-01, Meta loss averaged over last 500 steps = 4.3319e-01, PNorm = 69.4699, GNorm = 0.1437
Meta loss on this task batch = 3.8879e-01, Meta loss averaged over last 500 steps = 4.3310e-01, PNorm = 69.4843, GNorm = 0.1936
Meta loss on this task batch = 4.0165e-01, Meta loss averaged over last 500 steps = 4.3314e-01, PNorm = 69.4983, GNorm = 0.1358
Meta loss on this task batch = 4.2894e-01, Meta loss averaged over last 500 steps = 4.3312e-01, PNorm = 69.5130, GNorm = 0.1729
Meta loss on this task batch = 4.4240e-01, Meta loss averaged over last 500 steps = 4.3317e-01, PNorm = 69.5284, GNorm = 0.1590
Meta loss on this task batch = 4.8824e-01, Meta loss averaged over last 500 steps = 4.3333e-01, PNorm = 69.5427, GNorm = 0.2078
Took 115.99700260162354 seconds to complete one epoch of meta training
Took 123.00350141525269 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479668
Epoch 86
Meta loss on this task batch = 3.8802e-01, Meta loss averaged over last 500 steps = 4.3320e-01, PNorm = 69.5579, GNorm = 0.1423
Meta loss on this task batch = 3.8677e-01, Meta loss averaged over last 500 steps = 4.3311e-01, PNorm = 69.5734, GNorm = 0.1729
Meta loss on this task batch = 4.1428e-01, Meta loss averaged over last 500 steps = 4.3311e-01, PNorm = 69.5892, GNorm = 0.1455
Meta loss on this task batch = 3.7583e-01, Meta loss averaged over last 500 steps = 4.3299e-01, PNorm = 69.6060, GNorm = 0.2428
Meta loss on this task batch = 4.0210e-01, Meta loss averaged over last 500 steps = 4.3282e-01, PNorm = 69.6243, GNorm = 0.4665
Meta loss on this task batch = 4.3446e-01, Meta loss averaged over last 500 steps = 4.3285e-01, PNorm = 69.6424, GNorm = 0.2896
Meta loss on this task batch = 4.4074e-01, Meta loss averaged over last 500 steps = 4.3284e-01, PNorm = 69.6595, GNorm = 0.1714
Meta loss on this task batch = 4.6486e-01, Meta loss averaged over last 500 steps = 4.3288e-01, PNorm = 69.6750, GNorm = 0.2570
Meta loss on this task batch = 4.8023e-01, Meta loss averaged over last 500 steps = 4.3306e-01, PNorm = 69.6874, GNorm = 0.2191
Meta loss on this task batch = 3.8694e-01, Meta loss averaged over last 500 steps = 4.3304e-01, PNorm = 69.6994, GNorm = 0.1404
Meta loss on this task batch = 3.6145e-01, Meta loss averaged over last 500 steps = 4.3295e-01, PNorm = 69.7123, GNorm = 0.1691
Meta loss on this task batch = 4.3820e-01, Meta loss averaged over last 500 steps = 4.3291e-01, PNorm = 69.7258, GNorm = 0.1371
Meta loss on this task batch = 4.3113e-01, Meta loss averaged over last 500 steps = 4.3300e-01, PNorm = 69.7400, GNorm = 0.2759
Meta loss on this task batch = 3.8650e-01, Meta loss averaged over last 500 steps = 4.3291e-01, PNorm = 69.7544, GNorm = 0.1551
Meta loss on this task batch = 4.7028e-01, Meta loss averaged over last 500 steps = 4.3293e-01, PNorm = 69.7681, GNorm = 0.2548
Meta loss on this task batch = 4.8944e-01, Meta loss averaged over last 500 steps = 4.3299e-01, PNorm = 69.7822, GNorm = 0.2461
Meta loss on this task batch = 3.8708e-01, Meta loss averaged over last 500 steps = 4.3289e-01, PNorm = 69.7968, GNorm = 0.1539
Meta loss on this task batch = 3.8071e-01, Meta loss averaged over last 500 steps = 4.3274e-01, PNorm = 69.8109, GNorm = 0.2837
Meta loss on this task batch = 4.5720e-01, Meta loss averaged over last 500 steps = 4.3267e-01, PNorm = 69.8229, GNorm = 0.2075
Took 119.04695057868958 seconds to complete one epoch of meta training
Took 127.14754104614258 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462959
Epoch 87
Meta loss on this task batch = 3.9851e-01, Meta loss averaged over last 500 steps = 4.3241e-01, PNorm = 69.8355, GNorm = 0.1393
Meta loss on this task batch = 3.9676e-01, Meta loss averaged over last 500 steps = 4.3223e-01, PNorm = 69.8474, GNorm = 0.2089
Meta loss on this task batch = 4.5243e-01, Meta loss averaged over last 500 steps = 4.3225e-01, PNorm = 69.8589, GNorm = 0.3812
Meta loss on this task batch = 4.4159e-01, Meta loss averaged over last 500 steps = 4.3235e-01, PNorm = 69.8702, GNorm = 0.5223
Meta loss on this task batch = 3.9192e-01, Meta loss averaged over last 500 steps = 4.3230e-01, PNorm = 69.8824, GNorm = 0.1422
Meta loss on this task batch = 4.5770e-01, Meta loss averaged over last 500 steps = 4.3228e-01, PNorm = 69.8965, GNorm = 0.1773
Meta loss on this task batch = 4.1907e-01, Meta loss averaged over last 500 steps = 4.3214e-01, PNorm = 69.9119, GNorm = 0.3699
Meta loss on this task batch = 4.3978e-01, Meta loss averaged over last 500 steps = 4.3208e-01, PNorm = 69.9270, GNorm = 0.2372
Meta loss on this task batch = 4.0962e-01, Meta loss averaged over last 500 steps = 4.3207e-01, PNorm = 69.9441, GNorm = 0.2066
Meta loss on this task batch = 4.5464e-01, Meta loss averaged over last 500 steps = 4.3200e-01, PNorm = 69.9615, GNorm = 0.1251
Meta loss on this task batch = 3.8887e-01, Meta loss averaged over last 500 steps = 4.3193e-01, PNorm = 69.9798, GNorm = 0.1822
Meta loss on this task batch = 3.7819e-01, Meta loss averaged over last 500 steps = 4.3180e-01, PNorm = 69.9988, GNorm = 0.1415
Meta loss on this task batch = 4.5021e-01, Meta loss averaged over last 500 steps = 4.3178e-01, PNorm = 70.0166, GNorm = 0.1961
Meta loss on this task batch = 4.2338e-01, Meta loss averaged over last 500 steps = 4.3177e-01, PNorm = 70.0329, GNorm = 0.1712
Meta loss on this task batch = 4.6529e-01, Meta loss averaged over last 500 steps = 4.3184e-01, PNorm = 70.0475, GNorm = 0.5033
Meta loss on this task batch = 4.1160e-01, Meta loss averaged over last 500 steps = 4.3181e-01, PNorm = 70.0609, GNorm = 0.2234
Meta loss on this task batch = 4.1761e-01, Meta loss averaged over last 500 steps = 4.3174e-01, PNorm = 70.0752, GNorm = 0.3245
Meta loss on this task batch = 4.1478e-01, Meta loss averaged over last 500 steps = 4.3181e-01, PNorm = 70.0890, GNorm = 0.1988
Meta loss on this task batch = 4.4494e-01, Meta loss averaged over last 500 steps = 4.3175e-01, PNorm = 70.1025, GNorm = 0.2236
Took 110.7493782043457 seconds to complete one epoch of meta training
Took 119.03183221817017 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479841
Epoch 88
Meta loss on this task batch = 4.1490e-01, Meta loss averaged over last 500 steps = 4.3158e-01, PNorm = 70.1167, GNorm = 0.1580
Meta loss on this task batch = 4.0836e-01, Meta loss averaged over last 500 steps = 4.3154e-01, PNorm = 70.1321, GNorm = 0.1816
Meta loss on this task batch = 3.6536e-01, Meta loss averaged over last 500 steps = 4.3137e-01, PNorm = 70.1474, GNorm = 0.1811
Meta loss on this task batch = 4.4223e-01, Meta loss averaged over last 500 steps = 4.3140e-01, PNorm = 70.1642, GNorm = 0.3759
Meta loss on this task batch = 3.7074e-01, Meta loss averaged over last 500 steps = 4.3131e-01, PNorm = 70.1803, GNorm = 0.3372
Meta loss on this task batch = 4.1406e-01, Meta loss averaged over last 500 steps = 4.3116e-01, PNorm = 70.1953, GNorm = 0.1776
Meta loss on this task batch = 4.5699e-01, Meta loss averaged over last 500 steps = 4.3116e-01, PNorm = 70.2108, GNorm = 0.3735
Meta loss on this task batch = 4.0728e-01, Meta loss averaged over last 500 steps = 4.3101e-01, PNorm = 70.2254, GNorm = 0.1755
Meta loss on this task batch = 4.4421e-01, Meta loss averaged over last 500 steps = 4.3108e-01, PNorm = 70.2398, GNorm = 0.2089
Meta loss on this task batch = 3.5355e-01, Meta loss averaged over last 500 steps = 4.3094e-01, PNorm = 70.2558, GNorm = 0.3256
Meta loss on this task batch = 4.3767e-01, Meta loss averaged over last 500 steps = 4.3085e-01, PNorm = 70.2712, GNorm = 0.1682
Meta loss on this task batch = 4.6869e-01, Meta loss averaged over last 500 steps = 4.3086e-01, PNorm = 70.2856, GNorm = 0.1617
Meta loss on this task batch = 3.9412e-01, Meta loss averaged over last 500 steps = 4.3081e-01, PNorm = 70.2995, GNorm = 0.1912
Meta loss on this task batch = 4.4199e-01, Meta loss averaged over last 500 steps = 4.3087e-01, PNorm = 70.3140, GNorm = 0.1860
Meta loss on this task batch = 3.6173e-01, Meta loss averaged over last 500 steps = 4.3080e-01, PNorm = 70.3288, GNorm = 0.1376
Meta loss on this task batch = 5.0314e-01, Meta loss averaged over last 500 steps = 4.3088e-01, PNorm = 70.3415, GNorm = 0.1825
Meta loss on this task batch = 4.9304e-01, Meta loss averaged over last 500 steps = 4.3095e-01, PNorm = 70.3541, GNorm = 0.2528
Meta loss on this task batch = 4.2865e-01, Meta loss averaged over last 500 steps = 4.3083e-01, PNorm = 70.3664, GNorm = 0.1366
Meta loss on this task batch = 4.1382e-01, Meta loss averaged over last 500 steps = 4.3078e-01, PNorm = 70.3766, GNorm = 0.4430
Took 113.8122284412384 seconds to complete one epoch of meta training
Took 121.63278770446777 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454174
Epoch 89
Meta loss on this task batch = 4.9374e-01, Meta loss averaged over last 500 steps = 4.3086e-01, PNorm = 70.3859, GNorm = 0.1753
Meta loss on this task batch = 3.8533e-01, Meta loss averaged over last 500 steps = 4.3066e-01, PNorm = 70.3965, GNorm = 0.1155
Meta loss on this task batch = 3.7479e-01, Meta loss averaged over last 500 steps = 4.3049e-01, PNorm = 70.4090, GNorm = 0.1729
Meta loss on this task batch = 4.1654e-01, Meta loss averaged over last 500 steps = 4.3056e-01, PNorm = 70.4221, GNorm = 0.1909
Meta loss on this task batch = 4.0319e-01, Meta loss averaged over last 500 steps = 4.3038e-01, PNorm = 70.4365, GNorm = 0.1715
Meta loss on this task batch = 3.7440e-01, Meta loss averaged over last 500 steps = 4.3019e-01, PNorm = 70.4526, GNorm = 0.2361
Meta loss on this task batch = 4.5207e-01, Meta loss averaged over last 500 steps = 4.3023e-01, PNorm = 70.4686, GNorm = 0.2561
Meta loss on this task batch = 4.3616e-01, Meta loss averaged over last 500 steps = 4.3033e-01, PNorm = 70.4844, GNorm = 0.2447
Meta loss on this task batch = 3.2682e-01, Meta loss averaged over last 500 steps = 4.3013e-01, PNorm = 70.5008, GNorm = 0.1140
Meta loss on this task batch = 4.0511e-01, Meta loss averaged over last 500 steps = 4.3012e-01, PNorm = 70.5169, GNorm = 0.3514
Meta loss on this task batch = 4.3914e-01, Meta loss averaged over last 500 steps = 4.3018e-01, PNorm = 70.5318, GNorm = 0.1701
Meta loss on this task batch = 4.5453e-01, Meta loss averaged over last 500 steps = 4.3020e-01, PNorm = 70.5452, GNorm = 0.1753
Meta loss on this task batch = 3.7255e-01, Meta loss averaged over last 500 steps = 4.3005e-01, PNorm = 70.5598, GNorm = 0.3591
Meta loss on this task batch = 4.1639e-01, Meta loss averaged over last 500 steps = 4.2992e-01, PNorm = 70.5745, GNorm = 0.2969
Meta loss on this task batch = 4.2500e-01, Meta loss averaged over last 500 steps = 4.2978e-01, PNorm = 70.5887, GNorm = 0.3282
Meta loss on this task batch = 4.3800e-01, Meta loss averaged over last 500 steps = 4.2985e-01, PNorm = 70.6019, GNorm = 0.2422
Meta loss on this task batch = 4.8009e-01, Meta loss averaged over last 500 steps = 4.2992e-01, PNorm = 70.6133, GNorm = 0.2711
Meta loss on this task batch = 4.5991e-01, Meta loss averaged over last 500 steps = 4.3004e-01, PNorm = 70.6243, GNorm = 0.2050
Meta loss on this task batch = 3.9280e-01, Meta loss averaged over last 500 steps = 4.2998e-01, PNorm = 70.6348, GNorm = 0.2084
Took 114.85295033454895 seconds to complete one epoch of meta training
Took 122.74944972991943 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481154
Epoch 90
Meta loss on this task batch = 4.2783e-01, Meta loss averaged over last 500 steps = 4.2995e-01, PNorm = 70.6457, GNorm = 0.3303
Meta loss on this task batch = 4.3236e-01, Meta loss averaged over last 500 steps = 4.2984e-01, PNorm = 70.6568, GNorm = 0.3737
Meta loss on this task batch = 4.3443e-01, Meta loss averaged over last 500 steps = 4.2984e-01, PNorm = 70.6680, GNorm = 0.1615
Meta loss on this task batch = 3.6708e-01, Meta loss averaged over last 500 steps = 4.2966e-01, PNorm = 70.6801, GNorm = 0.1523
Meta loss on this task batch = 4.3901e-01, Meta loss averaged over last 500 steps = 4.2963e-01, PNorm = 70.6931, GNorm = 0.2255
Meta loss on this task batch = 4.2541e-01, Meta loss averaged over last 500 steps = 4.2961e-01, PNorm = 70.7076, GNorm = 0.3106
Meta loss on this task batch = 4.2218e-01, Meta loss averaged over last 500 steps = 4.2957e-01, PNorm = 70.7223, GNorm = 0.2534
Meta loss on this task batch = 4.4858e-01, Meta loss averaged over last 500 steps = 4.2960e-01, PNorm = 70.7373, GNorm = 0.1616
Meta loss on this task batch = 3.3980e-01, Meta loss averaged over last 500 steps = 4.2940e-01, PNorm = 70.7515, GNorm = 0.2289
Meta loss on this task batch = 4.0471e-01, Meta loss averaged over last 500 steps = 4.2938e-01, PNorm = 70.7644, GNorm = 0.2645
Meta loss on this task batch = 4.1966e-01, Meta loss averaged over last 500 steps = 4.2939e-01, PNorm = 70.7770, GNorm = 0.2338
Meta loss on this task batch = 4.4384e-01, Meta loss averaged over last 500 steps = 4.2946e-01, PNorm = 70.7897, GNorm = 0.1810
Meta loss on this task batch = 4.0047e-01, Meta loss averaged over last 500 steps = 4.2932e-01, PNorm = 70.8029, GNorm = 0.1647
Meta loss on this task batch = 3.8684e-01, Meta loss averaged over last 500 steps = 4.2927e-01, PNorm = 70.8175, GNorm = 0.1989
Meta loss on this task batch = 3.8330e-01, Meta loss averaged over last 500 steps = 4.2911e-01, PNorm = 70.8323, GNorm = 0.1380
Meta loss on this task batch = 5.0600e-01, Meta loss averaged over last 500 steps = 4.2935e-01, PNorm = 70.8449, GNorm = 0.2329
Meta loss on this task batch = 4.2239e-01, Meta loss averaged over last 500 steps = 4.2926e-01, PNorm = 70.8575, GNorm = 0.1697
Meta loss on this task batch = 3.7212e-01, Meta loss averaged over last 500 steps = 4.2919e-01, PNorm = 70.8727, GNorm = 0.4775
Meta loss on this task batch = 3.9805e-01, Meta loss averaged over last 500 steps = 4.2912e-01, PNorm = 70.8869, GNorm = 0.2673
Took 108.95592546463013 seconds to complete one epoch of meta training
Took 116.97863507270813 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475382
Epoch 91
Meta loss on this task batch = 4.2073e-01, Meta loss averaged over last 500 steps = 4.2901e-01, PNorm = 70.9008, GNorm = 0.1624
Meta loss on this task batch = 3.6118e-01, Meta loss averaged over last 500 steps = 4.2882e-01, PNorm = 70.9145, GNorm = 0.1511
Meta loss on this task batch = 3.9121e-01, Meta loss averaged over last 500 steps = 4.2886e-01, PNorm = 70.9282, GNorm = 0.2836
Meta loss on this task batch = 4.6705e-01, Meta loss averaged over last 500 steps = 4.2873e-01, PNorm = 70.9413, GNorm = 0.2040
Meta loss on this task batch = 3.5807e-01, Meta loss averaged over last 500 steps = 4.2852e-01, PNorm = 70.9544, GNorm = 0.1422
Meta loss on this task batch = 4.3058e-01, Meta loss averaged over last 500 steps = 4.2854e-01, PNorm = 70.9666, GNorm = 0.4286
Meta loss on this task batch = 4.3090e-01, Meta loss averaged over last 500 steps = 4.2849e-01, PNorm = 70.9798, GNorm = 0.2305
Meta loss on this task batch = 4.1752e-01, Meta loss averaged over last 500 steps = 4.2838e-01, PNorm = 70.9925, GNorm = 0.3651
Meta loss on this task batch = 4.1400e-01, Meta loss averaged over last 500 steps = 4.2842e-01, PNorm = 71.0057, GNorm = 0.1472
Meta loss on this task batch = 4.5396e-01, Meta loss averaged over last 500 steps = 4.2845e-01, PNorm = 71.0194, GNorm = 0.1975
Meta loss on this task batch = 4.4534e-01, Meta loss averaged over last 500 steps = 4.2848e-01, PNorm = 71.0313, GNorm = 0.2789
Meta loss on this task batch = 4.4443e-01, Meta loss averaged over last 500 steps = 4.2841e-01, PNorm = 71.0431, GNorm = 0.2639
Meta loss on this task batch = 3.8812e-01, Meta loss averaged over last 500 steps = 4.2835e-01, PNorm = 71.0556, GNorm = 0.1728
Meta loss on this task batch = 5.3276e-01, Meta loss averaged over last 500 steps = 4.2845e-01, PNorm = 71.0676, GNorm = 0.1561
Meta loss on this task batch = 3.7491e-01, Meta loss averaged over last 500 steps = 4.2842e-01, PNorm = 71.0802, GNorm = 0.2278
Meta loss on this task batch = 3.5227e-01, Meta loss averaged over last 500 steps = 4.2838e-01, PNorm = 71.0943, GNorm = 0.1997
Meta loss on this task batch = 4.0536e-01, Meta loss averaged over last 500 steps = 4.2823e-01, PNorm = 71.1084, GNorm = 0.2001
Meta loss on this task batch = 3.7498e-01, Meta loss averaged over last 500 steps = 4.2806e-01, PNorm = 71.1240, GNorm = 0.1903
Meta loss on this task batch = 4.1200e-01, Meta loss averaged over last 500 steps = 4.2802e-01, PNorm = 71.1398, GNorm = 0.1853
Took 122.46078324317932 seconds to complete one epoch of meta training
Took 129.59904503822327 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490738
Found better MAML checkpoint after meta validation, saving now
Epoch 92
Meta loss on this task batch = 3.7800e-01, Meta loss averaged over last 500 steps = 4.2795e-01, PNorm = 71.1554, GNorm = 0.1494
Meta loss on this task batch = 3.5811e-01, Meta loss averaged over last 500 steps = 4.2784e-01, PNorm = 71.1711, GNorm = 0.1598
Meta loss on this task batch = 4.3820e-01, Meta loss averaged over last 500 steps = 4.2789e-01, PNorm = 71.1864, GNorm = 0.2696
Meta loss on this task batch = 4.3369e-01, Meta loss averaged over last 500 steps = 4.2778e-01, PNorm = 71.2002, GNorm = 0.2125
Meta loss on this task batch = 4.5753e-01, Meta loss averaged over last 500 steps = 4.2782e-01, PNorm = 71.2153, GNorm = 0.4313
Meta loss on this task batch = 4.2496e-01, Meta loss averaged over last 500 steps = 4.2794e-01, PNorm = 71.2309, GNorm = 0.3105
Meta loss on this task batch = 4.2675e-01, Meta loss averaged over last 500 steps = 4.2785e-01, PNorm = 71.2468, GNorm = 0.1874
Meta loss on this task batch = 4.0648e-01, Meta loss averaged over last 500 steps = 4.2779e-01, PNorm = 71.2623, GNorm = 0.1527
Meta loss on this task batch = 3.9998e-01, Meta loss averaged over last 500 steps = 4.2771e-01, PNorm = 71.2766, GNorm = 0.3019
Meta loss on this task batch = 4.3821e-01, Meta loss averaged over last 500 steps = 4.2767e-01, PNorm = 71.2907, GNorm = 0.1810
Meta loss on this task batch = 3.5833e-01, Meta loss averaged over last 500 steps = 4.2735e-01, PNorm = 71.3054, GNorm = 0.2633
Meta loss on this task batch = 3.8742e-01, Meta loss averaged over last 500 steps = 4.2714e-01, PNorm = 71.3200, GNorm = 0.1827
Meta loss on this task batch = 5.0951e-01, Meta loss averaged over last 500 steps = 4.2723e-01, PNorm = 71.3326, GNorm = 0.3044
Meta loss on this task batch = 4.0658e-01, Meta loss averaged over last 500 steps = 4.2719e-01, PNorm = 71.3457, GNorm = 0.1595
Meta loss on this task batch = 4.6389e-01, Meta loss averaged over last 500 steps = 4.2734e-01, PNorm = 71.3592, GNorm = 0.3857
Meta loss on this task batch = 3.7486e-01, Meta loss averaged over last 500 steps = 4.2726e-01, PNorm = 71.3730, GNorm = 0.1728
Meta loss on this task batch = 4.3638e-01, Meta loss averaged over last 500 steps = 4.2728e-01, PNorm = 71.3860, GNorm = 0.2160
Meta loss on this task batch = 4.0054e-01, Meta loss averaged over last 500 steps = 4.2712e-01, PNorm = 71.3994, GNorm = 0.1371
Meta loss on this task batch = 3.7053e-01, Meta loss averaged over last 500 steps = 4.2700e-01, PNorm = 71.4139, GNorm = 0.1588
Took 118.85611343383789 seconds to complete one epoch of meta training
Took 126.50922846794128 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458334
Epoch 93
Meta loss on this task batch = 3.1880e-01, Meta loss averaged over last 500 steps = 4.2671e-01, PNorm = 71.4287, GNorm = 0.1542
Meta loss on this task batch = 4.1798e-01, Meta loss averaged over last 500 steps = 4.2657e-01, PNorm = 71.4439, GNorm = 0.1933
Meta loss on this task batch = 4.3114e-01, Meta loss averaged over last 500 steps = 4.2654e-01, PNorm = 71.4593, GNorm = 0.1705
Meta loss on this task batch = 4.1876e-01, Meta loss averaged over last 500 steps = 4.2638e-01, PNorm = 71.4751, GNorm = 0.2750
Meta loss on this task batch = 4.1725e-01, Meta loss averaged over last 500 steps = 4.2620e-01, PNorm = 71.4910, GNorm = 0.1661
Meta loss on this task batch = 3.9422e-01, Meta loss averaged over last 500 steps = 4.2611e-01, PNorm = 71.5052, GNorm = 0.2429
Meta loss on this task batch = 3.7433e-01, Meta loss averaged over last 500 steps = 4.2606e-01, PNorm = 71.5196, GNorm = 0.2113
Meta loss on this task batch = 5.1498e-01, Meta loss averaged over last 500 steps = 4.2630e-01, PNorm = 71.5332, GNorm = 0.1930
Meta loss on this task batch = 3.9665e-01, Meta loss averaged over last 500 steps = 4.2625e-01, PNorm = 71.5477, GNorm = 0.2222
Meta loss on this task batch = 4.1352e-01, Meta loss averaged over last 500 steps = 4.2627e-01, PNorm = 71.5619, GNorm = 0.1582
Meta loss on this task batch = 3.8425e-01, Meta loss averaged over last 500 steps = 4.2615e-01, PNorm = 71.5750, GNorm = 0.2364
Meta loss on this task batch = 4.4077e-01, Meta loss averaged over last 500 steps = 4.2621e-01, PNorm = 71.5897, GNorm = 0.2706
Meta loss on this task batch = 4.4144e-01, Meta loss averaged over last 500 steps = 4.2609e-01, PNorm = 71.6028, GNorm = 0.3654
Meta loss on this task batch = 4.1699e-01, Meta loss averaged over last 500 steps = 4.2596e-01, PNorm = 71.6166, GNorm = 0.1920
Meta loss on this task batch = 4.1808e-01, Meta loss averaged over last 500 steps = 4.2599e-01, PNorm = 71.6298, GNorm = 0.1617
Meta loss on this task batch = 5.0325e-01, Meta loss averaged over last 500 steps = 4.2603e-01, PNorm = 71.6431, GNorm = 0.1984
Meta loss on this task batch = 3.8398e-01, Meta loss averaged over last 500 steps = 4.2590e-01, PNorm = 71.6562, GNorm = 0.2013
Meta loss on this task batch = 4.0638e-01, Meta loss averaged over last 500 steps = 4.2576e-01, PNorm = 71.6690, GNorm = 0.1643
Meta loss on this task batch = 3.6916e-01, Meta loss averaged over last 500 steps = 4.2556e-01, PNorm = 71.6818, GNorm = 0.1639
Took 120.83394765853882 seconds to complete one epoch of meta training
Took 127.6421639919281 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462293
Epoch 94
Meta loss on this task batch = 4.5325e-01, Meta loss averaged over last 500 steps = 4.2553e-01, PNorm = 71.6954, GNorm = 0.2422
Meta loss on this task batch = 4.1394e-01, Meta loss averaged over last 500 steps = 4.2539e-01, PNorm = 71.7086, GNorm = 0.1667
Meta loss on this task batch = 4.3550e-01, Meta loss averaged over last 500 steps = 4.2535e-01, PNorm = 71.7208, GNorm = 0.1984
Meta loss on this task batch = 3.6550e-01, Meta loss averaged over last 500 steps = 4.2510e-01, PNorm = 71.7333, GNorm = 0.2003
Meta loss on this task batch = 3.8116e-01, Meta loss averaged over last 500 steps = 4.2498e-01, PNorm = 71.7462, GNorm = 0.1480
Meta loss on this task batch = 4.7032e-01, Meta loss averaged over last 500 steps = 4.2497e-01, PNorm = 71.7597, GNorm = 0.3519
Meta loss on this task batch = 4.4769e-01, Meta loss averaged over last 500 steps = 4.2495e-01, PNorm = 71.7736, GNorm = 0.1688
Meta loss on this task batch = 3.4666e-01, Meta loss averaged over last 500 steps = 4.2465e-01, PNorm = 71.7886, GNorm = 0.1825
Meta loss on this task batch = 4.6814e-01, Meta loss averaged over last 500 steps = 4.2468e-01, PNorm = 71.8032, GNorm = 0.1463
Meta loss on this task batch = 4.4728e-01, Meta loss averaged over last 500 steps = 4.2480e-01, PNorm = 71.8174, GNorm = 0.1904
Meta loss on this task batch = 4.0599e-01, Meta loss averaged over last 500 steps = 4.2474e-01, PNorm = 71.8311, GNorm = 0.1533
Meta loss on this task batch = 4.4249e-01, Meta loss averaged over last 500 steps = 4.2476e-01, PNorm = 71.8451, GNorm = 0.1747
Meta loss on this task batch = 4.1613e-01, Meta loss averaged over last 500 steps = 4.2472e-01, PNorm = 71.8590, GNorm = 0.1407
Meta loss on this task batch = 3.8859e-01, Meta loss averaged over last 500 steps = 4.2458e-01, PNorm = 71.8725, GNorm = 0.1511
Meta loss on this task batch = 4.3996e-01, Meta loss averaged over last 500 steps = 4.2464e-01, PNorm = 71.8862, GNorm = 0.1750
Meta loss on this task batch = 4.0592e-01, Meta loss averaged over last 500 steps = 4.2469e-01, PNorm = 71.8996, GNorm = 0.1633
Meta loss on this task batch = 4.1085e-01, Meta loss averaged over last 500 steps = 4.2474e-01, PNorm = 71.9130, GNorm = 0.2123
Meta loss on this task batch = 3.8738e-01, Meta loss averaged over last 500 steps = 4.2459e-01, PNorm = 71.9263, GNorm = 0.1934
Meta loss on this task batch = 3.6726e-01, Meta loss averaged over last 500 steps = 4.2438e-01, PNorm = 71.9402, GNorm = 0.1652
Took 118.45567393302917 seconds to complete one epoch of meta training
Took 126.39733958244324 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483491
Epoch 95
Meta loss on this task batch = 4.1328e-01, Meta loss averaged over last 500 steps = 4.2430e-01, PNorm = 71.9544, GNorm = 0.1455
Meta loss on this task batch = 3.6568e-01, Meta loss averaged over last 500 steps = 4.2412e-01, PNorm = 71.9691, GNorm = 0.2225
Meta loss on this task batch = 3.3071e-01, Meta loss averaged over last 500 steps = 4.2387e-01, PNorm = 71.9846, GNorm = 0.2183
Meta loss on this task batch = 4.7262e-01, Meta loss averaged over last 500 steps = 4.2387e-01, PNorm = 71.9992, GNorm = 0.2090
Meta loss on this task batch = 4.5897e-01, Meta loss averaged over last 500 steps = 4.2393e-01, PNorm = 72.0121, GNorm = 0.2276
Meta loss on this task batch = 4.0206e-01, Meta loss averaged over last 500 steps = 4.2378e-01, PNorm = 72.0249, GNorm = 0.1782
Meta loss on this task batch = 4.6840e-01, Meta loss averaged over last 500 steps = 4.2380e-01, PNorm = 72.0372, GNorm = 0.3370
Meta loss on this task batch = 4.1558e-01, Meta loss averaged over last 500 steps = 4.2389e-01, PNorm = 72.0504, GNorm = 0.1720
Meta loss on this task batch = 4.3344e-01, Meta loss averaged over last 500 steps = 4.2373e-01, PNorm = 72.0636, GNorm = 0.1697
Meta loss on this task batch = 4.5240e-01, Meta loss averaged over last 500 steps = 4.2382e-01, PNorm = 72.0762, GNorm = 0.2487
Meta loss on this task batch = 4.2334e-01, Meta loss averaged over last 500 steps = 4.2376e-01, PNorm = 72.0878, GNorm = 0.2741
Meta loss on this task batch = 3.7548e-01, Meta loss averaged over last 500 steps = 4.2362e-01, PNorm = 72.1008, GNorm = 0.1378
Meta loss on this task batch = 4.0071e-01, Meta loss averaged over last 500 steps = 4.2351e-01, PNorm = 72.1134, GNorm = 0.2203
Meta loss on this task batch = 4.4840e-01, Meta loss averaged over last 500 steps = 4.2349e-01, PNorm = 72.1254, GNorm = 0.2175
Meta loss on this task batch = 3.8618e-01, Meta loss averaged over last 500 steps = 4.2328e-01, PNorm = 72.1385, GNorm = 0.1548
Meta loss on this task batch = 3.6193e-01, Meta loss averaged over last 500 steps = 4.2313e-01, PNorm = 72.1533, GNorm = 0.1621
Meta loss on this task batch = 3.7433e-01, Meta loss averaged over last 500 steps = 4.2296e-01, PNorm = 72.1688, GNorm = 0.1772
Meta loss on this task batch = 4.1952e-01, Meta loss averaged over last 500 steps = 4.2299e-01, PNorm = 72.1833, GNorm = 0.2068
Meta loss on this task batch = 4.0159e-01, Meta loss averaged over last 500 steps = 4.2298e-01, PNorm = 72.1972, GNorm = 0.2160
Took 114.36791944503784 seconds to complete one epoch of meta training
Took 122.09251308441162 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449571
Epoch 96
Meta loss on this task batch = 4.5313e-01, Meta loss averaged over last 500 steps = 4.2310e-01, PNorm = 72.2117, GNorm = 0.1796
Meta loss on this task batch = 3.8573e-01, Meta loss averaged over last 500 steps = 4.2305e-01, PNorm = 72.2259, GNorm = 0.1671
Meta loss on this task batch = 3.9660e-01, Meta loss averaged over last 500 steps = 4.2303e-01, PNorm = 72.2396, GNorm = 0.1611
Meta loss on this task batch = 3.7269e-01, Meta loss averaged over last 500 steps = 4.2296e-01, PNorm = 72.2538, GNorm = 0.1721
Meta loss on this task batch = 4.2004e-01, Meta loss averaged over last 500 steps = 4.2291e-01, PNorm = 72.2671, GNorm = 0.2629
Meta loss on this task batch = 4.3981e-01, Meta loss averaged over last 500 steps = 4.2286e-01, PNorm = 72.2800, GNorm = 0.1941
Meta loss on this task batch = 4.2875e-01, Meta loss averaged over last 500 steps = 4.2293e-01, PNorm = 72.2933, GNorm = 0.2119
Meta loss on this task batch = 4.3213e-01, Meta loss averaged over last 500 steps = 4.2292e-01, PNorm = 72.3048, GNorm = 0.4386
Meta loss on this task batch = 3.3917e-01, Meta loss averaged over last 500 steps = 4.2278e-01, PNorm = 72.3172, GNorm = 0.1959
Meta loss on this task batch = 3.9569e-01, Meta loss averaged over last 500 steps = 4.2274e-01, PNorm = 72.3306, GNorm = 0.1546
Meta loss on this task batch = 3.6015e-01, Meta loss averaged over last 500 steps = 4.2258e-01, PNorm = 72.3442, GNorm = 0.1422
Meta loss on this task batch = 4.1452e-01, Meta loss averaged over last 500 steps = 4.2247e-01, PNorm = 72.3577, GNorm = 0.1911
Meta loss on this task batch = 3.8066e-01, Meta loss averaged over last 500 steps = 4.2227e-01, PNorm = 72.3712, GNorm = 0.1672
Meta loss on this task batch = 3.2689e-01, Meta loss averaged over last 500 steps = 4.2198e-01, PNorm = 72.3837, GNorm = 0.2656
Meta loss on this task batch = 4.8429e-01, Meta loss averaged over last 500 steps = 4.2202e-01, PNorm = 72.3972, GNorm = 0.1969
Meta loss on this task batch = 3.9698e-01, Meta loss averaged over last 500 steps = 4.2193e-01, PNorm = 72.4113, GNorm = 0.1397
Meta loss on this task batch = 4.1066e-01, Meta loss averaged over last 500 steps = 4.2182e-01, PNorm = 72.4256, GNorm = 0.1726
Meta loss on this task batch = 4.6715e-01, Meta loss averaged over last 500 steps = 4.2194e-01, PNorm = 72.4382, GNorm = 0.1649
Meta loss on this task batch = 4.6734e-01, Meta loss averaged over last 500 steps = 4.2206e-01, PNorm = 72.4517, GNorm = 0.1952
Took 113.46088314056396 seconds to complete one epoch of meta training
Took 121.52565670013428 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495676
Found better MAML checkpoint after meta validation, saving now
Epoch 97
Meta loss on this task batch = 3.0318e-01, Meta loss averaged over last 500 steps = 4.2187e-01, PNorm = 72.4644, GNorm = 0.3235
Meta loss on this task batch = 3.6915e-01, Meta loss averaged over last 500 steps = 4.2167e-01, PNorm = 72.4772, GNorm = 0.1341
Meta loss on this task batch = 3.9698e-01, Meta loss averaged over last 500 steps = 4.2174e-01, PNorm = 72.4897, GNorm = 0.1792
Meta loss on this task batch = 4.6720e-01, Meta loss averaged over last 500 steps = 4.2182e-01, PNorm = 72.5015, GNorm = 0.1679
Meta loss on this task batch = 4.1553e-01, Meta loss averaged over last 500 steps = 4.2176e-01, PNorm = 72.5158, GNorm = 0.4436
Meta loss on this task batch = 3.9794e-01, Meta loss averaged over last 500 steps = 4.2161e-01, PNorm = 72.5298, GNorm = 0.1538
Meta loss on this task batch = 4.6918e-01, Meta loss averaged over last 500 steps = 4.2181e-01, PNorm = 72.5432, GNorm = 0.3370
Meta loss on this task batch = 4.3462e-01, Meta loss averaged over last 500 steps = 4.2175e-01, PNorm = 72.5577, GNorm = 0.1520
Meta loss on this task batch = 3.9214e-01, Meta loss averaged over last 500 steps = 4.2160e-01, PNorm = 72.5726, GNorm = 0.2159
Meta loss on this task batch = 4.0580e-01, Meta loss averaged over last 500 steps = 4.2158e-01, PNorm = 72.5857, GNorm = 0.4242
Meta loss on this task batch = 3.9083e-01, Meta loss averaged over last 500 steps = 4.2153e-01, PNorm = 72.5989, GNorm = 0.1577
Meta loss on this task batch = 4.3573e-01, Meta loss averaged over last 500 steps = 4.2148e-01, PNorm = 72.6124, GNorm = 0.1351
Meta loss on this task batch = 4.3045e-01, Meta loss averaged over last 500 steps = 4.2150e-01, PNorm = 72.6271, GNorm = 0.2874
Meta loss on this task batch = 4.6201e-01, Meta loss averaged over last 500 steps = 4.2155e-01, PNorm = 72.6414, GNorm = 0.1481
Meta loss on this task batch = 3.4176e-01, Meta loss averaged over last 500 steps = 4.2142e-01, PNorm = 72.6575, GNorm = 0.2864
Meta loss on this task batch = 4.9230e-01, Meta loss averaged over last 500 steps = 4.2168e-01, PNorm = 72.6723, GNorm = 0.1968
Meta loss on this task batch = 4.1039e-01, Meta loss averaged over last 500 steps = 4.2149e-01, PNorm = 72.6880, GNorm = 0.1678
Meta loss on this task batch = 3.9335e-01, Meta loss averaged over last 500 steps = 4.2143e-01, PNorm = 72.7049, GNorm = 0.2749
Meta loss on this task batch = 4.0768e-01, Meta loss averaged over last 500 steps = 4.2148e-01, PNorm = 72.7217, GNorm = 0.2347
Took 118.69492292404175 seconds to complete one epoch of meta training
Took 125.8606390953064 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473075
Epoch 98
Meta loss on this task batch = 3.7979e-01, Meta loss averaged over last 500 steps = 4.2137e-01, PNorm = 72.7377, GNorm = 0.3144
Meta loss on this task batch = 3.4736e-01, Meta loss averaged over last 500 steps = 4.2121e-01, PNorm = 72.7536, GNorm = 0.2121
Meta loss on this task batch = 2.9527e-01, Meta loss averaged over last 500 steps = 4.2083e-01, PNorm = 72.7702, GNorm = 0.1318
Meta loss on this task batch = 3.9806e-01, Meta loss averaged over last 500 steps = 4.2070e-01, PNorm = 72.7874, GNorm = 0.1595
Meta loss on this task batch = 4.4977e-01, Meta loss averaged over last 500 steps = 4.2088e-01, PNorm = 72.8051, GNorm = 0.1526
Meta loss on this task batch = 4.1059e-01, Meta loss averaged over last 500 steps = 4.2075e-01, PNorm = 72.8226, GNorm = 0.2156
Meta loss on this task batch = 4.4164e-01, Meta loss averaged over last 500 steps = 4.2075e-01, PNorm = 72.8396, GNorm = 0.1946
Meta loss on this task batch = 4.4418e-01, Meta loss averaged over last 500 steps = 4.2076e-01, PNorm = 72.8556, GNorm = 0.1543
Meta loss on this task batch = 3.9058e-01, Meta loss averaged over last 500 steps = 4.2057e-01, PNorm = 72.8717, GNorm = 0.1621
Meta loss on this task batch = 4.0081e-01, Meta loss averaged over last 500 steps = 4.2063e-01, PNorm = 72.8875, GNorm = 0.1494
Meta loss on this task batch = 4.5939e-01, Meta loss averaged over last 500 steps = 4.2066e-01, PNorm = 72.9020, GNorm = 0.2232
Meta loss on this task batch = 3.9221e-01, Meta loss averaged over last 500 steps = 4.2050e-01, PNorm = 72.9162, GNorm = 0.1743
Meta loss on this task batch = 4.8286e-01, Meta loss averaged over last 500 steps = 4.2053e-01, PNorm = 72.9294, GNorm = 0.2093
Meta loss on this task batch = 4.1278e-01, Meta loss averaged over last 500 steps = 4.2053e-01, PNorm = 72.9427, GNorm = 0.1642
Meta loss on this task batch = 4.1326e-01, Meta loss averaged over last 500 steps = 4.2046e-01, PNorm = 72.9564, GNorm = 0.2341
Meta loss on this task batch = 3.7826e-01, Meta loss averaged over last 500 steps = 4.2037e-01, PNorm = 72.9684, GNorm = 0.4408
Meta loss on this task batch = 4.5070e-01, Meta loss averaged over last 500 steps = 4.2039e-01, PNorm = 72.9792, GNorm = 0.2277
Meta loss on this task batch = 4.3575e-01, Meta loss averaged over last 500 steps = 4.2036e-01, PNorm = 72.9892, GNorm = 0.3586
Meta loss on this task batch = 3.6248e-01, Meta loss averaged over last 500 steps = 4.2028e-01, PNorm = 73.0001, GNorm = 0.2671
Took 112.76164102554321 seconds to complete one epoch of meta training
Took 120.80945706367493 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468739
Epoch 99
Meta loss on this task batch = 4.4316e-01, Meta loss averaged over last 500 steps = 4.2031e-01, PNorm = 73.0120, GNorm = 0.1574
Meta loss on this task batch = 4.5803e-01, Meta loss averaged over last 500 steps = 4.2019e-01, PNorm = 73.0240, GNorm = 0.2752
Meta loss on this task batch = 4.5889e-01, Meta loss averaged over last 500 steps = 4.2020e-01, PNorm = 73.0391, GNorm = 0.5819
Meta loss on this task batch = 3.4639e-01, Meta loss averaged over last 500 steps = 4.2006e-01, PNorm = 73.0547, GNorm = 0.2125
Meta loss on this task batch = 4.8861e-01, Meta loss averaged over last 500 steps = 4.2030e-01, PNorm = 73.0707, GNorm = 0.2293
Meta loss on this task batch = 4.3514e-01, Meta loss averaged over last 500 steps = 4.2042e-01, PNorm = 73.0863, GNorm = 0.1806
Meta loss on this task batch = 3.9956e-01, Meta loss averaged over last 500 steps = 4.2037e-01, PNorm = 73.1032, GNorm = 0.2758
Meta loss on this task batch = 3.9539e-01, Meta loss averaged over last 500 steps = 4.2021e-01, PNorm = 73.1201, GNorm = 0.2244
Meta loss on this task batch = 3.9496e-01, Meta loss averaged over last 500 steps = 4.2003e-01, PNorm = 73.1362, GNorm = 0.1653
Meta loss on this task batch = 4.0662e-01, Meta loss averaged over last 500 steps = 4.1993e-01, PNorm = 73.1532, GNorm = 0.1827
Meta loss on this task batch = 4.3197e-01, Meta loss averaged over last 500 steps = 4.1999e-01, PNorm = 73.1690, GNorm = 0.2170
Meta loss on this task batch = 3.4519e-01, Meta loss averaged over last 500 steps = 4.1988e-01, PNorm = 73.1847, GNorm = 0.2321
Meta loss on this task batch = 4.0860e-01, Meta loss averaged over last 500 steps = 4.1987e-01, PNorm = 73.2008, GNorm = 0.2389
Meta loss on this task batch = 4.2799e-01, Meta loss averaged over last 500 steps = 4.1982e-01, PNorm = 73.2160, GNorm = 0.2505
Meta loss on this task batch = 4.2484e-01, Meta loss averaged over last 500 steps = 4.1984e-01, PNorm = 73.2304, GNorm = 0.2700
Meta loss on this task batch = 3.9867e-01, Meta loss averaged over last 500 steps = 4.1975e-01, PNorm = 73.2456, GNorm = 0.3944
Meta loss on this task batch = 3.4530e-01, Meta loss averaged over last 500 steps = 4.1950e-01, PNorm = 73.2604, GNorm = 0.1578
Meta loss on this task batch = 3.7718e-01, Meta loss averaged over last 500 steps = 4.1931e-01, PNorm = 73.2747, GNorm = 0.1476
Meta loss on this task batch = 4.5175e-01, Meta loss averaged over last 500 steps = 4.1940e-01, PNorm = 73.2893, GNorm = 0.2204
Took 111.69593095779419 seconds to complete one epoch of meta training
Took 119.45991706848145 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467550
Epoch 100
Meta loss on this task batch = 4.4568e-01, Meta loss averaged over last 500 steps = 4.1954e-01, PNorm = 73.3022, GNorm = 0.1481
Meta loss on this task batch = 4.4975e-01, Meta loss averaged over last 500 steps = 4.1955e-01, PNorm = 73.3135, GNorm = 0.2565
Meta loss on this task batch = 3.7039e-01, Meta loss averaged over last 500 steps = 4.1937e-01, PNorm = 73.3254, GNorm = 0.3801
Meta loss on this task batch = 3.8046e-01, Meta loss averaged over last 500 steps = 4.1939e-01, PNorm = 73.3382, GNorm = 0.3152
Meta loss on this task batch = 3.7708e-01, Meta loss averaged over last 500 steps = 4.1928e-01, PNorm = 73.3523, GNorm = 0.4014
Meta loss on this task batch = 4.2922e-01, Meta loss averaged over last 500 steps = 4.1923e-01, PNorm = 73.3668, GNorm = 0.1693
Meta loss on this task batch = 3.9418e-01, Meta loss averaged over last 500 steps = 4.1917e-01, PNorm = 73.3819, GNorm = 0.2589
Meta loss on this task batch = 4.5585e-01, Meta loss averaged over last 500 steps = 4.1918e-01, PNorm = 73.3974, GNorm = 0.1614
Meta loss on this task batch = 4.0254e-01, Meta loss averaged over last 500 steps = 4.1911e-01, PNorm = 73.4132, GNorm = 0.2150
Meta loss on this task batch = 3.9614e-01, Meta loss averaged over last 500 steps = 4.1901e-01, PNorm = 73.4302, GNorm = 0.1509
Meta loss on this task batch = 3.8078e-01, Meta loss averaged over last 500 steps = 4.1896e-01, PNorm = 73.4474, GNorm = 0.2104
Meta loss on this task batch = 4.6259e-01, Meta loss averaged over last 500 steps = 4.1893e-01, PNorm = 73.4630, GNorm = 0.2634
Meta loss on this task batch = 4.7747e-01, Meta loss averaged over last 500 steps = 4.1909e-01, PNorm = 73.4766, GNorm = 0.2451
Meta loss on this task batch = 4.6152e-01, Meta loss averaged over last 500 steps = 4.1914e-01, PNorm = 73.4900, GNorm = 0.4032
Meta loss on this task batch = 3.8795e-01, Meta loss averaged over last 500 steps = 4.1904e-01, PNorm = 73.5043, GNorm = 0.1463
Meta loss on this task batch = 4.4252e-01, Meta loss averaged over last 500 steps = 4.1903e-01, PNorm = 73.5197, GNorm = 0.2307
Meta loss on this task batch = 3.1427e-01, Meta loss averaged over last 500 steps = 4.1893e-01, PNorm = 73.5351, GNorm = 0.2223
Meta loss on this task batch = 4.5764e-01, Meta loss averaged over last 500 steps = 4.1911e-01, PNorm = 73.5512, GNorm = 0.3376
Meta loss on this task batch = 4.0999e-01, Meta loss averaged over last 500 steps = 4.1919e-01, PNorm = 73.5684, GNorm = 0.1495
Took 109.50560283660889 seconds to complete one epoch of meta training
Took 117.283043384552 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447944
Epoch 101
Meta loss on this task batch = 4.0109e-01, Meta loss averaged over last 500 steps = 4.1906e-01, PNorm = 73.5856, GNorm = 0.1669
Meta loss on this task batch = 4.3238e-01, Meta loss averaged over last 500 steps = 4.1889e-01, PNorm = 73.6037, GNorm = 0.1585
Meta loss on this task batch = 4.0940e-01, Meta loss averaged over last 500 steps = 4.1894e-01, PNorm = 73.6221, GNorm = 0.2260
Meta loss on this task batch = 4.4004e-01, Meta loss averaged over last 500 steps = 4.1897e-01, PNorm = 73.6409, GNorm = 0.2403
Meta loss on this task batch = 3.9375e-01, Meta loss averaged over last 500 steps = 4.1885e-01, PNorm = 73.6588, GNorm = 0.1964
Meta loss on this task batch = 4.0221e-01, Meta loss averaged over last 500 steps = 4.1888e-01, PNorm = 73.6770, GNorm = 0.2554
Meta loss on this task batch = 3.7454e-01, Meta loss averaged over last 500 steps = 4.1877e-01, PNorm = 73.6965, GNorm = 0.1563
Meta loss on this task batch = 3.4788e-01, Meta loss averaged over last 500 steps = 4.1865e-01, PNorm = 73.7156, GNorm = 0.1613
Meta loss on this task batch = 4.1851e-01, Meta loss averaged over last 500 steps = 4.1862e-01, PNorm = 73.7348, GNorm = 0.1624
Meta loss on this task batch = 3.9850e-01, Meta loss averaged over last 500 steps = 4.1868e-01, PNorm = 73.7542, GNorm = 0.3179
Meta loss on this task batch = 3.5217e-01, Meta loss averaged over last 500 steps = 4.1851e-01, PNorm = 73.7742, GNorm = 0.1567
Meta loss on this task batch = 4.3460e-01, Meta loss averaged over last 500 steps = 4.1850e-01, PNorm = 73.7926, GNorm = 0.2276
Meta loss on this task batch = 4.7814e-01, Meta loss averaged over last 500 steps = 4.1857e-01, PNorm = 73.8093, GNorm = 0.2125
Meta loss on this task batch = 4.3832e-01, Meta loss averaged over last 500 steps = 4.1863e-01, PNorm = 73.8252, GNorm = 0.2791
Meta loss on this task batch = 3.6969e-01, Meta loss averaged over last 500 steps = 4.1837e-01, PNorm = 73.8412, GNorm = 0.1960
Meta loss on this task batch = 4.0219e-01, Meta loss averaged over last 500 steps = 4.1829e-01, PNorm = 73.8574, GNorm = 0.1840
Meta loss on this task batch = 4.6401e-01, Meta loss averaged over last 500 steps = 4.1834e-01, PNorm = 73.8716, GNorm = 0.1963
Meta loss on this task batch = 4.4579e-01, Meta loss averaged over last 500 steps = 4.1841e-01, PNorm = 73.8852, GNorm = 0.1896
Meta loss on this task batch = 4.1799e-01, Meta loss averaged over last 500 steps = 4.1841e-01, PNorm = 73.8982, GNorm = 0.2953
Took 111.23124313354492 seconds to complete one epoch of meta training
Took 118.92596697807312 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456190
Epoch 102
Meta loss on this task batch = 4.0221e-01, Meta loss averaged over last 500 steps = 4.1839e-01, PNorm = 73.9113, GNorm = 0.2026
Meta loss on this task batch = 4.3666e-01, Meta loss averaged over last 500 steps = 4.1844e-01, PNorm = 73.9251, GNorm = 0.1722
Meta loss on this task batch = 4.3926e-01, Meta loss averaged over last 500 steps = 4.1844e-01, PNorm = 73.9379, GNorm = 0.1298
Meta loss on this task batch = 3.7671e-01, Meta loss averaged over last 500 steps = 4.1844e-01, PNorm = 73.9513, GNorm = 0.2125
Meta loss on this task batch = 4.0225e-01, Meta loss averaged over last 500 steps = 4.1849e-01, PNorm = 73.9652, GNorm = 0.2630
Meta loss on this task batch = 3.8281e-01, Meta loss averaged over last 500 steps = 4.1846e-01, PNorm = 73.9786, GNorm = 0.1531
Meta loss on this task batch = 3.8406e-01, Meta loss averaged over last 500 steps = 4.1838e-01, PNorm = 73.9928, GNorm = 0.1909
Meta loss on this task batch = 4.3362e-01, Meta loss averaged over last 500 steps = 4.1844e-01, PNorm = 74.0065, GNorm = 0.1951
Meta loss on this task batch = 3.9394e-01, Meta loss averaged over last 500 steps = 4.1832e-01, PNorm = 74.0214, GNorm = 0.2847
Meta loss on this task batch = 4.1166e-01, Meta loss averaged over last 500 steps = 4.1834e-01, PNorm = 74.0370, GNorm = 0.1741
Meta loss on this task batch = 4.1036e-01, Meta loss averaged over last 500 steps = 4.1829e-01, PNorm = 74.0524, GNorm = 0.4227
Meta loss on this task batch = 4.1660e-01, Meta loss averaged over last 500 steps = 4.1848e-01, PNorm = 74.0679, GNorm = 0.2364
Meta loss on this task batch = 4.6374e-01, Meta loss averaged over last 500 steps = 4.1857e-01, PNorm = 74.0826, GNorm = 0.1944
Meta loss on this task batch = 3.9902e-01, Meta loss averaged over last 500 steps = 4.1843e-01, PNorm = 74.0970, GNorm = 0.1758
Meta loss on this task batch = 3.8993e-01, Meta loss averaged over last 500 steps = 4.1835e-01, PNorm = 74.1109, GNorm = 0.1604
Meta loss on this task batch = 3.6663e-01, Meta loss averaged over last 500 steps = 4.1816e-01, PNorm = 74.1240, GNorm = 0.3607
Meta loss on this task batch = 3.7351e-01, Meta loss averaged over last 500 steps = 4.1793e-01, PNorm = 74.1379, GNorm = 0.1560
Meta loss on this task batch = 4.5336e-01, Meta loss averaged over last 500 steps = 4.1805e-01, PNorm = 74.1509, GNorm = 0.1950
Meta loss on this task batch = 3.3562e-01, Meta loss averaged over last 500 steps = 4.1785e-01, PNorm = 74.1631, GNorm = 0.1762
Took 110.86460947990417 seconds to complete one epoch of meta training
Took 118.78535866737366 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465683
Epoch 103
Meta loss on this task batch = 3.7826e-01, Meta loss averaged over last 500 steps = 4.1784e-01, PNorm = 74.1759, GNorm = 0.1856
Meta loss on this task batch = 4.3529e-01, Meta loss averaged over last 500 steps = 4.1785e-01, PNorm = 74.1890, GNorm = 0.1668
Meta loss on this task batch = 4.0490e-01, Meta loss averaged over last 500 steps = 4.1762e-01, PNorm = 74.2016, GNorm = 0.2639
Meta loss on this task batch = 3.9564e-01, Meta loss averaged over last 500 steps = 4.1761e-01, PNorm = 74.2143, GNorm = 0.1905
Meta loss on this task batch = 3.6904e-01, Meta loss averaged over last 500 steps = 4.1744e-01, PNorm = 74.2278, GNorm = 0.1771
Meta loss on this task batch = 3.9367e-01, Meta loss averaged over last 500 steps = 4.1733e-01, PNorm = 74.2417, GNorm = 0.2232
Meta loss on this task batch = 4.4840e-01, Meta loss averaged over last 500 steps = 4.1736e-01, PNorm = 74.2553, GNorm = 0.2105
Meta loss on this task batch = 4.0251e-01, Meta loss averaged over last 500 steps = 4.1740e-01, PNorm = 74.2694, GNorm = 0.2247
Meta loss on this task batch = 4.1015e-01, Meta loss averaged over last 500 steps = 4.1741e-01, PNorm = 74.2835, GNorm = 0.2447
Meta loss on this task batch = 4.0683e-01, Meta loss averaged over last 500 steps = 4.1735e-01, PNorm = 74.2978, GNorm = 0.1911
Meta loss on this task batch = 4.3746e-01, Meta loss averaged over last 500 steps = 4.1733e-01, PNorm = 74.3110, GNorm = 0.2861
Meta loss on this task batch = 3.6405e-01, Meta loss averaged over last 500 steps = 4.1713e-01, PNorm = 74.3240, GNorm = 0.1805
Meta loss on this task batch = 4.0293e-01, Meta loss averaged over last 500 steps = 4.1710e-01, PNorm = 74.3372, GNorm = 0.1967
Meta loss on this task batch = 4.0231e-01, Meta loss averaged over last 500 steps = 4.1704e-01, PNorm = 74.3512, GNorm = 0.3575
Meta loss on this task batch = 4.4963e-01, Meta loss averaged over last 500 steps = 4.1703e-01, PNorm = 74.3653, GNorm = 0.2332
Meta loss on this task batch = 4.4434e-01, Meta loss averaged over last 500 steps = 4.1710e-01, PNorm = 74.3779, GNorm = 0.3182
Meta loss on this task batch = 3.5358e-01, Meta loss averaged over last 500 steps = 4.1695e-01, PNorm = 74.3909, GNorm = 0.1738
Meta loss on this task batch = 4.3325e-01, Meta loss averaged over last 500 steps = 4.1678e-01, PNorm = 74.4026, GNorm = 0.2396
Meta loss on this task batch = 3.0971e-01, Meta loss averaged over last 500 steps = 4.1649e-01, PNorm = 74.4133, GNorm = 0.2498
Took 111.01647639274597 seconds to complete one epoch of meta training
Took 118.78989148139954 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460920
Epoch 104
Meta loss on this task batch = 4.2517e-01, Meta loss averaged over last 500 steps = 4.1647e-01, PNorm = 74.4241, GNorm = 0.1992
Meta loss on this task batch = 3.9236e-01, Meta loss averaged over last 500 steps = 4.1637e-01, PNorm = 74.4366, GNorm = 0.3004
Meta loss on this task batch = 3.7046e-01, Meta loss averaged over last 500 steps = 4.1616e-01, PNorm = 74.4498, GNorm = 0.2337
Meta loss on this task batch = 3.8415e-01, Meta loss averaged over last 500 steps = 4.1609e-01, PNorm = 74.4634, GNorm = 0.1696
Meta loss on this task batch = 4.4797e-01, Meta loss averaged over last 500 steps = 4.1616e-01, PNorm = 74.4766, GNorm = 0.1970
Meta loss on this task batch = 4.3751e-01, Meta loss averaged over last 500 steps = 4.1628e-01, PNorm = 74.4902, GNorm = 0.3040
Meta loss on this task batch = 4.0897e-01, Meta loss averaged over last 500 steps = 4.1626e-01, PNorm = 74.5044, GNorm = 0.1618
Meta loss on this task batch = 3.8270e-01, Meta loss averaged over last 500 steps = 4.1612e-01, PNorm = 74.5194, GNorm = 0.2326
Meta loss on this task batch = 3.9749e-01, Meta loss averaged over last 500 steps = 4.1619e-01, PNorm = 74.5354, GNorm = 0.3227
Meta loss on this task batch = 4.3041e-01, Meta loss averaged over last 500 steps = 4.1607e-01, PNorm = 74.5508, GNorm = 0.2004
Meta loss on this task batch = 3.9522e-01, Meta loss averaged over last 500 steps = 4.1611e-01, PNorm = 74.5651, GNorm = 0.2326
Meta loss on this task batch = 4.5405e-01, Meta loss averaged over last 500 steps = 4.1608e-01, PNorm = 74.5794, GNorm = 0.2694
Meta loss on this task batch = 4.0307e-01, Meta loss averaged over last 500 steps = 4.1595e-01, PNorm = 74.5921, GNorm = 0.2179
Meta loss on this task batch = 4.5675e-01, Meta loss averaged over last 500 steps = 4.1621e-01, PNorm = 74.6047, GNorm = 0.2066
Meta loss on this task batch = 3.7379e-01, Meta loss averaged over last 500 steps = 4.1600e-01, PNorm = 74.6169, GNorm = 0.3101
Meta loss on this task batch = 4.2820e-01, Meta loss averaged over last 500 steps = 4.1590e-01, PNorm = 74.6284, GNorm = 0.1689
Meta loss on this task batch = 3.8045e-01, Meta loss averaged over last 500 steps = 4.1586e-01, PNorm = 74.6402, GNorm = 0.3491
Meta loss on this task batch = 4.1757e-01, Meta loss averaged over last 500 steps = 4.1579e-01, PNorm = 74.6517, GNorm = 0.2510
Meta loss on this task batch = 3.8638e-01, Meta loss averaged over last 500 steps = 4.1558e-01, PNorm = 74.6635, GNorm = 0.1915
Took 112.8498842716217 seconds to complete one epoch of meta training
Took 120.87837386131287 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478422
Epoch 105
Meta loss on this task batch = 4.1674e-01, Meta loss averaged over last 500 steps = 4.1561e-01, PNorm = 74.6735, GNorm = 0.2896
Meta loss on this task batch = 4.4977e-01, Meta loss averaged over last 500 steps = 4.1570e-01, PNorm = 74.6851, GNorm = 0.1858
Meta loss on this task batch = 3.5000e-01, Meta loss averaged over last 500 steps = 4.1560e-01, PNorm = 74.6969, GNorm = 0.1369
Meta loss on this task batch = 4.2760e-01, Meta loss averaged over last 500 steps = 4.1568e-01, PNorm = 74.7093, GNorm = 0.1770
Meta loss on this task batch = 3.8784e-01, Meta loss averaged over last 500 steps = 4.1554e-01, PNorm = 74.7229, GNorm = 0.1902
Meta loss on this task batch = 4.3437e-01, Meta loss averaged over last 500 steps = 4.1558e-01, PNorm = 74.7372, GNorm = 0.1872
Meta loss on this task batch = 3.8496e-01, Meta loss averaged over last 500 steps = 4.1552e-01, PNorm = 74.7537, GNorm = 0.3791
Meta loss on this task batch = 4.2857e-01, Meta loss averaged over last 500 steps = 4.1549e-01, PNorm = 74.7699, GNorm = 0.1932
Meta loss on this task batch = 4.0650e-01, Meta loss averaged over last 500 steps = 4.1541e-01, PNorm = 74.7861, GNorm = 0.1780
Meta loss on this task batch = 3.9131e-01, Meta loss averaged over last 500 steps = 4.1539e-01, PNorm = 74.8028, GNorm = 0.1534
Meta loss on this task batch = 4.2642e-01, Meta loss averaged over last 500 steps = 4.1545e-01, PNorm = 74.8194, GNorm = 0.1705
Meta loss on this task batch = 4.2096e-01, Meta loss averaged over last 500 steps = 4.1556e-01, PNorm = 74.8353, GNorm = 0.2571
Meta loss on this task batch = 3.4216e-01, Meta loss averaged over last 500 steps = 4.1532e-01, PNorm = 74.8524, GNorm = 0.3180
Meta loss on this task batch = 4.3298e-01, Meta loss averaged over last 500 steps = 4.1526e-01, PNorm = 74.8693, GNorm = 0.1734
Meta loss on this task batch = 3.9835e-01, Meta loss averaged over last 500 steps = 4.1521e-01, PNorm = 74.8871, GNorm = 0.3101
Meta loss on this task batch = 3.4205e-01, Meta loss averaged over last 500 steps = 4.1499e-01, PNorm = 74.9046, GNorm = 0.1697
Meta loss on this task batch = 3.7390e-01, Meta loss averaged over last 500 steps = 4.1483e-01, PNorm = 74.9208, GNorm = 0.2328
Meta loss on this task batch = 3.6437e-01, Meta loss averaged over last 500 steps = 4.1477e-01, PNorm = 74.9376, GNorm = 0.2327
Meta loss on this task batch = 4.3340e-01, Meta loss averaged over last 500 steps = 4.1477e-01, PNorm = 74.9541, GNorm = 0.2198
Took 110.10728693008423 seconds to complete one epoch of meta training
Took 118.0458984375 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487156
Epoch 106
Meta loss on this task batch = 3.9976e-01, Meta loss averaged over last 500 steps = 4.1462e-01, PNorm = 74.9697, GNorm = 0.2063
Meta loss on this task batch = 3.9887e-01, Meta loss averaged over last 500 steps = 4.1455e-01, PNorm = 74.9831, GNorm = 0.1957
Meta loss on this task batch = 4.1555e-01, Meta loss averaged over last 500 steps = 4.1443e-01, PNorm = 74.9955, GNorm = 0.2017
Meta loss on this task batch = 4.1411e-01, Meta loss averaged over last 500 steps = 4.1464e-01, PNorm = 75.0074, GNorm = 0.2217
Meta loss on this task batch = 3.7680e-01, Meta loss averaged over last 500 steps = 4.1443e-01, PNorm = 75.0201, GNorm = 0.2526
Meta loss on this task batch = 3.4952e-01, Meta loss averaged over last 500 steps = 4.1435e-01, PNorm = 75.0328, GNorm = 0.1585
Meta loss on this task batch = 4.2418e-01, Meta loss averaged over last 500 steps = 4.1449e-01, PNorm = 75.0445, GNorm = 0.2896
Meta loss on this task batch = 3.8767e-01, Meta loss averaged over last 500 steps = 4.1436e-01, PNorm = 75.0564, GNorm = 0.1731
Meta loss on this task batch = 4.2310e-01, Meta loss averaged over last 500 steps = 4.1441e-01, PNorm = 75.0678, GNorm = 0.3227
Meta loss on this task batch = 3.8896e-01, Meta loss averaged over last 500 steps = 4.1441e-01, PNorm = 75.0795, GNorm = 0.3809
Meta loss on this task batch = 4.4290e-01, Meta loss averaged over last 500 steps = 4.1441e-01, PNorm = 75.0929, GNorm = 0.3052
Meta loss on this task batch = 4.1238e-01, Meta loss averaged over last 500 steps = 4.1431e-01, PNorm = 75.1049, GNorm = 0.2218
Meta loss on this task batch = 4.1200e-01, Meta loss averaged over last 500 steps = 4.1421e-01, PNorm = 75.1158, GNorm = 0.3461
Meta loss on this task batch = 4.1447e-01, Meta loss averaged over last 500 steps = 4.1413e-01, PNorm = 75.1274, GNorm = 0.1768
Meta loss on this task batch = 3.9601e-01, Meta loss averaged over last 500 steps = 4.1416e-01, PNorm = 75.1408, GNorm = 0.3475
Meta loss on this task batch = 4.2710e-01, Meta loss averaged over last 500 steps = 4.1417e-01, PNorm = 75.1546, GNorm = 0.1746
Meta loss on this task batch = 4.1669e-01, Meta loss averaged over last 500 steps = 4.1415e-01, PNorm = 75.1683, GNorm = 0.2081
Meta loss on this task batch = 3.5770e-01, Meta loss averaged over last 500 steps = 4.1407e-01, PNorm = 75.1833, GNorm = 0.2294
Meta loss on this task batch = 3.9790e-01, Meta loss averaged over last 500 steps = 4.1408e-01, PNorm = 75.1985, GNorm = 0.1642
Took 113.90532684326172 seconds to complete one epoch of meta training
Took 121.78433609008789 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463112
Epoch 107
Meta loss on this task batch = 4.6639e-01, Meta loss averaged over last 500 steps = 4.1420e-01, PNorm = 75.2126, GNorm = 0.2014
Meta loss on this task batch = 4.0134e-01, Meta loss averaged over last 500 steps = 4.1404e-01, PNorm = 75.2256, GNorm = 0.1772
Meta loss on this task batch = 4.0753e-01, Meta loss averaged over last 500 steps = 4.1397e-01, PNorm = 75.2391, GNorm = 0.2209
Meta loss on this task batch = 3.2871e-01, Meta loss averaged over last 500 steps = 4.1384e-01, PNorm = 75.2541, GNorm = 0.4677
Meta loss on this task batch = 3.6751e-01, Meta loss averaged over last 500 steps = 4.1380e-01, PNorm = 75.2697, GNorm = 0.1587
Meta loss on this task batch = 4.3710e-01, Meta loss averaged over last 500 steps = 4.1365e-01, PNorm = 75.2861, GNorm = 0.3688
Meta loss on this task batch = 3.5501e-01, Meta loss averaged over last 500 steps = 4.1345e-01, PNorm = 75.3012, GNorm = 0.3319
Meta loss on this task batch = 4.1591e-01, Meta loss averaged over last 500 steps = 4.1349e-01, PNorm = 75.3163, GNorm = 0.2584
Meta loss on this task batch = 3.4187e-01, Meta loss averaged over last 500 steps = 4.1331e-01, PNorm = 75.3314, GNorm = 0.1489
Meta loss on this task batch = 4.8088e-01, Meta loss averaged over last 500 steps = 4.1350e-01, PNorm = 75.3449, GNorm = 0.2313
Meta loss on this task batch = 3.8586e-01, Meta loss averaged over last 500 steps = 4.1337e-01, PNorm = 75.3577, GNorm = 0.1915
Meta loss on this task batch = 4.1229e-01, Meta loss averaged over last 500 steps = 4.1339e-01, PNorm = 75.3714, GNorm = 0.2336
Meta loss on this task batch = 4.0834e-01, Meta loss averaged over last 500 steps = 4.1359e-01, PNorm = 75.3824, GNorm = 0.6492
Meta loss on this task batch = 4.2880e-01, Meta loss averaged over last 500 steps = 4.1364e-01, PNorm = 75.3934, GNorm = 0.1972
Meta loss on this task batch = 4.2346e-01, Meta loss averaged over last 500 steps = 4.1348e-01, PNorm = 75.4027, GNorm = 0.3111
Meta loss on this task batch = 4.8541e-01, Meta loss averaged over last 500 steps = 4.1355e-01, PNorm = 75.4114, GNorm = 0.3321
Meta loss on this task batch = 4.2694e-01, Meta loss averaged over last 500 steps = 4.1358e-01, PNorm = 75.4209, GNorm = 0.1801
Meta loss on this task batch = 3.2999e-01, Meta loss averaged over last 500 steps = 4.1335e-01, PNorm = 75.4324, GNorm = 0.1447
Meta loss on this task batch = 3.3255e-01, Meta loss averaged over last 500 steps = 4.1324e-01, PNorm = 75.4462, GNorm = 0.1988
Took 115.28966927528381 seconds to complete one epoch of meta training
Took 122.93215298652649 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471080
Epoch 108
Meta loss on this task batch = 4.0412e-01, Meta loss averaged over last 500 steps = 4.1318e-01, PNorm = 75.4610, GNorm = 0.1923
Meta loss on this task batch = 4.3327e-01, Meta loss averaged over last 500 steps = 4.1324e-01, PNorm = 75.4761, GNorm = 0.1651
Meta loss on this task batch = 4.2350e-01, Meta loss averaged over last 500 steps = 4.1317e-01, PNorm = 75.4921, GNorm = 0.2827
Meta loss on this task batch = 4.1219e-01, Meta loss averaged over last 500 steps = 4.1323e-01, PNorm = 75.5102, GNorm = 0.2827
Meta loss on this task batch = 4.1527e-01, Meta loss averaged over last 500 steps = 4.1309e-01, PNorm = 75.5283, GNorm = 0.1575
Meta loss on this task batch = 3.7903e-01, Meta loss averaged over last 500 steps = 4.1295e-01, PNorm = 75.5473, GNorm = 0.1500
Meta loss on this task batch = 4.5736e-01, Meta loss averaged over last 500 steps = 4.1299e-01, PNorm = 75.5665, GNorm = 0.1664
Meta loss on this task batch = 3.4704e-01, Meta loss averaged over last 500 steps = 4.1287e-01, PNorm = 75.5865, GNorm = 0.1927
Meta loss on this task batch = 3.4583e-01, Meta loss averaged over last 500 steps = 4.1267e-01, PNorm = 75.6053, GNorm = 0.2473
Meta loss on this task batch = 4.1524e-01, Meta loss averaged over last 500 steps = 4.1275e-01, PNorm = 75.6261, GNorm = 0.6217
Meta loss on this task batch = 4.3658e-01, Meta loss averaged over last 500 steps = 4.1284e-01, PNorm = 75.6464, GNorm = 0.2123
Meta loss on this task batch = 4.2637e-01, Meta loss averaged over last 500 steps = 4.1287e-01, PNorm = 75.6646, GNorm = 0.1755
Meta loss on this task batch = 4.3349e-01, Meta loss averaged over last 500 steps = 4.1288e-01, PNorm = 75.6835, GNorm = 0.2606
Meta loss on this task batch = 4.0495e-01, Meta loss averaged over last 500 steps = 4.1277e-01, PNorm = 75.7014, GNorm = 0.2896
Meta loss on this task batch = 3.5278e-01, Meta loss averaged over last 500 steps = 4.1254e-01, PNorm = 75.7192, GNorm = 0.1877
Meta loss on this task batch = 3.9188e-01, Meta loss averaged over last 500 steps = 4.1239e-01, PNorm = 75.7366, GNorm = 0.1737
Meta loss on this task batch = 3.7853e-01, Meta loss averaged over last 500 steps = 4.1234e-01, PNorm = 75.7528, GNorm = 0.2313
Meta loss on this task batch = 3.7774e-01, Meta loss averaged over last 500 steps = 4.1234e-01, PNorm = 75.7685, GNorm = 0.2171
Meta loss on this task batch = 3.9708e-01, Meta loss averaged over last 500 steps = 4.1235e-01, PNorm = 75.7851, GNorm = 0.3292
Took 114.10050129890442 seconds to complete one epoch of meta training
Took 121.57339310646057 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465922
Epoch 109
Meta loss on this task batch = 4.1162e-01, Meta loss averaged over last 500 steps = 4.1222e-01, PNorm = 75.8009, GNorm = 0.2379
Meta loss on this task batch = 3.7043e-01, Meta loss averaged over last 500 steps = 4.1218e-01, PNorm = 75.8161, GNorm = 0.1976
Meta loss on this task batch = 4.2268e-01, Meta loss averaged over last 500 steps = 4.1223e-01, PNorm = 75.8300, GNorm = 0.2522
Meta loss on this task batch = 3.8030e-01, Meta loss averaged over last 500 steps = 4.1216e-01, PNorm = 75.8427, GNorm = 0.3058
Meta loss on this task batch = 4.3609e-01, Meta loss averaged over last 500 steps = 4.1212e-01, PNorm = 75.8554, GNorm = 0.4097
Meta loss on this task batch = 4.0150e-01, Meta loss averaged over last 500 steps = 4.1217e-01, PNorm = 75.8679, GNorm = 0.2009
Meta loss on this task batch = 4.6969e-01, Meta loss averaged over last 500 steps = 4.1221e-01, PNorm = 75.8789, GNorm = 0.2037
Meta loss on this task batch = 4.3521e-01, Meta loss averaged over last 500 steps = 4.1224e-01, PNorm = 75.8887, GNorm = 0.3210
Meta loss on this task batch = 3.9860e-01, Meta loss averaged over last 500 steps = 4.1235e-01, PNorm = 75.8981, GNorm = 0.4255
Meta loss on this task batch = 4.6583e-01, Meta loss averaged over last 500 steps = 4.1251e-01, PNorm = 75.9082, GNorm = 0.2475
Meta loss on this task batch = 4.5491e-01, Meta loss averaged over last 500 steps = 4.1253e-01, PNorm = 75.9178, GNorm = 0.2471
Meta loss on this task batch = 3.6795e-01, Meta loss averaged over last 500 steps = 4.1237e-01, PNorm = 75.9293, GNorm = 0.2278
Meta loss on this task batch = 3.2341e-01, Meta loss averaged over last 500 steps = 4.1218e-01, PNorm = 75.9429, GNorm = 0.2063
Meta loss on this task batch = 4.0119e-01, Meta loss averaged over last 500 steps = 4.1226e-01, PNorm = 75.9568, GNorm = 0.2248
Meta loss on this task batch = 3.6476e-01, Meta loss averaged over last 500 steps = 4.1206e-01, PNorm = 75.9706, GNorm = 0.1648
Meta loss on this task batch = 4.5085e-01, Meta loss averaged over last 500 steps = 4.1213e-01, PNorm = 75.9834, GNorm = 0.1704
Meta loss on this task batch = 3.8829e-01, Meta loss averaged over last 500 steps = 4.1191e-01, PNorm = 75.9962, GNorm = 0.1783
Meta loss on this task batch = 4.2363e-01, Meta loss averaged over last 500 steps = 4.1190e-01, PNorm = 76.0092, GNorm = 0.1795
Meta loss on this task batch = 3.7050e-01, Meta loss averaged over last 500 steps = 4.1192e-01, PNorm = 76.0241, GNorm = 0.2734
Took 110.17219877243042 seconds to complete one epoch of meta training
Took 117.94331550598145 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.439355
Epoch 110
Meta loss on this task batch = 3.8742e-01, Meta loss averaged over last 500 steps = 4.1187e-01, PNorm = 76.0398, GNorm = 0.2870
Meta loss on this task batch = 3.9864e-01, Meta loss averaged over last 500 steps = 4.1193e-01, PNorm = 76.0558, GNorm = 0.1861
Meta loss on this task batch = 4.1733e-01, Meta loss averaged over last 500 steps = 4.1186e-01, PNorm = 76.0709, GNorm = 0.2348
Meta loss on this task batch = 4.6238e-01, Meta loss averaged over last 500 steps = 4.1182e-01, PNorm = 76.0841, GNorm = 0.2203
Meta loss on this task batch = 4.1947e-01, Meta loss averaged over last 500 steps = 4.1186e-01, PNorm = 76.0981, GNorm = 0.1828
Meta loss on this task batch = 3.8949e-01, Meta loss averaged over last 500 steps = 4.1186e-01, PNorm = 76.1120, GNorm = 0.3836
Meta loss on this task batch = 3.6115e-01, Meta loss averaged over last 500 steps = 4.1175e-01, PNorm = 76.1276, GNorm = 0.2853
Meta loss on this task batch = 4.1799e-01, Meta loss averaged over last 500 steps = 4.1169e-01, PNorm = 76.1435, GNorm = 0.2720
Meta loss on this task batch = 3.4517e-01, Meta loss averaged over last 500 steps = 4.1165e-01, PNorm = 76.1603, GNorm = 0.2411
Meta loss on this task batch = 3.7460e-01, Meta loss averaged over last 500 steps = 4.1157e-01, PNorm = 76.1773, GNorm = 0.2719
Meta loss on this task batch = 4.6751e-01, Meta loss averaged over last 500 steps = 4.1178e-01, PNorm = 76.1922, GNorm = 0.3717
Meta loss on this task batch = 4.1213e-01, Meta loss averaged over last 500 steps = 4.1179e-01, PNorm = 76.2062, GNorm = 0.1646
Meta loss on this task batch = 3.4588e-01, Meta loss averaged over last 500 steps = 4.1165e-01, PNorm = 76.2198, GNorm = 0.2449
Meta loss on this task batch = 4.3980e-01, Meta loss averaged over last 500 steps = 4.1190e-01, PNorm = 76.2321, GNorm = 0.3324
Meta loss on this task batch = 3.8394e-01, Meta loss averaged over last 500 steps = 4.1176e-01, PNorm = 76.2440, GNorm = 0.2844
Meta loss on this task batch = 4.7518e-01, Meta loss averaged over last 500 steps = 4.1182e-01, PNorm = 76.2548, GNorm = 0.1848
Meta loss on this task batch = 4.0562e-01, Meta loss averaged over last 500 steps = 4.1171e-01, PNorm = 76.2649, GNorm = 0.1808
Meta loss on this task batch = 4.5407e-01, Meta loss averaged over last 500 steps = 4.1175e-01, PNorm = 76.2753, GNorm = 0.1690
Meta loss on this task batch = 3.9817e-01, Meta loss averaged over last 500 steps = 4.1159e-01, PNorm = 76.2860, GNorm = 0.1969
Took 113.30364036560059 seconds to complete one epoch of meta training
Took 120.18829894065857 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455491
Epoch 111
Meta loss on this task batch = 4.2263e-01, Meta loss averaged over last 500 steps = 4.1167e-01, PNorm = 76.2963, GNorm = 0.2371
Meta loss on this task batch = 3.4168e-01, Meta loss averaged over last 500 steps = 4.1149e-01, PNorm = 76.3073, GNorm = 0.2378
Meta loss on this task batch = 3.2112e-01, Meta loss averaged over last 500 steps = 4.1133e-01, PNorm = 76.3184, GNorm = 0.1523
Meta loss on this task batch = 3.7198e-01, Meta loss averaged over last 500 steps = 4.1120e-01, PNorm = 76.3298, GNorm = 0.1891
Meta loss on this task batch = 3.8561e-01, Meta loss averaged over last 500 steps = 4.1105e-01, PNorm = 76.3422, GNorm = 0.1631
Meta loss on this task batch = 4.0107e-01, Meta loss averaged over last 500 steps = 4.1096e-01, PNorm = 76.3546, GNorm = 0.1607
Meta loss on this task batch = 3.7036e-01, Meta loss averaged over last 500 steps = 4.1078e-01, PNorm = 76.3680, GNorm = 0.2394
Meta loss on this task batch = 3.5126e-01, Meta loss averaged over last 500 steps = 4.1067e-01, PNorm = 76.3809, GNorm = 0.2112
Meta loss on this task batch = 4.2806e-01, Meta loss averaged over last 500 steps = 4.1066e-01, PNorm = 76.3946, GNorm = 0.2735
Meta loss on this task batch = 4.0995e-01, Meta loss averaged over last 500 steps = 4.1073e-01, PNorm = 76.4077, GNorm = 0.3821
Meta loss on this task batch = 4.2391e-01, Meta loss averaged over last 500 steps = 4.1076e-01, PNorm = 76.4206, GNorm = 0.2177
Meta loss on this task batch = 3.9043e-01, Meta loss averaged over last 500 steps = 4.1074e-01, PNorm = 76.4340, GNorm = 0.2030
Meta loss on this task batch = 3.4778e-01, Meta loss averaged over last 500 steps = 4.1059e-01, PNorm = 76.4473, GNorm = 0.1812
Meta loss on this task batch = 4.0143e-01, Meta loss averaged over last 500 steps = 4.1059e-01, PNorm = 76.4606, GNorm = 0.1854
Meta loss on this task batch = 4.4053e-01, Meta loss averaged over last 500 steps = 4.1065e-01, PNorm = 76.4730, GNorm = 0.1785
Meta loss on this task batch = 4.2485e-01, Meta loss averaged over last 500 steps = 4.1061e-01, PNorm = 76.4856, GNorm = 0.2366
Meta loss on this task batch = 4.2360e-01, Meta loss averaged over last 500 steps = 4.1059e-01, PNorm = 76.4970, GNorm = 0.2123
Meta loss on this task batch = 4.4477e-01, Meta loss averaged over last 500 steps = 4.1060e-01, PNorm = 76.5075, GNorm = 0.2219
Meta loss on this task batch = 4.0728e-01, Meta loss averaged over last 500 steps = 4.1053e-01, PNorm = 76.5184, GNorm = 0.2305
Took 109.94035387039185 seconds to complete one epoch of meta training
Took 117.76037907600403 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457296
Epoch 112
Meta loss on this task batch = 3.9086e-01, Meta loss averaged over last 500 steps = 4.1047e-01, PNorm = 76.5298, GNorm = 0.1908
Meta loss on this task batch = 3.6356e-01, Meta loss averaged over last 500 steps = 4.1042e-01, PNorm = 76.5424, GNorm = 0.2065
Meta loss on this task batch = 4.2769e-01, Meta loss averaged over last 500 steps = 4.1047e-01, PNorm = 76.5553, GNorm = 0.2025
Meta loss on this task batch = 3.9802e-01, Meta loss averaged over last 500 steps = 4.1041e-01, PNorm = 76.5685, GNorm = 0.2619
Meta loss on this task batch = 3.7655e-01, Meta loss averaged over last 500 steps = 4.1028e-01, PNorm = 76.5819, GNorm = 0.2619
Meta loss on this task batch = 3.7627e-01, Meta loss averaged over last 500 steps = 4.1005e-01, PNorm = 76.5950, GNorm = 0.1817
Meta loss on this task batch = 4.3227e-01, Meta loss averaged over last 500 steps = 4.1014e-01, PNorm = 76.6074, GNorm = 0.2024
Meta loss on this task batch = 3.7793e-01, Meta loss averaged over last 500 steps = 4.1013e-01, PNorm = 76.6206, GNorm = 0.4051
Meta loss on this task batch = 4.3275e-01, Meta loss averaged over last 500 steps = 4.1016e-01, PNorm = 76.6326, GNorm = 0.3477
Meta loss on this task batch = 4.4818e-01, Meta loss averaged over last 500 steps = 4.1031e-01, PNorm = 76.6440, GNorm = 0.1883
Meta loss on this task batch = 3.8015e-01, Meta loss averaged over last 500 steps = 4.1026e-01, PNorm = 76.6566, GNorm = 0.3017
Meta loss on this task batch = 3.8327e-01, Meta loss averaged over last 500 steps = 4.1016e-01, PNorm = 76.6693, GNorm = 0.1684
Meta loss on this task batch = 3.7642e-01, Meta loss averaged over last 500 steps = 4.1003e-01, PNorm = 76.6816, GNorm = 0.2394
Meta loss on this task batch = 3.3612e-01, Meta loss averaged over last 500 steps = 4.0977e-01, PNorm = 76.6948, GNorm = 0.1967
Meta loss on this task batch = 4.2104e-01, Meta loss averaged over last 500 steps = 4.0966e-01, PNorm = 76.7084, GNorm = 0.2241
Meta loss on this task batch = 4.4195e-01, Meta loss averaged over last 500 steps = 4.0977e-01, PNorm = 76.7221, GNorm = 0.2249
Meta loss on this task batch = 3.9810e-01, Meta loss averaged over last 500 steps = 4.0984e-01, PNorm = 76.7353, GNorm = 0.2113
Meta loss on this task batch = 3.5773e-01, Meta loss averaged over last 500 steps = 4.0968e-01, PNorm = 76.7483, GNorm = 0.1993
Meta loss on this task batch = 3.8980e-01, Meta loss averaged over last 500 steps = 4.0960e-01, PNorm = 76.7609, GNorm = 0.2181
Took 110.04471468925476 seconds to complete one epoch of meta training
Took 117.79376816749573 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456719
Epoch 113
Meta loss on this task batch = 3.5733e-01, Meta loss averaged over last 500 steps = 4.0954e-01, PNorm = 76.7729, GNorm = 0.2192
Meta loss on this task batch = 4.4960e-01, Meta loss averaged over last 500 steps = 4.0950e-01, PNorm = 76.7840, GNorm = 0.4849
Meta loss on this task batch = 3.6122e-01, Meta loss averaged over last 500 steps = 4.0924e-01, PNorm = 76.7956, GNorm = 0.2862
Meta loss on this task batch = 3.8879e-01, Meta loss averaged over last 500 steps = 4.0924e-01, PNorm = 76.8067, GNorm = 0.2034
Meta loss on this task batch = 4.2361e-01, Meta loss averaged over last 500 steps = 4.0933e-01, PNorm = 76.8177, GNorm = 0.2349
Meta loss on this task batch = 4.4916e-01, Meta loss averaged over last 500 steps = 4.0931e-01, PNorm = 76.8297, GNorm = 0.3792
Meta loss on this task batch = 3.8439e-01, Meta loss averaged over last 500 steps = 4.0928e-01, PNorm = 76.8414, GNorm = 0.1653
Meta loss on this task batch = 4.1993e-01, Meta loss averaged over last 500 steps = 4.0933e-01, PNorm = 76.8530, GNorm = 0.1641
Meta loss on this task batch = 4.0321e-01, Meta loss averaged over last 500 steps = 4.0923e-01, PNorm = 76.8642, GNorm = 0.1930
Meta loss on this task batch = 4.7062e-01, Meta loss averaged over last 500 steps = 4.0929e-01, PNorm = 76.8738, GNorm = 0.2726
Meta loss on this task batch = 4.6872e-01, Meta loss averaged over last 500 steps = 4.0944e-01, PNorm = 76.8836, GNorm = 0.1975
Meta loss on this task batch = 3.3763e-01, Meta loss averaged over last 500 steps = 4.0920e-01, PNorm = 76.8947, GNorm = 0.1787
Meta loss on this task batch = 4.2186e-01, Meta loss averaged over last 500 steps = 4.0921e-01, PNorm = 76.9070, GNorm = 0.2138
Meta loss on this task batch = 3.6198e-01, Meta loss averaged over last 500 steps = 4.0905e-01, PNorm = 76.9206, GNorm = 0.2789
Meta loss on this task batch = 3.9004e-01, Meta loss averaged over last 500 steps = 4.0902e-01, PNorm = 76.9353, GNorm = 0.1582
Meta loss on this task batch = 3.4245e-01, Meta loss averaged over last 500 steps = 4.0879e-01, PNorm = 76.9498, GNorm = 0.1842
Meta loss on this task batch = 4.0416e-01, Meta loss averaged over last 500 steps = 4.0882e-01, PNorm = 76.9654, GNorm = 0.2767
Meta loss on this task batch = 4.2590e-01, Meta loss averaged over last 500 steps = 4.0892e-01, PNorm = 76.9801, GNorm = 0.2460
Meta loss on this task batch = 3.2335e-01, Meta loss averaged over last 500 steps = 4.0866e-01, PNorm = 76.9936, GNorm = 0.2146
Took 115.65086793899536 seconds to complete one epoch of meta training
Took 123.45023703575134 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.440397
Epoch 114
Meta loss on this task batch = 3.6080e-01, Meta loss averaged over last 500 steps = 4.0854e-01, PNorm = 77.0079, GNorm = 0.3347
Meta loss on this task batch = 4.1844e-01, Meta loss averaged over last 500 steps = 4.0844e-01, PNorm = 77.0219, GNorm = 0.2064
Meta loss on this task batch = 4.1277e-01, Meta loss averaged over last 500 steps = 4.0845e-01, PNorm = 77.0349, GNorm = 0.2687
Meta loss on this task batch = 3.5852e-01, Meta loss averaged over last 500 steps = 4.0833e-01, PNorm = 77.0485, GNorm = 0.2219
Meta loss on this task batch = 3.8686e-01, Meta loss averaged over last 500 steps = 4.0827e-01, PNorm = 77.0624, GNorm = 0.3198
Meta loss on this task batch = 4.3103e-01, Meta loss averaged over last 500 steps = 4.0824e-01, PNorm = 77.0744, GNorm = 0.2553
Meta loss on this task batch = 3.4082e-01, Meta loss averaged over last 500 steps = 4.0810e-01, PNorm = 77.0858, GNorm = 0.1893
Meta loss on this task batch = 3.3445e-01, Meta loss averaged over last 500 steps = 4.0795e-01, PNorm = 77.0970, GNorm = 0.1560
Meta loss on this task batch = 4.1121e-01, Meta loss averaged over last 500 steps = 4.0804e-01, PNorm = 77.1084, GNorm = 0.2220
Meta loss on this task batch = 3.8967e-01, Meta loss averaged over last 500 steps = 4.0794e-01, PNorm = 77.1200, GNorm = 0.1869
Meta loss on this task batch = 4.3973e-01, Meta loss averaged over last 500 steps = 4.0807e-01, PNorm = 77.1309, GNorm = 0.1894
Meta loss on this task batch = 3.5910e-01, Meta loss averaged over last 500 steps = 4.0796e-01, PNorm = 77.1415, GNorm = 0.1999
Meta loss on this task batch = 4.3877e-01, Meta loss averaged over last 500 steps = 4.0793e-01, PNorm = 77.1512, GNorm = 0.2528
Meta loss on this task batch = 4.0753e-01, Meta loss averaged over last 500 steps = 4.0793e-01, PNorm = 77.1608, GNorm = 0.2810
Meta loss on this task batch = 4.3079e-01, Meta loss averaged over last 500 steps = 4.0790e-01, PNorm = 77.1706, GNorm = 0.3608
Meta loss on this task batch = 4.3445e-01, Meta loss averaged over last 500 steps = 4.0806e-01, PNorm = 77.1813, GNorm = 0.2026
Meta loss on this task batch = 4.3847e-01, Meta loss averaged over last 500 steps = 4.0806e-01, PNorm = 77.1922, GNorm = 0.3063
Meta loss on this task batch = 3.5612e-01, Meta loss averaged over last 500 steps = 4.0784e-01, PNorm = 77.2049, GNorm = 0.2299
Meta loss on this task batch = 3.0094e-01, Meta loss averaged over last 500 steps = 4.0765e-01, PNorm = 77.2186, GNorm = 0.2079
Took 121.2848527431488 seconds to complete one epoch of meta training
Took 129.50706219673157 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.436939
Epoch 115
Meta loss on this task batch = 3.8705e-01, Meta loss averaged over last 500 steps = 4.0754e-01, PNorm = 77.2334, GNorm = 0.4561
Meta loss on this task batch = 4.0762e-01, Meta loss averaged over last 500 steps = 4.0763e-01, PNorm = 77.2487, GNorm = 0.1839
Meta loss on this task batch = 4.2328e-01, Meta loss averaged over last 500 steps = 4.0747e-01, PNorm = 77.2638, GNorm = 0.2527
Meta loss on this task batch = 3.9196e-01, Meta loss averaged over last 500 steps = 4.0727e-01, PNorm = 77.2782, GNorm = 0.3653
Meta loss on this task batch = 4.0872e-01, Meta loss averaged over last 500 steps = 4.0723e-01, PNorm = 77.2936, GNorm = 0.2974
Meta loss on this task batch = 4.1059e-01, Meta loss averaged over last 500 steps = 4.0723e-01, PNorm = 77.3083, GNorm = 0.1967
Meta loss on this task batch = 4.2037e-01, Meta loss averaged over last 500 steps = 4.0708e-01, PNorm = 77.3226, GNorm = 0.1901
Meta loss on this task batch = 3.7914e-01, Meta loss averaged over last 500 steps = 4.0707e-01, PNorm = 77.3370, GNorm = 0.2132
Meta loss on this task batch = 4.0636e-01, Meta loss averaged over last 500 steps = 4.0713e-01, PNorm = 77.3510, GNorm = 0.1675
Meta loss on this task batch = 4.6835e-01, Meta loss averaged over last 500 steps = 4.0723e-01, PNorm = 77.3651, GNorm = 0.1970
Meta loss on this task batch = 3.8977e-01, Meta loss averaged over last 500 steps = 4.0721e-01, PNorm = 77.3797, GNorm = 0.1814
Meta loss on this task batch = 3.0945e-01, Meta loss averaged over last 500 steps = 4.0708e-01, PNorm = 77.3950, GNorm = 0.1549
Meta loss on this task batch = 3.5379e-01, Meta loss averaged over last 500 steps = 4.0688e-01, PNorm = 77.4104, GNorm = 0.2312
Meta loss on this task batch = 3.9616e-01, Meta loss averaged over last 500 steps = 4.0680e-01, PNorm = 77.4249, GNorm = 0.2215
Meta loss on this task batch = 4.1402e-01, Meta loss averaged over last 500 steps = 4.0697e-01, PNorm = 77.4382, GNorm = 0.2447
Meta loss on this task batch = 4.2445e-01, Meta loss averaged over last 500 steps = 4.0701e-01, PNorm = 77.4507, GNorm = 0.3029
Meta loss on this task batch = 4.2522e-01, Meta loss averaged over last 500 steps = 4.0699e-01, PNorm = 77.4632, GNorm = 0.2684
Meta loss on this task batch = 3.6544e-01, Meta loss averaged over last 500 steps = 4.0681e-01, PNorm = 77.4752, GNorm = 0.2146
Meta loss on this task batch = 4.1250e-01, Meta loss averaged over last 500 steps = 4.0689e-01, PNorm = 77.4877, GNorm = 0.2321
Took 117.23473358154297 seconds to complete one epoch of meta training
Took 125.60568237304688 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458988
Epoch 116
Meta loss on this task batch = 4.1655e-01, Meta loss averaged over last 500 steps = 4.0689e-01, PNorm = 77.5000, GNorm = 0.3595
Meta loss on this task batch = 3.5587e-01, Meta loss averaged over last 500 steps = 4.0675e-01, PNorm = 77.5114, GNorm = 0.1712
Meta loss on this task batch = 4.3262e-01, Meta loss averaged over last 500 steps = 4.0674e-01, PNorm = 77.5237, GNorm = 0.4716
Meta loss on this task batch = 3.9780e-01, Meta loss averaged over last 500 steps = 4.0657e-01, PNorm = 77.5356, GNorm = 0.2180
Meta loss on this task batch = 4.6508e-01, Meta loss averaged over last 500 steps = 4.0658e-01, PNorm = 77.5468, GNorm = 0.2176
Meta loss on this task batch = 3.8594e-01, Meta loss averaged over last 500 steps = 4.0657e-01, PNorm = 77.5580, GNorm = 0.2108
Meta loss on this task batch = 3.8038e-01, Meta loss averaged over last 500 steps = 4.0648e-01, PNorm = 77.5707, GNorm = 0.1984
Meta loss on this task batch = 3.6580e-01, Meta loss averaged over last 500 steps = 4.0634e-01, PNorm = 77.5829, GNorm = 0.2328
Meta loss on this task batch = 4.4265e-01, Meta loss averaged over last 500 steps = 4.0636e-01, PNorm = 77.5961, GNorm = 0.3101
Meta loss on this task batch = 3.5337e-01, Meta loss averaged over last 500 steps = 4.0633e-01, PNorm = 77.6081, GNorm = 0.3694
Meta loss on this task batch = 3.9664e-01, Meta loss averaged over last 500 steps = 4.0625e-01, PNorm = 77.6217, GNorm = 0.3567
Meta loss on this task batch = 3.7422e-01, Meta loss averaged over last 500 steps = 4.0614e-01, PNorm = 77.6355, GNorm = 0.3215
Meta loss on this task batch = 3.6806e-01, Meta loss averaged over last 500 steps = 4.0604e-01, PNorm = 77.6485, GNorm = 0.3274
Meta loss on this task batch = 3.8229e-01, Meta loss averaged over last 500 steps = 4.0590e-01, PNorm = 77.6618, GNorm = 0.1747
Meta loss on this task batch = 3.9670e-01, Meta loss averaged over last 500 steps = 4.0602e-01, PNorm = 77.6743, GNorm = 0.1944
Meta loss on this task batch = 4.1846e-01, Meta loss averaged over last 500 steps = 4.0605e-01, PNorm = 77.6861, GNorm = 0.2095
Meta loss on this task batch = 3.4628e-01, Meta loss averaged over last 500 steps = 4.0590e-01, PNorm = 77.6987, GNorm = 0.2855
Meta loss on this task batch = 3.5832e-01, Meta loss averaged over last 500 steps = 4.0573e-01, PNorm = 77.7106, GNorm = 0.2027
Meta loss on this task batch = 4.5233e-01, Meta loss averaged over last 500 steps = 4.0583e-01, PNorm = 77.7220, GNorm = 0.3397
Took 144.34591627120972 seconds to complete one epoch of meta training
Took 153.1867175102234 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.442685
Epoch 117
Meta loss on this task batch = 4.5630e-01, Meta loss averaged over last 500 steps = 4.0597e-01, PNorm = 77.7327, GNorm = 0.2932
Meta loss on this task batch = 3.7506e-01, Meta loss averaged over last 500 steps = 4.0595e-01, PNorm = 77.7439, GNorm = 0.2166
Meta loss on this task batch = 4.4260e-01, Meta loss averaged over last 500 steps = 4.0583e-01, PNorm = 77.7542, GNorm = 0.2707
Meta loss on this task batch = 3.9827e-01, Meta loss averaged over last 500 steps = 4.0578e-01, PNorm = 77.7645, GNorm = 0.1541
Meta loss on this task batch = 4.0708e-01, Meta loss averaged over last 500 steps = 4.0585e-01, PNorm = 77.7756, GNorm = 0.4772
Meta loss on this task batch = 3.9372e-01, Meta loss averaged over last 500 steps = 4.0584e-01, PNorm = 77.7861, GNorm = 0.3179
Meta loss on this task batch = 3.6790e-01, Meta loss averaged over last 500 steps = 4.0573e-01, PNorm = 77.7980, GNorm = 0.3022
Meta loss on this task batch = 3.5244e-01, Meta loss averaged over last 500 steps = 4.0572e-01, PNorm = 77.8107, GNorm = 0.1538
Meta loss on this task batch = 3.7172e-01, Meta loss averaged over last 500 steps = 4.0568e-01, PNorm = 77.8235, GNorm = 0.2387
Meta loss on this task batch = 4.1968e-01, Meta loss averaged over last 500 steps = 4.0558e-01, PNorm = 77.8367, GNorm = 0.2925
Meta loss on this task batch = 3.6200e-01, Meta loss averaged over last 500 steps = 4.0559e-01, PNorm = 77.8499, GNorm = 0.1827
Meta loss on this task batch = 3.4785e-01, Meta loss averaged over last 500 steps = 4.0543e-01, PNorm = 77.8625, GNorm = 0.2510
Meta loss on this task batch = 3.5950e-01, Meta loss averaged over last 500 steps = 4.0528e-01, PNorm = 77.8756, GNorm = 0.1730
Meta loss on this task batch = 4.0752e-01, Meta loss averaged over last 500 steps = 4.0526e-01, PNorm = 77.8890, GNorm = 0.2120
Meta loss on this task batch = 3.7839e-01, Meta loss averaged over last 500 steps = 4.0519e-01, PNorm = 77.9028, GNorm = 0.2395
Meta loss on this task batch = 4.8787e-01, Meta loss averaged over last 500 steps = 4.0526e-01, PNorm = 77.9149, GNorm = 0.3054
Meta loss on this task batch = 4.0248e-01, Meta loss averaged over last 500 steps = 4.0517e-01, PNorm = 77.9272, GNorm = 0.1887
Meta loss on this task batch = 3.8400e-01, Meta loss averaged over last 500 steps = 4.0505e-01, PNorm = 77.9409, GNorm = 0.4339
Meta loss on this task batch = 3.5388e-01, Meta loss averaged over last 500 steps = 4.0498e-01, PNorm = 77.9545, GNorm = 0.1850
Took 123.24061512947083 seconds to complete one epoch of meta training
Took 131.10134506225586 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472115
Epoch 118
Meta loss on this task batch = 3.8150e-01, Meta loss averaged over last 500 steps = 4.0468e-01, PNorm = 77.9685, GNorm = 0.1617
Meta loss on this task batch = 3.2766e-01, Meta loss averaged over last 500 steps = 4.0459e-01, PNorm = 77.9830, GNorm = 0.2259
Meta loss on this task batch = 3.5155e-01, Meta loss averaged over last 500 steps = 4.0459e-01, PNorm = 77.9972, GNorm = 0.2218
Meta loss on this task batch = 4.1806e-01, Meta loss averaged over last 500 steps = 4.0461e-01, PNorm = 78.0111, GNorm = 0.1942
Meta loss on this task batch = 4.3318e-01, Meta loss averaged over last 500 steps = 4.0473e-01, PNorm = 78.0228, GNorm = 0.4727
Meta loss on this task batch = 4.2927e-01, Meta loss averaged over last 500 steps = 4.0476e-01, PNorm = 78.0347, GNorm = 0.2304
Meta loss on this task batch = 3.2779e-01, Meta loss averaged over last 500 steps = 4.0466e-01, PNorm = 78.0471, GNorm = 0.2641
Meta loss on this task batch = 3.2376e-01, Meta loss averaged over last 500 steps = 4.0459e-01, PNorm = 78.0604, GNorm = 0.1911
Meta loss on this task batch = 5.0406e-01, Meta loss averaged over last 500 steps = 4.0472e-01, PNorm = 78.0729, GNorm = 0.2583
Meta loss on this task batch = 3.3645e-01, Meta loss averaged over last 500 steps = 4.0453e-01, PNorm = 78.0858, GNorm = 0.2441
Meta loss on this task batch = 4.3563e-01, Meta loss averaged over last 500 steps = 4.0449e-01, PNorm = 78.0975, GNorm = 0.2164
Meta loss on this task batch = 4.3289e-01, Meta loss averaged over last 500 steps = 4.0450e-01, PNorm = 78.1110, GNorm = 0.2221
Meta loss on this task batch = 4.3095e-01, Meta loss averaged over last 500 steps = 4.0451e-01, PNorm = 78.1245, GNorm = 0.2309
Meta loss on this task batch = 4.3468e-01, Meta loss averaged over last 500 steps = 4.0457e-01, PNorm = 78.1378, GNorm = 0.3434
Meta loss on this task batch = 3.8295e-01, Meta loss averaged over last 500 steps = 4.0453e-01, PNorm = 78.1511, GNorm = 0.1913
Meta loss on this task batch = 3.5647e-01, Meta loss averaged over last 500 steps = 4.0437e-01, PNorm = 78.1646, GNorm = 0.2769
Meta loss on this task batch = 3.9147e-01, Meta loss averaged over last 500 steps = 4.0444e-01, PNorm = 78.1784, GNorm = 0.1755
Meta loss on this task batch = 4.1987e-01, Meta loss averaged over last 500 steps = 4.0450e-01, PNorm = 78.1923, GNorm = 0.2013
Meta loss on this task batch = 4.5086e-01, Meta loss averaged over last 500 steps = 4.0438e-01, PNorm = 78.2058, GNorm = 0.2463
Took 125.61506009101868 seconds to complete one epoch of meta training
Took 134.33445978164673 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483076
Epoch 119
Meta loss on this task batch = 3.9575e-01, Meta loss averaged over last 500 steps = 4.0436e-01, PNorm = 78.2197, GNorm = 0.2484
Meta loss on this task batch = 3.6431e-01, Meta loss averaged over last 500 steps = 4.0416e-01, PNorm = 78.2353, GNorm = 0.3775
Meta loss on this task batch = 3.5988e-01, Meta loss averaged over last 500 steps = 4.0413e-01, PNorm = 78.2517, GNorm = 0.2667
Meta loss on this task batch = 3.8752e-01, Meta loss averaged over last 500 steps = 4.0403e-01, PNorm = 78.2673, GNorm = 0.1790
Meta loss on this task batch = 4.0160e-01, Meta loss averaged over last 500 steps = 4.0404e-01, PNorm = 78.2828, GNorm = 0.2503
Meta loss on this task batch = 4.6098e-01, Meta loss averaged over last 500 steps = 4.0422e-01, PNorm = 78.2969, GNorm = 0.2186
Meta loss on this task batch = 4.0011e-01, Meta loss averaged over last 500 steps = 4.0438e-01, PNorm = 78.3099, GNorm = 0.1973
Meta loss on this task batch = 4.0308e-01, Meta loss averaged over last 500 steps = 4.0435e-01, PNorm = 78.3225, GNorm = 0.1714
Meta loss on this task batch = 4.0544e-01, Meta loss averaged over last 500 steps = 4.0430e-01, PNorm = 78.3347, GNorm = 0.3409
Meta loss on this task batch = 3.8183e-01, Meta loss averaged over last 500 steps = 4.0423e-01, PNorm = 78.3471, GNorm = 0.2677
Meta loss on this task batch = 3.7004e-01, Meta loss averaged over last 500 steps = 4.0413e-01, PNorm = 78.3601, GNorm = 0.1551
Meta loss on this task batch = 4.1437e-01, Meta loss averaged over last 500 steps = 4.0417e-01, PNorm = 78.3738, GNorm = 0.1689
Meta loss on this task batch = 4.4969e-01, Meta loss averaged over last 500 steps = 4.0432e-01, PNorm = 78.3875, GNorm = 0.2599
Meta loss on this task batch = 3.3214e-01, Meta loss averaged over last 500 steps = 4.0396e-01, PNorm = 78.4016, GNorm = 0.1871
Meta loss on this task batch = 4.4895e-01, Meta loss averaged over last 500 steps = 4.0406e-01, PNorm = 78.4150, GNorm = 0.2841
Meta loss on this task batch = 3.5201e-01, Meta loss averaged over last 500 steps = 4.0394e-01, PNorm = 78.4286, GNorm = 0.2150
Meta loss on this task batch = 3.6454e-01, Meta loss averaged over last 500 steps = 4.0390e-01, PNorm = 78.4421, GNorm = 0.3058
Meta loss on this task batch = 3.3025e-01, Meta loss averaged over last 500 steps = 4.0368e-01, PNorm = 78.4561, GNorm = 0.1752
Meta loss on this task batch = 4.0048e-01, Meta loss averaged over last 500 steps = 4.0360e-01, PNorm = 78.4698, GNorm = 0.2074
Took 125.3905816078186 seconds to complete one epoch of meta training
Took 133.81359815597534 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482818
Epoch 120
Meta loss on this task batch = 3.8752e-01, Meta loss averaged over last 500 steps = 4.0354e-01, PNorm = 78.4836, GNorm = 0.3502
Meta loss on this task batch = 4.0680e-01, Meta loss averaged over last 500 steps = 4.0351e-01, PNorm = 78.4963, GNorm = 0.3230
Meta loss on this task batch = 3.5127e-01, Meta loss averaged over last 500 steps = 4.0321e-01, PNorm = 78.5101, GNorm = 0.2547
Meta loss on this task batch = 3.5350e-01, Meta loss averaged over last 500 steps = 4.0315e-01, PNorm = 78.5228, GNorm = 0.2125
Meta loss on this task batch = 3.8535e-01, Meta loss averaged over last 500 steps = 4.0311e-01, PNorm = 78.5348, GNorm = 0.3150
Meta loss on this task batch = 3.6568e-01, Meta loss averaged over last 500 steps = 4.0310e-01, PNorm = 78.5477, GNorm = 0.3980
Meta loss on this task batch = 3.8621e-01, Meta loss averaged over last 500 steps = 4.0297e-01, PNorm = 78.5610, GNorm = 0.2444
Meta loss on this task batch = 4.3115e-01, Meta loss averaged over last 500 steps = 4.0300e-01, PNorm = 78.5724, GNorm = 0.4175
Meta loss on this task batch = 4.3615e-01, Meta loss averaged over last 500 steps = 4.0300e-01, PNorm = 78.5836, GNorm = 0.2087
Meta loss on this task batch = 4.3762e-01, Meta loss averaged over last 500 steps = 4.0315e-01, PNorm = 78.5952, GNorm = 0.2061
Meta loss on this task batch = 4.1242e-01, Meta loss averaged over last 500 steps = 4.0321e-01, PNorm = 78.6064, GNorm = 0.2569
Meta loss on this task batch = 3.3262e-01, Meta loss averaged over last 500 steps = 4.0293e-01, PNorm = 78.6190, GNorm = 0.2592
Meta loss on this task batch = 3.6231e-01, Meta loss averaged over last 500 steps = 4.0276e-01, PNorm = 78.6315, GNorm = 0.3362
Meta loss on this task batch = 4.0689e-01, Meta loss averaged over last 500 steps = 4.0288e-01, PNorm = 78.6446, GNorm = 0.2455
Meta loss on this task batch = 3.6120e-01, Meta loss averaged over last 500 steps = 4.0267e-01, PNorm = 78.6593, GNorm = 0.3054
Meta loss on this task batch = 4.4112e-01, Meta loss averaged over last 500 steps = 4.0266e-01, PNorm = 78.6728, GNorm = 0.2440
Meta loss on this task batch = 3.7746e-01, Meta loss averaged over last 500 steps = 4.0260e-01, PNorm = 78.6866, GNorm = 0.1837
Meta loss on this task batch = 4.3853e-01, Meta loss averaged over last 500 steps = 4.0259e-01, PNorm = 78.7009, GNorm = 0.1878
Meta loss on this task batch = 3.4138e-01, Meta loss averaged over last 500 steps = 4.0244e-01, PNorm = 78.7155, GNorm = 0.1980
Took 126.68614745140076 seconds to complete one epoch of meta training
Took 135.66658782958984 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471702
Epoch 121
Meta loss on this task batch = 3.5858e-01, Meta loss averaged over last 500 steps = 4.0238e-01, PNorm = 78.7295, GNorm = 0.1581
Meta loss on this task batch = 4.4452e-01, Meta loss averaged over last 500 steps = 4.0239e-01, PNorm = 78.7429, GNorm = 0.3255
Meta loss on this task batch = 4.0210e-01, Meta loss averaged over last 500 steps = 4.0238e-01, PNorm = 78.7562, GNorm = 0.2517
Meta loss on this task batch = 3.6927e-01, Meta loss averaged over last 500 steps = 4.0230e-01, PNorm = 78.7691, GNorm = 0.1766
Meta loss on this task batch = 4.4708e-01, Meta loss averaged over last 500 steps = 4.0242e-01, PNorm = 78.7798, GNorm = 0.2440
Meta loss on this task batch = 3.6446e-01, Meta loss averaged over last 500 steps = 4.0241e-01, PNorm = 78.7906, GNorm = 0.1823
Meta loss on this task batch = 3.0942e-01, Meta loss averaged over last 500 steps = 4.0221e-01, PNorm = 78.8009, GNorm = 0.3135
Meta loss on this task batch = 3.4537e-01, Meta loss averaged over last 500 steps = 4.0217e-01, PNorm = 78.8121, GNorm = 0.1913
Meta loss on this task batch = 3.6326e-01, Meta loss averaged over last 500 steps = 4.0223e-01, PNorm = 78.8227, GNorm = 0.2078
Meta loss on this task batch = 3.8379e-01, Meta loss averaged over last 500 steps = 4.0205e-01, PNorm = 78.8328, GNorm = 0.2199
Meta loss on this task batch = 3.5451e-01, Meta loss averaged over last 500 steps = 4.0184e-01, PNorm = 78.8426, GNorm = 0.2406
Meta loss on this task batch = 4.2797e-01, Meta loss averaged over last 500 steps = 4.0190e-01, PNorm = 78.8525, GNorm = 0.2335
Meta loss on this task batch = 4.2180e-01, Meta loss averaged over last 500 steps = 4.0180e-01, PNorm = 78.8626, GNorm = 0.2217
Meta loss on this task batch = 4.1962e-01, Meta loss averaged over last 500 steps = 4.0181e-01, PNorm = 78.8722, GNorm = 0.3178
Meta loss on this task batch = 3.6734e-01, Meta loss averaged over last 500 steps = 4.0168e-01, PNorm = 78.8833, GNorm = 0.3321
Meta loss on this task batch = 4.2567e-01, Meta loss averaged over last 500 steps = 4.0163e-01, PNorm = 78.8947, GNorm = 0.2549
Meta loss on this task batch = 4.2345e-01, Meta loss averaged over last 500 steps = 4.0163e-01, PNorm = 78.9063, GNorm = 0.2721
Meta loss on this task batch = 2.9798e-01, Meta loss averaged over last 500 steps = 4.0147e-01, PNorm = 78.9189, GNorm = 0.1583
Meta loss on this task batch = 3.8745e-01, Meta loss averaged over last 500 steps = 4.0144e-01, PNorm = 78.9321, GNorm = 0.2576
Took 124.51284623146057 seconds to complete one epoch of meta training
Took 132.07775378227234 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.445010
Epoch 122
Meta loss on this task batch = 3.9976e-01, Meta loss averaged over last 500 steps = 4.0135e-01, PNorm = 78.9456, GNorm = 0.1870
Meta loss on this task batch = 4.4254e-01, Meta loss averaged over last 500 steps = 4.0146e-01, PNorm = 78.9595, GNorm = 0.2162
Meta loss on this task batch = 3.6085e-01, Meta loss averaged over last 500 steps = 4.0146e-01, PNorm = 78.9742, GNorm = 0.1633
Meta loss on this task batch = 3.7340e-01, Meta loss averaged over last 500 steps = 4.0146e-01, PNorm = 78.9899, GNorm = 0.2119
Meta loss on this task batch = 3.4902e-01, Meta loss averaged over last 500 steps = 4.0131e-01, PNorm = 79.0053, GNorm = 0.1677
Meta loss on this task batch = 4.1589e-01, Meta loss averaged over last 500 steps = 4.0134e-01, PNorm = 79.0197, GNorm = 0.2404
Meta loss on this task batch = 4.1411e-01, Meta loss averaged over last 500 steps = 4.0126e-01, PNorm = 79.0345, GNorm = 0.2799
Meta loss on this task batch = 3.9030e-01, Meta loss averaged over last 500 steps = 4.0127e-01, PNorm = 79.0481, GNorm = 0.2865
Meta loss on this task batch = 3.8577e-01, Meta loss averaged over last 500 steps = 4.0125e-01, PNorm = 79.0612, GNorm = 0.2051
Meta loss on this task batch = 3.9694e-01, Meta loss averaged over last 500 steps = 4.0130e-01, PNorm = 79.0725, GNorm = 0.2890
Meta loss on this task batch = 3.7048e-01, Meta loss averaged over last 500 steps = 4.0120e-01, PNorm = 79.0837, GNorm = 0.1785
Meta loss on this task batch = 3.6131e-01, Meta loss averaged over last 500 steps = 4.0104e-01, PNorm = 79.0953, GNorm = 0.2131
Meta loss on this task batch = 4.0256e-01, Meta loss averaged over last 500 steps = 4.0099e-01, PNorm = 79.1064, GNorm = 0.3157
Meta loss on this task batch = 3.0692e-01, Meta loss averaged over last 500 steps = 4.0074e-01, PNorm = 79.1178, GNorm = 0.3000
Meta loss on this task batch = 4.6243e-01, Meta loss averaged over last 500 steps = 4.0099e-01, PNorm = 79.1280, GNorm = 0.2455
Meta loss on this task batch = 3.7049e-01, Meta loss averaged over last 500 steps = 4.0094e-01, PNorm = 79.1384, GNorm = 0.2044
Meta loss on this task batch = 4.1820e-01, Meta loss averaged over last 500 steps = 4.0105e-01, PNorm = 79.1486, GNorm = 0.3005
Meta loss on this task batch = 3.4293e-01, Meta loss averaged over last 500 steps = 4.0091e-01, PNorm = 79.1593, GNorm = 0.1460
Meta loss on this task batch = 4.2223e-01, Meta loss averaged over last 500 steps = 4.0099e-01, PNorm = 79.1705, GNorm = 0.2339
Took 125.58120155334473 seconds to complete one epoch of meta training
Took 134.25201654434204 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455567
Epoch 123
Meta loss on this task batch = 4.1856e-01, Meta loss averaged over last 500 steps = 4.0118e-01, PNorm = 79.1820, GNorm = 0.2400
Meta loss on this task batch = 3.2476e-01, Meta loss averaged over last 500 steps = 4.0086e-01, PNorm = 79.1941, GNorm = 0.1475
Meta loss on this task batch = 4.2601e-01, Meta loss averaged over last 500 steps = 4.0092e-01, PNorm = 79.2070, GNorm = 0.2838
Meta loss on this task batch = 4.2812e-01, Meta loss averaged over last 500 steps = 4.0095e-01, PNorm = 79.2193, GNorm = 0.2158
Meta loss on this task batch = 3.6552e-01, Meta loss averaged over last 500 steps = 4.0075e-01, PNorm = 79.2327, GNorm = 0.3524
Meta loss on this task batch = 4.1284e-01, Meta loss averaged over last 500 steps = 4.0064e-01, PNorm = 79.2461, GNorm = 0.2917
Meta loss on this task batch = 4.0631e-01, Meta loss averaged over last 500 steps = 4.0085e-01, PNorm = 79.2597, GNorm = 0.2091
Meta loss on this task batch = 3.7106e-01, Meta loss averaged over last 500 steps = 4.0085e-01, PNorm = 79.2730, GNorm = 0.1813
Meta loss on this task batch = 4.1590e-01, Meta loss averaged over last 500 steps = 4.0089e-01, PNorm = 79.2854, GNorm = 0.2208
Meta loss on this task batch = 3.8854e-01, Meta loss averaged over last 500 steps = 4.0073e-01, PNorm = 79.2990, GNorm = 0.4288
Meta loss on this task batch = 3.2235e-01, Meta loss averaged over last 500 steps = 4.0054e-01, PNorm = 79.3136, GNorm = 0.1661
Meta loss on this task batch = 3.8670e-01, Meta loss averaged over last 500 steps = 4.0052e-01, PNorm = 79.3279, GNorm = 0.2724
Meta loss on this task batch = 3.2938e-01, Meta loss averaged over last 500 steps = 4.0024e-01, PNorm = 79.3429, GNorm = 0.1872
Meta loss on this task batch = 4.4760e-01, Meta loss averaged over last 500 steps = 4.0027e-01, PNorm = 79.3571, GNorm = 0.2332
Meta loss on this task batch = 4.3015e-01, Meta loss averaged over last 500 steps = 4.0034e-01, PNorm = 79.3710, GNorm = 0.2223
Meta loss on this task batch = 3.8358e-01, Meta loss averaged over last 500 steps = 4.0030e-01, PNorm = 79.3841, GNorm = 0.2423
Meta loss on this task batch = 3.6374e-01, Meta loss averaged over last 500 steps = 4.0024e-01, PNorm = 79.3978, GNorm = 0.2760
Meta loss on this task batch = 4.1844e-01, Meta loss averaged over last 500 steps = 4.0021e-01, PNorm = 79.4113, GNorm = 0.2130
Meta loss on this task batch = 4.0003e-01, Meta loss averaged over last 500 steps = 4.0015e-01, PNorm = 79.4245, GNorm = 0.2431
Took 121.3991551399231 seconds to complete one epoch of meta training
Took 130.343248128891 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479708
Epoch 124
Meta loss on this task batch = 3.7742e-01, Meta loss averaged over last 500 steps = 3.9998e-01, PNorm = 79.4379, GNorm = 0.2265
Meta loss on this task batch = 3.5178e-01, Meta loss averaged over last 500 steps = 4.0000e-01, PNorm = 79.4522, GNorm = 0.1876
Meta loss on this task batch = 3.6995e-01, Meta loss averaged over last 500 steps = 3.9976e-01, PNorm = 79.4668, GNorm = 0.1966
Meta loss on this task batch = 4.2125e-01, Meta loss averaged over last 500 steps = 3.9978e-01, PNorm = 79.4801, GNorm = 0.2463
Meta loss on this task batch = 4.2934e-01, Meta loss averaged over last 500 steps = 3.9985e-01, PNorm = 79.4908, GNorm = 0.4589
Meta loss on this task batch = 3.8145e-01, Meta loss averaged over last 500 steps = 3.9980e-01, PNorm = 79.5012, GNorm = 0.2058
Meta loss on this task batch = 3.6541e-01, Meta loss averaged over last 500 steps = 3.9977e-01, PNorm = 79.5129, GNorm = 0.2596
Meta loss on this task batch = 3.8818e-01, Meta loss averaged over last 500 steps = 3.9985e-01, PNorm = 79.5252, GNorm = 0.4783
Meta loss on this task batch = 3.1962e-01, Meta loss averaged over last 500 steps = 3.9990e-01, PNorm = 79.5376, GNorm = 0.1905
Meta loss on this task batch = 4.2578e-01, Meta loss averaged over last 500 steps = 3.9995e-01, PNorm = 79.5507, GNorm = 0.2216
Meta loss on this task batch = 3.4496e-01, Meta loss averaged over last 500 steps = 3.9974e-01, PNorm = 79.5646, GNorm = 0.2493
Meta loss on this task batch = 3.3996e-01, Meta loss averaged over last 500 steps = 3.9960e-01, PNorm = 79.5790, GNorm = 0.2316
Meta loss on this task batch = 4.5061e-01, Meta loss averaged over last 500 steps = 3.9962e-01, PNorm = 79.5931, GNorm = 0.2698
Meta loss on this task batch = 4.2279e-01, Meta loss averaged over last 500 steps = 3.9958e-01, PNorm = 79.6059, GNorm = 0.2350
Meta loss on this task batch = 3.7282e-01, Meta loss averaged over last 500 steps = 3.9954e-01, PNorm = 79.6182, GNorm = 0.2012
Meta loss on this task batch = 3.8673e-01, Meta loss averaged over last 500 steps = 3.9951e-01, PNorm = 79.6302, GNorm = 0.1921
Meta loss on this task batch = 3.9433e-01, Meta loss averaged over last 500 steps = 3.9938e-01, PNorm = 79.6412, GNorm = 0.4474
Meta loss on this task batch = 4.1767e-01, Meta loss averaged over last 500 steps = 3.9943e-01, PNorm = 79.6534, GNorm = 0.2385
Meta loss on this task batch = 4.4439e-01, Meta loss averaged over last 500 steps = 3.9936e-01, PNorm = 79.6648, GNorm = 0.4388
Took 129.633061170578 seconds to complete one epoch of meta training
Took 138.76188969612122 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461542
Epoch 125
Meta loss on this task batch = 4.3453e-01, Meta loss averaged over last 500 steps = 3.9940e-01, PNorm = 79.6760, GNorm = 0.2166
Meta loss on this task batch = 4.4802e-01, Meta loss averaged over last 500 steps = 3.9947e-01, PNorm = 79.6876, GNorm = 0.2781
Meta loss on this task batch = 3.8446e-01, Meta loss averaged over last 500 steps = 3.9948e-01, PNorm = 79.6996, GNorm = 0.2365
Meta loss on this task batch = 3.9972e-01, Meta loss averaged over last 500 steps = 3.9938e-01, PNorm = 79.7120, GNorm = 0.2180
Meta loss on this task batch = 4.7220e-01, Meta loss averaged over last 500 steps = 3.9945e-01, PNorm = 79.7241, GNorm = 0.2186
Meta loss on this task batch = 3.3421e-01, Meta loss averaged over last 500 steps = 3.9940e-01, PNorm = 79.7373, GNorm = 0.2918
Meta loss on this task batch = 3.8333e-01, Meta loss averaged over last 500 steps = 3.9928e-01, PNorm = 79.7504, GNorm = 0.2254
Meta loss on this task batch = 3.8932e-01, Meta loss averaged over last 500 steps = 3.9914e-01, PNorm = 79.7634, GNorm = 0.2103
Meta loss on this task batch = 3.7441e-01, Meta loss averaged over last 500 steps = 3.9897e-01, PNorm = 79.7764, GNorm = 0.1838
Meta loss on this task batch = 4.1615e-01, Meta loss averaged over last 500 steps = 3.9911e-01, PNorm = 79.7886, GNorm = 0.1888
Meta loss on this task batch = 3.5876e-01, Meta loss averaged over last 500 steps = 3.9885e-01, PNorm = 79.8010, GNorm = 0.1955
Meta loss on this task batch = 3.7067e-01, Meta loss averaged over last 500 steps = 3.9872e-01, PNorm = 79.8128, GNorm = 0.2314
Meta loss on this task batch = 3.4265e-01, Meta loss averaged over last 500 steps = 3.9861e-01, PNorm = 79.8249, GNorm = 0.1942
Meta loss on this task batch = 3.9623e-01, Meta loss averaged over last 500 steps = 3.9861e-01, PNorm = 79.8357, GNorm = 0.2880
Meta loss on this task batch = 3.3574e-01, Meta loss averaged over last 500 steps = 3.9849e-01, PNorm = 79.8465, GNorm = 0.2116
Meta loss on this task batch = 3.4622e-01, Meta loss averaged over last 500 steps = 3.9837e-01, PNorm = 79.8578, GNorm = 0.2429
Meta loss on this task batch = 3.6461e-01, Meta loss averaged over last 500 steps = 3.9824e-01, PNorm = 79.8691, GNorm = 0.2609
Meta loss on this task batch = 4.0385e-01, Meta loss averaged over last 500 steps = 3.9835e-01, PNorm = 79.8806, GNorm = 0.1947
Meta loss on this task batch = 4.2190e-01, Meta loss averaged over last 500 steps = 3.9838e-01, PNorm = 79.8910, GNorm = 0.4791
Took 124.53238558769226 seconds to complete one epoch of meta training
Took 132.92958879470825 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476745
Epoch 126
Meta loss on this task batch = 3.9611e-01, Meta loss averaged over last 500 steps = 3.9832e-01, PNorm = 79.9012, GNorm = 0.1946
Meta loss on this task batch = 3.4446e-01, Meta loss averaged over last 500 steps = 3.9816e-01, PNorm = 79.9127, GNorm = 0.2540
Meta loss on this task batch = 3.7161e-01, Meta loss averaged over last 500 steps = 3.9810e-01, PNorm = 79.9247, GNorm = 0.2517
Meta loss on this task batch = 3.6899e-01, Meta loss averaged over last 500 steps = 3.9815e-01, PNorm = 79.9375, GNorm = 0.4021
Meta loss on this task batch = 4.1777e-01, Meta loss averaged over last 500 steps = 3.9823e-01, PNorm = 79.9501, GNorm = 0.3022
Meta loss on this task batch = 4.2910e-01, Meta loss averaged over last 500 steps = 3.9819e-01, PNorm = 79.9632, GNorm = 0.2560
Meta loss on this task batch = 4.0820e-01, Meta loss averaged over last 500 steps = 3.9811e-01, PNorm = 79.9766, GNorm = 0.1942
Meta loss on this task batch = 3.8357e-01, Meta loss averaged over last 500 steps = 3.9798e-01, PNorm = 79.9900, GNorm = 0.2747
Meta loss on this task batch = 3.4893e-01, Meta loss averaged over last 500 steps = 3.9793e-01, PNorm = 80.0028, GNorm = 0.2470
Meta loss on this task batch = 3.2910e-01, Meta loss averaged over last 500 steps = 3.9783e-01, PNorm = 80.0166, GNorm = 0.1975
Meta loss on this task batch = 3.6508e-01, Meta loss averaged over last 500 steps = 3.9781e-01, PNorm = 80.0306, GNorm = 0.3146
Meta loss on this task batch = 3.8953e-01, Meta loss averaged over last 500 steps = 3.9773e-01, PNorm = 80.0440, GNorm = 0.2096
Meta loss on this task batch = 3.4237e-01, Meta loss averaged over last 500 steps = 3.9763e-01, PNorm = 80.0571, GNorm = 0.2446
Meta loss on this task batch = 4.2752e-01, Meta loss averaged over last 500 steps = 3.9757e-01, PNorm = 80.0691, GNorm = 0.2589
Meta loss on this task batch = 3.7322e-01, Meta loss averaged over last 500 steps = 3.9751e-01, PNorm = 80.0808, GNorm = 0.2584
Meta loss on this task batch = 3.9722e-01, Meta loss averaged over last 500 steps = 3.9751e-01, PNorm = 80.0921, GNorm = 0.1920
Meta loss on this task batch = 3.8623e-01, Meta loss averaged over last 500 steps = 3.9752e-01, PNorm = 80.1034, GNorm = 0.3070
Meta loss on this task batch = 4.4143e-01, Meta loss averaged over last 500 steps = 3.9748e-01, PNorm = 80.1127, GNorm = 0.4598
Meta loss on this task batch = 3.9470e-01, Meta loss averaged over last 500 steps = 3.9731e-01, PNorm = 80.1207, GNorm = 0.5050
Took 127.04521012306213 seconds to complete one epoch of meta training
Took 134.66653060913086 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482872
Epoch 127
Meta loss on this task batch = 3.3405e-01, Meta loss averaged over last 500 steps = 3.9706e-01, PNorm = 80.1296, GNorm = 0.2668
Meta loss on this task batch = 4.2110e-01, Meta loss averaged over last 500 steps = 3.9713e-01, PNorm = 80.1385, GNorm = 0.2127
Meta loss on this task batch = 4.2707e-01, Meta loss averaged over last 500 steps = 3.9710e-01, PNorm = 80.1478, GNorm = 0.1957
Meta loss on this task batch = 3.8861e-01, Meta loss averaged over last 500 steps = 3.9724e-01, PNorm = 80.1584, GNorm = 0.3284
Meta loss on this task batch = 3.8816e-01, Meta loss averaged over last 500 steps = 3.9711e-01, PNorm = 80.1696, GNorm = 0.1953
Meta loss on this task batch = 4.3438e-01, Meta loss averaged over last 500 steps = 3.9715e-01, PNorm = 80.1808, GNorm = 0.2885
Meta loss on this task batch = 3.4924e-01, Meta loss averaged over last 500 steps = 3.9705e-01, PNorm = 80.1920, GNorm = 0.1734
Meta loss on this task batch = 4.5453e-01, Meta loss averaged over last 500 steps = 3.9709e-01, PNorm = 80.2030, GNorm = 0.3438
Meta loss on this task batch = 3.5364e-01, Meta loss averaged over last 500 steps = 3.9698e-01, PNorm = 80.2139, GNorm = 0.1608
Meta loss on this task batch = 4.0422e-01, Meta loss averaged over last 500 steps = 3.9691e-01, PNorm = 80.2253, GNorm = 0.2183
Meta loss on this task batch = 3.2115e-01, Meta loss averaged over last 500 steps = 3.9677e-01, PNorm = 80.2377, GNorm = 0.2047
Meta loss on this task batch = 3.7255e-01, Meta loss averaged over last 500 steps = 3.9671e-01, PNorm = 80.2498, GNorm = 0.1972
Meta loss on this task batch = 3.8424e-01, Meta loss averaged over last 500 steps = 3.9673e-01, PNorm = 80.2622, GNorm = 0.2019
Meta loss on this task batch = 3.5799e-01, Meta loss averaged over last 500 steps = 3.9675e-01, PNorm = 80.2757, GNorm = 0.2194
Meta loss on this task batch = 4.2973e-01, Meta loss averaged over last 500 steps = 3.9677e-01, PNorm = 80.2877, GNorm = 0.2333
Meta loss on this task batch = 3.9527e-01, Meta loss averaged over last 500 steps = 3.9676e-01, PNorm = 80.2989, GNorm = 0.3535
Meta loss on this task batch = 2.7841e-01, Meta loss averaged over last 500 steps = 3.9661e-01, PNorm = 80.3110, GNorm = 0.2609
Meta loss on this task batch = 3.7651e-01, Meta loss averaged over last 500 steps = 3.9650e-01, PNorm = 80.3224, GNorm = 0.3693
Meta loss on this task batch = 3.7478e-01, Meta loss averaged over last 500 steps = 3.9629e-01, PNorm = 80.3342, GNorm = 0.3295
Took 124.98319125175476 seconds to complete one epoch of meta training
Took 133.25186204910278 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484891
Epoch 128
Meta loss on this task batch = 4.5647e-01, Meta loss averaged over last 500 steps = 3.9633e-01, PNorm = 80.3456, GNorm = 0.3282
Meta loss on this task batch = 3.4808e-01, Meta loss averaged over last 500 steps = 3.9629e-01, PNorm = 80.3574, GNorm = 0.2314
Meta loss on this task batch = 3.4319e-01, Meta loss averaged over last 500 steps = 3.9617e-01, PNorm = 80.3689, GNorm = 0.1979
Meta loss on this task batch = 4.2530e-01, Meta loss averaged over last 500 steps = 3.9609e-01, PNorm = 80.3801, GNorm = 0.3426
Meta loss on this task batch = 3.6122e-01, Meta loss averaged over last 500 steps = 3.9592e-01, PNorm = 80.3919, GNorm = 0.2517
Meta loss on this task batch = 3.4964e-01, Meta loss averaged over last 500 steps = 3.9578e-01, PNorm = 80.4032, GNorm = 0.1900
Meta loss on this task batch = 3.6856e-01, Meta loss averaged over last 500 steps = 3.9572e-01, PNorm = 80.4147, GNorm = 0.2499
Meta loss on this task batch = 4.2096e-01, Meta loss averaged over last 500 steps = 3.9569e-01, PNorm = 80.4262, GNorm = 0.2301
Meta loss on this task batch = 4.1053e-01, Meta loss averaged over last 500 steps = 3.9563e-01, PNorm = 80.4378, GNorm = 0.3212
Meta loss on this task batch = 3.6064e-01, Meta loss averaged over last 500 steps = 3.9560e-01, PNorm = 80.4491, GNorm = 0.2006
Meta loss on this task batch = 3.8784e-01, Meta loss averaged over last 500 steps = 3.9557e-01, PNorm = 80.4606, GNorm = 0.2510
Meta loss on this task batch = 3.8410e-01, Meta loss averaged over last 500 steps = 3.9557e-01, PNorm = 80.4731, GNorm = 0.1720
Meta loss on this task batch = 3.9586e-01, Meta loss averaged over last 500 steps = 3.9559e-01, PNorm = 80.4850, GNorm = 0.1803
Meta loss on this task batch = 3.9464e-01, Meta loss averaged over last 500 steps = 3.9551e-01, PNorm = 80.4974, GNorm = 0.1934
Meta loss on this task batch = 3.7439e-01, Meta loss averaged over last 500 steps = 3.9548e-01, PNorm = 80.5105, GNorm = 0.1797
Meta loss on this task batch = 3.5338e-01, Meta loss averaged over last 500 steps = 3.9536e-01, PNorm = 80.5229, GNorm = 0.1958
Meta loss on this task batch = 4.0018e-01, Meta loss averaged over last 500 steps = 3.9534e-01, PNorm = 80.5349, GNorm = 0.1904
Meta loss on this task batch = 4.1046e-01, Meta loss averaged over last 500 steps = 3.9533e-01, PNorm = 80.5461, GNorm = 0.2152
Meta loss on this task batch = 3.6587e-01, Meta loss averaged over last 500 steps = 3.9513e-01, PNorm = 80.5572, GNorm = 0.2017
Took 125.48707509040833 seconds to complete one epoch of meta training
Took 134.71836614608765 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475588
Epoch 129
Meta loss on this task batch = 4.2851e-01, Meta loss averaged over last 500 steps = 3.9519e-01, PNorm = 80.5679, GNorm = 0.2475
Meta loss on this task batch = 3.8253e-01, Meta loss averaged over last 500 steps = 3.9518e-01, PNorm = 80.5783, GNorm = 0.2764
Meta loss on this task batch = 3.8092e-01, Meta loss averaged over last 500 steps = 3.9520e-01, PNorm = 80.5892, GNorm = 0.2615
Meta loss on this task batch = 3.4457e-01, Meta loss averaged over last 500 steps = 3.9515e-01, PNorm = 80.6008, GNorm = 0.2461
Meta loss on this task batch = 3.3536e-01, Meta loss averaged over last 500 steps = 3.9491e-01, PNorm = 80.6127, GNorm = 0.1924
Meta loss on this task batch = 3.9112e-01, Meta loss averaged over last 500 steps = 3.9502e-01, PNorm = 80.6241, GNorm = 0.1982
Meta loss on this task batch = 4.1169e-01, Meta loss averaged over last 500 steps = 3.9509e-01, PNorm = 80.6360, GNorm = 0.2510
Meta loss on this task batch = 4.0008e-01, Meta loss averaged over last 500 steps = 3.9502e-01, PNorm = 80.6476, GNorm = 0.2363
Meta loss on this task batch = 3.9727e-01, Meta loss averaged over last 500 steps = 3.9500e-01, PNorm = 80.6584, GNorm = 0.2225
Meta loss on this task batch = 4.1187e-01, Meta loss averaged over last 500 steps = 3.9503e-01, PNorm = 80.6700, GNorm = 0.2677
Meta loss on this task batch = 3.8614e-01, Meta loss averaged over last 500 steps = 3.9507e-01, PNorm = 80.6816, GNorm = 0.2691
Meta loss on this task batch = 3.0073e-01, Meta loss averaged over last 500 steps = 3.9488e-01, PNorm = 80.6947, GNorm = 0.2528
Meta loss on this task batch = 3.3920e-01, Meta loss averaged over last 500 steps = 3.9466e-01, PNorm = 80.7086, GNorm = 0.1742
Meta loss on this task batch = 3.6251e-01, Meta loss averaged over last 500 steps = 3.9458e-01, PNorm = 80.7217, GNorm = 0.3093
Meta loss on this task batch = 3.7144e-01, Meta loss averaged over last 500 steps = 3.9451e-01, PNorm = 80.7350, GNorm = 0.3922
Meta loss on this task batch = 3.8826e-01, Meta loss averaged over last 500 steps = 3.9447e-01, PNorm = 80.7482, GNorm = 0.2764
Meta loss on this task batch = 4.4793e-01, Meta loss averaged over last 500 steps = 3.9449e-01, PNorm = 80.7617, GNorm = 0.4523
Meta loss on this task batch = 4.1939e-01, Meta loss averaged over last 500 steps = 3.9460e-01, PNorm = 80.7754, GNorm = 0.3518
Meta loss on this task batch = 3.0400e-01, Meta loss averaged over last 500 steps = 3.9440e-01, PNorm = 80.7903, GNorm = 0.2349
Took 126.51001596450806 seconds to complete one epoch of meta training
Took 135.63300013542175 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471572
Epoch 130
Meta loss on this task batch = 4.3017e-01, Meta loss averaged over last 500 steps = 3.9446e-01, PNorm = 80.8046, GNorm = 0.2814
Meta loss on this task batch = 4.5293e-01, Meta loss averaged over last 500 steps = 3.9447e-01, PNorm = 80.8173, GNorm = 0.3296
Meta loss on this task batch = 3.7220e-01, Meta loss averaged over last 500 steps = 3.9432e-01, PNorm = 80.8305, GNorm = 0.4887
Meta loss on this task batch = 3.4961e-01, Meta loss averaged over last 500 steps = 3.9431e-01, PNorm = 80.8443, GNorm = 0.5088
Meta loss on this task batch = 3.8300e-01, Meta loss averaged over last 500 steps = 3.9421e-01, PNorm = 80.8587, GNorm = 0.2548
Meta loss on this task batch = 3.6450e-01, Meta loss averaged over last 500 steps = 3.9432e-01, PNorm = 80.8735, GNorm = 0.1568
Meta loss on this task batch = 3.7802e-01, Meta loss averaged over last 500 steps = 3.9423e-01, PNorm = 80.8868, GNorm = 0.3351
Meta loss on this task batch = 3.3638e-01, Meta loss averaged over last 500 steps = 3.9412e-01, PNorm = 80.9010, GNorm = 0.2680
Meta loss on this task batch = 3.9096e-01, Meta loss averaged over last 500 steps = 3.9416e-01, PNorm = 80.9154, GNorm = 0.1911
Meta loss on this task batch = 3.9047e-01, Meta loss averaged over last 500 steps = 3.9417e-01, PNorm = 80.9287, GNorm = 0.1947
Meta loss on this task batch = 3.8594e-01, Meta loss averaged over last 500 steps = 3.9405e-01, PNorm = 80.9423, GNorm = 0.4080
Meta loss on this task batch = 3.7709e-01, Meta loss averaged over last 500 steps = 3.9393e-01, PNorm = 80.9559, GNorm = 0.2522
Meta loss on this task batch = 4.3230e-01, Meta loss averaged over last 500 steps = 3.9397e-01, PNorm = 80.9688, GNorm = 0.2625
Meta loss on this task batch = 4.2933e-01, Meta loss averaged over last 500 steps = 3.9407e-01, PNorm = 80.9797, GNorm = 0.3109
Meta loss on this task batch = 3.4550e-01, Meta loss averaged over last 500 steps = 3.9396e-01, PNorm = 80.9919, GNorm = 0.1859
Meta loss on this task batch = 3.9247e-01, Meta loss averaged over last 500 steps = 3.9389e-01, PNorm = 81.0046, GNorm = 0.1974
Meta loss on this task batch = 3.5420e-01, Meta loss averaged over last 500 steps = 3.9380e-01, PNorm = 81.0185, GNorm = 0.2346
Meta loss on this task batch = 4.3996e-01, Meta loss averaged over last 500 steps = 3.9378e-01, PNorm = 81.0324, GNorm = 0.2070
Meta loss on this task batch = 3.9284e-01, Meta loss averaged over last 500 steps = 3.9375e-01, PNorm = 81.0467, GNorm = 0.2379
Took 125.35540223121643 seconds to complete one epoch of meta training
Took 134.00091314315796 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471008
Epoch 131
Meta loss on this task batch = 3.8833e-01, Meta loss averaged over last 500 steps = 3.9362e-01, PNorm = 81.0614, GNorm = 0.1953
Meta loss on this task batch = 3.4899e-01, Meta loss averaged over last 500 steps = 3.9357e-01, PNorm = 81.0778, GNorm = 0.3812
Meta loss on this task batch = 3.7450e-01, Meta loss averaged over last 500 steps = 3.9346e-01, PNorm = 81.0939, GNorm = 0.1795
Meta loss on this task batch = 3.9000e-01, Meta loss averaged over last 500 steps = 3.9348e-01, PNorm = 81.1093, GNorm = 0.2156
Meta loss on this task batch = 4.1088e-01, Meta loss averaged over last 500 steps = 3.9347e-01, PNorm = 81.1230, GNorm = 0.2201
Meta loss on this task batch = 3.8333e-01, Meta loss averaged over last 500 steps = 3.9346e-01, PNorm = 81.1353, GNorm = 0.3751
Meta loss on this task batch = 3.5166e-01, Meta loss averaged over last 500 steps = 3.9333e-01, PNorm = 81.1478, GNorm = 0.1920
Meta loss on this task batch = 3.2504e-01, Meta loss averaged over last 500 steps = 3.9308e-01, PNorm = 81.1606, GNorm = 0.2107
Meta loss on this task batch = 3.6735e-01, Meta loss averaged over last 500 steps = 3.9312e-01, PNorm = 81.1730, GNorm = 0.2635
Meta loss on this task batch = 3.9308e-01, Meta loss averaged over last 500 steps = 3.9305e-01, PNorm = 81.1847, GNorm = 0.2177
Meta loss on this task batch = 4.1634e-01, Meta loss averaged over last 500 steps = 3.9310e-01, PNorm = 81.1961, GNorm = 0.2746
Meta loss on this task batch = 3.5591e-01, Meta loss averaged over last 500 steps = 3.9295e-01, PNorm = 81.2072, GNorm = 0.2346
Meta loss on this task batch = 3.6757e-01, Meta loss averaged over last 500 steps = 3.9291e-01, PNorm = 81.2192, GNorm = 0.2275
Meta loss on this task batch = 4.1217e-01, Meta loss averaged over last 500 steps = 3.9288e-01, PNorm = 81.2301, GNorm = 0.3092
Meta loss on this task batch = 4.6164e-01, Meta loss averaged over last 500 steps = 3.9299e-01, PNorm = 81.2396, GNorm = 0.2557
Meta loss on this task batch = 3.7348e-01, Meta loss averaged over last 500 steps = 3.9295e-01, PNorm = 81.2491, GNorm = 0.3498
Meta loss on this task batch = 3.8032e-01, Meta loss averaged over last 500 steps = 3.9286e-01, PNorm = 81.2586, GNorm = 0.4225
Meta loss on this task batch = 4.3636e-01, Meta loss averaged over last 500 steps = 3.9289e-01, PNorm = 81.2676, GNorm = 0.4198
Meta loss on this task batch = 3.9799e-01, Meta loss averaged over last 500 steps = 3.9300e-01, PNorm = 81.2770, GNorm = 0.3477
Took 126.05908584594727 seconds to complete one epoch of meta training
Took 134.52853798866272 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472052
Epoch 132
Meta loss on this task batch = 3.5701e-01, Meta loss averaged over last 500 steps = 3.9285e-01, PNorm = 81.2866, GNorm = 0.2097
Meta loss on this task batch = 3.9575e-01, Meta loss averaged over last 500 steps = 3.9285e-01, PNorm = 81.2963, GNorm = 0.1881
Meta loss on this task batch = 3.7311e-01, Meta loss averaged over last 500 steps = 3.9291e-01, PNorm = 81.3066, GNorm = 0.1765
Meta loss on this task batch = 3.7142e-01, Meta loss averaged over last 500 steps = 3.9290e-01, PNorm = 81.3183, GNorm = 0.2202
Meta loss on this task batch = 3.8021e-01, Meta loss averaged over last 500 steps = 3.9294e-01, PNorm = 81.3313, GNorm = 0.1925
Meta loss on this task batch = 4.3047e-01, Meta loss averaged over last 500 steps = 3.9293e-01, PNorm = 81.3444, GNorm = 0.1928
Meta loss on this task batch = 3.7007e-01, Meta loss averaged over last 500 steps = 3.9287e-01, PNorm = 81.3575, GNorm = 0.2051
Meta loss on this task batch = 4.1930e-01, Meta loss averaged over last 500 steps = 3.9291e-01, PNorm = 81.3702, GNorm = 0.2428
Meta loss on this task batch = 4.4686e-01, Meta loss averaged over last 500 steps = 3.9297e-01, PNorm = 81.3825, GNorm = 0.3597
Meta loss on this task batch = 3.3769e-01, Meta loss averaged over last 500 steps = 3.9282e-01, PNorm = 81.3959, GNorm = 0.1741
Meta loss on this task batch = 3.5868e-01, Meta loss averaged over last 500 steps = 3.9278e-01, PNorm = 81.4104, GNorm = 0.2348
Meta loss on this task batch = 4.1946e-01, Meta loss averaged over last 500 steps = 3.9292e-01, PNorm = 81.4244, GNorm = 0.2337
Meta loss on this task batch = 4.0702e-01, Meta loss averaged over last 500 steps = 3.9289e-01, PNorm = 81.4387, GNorm = 0.2265
Meta loss on this task batch = 3.6867e-01, Meta loss averaged over last 500 steps = 3.9285e-01, PNorm = 81.4531, GNorm = 0.1735
Meta loss on this task batch = 3.8072e-01, Meta loss averaged over last 500 steps = 3.9277e-01, PNorm = 81.4673, GNorm = 0.2929
Meta loss on this task batch = 3.5872e-01, Meta loss averaged over last 500 steps = 3.9271e-01, PNorm = 81.4818, GNorm = 0.2314
Meta loss on this task batch = 4.3839e-01, Meta loss averaged over last 500 steps = 3.9270e-01, PNorm = 81.4957, GNorm = 0.2787
Meta loss on this task batch = 3.7246e-01, Meta loss averaged over last 500 steps = 3.9262e-01, PNorm = 81.5094, GNorm = 0.2973
Meta loss on this task batch = 3.8381e-01, Meta loss averaged over last 500 steps = 3.9256e-01, PNorm = 81.5231, GNorm = 0.2241
Took 121.61914229393005 seconds to complete one epoch of meta training
Took 129.8718192577362 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466458
Epoch 133
Meta loss on this task batch = 4.2019e-01, Meta loss averaged over last 500 steps = 3.9257e-01, PNorm = 81.5367, GNorm = 0.2341
Meta loss on this task batch = 4.7417e-01, Meta loss averaged over last 500 steps = 3.9273e-01, PNorm = 81.5489, GNorm = 0.2343
Meta loss on this task batch = 3.8519e-01, Meta loss averaged over last 500 steps = 3.9265e-01, PNorm = 81.5615, GNorm = 0.2879
Meta loss on this task batch = 4.1462e-01, Meta loss averaged over last 500 steps = 3.9264e-01, PNorm = 81.5727, GNorm = 0.3383
Meta loss on this task batch = 3.6485e-01, Meta loss averaged over last 500 steps = 3.9266e-01, PNorm = 81.5842, GNorm = 0.2632
Meta loss on this task batch = 4.4345e-01, Meta loss averaged over last 500 steps = 3.9275e-01, PNorm = 81.5944, GNorm = 0.2943
Meta loss on this task batch = 4.0328e-01, Meta loss averaged over last 500 steps = 3.9262e-01, PNorm = 81.6053, GNorm = 0.2893
Meta loss on this task batch = 3.7568e-01, Meta loss averaged over last 500 steps = 3.9257e-01, PNorm = 81.6170, GNorm = 0.2225
Meta loss on this task batch = 3.4484e-01, Meta loss averaged over last 500 steps = 3.9244e-01, PNorm = 81.6291, GNorm = 0.2059
Meta loss on this task batch = 3.4046e-01, Meta loss averaged over last 500 steps = 3.9247e-01, PNorm = 81.6416, GNorm = 0.2073
Meta loss on this task batch = 3.7064e-01, Meta loss averaged over last 500 steps = 3.9247e-01, PNorm = 81.6540, GNorm = 0.2055
Meta loss on this task batch = 3.7825e-01, Meta loss averaged over last 500 steps = 3.9236e-01, PNorm = 81.6666, GNorm = 0.2048
Meta loss on this task batch = 4.0344e-01, Meta loss averaged over last 500 steps = 3.9245e-01, PNorm = 81.6788, GNorm = 0.2229
Meta loss on this task batch = 3.1622e-01, Meta loss averaged over last 500 steps = 3.9225e-01, PNorm = 81.6904, GNorm = 0.2022
Meta loss on this task batch = 3.6580e-01, Meta loss averaged over last 500 steps = 3.9230e-01, PNorm = 81.7012, GNorm = 0.2239
Meta loss on this task batch = 3.7001e-01, Meta loss averaged over last 500 steps = 3.9208e-01, PNorm = 81.7119, GNorm = 0.2588
Meta loss on this task batch = 3.6440e-01, Meta loss averaged over last 500 steps = 3.9204e-01, PNorm = 81.7218, GNorm = 0.2124
Meta loss on this task batch = 3.8774e-01, Meta loss averaged over last 500 steps = 3.9199e-01, PNorm = 81.7315, GNorm = 0.3544
Meta loss on this task batch = 3.8627e-01, Meta loss averaged over last 500 steps = 3.9194e-01, PNorm = 81.7410, GNorm = 0.3850
Took 120.96659851074219 seconds to complete one epoch of meta training
Took 129.50025701522827 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466615
Epoch 134
Meta loss on this task batch = 4.1099e-01, Meta loss averaged over last 500 steps = 3.9191e-01, PNorm = 81.7498, GNorm = 0.2267
Meta loss on this task batch = 2.9138e-01, Meta loss averaged over last 500 steps = 3.9164e-01, PNorm = 81.7595, GNorm = 0.1630
Meta loss on this task batch = 3.8959e-01, Meta loss averaged over last 500 steps = 3.9145e-01, PNorm = 81.7692, GNorm = 0.2607
Meta loss on this task batch = 4.3809e-01, Meta loss averaged over last 500 steps = 3.9147e-01, PNorm = 81.7786, GNorm = 0.2695
Meta loss on this task batch = 3.3428e-01, Meta loss averaged over last 500 steps = 3.9148e-01, PNorm = 81.7884, GNorm = 0.3050
Meta loss on this task batch = 3.6005e-01, Meta loss averaged over last 500 steps = 3.9154e-01, PNorm = 81.7989, GNorm = 0.2306
Meta loss on this task batch = 3.4560e-01, Meta loss averaged over last 500 steps = 3.9142e-01, PNorm = 81.8100, GNorm = 0.3665
Meta loss on this task batch = 4.0308e-01, Meta loss averaged over last 500 steps = 3.9136e-01, PNorm = 81.8208, GNorm = 0.3927
Meta loss on this task batch = 3.8272e-01, Meta loss averaged over last 500 steps = 3.9128e-01, PNorm = 81.8313, GNorm = 0.2618
Meta loss on this task batch = 3.5155e-01, Meta loss averaged over last 500 steps = 3.9116e-01, PNorm = 81.8426, GNorm = 0.3483
Meta loss on this task batch = 3.8522e-01, Meta loss averaged over last 500 steps = 3.9110e-01, PNorm = 81.8552, GNorm = 0.5957
Meta loss on this task batch = 3.6151e-01, Meta loss averaged over last 500 steps = 3.9106e-01, PNorm = 81.8682, GNorm = 0.1901
Meta loss on this task batch = 3.8692e-01, Meta loss averaged over last 500 steps = 3.9092e-01, PNorm = 81.8817, GNorm = 0.1948
Meta loss on this task batch = 4.4092e-01, Meta loss averaged over last 500 steps = 3.9111e-01, PNorm = 81.8940, GNorm = 0.5284
Meta loss on this task batch = 3.7352e-01, Meta loss averaged over last 500 steps = 3.9116e-01, PNorm = 81.9063, GNorm = 0.3305
Meta loss on this task batch = 3.7690e-01, Meta loss averaged over last 500 steps = 3.9109e-01, PNorm = 81.9190, GNorm = 0.2530
Meta loss on this task batch = 4.5260e-01, Meta loss averaged over last 500 steps = 3.9112e-01, PNorm = 81.9303, GNorm = 0.2383
Meta loss on this task batch = 4.0108e-01, Meta loss averaged over last 500 steps = 3.9107e-01, PNorm = 81.9407, GNorm = 0.3848
Meta loss on this task batch = 3.4270e-01, Meta loss averaged over last 500 steps = 3.9089e-01, PNorm = 81.9514, GNorm = 0.2473
Took 122.86482381820679 seconds to complete one epoch of meta training
Took 131.6098973751068 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467240
Epoch 135
Meta loss on this task batch = 3.8495e-01, Meta loss averaged over last 500 steps = 3.9085e-01, PNorm = 81.9618, GNorm = 0.3556
Meta loss on this task batch = 3.6523e-01, Meta loss averaged over last 500 steps = 3.9087e-01, PNorm = 81.9725, GNorm = 0.1967
Meta loss on this task batch = 4.5688e-01, Meta loss averaged over last 500 steps = 3.9100e-01, PNorm = 81.9836, GNorm = 0.2246
Meta loss on this task batch = 3.9601e-01, Meta loss averaged over last 500 steps = 3.9104e-01, PNorm = 81.9951, GNorm = 0.3015
Meta loss on this task batch = 3.5183e-01, Meta loss averaged over last 500 steps = 3.9099e-01, PNorm = 82.0070, GNorm = 0.1888
Meta loss on this task batch = 4.4820e-01, Meta loss averaged over last 500 steps = 3.9109e-01, PNorm = 82.0175, GNorm = 0.2405
Meta loss on this task batch = 4.2850e-01, Meta loss averaged over last 500 steps = 3.9112e-01, PNorm = 82.0289, GNorm = 0.2189
Meta loss on this task batch = 3.3603e-01, Meta loss averaged over last 500 steps = 3.9105e-01, PNorm = 82.0409, GNorm = 0.2040
Meta loss on this task batch = 3.1349e-01, Meta loss averaged over last 500 steps = 3.9083e-01, PNorm = 82.0543, GNorm = 0.2225
Meta loss on this task batch = 3.4333e-01, Meta loss averaged over last 500 steps = 3.9076e-01, PNorm = 82.0685, GNorm = 0.2236
Meta loss on this task batch = 3.7224e-01, Meta loss averaged over last 500 steps = 3.9063e-01, PNorm = 82.0816, GNorm = 0.1652
Meta loss on this task batch = 3.1113e-01, Meta loss averaged over last 500 steps = 3.9045e-01, PNorm = 82.0949, GNorm = 0.2563
Meta loss on this task batch = 3.5733e-01, Meta loss averaged over last 500 steps = 3.9023e-01, PNorm = 82.1083, GNorm = 0.2286
Meta loss on this task batch = 4.0540e-01, Meta loss averaged over last 500 steps = 3.9017e-01, PNorm = 82.1210, GNorm = 0.3767
Meta loss on this task batch = 3.8217e-01, Meta loss averaged over last 500 steps = 3.9014e-01, PNorm = 82.1335, GNorm = 0.2382
Meta loss on this task batch = 3.5502e-01, Meta loss averaged over last 500 steps = 3.8991e-01, PNorm = 82.1463, GNorm = 0.2100
Meta loss on this task batch = 3.6693e-01, Meta loss averaged over last 500 steps = 3.8974e-01, PNorm = 82.1586, GNorm = 0.2539
Meta loss on this task batch = 3.8131e-01, Meta loss averaged over last 500 steps = 3.8976e-01, PNorm = 82.1700, GNorm = 0.2414
Meta loss on this task batch = 3.3879e-01, Meta loss averaged over last 500 steps = 3.8980e-01, PNorm = 82.1810, GNorm = 0.4040
Took 112.07791352272034 seconds to complete one epoch of meta training
Took 119.92732501029968 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486087
Epoch 136
Meta loss on this task batch = 4.0835e-01, Meta loss averaged over last 500 steps = 3.8981e-01, PNorm = 82.1910, GNorm = 0.1952
Meta loss on this task batch = 3.1835e-01, Meta loss averaged over last 500 steps = 3.8972e-01, PNorm = 82.2013, GNorm = 0.2476
Meta loss on this task batch = 4.3957e-01, Meta loss averaged over last 500 steps = 3.8969e-01, PNorm = 82.2104, GNorm = 0.3855
Meta loss on this task batch = 3.4616e-01, Meta loss averaged over last 500 steps = 3.8961e-01, PNorm = 82.2203, GNorm = 0.1946
Meta loss on this task batch = 3.7722e-01, Meta loss averaged over last 500 steps = 3.8952e-01, PNorm = 82.2301, GNorm = 0.1714
Meta loss on this task batch = 3.5054e-01, Meta loss averaged over last 500 steps = 3.8948e-01, PNorm = 82.2398, GNorm = 0.1975
Meta loss on this task batch = 3.3133e-01, Meta loss averaged over last 500 steps = 3.8936e-01, PNorm = 82.2505, GNorm = 0.1849
Meta loss on this task batch = 3.5380e-01, Meta loss averaged over last 500 steps = 3.8928e-01, PNorm = 82.2616, GNorm = 0.2122
Meta loss on this task batch = 4.2970e-01, Meta loss averaged over last 500 steps = 3.8930e-01, PNorm = 82.2723, GNorm = 0.3617
Meta loss on this task batch = 3.8509e-01, Meta loss averaged over last 500 steps = 3.8915e-01, PNorm = 82.2826, GNorm = 0.2583
Meta loss on this task batch = 4.0003e-01, Meta loss averaged over last 500 steps = 3.8911e-01, PNorm = 82.2936, GNorm = 0.2413
Meta loss on this task batch = 3.6481e-01, Meta loss averaged over last 500 steps = 3.8906e-01, PNorm = 82.3045, GNorm = 0.1978
Meta loss on this task batch = 3.9550e-01, Meta loss averaged over last 500 steps = 3.8913e-01, PNorm = 82.3144, GNorm = 0.2253
Meta loss on this task batch = 3.3484e-01, Meta loss averaged over last 500 steps = 3.8896e-01, PNorm = 82.3258, GNorm = 0.2017
Meta loss on this task batch = 3.6087e-01, Meta loss averaged over last 500 steps = 3.8899e-01, PNorm = 82.3373, GNorm = 0.2912
Meta loss on this task batch = 3.7081e-01, Meta loss averaged over last 500 steps = 3.8898e-01, PNorm = 82.3494, GNorm = 0.1921
Meta loss on this task batch = 3.4795e-01, Meta loss averaged over last 500 steps = 3.8874e-01, PNorm = 82.3621, GNorm = 0.1913
Meta loss on this task batch = 3.6896e-01, Meta loss averaged over last 500 steps = 3.8866e-01, PNorm = 82.3755, GNorm = 0.2543
Meta loss on this task batch = 3.9372e-01, Meta loss averaged over last 500 steps = 3.8875e-01, PNorm = 82.3881, GNorm = 0.3695
Took 112.97567296028137 seconds to complete one epoch of meta training
Took 120.8062686920166 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460552
Epoch 137
Meta loss on this task batch = 3.6621e-01, Meta loss averaged over last 500 steps = 3.8861e-01, PNorm = 82.4016, GNorm = 0.2593
Meta loss on this task batch = 3.9033e-01, Meta loss averaged over last 500 steps = 3.8862e-01, PNorm = 82.4148, GNorm = 0.3211
Meta loss on this task batch = 3.7229e-01, Meta loss averaged over last 500 steps = 3.8841e-01, PNorm = 82.4295, GNorm = 0.3535
Meta loss on this task batch = 4.5057e-01, Meta loss averaged over last 500 steps = 3.8850e-01, PNorm = 82.4431, GNorm = 0.2768
Meta loss on this task batch = 3.5107e-01, Meta loss averaged over last 500 steps = 3.8830e-01, PNorm = 82.4575, GNorm = 0.2905
Meta loss on this task batch = 3.7224e-01, Meta loss averaged over last 500 steps = 3.8825e-01, PNorm = 82.4715, GNorm = 0.1930
Meta loss on this task batch = 3.5477e-01, Meta loss averaged over last 500 steps = 3.8811e-01, PNorm = 82.4860, GNorm = 0.2347
Meta loss on this task batch = 3.7696e-01, Meta loss averaged over last 500 steps = 3.8818e-01, PNorm = 82.4987, GNorm = 0.2960
Meta loss on this task batch = 3.9186e-01, Meta loss averaged over last 500 steps = 3.8832e-01, PNorm = 82.5108, GNorm = 0.2500
Meta loss on this task batch = 3.3774e-01, Meta loss averaged over last 500 steps = 3.8825e-01, PNorm = 82.5235, GNorm = 0.2607
Meta loss on this task batch = 3.3904e-01, Meta loss averaged over last 500 steps = 3.8816e-01, PNorm = 82.5366, GNorm = 0.4900
Meta loss on this task batch = 3.3319e-01, Meta loss averaged over last 500 steps = 3.8802e-01, PNorm = 82.5491, GNorm = 0.1932
Meta loss on this task batch = 3.9402e-01, Meta loss averaged over last 500 steps = 3.8807e-01, PNorm = 82.5603, GNorm = 0.2782
Meta loss on this task batch = 4.4923e-01, Meta loss averaged over last 500 steps = 3.8827e-01, PNorm = 82.5696, GNorm = 0.3596
Meta loss on this task batch = 3.9284e-01, Meta loss averaged over last 500 steps = 3.8820e-01, PNorm = 82.5788, GNorm = 0.2013
Meta loss on this task batch = 3.8851e-01, Meta loss averaged over last 500 steps = 3.8815e-01, PNorm = 82.5870, GNorm = 0.1976
Meta loss on this task batch = 4.0494e-01, Meta loss averaged over last 500 steps = 3.8812e-01, PNorm = 82.5944, GNorm = 0.2766
Meta loss on this task batch = 4.0343e-01, Meta loss averaged over last 500 steps = 3.8814e-01, PNorm = 82.6034, GNorm = 0.1986
Meta loss on this task batch = 4.0323e-01, Meta loss averaged over last 500 steps = 3.8825e-01, PNorm = 82.6127, GNorm = 0.2542
Took 112.08794689178467 seconds to complete one epoch of meta training
Took 119.12702584266663 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470371
Epoch 138
Meta loss on this task batch = 4.0682e-01, Meta loss averaged over last 500 steps = 3.8826e-01, PNorm = 82.6219, GNorm = 0.2100
Meta loss on this task batch = 3.8310e-01, Meta loss averaged over last 500 steps = 3.8815e-01, PNorm = 82.6326, GNorm = 0.2225
Meta loss on this task batch = 4.1410e-01, Meta loss averaged over last 500 steps = 3.8813e-01, PNorm = 82.6447, GNorm = 0.2557
Meta loss on this task batch = 4.0770e-01, Meta loss averaged over last 500 steps = 3.8810e-01, PNorm = 82.6564, GNorm = 0.2339
Meta loss on this task batch = 4.0831e-01, Meta loss averaged over last 500 steps = 3.8802e-01, PNorm = 82.6681, GNorm = 0.3040
Meta loss on this task batch = 3.6788e-01, Meta loss averaged over last 500 steps = 3.8794e-01, PNorm = 82.6806, GNorm = 0.1955
Meta loss on this task batch = 3.9209e-01, Meta loss averaged over last 500 steps = 3.8795e-01, PNorm = 82.6926, GNorm = 0.2276
Meta loss on this task batch = 3.6174e-01, Meta loss averaged over last 500 steps = 3.8794e-01, PNorm = 82.7048, GNorm = 0.1952
Meta loss on this task batch = 3.5521e-01, Meta loss averaged over last 500 steps = 3.8780e-01, PNorm = 82.7177, GNorm = 0.1854
Meta loss on this task batch = 3.7808e-01, Meta loss averaged over last 500 steps = 3.8776e-01, PNorm = 82.7303, GNorm = 0.2050
Meta loss on this task batch = 3.6067e-01, Meta loss averaged over last 500 steps = 3.8773e-01, PNorm = 82.7439, GNorm = 0.3152
Meta loss on this task batch = 3.5438e-01, Meta loss averaged over last 500 steps = 3.8768e-01, PNorm = 82.7584, GNorm = 0.4251
Meta loss on this task batch = 4.0295e-01, Meta loss averaged over last 500 steps = 3.8762e-01, PNorm = 82.7716, GNorm = 0.2539
Meta loss on this task batch = 3.5704e-01, Meta loss averaged over last 500 steps = 3.8758e-01, PNorm = 82.7844, GNorm = 0.3453
Meta loss on this task batch = 3.3234e-01, Meta loss averaged over last 500 steps = 3.8738e-01, PNorm = 82.7980, GNorm = 0.4356
Meta loss on this task batch = 3.9884e-01, Meta loss averaged over last 500 steps = 3.8728e-01, PNorm = 82.8113, GNorm = 0.2230
Meta loss on this task batch = 3.8980e-01, Meta loss averaged over last 500 steps = 3.8730e-01, PNorm = 82.8243, GNorm = 0.2144
Meta loss on this task batch = 3.5375e-01, Meta loss averaged over last 500 steps = 3.8724e-01, PNorm = 82.8358, GNorm = 0.4357
Meta loss on this task batch = 3.3698e-01, Meta loss averaged over last 500 steps = 3.8716e-01, PNorm = 82.8475, GNorm = 0.2718
Took 112.26250958442688 seconds to complete one epoch of meta training
Took 119.94142651557922 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477392
Epoch 139
Meta loss on this task batch = 3.5935e-01, Meta loss averaged over last 500 steps = 3.8721e-01, PNorm = 82.8591, GNorm = 0.1928
Meta loss on this task batch = 3.4526e-01, Meta loss averaged over last 500 steps = 3.8706e-01, PNorm = 82.8704, GNorm = 0.2546
Meta loss on this task batch = 4.0996e-01, Meta loss averaged over last 500 steps = 3.8700e-01, PNorm = 82.8808, GNorm = 0.3755
Meta loss on this task batch = 3.7081e-01, Meta loss averaged over last 500 steps = 3.8694e-01, PNorm = 82.8919, GNorm = 0.2317
Meta loss on this task batch = 3.9588e-01, Meta loss averaged over last 500 steps = 3.8702e-01, PNorm = 82.9033, GNorm = 0.2006
Meta loss on this task batch = 3.7246e-01, Meta loss averaged over last 500 steps = 3.8698e-01, PNorm = 82.9154, GNorm = 0.2640
Meta loss on this task batch = 3.0400e-01, Meta loss averaged over last 500 steps = 3.8688e-01, PNorm = 82.9282, GNorm = 0.2265
Meta loss on this task batch = 4.0846e-01, Meta loss averaged over last 500 steps = 3.8679e-01, PNorm = 82.9393, GNorm = 0.3444
Meta loss on this task batch = 4.1561e-01, Meta loss averaged over last 500 steps = 3.8690e-01, PNorm = 82.9515, GNorm = 0.4034
Meta loss on this task batch = 4.0458e-01, Meta loss averaged over last 500 steps = 3.8693e-01, PNorm = 82.9648, GNorm = 0.2741
Meta loss on this task batch = 3.7154e-01, Meta loss averaged over last 500 steps = 3.8683e-01, PNorm = 82.9782, GNorm = 0.2807
Meta loss on this task batch = 3.6503e-01, Meta loss averaged over last 500 steps = 3.8666e-01, PNorm = 82.9910, GNorm = 0.2258
Meta loss on this task batch = 3.5898e-01, Meta loss averaged over last 500 steps = 3.8661e-01, PNorm = 83.0041, GNorm = 0.2206
Meta loss on this task batch = 3.7577e-01, Meta loss averaged over last 500 steps = 3.8652e-01, PNorm = 83.0178, GNorm = 0.3265
Meta loss on this task batch = 3.8080e-01, Meta loss averaged over last 500 steps = 3.8648e-01, PNorm = 83.0314, GNorm = 0.2218
Meta loss on this task batch = 4.8122e-01, Meta loss averaged over last 500 steps = 3.8650e-01, PNorm = 83.0436, GNorm = 0.2947
Meta loss on this task batch = 3.7542e-01, Meta loss averaged over last 500 steps = 3.8631e-01, PNorm = 83.0562, GNorm = 0.2307
Meta loss on this task batch = 3.5463e-01, Meta loss averaged over last 500 steps = 3.8635e-01, PNorm = 83.0694, GNorm = 0.2654
Meta loss on this task batch = 3.5153e-01, Meta loss averaged over last 500 steps = 3.8621e-01, PNorm = 83.0837, GNorm = 0.2311
Took 111.53155970573425 seconds to complete one epoch of meta training
Took 119.17610692977905 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459634
Epoch 140
Meta loss on this task batch = 3.2005e-01, Meta loss averaged over last 500 steps = 3.8612e-01, PNorm = 83.0985, GNorm = 0.2844
Meta loss on this task batch = 4.1055e-01, Meta loss averaged over last 500 steps = 3.8616e-01, PNorm = 83.1128, GNorm = 0.3673
Meta loss on this task batch = 3.5378e-01, Meta loss averaged over last 500 steps = 3.8618e-01, PNorm = 83.1275, GNorm = 0.2121
Meta loss on this task batch = 3.3802e-01, Meta loss averaged over last 500 steps = 3.8605e-01, PNorm = 83.1413, GNorm = 0.2517
Meta loss on this task batch = 3.0303e-01, Meta loss averaged over last 500 steps = 3.8581e-01, PNorm = 83.1544, GNorm = 0.1993
Meta loss on this task batch = 3.8438e-01, Meta loss averaged over last 500 steps = 3.8593e-01, PNorm = 83.1665, GNorm = 0.2110
Meta loss on this task batch = 3.8324e-01, Meta loss averaged over last 500 steps = 3.8597e-01, PNorm = 83.1765, GNorm = 0.2540
Meta loss on this task batch = 4.2436e-01, Meta loss averaged over last 500 steps = 3.8599e-01, PNorm = 83.1866, GNorm = 0.3588
Meta loss on this task batch = 4.5260e-01, Meta loss averaged over last 500 steps = 3.8607e-01, PNorm = 83.1969, GNorm = 0.3991
Meta loss on this task batch = 3.9654e-01, Meta loss averaged over last 500 steps = 3.8614e-01, PNorm = 83.2072, GNorm = 0.1793
Meta loss on this task batch = 3.7481e-01, Meta loss averaged over last 500 steps = 3.8612e-01, PNorm = 83.2178, GNorm = 0.1905
Meta loss on this task batch = 3.7941e-01, Meta loss averaged over last 500 steps = 3.8601e-01, PNorm = 83.2278, GNorm = 0.1776
Meta loss on this task batch = 3.4905e-01, Meta loss averaged over last 500 steps = 3.8603e-01, PNorm = 83.2383, GNorm = 0.1877
Meta loss on this task batch = 3.9603e-01, Meta loss averaged over last 500 steps = 3.8615e-01, PNorm = 83.2484, GNorm = 0.2581
Meta loss on this task batch = 3.3717e-01, Meta loss averaged over last 500 steps = 3.8601e-01, PNorm = 83.2599, GNorm = 0.2590
Meta loss on this task batch = 4.0908e-01, Meta loss averaged over last 500 steps = 3.8604e-01, PNorm = 83.2716, GNorm = 0.2096
Meta loss on this task batch = 4.0805e-01, Meta loss averaged over last 500 steps = 3.8598e-01, PNorm = 83.2832, GNorm = 0.2025
Meta loss on this task batch = 3.4663e-01, Meta loss averaged over last 500 steps = 3.8596e-01, PNorm = 83.2962, GNorm = 0.2882
Meta loss on this task batch = 4.4568e-01, Meta loss averaged over last 500 steps = 3.8597e-01, PNorm = 83.3088, GNorm = 0.2895
Took 110.38192701339722 seconds to complete one epoch of meta training
Took 118.37576413154602 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461253
Epoch 141
Meta loss on this task batch = 4.3355e-01, Meta loss averaged over last 500 steps = 3.8602e-01, PNorm = 83.3205, GNorm = 0.2236
Meta loss on this task batch = 4.0934e-01, Meta loss averaged over last 500 steps = 3.8598e-01, PNorm = 83.3317, GNorm = 0.2840
Meta loss on this task batch = 3.6620e-01, Meta loss averaged over last 500 steps = 3.8584e-01, PNorm = 83.3443, GNorm = 0.2595
Meta loss on this task batch = 3.8928e-01, Meta loss averaged over last 500 steps = 3.8574e-01, PNorm = 83.3570, GNorm = 0.2519
Meta loss on this task batch = 3.1273e-01, Meta loss averaged over last 500 steps = 3.8566e-01, PNorm = 83.3706, GNorm = 0.1829
Meta loss on this task batch = 3.8249e-01, Meta loss averaged over last 500 steps = 3.8582e-01, PNorm = 83.3838, GNorm = 0.2811
Meta loss on this task batch = 3.5148e-01, Meta loss averaged over last 500 steps = 3.8575e-01, PNorm = 83.3978, GNorm = 0.1968
Meta loss on this task batch = 4.0592e-01, Meta loss averaged over last 500 steps = 3.8575e-01, PNorm = 83.4109, GNorm = 0.2767
Meta loss on this task batch = 3.3050e-01, Meta loss averaged over last 500 steps = 3.8556e-01, PNorm = 83.4233, GNorm = 0.2115
Meta loss on this task batch = 4.0513e-01, Meta loss averaged over last 500 steps = 3.8559e-01, PNorm = 83.4337, GNorm = 0.2500
Meta loss on this task batch = 3.9937e-01, Meta loss averaged over last 500 steps = 3.8557e-01, PNorm = 83.4432, GNorm = 0.2241
Meta loss on this task batch = 3.9607e-01, Meta loss averaged over last 500 steps = 3.8554e-01, PNorm = 83.4519, GNorm = 0.4037
Meta loss on this task batch = 3.5378e-01, Meta loss averaged over last 500 steps = 3.8541e-01, PNorm = 83.4618, GNorm = 0.3079
Meta loss on this task batch = 4.0852e-01, Meta loss averaged over last 500 steps = 3.8546e-01, PNorm = 83.4719, GNorm = 0.2893
Meta loss on this task batch = 3.5055e-01, Meta loss averaged over last 500 steps = 3.8535e-01, PNorm = 83.4830, GNorm = 0.3858
Meta loss on this task batch = 3.5937e-01, Meta loss averaged over last 500 steps = 3.8513e-01, PNorm = 83.4948, GNorm = 0.2506
Meta loss on this task batch = 4.1499e-01, Meta loss averaged over last 500 steps = 3.8519e-01, PNorm = 83.5055, GNorm = 0.2405
Meta loss on this task batch = 3.4058e-01, Meta loss averaged over last 500 steps = 3.8525e-01, PNorm = 83.5169, GNorm = 0.2687
Meta loss on this task batch = 4.0351e-01, Meta loss averaged over last 500 steps = 3.8535e-01, PNorm = 83.5284, GNorm = 0.3054
Took 113.15302085876465 seconds to complete one epoch of meta training
Took 120.77869272232056 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459384
Epoch 142
Meta loss on this task batch = 3.3441e-01, Meta loss averaged over last 500 steps = 3.8522e-01, PNorm = 83.5399, GNorm = 0.2345
Meta loss on this task batch = 4.1767e-01, Meta loss averaged over last 500 steps = 3.8523e-01, PNorm = 83.5511, GNorm = 0.2613
Meta loss on this task batch = 4.2291e-01, Meta loss averaged over last 500 steps = 3.8523e-01, PNorm = 83.5619, GNorm = 0.2753
Meta loss on this task batch = 3.6621e-01, Meta loss averaged over last 500 steps = 3.8511e-01, PNorm = 83.5737, GNorm = 0.1616
Meta loss on this task batch = 3.0579e-01, Meta loss averaged over last 500 steps = 3.8499e-01, PNorm = 83.5864, GNorm = 0.2106
Meta loss on this task batch = 3.1722e-01, Meta loss averaged over last 500 steps = 3.8480e-01, PNorm = 83.5998, GNorm = 0.1833
Meta loss on this task batch = 4.1759e-01, Meta loss averaged over last 500 steps = 3.8480e-01, PNorm = 83.6131, GNorm = 0.3971
Meta loss on this task batch = 3.9980e-01, Meta loss averaged over last 500 steps = 3.8489e-01, PNorm = 83.6263, GNorm = 0.3479
Meta loss on this task batch = 3.7305e-01, Meta loss averaged over last 500 steps = 3.8477e-01, PNorm = 83.6380, GNorm = 0.2122
Meta loss on this task batch = 4.1206e-01, Meta loss averaged over last 500 steps = 3.8480e-01, PNorm = 83.6497, GNorm = 0.2477
Meta loss on this task batch = 3.9210e-01, Meta loss averaged over last 500 steps = 3.8465e-01, PNorm = 83.6612, GNorm = 0.2893
Meta loss on this task batch = 3.4082e-01, Meta loss averaged over last 500 steps = 3.8456e-01, PNorm = 83.6727, GNorm = 0.2073
Meta loss on this task batch = 3.5891e-01, Meta loss averaged over last 500 steps = 3.8452e-01, PNorm = 83.6846, GNorm = 0.1978
Meta loss on this task batch = 4.2647e-01, Meta loss averaged over last 500 steps = 3.8464e-01, PNorm = 83.6955, GNorm = 0.4250
Meta loss on this task batch = 3.6118e-01, Meta loss averaged over last 500 steps = 3.8448e-01, PNorm = 83.7066, GNorm = 0.4739
Meta loss on this task batch = 3.8661e-01, Meta loss averaged over last 500 steps = 3.8455e-01, PNorm = 83.7176, GNorm = 0.3589
Meta loss on this task batch = 3.7793e-01, Meta loss averaged over last 500 steps = 3.8451e-01, PNorm = 83.7282, GNorm = 0.2373
Meta loss on this task batch = 3.4908e-01, Meta loss averaged over last 500 steps = 3.8446e-01, PNorm = 83.7391, GNorm = 0.2090
Meta loss on this task batch = 4.1189e-01, Meta loss averaged over last 500 steps = 3.8454e-01, PNorm = 83.7503, GNorm = 0.4905
Took 113.68313097953796 seconds to complete one epoch of meta training
Took 121.68109893798828 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474341
Epoch 143
Meta loss on this task batch = 3.9971e-01, Meta loss averaged over last 500 steps = 3.8458e-01, PNorm = 83.7615, GNorm = 0.3700
Meta loss on this task batch = 3.8087e-01, Meta loss averaged over last 500 steps = 3.8455e-01, PNorm = 83.7730, GNorm = 0.2453
Meta loss on this task batch = 3.2059e-01, Meta loss averaged over last 500 steps = 3.8435e-01, PNorm = 83.7851, GNorm = 0.1995
Meta loss on this task batch = 4.2153e-01, Meta loss averaged over last 500 steps = 3.8450e-01, PNorm = 83.7967, GNorm = 0.2155
Meta loss on this task batch = 3.8755e-01, Meta loss averaged over last 500 steps = 3.8456e-01, PNorm = 83.8088, GNorm = 0.2752
Meta loss on this task batch = 4.2294e-01, Meta loss averaged over last 500 steps = 3.8450e-01, PNorm = 83.8204, GNorm = 0.5230
Meta loss on this task batch = 3.9453e-01, Meta loss averaged over last 500 steps = 3.8438e-01, PNorm = 83.8315, GNorm = 0.4128
Meta loss on this task batch = 3.8909e-01, Meta loss averaged over last 500 steps = 3.8441e-01, PNorm = 83.8426, GNorm = 0.1958
Meta loss on this task batch = 3.7004e-01, Meta loss averaged over last 500 steps = 3.8426e-01, PNorm = 83.8544, GNorm = 0.1981
Meta loss on this task batch = 3.9301e-01, Meta loss averaged over last 500 steps = 3.8425e-01, PNorm = 83.8659, GNorm = 0.2148
Meta loss on this task batch = 3.5916e-01, Meta loss averaged over last 500 steps = 3.8416e-01, PNorm = 83.8769, GNorm = 0.1919
Meta loss on this task batch = 3.6110e-01, Meta loss averaged over last 500 steps = 3.8409e-01, PNorm = 83.8874, GNorm = 0.2106
Meta loss on this task batch = 3.0677e-01, Meta loss averaged over last 500 steps = 3.8397e-01, PNorm = 83.8991, GNorm = 0.4295
Meta loss on this task batch = 3.1493e-01, Meta loss averaged over last 500 steps = 3.8389e-01, PNorm = 83.9107, GNorm = 0.1956
Meta loss on this task batch = 3.9283e-01, Meta loss averaged over last 500 steps = 3.8394e-01, PNorm = 83.9233, GNorm = 0.3064
Meta loss on this task batch = 3.4833e-01, Meta loss averaged over last 500 steps = 3.8379e-01, PNorm = 83.9359, GNorm = 0.2304
Meta loss on this task batch = 4.4493e-01, Meta loss averaged over last 500 steps = 3.8396e-01, PNorm = 83.9484, GNorm = 0.2562
Meta loss on this task batch = 4.1604e-01, Meta loss averaged over last 500 steps = 3.8409e-01, PNorm = 83.9607, GNorm = 0.2194
Meta loss on this task batch = 3.9533e-01, Meta loss averaged over last 500 steps = 3.8417e-01, PNorm = 83.9729, GNorm = 0.3615
Took 113.87246251106262 seconds to complete one epoch of meta training
Took 121.89977645874023 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466559
Epoch 144
Meta loss on this task batch = 3.9628e-01, Meta loss averaged over last 500 steps = 3.8414e-01, PNorm = 83.9862, GNorm = 0.2281
Meta loss on this task batch = 3.8704e-01, Meta loss averaged over last 500 steps = 3.8416e-01, PNorm = 83.9996, GNorm = 0.2712
Meta loss on this task batch = 3.2278e-01, Meta loss averaged over last 500 steps = 3.8383e-01, PNorm = 84.0139, GNorm = 0.2828
Meta loss on this task batch = 4.0878e-01, Meta loss averaged over last 500 steps = 3.8384e-01, PNorm = 84.0270, GNorm = 0.2515
Meta loss on this task batch = 3.6861e-01, Meta loss averaged over last 500 steps = 3.8381e-01, PNorm = 84.0403, GNorm = 0.2221
Meta loss on this task batch = 3.1447e-01, Meta loss averaged over last 500 steps = 3.8373e-01, PNorm = 84.0536, GNorm = 0.3050
Meta loss on this task batch = 3.7964e-01, Meta loss averaged over last 500 steps = 3.8373e-01, PNorm = 84.0651, GNorm = 0.3993
Meta loss on this task batch = 3.5173e-01, Meta loss averaged over last 500 steps = 3.8378e-01, PNorm = 84.0764, GNorm = 0.3009
Meta loss on this task batch = 3.9759e-01, Meta loss averaged over last 500 steps = 3.8387e-01, PNorm = 84.0871, GNorm = 0.4125
Meta loss on this task batch = 3.5054e-01, Meta loss averaged over last 500 steps = 3.8374e-01, PNorm = 84.0981, GNorm = 0.3345
Meta loss on this task batch = 4.3543e-01, Meta loss averaged over last 500 steps = 3.8374e-01, PNorm = 84.1082, GNorm = 0.2571
Meta loss on this task batch = 3.2959e-01, Meta loss averaged over last 500 steps = 3.8354e-01, PNorm = 84.1181, GNorm = 0.2505
Meta loss on this task batch = 3.8880e-01, Meta loss averaged over last 500 steps = 3.8366e-01, PNorm = 84.1271, GNorm = 0.2620
Meta loss on this task batch = 3.5967e-01, Meta loss averaged over last 500 steps = 3.8373e-01, PNorm = 84.1360, GNorm = 0.2884
Meta loss on this task batch = 4.0816e-01, Meta loss averaged over last 500 steps = 3.8354e-01, PNorm = 84.1462, GNorm = 0.2869
Meta loss on this task batch = 3.9193e-01, Meta loss averaged over last 500 steps = 3.8365e-01, PNorm = 84.1560, GNorm = 0.5288
Meta loss on this task batch = 3.7069e-01, Meta loss averaged over last 500 steps = 3.8352e-01, PNorm = 84.1661, GNorm = 0.2429
Meta loss on this task batch = 4.0729e-01, Meta loss averaged over last 500 steps = 3.8347e-01, PNorm = 84.1766, GNorm = 0.3543
Meta loss on this task batch = 3.7276e-01, Meta loss averaged over last 500 steps = 3.8336e-01, PNorm = 84.1868, GNorm = 0.2383
Took 112.83554077148438 seconds to complete one epoch of meta training
Took 119.68655705451965 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479454
Epoch 145
Meta loss on this task batch = 3.9268e-01, Meta loss averaged over last 500 steps = 3.8327e-01, PNorm = 84.1985, GNorm = 0.1894
Meta loss on this task batch = 4.5138e-01, Meta loss averaged over last 500 steps = 3.8341e-01, PNorm = 84.2107, GNorm = 0.2966
Meta loss on this task batch = 3.4951e-01, Meta loss averaged over last 500 steps = 3.8340e-01, PNorm = 84.2239, GNorm = 0.2745
Meta loss on this task batch = 3.6227e-01, Meta loss averaged over last 500 steps = 3.8334e-01, PNorm = 84.2371, GNorm = 0.2164
Meta loss on this task batch = 3.6519e-01, Meta loss averaged over last 500 steps = 3.8323e-01, PNorm = 84.2502, GNorm = 0.1902
Meta loss on this task batch = 3.8371e-01, Meta loss averaged over last 500 steps = 3.8309e-01, PNorm = 84.2639, GNorm = 0.2464
Meta loss on this task batch = 3.4810e-01, Meta loss averaged over last 500 steps = 3.8300e-01, PNorm = 84.2778, GNorm = 0.1995
Meta loss on this task batch = 3.0688e-01, Meta loss averaged over last 500 steps = 3.8288e-01, PNorm = 84.2915, GNorm = 0.1996
Meta loss on this task batch = 3.9896e-01, Meta loss averaged over last 500 steps = 3.8296e-01, PNorm = 84.3044, GNorm = 0.3537
Meta loss on this task batch = 3.7324e-01, Meta loss averaged over last 500 steps = 3.8293e-01, PNorm = 84.3172, GNorm = 0.1995
Meta loss on this task batch = 3.4141e-01, Meta loss averaged over last 500 steps = 3.8281e-01, PNorm = 84.3297, GNorm = 0.1959
Meta loss on this task batch = 3.3074e-01, Meta loss averaged over last 500 steps = 3.8255e-01, PNorm = 84.3440, GNorm = 0.4053
Meta loss on this task batch = 4.0402e-01, Meta loss averaged over last 500 steps = 3.8256e-01, PNorm = 84.3574, GNorm = 0.2578
Meta loss on this task batch = 4.0820e-01, Meta loss averaged over last 500 steps = 3.8257e-01, PNorm = 84.3706, GNorm = 0.2611
Meta loss on this task batch = 3.7661e-01, Meta loss averaged over last 500 steps = 3.8251e-01, PNorm = 84.3840, GNorm = 0.2232
Meta loss on this task batch = 3.6432e-01, Meta loss averaged over last 500 steps = 3.8248e-01, PNorm = 84.3970, GNorm = 0.2243
Meta loss on this task batch = 3.5670e-01, Meta loss averaged over last 500 steps = 3.8245e-01, PNorm = 84.4097, GNorm = 0.2274
Meta loss on this task batch = 3.1196e-01, Meta loss averaged over last 500 steps = 3.8225e-01, PNorm = 84.4225, GNorm = 0.2340
Meta loss on this task batch = 2.9593e-01, Meta loss averaged over last 500 steps = 3.8194e-01, PNorm = 84.4353, GNorm = 0.2511
Took 112.49003791809082 seconds to complete one epoch of meta training
Took 119.20201468467712 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475707
Epoch 146
Meta loss on this task batch = 3.7099e-01, Meta loss averaged over last 500 steps = 3.8202e-01, PNorm = 84.4469, GNorm = 0.2554
Meta loss on this task batch = 4.5280e-01, Meta loss averaged over last 500 steps = 3.8202e-01, PNorm = 84.4567, GNorm = 0.2436
Meta loss on this task batch = 3.4221e-01, Meta loss averaged over last 500 steps = 3.8200e-01, PNorm = 84.4664, GNorm = 0.4349
Meta loss on this task batch = 3.9796e-01, Meta loss averaged over last 500 steps = 3.8207e-01, PNorm = 84.4762, GNorm = 0.2506
Meta loss on this task batch = 3.2174e-01, Meta loss averaged over last 500 steps = 3.8205e-01, PNorm = 84.4869, GNorm = 0.2923
Meta loss on this task batch = 3.5007e-01, Meta loss averaged over last 500 steps = 3.8195e-01, PNorm = 84.4976, GNorm = 0.2520
Meta loss on this task batch = 4.3334e-01, Meta loss averaged over last 500 steps = 3.8204e-01, PNorm = 84.5072, GNorm = 0.3360
Meta loss on this task batch = 3.5325e-01, Meta loss averaged over last 500 steps = 3.8194e-01, PNorm = 84.5178, GNorm = 0.2406
Meta loss on this task batch = 3.9642e-01, Meta loss averaged over last 500 steps = 3.8203e-01, PNorm = 84.5289, GNorm = 0.2362
Meta loss on this task batch = 3.0765e-01, Meta loss averaged over last 500 steps = 3.8194e-01, PNorm = 84.5402, GNorm = 0.2398
Meta loss on this task batch = 3.9064e-01, Meta loss averaged over last 500 steps = 3.8195e-01, PNorm = 84.5519, GNorm = 0.2614
Meta loss on this task batch = 3.8774e-01, Meta loss averaged over last 500 steps = 3.8199e-01, PNorm = 84.5641, GNorm = 0.4510
Meta loss on this task batch = 3.7821e-01, Meta loss averaged over last 500 steps = 3.8197e-01, PNorm = 84.5769, GNorm = 0.2087
Meta loss on this task batch = 3.7594e-01, Meta loss averaged over last 500 steps = 3.8186e-01, PNorm = 84.5910, GNorm = 0.4950
Meta loss on this task batch = 3.8188e-01, Meta loss averaged over last 500 steps = 3.8176e-01, PNorm = 84.6052, GNorm = 0.1897
Meta loss on this task batch = 4.4011e-01, Meta loss averaged over last 500 steps = 3.8176e-01, PNorm = 84.6174, GNorm = 0.4924
Meta loss on this task batch = 3.5058e-01, Meta loss averaged over last 500 steps = 3.8164e-01, PNorm = 84.6294, GNorm = 0.2957
Meta loss on this task batch = 3.7917e-01, Meta loss averaged over last 500 steps = 3.8173e-01, PNorm = 84.6418, GNorm = 0.3375
Meta loss on this task batch = 4.2981e-01, Meta loss averaged over last 500 steps = 3.8187e-01, PNorm = 84.6551, GNorm = 0.3654
Took 113.29002332687378 seconds to complete one epoch of meta training
Took 121.2592134475708 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505445
Found better MAML checkpoint after meta validation, saving now
Epoch 147
Meta loss on this task batch = 3.6711e-01, Meta loss averaged over last 500 steps = 3.8179e-01, PNorm = 84.6676, GNorm = 0.2203
Meta loss on this task batch = 3.5789e-01, Meta loss averaged over last 500 steps = 3.8178e-01, PNorm = 84.6804, GNorm = 0.2475
Meta loss on this task batch = 3.9146e-01, Meta loss averaged over last 500 steps = 3.8168e-01, PNorm = 84.6934, GNorm = 0.2523
Meta loss on this task batch = 3.5273e-01, Meta loss averaged over last 500 steps = 3.8163e-01, PNorm = 84.7058, GNorm = 0.2089
Meta loss on this task batch = 4.0910e-01, Meta loss averaged over last 500 steps = 3.8157e-01, PNorm = 84.7177, GNorm = 0.2995
Meta loss on this task batch = 3.6763e-01, Meta loss averaged over last 500 steps = 3.8162e-01, PNorm = 84.7289, GNorm = 0.2408
Meta loss on this task batch = 4.4635e-01, Meta loss averaged over last 500 steps = 3.8180e-01, PNorm = 84.7390, GNorm = 0.2271
Meta loss on this task batch = 3.8876e-01, Meta loss averaged over last 500 steps = 3.8169e-01, PNorm = 84.7483, GNorm = 0.4301
Meta loss on this task batch = 3.7907e-01, Meta loss averaged over last 500 steps = 3.8164e-01, PNorm = 84.7570, GNorm = 0.2211
Meta loss on this task batch = 3.4489e-01, Meta loss averaged over last 500 steps = 3.8159e-01, PNorm = 84.7673, GNorm = 0.1810
Meta loss on this task batch = 3.4980e-01, Meta loss averaged over last 500 steps = 3.8140e-01, PNorm = 84.7785, GNorm = 0.2213
Meta loss on this task batch = 3.4258e-01, Meta loss averaged over last 500 steps = 3.8135e-01, PNorm = 84.7892, GNorm = 0.2219
Meta loss on this task batch = 3.2495e-01, Meta loss averaged over last 500 steps = 3.8139e-01, PNorm = 84.8010, GNorm = 0.1788
Meta loss on this task batch = 3.6135e-01, Meta loss averaged over last 500 steps = 3.8142e-01, PNorm = 84.8137, GNorm = 0.2169
Meta loss on this task batch = 3.5205e-01, Meta loss averaged over last 500 steps = 3.8140e-01, PNorm = 84.8257, GNorm = 0.2077
Meta loss on this task batch = 3.7777e-01, Meta loss averaged over last 500 steps = 3.8138e-01, PNorm = 84.8369, GNorm = 0.2447
Meta loss on this task batch = 4.3305e-01, Meta loss averaged over last 500 steps = 3.8154e-01, PNorm = 84.8470, GNorm = 0.3164
Meta loss on this task batch = 4.0188e-01, Meta loss averaged over last 500 steps = 3.8149e-01, PNorm = 84.8558, GNorm = 0.4436
Meta loss on this task batch = 3.2792e-01, Meta loss averaged over last 500 steps = 3.8130e-01, PNorm = 84.8652, GNorm = 0.2482
Took 111.16532468795776 seconds to complete one epoch of meta training
Took 118.87614727020264 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490186
Epoch 148
Meta loss on this task batch = 4.4337e-01, Meta loss averaged over last 500 steps = 3.8135e-01, PNorm = 84.8742, GNorm = 0.2769
Meta loss on this task batch = 3.3421e-01, Meta loss averaged over last 500 steps = 3.8128e-01, PNorm = 84.8840, GNorm = 0.2625
Meta loss on this task batch = 3.8190e-01, Meta loss averaged over last 500 steps = 3.8119e-01, PNorm = 84.8945, GNorm = 0.2164
Meta loss on this task batch = 3.5043e-01, Meta loss averaged over last 500 steps = 3.8105e-01, PNorm = 84.9060, GNorm = 0.1842
Meta loss on this task batch = 3.5555e-01, Meta loss averaged over last 500 steps = 3.8116e-01, PNorm = 84.9164, GNorm = 0.2848
Meta loss on this task batch = 3.5610e-01, Meta loss averaged over last 500 steps = 3.8110e-01, PNorm = 84.9276, GNorm = 0.2788
Meta loss on this task batch = 4.1439e-01, Meta loss averaged over last 500 steps = 3.8113e-01, PNorm = 84.9382, GNorm = 0.3596
Meta loss on this task batch = 4.3709e-01, Meta loss averaged over last 500 steps = 3.8112e-01, PNorm = 84.9494, GNorm = 0.3474
Meta loss on this task batch = 3.8321e-01, Meta loss averaged over last 500 steps = 3.8116e-01, PNorm = 84.9606, GNorm = 0.2066
Meta loss on this task batch = 3.7968e-01, Meta loss averaged over last 500 steps = 3.8118e-01, PNorm = 84.9719, GNorm = 0.3515
Meta loss on this task batch = 3.9829e-01, Meta loss averaged over last 500 steps = 3.8127e-01, PNorm = 84.9833, GNorm = 0.2142
Meta loss on this task batch = 3.2185e-01, Meta loss averaged over last 500 steps = 3.8109e-01, PNorm = 84.9957, GNorm = 0.2098
Meta loss on this task batch = 3.6004e-01, Meta loss averaged over last 500 steps = 3.8098e-01, PNorm = 85.0082, GNorm = 0.2602
Meta loss on this task batch = 4.1132e-01, Meta loss averaged over last 500 steps = 3.8102e-01, PNorm = 85.0200, GNorm = 0.2520
Meta loss on this task batch = 3.7286e-01, Meta loss averaged over last 500 steps = 3.8099e-01, PNorm = 85.0315, GNorm = 0.2157
Meta loss on this task batch = 4.0902e-01, Meta loss averaged over last 500 steps = 3.8102e-01, PNorm = 85.0429, GNorm = 0.4510
Meta loss on this task batch = 3.2490e-01, Meta loss averaged over last 500 steps = 3.8093e-01, PNorm = 85.0556, GNorm = 0.3106
Meta loss on this task batch = 3.5949e-01, Meta loss averaged over last 500 steps = 3.8092e-01, PNorm = 85.0690, GNorm = 0.3313
Meta loss on this task batch = 4.0944e-01, Meta loss averaged over last 500 steps = 3.8094e-01, PNorm = 85.0824, GNorm = 0.3170
Took 114.37164402008057 seconds to complete one epoch of meta training
Took 122.01080322265625 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478262
Epoch 149
Meta loss on this task batch = 3.7449e-01, Meta loss averaged over last 500 steps = 3.8107e-01, PNorm = 85.0958, GNorm = 0.2973
Meta loss on this task batch = 3.6615e-01, Meta loss averaged over last 500 steps = 3.8088e-01, PNorm = 85.1094, GNorm = 0.3885
Meta loss on this task batch = 3.7616e-01, Meta loss averaged over last 500 steps = 3.8089e-01, PNorm = 85.1224, GNorm = 0.1969
Meta loss on this task batch = 3.8665e-01, Meta loss averaged over last 500 steps = 3.8083e-01, PNorm = 85.1345, GNorm = 0.3057
Meta loss on this task batch = 4.4369e-01, Meta loss averaged over last 500 steps = 3.8103e-01, PNorm = 85.1457, GNorm = 0.3042
Meta loss on this task batch = 3.9368e-01, Meta loss averaged over last 500 steps = 3.8097e-01, PNorm = 85.1576, GNorm = 0.2872
Meta loss on this task batch = 3.4717e-01, Meta loss averaged over last 500 steps = 3.8083e-01, PNorm = 85.1681, GNorm = 0.3411
Meta loss on this task batch = 3.7954e-01, Meta loss averaged over last 500 steps = 3.8094e-01, PNorm = 85.1788, GNorm = 0.3791
Meta loss on this task batch = 3.6991e-01, Meta loss averaged over last 500 steps = 3.8083e-01, PNorm = 85.1907, GNorm = 0.2997
Meta loss on this task batch = 4.0265e-01, Meta loss averaged over last 500 steps = 3.8078e-01, PNorm = 85.2033, GNorm = 0.3274
Meta loss on this task batch = 3.6957e-01, Meta loss averaged over last 500 steps = 3.8078e-01, PNorm = 85.2149, GNorm = 0.2222
Meta loss on this task batch = 3.8132e-01, Meta loss averaged over last 500 steps = 3.8072e-01, PNorm = 85.2269, GNorm = 0.2998
Meta loss on this task batch = 3.4674e-01, Meta loss averaged over last 500 steps = 3.8060e-01, PNorm = 85.2396, GNorm = 0.3055
Meta loss on this task batch = 3.7479e-01, Meta loss averaged over last 500 steps = 3.8061e-01, PNorm = 85.2510, GNorm = 0.2440
Meta loss on this task batch = 4.1821e-01, Meta loss averaged over last 500 steps = 3.8061e-01, PNorm = 85.2619, GNorm = 0.3327
Meta loss on this task batch = 2.9466e-01, Meta loss averaged over last 500 steps = 3.8043e-01, PNorm = 85.2735, GNorm = 0.2190
Meta loss on this task batch = 3.4931e-01, Meta loss averaged over last 500 steps = 3.8048e-01, PNorm = 85.2851, GNorm = 0.3988
Meta loss on this task batch = 4.1394e-01, Meta loss averaged over last 500 steps = 3.8054e-01, PNorm = 85.2954, GNorm = 0.2468
Meta loss on this task batch = 3.7586e-01, Meta loss averaged over last 500 steps = 3.8063e-01, PNorm = 85.3061, GNorm = 0.3712
Took 114.23713660240173 seconds to complete one epoch of meta training
Took 121.83027124404907 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468021
Epoch 150
Meta loss on this task batch = 3.7918e-01, Meta loss averaged over last 500 steps = 3.8049e-01, PNorm = 85.3171, GNorm = 0.2051
Meta loss on this task batch = 3.7383e-01, Meta loss averaged over last 500 steps = 3.8038e-01, PNorm = 85.3285, GNorm = 0.1793
Meta loss on this task batch = 4.1425e-01, Meta loss averaged over last 500 steps = 3.8044e-01, PNorm = 85.3407, GNorm = 0.3075
Meta loss on this task batch = 4.0662e-01, Meta loss averaged over last 500 steps = 3.8053e-01, PNorm = 85.3530, GNorm = 0.2026
Meta loss on this task batch = 3.0218e-01, Meta loss averaged over last 500 steps = 3.8029e-01, PNorm = 85.3664, GNorm = 0.2865
Meta loss on this task batch = 3.9772e-01, Meta loss averaged over last 500 steps = 3.8029e-01, PNorm = 85.3805, GNorm = 0.2326
Meta loss on this task batch = 3.3402e-01, Meta loss averaged over last 500 steps = 3.8020e-01, PNorm = 85.3946, GNorm = 0.2703
Meta loss on this task batch = 4.3540e-01, Meta loss averaged over last 500 steps = 3.8037e-01, PNorm = 85.4070, GNorm = 0.4979
Meta loss on this task batch = 4.0425e-01, Meta loss averaged over last 500 steps = 3.8044e-01, PNorm = 85.4192, GNorm = 0.2991
Meta loss on this task batch = 4.2925e-01, Meta loss averaged over last 500 steps = 3.8045e-01, PNorm = 85.4302, GNorm = 0.4248
Meta loss on this task batch = 3.8166e-01, Meta loss averaged over last 500 steps = 3.8036e-01, PNorm = 85.4415, GNorm = 0.2192
Meta loss on this task batch = 3.1389e-01, Meta loss averaged over last 500 steps = 3.8022e-01, PNorm = 85.4526, GNorm = 0.2003
Meta loss on this task batch = 3.6446e-01, Meta loss averaged over last 500 steps = 3.8022e-01, PNorm = 85.4635, GNorm = 0.1932
Meta loss on this task batch = 4.3700e-01, Meta loss averaged over last 500 steps = 3.8032e-01, PNorm = 85.4745, GNorm = 0.2357
Meta loss on this task batch = 3.1886e-01, Meta loss averaged over last 500 steps = 3.8032e-01, PNorm = 85.4865, GNorm = 0.2148
Meta loss on this task batch = 3.3747e-01, Meta loss averaged over last 500 steps = 3.8014e-01, PNorm = 85.4989, GNorm = 0.2046
Meta loss on this task batch = 3.8159e-01, Meta loss averaged over last 500 steps = 3.8021e-01, PNorm = 85.5116, GNorm = 0.5826
Meta loss on this task batch = 3.5732e-01, Meta loss averaged over last 500 steps = 3.8025e-01, PNorm = 85.5245, GNorm = 0.2044
Meta loss on this task batch = 4.0002e-01, Meta loss averaged over last 500 steps = 3.8015e-01, PNorm = 85.5369, GNorm = 0.3010
Took 113.32381820678711 seconds to complete one epoch of meta training
Took 120.01127815246582 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455688
Epoch 151
Meta loss on this task batch = 3.8912e-01, Meta loss averaged over last 500 steps = 3.8008e-01, PNorm = 85.5494, GNorm = 0.3607
Meta loss on this task batch = 4.1906e-01, Meta loss averaged over last 500 steps = 3.8017e-01, PNorm = 85.5613, GNorm = 0.2319
Meta loss on this task batch = 4.1397e-01, Meta loss averaged over last 500 steps = 3.8023e-01, PNorm = 85.5736, GNorm = 0.2243
Meta loss on this task batch = 3.7898e-01, Meta loss averaged over last 500 steps = 3.8020e-01, PNorm = 85.5863, GNorm = 0.2191
Meta loss on this task batch = 3.3134e-01, Meta loss averaged over last 500 steps = 3.8002e-01, PNorm = 85.5994, GNorm = 0.2194
Meta loss on this task batch = 3.2667e-01, Meta loss averaged over last 500 steps = 3.7979e-01, PNorm = 85.6126, GNorm = 0.2182
Meta loss on this task batch = 3.8316e-01, Meta loss averaged over last 500 steps = 3.7969e-01, PNorm = 85.6257, GNorm = 0.2673
Meta loss on this task batch = 3.6179e-01, Meta loss averaged over last 500 steps = 3.7951e-01, PNorm = 85.6389, GNorm = 0.2992
Meta loss on this task batch = 3.4573e-01, Meta loss averaged over last 500 steps = 3.7944e-01, PNorm = 85.6530, GNorm = 0.2058
Meta loss on this task batch = 3.8296e-01, Meta loss averaged over last 500 steps = 3.7940e-01, PNorm = 85.6668, GNorm = 0.2364
Meta loss on this task batch = 3.6475e-01, Meta loss averaged over last 500 steps = 3.7919e-01, PNorm = 85.6805, GNorm = 0.2531
Meta loss on this task batch = 3.5619e-01, Meta loss averaged over last 500 steps = 3.7923e-01, PNorm = 85.6942, GNorm = 0.2185
Meta loss on this task batch = 3.4633e-01, Meta loss averaged over last 500 steps = 3.7916e-01, PNorm = 85.7080, GNorm = 0.2166
Meta loss on this task batch = 3.9922e-01, Meta loss averaged over last 500 steps = 3.7918e-01, PNorm = 85.7212, GNorm = 0.3009
Meta loss on this task batch = 3.4654e-01, Meta loss averaged over last 500 steps = 3.7912e-01, PNorm = 85.7349, GNorm = 0.2175
Meta loss on this task batch = 4.4311e-01, Meta loss averaged over last 500 steps = 3.7918e-01, PNorm = 85.7465, GNorm = 0.3425
Meta loss on this task batch = 3.6042e-01, Meta loss averaged over last 500 steps = 3.7918e-01, PNorm = 85.7587, GNorm = 0.2321
Meta loss on this task batch = 3.2808e-01, Meta loss averaged over last 500 steps = 3.7909e-01, PNorm = 85.7710, GNorm = 0.2146
Meta loss on this task batch = 3.6576e-01, Meta loss averaged over last 500 steps = 3.7914e-01, PNorm = 85.7826, GNorm = 0.2833
Took 114.38430428504944 seconds to complete one epoch of meta training
Took 121.36210775375366 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448056
Epoch 152
Meta loss on this task batch = 3.4168e-01, Meta loss averaged over last 500 steps = 3.7903e-01, PNorm = 85.7937, GNorm = 0.2001
Meta loss on this task batch = 4.1582e-01, Meta loss averaged over last 500 steps = 3.7919e-01, PNorm = 85.8047, GNorm = 0.2512
Meta loss on this task batch = 3.7214e-01, Meta loss averaged over last 500 steps = 3.7924e-01, PNorm = 85.8153, GNorm = 0.2308
Meta loss on this task batch = 4.3872e-01, Meta loss averaged over last 500 steps = 3.7939e-01, PNorm = 85.8267, GNorm = 0.3256
Meta loss on this task batch = 3.3235e-01, Meta loss averaged over last 500 steps = 3.7925e-01, PNorm = 85.8381, GNorm = 0.1773
Meta loss on this task batch = 3.9360e-01, Meta loss averaged over last 500 steps = 3.7919e-01, PNorm = 85.8496, GNorm = 0.2186
Meta loss on this task batch = 3.8166e-01, Meta loss averaged over last 500 steps = 3.7916e-01, PNorm = 85.8613, GNorm = 0.2519
Meta loss on this task batch = 3.3707e-01, Meta loss averaged over last 500 steps = 3.7915e-01, PNorm = 85.8738, GNorm = 0.3225
Meta loss on this task batch = 3.5556e-01, Meta loss averaged over last 500 steps = 3.7912e-01, PNorm = 85.8868, GNorm = 0.2004
Meta loss on this task batch = 4.0966e-01, Meta loss averaged over last 500 steps = 3.7920e-01, PNorm = 85.8987, GNorm = 0.3019
Meta loss on this task batch = 3.0657e-01, Meta loss averaged over last 500 steps = 3.7897e-01, PNorm = 85.9102, GNorm = 0.2055
Meta loss on this task batch = 3.9616e-01, Meta loss averaged over last 500 steps = 3.7891e-01, PNorm = 85.9218, GNorm = 0.2852
Meta loss on this task batch = 3.9413e-01, Meta loss averaged over last 500 steps = 3.7888e-01, PNorm = 85.9327, GNorm = 0.3353
Meta loss on this task batch = 3.7470e-01, Meta loss averaged over last 500 steps = 3.7886e-01, PNorm = 85.9440, GNorm = 0.2700
Meta loss on this task batch = 3.7058e-01, Meta loss averaged over last 500 steps = 3.7891e-01, PNorm = 85.9553, GNorm = 0.2389
Meta loss on this task batch = 3.2983e-01, Meta loss averaged over last 500 steps = 3.7891e-01, PNorm = 85.9667, GNorm = 0.3647
Meta loss on this task batch = 4.1281e-01, Meta loss averaged over last 500 steps = 3.7900e-01, PNorm = 85.9771, GNorm = 0.3306
Meta loss on this task batch = 2.8457e-01, Meta loss averaged over last 500 steps = 3.7879e-01, PNorm = 85.9884, GNorm = 0.1720
Meta loss on this task batch = 4.1196e-01, Meta loss averaged over last 500 steps = 3.7893e-01, PNorm = 85.9996, GNorm = 0.4293
Took 112.9574556350708 seconds to complete one epoch of meta training
Took 120.47359609603882 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474618
Epoch 153
Meta loss on this task batch = 3.2989e-01, Meta loss averaged over last 500 steps = 3.7874e-01, PNorm = 86.0115, GNorm = 0.2336
Meta loss on this task batch = 3.4615e-01, Meta loss averaged over last 500 steps = 3.7868e-01, PNorm = 86.0238, GNorm = 0.2802
Meta loss on this task batch = 3.5866e-01, Meta loss averaged over last 500 steps = 3.7861e-01, PNorm = 86.0365, GNorm = 0.1959
Meta loss on this task batch = 4.1728e-01, Meta loss averaged over last 500 steps = 3.7867e-01, PNorm = 86.0477, GNorm = 0.2639
Meta loss on this task batch = 3.5224e-01, Meta loss averaged over last 500 steps = 3.7849e-01, PNorm = 86.0592, GNorm = 0.2472
Meta loss on this task batch = 3.4037e-01, Meta loss averaged over last 500 steps = 3.7838e-01, PNorm = 86.0715, GNorm = 0.1927
Meta loss on this task batch = 3.7555e-01, Meta loss averaged over last 500 steps = 3.7846e-01, PNorm = 86.0839, GNorm = 0.1933
Meta loss on this task batch = 3.7652e-01, Meta loss averaged over last 500 steps = 3.7837e-01, PNorm = 86.0962, GNorm = 0.3231
Meta loss on this task batch = 3.7100e-01, Meta loss averaged over last 500 steps = 3.7826e-01, PNorm = 86.1081, GNorm = 0.3964
Meta loss on this task batch = 3.6299e-01, Meta loss averaged over last 500 steps = 3.7821e-01, PNorm = 86.1191, GNorm = 0.2992
Meta loss on this task batch = 4.1916e-01, Meta loss averaged over last 500 steps = 3.7827e-01, PNorm = 86.1297, GNorm = 0.2314
Meta loss on this task batch = 3.0614e-01, Meta loss averaged over last 500 steps = 3.7802e-01, PNorm = 86.1403, GNorm = 0.1975
Meta loss on this task batch = 3.5797e-01, Meta loss averaged over last 500 steps = 3.7803e-01, PNorm = 86.1512, GNorm = 0.2570
Meta loss on this task batch = 4.0306e-01, Meta loss averaged over last 500 steps = 3.7793e-01, PNorm = 86.1619, GNorm = 0.3294
Meta loss on this task batch = 3.6188e-01, Meta loss averaged over last 500 steps = 3.7795e-01, PNorm = 86.1733, GNorm = 0.2647
Meta loss on this task batch = 4.0673e-01, Meta loss averaged over last 500 steps = 3.7795e-01, PNorm = 86.1848, GNorm = 0.2102
Meta loss on this task batch = 3.0965e-01, Meta loss averaged over last 500 steps = 3.7793e-01, PNorm = 86.1961, GNorm = 0.3120
Meta loss on this task batch = 3.8053e-01, Meta loss averaged over last 500 steps = 3.7795e-01, PNorm = 86.2066, GNorm = 0.2294
Meta loss on this task batch = 3.8540e-01, Meta loss averaged over last 500 steps = 3.7795e-01, PNorm = 86.2175, GNorm = 0.2977
Took 110.34907078742981 seconds to complete one epoch of meta training
Took 117.16592311859131 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457233
Epoch 154
Meta loss on this task batch = 3.5079e-01, Meta loss averaged over last 500 steps = 3.7793e-01, PNorm = 86.2291, GNorm = 0.2714
Meta loss on this task batch = 3.8394e-01, Meta loss averaged over last 500 steps = 3.7784e-01, PNorm = 86.2407, GNorm = 0.1986
Meta loss on this task batch = 4.1718e-01, Meta loss averaged over last 500 steps = 3.7789e-01, PNorm = 86.2515, GNorm = 0.2363
Meta loss on this task batch = 3.7191e-01, Meta loss averaged over last 500 steps = 3.7807e-01, PNorm = 86.2625, GNorm = 0.2447
Meta loss on this task batch = 3.5876e-01, Meta loss averaged over last 500 steps = 3.7804e-01, PNorm = 86.2745, GNorm = 0.1919
Meta loss on this task batch = 3.8187e-01, Meta loss averaged over last 500 steps = 3.7805e-01, PNorm = 86.2878, GNorm = 0.3346
Meta loss on this task batch = 3.3530e-01, Meta loss averaged over last 500 steps = 3.7781e-01, PNorm = 86.3025, GNorm = 0.3630
Meta loss on this task batch = 4.0982e-01, Meta loss averaged over last 500 steps = 3.7793e-01, PNorm = 86.3179, GNorm = 0.2529
Meta loss on this task batch = 4.1359e-01, Meta loss averaged over last 500 steps = 3.7807e-01, PNorm = 86.3327, GNorm = 0.2318
Meta loss on this task batch = 3.4701e-01, Meta loss averaged over last 500 steps = 3.7792e-01, PNorm = 86.3467, GNorm = 0.1768
Meta loss on this task batch = 3.4386e-01, Meta loss averaged over last 500 steps = 3.7788e-01, PNorm = 86.3604, GNorm = 0.2739
Meta loss on this task batch = 4.1332e-01, Meta loss averaged over last 500 steps = 3.7801e-01, PNorm = 86.3734, GNorm = 0.3055
Meta loss on this task batch = 3.4370e-01, Meta loss averaged over last 500 steps = 3.7796e-01, PNorm = 86.3864, GNorm = 0.3024
Meta loss on this task batch = 3.6537e-01, Meta loss averaged over last 500 steps = 3.7785e-01, PNorm = 86.3986, GNorm = 0.2088
Meta loss on this task batch = 2.7409e-01, Meta loss averaged over last 500 steps = 3.7758e-01, PNorm = 86.4109, GNorm = 0.2329
Meta loss on this task batch = 4.2953e-01, Meta loss averaged over last 500 steps = 3.7771e-01, PNorm = 86.4224, GNorm = 0.3083
Meta loss on this task batch = 4.0083e-01, Meta loss averaged over last 500 steps = 3.7774e-01, PNorm = 86.4334, GNorm = 0.2528
Meta loss on this task batch = 4.0523e-01, Meta loss averaged over last 500 steps = 3.7778e-01, PNorm = 86.4430, GNorm = 0.2524
Meta loss on this task batch = 3.8132e-01, Meta loss averaged over last 500 steps = 3.7775e-01, PNorm = 86.4529, GNorm = 0.2775
Took 111.71132516860962 seconds to complete one epoch of meta training
Took 119.15067386627197 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479580
Epoch 155
Meta loss on this task batch = 3.5459e-01, Meta loss averaged over last 500 steps = 3.7767e-01, PNorm = 86.4624, GNorm = 0.2363
Meta loss on this task batch = 3.5332e-01, Meta loss averaged over last 500 steps = 3.7763e-01, PNorm = 86.4728, GNorm = 0.2901
Meta loss on this task batch = 3.6709e-01, Meta loss averaged over last 500 steps = 3.7766e-01, PNorm = 86.4837, GNorm = 0.2652
Meta loss on this task batch = 3.9578e-01, Meta loss averaged over last 500 steps = 3.7765e-01, PNorm = 86.4935, GNorm = 0.3846
Meta loss on this task batch = 3.4152e-01, Meta loss averaged over last 500 steps = 3.7751e-01, PNorm = 86.5026, GNorm = 0.4059
Meta loss on this task batch = 3.5047e-01, Meta loss averaged over last 500 steps = 3.7748e-01, PNorm = 86.5122, GNorm = 0.3751
Meta loss on this task batch = 3.5117e-01, Meta loss averaged over last 500 steps = 3.7733e-01, PNorm = 86.5228, GNorm = 0.4229
Meta loss on this task batch = 3.6850e-01, Meta loss averaged over last 500 steps = 3.7730e-01, PNorm = 86.5336, GNorm = 0.2626
Meta loss on this task batch = 3.2531e-01, Meta loss averaged over last 500 steps = 3.7719e-01, PNorm = 86.5448, GNorm = 0.3252
Meta loss on this task batch = 3.1726e-01, Meta loss averaged over last 500 steps = 3.7713e-01, PNorm = 86.5558, GNorm = 0.1863
Meta loss on this task batch = 3.4388e-01, Meta loss averaged over last 500 steps = 3.7715e-01, PNorm = 86.5665, GNorm = 0.2932
Meta loss on this task batch = 3.9266e-01, Meta loss averaged over last 500 steps = 3.7715e-01, PNorm = 86.5775, GNorm = 0.2335
Meta loss on this task batch = 3.4635e-01, Meta loss averaged over last 500 steps = 3.7702e-01, PNorm = 86.5889, GNorm = 0.1974
Meta loss on this task batch = 4.4204e-01, Meta loss averaged over last 500 steps = 3.7711e-01, PNorm = 86.6005, GNorm = 0.3183
Meta loss on this task batch = 3.9144e-01, Meta loss averaged over last 500 steps = 3.7709e-01, PNorm = 86.6118, GNorm = 0.2036
Meta loss on this task batch = 3.8773e-01, Meta loss averaged over last 500 steps = 3.7705e-01, PNorm = 86.6223, GNorm = 0.2248
Meta loss on this task batch = 3.8311e-01, Meta loss averaged over last 500 steps = 3.7704e-01, PNorm = 86.6316, GNorm = 0.2917
Meta loss on this task batch = 3.5611e-01, Meta loss averaged over last 500 steps = 3.7715e-01, PNorm = 86.6408, GNorm = 0.2942
Meta loss on this task batch = 3.2842e-01, Meta loss averaged over last 500 steps = 3.7713e-01, PNorm = 86.6505, GNorm = 0.5076
Took 110.18092918395996 seconds to complete one epoch of meta training
Took 117.99928712844849 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483017
Epoch 156
Meta loss on this task batch = 4.2100e-01, Meta loss averaged over last 500 steps = 3.7725e-01, PNorm = 86.6593, GNorm = 0.2336
Meta loss on this task batch = 3.5437e-01, Meta loss averaged over last 500 steps = 3.7721e-01, PNorm = 86.6689, GNorm = 0.2120
Meta loss on this task batch = 3.1623e-01, Meta loss averaged over last 500 steps = 3.7707e-01, PNorm = 86.6789, GNorm = 0.1898
Meta loss on this task batch = 3.3684e-01, Meta loss averaged over last 500 steps = 3.7684e-01, PNorm = 86.6895, GNorm = 0.1853
Meta loss on this task batch = 3.5648e-01, Meta loss averaged over last 500 steps = 3.7672e-01, PNorm = 86.7005, GNorm = 0.1966
Meta loss on this task batch = 4.1446e-01, Meta loss averaged over last 500 steps = 3.7694e-01, PNorm = 86.7099, GNorm = 0.2640
Meta loss on this task batch = 3.6362e-01, Meta loss averaged over last 500 steps = 3.7681e-01, PNorm = 86.7200, GNorm = 0.2015
Meta loss on this task batch = 3.7262e-01, Meta loss averaged over last 500 steps = 3.7665e-01, PNorm = 86.7297, GNorm = 0.2058
Meta loss on this task batch = 3.7261e-01, Meta loss averaged over last 500 steps = 3.7665e-01, PNorm = 86.7405, GNorm = 0.4534
Meta loss on this task batch = 3.8142e-01, Meta loss averaged over last 500 steps = 3.7671e-01, PNorm = 86.7515, GNorm = 0.2845
Meta loss on this task batch = 3.9552e-01, Meta loss averaged over last 500 steps = 3.7674e-01, PNorm = 86.7631, GNorm = 0.2301
Meta loss on this task batch = 4.0339e-01, Meta loss averaged over last 500 steps = 3.7681e-01, PNorm = 86.7748, GNorm = 0.2503
Meta loss on this task batch = 3.2498e-01, Meta loss averaged over last 500 steps = 3.7671e-01, PNorm = 86.7869, GNorm = 0.2228
Meta loss on this task batch = 4.0463e-01, Meta loss averaged over last 500 steps = 3.7684e-01, PNorm = 86.7986, GNorm = 0.1986
Meta loss on this task batch = 3.1430e-01, Meta loss averaged over last 500 steps = 3.7669e-01, PNorm = 86.8108, GNorm = 0.2159
Meta loss on this task batch = 3.4830e-01, Meta loss averaged over last 500 steps = 3.7661e-01, PNorm = 86.8238, GNorm = 0.2755
Meta loss on this task batch = 3.9579e-01, Meta loss averaged over last 500 steps = 3.7663e-01, PNorm = 86.8367, GNorm = 0.2330
Meta loss on this task batch = 4.0288e-01, Meta loss averaged over last 500 steps = 3.7668e-01, PNorm = 86.8483, GNorm = 0.2425
Meta loss on this task batch = 3.4425e-01, Meta loss averaged over last 500 steps = 3.7650e-01, PNorm = 86.8605, GNorm = 0.2458
Took 111.33918356895447 seconds to complete one epoch of meta training
Took 118.0421450138092 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476520
Epoch 157
Meta loss on this task batch = 3.3021e-01, Meta loss averaged over last 500 steps = 3.7630e-01, PNorm = 86.8725, GNorm = 0.2907
Meta loss on this task batch = 3.2855e-01, Meta loss averaged over last 500 steps = 3.7627e-01, PNorm = 86.8855, GNorm = 0.2873
Meta loss on this task batch = 4.4999e-01, Meta loss averaged over last 500 steps = 3.7638e-01, PNorm = 86.8961, GNorm = 0.5265
Meta loss on this task batch = 3.7890e-01, Meta loss averaged over last 500 steps = 3.7643e-01, PNorm = 86.9061, GNorm = 0.3082
Meta loss on this task batch = 4.0720e-01, Meta loss averaged over last 500 steps = 3.7637e-01, PNorm = 86.9151, GNorm = 0.3358
Meta loss on this task batch = 3.6015e-01, Meta loss averaged over last 500 steps = 3.7630e-01, PNorm = 86.9249, GNorm = 0.2997
Meta loss on this task batch = 4.1922e-01, Meta loss averaged over last 500 steps = 3.7636e-01, PNorm = 86.9337, GNorm = 0.2628
Meta loss on this task batch = 3.6752e-01, Meta loss averaged over last 500 steps = 3.7640e-01, PNorm = 86.9434, GNorm = 0.4700
Meta loss on this task batch = 3.0016e-01, Meta loss averaged over last 500 steps = 3.7625e-01, PNorm = 86.9538, GNorm = 0.2684
Meta loss on this task batch = 3.4351e-01, Meta loss averaged over last 500 steps = 3.7616e-01, PNorm = 86.9645, GNorm = 0.3311
Meta loss on this task batch = 4.0899e-01, Meta loss averaged over last 500 steps = 3.7616e-01, PNorm = 86.9748, GNorm = 0.2642
Meta loss on this task batch = 3.3536e-01, Meta loss averaged over last 500 steps = 3.7606e-01, PNorm = 86.9860, GNorm = 0.3583
Meta loss on this task batch = 3.1862e-01, Meta loss averaged over last 500 steps = 3.7599e-01, PNorm = 86.9971, GNorm = 0.2497
Meta loss on this task batch = 3.4437e-01, Meta loss averaged over last 500 steps = 3.7603e-01, PNorm = 87.0091, GNorm = 0.2244
Meta loss on this task batch = 3.8895e-01, Meta loss averaged over last 500 steps = 3.7608e-01, PNorm = 87.0223, GNorm = 0.2079
Meta loss on this task batch = 4.0258e-01, Meta loss averaged over last 500 steps = 3.7610e-01, PNorm = 87.0346, GNorm = 0.2861
Meta loss on this task batch = 3.4291e-01, Meta loss averaged over last 500 steps = 3.7595e-01, PNorm = 87.0467, GNorm = 0.4098
Meta loss on this task batch = 3.6493e-01, Meta loss averaged over last 500 steps = 3.7597e-01, PNorm = 87.0592, GNorm = 0.2201
Meta loss on this task batch = 3.6034e-01, Meta loss averaged over last 500 steps = 3.7595e-01, PNorm = 87.0717, GNorm = 0.3624
Took 114.78400468826294 seconds to complete one epoch of meta training
Took 122.3506338596344 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456630
Epoch 158
Meta loss on this task batch = 3.9949e-01, Meta loss averaged over last 500 steps = 3.7593e-01, PNorm = 87.0831, GNorm = 0.2210
Meta loss on this task batch = 3.9750e-01, Meta loss averaged over last 500 steps = 3.7580e-01, PNorm = 87.0954, GNorm = 0.2919
Meta loss on this task batch = 3.6407e-01, Meta loss averaged over last 500 steps = 3.7578e-01, PNorm = 87.1072, GNorm = 0.2213
Meta loss on this task batch = 3.4322e-01, Meta loss averaged over last 500 steps = 3.7571e-01, PNorm = 87.1179, GNorm = 0.2529
Meta loss on this task batch = 3.9091e-01, Meta loss averaged over last 500 steps = 3.7561e-01, PNorm = 87.1260, GNorm = 0.3092
Meta loss on this task batch = 3.4108e-01, Meta loss averaged over last 500 steps = 3.7550e-01, PNorm = 87.1346, GNorm = 0.2770
Meta loss on this task batch = 3.8196e-01, Meta loss averaged over last 500 steps = 3.7555e-01, PNorm = 87.1433, GNorm = 0.2257
Meta loss on this task batch = 3.4002e-01, Meta loss averaged over last 500 steps = 3.7544e-01, PNorm = 87.1526, GNorm = 0.2097
Meta loss on this task batch = 4.3263e-01, Meta loss averaged over last 500 steps = 3.7556e-01, PNorm = 87.1618, GNorm = 0.2363
Meta loss on this task batch = 4.0078e-01, Meta loss averaged over last 500 steps = 3.7562e-01, PNorm = 87.1726, GNorm = 0.2845
Meta loss on this task batch = 3.5779e-01, Meta loss averaged over last 500 steps = 3.7557e-01, PNorm = 87.1841, GNorm = 0.2386
Meta loss on this task batch = 3.2892e-01, Meta loss averaged over last 500 steps = 3.7537e-01, PNorm = 87.1964, GNorm = 0.2307
Meta loss on this task batch = 3.8987e-01, Meta loss averaged over last 500 steps = 3.7541e-01, PNorm = 87.2094, GNorm = 0.2286
Meta loss on this task batch = 3.3300e-01, Meta loss averaged over last 500 steps = 3.7524e-01, PNorm = 87.2223, GNorm = 0.1922
Meta loss on this task batch = 3.8981e-01, Meta loss averaged over last 500 steps = 3.7512e-01, PNorm = 87.2346, GNorm = 0.3393
Meta loss on this task batch = 3.6479e-01, Meta loss averaged over last 500 steps = 3.7518e-01, PNorm = 87.2466, GNorm = 0.2071
Meta loss on this task batch = 3.9713e-01, Meta loss averaged over last 500 steps = 3.7525e-01, PNorm = 87.2579, GNorm = 0.2454
Meta loss on this task batch = 3.8049e-01, Meta loss averaged over last 500 steps = 3.7517e-01, PNorm = 87.2683, GNorm = 0.2658
Meta loss on this task batch = 3.6663e-01, Meta loss averaged over last 500 steps = 3.7509e-01, PNorm = 87.2785, GNorm = 0.2420
Took 110.88986945152283 seconds to complete one epoch of meta training
Took 118.4951753616333 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448974
Epoch 159
Meta loss on this task batch = 3.3122e-01, Meta loss averaged over last 500 steps = 3.7502e-01, PNorm = 87.2882, GNorm = 0.2547
Meta loss on this task batch = 3.9465e-01, Meta loss averaged over last 500 steps = 3.7505e-01, PNorm = 87.2974, GNorm = 0.2698
Meta loss on this task batch = 3.6801e-01, Meta loss averaged over last 500 steps = 3.7507e-01, PNorm = 87.3077, GNorm = 0.2718
Meta loss on this task batch = 3.1302e-01, Meta loss averaged over last 500 steps = 3.7481e-01, PNorm = 87.3187, GNorm = 0.1779
Meta loss on this task batch = 3.7274e-01, Meta loss averaged over last 500 steps = 3.7482e-01, PNorm = 87.3300, GNorm = 0.2460
Meta loss on this task batch = 4.2145e-01, Meta loss averaged over last 500 steps = 3.7489e-01, PNorm = 87.3400, GNorm = 0.2407
Meta loss on this task batch = 3.3436e-01, Meta loss averaged over last 500 steps = 3.7472e-01, PNorm = 87.3509, GNorm = 0.3483
Meta loss on this task batch = 3.6703e-01, Meta loss averaged over last 500 steps = 3.7450e-01, PNorm = 87.3609, GNorm = 0.2338
Meta loss on this task batch = 3.4951e-01, Meta loss averaged over last 500 steps = 3.7443e-01, PNorm = 87.3711, GNorm = 0.2795
Meta loss on this task batch = 3.7061e-01, Meta loss averaged over last 500 steps = 3.7435e-01, PNorm = 87.3815, GNorm = 0.2596
Meta loss on this task batch = 3.9619e-01, Meta loss averaged over last 500 steps = 3.7441e-01, PNorm = 87.3911, GNorm = 0.3428
Meta loss on this task batch = 4.0674e-01, Meta loss averaged over last 500 steps = 3.7433e-01, PNorm = 87.4001, GNorm = 0.2615
Meta loss on this task batch = 4.0658e-01, Meta loss averaged over last 500 steps = 3.7434e-01, PNorm = 87.4093, GNorm = 0.2389
Meta loss on this task batch = 3.3087e-01, Meta loss averaged over last 500 steps = 3.7425e-01, PNorm = 87.4188, GNorm = 0.2222
Meta loss on this task batch = 4.0052e-01, Meta loss averaged over last 500 steps = 3.7436e-01, PNorm = 87.4285, GNorm = 0.2553
Meta loss on this task batch = 3.2597e-01, Meta loss averaged over last 500 steps = 3.7433e-01, PNorm = 87.4386, GNorm = 0.1747
Meta loss on this task batch = 3.3784e-01, Meta loss averaged over last 500 steps = 3.7427e-01, PNorm = 87.4496, GNorm = 0.2778
Meta loss on this task batch = 3.8767e-01, Meta loss averaged over last 500 steps = 3.7429e-01, PNorm = 87.4603, GNorm = 0.2327
Meta loss on this task batch = 3.3256e-01, Meta loss averaged over last 500 steps = 3.7415e-01, PNorm = 87.4713, GNorm = 0.2704
Took 111.6707923412323 seconds to complete one epoch of meta training
Took 119.6342601776123 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478801
Epoch 160
Meta loss on this task batch = 4.0145e-01, Meta loss averaged over last 500 steps = 3.7432e-01, PNorm = 87.4827, GNorm = 0.2675
Meta loss on this task batch = 3.5529e-01, Meta loss averaged over last 500 steps = 3.7429e-01, PNorm = 87.4947, GNorm = 0.2228
Meta loss on this task batch = 3.4489e-01, Meta loss averaged over last 500 steps = 3.7424e-01, PNorm = 87.5065, GNorm = 0.2659
Meta loss on this task batch = 3.7328e-01, Meta loss averaged over last 500 steps = 3.7426e-01, PNorm = 87.5187, GNorm = 0.2502
Meta loss on this task batch = 3.7447e-01, Meta loss averaged over last 500 steps = 3.7424e-01, PNorm = 87.5304, GNorm = 0.3054
Meta loss on this task batch = 3.4918e-01, Meta loss averaged over last 500 steps = 3.7416e-01, PNorm = 87.5411, GNorm = 0.3619
Meta loss on this task batch = 2.8747e-01, Meta loss averaged over last 500 steps = 3.7391e-01, PNorm = 87.5518, GNorm = 0.2514
Meta loss on this task batch = 3.3780e-01, Meta loss averaged over last 500 steps = 3.7401e-01, PNorm = 87.5626, GNorm = 0.3418
Meta loss on this task batch = 3.1146e-01, Meta loss averaged over last 500 steps = 3.7385e-01, PNorm = 87.5734, GNorm = 0.3049
Meta loss on this task batch = 3.9892e-01, Meta loss averaged over last 500 steps = 3.7377e-01, PNorm = 87.5828, GNorm = 0.2633
Meta loss on this task batch = 4.3933e-01, Meta loss averaged over last 500 steps = 3.7398e-01, PNorm = 87.5920, GNorm = 0.2835
Meta loss on this task batch = 3.6139e-01, Meta loss averaged over last 500 steps = 3.7399e-01, PNorm = 87.6022, GNorm = 0.2290
Meta loss on this task batch = 3.7666e-01, Meta loss averaged over last 500 steps = 3.7405e-01, PNorm = 87.6115, GNorm = 0.2153
Meta loss on this task batch = 3.8110e-01, Meta loss averaged over last 500 steps = 3.7400e-01, PNorm = 87.6201, GNorm = 0.2344
Meta loss on this task batch = 3.5679e-01, Meta loss averaged over last 500 steps = 3.7395e-01, PNorm = 87.6287, GNorm = 0.2750
Meta loss on this task batch = 3.6375e-01, Meta loss averaged over last 500 steps = 3.7398e-01, PNorm = 87.6378, GNorm = 0.2190
Meta loss on this task batch = 3.9761e-01, Meta loss averaged over last 500 steps = 3.7400e-01, PNorm = 87.6476, GNorm = 0.2240
Meta loss on this task batch = 3.3849e-01, Meta loss averaged over last 500 steps = 3.7396e-01, PNorm = 87.6583, GNorm = 0.2615
Meta loss on this task batch = 3.5495e-01, Meta loss averaged over last 500 steps = 3.7389e-01, PNorm = 87.6703, GNorm = 0.2688
Took 112.9970633983612 seconds to complete one epoch of meta training
Took 120.70945715904236 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472325
Epoch 161
Meta loss on this task batch = 3.5577e-01, Meta loss averaged over last 500 steps = 3.7372e-01, PNorm = 87.6824, GNorm = 0.2542
Meta loss on this task batch = 3.4301e-01, Meta loss averaged over last 500 steps = 3.7366e-01, PNorm = 87.6949, GNorm = 0.2161
Meta loss on this task batch = 3.7688e-01, Meta loss averaged over last 500 steps = 3.7366e-01, PNorm = 87.7081, GNorm = 0.2189
Meta loss on this task batch = 3.6516e-01, Meta loss averaged over last 500 steps = 3.7348e-01, PNorm = 87.7207, GNorm = 0.2557
Meta loss on this task batch = 2.8837e-01, Meta loss averaged over last 500 steps = 3.7326e-01, PNorm = 87.7341, GNorm = 0.2097
Meta loss on this task batch = 3.9124e-01, Meta loss averaged over last 500 steps = 3.7336e-01, PNorm = 87.7462, GNorm = 0.3248
Meta loss on this task batch = 3.8661e-01, Meta loss averaged over last 500 steps = 3.7336e-01, PNorm = 87.7582, GNorm = 0.3026
Meta loss on this task batch = 3.6753e-01, Meta loss averaged over last 500 steps = 3.7336e-01, PNorm = 87.7696, GNorm = 0.3268
Meta loss on this task batch = 3.9004e-01, Meta loss averaged over last 500 steps = 3.7323e-01, PNorm = 87.7801, GNorm = 0.2383
Meta loss on this task batch = 3.4636e-01, Meta loss averaged over last 500 steps = 3.7313e-01, PNorm = 87.7910, GNorm = 0.2340
Meta loss on this task batch = 3.8205e-01, Meta loss averaged over last 500 steps = 3.7319e-01, PNorm = 87.8012, GNorm = 0.2760
Meta loss on this task batch = 3.7114e-01, Meta loss averaged over last 500 steps = 3.7304e-01, PNorm = 87.8101, GNorm = 0.2355
Meta loss on this task batch = 3.6847e-01, Meta loss averaged over last 500 steps = 3.7292e-01, PNorm = 87.8192, GNorm = 0.3482
Meta loss on this task batch = 4.0350e-01, Meta loss averaged over last 500 steps = 3.7305e-01, PNorm = 87.8289, GNorm = 0.4167
Meta loss on this task batch = 3.5494e-01, Meta loss averaged over last 500 steps = 3.7314e-01, PNorm = 87.8380, GNorm = 0.2606
Meta loss on this task batch = 3.8442e-01, Meta loss averaged over last 500 steps = 3.7322e-01, PNorm = 87.8474, GNorm = 0.2754
Meta loss on this task batch = 3.4350e-01, Meta loss averaged over last 500 steps = 3.7316e-01, PNorm = 87.8568, GNorm = 0.2779
Meta loss on this task batch = 4.3840e-01, Meta loss averaged over last 500 steps = 3.7341e-01, PNorm = 87.8657, GNorm = 0.3847
Meta loss on this task batch = 3.8192e-01, Meta loss averaged over last 500 steps = 3.7346e-01, PNorm = 87.8752, GNorm = 0.2719
Took 113.97935009002686 seconds to complete one epoch of meta training
Took 121.6446692943573 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484395
Epoch 162
Meta loss on this task batch = 4.1524e-01, Meta loss averaged over last 500 steps = 3.7348e-01, PNorm = 87.8851, GNorm = 0.2613
Meta loss on this task batch = 3.7965e-01, Meta loss averaged over last 500 steps = 3.7348e-01, PNorm = 87.8951, GNorm = 0.2465
Meta loss on this task batch = 3.5599e-01, Meta loss averaged over last 500 steps = 3.7348e-01, PNorm = 87.9050, GNorm = 0.2301
Meta loss on this task batch = 4.0681e-01, Meta loss averaged over last 500 steps = 3.7356e-01, PNorm = 87.9146, GNorm = 0.2403
Meta loss on this task batch = 3.0572e-01, Meta loss averaged over last 500 steps = 3.7341e-01, PNorm = 87.9252, GNorm = 0.1904
Meta loss on this task batch = 4.0951e-01, Meta loss averaged over last 500 steps = 3.7355e-01, PNorm = 87.9356, GNorm = 0.2679
Meta loss on this task batch = 4.2289e-01, Meta loss averaged over last 500 steps = 3.7358e-01, PNorm = 87.9459, GNorm = 0.3097
Meta loss on this task batch = 3.7309e-01, Meta loss averaged over last 500 steps = 3.7369e-01, PNorm = 87.9569, GNorm = 0.2731
Meta loss on this task batch = 3.3588e-01, Meta loss averaged over last 500 steps = 3.7348e-01, PNorm = 87.9676, GNorm = 0.3651
Meta loss on this task batch = 3.6048e-01, Meta loss averaged over last 500 steps = 3.7351e-01, PNorm = 87.9781, GNorm = 0.2303
Meta loss on this task batch = 3.9480e-01, Meta loss averaged over last 500 steps = 3.7355e-01, PNorm = 87.9883, GNorm = 0.3262
Meta loss on this task batch = 3.0809e-01, Meta loss averaged over last 500 steps = 3.7346e-01, PNorm = 87.9987, GNorm = 0.1838
Meta loss on this task batch = 3.9885e-01, Meta loss averaged over last 500 steps = 3.7360e-01, PNorm = 88.0080, GNorm = 0.3911
Meta loss on this task batch = 3.1418e-01, Meta loss averaged over last 500 steps = 3.7352e-01, PNorm = 88.0184, GNorm = 0.1999
Meta loss on this task batch = 3.5466e-01, Meta loss averaged over last 500 steps = 3.7337e-01, PNorm = 88.0287, GNorm = 0.2119
Meta loss on this task batch = 3.9152e-01, Meta loss averaged over last 500 steps = 3.7338e-01, PNorm = 88.0398, GNorm = 0.2860
Meta loss on this task batch = 3.4820e-01, Meta loss averaged over last 500 steps = 3.7328e-01, PNorm = 88.0501, GNorm = 0.2509
Meta loss on this task batch = 3.8388e-01, Meta loss averaged over last 500 steps = 3.7331e-01, PNorm = 88.0615, GNorm = 0.3747
Meta loss on this task batch = 3.7548e-01, Meta loss averaged over last 500 steps = 3.7327e-01, PNorm = 88.0733, GNorm = 0.2545
Took 113.2634699344635 seconds to complete one epoch of meta training
Took 121.39860653877258 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450361
Epoch 163
Meta loss on this task batch = 2.9253e-01, Meta loss averaged over last 500 steps = 3.7319e-01, PNorm = 88.0854, GNorm = 0.1764
Meta loss on this task batch = 3.3411e-01, Meta loss averaged over last 500 steps = 3.7314e-01, PNorm = 88.0984, GNorm = 0.2641
Meta loss on this task batch = 3.7922e-01, Meta loss averaged over last 500 steps = 3.7315e-01, PNorm = 88.1111, GNorm = 0.2182
Meta loss on this task batch = 4.2819e-01, Meta loss averaged over last 500 steps = 3.7331e-01, PNorm = 88.1230, GNorm = 0.2349
Meta loss on this task batch = 3.7182e-01, Meta loss averaged over last 500 steps = 3.7332e-01, PNorm = 88.1352, GNorm = 0.2750
Meta loss on this task batch = 3.5081e-01, Meta loss averaged over last 500 steps = 3.7323e-01, PNorm = 88.1471, GNorm = 0.2177
Meta loss on this task batch = 3.8211e-01, Meta loss averaged over last 500 steps = 3.7326e-01, PNorm = 88.1585, GNorm = 0.2160
Meta loss on this task batch = 2.8890e-01, Meta loss averaged over last 500 steps = 3.7306e-01, PNorm = 88.1705, GNorm = 0.2784
Meta loss on this task batch = 4.1182e-01, Meta loss averaged over last 500 steps = 3.7314e-01, PNorm = 88.1814, GNorm = 0.3545
Meta loss on this task batch = 3.1734e-01, Meta loss averaged over last 500 steps = 3.7287e-01, PNorm = 88.1924, GNorm = 0.2350
Meta loss on this task batch = 3.5938e-01, Meta loss averaged over last 500 steps = 3.7289e-01, PNorm = 88.2035, GNorm = 0.3753
Meta loss on this task batch = 3.9154e-01, Meta loss averaged over last 500 steps = 3.7293e-01, PNorm = 88.2144, GNorm = 0.3276
Meta loss on this task batch = 3.3655e-01, Meta loss averaged over last 500 steps = 3.7289e-01, PNorm = 88.2242, GNorm = 0.3462
Meta loss on this task batch = 3.6170e-01, Meta loss averaged over last 500 steps = 3.7286e-01, PNorm = 88.2342, GNorm = 0.2619
Meta loss on this task batch = 3.6929e-01, Meta loss averaged over last 500 steps = 3.7282e-01, PNorm = 88.2449, GNorm = 0.2063
Meta loss on this task batch = 3.5840e-01, Meta loss averaged over last 500 steps = 3.7286e-01, PNorm = 88.2551, GNorm = 0.2417
Meta loss on this task batch = 4.2204e-01, Meta loss averaged over last 500 steps = 3.7302e-01, PNorm = 88.2649, GNorm = 0.2094
Meta loss on this task batch = 3.6759e-01, Meta loss averaged over last 500 steps = 3.7309e-01, PNorm = 88.2757, GNorm = 0.1855
Meta loss on this task batch = 4.0540e-01, Meta loss averaged over last 500 steps = 3.7312e-01, PNorm = 88.2865, GNorm = 0.4309
Took 114.98791885375977 seconds to complete one epoch of meta training
Took 122.51871991157532 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462457
Epoch 164
Meta loss on this task batch = 3.8101e-01, Meta loss averaged over last 500 steps = 3.7298e-01, PNorm = 88.2970, GNorm = 0.2840
Meta loss on this task batch = 3.9735e-01, Meta loss averaged over last 500 steps = 3.7299e-01, PNorm = 88.3071, GNorm = 0.3117
Meta loss on this task batch = 3.0052e-01, Meta loss averaged over last 500 steps = 3.7281e-01, PNorm = 88.3182, GNorm = 0.3638
Meta loss on this task batch = 3.5659e-01, Meta loss averaged over last 500 steps = 3.7272e-01, PNorm = 88.3291, GNorm = 0.2015
Meta loss on this task batch = 3.5978e-01, Meta loss averaged over last 500 steps = 3.7263e-01, PNorm = 88.3403, GNorm = 0.2134
Meta loss on this task batch = 3.2927e-01, Meta loss averaged over last 500 steps = 3.7248e-01, PNorm = 88.3518, GNorm = 0.1966
Meta loss on this task batch = 3.0230e-01, Meta loss averaged over last 500 steps = 3.7227e-01, PNorm = 88.3636, GNorm = 0.2825
Meta loss on this task batch = 3.5109e-01, Meta loss averaged over last 500 steps = 3.7221e-01, PNorm = 88.3752, GNorm = 0.3004
Meta loss on this task batch = 4.1075e-01, Meta loss averaged over last 500 steps = 3.7220e-01, PNorm = 88.3868, GNorm = 0.2599
Meta loss on this task batch = 3.9061e-01, Meta loss averaged over last 500 steps = 3.7217e-01, PNorm = 88.3983, GNorm = 0.2182
Meta loss on this task batch = 3.6678e-01, Meta loss averaged over last 500 steps = 3.7208e-01, PNorm = 88.4091, GNorm = 0.2904
Meta loss on this task batch = 3.9505e-01, Meta loss averaged over last 500 steps = 3.7214e-01, PNorm = 88.4201, GNorm = 0.2734
Meta loss on this task batch = 3.1676e-01, Meta loss averaged over last 500 steps = 3.7199e-01, PNorm = 88.4307, GNorm = 0.2572
Meta loss on this task batch = 3.0712e-01, Meta loss averaged over last 500 steps = 3.7188e-01, PNorm = 88.4424, GNorm = 0.2141
Meta loss on this task batch = 3.8312e-01, Meta loss averaged over last 500 steps = 3.7193e-01, PNorm = 88.4527, GNorm = 0.3035
Meta loss on this task batch = 3.5319e-01, Meta loss averaged over last 500 steps = 3.7188e-01, PNorm = 88.4631, GNorm = 0.2312
Meta loss on this task batch = 4.2901e-01, Meta loss averaged over last 500 steps = 3.7202e-01, PNorm = 88.4715, GNorm = 0.4056
Meta loss on this task batch = 3.6733e-01, Meta loss averaged over last 500 steps = 3.7205e-01, PNorm = 88.4800, GNorm = 0.3637
Meta loss on this task batch = 4.4534e-01, Meta loss averaged over last 500 steps = 3.7213e-01, PNorm = 88.4869, GNorm = 0.3410
Took 110.10154128074646 seconds to complete one epoch of meta training
Took 117.84115600585938 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465304
Epoch 165
Meta loss on this task batch = 3.2619e-01, Meta loss averaged over last 500 steps = 3.7207e-01, PNorm = 88.4952, GNorm = 0.3174
Meta loss on this task batch = 4.0020e-01, Meta loss averaged over last 500 steps = 3.7221e-01, PNorm = 88.5039, GNorm = 0.2502
Meta loss on this task batch = 3.7580e-01, Meta loss averaged over last 500 steps = 3.7216e-01, PNorm = 88.5135, GNorm = 0.2373
Meta loss on this task batch = 3.6809e-01, Meta loss averaged over last 500 steps = 3.7212e-01, PNorm = 88.5235, GNorm = 0.2493
Meta loss on this task batch = 3.3708e-01, Meta loss averaged over last 500 steps = 3.7208e-01, PNorm = 88.5337, GNorm = 0.3097
Meta loss on this task batch = 3.9511e-01, Meta loss averaged over last 500 steps = 3.7220e-01, PNorm = 88.5445, GNorm = 0.3413
Meta loss on this task batch = 3.3845e-01, Meta loss averaged over last 500 steps = 3.7216e-01, PNorm = 88.5563, GNorm = 0.3197
Meta loss on this task batch = 3.4711e-01, Meta loss averaged over last 500 steps = 3.7216e-01, PNorm = 88.5686, GNorm = 0.2383
Meta loss on this task batch = 3.7471e-01, Meta loss averaged over last 500 steps = 3.7209e-01, PNorm = 88.5810, GNorm = 0.2117
Meta loss on this task batch = 3.5409e-01, Meta loss averaged over last 500 steps = 3.7206e-01, PNorm = 88.5936, GNorm = 0.2440
Meta loss on this task batch = 3.5238e-01, Meta loss averaged over last 500 steps = 3.7197e-01, PNorm = 88.6067, GNorm = 0.4036
Meta loss on this task batch = 3.6991e-01, Meta loss averaged over last 500 steps = 3.7197e-01, PNorm = 88.6201, GNorm = 0.2857
Meta loss on this task batch = 3.6641e-01, Meta loss averaged over last 500 steps = 3.7209e-01, PNorm = 88.6324, GNorm = 0.3201
Meta loss on this task batch = 3.7610e-01, Meta loss averaged over last 500 steps = 3.7203e-01, PNorm = 88.6447, GNorm = 0.2345
Meta loss on this task batch = 3.6031e-01, Meta loss averaged over last 500 steps = 3.7191e-01, PNorm = 88.6558, GNorm = 0.3145
Meta loss on this task batch = 3.6178e-01, Meta loss averaged over last 500 steps = 3.7183e-01, PNorm = 88.6666, GNorm = 0.2757
Meta loss on this task batch = 3.6650e-01, Meta loss averaged over last 500 steps = 3.7182e-01, PNorm = 88.6774, GNorm = 0.1991
Meta loss on this task batch = 3.2040e-01, Meta loss averaged over last 500 steps = 3.7173e-01, PNorm = 88.6881, GNorm = 0.1952
Meta loss on this task batch = 4.1246e-01, Meta loss averaged over last 500 steps = 3.7184e-01, PNorm = 88.6977, GNorm = 0.2791
Took 112.29311919212341 seconds to complete one epoch of meta training
Took 120.13943457603455 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469050
Epoch 166
Meta loss on this task batch = 3.5460e-01, Meta loss averaged over last 500 steps = 3.7179e-01, PNorm = 88.7074, GNorm = 0.1836
Meta loss on this task batch = 3.7679e-01, Meta loss averaged over last 500 steps = 3.7179e-01, PNorm = 88.7170, GNorm = 0.2823
Meta loss on this task batch = 3.9322e-01, Meta loss averaged over last 500 steps = 3.7161e-01, PNorm = 88.7266, GNorm = 0.2213
Meta loss on this task batch = 3.9198e-01, Meta loss averaged over last 500 steps = 3.7164e-01, PNorm = 88.7356, GNorm = 0.2061
Meta loss on this task batch = 3.8994e-01, Meta loss averaged over last 500 steps = 3.7171e-01, PNorm = 88.7448, GNorm = 0.2317
Meta loss on this task batch = 3.7924e-01, Meta loss averaged over last 500 steps = 3.7177e-01, PNorm = 88.7544, GNorm = 0.2386
Meta loss on this task batch = 3.3910e-01, Meta loss averaged over last 500 steps = 3.7181e-01, PNorm = 88.7633, GNorm = 0.2067
Meta loss on this task batch = 3.2917e-01, Meta loss averaged over last 500 steps = 3.7164e-01, PNorm = 88.7727, GNorm = 0.1971
Meta loss on this task batch = 3.3410e-01, Meta loss averaged over last 500 steps = 3.7161e-01, PNorm = 88.7832, GNorm = 0.3508
Meta loss on this task batch = 3.1786e-01, Meta loss averaged over last 500 steps = 3.7157e-01, PNorm = 88.7946, GNorm = 0.2838
Meta loss on this task batch = 3.7527e-01, Meta loss averaged over last 500 steps = 3.7171e-01, PNorm = 88.8063, GNorm = 0.2237
Meta loss on this task batch = 3.9476e-01, Meta loss averaged over last 500 steps = 3.7173e-01, PNorm = 88.8172, GNorm = 0.2157
Meta loss on this task batch = 3.7168e-01, Meta loss averaged over last 500 steps = 3.7171e-01, PNorm = 88.8284, GNorm = 0.2305
Meta loss on this task batch = 3.3284e-01, Meta loss averaged over last 500 steps = 3.7152e-01, PNorm = 88.8399, GNorm = 0.2675
Meta loss on this task batch = 3.3918e-01, Meta loss averaged over last 500 steps = 3.7130e-01, PNorm = 88.8512, GNorm = 0.1908
Meta loss on this task batch = 3.3737e-01, Meta loss averaged over last 500 steps = 3.7118e-01, PNorm = 88.8630, GNorm = 0.2425
Meta loss on this task batch = 3.7549e-01, Meta loss averaged over last 500 steps = 3.7118e-01, PNorm = 88.8743, GNorm = 0.2913
Meta loss on this task batch = 4.6085e-01, Meta loss averaged over last 500 steps = 3.7134e-01, PNorm = 88.8835, GNorm = 0.4199
Meta loss on this task batch = 3.1706e-01, Meta loss averaged over last 500 steps = 3.7128e-01, PNorm = 88.8935, GNorm = 0.3427
Took 113.04629015922546 seconds to complete one epoch of meta training
Took 120.8936653137207 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480356
Epoch 167
Meta loss on this task batch = 3.5084e-01, Meta loss averaged over last 500 steps = 3.7119e-01, PNorm = 88.9034, GNorm = 0.2177
Meta loss on this task batch = 3.7997e-01, Meta loss averaged over last 500 steps = 3.7127e-01, PNorm = 88.9131, GNorm = 0.3342
Meta loss on this task batch = 4.2740e-01, Meta loss averaged over last 500 steps = 3.7131e-01, PNorm = 88.9225, GNorm = 0.2492
Meta loss on this task batch = 3.6675e-01, Meta loss averaged over last 500 steps = 3.7123e-01, PNorm = 88.9310, GNorm = 0.2356
Meta loss on this task batch = 3.5588e-01, Meta loss averaged over last 500 steps = 3.7125e-01, PNorm = 88.9397, GNorm = 0.2196
Meta loss on this task batch = 4.1051e-01, Meta loss averaged over last 500 steps = 3.7118e-01, PNorm = 88.9472, GNorm = 0.2364
Meta loss on this task batch = 2.5766e-01, Meta loss averaged over last 500 steps = 3.7083e-01, PNorm = 88.9559, GNorm = 0.2308
Meta loss on this task batch = 3.3777e-01, Meta loss averaged over last 500 steps = 3.7068e-01, PNorm = 88.9643, GNorm = 0.2235
Meta loss on this task batch = 3.7256e-01, Meta loss averaged over last 500 steps = 3.7069e-01, PNorm = 88.9729, GNorm = 0.2043
Meta loss on this task batch = 3.8355e-01, Meta loss averaged over last 500 steps = 3.7068e-01, PNorm = 88.9816, GNorm = 0.3244
Meta loss on this task batch = 3.4977e-01, Meta loss averaged over last 500 steps = 3.7076e-01, PNorm = 88.9902, GNorm = 0.2049
Meta loss on this task batch = 3.0888e-01, Meta loss averaged over last 500 steps = 3.7061e-01, PNorm = 89.0004, GNorm = 0.3497
Meta loss on this task batch = 3.6965e-01, Meta loss averaged over last 500 steps = 3.7065e-01, PNorm = 89.0104, GNorm = 0.2956
Meta loss on this task batch = 4.1194e-01, Meta loss averaged over last 500 steps = 3.7066e-01, PNorm = 89.0204, GNorm = 0.4526
Meta loss on this task batch = 3.6240e-01, Meta loss averaged over last 500 steps = 3.7072e-01, PNorm = 89.0315, GNorm = 0.2101
Meta loss on this task batch = 3.8701e-01, Meta loss averaged over last 500 steps = 3.7069e-01, PNorm = 89.0417, GNorm = 0.2303
Meta loss on this task batch = 3.6313e-01, Meta loss averaged over last 500 steps = 3.7061e-01, PNorm = 89.0523, GNorm = 0.2841
Meta loss on this task batch = 3.8029e-01, Meta loss averaged over last 500 steps = 3.7058e-01, PNorm = 89.0626, GNorm = 0.2857
Meta loss on this task batch = 3.7253e-01, Meta loss averaged over last 500 steps = 3.7062e-01, PNorm = 89.0732, GNorm = 0.4105
Took 111.97933292388916 seconds to complete one epoch of meta training
Took 119.98603463172913 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474675
Epoch 168
Meta loss on this task batch = 3.9738e-01, Meta loss averaged over last 500 steps = 3.7060e-01, PNorm = 89.0838, GNorm = 0.2472
Meta loss on this task batch = 3.2812e-01, Meta loss averaged over last 500 steps = 3.7055e-01, PNorm = 89.0941, GNorm = 0.2223
Meta loss on this task batch = 3.8942e-01, Meta loss averaged over last 500 steps = 3.7061e-01, PNorm = 89.1040, GNorm = 0.2583
Meta loss on this task batch = 3.3080e-01, Meta loss averaged over last 500 steps = 3.7044e-01, PNorm = 89.1137, GNorm = 0.2366
Meta loss on this task batch = 3.5162e-01, Meta loss averaged over last 500 steps = 3.7047e-01, PNorm = 89.1227, GNorm = 0.2823
Meta loss on this task batch = 4.2064e-01, Meta loss averaged over last 500 steps = 3.7050e-01, PNorm = 89.1309, GNorm = 0.2654
Meta loss on this task batch = 3.6696e-01, Meta loss averaged over last 500 steps = 3.7057e-01, PNorm = 89.1394, GNorm = 0.2247
Meta loss on this task batch = 3.9757e-01, Meta loss averaged over last 500 steps = 3.7053e-01, PNorm = 89.1477, GNorm = 0.2710
Meta loss on this task batch = 3.2550e-01, Meta loss averaged over last 500 steps = 3.7033e-01, PNorm = 89.1568, GNorm = 0.3193
Meta loss on this task batch = 3.6401e-01, Meta loss averaged over last 500 steps = 3.7033e-01, PNorm = 89.1662, GNorm = 0.3554
Meta loss on this task batch = 3.8869e-01, Meta loss averaged over last 500 steps = 3.7049e-01, PNorm = 89.1768, GNorm = 0.3593
Meta loss on this task batch = 3.7779e-01, Meta loss averaged over last 500 steps = 3.7061e-01, PNorm = 89.1875, GNorm = 0.2496
Meta loss on this task batch = 3.5494e-01, Meta loss averaged over last 500 steps = 3.7049e-01, PNorm = 89.1980, GNorm = 0.3624
Meta loss on this task batch = 3.4186e-01, Meta loss averaged over last 500 steps = 3.7037e-01, PNorm = 89.2089, GNorm = 0.2289
Meta loss on this task batch = 3.6162e-01, Meta loss averaged over last 500 steps = 3.7035e-01, PNorm = 89.2190, GNorm = 0.2967
Meta loss on this task batch = 3.5675e-01, Meta loss averaged over last 500 steps = 3.7024e-01, PNorm = 89.2298, GNorm = 0.2477
Meta loss on this task batch = 3.4337e-01, Meta loss averaged over last 500 steps = 3.7014e-01, PNorm = 89.2390, GNorm = 0.2309
Meta loss on this task batch = 3.3016e-01, Meta loss averaged over last 500 steps = 3.7012e-01, PNorm = 89.2486, GNorm = 0.3467
Meta loss on this task batch = 3.9376e-01, Meta loss averaged over last 500 steps = 3.7019e-01, PNorm = 89.2576, GNorm = 0.3648
Took 111.98063397407532 seconds to complete one epoch of meta training
Took 119.80515336990356 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487412
Epoch 169
Meta loss on this task batch = 3.9789e-01, Meta loss averaged over last 500 steps = 3.7013e-01, PNorm = 89.2661, GNorm = 0.3980
Meta loss on this task batch = 3.1681e-01, Meta loss averaged over last 500 steps = 3.7004e-01, PNorm = 89.2750, GNorm = 0.2392
Meta loss on this task batch = 3.5887e-01, Meta loss averaged over last 500 steps = 3.6999e-01, PNorm = 89.2842, GNorm = 0.2267
Meta loss on this task batch = 3.6008e-01, Meta loss averaged over last 500 steps = 3.6995e-01, PNorm = 89.2937, GNorm = 0.1819
Meta loss on this task batch = 2.9146e-01, Meta loss averaged over last 500 steps = 3.6984e-01, PNorm = 89.3042, GNorm = 0.2315
Meta loss on this task batch = 3.8172e-01, Meta loss averaged over last 500 steps = 3.6978e-01, PNorm = 89.3147, GNorm = 0.2517
Meta loss on this task batch = 3.7516e-01, Meta loss averaged over last 500 steps = 3.6973e-01, PNorm = 89.3254, GNorm = 0.3736
Meta loss on this task batch = 3.8533e-01, Meta loss averaged over last 500 steps = 3.6974e-01, PNorm = 89.3366, GNorm = 0.4447
Meta loss on this task batch = 3.7053e-01, Meta loss averaged over last 500 steps = 3.6984e-01, PNorm = 89.3480, GNorm = 0.2245
Meta loss on this task batch = 3.9771e-01, Meta loss averaged over last 500 steps = 3.6979e-01, PNorm = 89.3581, GNorm = 0.2808
Meta loss on this task batch = 3.2436e-01, Meta loss averaged over last 500 steps = 3.6966e-01, PNorm = 89.3683, GNorm = 0.2292
Meta loss on this task batch = 3.7375e-01, Meta loss averaged over last 500 steps = 3.6956e-01, PNorm = 89.3798, GNorm = 0.2340
Meta loss on this task batch = 3.9389e-01, Meta loss averaged over last 500 steps = 3.6956e-01, PNorm = 89.3908, GNorm = 0.4064
Meta loss on this task batch = 3.1893e-01, Meta loss averaged over last 500 steps = 3.6942e-01, PNorm = 89.4014, GNorm = 0.2420
Meta loss on this task batch = 3.1726e-01, Meta loss averaged over last 500 steps = 3.6932e-01, PNorm = 89.4124, GNorm = 0.2780
Meta loss on this task batch = 3.6974e-01, Meta loss averaged over last 500 steps = 3.6927e-01, PNorm = 89.4231, GNorm = 0.3010
Meta loss on this task batch = 3.6686e-01, Meta loss averaged over last 500 steps = 3.6929e-01, PNorm = 89.4333, GNorm = 0.2429
Meta loss on this task batch = 3.6557e-01, Meta loss averaged over last 500 steps = 3.6929e-01, PNorm = 89.4442, GNorm = 0.4343
Meta loss on this task batch = 4.6030e-01, Meta loss averaged over last 500 steps = 3.6960e-01, PNorm = 89.4545, GNorm = 0.2748
Took 109.91974377632141 seconds to complete one epoch of meta training
Took 117.3982093334198 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457699
Epoch 170
Meta loss on this task batch = 3.5233e-01, Meta loss averaged over last 500 steps = 3.6968e-01, PNorm = 89.4648, GNorm = 0.3365
Meta loss on this task batch = 2.9597e-01, Meta loss averaged over last 500 steps = 3.6948e-01, PNorm = 89.4752, GNorm = 0.2690
Meta loss on this task batch = 3.4570e-01, Meta loss averaged over last 500 steps = 3.6948e-01, PNorm = 89.4859, GNorm = 0.2347
Meta loss on this task batch = 3.3818e-01, Meta loss averaged over last 500 steps = 3.6926e-01, PNorm = 89.4977, GNorm = 0.6078
Meta loss on this task batch = 3.8548e-01, Meta loss averaged over last 500 steps = 3.6920e-01, PNorm = 89.5089, GNorm = 0.3079
Meta loss on this task batch = 3.5160e-01, Meta loss averaged over last 500 steps = 3.6912e-01, PNorm = 89.5200, GNorm = 0.2708
Meta loss on this task batch = 3.2880e-01, Meta loss averaged over last 500 steps = 3.6898e-01, PNorm = 89.5308, GNorm = 0.3293
Meta loss on this task batch = 3.5088e-01, Meta loss averaged over last 500 steps = 3.6891e-01, PNorm = 89.5422, GNorm = 0.2226
Meta loss on this task batch = 3.9050e-01, Meta loss averaged over last 500 steps = 3.6904e-01, PNorm = 89.5525, GNorm = 0.2631
Meta loss on this task batch = 3.6834e-01, Meta loss averaged over last 500 steps = 3.6896e-01, PNorm = 89.5613, GNorm = 0.2372
Meta loss on this task batch = 3.5585e-01, Meta loss averaged over last 500 steps = 3.6894e-01, PNorm = 89.5704, GNorm = 0.2813
Meta loss on this task batch = 3.6726e-01, Meta loss averaged over last 500 steps = 3.6904e-01, PNorm = 89.5796, GNorm = 0.2654
Meta loss on this task batch = 3.9894e-01, Meta loss averaged over last 500 steps = 3.6908e-01, PNorm = 89.5881, GNorm = 0.2800
Meta loss on this task batch = 3.5782e-01, Meta loss averaged over last 500 steps = 3.6909e-01, PNorm = 89.5977, GNorm = 0.2154
Meta loss on this task batch = 3.7096e-01, Meta loss averaged over last 500 steps = 3.6904e-01, PNorm = 89.6076, GNorm = 0.2156
Meta loss on this task batch = 4.1631e-01, Meta loss averaged over last 500 steps = 3.6917e-01, PNorm = 89.6171, GNorm = 0.2373
Meta loss on this task batch = 3.7703e-01, Meta loss averaged over last 500 steps = 3.6905e-01, PNorm = 89.6264, GNorm = 0.2432
Meta loss on this task batch = 3.3443e-01, Meta loss averaged over last 500 steps = 3.6906e-01, PNorm = 89.6361, GNorm = 0.2909
Meta loss on this task batch = 4.3364e-01, Meta loss averaged over last 500 steps = 3.6915e-01, PNorm = 89.6457, GNorm = 0.2539
Took 112.90716648101807 seconds to complete one epoch of meta training
Took 120.74573254585266 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470011
Epoch 171
Meta loss on this task batch = 3.2303e-01, Meta loss averaged over last 500 steps = 3.6908e-01, PNorm = 89.6552, GNorm = 0.3339
Meta loss on this task batch = 4.1607e-01, Meta loss averaged over last 500 steps = 3.6910e-01, PNorm = 89.6631, GNorm = 0.4572
Meta loss on this task batch = 4.2022e-01, Meta loss averaged over last 500 steps = 3.6915e-01, PNorm = 89.6712, GNorm = 0.2363
Meta loss on this task batch = 3.6099e-01, Meta loss averaged over last 500 steps = 3.6913e-01, PNorm = 89.6805, GNorm = 0.3535
Meta loss on this task batch = 3.6732e-01, Meta loss averaged over last 500 steps = 3.6905e-01, PNorm = 89.6907, GNorm = 0.2097
Meta loss on this task batch = 3.2705e-01, Meta loss averaged over last 500 steps = 3.6896e-01, PNorm = 89.7016, GNorm = 0.2047
Meta loss on this task batch = 3.8660e-01, Meta loss averaged over last 500 steps = 3.6895e-01, PNorm = 89.7124, GNorm = 0.2071
Meta loss on this task batch = 3.2748e-01, Meta loss averaged over last 500 steps = 3.6870e-01, PNorm = 89.7241, GNorm = 0.3198
Meta loss on this task batch = 3.8519e-01, Meta loss averaged over last 500 steps = 3.6877e-01, PNorm = 89.7363, GNorm = 0.3092
Meta loss on this task batch = 3.5302e-01, Meta loss averaged over last 500 steps = 3.6876e-01, PNorm = 89.7490, GNorm = 0.2092
Meta loss on this task batch = 3.6002e-01, Meta loss averaged over last 500 steps = 3.6875e-01, PNorm = 89.7628, GNorm = 0.4641
Meta loss on this task batch = 2.8793e-01, Meta loss averaged over last 500 steps = 3.6855e-01, PNorm = 89.7766, GNorm = 0.2443
Meta loss on this task batch = 3.1207e-01, Meta loss averaged over last 500 steps = 3.6848e-01, PNorm = 89.7903, GNorm = 0.2545
Meta loss on this task batch = 3.7297e-01, Meta loss averaged over last 500 steps = 3.6861e-01, PNorm = 89.8033, GNorm = 0.3102
Meta loss on this task batch = 3.9223e-01, Meta loss averaged over last 500 steps = 3.6860e-01, PNorm = 89.8153, GNorm = 0.4320
Meta loss on this task batch = 3.4379e-01, Meta loss averaged over last 500 steps = 3.6854e-01, PNorm = 89.8275, GNorm = 0.2690
Meta loss on this task batch = 4.3886e-01, Meta loss averaged over last 500 steps = 3.6874e-01, PNorm = 89.8389, GNorm = 0.3848
Meta loss on this task batch = 3.1597e-01, Meta loss averaged over last 500 steps = 3.6871e-01, PNorm = 89.8502, GNorm = 0.2045
Meta loss on this task batch = 3.8716e-01, Meta loss averaged over last 500 steps = 3.6867e-01, PNorm = 89.8612, GNorm = 0.4768
Took 112.27431726455688 seconds to complete one epoch of meta training
Took 119.94862484931946 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491328
Epoch 172
Meta loss on this task batch = 3.3729e-01, Meta loss averaged over last 500 steps = 3.6853e-01, PNorm = 89.8724, GNorm = 0.3021
Meta loss on this task batch = 3.5763e-01, Meta loss averaged over last 500 steps = 3.6849e-01, PNorm = 89.8828, GNorm = 0.2999
Meta loss on this task batch = 3.4450e-01, Meta loss averaged over last 500 steps = 3.6845e-01, PNorm = 89.8937, GNorm = 0.2090
Meta loss on this task batch = 3.8981e-01, Meta loss averaged over last 500 steps = 3.6852e-01, PNorm = 89.9037, GNorm = 0.2797
Meta loss on this task batch = 3.6179e-01, Meta loss averaged over last 500 steps = 3.6862e-01, PNorm = 89.9142, GNorm = 0.3664
Meta loss on this task batch = 3.5161e-01, Meta loss averaged over last 500 steps = 3.6873e-01, PNorm = 89.9237, GNorm = 0.2217
Meta loss on this task batch = 3.6296e-01, Meta loss averaged over last 500 steps = 3.6871e-01, PNorm = 89.9329, GNorm = 0.2333
Meta loss on this task batch = 3.3904e-01, Meta loss averaged over last 500 steps = 3.6849e-01, PNorm = 89.9422, GNorm = 0.3888
Meta loss on this task batch = 3.4526e-01, Meta loss averaged over last 500 steps = 3.6849e-01, PNorm = 89.9516, GNorm = 0.2312
Meta loss on this task batch = 3.9363e-01, Meta loss averaged over last 500 steps = 3.6848e-01, PNorm = 89.9605, GNorm = 0.2694
Meta loss on this task batch = 3.9215e-01, Meta loss averaged over last 500 steps = 3.6863e-01, PNorm = 89.9691, GNorm = 0.2208
Meta loss on this task batch = 3.5189e-01, Meta loss averaged over last 500 steps = 3.6863e-01, PNorm = 89.9780, GNorm = 0.2712
Meta loss on this task batch = 3.6956e-01, Meta loss averaged over last 500 steps = 3.6850e-01, PNorm = 89.9869, GNorm = 0.2211
Meta loss on this task batch = 4.4320e-01, Meta loss averaged over last 500 steps = 3.6868e-01, PNorm = 89.9964, GNorm = 0.2891
Meta loss on this task batch = 3.0993e-01, Meta loss averaged over last 500 steps = 3.6851e-01, PNorm = 90.0063, GNorm = 0.1732
Meta loss on this task batch = 3.6924e-01, Meta loss averaged over last 500 steps = 3.6863e-01, PNorm = 90.0161, GNorm = 0.3274
Meta loss on this task batch = 3.9500e-01, Meta loss averaged over last 500 steps = 3.6864e-01, PNorm = 90.0258, GNorm = 0.2849
Meta loss on this task batch = 3.1567e-01, Meta loss averaged over last 500 steps = 3.6850e-01, PNorm = 90.0366, GNorm = 0.2188
Meta loss on this task batch = 3.4623e-01, Meta loss averaged over last 500 steps = 3.6843e-01, PNorm = 90.0481, GNorm = 0.2983
Took 112.48967742919922 seconds to complete one epoch of meta training
Took 120.37309694290161 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457061
Epoch 173
Meta loss on this task batch = 4.0521e-01, Meta loss averaged over last 500 steps = 3.6849e-01, PNorm = 90.0595, GNorm = 0.2318
Meta loss on this task batch = 3.0756e-01, Meta loss averaged over last 500 steps = 3.6834e-01, PNorm = 90.0718, GNorm = 0.2419
Meta loss on this task batch = 3.6208e-01, Meta loss averaged over last 500 steps = 3.6819e-01, PNorm = 90.0838, GNorm = 0.2658
Meta loss on this task batch = 3.8408e-01, Meta loss averaged over last 500 steps = 3.6825e-01, PNorm = 90.0954, GNorm = 0.3225
Meta loss on this task batch = 3.8513e-01, Meta loss averaged over last 500 steps = 3.6827e-01, PNorm = 90.1064, GNorm = 0.2754
Meta loss on this task batch = 3.4011e-01, Meta loss averaged over last 500 steps = 3.6809e-01, PNorm = 90.1173, GNorm = 0.1983
Meta loss on this task batch = 3.2897e-01, Meta loss averaged over last 500 steps = 3.6801e-01, PNorm = 90.1283, GNorm = 0.3129
Meta loss on this task batch = 3.2536e-01, Meta loss averaged over last 500 steps = 3.6794e-01, PNorm = 90.1394, GNorm = 0.2303
Meta loss on this task batch = 3.7310e-01, Meta loss averaged over last 500 steps = 3.6791e-01, PNorm = 90.1500, GNorm = 0.2566
Meta loss on this task batch = 3.4884e-01, Meta loss averaged over last 500 steps = 3.6790e-01, PNorm = 90.1605, GNorm = 0.2748
Meta loss on this task batch = 3.3991e-01, Meta loss averaged over last 500 steps = 3.6776e-01, PNorm = 90.1718, GNorm = 0.2755
Meta loss on this task batch = 3.5505e-01, Meta loss averaged over last 500 steps = 3.6774e-01, PNorm = 90.1832, GNorm = 0.2562
Meta loss on this task batch = 3.8383e-01, Meta loss averaged over last 500 steps = 3.6761e-01, PNorm = 90.1959, GNorm = 0.4588
Meta loss on this task batch = 3.4945e-01, Meta loss averaged over last 500 steps = 3.6753e-01, PNorm = 90.2086, GNorm = 0.3182
Meta loss on this task batch = 4.6437e-01, Meta loss averaged over last 500 steps = 3.6770e-01, PNorm = 90.2203, GNorm = 0.2655
Meta loss on this task batch = 3.2679e-01, Meta loss averaged over last 500 steps = 3.6767e-01, PNorm = 90.2317, GNorm = 0.1922
Meta loss on this task batch = 3.5899e-01, Meta loss averaged over last 500 steps = 3.6769e-01, PNorm = 90.2429, GNorm = 0.3772
Meta loss on this task batch = 3.5152e-01, Meta loss averaged over last 500 steps = 3.6770e-01, PNorm = 90.2548, GNorm = 0.2157
Meta loss on this task batch = 3.9260e-01, Meta loss averaged over last 500 steps = 3.6784e-01, PNorm = 90.2661, GNorm = 0.3535
Took 111.77817630767822 seconds to complete one epoch of meta training
Took 119.35565376281738 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454646
Epoch 174
Meta loss on this task batch = 3.3658e-01, Meta loss averaged over last 500 steps = 3.6779e-01, PNorm = 90.2778, GNorm = 0.2183
Meta loss on this task batch = 3.8765e-01, Meta loss averaged over last 500 steps = 3.6786e-01, PNorm = 90.2886, GNorm = 0.4868
Meta loss on this task batch = 3.7962e-01, Meta loss averaged over last 500 steps = 3.6786e-01, PNorm = 90.2998, GNorm = 0.2913
Meta loss on this task batch = 3.5391e-01, Meta loss averaged over last 500 steps = 3.6771e-01, PNorm = 90.3106, GNorm = 0.2579
Meta loss on this task batch = 3.6915e-01, Meta loss averaged over last 500 steps = 3.6764e-01, PNorm = 90.3205, GNorm = 0.3068
Meta loss on this task batch = 3.9627e-01, Meta loss averaged over last 500 steps = 3.6778e-01, PNorm = 90.3288, GNorm = 0.2608
Meta loss on this task batch = 3.8611e-01, Meta loss averaged over last 500 steps = 3.6766e-01, PNorm = 90.3369, GNorm = 0.2162
Meta loss on this task batch = 3.7397e-01, Meta loss averaged over last 500 steps = 3.6774e-01, PNorm = 90.3451, GNorm = 0.4484
Meta loss on this task batch = 3.6958e-01, Meta loss averaged over last 500 steps = 3.6772e-01, PNorm = 90.3538, GNorm = 0.4829
Meta loss on this task batch = 3.6953e-01, Meta loss averaged over last 500 steps = 3.6776e-01, PNorm = 90.3633, GNorm = 0.4233
Meta loss on this task batch = 3.6195e-01, Meta loss averaged over last 500 steps = 3.6777e-01, PNorm = 90.3730, GNorm = 0.2039
Meta loss on this task batch = 3.8905e-01, Meta loss averaged over last 500 steps = 3.6783e-01, PNorm = 90.3833, GNorm = 0.2466
Meta loss on this task batch = 3.7370e-01, Meta loss averaged over last 500 steps = 3.6775e-01, PNorm = 90.3947, GNorm = 0.2334
Meta loss on this task batch = 3.8355e-01, Meta loss averaged over last 500 steps = 3.6765e-01, PNorm = 90.4062, GNorm = 0.2322
Meta loss on this task batch = 3.5598e-01, Meta loss averaged over last 500 steps = 3.6759e-01, PNorm = 90.4180, GNorm = 0.2229
Meta loss on this task batch = 3.5006e-01, Meta loss averaged over last 500 steps = 3.6753e-01, PNorm = 90.4302, GNorm = 0.2155
Meta loss on this task batch = 3.4604e-01, Meta loss averaged over last 500 steps = 3.6743e-01, PNorm = 90.4424, GNorm = 0.2140
Meta loss on this task batch = 3.2699e-01, Meta loss averaged over last 500 steps = 3.6744e-01, PNorm = 90.4546, GNorm = 0.2104
Meta loss on this task batch = 3.8411e-01, Meta loss averaged over last 500 steps = 3.6749e-01, PNorm = 90.4655, GNorm = 0.2938
Took 110.9243836402893 seconds to complete one epoch of meta training
Took 118.80757069587708 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.441999
Epoch 175
Meta loss on this task batch = 3.8552e-01, Meta loss averaged over last 500 steps = 3.6743e-01, PNorm = 90.4757, GNorm = 0.2522
Meta loss on this task batch = 4.2846e-01, Meta loss averaged over last 500 steps = 3.6755e-01, PNorm = 90.4850, GNorm = 0.2796
Meta loss on this task batch = 3.6135e-01, Meta loss averaged over last 500 steps = 3.6745e-01, PNorm = 90.4957, GNorm = 0.3438
Meta loss on this task batch = 4.0336e-01, Meta loss averaged over last 500 steps = 3.6761e-01, PNorm = 90.5062, GNorm = 0.2714
Meta loss on this task batch = 4.3347e-01, Meta loss averaged over last 500 steps = 3.6776e-01, PNorm = 90.5167, GNorm = 0.2915
Meta loss on this task batch = 3.5740e-01, Meta loss averaged over last 500 steps = 3.6765e-01, PNorm = 90.5272, GNorm = 0.2151
Meta loss on this task batch = 3.1953e-01, Meta loss averaged over last 500 steps = 3.6754e-01, PNorm = 90.5384, GNorm = 0.2102
Meta loss on this task batch = 3.2546e-01, Meta loss averaged over last 500 steps = 3.6746e-01, PNorm = 90.5502, GNorm = 0.2181
Meta loss on this task batch = 3.1938e-01, Meta loss averaged over last 500 steps = 3.6735e-01, PNorm = 90.5625, GNorm = 0.3180
Meta loss on this task batch = 3.5533e-01, Meta loss averaged over last 500 steps = 3.6728e-01, PNorm = 90.5753, GNorm = 0.2788
Meta loss on this task batch = 3.7835e-01, Meta loss averaged over last 500 steps = 3.6715e-01, PNorm = 90.5875, GNorm = 0.3628
Meta loss on this task batch = 3.4727e-01, Meta loss averaged over last 500 steps = 3.6706e-01, PNorm = 90.6006, GNorm = 0.7176
Meta loss on this task batch = 3.5587e-01, Meta loss averaged over last 500 steps = 3.6708e-01, PNorm = 90.6135, GNorm = 0.3112
Meta loss on this task batch = 3.7824e-01, Meta loss averaged over last 500 steps = 3.6707e-01, PNorm = 90.6259, GNorm = 0.2645
Meta loss on this task batch = 3.6933e-01, Meta loss averaged over last 500 steps = 3.6707e-01, PNorm = 90.6392, GNorm = 0.4200
Meta loss on this task batch = 3.7246e-01, Meta loss averaged over last 500 steps = 3.6701e-01, PNorm = 90.6531, GNorm = 0.2776
Meta loss on this task batch = 3.6343e-01, Meta loss averaged over last 500 steps = 3.6700e-01, PNorm = 90.6660, GNorm = 0.2209
Meta loss on this task batch = 3.5091e-01, Meta loss averaged over last 500 steps = 3.6694e-01, PNorm = 90.6786, GNorm = 0.3118
Meta loss on this task batch = 4.3345e-01, Meta loss averaged over last 500 steps = 3.6711e-01, PNorm = 90.6895, GNorm = 0.3834
Took 115.52916955947876 seconds to complete one epoch of meta training
Took 123.28120946884155 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486326
Epoch 176
Meta loss on this task batch = 3.2776e-01, Meta loss averaged over last 500 steps = 3.6702e-01, PNorm = 90.7011, GNorm = 0.2572
Meta loss on this task batch = 3.6461e-01, Meta loss averaged over last 500 steps = 3.6691e-01, PNorm = 90.7126, GNorm = 0.2896
Meta loss on this task batch = 3.7772e-01, Meta loss averaged over last 500 steps = 3.6708e-01, PNorm = 90.7241, GNorm = 0.2275
Meta loss on this task batch = 3.7763e-01, Meta loss averaged over last 500 steps = 3.6714e-01, PNorm = 90.7346, GNorm = 0.2130
Meta loss on this task batch = 3.6202e-01, Meta loss averaged over last 500 steps = 3.6703e-01, PNorm = 90.7452, GNorm = 0.2138
Meta loss on this task batch = 3.5299e-01, Meta loss averaged over last 500 steps = 3.6699e-01, PNorm = 90.7559, GNorm = 0.1922
Meta loss on this task batch = 3.8552e-01, Meta loss averaged over last 500 steps = 3.6700e-01, PNorm = 90.7665, GNorm = 0.2762
Meta loss on this task batch = 4.2669e-01, Meta loss averaged over last 500 steps = 3.6710e-01, PNorm = 90.7766, GNorm = 0.2279
Meta loss on this task batch = 3.1627e-01, Meta loss averaged over last 500 steps = 3.6691e-01, PNorm = 90.7865, GNorm = 0.4013
Meta loss on this task batch = 3.7083e-01, Meta loss averaged over last 500 steps = 3.6684e-01, PNorm = 90.7964, GNorm = 0.2475
Meta loss on this task batch = 3.3327e-01, Meta loss averaged over last 500 steps = 3.6690e-01, PNorm = 90.8064, GNorm = 0.3479
Meta loss on this task batch = 3.1974e-01, Meta loss averaged over last 500 steps = 3.6674e-01, PNorm = 90.8157, GNorm = 0.1995
Meta loss on this task batch = 3.6234e-01, Meta loss averaged over last 500 steps = 3.6680e-01, PNorm = 90.8250, GNorm = 0.2674
Meta loss on this task batch = 3.4237e-01, Meta loss averaged over last 500 steps = 3.6661e-01, PNorm = 90.8349, GNorm = 0.2004
Meta loss on this task batch = 3.5960e-01, Meta loss averaged over last 500 steps = 3.6652e-01, PNorm = 90.8445, GNorm = 0.2309
Meta loss on this task batch = 3.1696e-01, Meta loss averaged over last 500 steps = 3.6630e-01, PNorm = 90.8544, GNorm = 0.1803
Meta loss on this task batch = 3.5350e-01, Meta loss averaged over last 500 steps = 3.6624e-01, PNorm = 90.8637, GNorm = 0.2992
Meta loss on this task batch = 3.6300e-01, Meta loss averaged over last 500 steps = 3.6634e-01, PNorm = 90.8736, GNorm = 0.2535
Meta loss on this task batch = 4.1498e-01, Meta loss averaged over last 500 steps = 3.6644e-01, PNorm = 90.8835, GNorm = 0.2872
Took 109.14702773094177 seconds to complete one epoch of meta training
Took 117.24256134033203 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465866
Epoch 177
Meta loss on this task batch = 3.4543e-01, Meta loss averaged over last 500 steps = 3.6626e-01, PNorm = 90.8930, GNorm = 0.2254
Meta loss on this task batch = 3.6890e-01, Meta loss averaged over last 500 steps = 3.6636e-01, PNorm = 90.9025, GNorm = 0.3087
Meta loss on this task batch = 3.4505e-01, Meta loss averaged over last 500 steps = 3.6637e-01, PNorm = 90.9117, GNorm = 0.2866
Meta loss on this task batch = 3.5933e-01, Meta loss averaged over last 500 steps = 3.6633e-01, PNorm = 90.9205, GNorm = 0.3444
Meta loss on this task batch = 3.9104e-01, Meta loss averaged over last 500 steps = 3.6640e-01, PNorm = 90.9288, GNorm = 0.2775
Meta loss on this task batch = 3.6317e-01, Meta loss averaged over last 500 steps = 3.6632e-01, PNorm = 90.9380, GNorm = 0.2392
Meta loss on this task batch = 2.9942e-01, Meta loss averaged over last 500 steps = 3.6614e-01, PNorm = 90.9473, GNorm = 0.2448
Meta loss on this task batch = 4.5285e-01, Meta loss averaged over last 500 steps = 3.6621e-01, PNorm = 90.9562, GNorm = 0.2310
Meta loss on this task batch = 3.8295e-01, Meta loss averaged over last 500 steps = 3.6615e-01, PNorm = 90.9647, GNorm = 0.2301
Meta loss on this task batch = 3.1757e-01, Meta loss averaged over last 500 steps = 3.6603e-01, PNorm = 90.9737, GNorm = 0.2267
Meta loss on this task batch = 3.9455e-01, Meta loss averaged over last 500 steps = 3.6615e-01, PNorm = 90.9826, GNorm = 0.2702
Meta loss on this task batch = 3.3857e-01, Meta loss averaged over last 500 steps = 3.6618e-01, PNorm = 90.9919, GNorm = 0.3370
Meta loss on this task batch = 3.4609e-01, Meta loss averaged over last 500 steps = 3.6610e-01, PNorm = 91.0011, GNorm = 0.2927
Meta loss on this task batch = 3.1836e-01, Meta loss averaged over last 500 steps = 3.6602e-01, PNorm = 91.0102, GNorm = 0.2320
Meta loss on this task batch = 3.5846e-01, Meta loss averaged over last 500 steps = 3.6604e-01, PNorm = 91.0197, GNorm = 0.2890
Meta loss on this task batch = 3.3437e-01, Meta loss averaged over last 500 steps = 3.6594e-01, PNorm = 91.0290, GNorm = 0.2465
Meta loss on this task batch = 3.6300e-01, Meta loss averaged over last 500 steps = 3.6594e-01, PNorm = 91.0387, GNorm = 0.2834
Meta loss on this task batch = 3.5379e-01, Meta loss averaged over last 500 steps = 3.6594e-01, PNorm = 91.0491, GNorm = 0.5523
Meta loss on this task batch = 3.8886e-01, Meta loss averaged over last 500 steps = 3.6602e-01, PNorm = 91.0591, GNorm = 0.2814
Took 111.29754853248596 seconds to complete one epoch of meta training
Took 118.99131679534912 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468969
Epoch 178
Meta loss on this task batch = 3.3817e-01, Meta loss averaged over last 500 steps = 3.6590e-01, PNorm = 91.0698, GNorm = 0.3150
Meta loss on this task batch = 3.7757e-01, Meta loss averaged over last 500 steps = 3.6596e-01, PNorm = 91.0814, GNorm = 0.2567
Meta loss on this task batch = 3.8345e-01, Meta loss averaged over last 500 steps = 3.6584e-01, PNorm = 91.0924, GNorm = 0.3281
Meta loss on this task batch = 4.5365e-01, Meta loss averaged over last 500 steps = 3.6603e-01, PNorm = 91.1022, GNorm = 0.2419
Meta loss on this task batch = 2.9160e-01, Meta loss averaged over last 500 steps = 3.6596e-01, PNorm = 91.1123, GNorm = 0.2117
Meta loss on this task batch = 3.5837e-01, Meta loss averaged over last 500 steps = 3.6594e-01, PNorm = 91.1225, GNorm = 0.2190
Meta loss on this task batch = 3.4289e-01, Meta loss averaged over last 500 steps = 3.6594e-01, PNorm = 91.1324, GNorm = 0.2298
Meta loss on this task batch = 2.7164e-01, Meta loss averaged over last 500 steps = 3.6565e-01, PNorm = 91.1430, GNorm = 0.2502
Meta loss on this task batch = 4.0623e-01, Meta loss averaged over last 500 steps = 3.6572e-01, PNorm = 91.1536, GNorm = 0.2427
Meta loss on this task batch = 3.7710e-01, Meta loss averaged over last 500 steps = 3.6560e-01, PNorm = 91.1640, GNorm = 0.2029
Meta loss on this task batch = 3.2169e-01, Meta loss averaged over last 500 steps = 3.6558e-01, PNorm = 91.1743, GNorm = 0.1860
Meta loss on this task batch = 3.6591e-01, Meta loss averaged over last 500 steps = 3.6552e-01, PNorm = 91.1843, GNorm = 0.2620
Meta loss on this task batch = 3.3632e-01, Meta loss averaged over last 500 steps = 3.6543e-01, PNorm = 91.1942, GNorm = 0.2679
Meta loss on this task batch = 3.1740e-01, Meta loss averaged over last 500 steps = 3.6539e-01, PNorm = 91.2043, GNorm = 0.2189
Meta loss on this task batch = 3.3470e-01, Meta loss averaged over last 500 steps = 3.6535e-01, PNorm = 91.2148, GNorm = 0.2409
Meta loss on this task batch = 3.7339e-01, Meta loss averaged over last 500 steps = 3.6528e-01, PNorm = 91.2243, GNorm = 0.3622
Meta loss on this task batch = 3.7165e-01, Meta loss averaged over last 500 steps = 3.6541e-01, PNorm = 91.2335, GNorm = 0.2361
Meta loss on this task batch = 3.3666e-01, Meta loss averaged over last 500 steps = 3.6529e-01, PNorm = 91.2429, GNorm = 0.2654
Meta loss on this task batch = 3.7559e-01, Meta loss averaged over last 500 steps = 3.6525e-01, PNorm = 91.2512, GNorm = 0.4209
Took 111.90143299102783 seconds to complete one epoch of meta training
Took 120.03938245773315 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475003
Epoch 179
Meta loss on this task batch = 3.1398e-01, Meta loss averaged over last 500 steps = 3.6513e-01, PNorm = 91.2604, GNorm = 0.4032
Meta loss on this task batch = 4.4683e-01, Meta loss averaged over last 500 steps = 3.6528e-01, PNorm = 91.2686, GNorm = 0.3747
Meta loss on this task batch = 3.7462e-01, Meta loss averaged over last 500 steps = 3.6537e-01, PNorm = 91.2769, GNorm = 0.3040
Meta loss on this task batch = 3.7495e-01, Meta loss averaged over last 500 steps = 3.6530e-01, PNorm = 91.2856, GNorm = 0.2355
Meta loss on this task batch = 3.4909e-01, Meta loss averaged over last 500 steps = 3.6543e-01, PNorm = 91.2950, GNorm = 0.2274
Meta loss on this task batch = 3.8019e-01, Meta loss averaged over last 500 steps = 3.6536e-01, PNorm = 91.3043, GNorm = 0.2419
Meta loss on this task batch = 3.4099e-01, Meta loss averaged over last 500 steps = 3.6539e-01, PNorm = 91.3131, GNorm = 0.3722
Meta loss on this task batch = 3.8474e-01, Meta loss averaged over last 500 steps = 3.6546e-01, PNorm = 91.3219, GNorm = 0.2341
Meta loss on this task batch = 3.4079e-01, Meta loss averaged over last 500 steps = 3.6543e-01, PNorm = 91.3322, GNorm = 0.3235
Meta loss on this task batch = 3.4797e-01, Meta loss averaged over last 500 steps = 3.6529e-01, PNorm = 91.3434, GNorm = 0.2796
Meta loss on this task batch = 3.2592e-01, Meta loss averaged over last 500 steps = 3.6524e-01, PNorm = 91.3567, GNorm = 0.3794
Meta loss on this task batch = 3.6263e-01, Meta loss averaged over last 500 steps = 3.6528e-01, PNorm = 91.3708, GNorm = 0.2649
Meta loss on this task batch = 3.5790e-01, Meta loss averaged over last 500 steps = 3.6524e-01, PNorm = 91.3853, GNorm = 0.2214
Meta loss on this task batch = 3.3741e-01, Meta loss averaged over last 500 steps = 3.6517e-01, PNorm = 91.4002, GNorm = 0.2853
Meta loss on this task batch = 3.3388e-01, Meta loss averaged over last 500 steps = 3.6509e-01, PNorm = 91.4145, GNorm = 0.2827
Meta loss on this task batch = 3.6042e-01, Meta loss averaged over last 500 steps = 3.6509e-01, PNorm = 91.4282, GNorm = 0.2583
Meta loss on this task batch = 3.7412e-01, Meta loss averaged over last 500 steps = 3.6500e-01, PNorm = 91.4405, GNorm = 0.2304
Meta loss on this task batch = 3.3313e-01, Meta loss averaged over last 500 steps = 3.6505e-01, PNorm = 91.4532, GNorm = 0.1929
Meta loss on this task batch = 4.3929e-01, Meta loss averaged over last 500 steps = 3.6521e-01, PNorm = 91.4639, GNorm = 0.3034
Took 113.39196991920471 seconds to complete one epoch of meta training
Took 121.30460453033447 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467199
Epoch 180
Meta loss on this task batch = 3.6757e-01, Meta loss averaged over last 500 steps = 3.6514e-01, PNorm = 91.4741, GNorm = 0.2328
Meta loss on this task batch = 3.5480e-01, Meta loss averaged over last 500 steps = 3.6513e-01, PNorm = 91.4844, GNorm = 0.2506
Meta loss on this task batch = 3.4876e-01, Meta loss averaged over last 500 steps = 3.6501e-01, PNorm = 91.4947, GNorm = 0.2447
Meta loss on this task batch = 3.4254e-01, Meta loss averaged over last 500 steps = 3.6508e-01, PNorm = 91.5053, GNorm = 0.3224
Meta loss on this task batch = 3.3650e-01, Meta loss averaged over last 500 steps = 3.6499e-01, PNorm = 91.5168, GNorm = 0.4269
Meta loss on this task batch = 3.4460e-01, Meta loss averaged over last 500 steps = 3.6491e-01, PNorm = 91.5280, GNorm = 0.2988
Meta loss on this task batch = 3.0822e-01, Meta loss averaged over last 500 steps = 3.6482e-01, PNorm = 91.5389, GNorm = 0.3134
Meta loss on this task batch = 4.3421e-01, Meta loss averaged over last 500 steps = 3.6492e-01, PNorm = 91.5481, GNorm = 0.2892
Meta loss on this task batch = 2.9748e-01, Meta loss averaged over last 500 steps = 3.6468e-01, PNorm = 91.5570, GNorm = 0.2423
Meta loss on this task batch = 3.9580e-01, Meta loss averaged over last 500 steps = 3.6473e-01, PNorm = 91.5651, GNorm = 0.2373
Meta loss on this task batch = 3.9690e-01, Meta loss averaged over last 500 steps = 3.6481e-01, PNorm = 91.5729, GNorm = 0.2595
Meta loss on this task batch = 3.3588e-01, Meta loss averaged over last 500 steps = 3.6472e-01, PNorm = 91.5806, GNorm = 0.2342
Meta loss on this task batch = 3.4887e-01, Meta loss averaged over last 500 steps = 3.6474e-01, PNorm = 91.5882, GNorm = 0.2367
Meta loss on this task batch = 3.3151e-01, Meta loss averaged over last 500 steps = 3.6459e-01, PNorm = 91.5959, GNorm = 0.3169
Meta loss on this task batch = 3.7584e-01, Meta loss averaged over last 500 steps = 3.6451e-01, PNorm = 91.6053, GNorm = 0.2166
Meta loss on this task batch = 3.2686e-01, Meta loss averaged over last 500 steps = 3.6447e-01, PNorm = 91.6154, GNorm = 0.2408
Meta loss on this task batch = 3.4130e-01, Meta loss averaged over last 500 steps = 3.6447e-01, PNorm = 91.6248, GNorm = 0.3315
Meta loss on this task batch = 3.9008e-01, Meta loss averaged over last 500 steps = 3.6442e-01, PNorm = 91.6340, GNorm = 0.2330
Meta loss on this task batch = 3.4843e-01, Meta loss averaged over last 500 steps = 3.6443e-01, PNorm = 91.6437, GNorm = 0.2553
Took 112.17504048347473 seconds to complete one epoch of meta training
Took 119.27256608009338 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493231
Epoch 181
Meta loss on this task batch = 3.7677e-01, Meta loss averaged over last 500 steps = 3.6445e-01, PNorm = 91.6527, GNorm = 0.2824
Meta loss on this task batch = 2.9727e-01, Meta loss averaged over last 500 steps = 3.6450e-01, PNorm = 91.6625, GNorm = 0.2405
Meta loss on this task batch = 3.5331e-01, Meta loss averaged over last 500 steps = 3.6435e-01, PNorm = 91.6729, GNorm = 0.2643
Meta loss on this task batch = 3.2603e-01, Meta loss averaged over last 500 steps = 3.6420e-01, PNorm = 91.6832, GNorm = 0.2379
Meta loss on this task batch = 2.8566e-01, Meta loss averaged over last 500 steps = 3.6396e-01, PNorm = 91.6949, GNorm = 0.2443
Meta loss on this task batch = 3.8627e-01, Meta loss averaged over last 500 steps = 3.6397e-01, PNorm = 91.7060, GNorm = 0.2581
Meta loss on this task batch = 4.0936e-01, Meta loss averaged over last 500 steps = 3.6408e-01, PNorm = 91.7168, GNorm = 0.2994
Meta loss on this task batch = 3.4114e-01, Meta loss averaged over last 500 steps = 3.6405e-01, PNorm = 91.7269, GNorm = 0.2950
Meta loss on this task batch = 3.0091e-01, Meta loss averaged over last 500 steps = 3.6392e-01, PNorm = 91.7373, GNorm = 0.2181
Meta loss on this task batch = 3.4200e-01, Meta loss averaged over last 500 steps = 3.6381e-01, PNorm = 91.7477, GNorm = 0.2678
Meta loss on this task batch = 3.5001e-01, Meta loss averaged over last 500 steps = 3.6383e-01, PNorm = 91.7580, GNorm = 0.2831
Meta loss on this task batch = 4.0630e-01, Meta loss averaged over last 500 steps = 3.6394e-01, PNorm = 91.7683, GNorm = 0.5211
Meta loss on this task batch = 3.7869e-01, Meta loss averaged over last 500 steps = 3.6400e-01, PNorm = 91.7790, GNorm = 0.2345
Meta loss on this task batch = 3.5995e-01, Meta loss averaged over last 500 steps = 3.6398e-01, PNorm = 91.7894, GNorm = 0.2492
Meta loss on this task batch = 3.6858e-01, Meta loss averaged over last 500 steps = 3.6407e-01, PNorm = 91.8000, GNorm = 0.3812
Meta loss on this task batch = 3.8094e-01, Meta loss averaged over last 500 steps = 3.6419e-01, PNorm = 91.8110, GNorm = 0.5546
Meta loss on this task batch = 3.2119e-01, Meta loss averaged over last 500 steps = 3.6415e-01, PNorm = 91.8214, GNorm = 0.3390
Meta loss on this task batch = 3.8425e-01, Meta loss averaged over last 500 steps = 3.6413e-01, PNorm = 91.8324, GNorm = 0.4378
Meta loss on this task batch = 3.5032e-01, Meta loss averaged over last 500 steps = 3.6414e-01, PNorm = 91.8434, GNorm = 0.2507
Took 111.14445495605469 seconds to complete one epoch of meta training
Took 119.24676847457886 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478038
Epoch 182
Meta loss on this task batch = 3.5267e-01, Meta loss averaged over last 500 steps = 3.6396e-01, PNorm = 91.8545, GNorm = 0.3370
Meta loss on this task batch = 3.8976e-01, Meta loss averaged over last 500 steps = 3.6396e-01, PNorm = 91.8656, GNorm = 0.2670
Meta loss on this task batch = 3.2467e-01, Meta loss averaged over last 500 steps = 3.6383e-01, PNorm = 91.8770, GNorm = 0.2217
Meta loss on this task batch = 3.1727e-01, Meta loss averaged over last 500 steps = 3.6370e-01, PNorm = 91.8875, GNorm = 0.2313
Meta loss on this task batch = 3.2386e-01, Meta loss averaged over last 500 steps = 3.6363e-01, PNorm = 91.8975, GNorm = 0.2478
Meta loss on this task batch = 3.4287e-01, Meta loss averaged over last 500 steps = 3.6366e-01, PNorm = 91.9078, GNorm = 0.2632
Meta loss on this task batch = 4.1157e-01, Meta loss averaged over last 500 steps = 3.6364e-01, PNorm = 91.9170, GNorm = 0.3187
Meta loss on this task batch = 4.0703e-01, Meta loss averaged over last 500 steps = 3.6375e-01, PNorm = 91.9249, GNorm = 0.3332
Meta loss on this task batch = 3.7421e-01, Meta loss averaged over last 500 steps = 3.6387e-01, PNorm = 91.9322, GNorm = 0.2588
Meta loss on this task batch = 3.2351e-01, Meta loss averaged over last 500 steps = 3.6384e-01, PNorm = 91.9398, GNorm = 0.2363
Meta loss on this task batch = 4.2678e-01, Meta loss averaged over last 500 steps = 3.6398e-01, PNorm = 91.9470, GNorm = 0.2388
Meta loss on this task batch = 3.1589e-01, Meta loss averaged over last 500 steps = 3.6378e-01, PNorm = 91.9542, GNorm = 0.3323
Meta loss on this task batch = 4.0166e-01, Meta loss averaged over last 500 steps = 3.6386e-01, PNorm = 91.9620, GNorm = 0.3028
Meta loss on this task batch = 3.7965e-01, Meta loss averaged over last 500 steps = 3.6387e-01, PNorm = 91.9686, GNorm = 0.4043
Meta loss on this task batch = 3.7628e-01, Meta loss averaged over last 500 steps = 3.6388e-01, PNorm = 91.9760, GNorm = 0.2404
Meta loss on this task batch = 3.5260e-01, Meta loss averaged over last 500 steps = 3.6382e-01, PNorm = 91.9834, GNorm = 0.2628
Meta loss on this task batch = 3.9975e-01, Meta loss averaged over last 500 steps = 3.6383e-01, PNorm = 91.9920, GNorm = 0.3768
Meta loss on this task batch = 3.4641e-01, Meta loss averaged over last 500 steps = 3.6372e-01, PNorm = 92.0007, GNorm = 0.2307
Meta loss on this task batch = 3.3380e-01, Meta loss averaged over last 500 steps = 3.6373e-01, PNorm = 92.0099, GNorm = 0.3227
Took 112.61910319328308 seconds to complete one epoch of meta training
Took 120.80971360206604 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475212
Epoch 183
Meta loss on this task batch = 3.8770e-01, Meta loss averaged over last 500 steps = 3.6370e-01, PNorm = 92.0203, GNorm = 0.2221
Meta loss on this task batch = 3.9963e-01, Meta loss averaged over last 500 steps = 3.6387e-01, PNorm = 92.0309, GNorm = 0.2452
Meta loss on this task batch = 3.4486e-01, Meta loss averaged over last 500 steps = 3.6386e-01, PNorm = 92.0409, GNorm = 0.3082
Meta loss on this task batch = 3.6045e-01, Meta loss averaged over last 500 steps = 3.6379e-01, PNorm = 92.0521, GNorm = 0.3184
Meta loss on this task batch = 3.6169e-01, Meta loss averaged over last 500 steps = 3.6371e-01, PNorm = 92.0634, GNorm = 0.2154
Meta loss on this task batch = 3.5410e-01, Meta loss averaged over last 500 steps = 3.6373e-01, PNorm = 92.0744, GNorm = 0.3184
Meta loss on this task batch = 3.9572e-01, Meta loss averaged over last 500 steps = 3.6386e-01, PNorm = 92.0842, GNorm = 0.2316
Meta loss on this task batch = 3.4115e-01, Meta loss averaged over last 500 steps = 3.6389e-01, PNorm = 92.0943, GNorm = 0.2483
Meta loss on this task batch = 3.9560e-01, Meta loss averaged over last 500 steps = 3.6378e-01, PNorm = 92.1024, GNorm = 0.2939
Meta loss on this task batch = 3.7634e-01, Meta loss averaged over last 500 steps = 3.6377e-01, PNorm = 92.1112, GNorm = 0.3839
Meta loss on this task batch = 3.9036e-01, Meta loss averaged over last 500 steps = 3.6374e-01, PNorm = 92.1203, GNorm = 0.2347
Meta loss on this task batch = 3.7596e-01, Meta loss averaged over last 500 steps = 3.6377e-01, PNorm = 92.1303, GNorm = 0.4168
Meta loss on this task batch = 3.5833e-01, Meta loss averaged over last 500 steps = 3.6365e-01, PNorm = 92.1410, GNorm = 0.2521
Meta loss on this task batch = 2.8402e-01, Meta loss averaged over last 500 steps = 3.6348e-01, PNorm = 92.1518, GNorm = 0.2214
Meta loss on this task batch = 3.7216e-01, Meta loss averaged over last 500 steps = 3.6363e-01, PNorm = 92.1630, GNorm = 0.4492
Meta loss on this task batch = 3.5624e-01, Meta loss averaged over last 500 steps = 3.6365e-01, PNorm = 92.1743, GNorm = 0.2755
Meta loss on this task batch = 3.5588e-01, Meta loss averaged over last 500 steps = 3.6355e-01, PNorm = 92.1864, GNorm = 0.2424
Meta loss on this task batch = 3.1339e-01, Meta loss averaged over last 500 steps = 3.6350e-01, PNorm = 92.1977, GNorm = 0.2850
Meta loss on this task batch = 3.5478e-01, Meta loss averaged over last 500 steps = 3.6357e-01, PNorm = 92.2080, GNorm = 0.3852
Took 113.18234252929688 seconds to complete one epoch of meta training
Took 120.98381018638611 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475168
Epoch 184
Meta loss on this task batch = 3.2629e-01, Meta loss averaged over last 500 steps = 3.6354e-01, PNorm = 92.2185, GNorm = 0.2577
Meta loss on this task batch = 3.6184e-01, Meta loss averaged over last 500 steps = 3.6348e-01, PNorm = 92.2290, GNorm = 0.2543
Meta loss on this task batch = 2.4920e-01, Meta loss averaged over last 500 steps = 3.6318e-01, PNorm = 92.2392, GNorm = 0.2285
Meta loss on this task batch = 4.0922e-01, Meta loss averaged over last 500 steps = 3.6331e-01, PNorm = 92.2485, GNorm = 0.3968
Meta loss on this task batch = 3.2885e-01, Meta loss averaged over last 500 steps = 3.6324e-01, PNorm = 92.2579, GNorm = 0.2869
Meta loss on this task batch = 3.1931e-01, Meta loss averaged over last 500 steps = 3.6316e-01, PNorm = 92.2668, GNorm = 0.2878
Meta loss on this task batch = 3.5541e-01, Meta loss averaged over last 500 steps = 3.6307e-01, PNorm = 92.2754, GNorm = 0.2126
Meta loss on this task batch = 3.9999e-01, Meta loss averaged over last 500 steps = 3.6307e-01, PNorm = 92.2844, GNorm = 0.3493
Meta loss on this task batch = 3.4691e-01, Meta loss averaged over last 500 steps = 3.6304e-01, PNorm = 92.2936, GNorm = 0.2886
Meta loss on this task batch = 4.0682e-01, Meta loss averaged over last 500 steps = 3.6317e-01, PNorm = 92.3028, GNorm = 0.2140
Meta loss on this task batch = 3.8070e-01, Meta loss averaged over last 500 steps = 3.6314e-01, PNorm = 92.3120, GNorm = 0.2183
Meta loss on this task batch = 3.8171e-01, Meta loss averaged over last 500 steps = 3.6323e-01, PNorm = 92.3208, GNorm = 0.4542
Meta loss on this task batch = 3.3717e-01, Meta loss averaged over last 500 steps = 3.6314e-01, PNorm = 92.3299, GNorm = 0.3333
Meta loss on this task batch = 3.9380e-01, Meta loss averaged over last 500 steps = 3.6324e-01, PNorm = 92.3386, GNorm = 0.3839
Meta loss on this task batch = 3.7043e-01, Meta loss averaged over last 500 steps = 3.6312e-01, PNorm = 92.3471, GNorm = 0.4038
Meta loss on this task batch = 3.9722e-01, Meta loss averaged over last 500 steps = 3.6311e-01, PNorm = 92.3547, GNorm = 0.3148
Meta loss on this task batch = 3.9203e-01, Meta loss averaged over last 500 steps = 3.6318e-01, PNorm = 92.3626, GNorm = 0.2127
Meta loss on this task batch = 3.2267e-01, Meta loss averaged over last 500 steps = 3.6317e-01, PNorm = 92.3717, GNorm = 0.4095
Meta loss on this task batch = 3.6227e-01, Meta loss averaged over last 500 steps = 3.6311e-01, PNorm = 92.3802, GNorm = 0.3202
Took 110.82550358772278 seconds to complete one epoch of meta training
Took 117.94212889671326 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505678
Found better MAML checkpoint after meta validation, saving now
Epoch 185
Meta loss on this task batch = 3.6265e-01, Meta loss averaged over last 500 steps = 3.6317e-01, PNorm = 92.3895, GNorm = 0.2842
Meta loss on this task batch = 3.4736e-01, Meta loss averaged over last 500 steps = 3.6309e-01, PNorm = 92.3997, GNorm = 0.2403
Meta loss on this task batch = 3.2221e-01, Meta loss averaged over last 500 steps = 3.6300e-01, PNorm = 92.4108, GNorm = 0.3578
Meta loss on this task batch = 3.7775e-01, Meta loss averaged over last 500 steps = 3.6296e-01, PNorm = 92.4219, GNorm = 0.2624
Meta loss on this task batch = 3.0009e-01, Meta loss averaged over last 500 steps = 3.6280e-01, PNorm = 92.4323, GNorm = 0.2410
Meta loss on this task batch = 3.3173e-01, Meta loss averaged over last 500 steps = 3.6273e-01, PNorm = 92.4427, GNorm = 0.2752
Meta loss on this task batch = 3.6516e-01, Meta loss averaged over last 500 steps = 3.6280e-01, PNorm = 92.4527, GNorm = 0.2648
Meta loss on this task batch = 2.8855e-01, Meta loss averaged over last 500 steps = 3.6259e-01, PNorm = 92.4636, GNorm = 0.2229
Meta loss on this task batch = 3.5150e-01, Meta loss averaged over last 500 steps = 3.6256e-01, PNorm = 92.4752, GNorm = 0.3930
Meta loss on this task batch = 3.8807e-01, Meta loss averaged over last 500 steps = 3.6271e-01, PNorm = 92.4862, GNorm = 0.2706
Meta loss on this task batch = 3.7088e-01, Meta loss averaged over last 500 steps = 3.6270e-01, PNorm = 92.4964, GNorm = 0.3235
Meta loss on this task batch = 4.1416e-01, Meta loss averaged over last 500 steps = 3.6269e-01, PNorm = 92.5059, GNorm = 0.4149
Meta loss on this task batch = 3.6606e-01, Meta loss averaged over last 500 steps = 3.6275e-01, PNorm = 92.5153, GNorm = 0.2848
Meta loss on this task batch = 3.3648e-01, Meta loss averaged over last 500 steps = 3.6269e-01, PNorm = 92.5248, GNorm = 0.2740
Meta loss on this task batch = 3.6818e-01, Meta loss averaged over last 500 steps = 3.6273e-01, PNorm = 92.5347, GNorm = 0.2595
Meta loss on this task batch = 3.0821e-01, Meta loss averaged over last 500 steps = 3.6260e-01, PNorm = 92.5461, GNorm = 0.2336
Meta loss on this task batch = 3.2642e-01, Meta loss averaged over last 500 steps = 3.6246e-01, PNorm = 92.5574, GNorm = 0.2576
Meta loss on this task batch = 4.0998e-01, Meta loss averaged over last 500 steps = 3.6247e-01, PNorm = 92.5690, GNorm = 0.2853
Meta loss on this task batch = 3.7352e-01, Meta loss averaged over last 500 steps = 3.6240e-01, PNorm = 92.5821, GNorm = 0.4634
Took 110.09856605529785 seconds to complete one epoch of meta training
Took 118.22212672233582 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472479
Epoch 186
Meta loss on this task batch = 3.3800e-01, Meta loss averaged over last 500 steps = 3.6242e-01, PNorm = 92.5964, GNorm = 0.3203
Meta loss on this task batch = 3.4346e-01, Meta loss averaged over last 500 steps = 3.6230e-01, PNorm = 92.6096, GNorm = 0.3260
Meta loss on this task batch = 3.2911e-01, Meta loss averaged over last 500 steps = 3.6231e-01, PNorm = 92.6235, GNorm = 0.2754
Meta loss on this task batch = 3.3007e-01, Meta loss averaged over last 500 steps = 3.6229e-01, PNorm = 92.6372, GNorm = 0.2843
Meta loss on this task batch = 3.9082e-01, Meta loss averaged over last 500 steps = 3.6230e-01, PNorm = 92.6499, GNorm = 0.2517
Meta loss on this task batch = 4.1041e-01, Meta loss averaged over last 500 steps = 3.6246e-01, PNorm = 92.6617, GNorm = 0.2526
Meta loss on this task batch = 3.4221e-01, Meta loss averaged over last 500 steps = 3.6234e-01, PNorm = 92.6729, GNorm = 0.2800
Meta loss on this task batch = 2.9908e-01, Meta loss averaged over last 500 steps = 3.6223e-01, PNorm = 92.6844, GNorm = 0.2206
Meta loss on this task batch = 3.2393e-01, Meta loss averaged over last 500 steps = 3.6218e-01, PNorm = 92.6964, GNorm = 0.3964
Meta loss on this task batch = 4.2599e-01, Meta loss averaged over last 500 steps = 3.6229e-01, PNorm = 92.7075, GNorm = 0.3906
Meta loss on this task batch = 4.0910e-01, Meta loss averaged over last 500 steps = 3.6236e-01, PNorm = 92.7181, GNorm = 0.2357
Meta loss on this task batch = 3.4508e-01, Meta loss averaged over last 500 steps = 3.6235e-01, PNorm = 92.7284, GNorm = 0.2291
Meta loss on this task batch = 3.2057e-01, Meta loss averaged over last 500 steps = 3.6242e-01, PNorm = 92.7387, GNorm = 0.1993
Meta loss on this task batch = 3.3900e-01, Meta loss averaged over last 500 steps = 3.6242e-01, PNorm = 92.7480, GNorm = 0.3449
Meta loss on this task batch = 3.4478e-01, Meta loss averaged over last 500 steps = 3.6248e-01, PNorm = 92.7577, GNorm = 0.3560
Meta loss on this task batch = 2.9873e-01, Meta loss averaged over last 500 steps = 3.6228e-01, PNorm = 92.7671, GNorm = 0.2161
Meta loss on this task batch = 4.0703e-01, Meta loss averaged over last 500 steps = 3.6222e-01, PNorm = 92.7763, GNorm = 0.2484
Meta loss on this task batch = 3.2813e-01, Meta loss averaged over last 500 steps = 3.6215e-01, PNorm = 92.7859, GNorm = 0.2270
Meta loss on this task batch = 4.0642e-01, Meta loss averaged over last 500 steps = 3.6221e-01, PNorm = 92.7948, GNorm = 0.2789
Took 115.09565663337708 seconds to complete one epoch of meta training
Took 121.81055498123169 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492060
Epoch 187
Meta loss on this task batch = 3.8591e-01, Meta loss averaged over last 500 steps = 3.6222e-01, PNorm = 92.8029, GNorm = 0.2501
Meta loss on this task batch = 3.8047e-01, Meta loss averaged over last 500 steps = 3.6227e-01, PNorm = 92.8103, GNorm = 0.2493
Meta loss on this task batch = 3.7434e-01, Meta loss averaged over last 500 steps = 3.6229e-01, PNorm = 92.8179, GNorm = 0.2806
Meta loss on this task batch = 3.0503e-01, Meta loss averaged over last 500 steps = 3.6211e-01, PNorm = 92.8266, GNorm = 0.2592
Meta loss on this task batch = 3.6645e-01, Meta loss averaged over last 500 steps = 3.6216e-01, PNorm = 92.8353, GNorm = 0.2411
Meta loss on this task batch = 3.2010e-01, Meta loss averaged over last 500 steps = 3.6209e-01, PNorm = 92.8444, GNorm = 0.1991
Meta loss on this task batch = 3.3824e-01, Meta loss averaged over last 500 steps = 3.6206e-01, PNorm = 92.8539, GNorm = 0.2113
Meta loss on this task batch = 3.0011e-01, Meta loss averaged over last 500 steps = 3.6197e-01, PNorm = 92.8645, GNorm = 0.2466
Meta loss on this task batch = 3.2668e-01, Meta loss averaged over last 500 steps = 3.6187e-01, PNorm = 92.8744, GNorm = 0.2046
Meta loss on this task batch = 3.1337e-01, Meta loss averaged over last 500 steps = 3.6177e-01, PNorm = 92.8839, GNorm = 0.2735
Meta loss on this task batch = 3.7636e-01, Meta loss averaged over last 500 steps = 3.6194e-01, PNorm = 92.8933, GNorm = 0.3251
Meta loss on this task batch = 3.6184e-01, Meta loss averaged over last 500 steps = 3.6188e-01, PNorm = 92.9023, GNorm = 0.2513
Meta loss on this task batch = 3.8625e-01, Meta loss averaged over last 500 steps = 3.6188e-01, PNorm = 92.9104, GNorm = 0.2733
Meta loss on this task batch = 4.1341e-01, Meta loss averaged over last 500 steps = 3.6198e-01, PNorm = 92.9167, GNorm = 0.2815
Meta loss on this task batch = 3.4759e-01, Meta loss averaged over last 500 steps = 3.6189e-01, PNorm = 92.9237, GNorm = 0.3116
Meta loss on this task batch = 3.3066e-01, Meta loss averaged over last 500 steps = 3.6186e-01, PNorm = 92.9319, GNorm = 0.2774
Meta loss on this task batch = 3.5727e-01, Meta loss averaged over last 500 steps = 3.6181e-01, PNorm = 92.9395, GNorm = 0.2050
Meta loss on this task batch = 3.9082e-01, Meta loss averaged over last 500 steps = 3.6185e-01, PNorm = 92.9482, GNorm = 0.5038
Meta loss on this task batch = 3.0761e-01, Meta loss averaged over last 500 steps = 3.6173e-01, PNorm = 92.9582, GNorm = 0.2280
Took 112.99029040336609 seconds to complete one epoch of meta training
Took 120.72873759269714 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481435
Epoch 188
Meta loss on this task batch = 3.1682e-01, Meta loss averaged over last 500 steps = 3.6155e-01, PNorm = 92.9685, GNorm = 0.3390
Meta loss on this task batch = 3.5067e-01, Meta loss averaged over last 500 steps = 3.6155e-01, PNorm = 92.9788, GNorm = 0.2375
Meta loss on this task batch = 3.4276e-01, Meta loss averaged over last 500 steps = 3.6146e-01, PNorm = 92.9890, GNorm = 0.3076
Meta loss on this task batch = 3.6745e-01, Meta loss averaged over last 500 steps = 3.6151e-01, PNorm = 92.9990, GNorm = 0.3575
Meta loss on this task batch = 3.3068e-01, Meta loss averaged over last 500 steps = 3.6129e-01, PNorm = 93.0094, GNorm = 0.3269
Meta loss on this task batch = 3.4499e-01, Meta loss averaged over last 500 steps = 3.6122e-01, PNorm = 93.0191, GNorm = 0.2624
Meta loss on this task batch = 3.6235e-01, Meta loss averaged over last 500 steps = 3.6111e-01, PNorm = 93.0286, GNorm = 0.2555
Meta loss on this task batch = 3.3539e-01, Meta loss averaged over last 500 steps = 3.6103e-01, PNorm = 93.0388, GNorm = 0.2277
Meta loss on this task batch = 3.3204e-01, Meta loss averaged over last 500 steps = 3.6098e-01, PNorm = 93.0483, GNorm = 0.2655
Meta loss on this task batch = 3.7027e-01, Meta loss averaged over last 500 steps = 3.6091e-01, PNorm = 93.0557, GNorm = 0.2950
Meta loss on this task batch = 3.7063e-01, Meta loss averaged over last 500 steps = 3.6104e-01, PNorm = 93.0614, GNorm = 0.2834
Meta loss on this task batch = 2.9774e-01, Meta loss averaged over last 500 steps = 3.6081e-01, PNorm = 93.0679, GNorm = 0.3026
Meta loss on this task batch = 3.6211e-01, Meta loss averaged over last 500 steps = 3.6069e-01, PNorm = 93.0745, GNorm = 0.2564
Meta loss on this task batch = 3.6584e-01, Meta loss averaged over last 500 steps = 3.6068e-01, PNorm = 93.0813, GNorm = 0.2803
Meta loss on this task batch = 3.7709e-01, Meta loss averaged over last 500 steps = 3.6076e-01, PNorm = 93.0885, GNorm = 0.2809
Meta loss on this task batch = 3.5648e-01, Meta loss averaged over last 500 steps = 3.6075e-01, PNorm = 93.0963, GNorm = 0.2429
Meta loss on this task batch = 3.6700e-01, Meta loss averaged over last 500 steps = 3.6069e-01, PNorm = 93.1042, GNorm = 0.2241
Meta loss on this task batch = 3.9137e-01, Meta loss averaged over last 500 steps = 3.6086e-01, PNorm = 93.1125, GNorm = 0.2901
Meta loss on this task batch = 3.1561e-01, Meta loss averaged over last 500 steps = 3.6069e-01, PNorm = 93.1217, GNorm = 0.2352
Took 113.49256014823914 seconds to complete one epoch of meta training
Took 121.0480682849884 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488276
Epoch 189
Meta loss on this task batch = 2.3259e-01, Meta loss averaged over last 500 steps = 3.6053e-01, PNorm = 93.1321, GNorm = 0.2671
Meta loss on this task batch = 3.4259e-01, Meta loss averaged over last 500 steps = 3.6051e-01, PNorm = 93.1432, GNorm = 0.2385
Meta loss on this task batch = 3.3977e-01, Meta loss averaged over last 500 steps = 3.6040e-01, PNorm = 93.1533, GNorm = 0.3138
Meta loss on this task batch = 2.9355e-01, Meta loss averaged over last 500 steps = 3.6029e-01, PNorm = 93.1637, GNorm = 0.2180
Meta loss on this task batch = 3.3796e-01, Meta loss averaged over last 500 steps = 3.6020e-01, PNorm = 93.1743, GNorm = 0.2326
Meta loss on this task batch = 3.5261e-01, Meta loss averaged over last 500 steps = 3.6016e-01, PNorm = 93.1846, GNorm = 0.3333
Meta loss on this task batch = 4.0032e-01, Meta loss averaged over last 500 steps = 3.6037e-01, PNorm = 93.1942, GNorm = 0.3042
Meta loss on this task batch = 3.0805e-01, Meta loss averaged over last 500 steps = 3.6032e-01, PNorm = 93.2036, GNorm = 0.3814
Meta loss on this task batch = 3.4822e-01, Meta loss averaged over last 500 steps = 3.6026e-01, PNorm = 93.2131, GNorm = 0.2703
Meta loss on this task batch = 3.4809e-01, Meta loss averaged over last 500 steps = 3.6010e-01, PNorm = 93.2223, GNorm = 0.2454
Meta loss on this task batch = 3.8437e-01, Meta loss averaged over last 500 steps = 3.6012e-01, PNorm = 93.2314, GNorm = 0.2628
Meta loss on this task batch = 3.3484e-01, Meta loss averaged over last 500 steps = 3.6009e-01, PNorm = 93.2399, GNorm = 0.3274
Meta loss on this task batch = 3.7881e-01, Meta loss averaged over last 500 steps = 3.6008e-01, PNorm = 93.2479, GNorm = 0.2967
Meta loss on this task batch = 3.6498e-01, Meta loss averaged over last 500 steps = 3.6024e-01, PNorm = 93.2568, GNorm = 0.4070
Meta loss on this task batch = 3.9375e-01, Meta loss averaged over last 500 steps = 3.6020e-01, PNorm = 93.2657, GNorm = 0.2281
Meta loss on this task batch = 3.6165e-01, Meta loss averaged over last 500 steps = 3.6029e-01, PNorm = 93.2753, GNorm = 0.2178
Meta loss on this task batch = 3.4491e-01, Meta loss averaged over last 500 steps = 3.6026e-01, PNorm = 93.2852, GNorm = 0.2345
Meta loss on this task batch = 3.6948e-01, Meta loss averaged over last 500 steps = 3.6022e-01, PNorm = 93.2945, GNorm = 0.2277
Meta loss on this task batch = 3.7496e-01, Meta loss averaged over last 500 steps = 3.6029e-01, PNorm = 93.3040, GNorm = 0.2563
Took 180.10805320739746 seconds to complete one epoch of meta training
Took 188.34152913093567 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472575
Epoch 190
Meta loss on this task batch = 3.5860e-01, Meta loss averaged over last 500 steps = 3.6029e-01, PNorm = 93.3139, GNorm = 0.2365
Meta loss on this task batch = 3.3982e-01, Meta loss averaged over last 500 steps = 3.6023e-01, PNorm = 93.3237, GNorm = 0.2143
Meta loss on this task batch = 3.8483e-01, Meta loss averaged over last 500 steps = 3.6028e-01, PNorm = 93.3333, GNorm = 0.2068
Meta loss on this task batch = 3.9584e-01, Meta loss averaged over last 500 steps = 3.6023e-01, PNorm = 93.3434, GNorm = 0.2635
Meta loss on this task batch = 3.9045e-01, Meta loss averaged over last 500 steps = 3.6027e-01, PNorm = 93.3528, GNorm = 0.4527
Meta loss on this task batch = 3.2245e-01, Meta loss averaged over last 500 steps = 3.6011e-01, PNorm = 93.3624, GNorm = 0.2956
Meta loss on this task batch = 3.7116e-01, Meta loss averaged over last 500 steps = 3.6009e-01, PNorm = 93.3725, GNorm = 0.3403
Meta loss on this task batch = 3.1318e-01, Meta loss averaged over last 500 steps = 3.5992e-01, PNorm = 93.3829, GNorm = 0.2404
Meta loss on this task batch = 3.1536e-01, Meta loss averaged over last 500 steps = 3.5995e-01, PNorm = 93.3940, GNorm = 0.3121
Meta loss on this task batch = 3.1095e-01, Meta loss averaged over last 500 steps = 3.5986e-01, PNorm = 93.4054, GNorm = 0.2443
Meta loss on this task batch = 3.5735e-01, Meta loss averaged over last 500 steps = 3.5985e-01, PNorm = 93.4157, GNorm = 0.2909
Meta loss on this task batch = 3.8793e-01, Meta loss averaged over last 500 steps = 3.5997e-01, PNorm = 93.4250, GNorm = 0.2357
Meta loss on this task batch = 3.4446e-01, Meta loss averaged over last 500 steps = 3.6006e-01, PNorm = 93.4335, GNorm = 0.2331
Meta loss on this task batch = 3.1603e-01, Meta loss averaged over last 500 steps = 3.5999e-01, PNorm = 93.4418, GNorm = 0.2506
Meta loss on this task batch = 3.7369e-01, Meta loss averaged over last 500 steps = 3.5991e-01, PNorm = 93.4496, GNorm = 0.3041
Meta loss on this task batch = 3.5908e-01, Meta loss averaged over last 500 steps = 3.5985e-01, PNorm = 93.4577, GNorm = 0.2590
Meta loss on this task batch = 3.0735e-01, Meta loss averaged over last 500 steps = 3.5973e-01, PNorm = 93.4665, GNorm = 0.3655
Meta loss on this task batch = 3.9468e-01, Meta loss averaged over last 500 steps = 3.5973e-01, PNorm = 93.4746, GNorm = 0.2914
Meta loss on this task batch = 3.6325e-01, Meta loss averaged over last 500 steps = 3.5982e-01, PNorm = 93.4827, GNorm = 0.2927
Took 114.04843401908875 seconds to complete one epoch of meta training
Took 122.16726684570312 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472712
Epoch 191
Meta loss on this task batch = 3.4244e-01, Meta loss averaged over last 500 steps = 3.5989e-01, PNorm = 93.4912, GNorm = 0.2044
Meta loss on this task batch = 3.9033e-01, Meta loss averaged over last 500 steps = 3.5991e-01, PNorm = 93.5000, GNorm = 0.2602
Meta loss on this task batch = 3.8244e-01, Meta loss averaged over last 500 steps = 3.5996e-01, PNorm = 93.5092, GNorm = 0.2415
Meta loss on this task batch = 3.5992e-01, Meta loss averaged over last 500 steps = 3.5983e-01, PNorm = 93.5185, GNorm = 0.2526
Meta loss on this task batch = 3.8846e-01, Meta loss averaged over last 500 steps = 3.5987e-01, PNorm = 93.5275, GNorm = 0.4057
Meta loss on this task batch = 3.3501e-01, Meta loss averaged over last 500 steps = 3.5965e-01, PNorm = 93.5357, GNorm = 0.4898
Meta loss on this task batch = 3.1044e-01, Meta loss averaged over last 500 steps = 3.5962e-01, PNorm = 93.5451, GNorm = 0.2556
Meta loss on this task batch = 3.0359e-01, Meta loss averaged over last 500 steps = 3.5942e-01, PNorm = 93.5552, GNorm = 0.2246
Meta loss on this task batch = 3.1154e-01, Meta loss averaged over last 500 steps = 3.5930e-01, PNorm = 93.5646, GNorm = 0.2476
Meta loss on this task batch = 3.5401e-01, Meta loss averaged over last 500 steps = 3.5927e-01, PNorm = 93.5743, GNorm = 0.3317
Meta loss on this task batch = 3.1666e-01, Meta loss averaged over last 500 steps = 3.5923e-01, PNorm = 93.5849, GNorm = 0.2709
Meta loss on this task batch = 3.9187e-01, Meta loss averaged over last 500 steps = 3.5922e-01, PNorm = 93.5945, GNorm = 0.4096
Meta loss on this task batch = 3.2185e-01, Meta loss averaged over last 500 steps = 3.5919e-01, PNorm = 93.6047, GNorm = 0.2478
Meta loss on this task batch = 3.4978e-01, Meta loss averaged over last 500 steps = 3.5919e-01, PNorm = 93.6143, GNorm = 0.2665
Meta loss on this task batch = 4.3034e-01, Meta loss averaged over last 500 steps = 3.5930e-01, PNorm = 93.6236, GNorm = 0.3231
Meta loss on this task batch = 3.1091e-01, Meta loss averaged over last 500 steps = 3.5922e-01, PNorm = 93.6336, GNorm = 0.3132
Meta loss on this task batch = 4.0338e-01, Meta loss averaged over last 500 steps = 3.5932e-01, PNorm = 93.6427, GNorm = 0.3000
Meta loss on this task batch = 3.6088e-01, Meta loss averaged over last 500 steps = 3.5930e-01, PNorm = 93.6522, GNorm = 0.2378
Meta loss on this task batch = 3.4603e-01, Meta loss averaged over last 500 steps = 3.5926e-01, PNorm = 93.6638, GNorm = 0.6447
Took 113.5353012084961 seconds to complete one epoch of meta training
Took 121.50877690315247 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474982
Epoch 192
Meta loss on this task batch = 3.5836e-01, Meta loss averaged over last 500 steps = 3.5922e-01, PNorm = 93.6758, GNorm = 0.2169
Meta loss on this task batch = 3.8386e-01, Meta loss averaged over last 500 steps = 3.5927e-01, PNorm = 93.6878, GNorm = 0.2452
Meta loss on this task batch = 4.0266e-01, Meta loss averaged over last 500 steps = 3.5935e-01, PNorm = 93.6999, GNorm = 0.3492
Meta loss on this task batch = 3.2584e-01, Meta loss averaged over last 500 steps = 3.5927e-01, PNorm = 93.7124, GNorm = 0.3059
Meta loss on this task batch = 3.3737e-01, Meta loss averaged over last 500 steps = 3.5931e-01, PNorm = 93.7258, GNorm = 0.2356
Meta loss on this task batch = 3.5094e-01, Meta loss averaged over last 500 steps = 3.5918e-01, PNorm = 93.7385, GNorm = 0.2768
Meta loss on this task batch = 4.0761e-01, Meta loss averaged over last 500 steps = 3.5929e-01, PNorm = 93.7503, GNorm = 0.2594
Meta loss on this task batch = 3.5600e-01, Meta loss averaged over last 500 steps = 3.5925e-01, PNorm = 93.7615, GNorm = 0.2265
Meta loss on this task batch = 3.5437e-01, Meta loss averaged over last 500 steps = 3.5917e-01, PNorm = 93.7725, GNorm = 0.4135
Meta loss on this task batch = 3.1787e-01, Meta loss averaged over last 500 steps = 3.5902e-01, PNorm = 93.7838, GNorm = 0.2537
Meta loss on this task batch = 3.2719e-01, Meta loss averaged over last 500 steps = 3.5890e-01, PNorm = 93.7947, GNorm = 0.2665
Meta loss on this task batch = 3.3812e-01, Meta loss averaged over last 500 steps = 3.5881e-01, PNorm = 93.8048, GNorm = 0.3047
Meta loss on this task batch = 3.5341e-01, Meta loss averaged over last 500 steps = 3.5884e-01, PNorm = 93.8141, GNorm = 0.2737
Meta loss on this task batch = 4.4002e-01, Meta loss averaged over last 500 steps = 3.5906e-01, PNorm = 93.8217, GNorm = 0.4321
Meta loss on this task batch = 3.0703e-01, Meta loss averaged over last 500 steps = 3.5901e-01, PNorm = 93.8295, GNorm = 0.2310
Meta loss on this task batch = 3.4609e-01, Meta loss averaged over last 500 steps = 3.5907e-01, PNorm = 93.8369, GNorm = 0.2715
Meta loss on this task batch = 3.4631e-01, Meta loss averaged over last 500 steps = 3.5901e-01, PNorm = 93.8433, GNorm = 0.2509
Meta loss on this task batch = 2.8684e-01, Meta loss averaged over last 500 steps = 3.5879e-01, PNorm = 93.8505, GNorm = 0.3163
Meta loss on this task batch = 2.5581e-01, Meta loss averaged over last 500 steps = 3.5856e-01, PNorm = 93.8578, GNorm = 0.2082
Took 112.37406420707703 seconds to complete one epoch of meta training
Took 120.30024576187134 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468186
Epoch 193
Meta loss on this task batch = 3.8347e-01, Meta loss averaged over last 500 steps = 3.5866e-01, PNorm = 93.8658, GNorm = 0.2897
Meta loss on this task batch = 3.6599e-01, Meta loss averaged over last 500 steps = 3.5872e-01, PNorm = 93.8751, GNorm = 0.2280
Meta loss on this task batch = 3.2232e-01, Meta loss averaged over last 500 steps = 3.5869e-01, PNorm = 93.8851, GNorm = 0.2636
Meta loss on this task batch = 3.3913e-01, Meta loss averaged over last 500 steps = 3.5861e-01, PNorm = 93.8953, GNorm = 0.2702
Meta loss on this task batch = 3.5135e-01, Meta loss averaged over last 500 steps = 3.5839e-01, PNorm = 93.9062, GNorm = 0.2565
Meta loss on this task batch = 4.4037e-01, Meta loss averaged over last 500 steps = 3.5864e-01, PNorm = 93.9160, GNorm = 0.2867
Meta loss on this task batch = 3.7846e-01, Meta loss averaged over last 500 steps = 3.5870e-01, PNorm = 93.9259, GNorm = 0.2389
Meta loss on this task batch = 2.9125e-01, Meta loss averaged over last 500 steps = 3.5852e-01, PNorm = 93.9355, GNorm = 0.1992
Meta loss on this task batch = 3.6474e-01, Meta loss averaged over last 500 steps = 3.5839e-01, PNorm = 93.9452, GNorm = 0.2539
Meta loss on this task batch = 3.0535e-01, Meta loss averaged over last 500 steps = 3.5827e-01, PNorm = 93.9559, GNorm = 0.2785
Meta loss on this task batch = 3.1749e-01, Meta loss averaged over last 500 steps = 3.5819e-01, PNorm = 93.9660, GNorm = 0.2168
Meta loss on this task batch = 3.0257e-01, Meta loss averaged over last 500 steps = 3.5798e-01, PNorm = 93.9757, GNorm = 0.2837
Meta loss on this task batch = 3.5683e-01, Meta loss averaged over last 500 steps = 3.5818e-01, PNorm = 93.9854, GNorm = 0.2531
Meta loss on this task batch = 3.6483e-01, Meta loss averaged over last 500 steps = 3.5823e-01, PNorm = 93.9953, GNorm = 0.2554
Meta loss on this task batch = 3.9088e-01, Meta loss averaged over last 500 steps = 3.5827e-01, PNorm = 94.0056, GNorm = 0.2695
Meta loss on this task batch = 3.3832e-01, Meta loss averaged over last 500 steps = 3.5818e-01, PNorm = 94.0153, GNorm = 0.3711
Meta loss on this task batch = 3.5871e-01, Meta loss averaged over last 500 steps = 3.5819e-01, PNorm = 94.0244, GNorm = 0.2191
Meta loss on this task batch = 3.0945e-01, Meta loss averaged over last 500 steps = 3.5820e-01, PNorm = 94.0336, GNorm = 0.2324
Meta loss on this task batch = 3.7517e-01, Meta loss averaged over last 500 steps = 3.5821e-01, PNorm = 94.0418, GNorm = 0.3148
Took 139.25463366508484 seconds to complete one epoch of meta training
Took 146.14545488357544 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463643
Epoch 194
Meta loss on this task batch = 3.9183e-01, Meta loss averaged over last 500 steps = 3.5817e-01, PNorm = 94.0497, GNorm = 0.2445
Meta loss on this task batch = 3.1438e-01, Meta loss averaged over last 500 steps = 3.5807e-01, PNorm = 94.0578, GNorm = 0.2021
Meta loss on this task batch = 3.7586e-01, Meta loss averaged over last 500 steps = 3.5805e-01, PNorm = 94.0658, GNorm = 0.2673
Meta loss on this task batch = 3.6446e-01, Meta loss averaged over last 500 steps = 3.5805e-01, PNorm = 94.0738, GNorm = 0.2408
Meta loss on this task batch = 3.9049e-01, Meta loss averaged over last 500 steps = 3.5807e-01, PNorm = 94.0818, GNorm = 0.3132
Meta loss on this task batch = 4.0619e-01, Meta loss averaged over last 500 steps = 3.5814e-01, PNorm = 94.0892, GNorm = 0.2566
Meta loss on this task batch = 3.6805e-01, Meta loss averaged over last 500 steps = 3.5808e-01, PNorm = 94.0972, GNorm = 0.3171
Meta loss on this task batch = 3.6020e-01, Meta loss averaged over last 500 steps = 3.5814e-01, PNorm = 94.1060, GNorm = 0.3164
Meta loss on this task batch = 3.3923e-01, Meta loss averaged over last 500 steps = 3.5804e-01, PNorm = 94.1148, GNorm = 0.3567
Meta loss on this task batch = 3.0127e-01, Meta loss averaged over last 500 steps = 3.5798e-01, PNorm = 94.1245, GNorm = 0.2268
Meta loss on this task batch = 3.4223e-01, Meta loss averaged over last 500 steps = 3.5797e-01, PNorm = 94.1341, GNorm = 0.2031
Meta loss on this task batch = 3.3390e-01, Meta loss averaged over last 500 steps = 3.5779e-01, PNorm = 94.1434, GNorm = 0.3764
Meta loss on this task batch = 3.6259e-01, Meta loss averaged over last 500 steps = 3.5778e-01, PNorm = 94.1527, GNorm = 0.3422
Meta loss on this task batch = 3.8265e-01, Meta loss averaged over last 500 steps = 3.5775e-01, PNorm = 94.1622, GNorm = 0.3079
Meta loss on this task batch = 3.3318e-01, Meta loss averaged over last 500 steps = 3.5777e-01, PNorm = 94.1714, GNorm = 0.2787
Meta loss on this task batch = 3.4084e-01, Meta loss averaged over last 500 steps = 3.5772e-01, PNorm = 94.1798, GNorm = 0.3410
Meta loss on this task batch = 3.7196e-01, Meta loss averaged over last 500 steps = 3.5769e-01, PNorm = 94.1867, GNorm = 0.4606
Meta loss on this task batch = 3.1976e-01, Meta loss averaged over last 500 steps = 3.5757e-01, PNorm = 94.1944, GNorm = 0.4748
Meta loss on this task batch = 3.4697e-01, Meta loss averaged over last 500 steps = 3.5756e-01, PNorm = 94.2012, GNorm = 0.3706
Took 116.40053153038025 seconds to complete one epoch of meta training
Took 123.97004270553589 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457363
Epoch 195
Meta loss on this task batch = 2.9060e-01, Meta loss averaged over last 500 steps = 3.5745e-01, PNorm = 94.2087, GNorm = 0.4004
Meta loss on this task batch = 3.8318e-01, Meta loss averaged over last 500 steps = 3.5750e-01, PNorm = 94.2157, GNorm = 0.3064
Meta loss on this task batch = 2.9980e-01, Meta loss averaged over last 500 steps = 3.5738e-01, PNorm = 94.2242, GNorm = 0.2330
Meta loss on this task batch = 4.0399e-01, Meta loss averaged over last 500 steps = 3.5750e-01, PNorm = 94.2324, GNorm = 0.3510
Meta loss on this task batch = 3.5071e-01, Meta loss averaged over last 500 steps = 3.5755e-01, PNorm = 94.2404, GNorm = 0.2430
Meta loss on this task batch = 3.5290e-01, Meta loss averaged over last 500 steps = 3.5746e-01, PNorm = 94.2485, GNorm = 0.3031
Meta loss on this task batch = 3.6557e-01, Meta loss averaged over last 500 steps = 3.5740e-01, PNorm = 94.2557, GNorm = 0.3018
Meta loss on this task batch = 3.1696e-01, Meta loss averaged over last 500 steps = 3.5740e-01, PNorm = 94.2645, GNorm = 0.4880
Meta loss on this task batch = 2.8965e-01, Meta loss averaged over last 500 steps = 3.5726e-01, PNorm = 94.2747, GNorm = 0.2884
Meta loss on this task batch = 3.5020e-01, Meta loss averaged over last 500 steps = 3.5724e-01, PNorm = 94.2845, GNorm = 0.2632
Meta loss on this task batch = 3.1580e-01, Meta loss averaged over last 500 steps = 3.5729e-01, PNorm = 94.2944, GNorm = 0.3445
Meta loss on this task batch = 3.9074e-01, Meta loss averaged over last 500 steps = 3.5731e-01, PNorm = 94.3038, GNorm = 0.2844
Meta loss on this task batch = 3.6958e-01, Meta loss averaged over last 500 steps = 3.5730e-01, PNorm = 94.3129, GNorm = 0.3012
Meta loss on this task batch = 3.8280e-01, Meta loss averaged over last 500 steps = 3.5729e-01, PNorm = 94.3227, GNorm = 0.2429
Meta loss on this task batch = 3.4964e-01, Meta loss averaged over last 500 steps = 3.5725e-01, PNorm = 94.3319, GNorm = 0.3092
Meta loss on this task batch = 3.5808e-01, Meta loss averaged over last 500 steps = 3.5717e-01, PNorm = 94.3414, GNorm = 0.2691
Meta loss on this task batch = 3.9492e-01, Meta loss averaged over last 500 steps = 3.5731e-01, PNorm = 94.3501, GNorm = 0.2899
Meta loss on this task batch = 3.8993e-01, Meta loss averaged over last 500 steps = 3.5734e-01, PNorm = 94.3589, GNorm = 0.2559
Meta loss on this task batch = 3.8811e-01, Meta loss averaged over last 500 steps = 3.5733e-01, PNorm = 94.3680, GNorm = 0.3245
Took 139.01366329193115 seconds to complete one epoch of meta training
Took 146.18347096443176 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462772
Epoch 196
Meta loss on this task batch = 3.5753e-01, Meta loss averaged over last 500 steps = 3.5741e-01, PNorm = 94.3769, GNorm = 0.2757
Meta loss on this task batch = 3.5552e-01, Meta loss averaged over last 500 steps = 3.5749e-01, PNorm = 94.3852, GNorm = 0.2141
Meta loss on this task batch = 2.6518e-01, Meta loss averaged over last 500 steps = 3.5728e-01, PNorm = 94.3940, GNorm = 0.1826
Meta loss on this task batch = 4.1972e-01, Meta loss averaged over last 500 steps = 3.5738e-01, PNorm = 94.4030, GNorm = 0.2353
Meta loss on this task batch = 3.6258e-01, Meta loss averaged over last 500 steps = 3.5738e-01, PNorm = 94.4123, GNorm = 0.4603
Meta loss on this task batch = 3.7711e-01, Meta loss averaged over last 500 steps = 3.5721e-01, PNorm = 94.4211, GNorm = 0.2557
Meta loss on this task batch = 3.8055e-01, Meta loss averaged over last 500 steps = 3.5727e-01, PNorm = 94.4304, GNorm = 0.1974
Meta loss on this task batch = 3.1312e-01, Meta loss averaged over last 500 steps = 3.5730e-01, PNorm = 94.4409, GNorm = 0.2452
Meta loss on this task batch = 3.1441e-01, Meta loss averaged over last 500 steps = 3.5724e-01, PNorm = 94.4522, GNorm = 0.4085
Meta loss on this task batch = 3.6995e-01, Meta loss averaged over last 500 steps = 3.5730e-01, PNorm = 94.4639, GNorm = 0.2312
Meta loss on this task batch = 3.5292e-01, Meta loss averaged over last 500 steps = 3.5724e-01, PNorm = 94.4764, GNorm = 0.3358
Meta loss on this task batch = 3.4041e-01, Meta loss averaged over last 500 steps = 3.5721e-01, PNorm = 94.4884, GNorm = 0.3092
Meta loss on this task batch = 2.9476e-01, Meta loss averaged over last 500 steps = 3.5715e-01, PNorm = 94.5008, GNorm = 0.2320
Meta loss on this task batch = 3.8492e-01, Meta loss averaged over last 500 steps = 3.5721e-01, PNorm = 94.5129, GNorm = 0.3426
Meta loss on this task batch = 3.7426e-01, Meta loss averaged over last 500 steps = 3.5718e-01, PNorm = 94.5243, GNorm = 0.4015
Meta loss on this task batch = 3.6828e-01, Meta loss averaged over last 500 steps = 3.5718e-01, PNorm = 94.5347, GNorm = 0.2917
Meta loss on this task batch = 3.1169e-01, Meta loss averaged over last 500 steps = 3.5709e-01, PNorm = 94.5456, GNorm = 0.3916
Meta loss on this task batch = 3.6714e-01, Meta loss averaged over last 500 steps = 3.5709e-01, PNorm = 94.5567, GNorm = 0.2618
Meta loss on this task batch = 3.2116e-01, Meta loss averaged over last 500 steps = 3.5694e-01, PNorm = 94.5679, GNorm = 0.2689
Took 112.87339615821838 seconds to complete one epoch of meta training
Took 120.50637459754944 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449789
Epoch 197
Meta loss on this task batch = 3.9153e-01, Meta loss averaged over last 500 steps = 3.5701e-01, PNorm = 94.5790, GNorm = 0.2873
Meta loss on this task batch = 3.3120e-01, Meta loss averaged over last 500 steps = 3.5693e-01, PNorm = 94.5903, GNorm = 0.2429
Meta loss on this task batch = 3.5984e-01, Meta loss averaged over last 500 steps = 3.5681e-01, PNorm = 94.6015, GNorm = 0.2655
Meta loss on this task batch = 3.4609e-01, Meta loss averaged over last 500 steps = 3.5675e-01, PNorm = 94.6132, GNorm = 0.3378
Meta loss on this task batch = 3.7500e-01, Meta loss averaged over last 500 steps = 3.5683e-01, PNorm = 94.6249, GNorm = 0.2525
Meta loss on this task batch = 2.9362e-01, Meta loss averaged over last 500 steps = 3.5655e-01, PNorm = 94.6378, GNorm = 0.2121
Meta loss on this task batch = 3.5349e-01, Meta loss averaged over last 500 steps = 3.5661e-01, PNorm = 94.6500, GNorm = 0.3083
Meta loss on this task batch = 3.0100e-01, Meta loss averaged over last 500 steps = 3.5638e-01, PNorm = 94.6617, GNorm = 0.2154
Meta loss on this task batch = 3.5868e-01, Meta loss averaged over last 500 steps = 3.5626e-01, PNorm = 94.6729, GNorm = 0.2376
Meta loss on this task batch = 3.3537e-01, Meta loss averaged over last 500 steps = 3.5621e-01, PNorm = 94.6841, GNorm = 0.2330
Meta loss on this task batch = 3.4372e-01, Meta loss averaged over last 500 steps = 3.5616e-01, PNorm = 94.6949, GNorm = 0.2318
Meta loss on this task batch = 3.6621e-01, Meta loss averaged over last 500 steps = 3.5624e-01, PNorm = 94.7054, GNorm = 0.2860
Meta loss on this task batch = 3.2241e-01, Meta loss averaged over last 500 steps = 3.5611e-01, PNorm = 94.7158, GNorm = 0.2271
Meta loss on this task batch = 3.9377e-01, Meta loss averaged over last 500 steps = 3.5624e-01, PNorm = 94.7258, GNorm = 0.2043
Meta loss on this task batch = 3.2934e-01, Meta loss averaged over last 500 steps = 3.5613e-01, PNorm = 94.7356, GNorm = 0.3275
Meta loss on this task batch = 3.6263e-01, Meta loss averaged over last 500 steps = 3.5615e-01, PNorm = 94.7442, GNorm = 0.2527
Meta loss on this task batch = 3.5987e-01, Meta loss averaged over last 500 steps = 3.5615e-01, PNorm = 94.7523, GNorm = 0.3454
Meta loss on this task batch = 4.0255e-01, Meta loss averaged over last 500 steps = 3.5638e-01, PNorm = 94.7599, GNorm = 0.3052
Meta loss on this task batch = 3.6390e-01, Meta loss averaged over last 500 steps = 3.5648e-01, PNorm = 94.7677, GNorm = 0.3999
Took 146.92804741859436 seconds to complete one epoch of meta training
Took 154.88364553451538 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466506
Epoch 198
Meta loss on this task batch = 3.6357e-01, Meta loss averaged over last 500 steps = 3.5647e-01, PNorm = 94.7748, GNorm = 0.3157
Meta loss on this task batch = 3.6392e-01, Meta loss averaged over last 500 steps = 3.5641e-01, PNorm = 94.7817, GNorm = 0.2754
Meta loss on this task batch = 3.9457e-01, Meta loss averaged over last 500 steps = 3.5651e-01, PNorm = 94.7886, GNorm = 0.2423
Meta loss on this task batch = 3.8041e-01, Meta loss averaged over last 500 steps = 3.5639e-01, PNorm = 94.7950, GNorm = 0.3045
Meta loss on this task batch = 3.0827e-01, Meta loss averaged over last 500 steps = 3.5638e-01, PNorm = 94.8021, GNorm = 0.2332
Meta loss on this task batch = 3.0203e-01, Meta loss averaged over last 500 steps = 3.5621e-01, PNorm = 94.8103, GNorm = 0.2359
Meta loss on this task batch = 3.6645e-01, Meta loss averaged over last 500 steps = 3.5627e-01, PNorm = 94.8187, GNorm = 0.2159
Meta loss on this task batch = 3.0604e-01, Meta loss averaged over last 500 steps = 3.5616e-01, PNorm = 94.8279, GNorm = 0.2753
Meta loss on this task batch = 3.2542e-01, Meta loss averaged over last 500 steps = 3.5612e-01, PNorm = 94.8374, GNorm = 0.2189
Meta loss on this task batch = 3.6828e-01, Meta loss averaged over last 500 steps = 3.5608e-01, PNorm = 94.8473, GNorm = 0.2955
Meta loss on this task batch = 3.6979e-01, Meta loss averaged over last 500 steps = 3.5610e-01, PNorm = 94.8580, GNorm = 0.3150
Meta loss on this task batch = 2.9972e-01, Meta loss averaged over last 500 steps = 3.5599e-01, PNorm = 94.8690, GNorm = 0.2114
Meta loss on this task batch = 3.3989e-01, Meta loss averaged over last 500 steps = 3.5595e-01, PNorm = 94.8805, GNorm = 0.2601
Meta loss on this task batch = 3.4011e-01, Meta loss averaged over last 500 steps = 3.5595e-01, PNorm = 94.8918, GNorm = 0.2347
Meta loss on this task batch = 3.4055e-01, Meta loss averaged over last 500 steps = 3.5594e-01, PNorm = 94.9025, GNorm = 0.2694
Meta loss on this task batch = 4.0911e-01, Meta loss averaged over last 500 steps = 3.5597e-01, PNorm = 94.9124, GNorm = 0.2478
Meta loss on this task batch = 3.9544e-01, Meta loss averaged over last 500 steps = 3.5598e-01, PNorm = 94.9219, GNorm = 0.4639
Meta loss on this task batch = 3.5145e-01, Meta loss averaged over last 500 steps = 3.5598e-01, PNorm = 94.9309, GNorm = 0.2990
Meta loss on this task batch = 3.4347e-01, Meta loss averaged over last 500 steps = 3.5593e-01, PNorm = 94.9407, GNorm = 0.3364
Took 109.7349579334259 seconds to complete one epoch of meta training
Took 117.4058210849762 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473053
Epoch 199
Meta loss on this task batch = 3.1964e-01, Meta loss averaged over last 500 steps = 3.5568e-01, PNorm = 94.9509, GNorm = 0.2747
Meta loss on this task batch = 3.5594e-01, Meta loss averaged over last 500 steps = 3.5577e-01, PNorm = 94.9606, GNorm = 0.2464
Meta loss on this task batch = 3.6341e-01, Meta loss averaged over last 500 steps = 3.5576e-01, PNorm = 94.9696, GNorm = 0.3043
Meta loss on this task batch = 4.0714e-01, Meta loss averaged over last 500 steps = 3.5578e-01, PNorm = 94.9785, GNorm = 0.2557
Meta loss on this task batch = 4.0256e-01, Meta loss averaged over last 500 steps = 3.5596e-01, PNorm = 94.9869, GNorm = 0.2418
Meta loss on this task batch = 3.1324e-01, Meta loss averaged over last 500 steps = 3.5589e-01, PNorm = 94.9958, GNorm = 0.2891
Meta loss on this task batch = 3.8884e-01, Meta loss averaged over last 500 steps = 3.5586e-01, PNorm = 95.0041, GNorm = 0.2320
Meta loss on this task batch = 3.5636e-01, Meta loss averaged over last 500 steps = 3.5596e-01, PNorm = 95.0126, GNorm = 0.2698
Meta loss on this task batch = 3.7966e-01, Meta loss averaged over last 500 steps = 3.5599e-01, PNorm = 95.0215, GNorm = 0.4029
Meta loss on this task batch = 3.5546e-01, Meta loss averaged over last 500 steps = 3.5593e-01, PNorm = 95.0308, GNorm = 0.2727
Meta loss on this task batch = 3.8179e-01, Meta loss averaged over last 500 steps = 3.5593e-01, PNorm = 95.0395, GNorm = 0.2890
Meta loss on this task batch = 3.7059e-01, Meta loss averaged over last 500 steps = 3.5599e-01, PNorm = 95.0479, GNorm = 0.3484
Meta loss on this task batch = 2.8270e-01, Meta loss averaged over last 500 steps = 3.5590e-01, PNorm = 95.0569, GNorm = 0.1920
Meta loss on this task batch = 3.2509e-01, Meta loss averaged over last 500 steps = 3.5589e-01, PNorm = 95.0662, GNorm = 0.3116
Meta loss on this task batch = 3.1787e-01, Meta loss averaged over last 500 steps = 3.5578e-01, PNorm = 95.0760, GNorm = 0.2255
Meta loss on this task batch = 3.8365e-01, Meta loss averaged over last 500 steps = 3.5585e-01, PNorm = 95.0856, GNorm = 0.2367
Meta loss on this task batch = 3.4468e-01, Meta loss averaged over last 500 steps = 3.5586e-01, PNorm = 95.0953, GNorm = 0.2833
Meta loss on this task batch = 3.7564e-01, Meta loss averaged over last 500 steps = 3.5590e-01, PNorm = 95.1049, GNorm = 0.3583
Meta loss on this task batch = 2.6822e-01, Meta loss averaged over last 500 steps = 3.5567e-01, PNorm = 95.1147, GNorm = 0.2508
Took 115.3848307132721 seconds to complete one epoch of meta training
Took 123.17705416679382 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468260
Epoch 200
Meta loss on this task batch = 3.3282e-01, Meta loss averaged over last 500 steps = 3.5564e-01, PNorm = 95.1248, GNorm = 0.2877
Meta loss on this task batch = 3.6272e-01, Meta loss averaged over last 500 steps = 3.5544e-01, PNorm = 95.1352, GNorm = 0.2360
Meta loss on this task batch = 3.7346e-01, Meta loss averaged over last 500 steps = 3.5553e-01, PNorm = 95.1449, GNorm = 0.3258
Meta loss on this task batch = 3.6597e-01, Meta loss averaged over last 500 steps = 3.5554e-01, PNorm = 95.1548, GNorm = 0.3421
Meta loss on this task batch = 4.0276e-01, Meta loss averaged over last 500 steps = 3.5565e-01, PNorm = 95.1642, GNorm = 0.6021
Meta loss on this task batch = 3.6952e-01, Meta loss averaged over last 500 steps = 3.5560e-01, PNorm = 95.1732, GNorm = 0.2543
Meta loss on this task batch = 3.2772e-01, Meta loss averaged over last 500 steps = 3.5558e-01, PNorm = 95.1833, GNorm = 0.2552
Meta loss on this task batch = 3.6315e-01, Meta loss averaged over last 500 steps = 3.5553e-01, PNorm = 95.1940, GNorm = 0.3097
Meta loss on this task batch = 3.6460e-01, Meta loss averaged over last 500 steps = 3.5550e-01, PNorm = 95.2052, GNorm = 0.2574
Meta loss on this task batch = 2.6406e-01, Meta loss averaged over last 500 steps = 3.5532e-01, PNorm = 95.2169, GNorm = 0.2134
Meta loss on this task batch = 3.4461e-01, Meta loss averaged over last 500 steps = 3.5527e-01, PNorm = 95.2275, GNorm = 0.2627
Meta loss on this task batch = 3.9026e-01, Meta loss averaged over last 500 steps = 3.5526e-01, PNorm = 95.2365, GNorm = 0.2532
Meta loss on this task batch = 2.9898e-01, Meta loss averaged over last 500 steps = 3.5509e-01, PNorm = 95.2458, GNorm = 0.2406
Meta loss on this task batch = 3.4723e-01, Meta loss averaged over last 500 steps = 3.5503e-01, PNorm = 95.2557, GNorm = 0.2567
Meta loss on this task batch = 3.6272e-01, Meta loss averaged over last 500 steps = 3.5502e-01, PNorm = 95.2646, GNorm = 0.2809
Meta loss on this task batch = 3.5754e-01, Meta loss averaged over last 500 steps = 3.5500e-01, PNorm = 95.2725, GNorm = 0.3298
Meta loss on this task batch = 3.5542e-01, Meta loss averaged over last 500 steps = 3.5498e-01, PNorm = 95.2808, GNorm = 0.2095
Meta loss on this task batch = 3.4148e-01, Meta loss averaged over last 500 steps = 3.5489e-01, PNorm = 95.2892, GNorm = 0.2228
Meta loss on this task batch = 3.4221e-01, Meta loss averaged over last 500 steps = 3.5483e-01, PNorm = 95.2979, GNorm = 0.2680
Took 111.53822422027588 seconds to complete one epoch of meta training
Took 119.11843514442444 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467597
Epoch 201
Meta loss on this task batch = 3.5563e-01, Meta loss averaged over last 500 steps = 3.5477e-01, PNorm = 95.3079, GNorm = 0.2380
Meta loss on this task batch = 3.2984e-01, Meta loss averaged over last 500 steps = 3.5472e-01, PNorm = 95.3172, GNorm = 0.2380
Meta loss on this task batch = 3.8626e-01, Meta loss averaged over last 500 steps = 3.5479e-01, PNorm = 95.3265, GNorm = 0.2542
Meta loss on this task batch = 3.0996e-01, Meta loss averaged over last 500 steps = 3.5472e-01, PNorm = 95.3361, GNorm = 0.2129
Meta loss on this task batch = 3.3103e-01, Meta loss averaged over last 500 steps = 3.5473e-01, PNorm = 95.3455, GNorm = 0.2204
Meta loss on this task batch = 3.5389e-01, Meta loss averaged over last 500 steps = 3.5467e-01, PNorm = 95.3544, GNorm = 0.2082
Meta loss on this task batch = 3.1187e-01, Meta loss averaged over last 500 steps = 3.5452e-01, PNorm = 95.3635, GNorm = 0.3105
Meta loss on this task batch = 3.6442e-01, Meta loss averaged over last 500 steps = 3.5439e-01, PNorm = 95.3716, GNorm = 0.3592
Meta loss on this task batch = 2.9755e-01, Meta loss averaged over last 500 steps = 3.5426e-01, PNorm = 95.3798, GNorm = 0.2344
Meta loss on this task batch = 3.3061e-01, Meta loss averaged over last 500 steps = 3.5412e-01, PNorm = 95.3880, GNorm = 0.2381
Meta loss on this task batch = 3.1927e-01, Meta loss averaged over last 500 steps = 3.5389e-01, PNorm = 95.3955, GNorm = 0.3200
Meta loss on this task batch = 3.7072e-01, Meta loss averaged over last 500 steps = 3.5392e-01, PNorm = 95.4028, GNorm = 0.2961
Meta loss on this task batch = 3.4186e-01, Meta loss averaged over last 500 steps = 3.5396e-01, PNorm = 95.4099, GNorm = 0.2284
Meta loss on this task batch = 3.7244e-01, Meta loss averaged over last 500 steps = 3.5405e-01, PNorm = 95.4170, GNorm = 0.2924
Meta loss on this task batch = 3.4541e-01, Meta loss averaged over last 500 steps = 3.5411e-01, PNorm = 95.4234, GNorm = 0.4497
Meta loss on this task batch = 3.8907e-01, Meta loss averaged over last 500 steps = 3.5417e-01, PNorm = 95.4294, GNorm = 0.3323
Meta loss on this task batch = 3.5742e-01, Meta loss averaged over last 500 steps = 3.5413e-01, PNorm = 95.4362, GNorm = 0.3436
Meta loss on this task batch = 3.6619e-01, Meta loss averaged over last 500 steps = 3.5417e-01, PNorm = 95.4434, GNorm = 0.3386
Meta loss on this task batch = 3.9691e-01, Meta loss averaged over last 500 steps = 3.5425e-01, PNorm = 95.4505, GNorm = 0.2796
Took 109.45345520973206 seconds to complete one epoch of meta training
Took 117.11749219894409 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467877
Epoch 202
Meta loss on this task batch = 3.6556e-01, Meta loss averaged over last 500 steps = 3.5423e-01, PNorm = 95.4588, GNorm = 0.4136
Meta loss on this task batch = 3.0221e-01, Meta loss averaged over last 500 steps = 3.5409e-01, PNorm = 95.4680, GNorm = 0.3752
Meta loss on this task batch = 3.4630e-01, Meta loss averaged over last 500 steps = 3.5404e-01, PNorm = 95.4777, GNorm = 0.4522
Meta loss on this task batch = 3.3817e-01, Meta loss averaged over last 500 steps = 3.5399e-01, PNorm = 95.4872, GNorm = 0.2248
Meta loss on this task batch = 3.8495e-01, Meta loss averaged over last 500 steps = 3.5406e-01, PNorm = 95.4974, GNorm = 0.4019
Meta loss on this task batch = 3.7831e-01, Meta loss averaged over last 500 steps = 3.5395e-01, PNorm = 95.5066, GNorm = 0.2444
Meta loss on this task batch = 3.1806e-01, Meta loss averaged over last 500 steps = 3.5393e-01, PNorm = 95.5163, GNorm = 0.3226
Meta loss on this task batch = 3.7484e-01, Meta loss averaged over last 500 steps = 3.5395e-01, PNorm = 95.5265, GNorm = 0.3162
Meta loss on this task batch = 3.3193e-01, Meta loss averaged over last 500 steps = 3.5386e-01, PNorm = 95.5371, GNorm = 0.2394
Meta loss on this task batch = 3.8529e-01, Meta loss averaged over last 500 steps = 3.5387e-01, PNorm = 95.5477, GNorm = 0.2559
Meta loss on this task batch = 3.2906e-01, Meta loss averaged over last 500 steps = 3.5381e-01, PNorm = 95.5584, GNorm = 0.2335
Meta loss on this task batch = 3.3195e-01, Meta loss averaged over last 500 steps = 3.5376e-01, PNorm = 95.5682, GNorm = 0.2588
Meta loss on this task batch = 3.1670e-01, Meta loss averaged over last 500 steps = 3.5363e-01, PNorm = 95.5775, GNorm = 0.2828
Meta loss on this task batch = 3.0330e-01, Meta loss averaged over last 500 steps = 3.5338e-01, PNorm = 95.5862, GNorm = 0.2831
Meta loss on this task batch = 3.5071e-01, Meta loss averaged over last 500 steps = 3.5345e-01, PNorm = 95.5948, GNorm = 0.2297
Meta loss on this task batch = 3.5116e-01, Meta loss averaged over last 500 steps = 3.5341e-01, PNorm = 95.6033, GNorm = 0.2620
Meta loss on this task batch = 3.5595e-01, Meta loss averaged over last 500 steps = 3.5345e-01, PNorm = 95.6130, GNorm = 0.2613
Meta loss on this task batch = 4.1766e-01, Meta loss averaged over last 500 steps = 3.5365e-01, PNorm = 95.6227, GNorm = 0.2461
Meta loss on this task batch = 3.7842e-01, Meta loss averaged over last 500 steps = 3.5368e-01, PNorm = 95.6326, GNorm = 0.2960
Took 111.98267865180969 seconds to complete one epoch of meta training
Took 120.18095326423645 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467484
Epoch 203
Meta loss on this task batch = 3.7279e-01, Meta loss averaged over last 500 steps = 3.5374e-01, PNorm = 95.6421, GNorm = 0.2025
Meta loss on this task batch = 3.5036e-01, Meta loss averaged over last 500 steps = 3.5372e-01, PNorm = 95.6527, GNorm = 0.3513
Meta loss on this task batch = 2.8138e-01, Meta loss averaged over last 500 steps = 3.5365e-01, PNorm = 95.6625, GNorm = 0.2876
Meta loss on this task batch = 3.7386e-01, Meta loss averaged over last 500 steps = 3.5369e-01, PNorm = 95.6721, GNorm = 0.3955
Meta loss on this task batch = 3.3238e-01, Meta loss averaged over last 500 steps = 3.5363e-01, PNorm = 95.6808, GNorm = 0.2385
Meta loss on this task batch = 3.8218e-01, Meta loss averaged over last 500 steps = 3.5357e-01, PNorm = 95.6887, GNorm = 0.4631
Meta loss on this task batch = 3.1929e-01, Meta loss averaged over last 500 steps = 3.5352e-01, PNorm = 95.6968, GNorm = 0.2564
Meta loss on this task batch = 3.6404e-01, Meta loss averaged over last 500 steps = 3.5351e-01, PNorm = 95.7052, GNorm = 0.1998
Meta loss on this task batch = 3.2809e-01, Meta loss averaged over last 500 steps = 3.5347e-01, PNorm = 95.7148, GNorm = 0.2704
Meta loss on this task batch = 3.8629e-01, Meta loss averaged over last 500 steps = 3.5353e-01, PNorm = 95.7243, GNorm = 0.5318
Meta loss on this task batch = 3.3356e-01, Meta loss averaged over last 500 steps = 3.5341e-01, PNorm = 95.7343, GNorm = 0.2133
Meta loss on this task batch = 3.2818e-01, Meta loss averaged over last 500 steps = 3.5334e-01, PNorm = 95.7444, GNorm = 0.2248
Meta loss on this task batch = 3.2743e-01, Meta loss averaged over last 500 steps = 3.5340e-01, PNorm = 95.7551, GNorm = 0.2271
Meta loss on this task batch = 3.3505e-01, Meta loss averaged over last 500 steps = 3.5316e-01, PNorm = 95.7662, GNorm = 0.2505
Meta loss on this task batch = 3.8990e-01, Meta loss averaged over last 500 steps = 3.5317e-01, PNorm = 95.7775, GNorm = 0.2377
Meta loss on this task batch = 2.8564e-01, Meta loss averaged over last 500 steps = 3.5311e-01, PNorm = 95.7895, GNorm = 0.4425
Meta loss on this task batch = 3.1742e-01, Meta loss averaged over last 500 steps = 3.5296e-01, PNorm = 95.8011, GNorm = 0.4722
Meta loss on this task batch = 3.9032e-01, Meta loss averaged over last 500 steps = 3.5306e-01, PNorm = 95.8128, GNorm = 0.2556
Meta loss on this task batch = 3.7076e-01, Meta loss averaged over last 500 steps = 3.5311e-01, PNorm = 95.8239, GNorm = 0.2561
Took 113.2677595615387 seconds to complete one epoch of meta training
Took 121.18836283683777 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481556
Epoch 204
Meta loss on this task batch = 3.7152e-01, Meta loss averaged over last 500 steps = 3.5322e-01, PNorm = 95.8346, GNorm = 0.2541
Meta loss on this task batch = 3.6763e-01, Meta loss averaged over last 500 steps = 3.5323e-01, PNorm = 95.8455, GNorm = 0.3232
Meta loss on this task batch = 3.5686e-01, Meta loss averaged over last 500 steps = 3.5328e-01, PNorm = 95.8554, GNorm = 0.2805
Meta loss on this task batch = 3.8086e-01, Meta loss averaged over last 500 steps = 3.5331e-01, PNorm = 95.8634, GNorm = 0.3256
Meta loss on this task batch = 3.4204e-01, Meta loss averaged over last 500 steps = 3.5329e-01, PNorm = 95.8710, GNorm = 0.2627
Meta loss on this task batch = 3.8033e-01, Meta loss averaged over last 500 steps = 3.5327e-01, PNorm = 95.8782, GNorm = 0.3169
Meta loss on this task batch = 3.0387e-01, Meta loss averaged over last 500 steps = 3.5321e-01, PNorm = 95.8860, GNorm = 0.2279
Meta loss on this task batch = 3.4202e-01, Meta loss averaged over last 500 steps = 3.5313e-01, PNorm = 95.8941, GNorm = 0.2587
Meta loss on this task batch = 3.3414e-01, Meta loss averaged over last 500 steps = 3.5304e-01, PNorm = 95.9029, GNorm = 0.3124
Meta loss on this task batch = 3.4614e-01, Meta loss averaged over last 500 steps = 3.5282e-01, PNorm = 95.9123, GNorm = 0.2327
Meta loss on this task batch = 3.1182e-01, Meta loss averaged over last 500 steps = 3.5286e-01, PNorm = 95.9214, GNorm = 0.2478
Meta loss on this task batch = 3.6163e-01, Meta loss averaged over last 500 steps = 3.5287e-01, PNorm = 95.9310, GNorm = 0.2595
Meta loss on this task batch = 3.3785e-01, Meta loss averaged over last 500 steps = 3.5286e-01, PNorm = 95.9407, GNorm = 0.2112
Meta loss on this task batch = 3.4713e-01, Meta loss averaged over last 500 steps = 3.5301e-01, PNorm = 95.9525, GNorm = 0.3228
Meta loss on this task batch = 3.5543e-01, Meta loss averaged over last 500 steps = 3.5291e-01, PNorm = 95.9633, GNorm = 0.3107
Meta loss on this task batch = 3.2364e-01, Meta loss averaged over last 500 steps = 3.5280e-01, PNorm = 95.9743, GNorm = 0.2170
Meta loss on this task batch = 3.2091e-01, Meta loss averaged over last 500 steps = 3.5280e-01, PNorm = 95.9858, GNorm = 0.3062
Meta loss on this task batch = 3.9895e-01, Meta loss averaged over last 500 steps = 3.5286e-01, PNorm = 95.9971, GNorm = 0.2750
Meta loss on this task batch = 3.4470e-01, Meta loss averaged over last 500 steps = 3.5288e-01, PNorm = 96.0072, GNorm = 0.3965
Took 114.2354884147644 seconds to complete one epoch of meta training
Took 122.09501791000366 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467562
Epoch 205
Meta loss on this task batch = 3.3399e-01, Meta loss averaged over last 500 steps = 3.5291e-01, PNorm = 96.0168, GNorm = 0.3717
Meta loss on this task batch = 3.0239e-01, Meta loss averaged over last 500 steps = 3.5285e-01, PNorm = 96.0264, GNorm = 0.2235
Meta loss on this task batch = 3.6467e-01, Meta loss averaged over last 500 steps = 3.5283e-01, PNorm = 96.0355, GNorm = 0.3036
Meta loss on this task batch = 4.2633e-01, Meta loss averaged over last 500 steps = 3.5294e-01, PNorm = 96.0444, GNorm = 0.5161
Meta loss on this task batch = 3.3486e-01, Meta loss averaged over last 500 steps = 3.5294e-01, PNorm = 96.0524, GNorm = 0.3209
Meta loss on this task batch = 3.8728e-01, Meta loss averaged over last 500 steps = 3.5296e-01, PNorm = 96.0605, GNorm = 0.2702
Meta loss on this task batch = 3.8856e-01, Meta loss averaged over last 500 steps = 3.5311e-01, PNorm = 96.0679, GNorm = 0.3191
Meta loss on this task batch = 2.9354e-01, Meta loss averaged over last 500 steps = 3.5280e-01, PNorm = 96.0757, GNorm = 0.2280
Meta loss on this task batch = 3.4119e-01, Meta loss averaged over last 500 steps = 3.5274e-01, PNorm = 96.0841, GNorm = 0.3898
Meta loss on this task batch = 3.2016e-01, Meta loss averaged over last 500 steps = 3.5263e-01, PNorm = 96.0933, GNorm = 0.2144
Meta loss on this task batch = 2.9926e-01, Meta loss averaged over last 500 steps = 3.5253e-01, PNorm = 96.1033, GNorm = 0.1921
Meta loss on this task batch = 3.3954e-01, Meta loss averaged over last 500 steps = 3.5245e-01, PNorm = 96.1129, GNorm = 0.2373
Meta loss on this task batch = 3.5561e-01, Meta loss averaged over last 500 steps = 3.5248e-01, PNorm = 96.1215, GNorm = 0.2680
Meta loss on this task batch = 2.9151e-01, Meta loss averaged over last 500 steps = 3.5229e-01, PNorm = 96.1308, GNorm = 0.2095
Meta loss on this task batch = 3.9768e-01, Meta loss averaged over last 500 steps = 3.5240e-01, PNorm = 96.1393, GNorm = 0.3091
Meta loss on this task batch = 3.4385e-01, Meta loss averaged over last 500 steps = 3.5240e-01, PNorm = 96.1481, GNorm = 0.3082
Meta loss on this task batch = 3.8138e-01, Meta loss averaged over last 500 steps = 3.5251e-01, PNorm = 96.1565, GNorm = 0.5445
Meta loss on this task batch = 3.8236e-01, Meta loss averaged over last 500 steps = 3.5255e-01, PNorm = 96.1660, GNorm = 0.2203
Meta loss on this task batch = 3.4433e-01, Meta loss averaged over last 500 steps = 3.5252e-01, PNorm = 96.1750, GNorm = 0.2576
Took 110.30193543434143 seconds to complete one epoch of meta training
Took 118.26312041282654 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472409
Epoch 206
Meta loss on this task batch = 3.5529e-01, Meta loss averaged over last 500 steps = 3.5255e-01, PNorm = 96.1842, GNorm = 0.2990
Meta loss on this task batch = 3.2086e-01, Meta loss averaged over last 500 steps = 3.5253e-01, PNorm = 96.1941, GNorm = 0.2775
Meta loss on this task batch = 3.5457e-01, Meta loss averaged over last 500 steps = 3.5252e-01, PNorm = 96.2040, GNorm = 0.2576
Meta loss on this task batch = 3.9168e-01, Meta loss averaged over last 500 steps = 3.5255e-01, PNorm = 96.2144, GNorm = 0.2559
Meta loss on this task batch = 3.2093e-01, Meta loss averaged over last 500 steps = 3.5253e-01, PNorm = 96.2250, GNorm = 0.2688
Meta loss on this task batch = 3.1188e-01, Meta loss averaged over last 500 steps = 3.5227e-01, PNorm = 96.2356, GNorm = 0.3150
Meta loss on this task batch = 2.9474e-01, Meta loss averaged over last 500 steps = 3.5213e-01, PNorm = 96.2466, GNorm = 0.2904
Meta loss on this task batch = 3.7226e-01, Meta loss averaged over last 500 steps = 3.5216e-01, PNorm = 96.2584, GNorm = 0.3809
Meta loss on this task batch = 3.5443e-01, Meta loss averaged over last 500 steps = 3.5217e-01, PNorm = 96.2696, GNorm = 0.2404
Meta loss on this task batch = 3.6147e-01, Meta loss averaged over last 500 steps = 3.5221e-01, PNorm = 96.2816, GNorm = 0.3891
Meta loss on this task batch = 2.7774e-01, Meta loss averaged over last 500 steps = 3.5209e-01, PNorm = 96.2948, GNorm = 0.3887
Meta loss on this task batch = 3.0400e-01, Meta loss averaged over last 500 steps = 3.5201e-01, PNorm = 96.3075, GNorm = 0.2366
Meta loss on this task batch = 3.5305e-01, Meta loss averaged over last 500 steps = 3.5210e-01, PNorm = 96.3207, GNorm = 0.2470
Meta loss on this task batch = 4.0304e-01, Meta loss averaged over last 500 steps = 3.5204e-01, PNorm = 96.3327, GNorm = 0.3545
Meta loss on this task batch = 3.0307e-01, Meta loss averaged over last 500 steps = 3.5205e-01, PNorm = 96.3445, GNorm = 0.2500
Meta loss on this task batch = 3.6412e-01, Meta loss averaged over last 500 steps = 3.5199e-01, PNorm = 96.3561, GNorm = 0.4436
Meta loss on this task batch = 3.2968e-01, Meta loss averaged over last 500 steps = 3.5185e-01, PNorm = 96.3670, GNorm = 0.2585
Meta loss on this task batch = 4.0743e-01, Meta loss averaged over last 500 steps = 3.5200e-01, PNorm = 96.3773, GNorm = 0.3216
Meta loss on this task batch = 4.5249e-01, Meta loss averaged over last 500 steps = 3.5220e-01, PNorm = 96.3858, GNorm = 0.5583
Took 111.67602753639221 seconds to complete one epoch of meta training
Took 119.41834807395935 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487434
Epoch 207
Meta loss on this task batch = 4.2605e-01, Meta loss averaged over last 500 steps = 3.5239e-01, PNorm = 96.3916, GNorm = 0.3298
Meta loss on this task batch = 3.3419e-01, Meta loss averaged over last 500 steps = 3.5231e-01, PNorm = 96.3980, GNorm = 0.4230
Meta loss on this task batch = 3.4838e-01, Meta loss averaged over last 500 steps = 3.5235e-01, PNorm = 96.4047, GNorm = 0.4404
Meta loss on this task batch = 3.2491e-01, Meta loss averaged over last 500 steps = 3.5232e-01, PNorm = 96.4120, GNorm = 0.2707
Meta loss on this task batch = 3.2190e-01, Meta loss averaged over last 500 steps = 3.5218e-01, PNorm = 96.4193, GNorm = 0.2414
Meta loss on this task batch = 3.1815e-01, Meta loss averaged over last 500 steps = 3.5212e-01, PNorm = 96.4272, GNorm = 0.2404
Meta loss on this task batch = 3.6955e-01, Meta loss averaged over last 500 steps = 3.5211e-01, PNorm = 96.4354, GNorm = 0.2685
Meta loss on this task batch = 3.4603e-01, Meta loss averaged over last 500 steps = 3.5221e-01, PNorm = 96.4445, GNorm = 0.2175
Meta loss on this task batch = 3.6661e-01, Meta loss averaged over last 500 steps = 3.5223e-01, PNorm = 96.4538, GNorm = 0.2343
Meta loss on this task batch = 3.6682e-01, Meta loss averaged over last 500 steps = 3.5231e-01, PNorm = 96.4634, GNorm = 0.2723
Meta loss on this task batch = 3.3076e-01, Meta loss averaged over last 500 steps = 3.5240e-01, PNorm = 96.4729, GNorm = 0.2440
Meta loss on this task batch = 3.3581e-01, Meta loss averaged over last 500 steps = 3.5230e-01, PNorm = 96.4824, GNorm = 0.3101
Meta loss on this task batch = 3.3418e-01, Meta loss averaged over last 500 steps = 3.5215e-01, PNorm = 96.4916, GNorm = 0.2325
Meta loss on this task batch = 3.6872e-01, Meta loss averaged over last 500 steps = 3.5221e-01, PNorm = 96.5000, GNorm = 0.4087
Meta loss on this task batch = 3.6267e-01, Meta loss averaged over last 500 steps = 3.5233e-01, PNorm = 96.5090, GNorm = 0.2402
Meta loss on this task batch = 3.2119e-01, Meta loss averaged over last 500 steps = 3.5229e-01, PNorm = 96.5184, GNorm = 0.2294
Meta loss on this task batch = 3.4725e-01, Meta loss averaged over last 500 steps = 3.5228e-01, PNorm = 96.5277, GNorm = 0.3003
Meta loss on this task batch = 3.3468e-01, Meta loss averaged over last 500 steps = 3.5214e-01, PNorm = 96.5375, GNorm = 0.2174
Meta loss on this task batch = 4.2823e-01, Meta loss averaged over last 500 steps = 3.5224e-01, PNorm = 96.5468, GNorm = 0.3498
Took 116.73581314086914 seconds to complete one epoch of meta training
Took 124.32231402397156 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496084
Epoch 208
Meta loss on this task batch = 4.3685e-01, Meta loss averaged over last 500 steps = 3.5239e-01, PNorm = 96.5538, GNorm = 0.3158
Meta loss on this task batch = 3.8322e-01, Meta loss averaged over last 500 steps = 3.5242e-01, PNorm = 96.5606, GNorm = 0.5055
Meta loss on this task batch = 3.1304e-01, Meta loss averaged over last 500 steps = 3.5229e-01, PNorm = 96.5686, GNorm = 0.2298
Meta loss on this task batch = 3.6065e-01, Meta loss averaged over last 500 steps = 3.5237e-01, PNorm = 96.5779, GNorm = 0.3615
Meta loss on this task batch = 3.2987e-01, Meta loss averaged over last 500 steps = 3.5226e-01, PNorm = 96.5883, GNorm = 0.2758
Meta loss on this task batch = 3.7591e-01, Meta loss averaged over last 500 steps = 3.5231e-01, PNorm = 96.5989, GNorm = 0.3313
Meta loss on this task batch = 2.7082e-01, Meta loss averaged over last 500 steps = 3.5214e-01, PNorm = 96.6106, GNorm = 0.2258
Meta loss on this task batch = 4.0915e-01, Meta loss averaged over last 500 steps = 3.5218e-01, PNorm = 96.6218, GNorm = 0.2545
Meta loss on this task batch = 2.8246e-01, Meta loss averaged over last 500 steps = 3.5210e-01, PNorm = 96.6331, GNorm = 0.2528
Meta loss on this task batch = 2.9262e-01, Meta loss averaged over last 500 steps = 3.5205e-01, PNorm = 96.6443, GNorm = 0.2191
Meta loss on this task batch = 3.1430e-01, Meta loss averaged over last 500 steps = 3.5203e-01, PNorm = 96.6564, GNorm = 0.4336
Meta loss on this task batch = 3.6925e-01, Meta loss averaged over last 500 steps = 3.5208e-01, PNorm = 96.6671, GNorm = 0.3414
Meta loss on this task batch = 3.0661e-01, Meta loss averaged over last 500 steps = 3.5187e-01, PNorm = 96.6786, GNorm = 0.2377
Meta loss on this task batch = 3.3639e-01, Meta loss averaged over last 500 steps = 3.5173e-01, PNorm = 96.6903, GNorm = 0.2603
Meta loss on this task batch = 4.0113e-01, Meta loss averaged over last 500 steps = 3.5179e-01, PNorm = 96.7012, GNorm = 0.2685
Meta loss on this task batch = 3.3248e-01, Meta loss averaged over last 500 steps = 3.5180e-01, PNorm = 96.7121, GNorm = 0.2782
Meta loss on this task batch = 3.7061e-01, Meta loss averaged over last 500 steps = 3.5169e-01, PNorm = 96.7220, GNorm = 0.3013
Meta loss on this task batch = 3.4950e-01, Meta loss averaged over last 500 steps = 3.5176e-01, PNorm = 96.7314, GNorm = 0.3802
Meta loss on this task batch = 3.7513e-01, Meta loss averaged over last 500 steps = 3.5171e-01, PNorm = 96.7412, GNorm = 0.3769
Took 112.71784257888794 seconds to complete one epoch of meta training
Took 120.8887689113617 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484870
Epoch 209
Meta loss on this task batch = 3.2406e-01, Meta loss averaged over last 500 steps = 3.5159e-01, PNorm = 96.7510, GNorm = 0.2573
Meta loss on this task batch = 3.5601e-01, Meta loss averaged over last 500 steps = 3.5155e-01, PNorm = 96.7600, GNorm = 0.3292
Meta loss on this task batch = 3.5704e-01, Meta loss averaged over last 500 steps = 3.5156e-01, PNorm = 96.7686, GNorm = 0.2611
Meta loss on this task batch = 3.5538e-01, Meta loss averaged over last 500 steps = 3.5147e-01, PNorm = 96.7767, GNorm = 0.2661
Meta loss on this task batch = 3.8139e-01, Meta loss averaged over last 500 steps = 3.5154e-01, PNorm = 96.7839, GNorm = 0.5268
Meta loss on this task batch = 3.5261e-01, Meta loss averaged over last 500 steps = 3.5158e-01, PNorm = 96.7915, GNorm = 0.3463
Meta loss on this task batch = 3.3595e-01, Meta loss averaged over last 500 steps = 3.5148e-01, PNorm = 96.8000, GNorm = 0.3018
Meta loss on this task batch = 2.6997e-01, Meta loss averaged over last 500 steps = 3.5122e-01, PNorm = 96.8085, GNorm = 0.2059
Meta loss on this task batch = 3.9070e-01, Meta loss averaged over last 500 steps = 3.5131e-01, PNorm = 96.8164, GNorm = 0.2621
Meta loss on this task batch = 2.9019e-01, Meta loss averaged over last 500 steps = 3.5117e-01, PNorm = 96.8252, GNorm = 0.2150
Meta loss on this task batch = 4.0218e-01, Meta loss averaged over last 500 steps = 3.5125e-01, PNorm = 96.8338, GNorm = 0.2722
Meta loss on this task batch = 3.4432e-01, Meta loss averaged over last 500 steps = 3.5123e-01, PNorm = 96.8431, GNorm = 0.2880
Meta loss on this task batch = 3.1657e-01, Meta loss averaged over last 500 steps = 3.5107e-01, PNorm = 96.8516, GNorm = 0.2345
Meta loss on this task batch = 3.1502e-01, Meta loss averaged over last 500 steps = 3.5102e-01, PNorm = 96.8597, GNorm = 0.2042
Meta loss on this task batch = 3.4821e-01, Meta loss averaged over last 500 steps = 3.5093e-01, PNorm = 96.8685, GNorm = 0.3141
Meta loss on this task batch = 2.7530e-01, Meta loss averaged over last 500 steps = 3.5072e-01, PNorm = 96.8772, GNorm = 0.2380
Meta loss on this task batch = 3.4791e-01, Meta loss averaged over last 500 steps = 3.5064e-01, PNorm = 96.8860, GNorm = 0.2500
Meta loss on this task batch = 3.7730e-01, Meta loss averaged over last 500 steps = 3.5064e-01, PNorm = 96.8946, GNorm = 0.2508
Meta loss on this task batch = 3.6151e-01, Meta loss averaged over last 500 steps = 3.5065e-01, PNorm = 96.9038, GNorm = 0.4095
Took 113.46939206123352 seconds to complete one epoch of meta training
Took 121.15090084075928 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469621
Epoch 210
Meta loss on this task batch = 3.3746e-01, Meta loss averaged over last 500 steps = 3.5076e-01, PNorm = 96.9133, GNorm = 0.2460
Meta loss on this task batch = 3.7721e-01, Meta loss averaged over last 500 steps = 3.5077e-01, PNorm = 96.9232, GNorm = 0.2822
Meta loss on this task batch = 3.4414e-01, Meta loss averaged over last 500 steps = 3.5074e-01, PNorm = 96.9332, GNorm = 0.3042
Meta loss on this task batch = 3.6503e-01, Meta loss averaged over last 500 steps = 3.5076e-01, PNorm = 96.9432, GNorm = 0.2344
Meta loss on this task batch = 2.8433e-01, Meta loss averaged over last 500 steps = 3.5070e-01, PNorm = 96.9532, GNorm = 0.3349
Meta loss on this task batch = 3.5465e-01, Meta loss averaged over last 500 steps = 3.5070e-01, PNorm = 96.9628, GNorm = 0.2080
Meta loss on this task batch = 3.2952e-01, Meta loss averaged over last 500 steps = 3.5071e-01, PNorm = 96.9733, GNorm = 0.2575
Meta loss on this task batch = 3.7622e-01, Meta loss averaged over last 500 steps = 3.5074e-01, PNorm = 96.9828, GNorm = 0.3551
Meta loss on this task batch = 3.6617e-01, Meta loss averaged over last 500 steps = 3.5097e-01, PNorm = 96.9914, GNorm = 0.2353
Meta loss on this task batch = 3.3757e-01, Meta loss averaged over last 500 steps = 3.5083e-01, PNorm = 97.0002, GNorm = 0.4523
Meta loss on this task batch = 3.3594e-01, Meta loss averaged over last 500 steps = 3.5084e-01, PNorm = 97.0091, GNorm = 0.2374
Meta loss on this task batch = 3.5136e-01, Meta loss averaged over last 500 steps = 3.5091e-01, PNorm = 97.0178, GNorm = 0.3064
Meta loss on this task batch = 3.5855e-01, Meta loss averaged over last 500 steps = 3.5091e-01, PNorm = 97.0261, GNorm = 0.2666
Meta loss on this task batch = 3.2498e-01, Meta loss averaged over last 500 steps = 3.5076e-01, PNorm = 97.0350, GNorm = 0.2706
Meta loss on this task batch = 3.6804e-01, Meta loss averaged over last 500 steps = 3.5080e-01, PNorm = 97.0434, GNorm = 0.3429
Meta loss on this task batch = 3.0581e-01, Meta loss averaged over last 500 steps = 3.5060e-01, PNorm = 97.0524, GNorm = 0.2224
Meta loss on this task batch = 3.3529e-01, Meta loss averaged over last 500 steps = 3.5051e-01, PNorm = 97.0616, GNorm = 0.2296
Meta loss on this task batch = 3.5822e-01, Meta loss averaged over last 500 steps = 3.5046e-01, PNorm = 97.0708, GNorm = 0.2446
Meta loss on this task batch = 3.8077e-01, Meta loss averaged over last 500 steps = 3.5055e-01, PNorm = 97.0791, GNorm = 0.3242
Took 114.51605200767517 seconds to complete one epoch of meta training
Took 122.586008310318 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494918
Epoch 211
Meta loss on this task batch = 3.3380e-01, Meta loss averaged over last 500 steps = 3.5043e-01, PNorm = 97.0881, GNorm = 0.2536
Meta loss on this task batch = 3.3789e-01, Meta loss averaged over last 500 steps = 3.5037e-01, PNorm = 97.0973, GNorm = 0.3388
Meta loss on this task batch = 3.4532e-01, Meta loss averaged over last 500 steps = 3.5026e-01, PNorm = 97.1055, GNorm = 0.2710
Meta loss on this task batch = 3.8203e-01, Meta loss averaged over last 500 steps = 3.5024e-01, PNorm = 97.1137, GNorm = 0.3220
Meta loss on this task batch = 3.7331e-01, Meta loss averaged over last 500 steps = 3.5034e-01, PNorm = 97.1217, GNorm = 0.2477
Meta loss on this task batch = 3.6532e-01, Meta loss averaged over last 500 steps = 3.5035e-01, PNorm = 97.1290, GNorm = 0.2700
Meta loss on this task batch = 3.2160e-01, Meta loss averaged over last 500 steps = 3.5027e-01, PNorm = 97.1374, GNorm = 0.2218
Meta loss on this task batch = 3.7385e-01, Meta loss averaged over last 500 steps = 3.5032e-01, PNorm = 97.1465, GNorm = 0.2119
Meta loss on this task batch = 3.6657e-01, Meta loss averaged over last 500 steps = 3.5041e-01, PNorm = 97.1554, GNorm = 0.2406
Meta loss on this task batch = 3.1614e-01, Meta loss averaged over last 500 steps = 3.5029e-01, PNorm = 97.1659, GNorm = 0.2584
Meta loss on this task batch = 3.8026e-01, Meta loss averaged over last 500 steps = 3.5045e-01, PNorm = 97.1762, GNorm = 0.2856
Meta loss on this task batch = 3.4196e-01, Meta loss averaged over last 500 steps = 3.5047e-01, PNorm = 97.1865, GNorm = 0.3171
Meta loss on this task batch = 3.9703e-01, Meta loss averaged over last 500 steps = 3.5053e-01, PNorm = 97.1961, GNorm = 0.2707
Meta loss on this task batch = 3.0580e-01, Meta loss averaged over last 500 steps = 3.5057e-01, PNorm = 97.2069, GNorm = 0.2354
Meta loss on this task batch = 3.3561e-01, Meta loss averaged over last 500 steps = 3.5053e-01, PNorm = 97.2166, GNorm = 0.3268
Meta loss on this task batch = 3.5161e-01, Meta loss averaged over last 500 steps = 3.5046e-01, PNorm = 97.2250, GNorm = 0.4430
Meta loss on this task batch = 2.6916e-01, Meta loss averaged over last 500 steps = 3.5026e-01, PNorm = 97.2335, GNorm = 0.1956
Meta loss on this task batch = 3.0878e-01, Meta loss averaged over last 500 steps = 3.5005e-01, PNorm = 97.2422, GNorm = 0.3389
Meta loss on this task batch = 3.1358e-01, Meta loss averaged over last 500 steps = 3.4994e-01, PNorm = 97.2511, GNorm = 0.3187
Took 117.55686616897583 seconds to complete one epoch of meta training
Took 125.65210938453674 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491694
Epoch 212
Meta loss on this task batch = 3.5026e-01, Meta loss averaged over last 500 steps = 3.4997e-01, PNorm = 97.2605, GNorm = 0.2806
Meta loss on this task batch = 3.3287e-01, Meta loss averaged over last 500 steps = 3.4990e-01, PNorm = 97.2703, GNorm = 0.2312
Meta loss on this task batch = 3.0972e-01, Meta loss averaged over last 500 steps = 3.4990e-01, PNorm = 97.2805, GNorm = 0.2633
Meta loss on this task batch = 3.9288e-01, Meta loss averaged over last 500 steps = 3.5003e-01, PNorm = 97.2910, GNorm = 0.3037
Meta loss on this task batch = 2.8890e-01, Meta loss averaged over last 500 steps = 3.4979e-01, PNorm = 97.3012, GNorm = 0.2332
Meta loss on this task batch = 4.0041e-01, Meta loss averaged over last 500 steps = 3.4985e-01, PNorm = 97.3108, GNorm = 0.3281
Meta loss on this task batch = 3.6596e-01, Meta loss averaged over last 500 steps = 3.4990e-01, PNorm = 97.3204, GNorm = 0.4054
Meta loss on this task batch = 3.7694e-01, Meta loss averaged over last 500 steps = 3.4997e-01, PNorm = 97.3301, GNorm = 0.2914
Meta loss on this task batch = 3.5251e-01, Meta loss averaged over last 500 steps = 3.5002e-01, PNorm = 97.3386, GNorm = 0.3040
Meta loss on this task batch = 3.4060e-01, Meta loss averaged over last 500 steps = 3.5004e-01, PNorm = 97.3473, GNorm = 0.2213
Meta loss on this task batch = 3.3618e-01, Meta loss averaged over last 500 steps = 3.4993e-01, PNorm = 97.3564, GNorm = 0.3597
Meta loss on this task batch = 3.1361e-01, Meta loss averaged over last 500 steps = 3.4973e-01, PNorm = 97.3665, GNorm = 0.2224
Meta loss on this task batch = 3.0277e-01, Meta loss averaged over last 500 steps = 3.4965e-01, PNorm = 97.3767, GNorm = 0.4187
Meta loss on this task batch = 3.1395e-01, Meta loss averaged over last 500 steps = 3.4968e-01, PNorm = 97.3861, GNorm = 0.2827
Meta loss on this task batch = 3.5508e-01, Meta loss averaged over last 500 steps = 3.4975e-01, PNorm = 97.3949, GNorm = 0.3118
Meta loss on this task batch = 3.5747e-01, Meta loss averaged over last 500 steps = 3.4961e-01, PNorm = 97.4045, GNorm = 0.4905
Meta loss on this task batch = 3.5353e-01, Meta loss averaged over last 500 steps = 3.4950e-01, PNorm = 97.4139, GNorm = 0.2415
Meta loss on this task batch = 3.0378e-01, Meta loss averaged over last 500 steps = 3.4942e-01, PNorm = 97.4237, GNorm = 0.3353
Meta loss on this task batch = 3.2181e-01, Meta loss averaged over last 500 steps = 3.4942e-01, PNorm = 97.4340, GNorm = 0.2906
Took 114.4925651550293 seconds to complete one epoch of meta training
Took 122.11634492874146 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498466
Epoch 213
Meta loss on this task batch = 3.2885e-01, Meta loss averaged over last 500 steps = 3.4940e-01, PNorm = 97.4453, GNorm = 0.3570
Meta loss on this task batch = 3.1861e-01, Meta loss averaged over last 500 steps = 3.4935e-01, PNorm = 97.4562, GNorm = 0.2344
Meta loss on this task batch = 3.6911e-01, Meta loss averaged over last 500 steps = 3.4949e-01, PNorm = 97.4665, GNorm = 0.3409
Meta loss on this task batch = 3.2378e-01, Meta loss averaged over last 500 steps = 3.4932e-01, PNorm = 97.4768, GNorm = 0.2520
Meta loss on this task batch = 3.5275e-01, Meta loss averaged over last 500 steps = 3.4937e-01, PNorm = 97.4874, GNorm = 0.2643
Meta loss on this task batch = 3.7911e-01, Meta loss averaged over last 500 steps = 3.4931e-01, PNorm = 97.4982, GNorm = 0.2545
Meta loss on this task batch = 2.9499e-01, Meta loss averaged over last 500 steps = 3.4913e-01, PNorm = 97.5094, GNorm = 0.2207
Meta loss on this task batch = 3.1158e-01, Meta loss averaged over last 500 steps = 3.4899e-01, PNorm = 97.5212, GNorm = 0.3275
Meta loss on this task batch = 2.9906e-01, Meta loss averaged over last 500 steps = 3.4884e-01, PNorm = 97.5328, GNorm = 0.2480
Meta loss on this task batch = 3.5928e-01, Meta loss averaged over last 500 steps = 3.4895e-01, PNorm = 97.5436, GNorm = 0.2379
Meta loss on this task batch = 3.7528e-01, Meta loss averaged over last 500 steps = 3.4897e-01, PNorm = 97.5535, GNorm = 0.2605
Meta loss on this task batch = 3.3552e-01, Meta loss averaged over last 500 steps = 3.4900e-01, PNorm = 97.5637, GNorm = 0.2734
Meta loss on this task batch = 3.6375e-01, Meta loss averaged over last 500 steps = 3.4905e-01, PNorm = 97.5735, GNorm = 0.3594
Meta loss on this task batch = 3.7692e-01, Meta loss averaged over last 500 steps = 3.4921e-01, PNorm = 97.5835, GNorm = 0.3449
Meta loss on this task batch = 4.2649e-01, Meta loss averaged over last 500 steps = 3.4941e-01, PNorm = 97.5922, GNorm = 0.3217
Meta loss on this task batch = 2.8333e-01, Meta loss averaged over last 500 steps = 3.4935e-01, PNorm = 97.6012, GNorm = 0.2708
Meta loss on this task batch = 4.0152e-01, Meta loss averaged over last 500 steps = 3.4940e-01, PNorm = 97.6093, GNorm = 0.2388
Meta loss on this task batch = 3.1003e-01, Meta loss averaged over last 500 steps = 3.4929e-01, PNorm = 97.6172, GNorm = 0.2317
Meta loss on this task batch = 3.5321e-01, Meta loss averaged over last 500 steps = 3.4923e-01, PNorm = 97.6244, GNorm = 0.3344
Took 110.74848890304565 seconds to complete one epoch of meta training
Took 118.4700939655304 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497169
Epoch 214
Meta loss on this task batch = 3.1497e-01, Meta loss averaged over last 500 steps = 3.4903e-01, PNorm = 97.6320, GNorm = 0.2906
Meta loss on this task batch = 3.4966e-01, Meta loss averaged over last 500 steps = 3.4903e-01, PNorm = 97.6387, GNorm = 0.2776
Meta loss on this task batch = 3.7307e-01, Meta loss averaged over last 500 steps = 3.4912e-01, PNorm = 97.6452, GNorm = 0.2707
Meta loss on this task batch = 3.1303e-01, Meta loss averaged over last 500 steps = 3.4903e-01, PNorm = 97.6517, GNorm = 0.3492
Meta loss on this task batch = 3.1740e-01, Meta loss averaged over last 500 steps = 3.4888e-01, PNorm = 97.6588, GNorm = 0.3542
Meta loss on this task batch = 3.4417e-01, Meta loss averaged over last 500 steps = 3.4896e-01, PNorm = 97.6662, GNorm = 0.2695
Meta loss on this task batch = 3.3327e-01, Meta loss averaged over last 500 steps = 3.4899e-01, PNorm = 97.6741, GNorm = 0.2710
Meta loss on this task batch = 3.3755e-01, Meta loss averaged over last 500 steps = 3.4896e-01, PNorm = 97.6823, GNorm = 0.2764
Meta loss on this task batch = 3.4344e-01, Meta loss averaged over last 500 steps = 3.4896e-01, PNorm = 97.6908, GNorm = 0.2358
Meta loss on this task batch = 3.5341e-01, Meta loss averaged over last 500 steps = 3.4894e-01, PNorm = 97.7005, GNorm = 0.3026
Meta loss on this task batch = 3.9626e-01, Meta loss averaged over last 500 steps = 3.4907e-01, PNorm = 97.7105, GNorm = 0.2639
Meta loss on this task batch = 3.1306e-01, Meta loss averaged over last 500 steps = 3.4900e-01, PNorm = 97.7223, GNorm = 0.3382
Meta loss on this task batch = 3.3031e-01, Meta loss averaged over last 500 steps = 3.4894e-01, PNorm = 97.7341, GNorm = 0.2447
Meta loss on this task batch = 3.9603e-01, Meta loss averaged over last 500 steps = 3.4906e-01, PNorm = 97.7456, GNorm = 0.4032
Meta loss on this task batch = 3.5277e-01, Meta loss averaged over last 500 steps = 3.4910e-01, PNorm = 97.7577, GNorm = 0.2568
Meta loss on this task batch = 3.9052e-01, Meta loss averaged over last 500 steps = 3.4914e-01, PNorm = 97.7687, GNorm = 0.2446
Meta loss on this task batch = 3.2228e-01, Meta loss averaged over last 500 steps = 3.4905e-01, PNorm = 97.7801, GNorm = 0.2545
Meta loss on this task batch = 3.6014e-01, Meta loss averaged over last 500 steps = 3.4917e-01, PNorm = 97.7910, GNorm = 0.2546
Meta loss on this task batch = 2.9676e-01, Meta loss averaged over last 500 steps = 3.4904e-01, PNorm = 97.8015, GNorm = 0.2757
Took 112.4955849647522 seconds to complete one epoch of meta training
Took 120.10646653175354 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482683
Epoch 215
Meta loss on this task batch = 3.3779e-01, Meta loss averaged over last 500 steps = 3.4898e-01, PNorm = 97.8127, GNorm = 0.2740
Meta loss on this task batch = 3.9915e-01, Meta loss averaged over last 500 steps = 3.4903e-01, PNorm = 97.8231, GNorm = 0.2825
Meta loss on this task batch = 3.2866e-01, Meta loss averaged over last 500 steps = 3.4897e-01, PNorm = 97.8331, GNorm = 0.3201
Meta loss on this task batch = 3.9206e-01, Meta loss averaged over last 500 steps = 3.4902e-01, PNorm = 97.8420, GNorm = 0.2457
Meta loss on this task batch = 3.5599e-01, Meta loss averaged over last 500 steps = 3.4895e-01, PNorm = 97.8505, GNorm = 0.3105
Meta loss on this task batch = 3.4637e-01, Meta loss averaged over last 500 steps = 3.4901e-01, PNorm = 97.8583, GNorm = 0.2708
Meta loss on this task batch = 3.5940e-01, Meta loss averaged over last 500 steps = 3.4927e-01, PNorm = 97.8672, GNorm = 0.5737
Meta loss on this task batch = 3.7669e-01, Meta loss averaged over last 500 steps = 3.4934e-01, PNorm = 97.8762, GNorm = 0.4187
Meta loss on this task batch = 2.9999e-01, Meta loss averaged over last 500 steps = 3.4926e-01, PNorm = 97.8859, GNorm = 0.2351
Meta loss on this task batch = 3.2635e-01, Meta loss averaged over last 500 steps = 3.4932e-01, PNorm = 97.8953, GNorm = 0.3092
Meta loss on this task batch = 3.3621e-01, Meta loss averaged over last 500 steps = 3.4932e-01, PNorm = 97.9046, GNorm = 0.2736
Meta loss on this task batch = 3.5926e-01, Meta loss averaged over last 500 steps = 3.4933e-01, PNorm = 97.9135, GNorm = 0.2263
Meta loss on this task batch = 3.5101e-01, Meta loss averaged over last 500 steps = 3.4923e-01, PNorm = 97.9222, GNorm = 0.3586
Meta loss on this task batch = 3.0657e-01, Meta loss averaged over last 500 steps = 3.4923e-01, PNorm = 97.9317, GNorm = 0.2600
Meta loss on this task batch = 3.2527e-01, Meta loss averaged over last 500 steps = 3.4918e-01, PNorm = 97.9409, GNorm = 0.2529
Meta loss on this task batch = 3.3836e-01, Meta loss averaged over last 500 steps = 3.4916e-01, PNorm = 97.9506, GNorm = 0.2154
Meta loss on this task batch = 3.6416e-01, Meta loss averaged over last 500 steps = 3.4912e-01, PNorm = 97.9609, GNorm = 0.2618
Meta loss on this task batch = 3.2739e-01, Meta loss averaged over last 500 steps = 3.4911e-01, PNorm = 97.9707, GNorm = 0.2426
Meta loss on this task batch = 3.3467e-01, Meta loss averaged over last 500 steps = 3.4902e-01, PNorm = 97.9802, GNorm = 0.3051
Took 113.52128481864929 seconds to complete one epoch of meta training
Took 120.41234469413757 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506302
Found better MAML checkpoint after meta validation, saving now
Epoch 216
Meta loss on this task batch = 3.4507e-01, Meta loss averaged over last 500 steps = 3.4898e-01, PNorm = 97.9896, GNorm = 0.2379
Meta loss on this task batch = 3.5561e-01, Meta loss averaged over last 500 steps = 3.4890e-01, PNorm = 98.0000, GNorm = 0.2308
Meta loss on this task batch = 3.9295e-01, Meta loss averaged over last 500 steps = 3.4897e-01, PNorm = 98.0097, GNorm = 0.3906
Meta loss on this task batch = 3.5585e-01, Meta loss averaged over last 500 steps = 3.4899e-01, PNorm = 98.0190, GNorm = 0.2689
Meta loss on this task batch = 3.8012e-01, Meta loss averaged over last 500 steps = 3.4901e-01, PNorm = 98.0279, GNorm = 0.3119
Meta loss on this task batch = 3.1386e-01, Meta loss averaged over last 500 steps = 3.4889e-01, PNorm = 98.0365, GNorm = 0.2341
Meta loss on this task batch = 3.5231e-01, Meta loss averaged over last 500 steps = 3.4888e-01, PNorm = 98.0442, GNorm = 0.4001
Meta loss on this task batch = 3.2655e-01, Meta loss averaged over last 500 steps = 3.4885e-01, PNorm = 98.0525, GNorm = 0.3871
Meta loss on this task batch = 3.3384e-01, Meta loss averaged over last 500 steps = 3.4875e-01, PNorm = 98.0606, GNorm = 0.2658
Meta loss on this task batch = 3.2242e-01, Meta loss averaged over last 500 steps = 3.4860e-01, PNorm = 98.0690, GNorm = 0.2125
Meta loss on this task batch = 3.5166e-01, Meta loss averaged over last 500 steps = 3.4852e-01, PNorm = 98.0776, GNorm = 0.3249
Meta loss on this task batch = 3.4096e-01, Meta loss averaged over last 500 steps = 3.4856e-01, PNorm = 98.0864, GNorm = 0.3185
Meta loss on this task batch = 3.0852e-01, Meta loss averaged over last 500 steps = 3.4843e-01, PNorm = 98.0962, GNorm = 0.2467
Meta loss on this task batch = 3.8794e-01, Meta loss averaged over last 500 steps = 3.4858e-01, PNorm = 98.1073, GNorm = 0.4036
Meta loss on this task batch = 3.4048e-01, Meta loss averaged over last 500 steps = 3.4863e-01, PNorm = 98.1181, GNorm = 0.2313
Meta loss on this task batch = 3.5184e-01, Meta loss averaged over last 500 steps = 3.4872e-01, PNorm = 98.1277, GNorm = 0.3445
Meta loss on this task batch = 3.8292e-01, Meta loss averaged over last 500 steps = 3.4877e-01, PNorm = 98.1372, GNorm = 0.3281
Meta loss on this task batch = 3.3353e-01, Meta loss averaged over last 500 steps = 3.4866e-01, PNorm = 98.1463, GNorm = 0.2614
Meta loss on this task batch = 3.2784e-01, Meta loss averaged over last 500 steps = 3.4862e-01, PNorm = 98.1554, GNorm = 0.2707
Took 108.98374605178833 seconds to complete one epoch of meta training
Took 116.6938943862915 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463284
Epoch 217
Meta loss on this task batch = 3.8473e-01, Meta loss averaged over last 500 steps = 3.4876e-01, PNorm = 98.1646, GNorm = 0.2650
Meta loss on this task batch = 3.0622e-01, Meta loss averaged over last 500 steps = 3.4863e-01, PNorm = 98.1743, GNorm = 0.2349
Meta loss on this task batch = 3.2547e-01, Meta loss averaged over last 500 steps = 3.4856e-01, PNorm = 98.1848, GNorm = 0.2053
Meta loss on this task batch = 3.2304e-01, Meta loss averaged over last 500 steps = 3.4859e-01, PNorm = 98.1956, GNorm = 0.2057
Meta loss on this task batch = 3.7985e-01, Meta loss averaged over last 500 steps = 3.4856e-01, PNorm = 98.2067, GNorm = 0.3262
Meta loss on this task batch = 3.7714e-01, Meta loss averaged over last 500 steps = 3.4859e-01, PNorm = 98.2179, GNorm = 0.2959
Meta loss on this task batch = 3.1434e-01, Meta loss averaged over last 500 steps = 3.4853e-01, PNorm = 98.2284, GNorm = 0.2408
Meta loss on this task batch = 3.2509e-01, Meta loss averaged over last 500 steps = 3.4840e-01, PNorm = 98.2388, GNorm = 0.2535
Meta loss on this task batch = 3.1235e-01, Meta loss averaged over last 500 steps = 3.4826e-01, PNorm = 98.2509, GNorm = 0.4841
Meta loss on this task batch = 3.5534e-01, Meta loss averaged over last 500 steps = 3.4825e-01, PNorm = 98.2631, GNorm = 0.2459
Meta loss on this task batch = 3.8127e-01, Meta loss averaged over last 500 steps = 3.4824e-01, PNorm = 98.2749, GNorm = 0.5465
Meta loss on this task batch = 4.1835e-01, Meta loss averaged over last 500 steps = 3.4841e-01, PNorm = 98.2852, GNorm = 0.3257
Meta loss on this task batch = 3.5563e-01, Meta loss averaged over last 500 steps = 3.4850e-01, PNorm = 98.2942, GNorm = 0.4685
Meta loss on this task batch = 2.7151e-01, Meta loss averaged over last 500 steps = 3.4843e-01, PNorm = 98.3036, GNorm = 0.2156
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 3.4837e-01, PNorm = 98.3133, GNorm = 0.2546
Meta loss on this task batch = 3.9014e-01, Meta loss averaged over last 500 steps = 3.4844e-01, PNorm = 98.3221, GNorm = 0.2859
Meta loss on this task batch = 3.4349e-01, Meta loss averaged over last 500 steps = 3.4849e-01, PNorm = 98.3310, GNorm = 0.3318
Meta loss on this task batch = 3.6969e-01, Meta loss averaged over last 500 steps = 3.4845e-01, PNorm = 98.3395, GNorm = 0.3077
Meta loss on this task batch = 3.5779e-01, Meta loss averaged over last 500 steps = 3.4852e-01, PNorm = 98.3485, GNorm = 0.2472
Took 111.24708700180054 seconds to complete one epoch of meta training
Took 118.59077882766724 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.516910
Found better MAML checkpoint after meta validation, saving now
Epoch 218
Meta loss on this task batch = 3.3831e-01, Meta loss averaged over last 500 steps = 3.4850e-01, PNorm = 98.3575, GNorm = 0.2579
Meta loss on this task batch = 3.8790e-01, Meta loss averaged over last 500 steps = 3.4841e-01, PNorm = 98.3659, GNorm = 0.2526
Meta loss on this task batch = 3.0810e-01, Meta loss averaged over last 500 steps = 3.4841e-01, PNorm = 98.3749, GNorm = 0.2174
Meta loss on this task batch = 2.9260e-01, Meta loss averaged over last 500 steps = 3.4819e-01, PNorm = 98.3844, GNorm = 0.2615
Meta loss on this task batch = 3.3039e-01, Meta loss averaged over last 500 steps = 3.4812e-01, PNorm = 98.3940, GNorm = 0.3495
Meta loss on this task batch = 3.8301e-01, Meta loss averaged over last 500 steps = 3.4820e-01, PNorm = 98.4028, GNorm = 0.3014
Meta loss on this task batch = 3.1254e-01, Meta loss averaged over last 500 steps = 3.4811e-01, PNorm = 98.4122, GNorm = 0.2140
Meta loss on this task batch = 3.5359e-01, Meta loss averaged over last 500 steps = 3.4805e-01, PNorm = 98.4217, GNorm = 0.2359
Meta loss on this task batch = 3.4243e-01, Meta loss averaged over last 500 steps = 3.4793e-01, PNorm = 98.4314, GNorm = 0.2822
Meta loss on this task batch = 3.6265e-01, Meta loss averaged over last 500 steps = 3.4800e-01, PNorm = 98.4410, GNorm = 0.2996
Meta loss on this task batch = 4.1775e-01, Meta loss averaged over last 500 steps = 3.4816e-01, PNorm = 98.4501, GNorm = 0.2559
Meta loss on this task batch = 2.9747e-01, Meta loss averaged over last 500 steps = 3.4805e-01, PNorm = 98.4596, GNorm = 0.2698
Meta loss on this task batch = 3.2790e-01, Meta loss averaged over last 500 steps = 3.4789e-01, PNorm = 98.4695, GNorm = 0.1935
Meta loss on this task batch = 3.6224e-01, Meta loss averaged over last 500 steps = 3.4791e-01, PNorm = 98.4787, GNorm = 0.2564
Meta loss on this task batch = 3.3732e-01, Meta loss averaged over last 500 steps = 3.4787e-01, PNorm = 98.4884, GNorm = 0.2495
Meta loss on this task batch = 3.3623e-01, Meta loss averaged over last 500 steps = 3.4791e-01, PNorm = 98.4983, GNorm = 0.2276
Meta loss on this task batch = 3.1372e-01, Meta loss averaged over last 500 steps = 3.4788e-01, PNorm = 98.5086, GNorm = 0.2675
Meta loss on this task batch = 3.3795e-01, Meta loss averaged over last 500 steps = 3.4788e-01, PNorm = 98.5196, GNorm = 0.3309
Meta loss on this task batch = 3.0792e-01, Meta loss averaged over last 500 steps = 3.4779e-01, PNorm = 98.5305, GNorm = 0.2255
Took 115.4325954914093 seconds to complete one epoch of meta training
Took 122.888986825943 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485746
Epoch 219
Meta loss on this task batch = 3.3109e-01, Meta loss averaged over last 500 steps = 3.4757e-01, PNorm = 98.5421, GNorm = 0.2467
Meta loss on this task batch = 2.9420e-01, Meta loss averaged over last 500 steps = 3.4755e-01, PNorm = 98.5531, GNorm = 0.2793
Meta loss on this task batch = 3.4803e-01, Meta loss averaged over last 500 steps = 3.4755e-01, PNorm = 98.5627, GNorm = 0.2850
Meta loss on this task batch = 3.2317e-01, Meta loss averaged over last 500 steps = 3.4750e-01, PNorm = 98.5723, GNorm = 0.3395
Meta loss on this task batch = 3.1633e-01, Meta loss averaged over last 500 steps = 3.4756e-01, PNorm = 98.5826, GNorm = 0.2508
Meta loss on this task batch = 3.3357e-01, Meta loss averaged over last 500 steps = 3.4772e-01, PNorm = 98.5929, GNorm = 0.3183
Meta loss on this task batch = 3.0791e-01, Meta loss averaged over last 500 steps = 3.4757e-01, PNorm = 98.6030, GNorm = 0.3170
Meta loss on this task batch = 3.6405e-01, Meta loss averaged over last 500 steps = 3.4756e-01, PNorm = 98.6128, GNorm = 0.2446
Meta loss on this task batch = 2.8266e-01, Meta loss averaged over last 500 steps = 3.4748e-01, PNorm = 98.6227, GNorm = 0.2475
Meta loss on this task batch = 3.9995e-01, Meta loss averaged over last 500 steps = 3.4761e-01, PNorm = 98.6324, GNorm = 0.2890
Meta loss on this task batch = 3.8242e-01, Meta loss averaged over last 500 steps = 3.4767e-01, PNorm = 98.6417, GNorm = 0.3360
Meta loss on this task batch = 3.0732e-01, Meta loss averaged over last 500 steps = 3.4740e-01, PNorm = 98.6505, GNorm = 0.4552
Meta loss on this task batch = 3.4177e-01, Meta loss averaged over last 500 steps = 3.4733e-01, PNorm = 98.6600, GNorm = 0.2411
Meta loss on this task batch = 3.7575e-01, Meta loss averaged over last 500 steps = 3.4750e-01, PNorm = 98.6688, GNorm = 0.2497
Meta loss on this task batch = 3.4835e-01, Meta loss averaged over last 500 steps = 3.4747e-01, PNorm = 98.6766, GNorm = 0.2404
Meta loss on this task batch = 3.3305e-01, Meta loss averaged over last 500 steps = 3.4752e-01, PNorm = 98.6846, GNorm = 0.2864
Meta loss on this task batch = 3.1685e-01, Meta loss averaged over last 500 steps = 3.4752e-01, PNorm = 98.6920, GNorm = 0.2919
Meta loss on this task batch = 3.6779e-01, Meta loss averaged over last 500 steps = 3.4765e-01, PNorm = 98.6993, GNorm = 0.3238
Meta loss on this task batch = 3.4104e-01, Meta loss averaged over last 500 steps = 3.4762e-01, PNorm = 98.7065, GNorm = 0.3061
Took 114.81374049186707 seconds to complete one epoch of meta training
Took 122.84578227996826 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504300
Epoch 220
Meta loss on this task batch = 3.3954e-01, Meta loss averaged over last 500 steps = 3.4757e-01, PNorm = 98.7140, GNorm = 0.2273
Meta loss on this task batch = 3.3633e-01, Meta loss averaged over last 500 steps = 3.4746e-01, PNorm = 98.7219, GNorm = 0.2485
Meta loss on this task batch = 3.4365e-01, Meta loss averaged over last 500 steps = 3.4747e-01, PNorm = 98.7295, GNorm = 0.2911
Meta loss on this task batch = 3.5296e-01, Meta loss averaged over last 500 steps = 3.4746e-01, PNorm = 98.7366, GNorm = 0.2741
Meta loss on this task batch = 3.9137e-01, Meta loss averaged over last 500 steps = 3.4762e-01, PNorm = 98.7443, GNorm = 0.2977
Meta loss on this task batch = 3.2944e-01, Meta loss averaged over last 500 steps = 3.4753e-01, PNorm = 98.7526, GNorm = 0.3443
Meta loss on this task batch = 3.2859e-01, Meta loss averaged over last 500 steps = 3.4740e-01, PNorm = 98.7607, GNorm = 0.3621
Meta loss on this task batch = 3.5045e-01, Meta loss averaged over last 500 steps = 3.4748e-01, PNorm = 98.7688, GNorm = 0.2833
Meta loss on this task batch = 3.4849e-01, Meta loss averaged over last 500 steps = 3.4742e-01, PNorm = 98.7770, GNorm = 0.2887
Meta loss on this task batch = 3.7151e-01, Meta loss averaged over last 500 steps = 3.4744e-01, PNorm = 98.7856, GNorm = 0.2939
Meta loss on this task batch = 2.8744e-01, Meta loss averaged over last 500 steps = 3.4723e-01, PNorm = 98.7943, GNorm = 0.2186
Meta loss on this task batch = 3.2738e-01, Meta loss averaged over last 500 steps = 3.4707e-01, PNorm = 98.8040, GNorm = 0.4002
Meta loss on this task batch = 3.1841e-01, Meta loss averaged over last 500 steps = 3.4697e-01, PNorm = 98.8138, GNorm = 0.2379
Meta loss on this task batch = 3.4917e-01, Meta loss averaged over last 500 steps = 3.4695e-01, PNorm = 98.8246, GNorm = 0.3917
Meta loss on this task batch = 3.6784e-01, Meta loss averaged over last 500 steps = 3.4701e-01, PNorm = 98.8351, GNorm = 0.3757
Meta loss on this task batch = 3.5830e-01, Meta loss averaged over last 500 steps = 3.4712e-01, PNorm = 98.8453, GNorm = 0.2914
Meta loss on this task batch = 3.5792e-01, Meta loss averaged over last 500 steps = 3.4715e-01, PNorm = 98.8542, GNorm = 0.2697
Meta loss on this task batch = 2.8549e-01, Meta loss averaged over last 500 steps = 3.4706e-01, PNorm = 98.8635, GNorm = 0.2444
Meta loss on this task batch = 3.9138e-01, Meta loss averaged over last 500 steps = 3.4711e-01, PNorm = 98.8722, GNorm = 0.3823
Took 115.93736219406128 seconds to complete one epoch of meta training
Took 123.70055031776428 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463499
Epoch 221
Meta loss on this task batch = 3.1224e-01, Meta loss averaged over last 500 steps = 3.4697e-01, PNorm = 98.8807, GNorm = 0.2283
Meta loss on this task batch = 3.4643e-01, Meta loss averaged over last 500 steps = 3.4700e-01, PNorm = 98.8895, GNorm = 0.2161
Meta loss on this task batch = 3.3842e-01, Meta loss averaged over last 500 steps = 3.4699e-01, PNorm = 98.8984, GNorm = 0.3638
Meta loss on this task batch = 3.5074e-01, Meta loss averaged over last 500 steps = 3.4695e-01, PNorm = 98.9077, GNorm = 0.2624
Meta loss on this task batch = 2.8334e-01, Meta loss averaged over last 500 steps = 3.4688e-01, PNorm = 98.9172, GNorm = 0.3918
Meta loss on this task batch = 3.5856e-01, Meta loss averaged over last 500 steps = 3.4690e-01, PNorm = 98.9272, GNorm = 0.2436
Meta loss on this task batch = 3.2184e-01, Meta loss averaged over last 500 steps = 3.4696e-01, PNorm = 98.9387, GNorm = 0.4218
Meta loss on this task batch = 3.3980e-01, Meta loss averaged over last 500 steps = 3.4688e-01, PNorm = 98.9497, GNorm = 0.2907
Meta loss on this task batch = 3.6809e-01, Meta loss averaged over last 500 steps = 3.4701e-01, PNorm = 98.9603, GNorm = 0.3318
Meta loss on this task batch = 3.3627e-01, Meta loss averaged over last 500 steps = 3.4688e-01, PNorm = 98.9697, GNorm = 0.3025
Meta loss on this task batch = 3.9389e-01, Meta loss averaged over last 500 steps = 3.4697e-01, PNorm = 98.9789, GNorm = 0.3842
Meta loss on this task batch = 3.4363e-01, Meta loss averaged over last 500 steps = 3.4695e-01, PNorm = 98.9881, GNorm = 0.3210
Meta loss on this task batch = 3.5744e-01, Meta loss averaged over last 500 steps = 3.4693e-01, PNorm = 98.9969, GNorm = 0.2649
Meta loss on this task batch = 3.1290e-01, Meta loss averaged over last 500 steps = 3.4692e-01, PNorm = 99.0054, GNorm = 0.2232
Meta loss on this task batch = 4.0126e-01, Meta loss averaged over last 500 steps = 3.4715e-01, PNorm = 99.0113, GNorm = 0.4052
Meta loss on this task batch = 3.6383e-01, Meta loss averaged over last 500 steps = 3.4717e-01, PNorm = 99.0164, GNorm = 0.2410
Meta loss on this task batch = 3.2844e-01, Meta loss averaged over last 500 steps = 3.4720e-01, PNorm = 99.0221, GNorm = 0.3300
Meta loss on this task batch = 3.4382e-01, Meta loss averaged over last 500 steps = 3.4710e-01, PNorm = 99.0282, GNorm = 0.2507
Meta loss on this task batch = 2.7704e-01, Meta loss averaged over last 500 steps = 3.4692e-01, PNorm = 99.0362, GNorm = 0.2750
Took 112.25656247138977 seconds to complete one epoch of meta training
Took 120.0421371459961 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488229
Epoch 222
Meta loss on this task batch = 3.4818e-01, Meta loss averaged over last 500 steps = 3.4685e-01, PNorm = 99.0447, GNorm = 0.2384
Meta loss on this task batch = 2.8287e-01, Meta loss averaged over last 500 steps = 3.4672e-01, PNorm = 99.0539, GNorm = 0.2195
Meta loss on this task batch = 3.5182e-01, Meta loss averaged over last 500 steps = 3.4670e-01, PNorm = 99.0643, GNorm = 0.3264
Meta loss on this task batch = 3.4156e-01, Meta loss averaged over last 500 steps = 3.4660e-01, PNorm = 99.0736, GNorm = 0.2878
Meta loss on this task batch = 4.0272e-01, Meta loss averaged over last 500 steps = 3.4662e-01, PNorm = 99.0820, GNorm = 0.2814
Meta loss on this task batch = 3.6013e-01, Meta loss averaged over last 500 steps = 3.4657e-01, PNorm = 99.0907, GNorm = 0.2946
Meta loss on this task batch = 3.9074e-01, Meta loss averaged over last 500 steps = 3.4663e-01, PNorm = 99.0990, GNorm = 0.2817
Meta loss on this task batch = 3.3466e-01, Meta loss averaged over last 500 steps = 3.4659e-01, PNorm = 99.1078, GNorm = 0.2169
Meta loss on this task batch = 3.4354e-01, Meta loss averaged over last 500 steps = 3.4675e-01, PNorm = 99.1162, GNorm = 0.2482
Meta loss on this task batch = 3.7731e-01, Meta loss averaged over last 500 steps = 3.4666e-01, PNorm = 99.1244, GNorm = 0.3126
Meta loss on this task batch = 2.9557e-01, Meta loss averaged over last 500 steps = 3.4653e-01, PNorm = 99.1332, GNorm = 0.3344
Meta loss on this task batch = 2.9721e-01, Meta loss averaged over last 500 steps = 3.4637e-01, PNorm = 99.1423, GNorm = 0.2071
Meta loss on this task batch = 2.9540e-01, Meta loss averaged over last 500 steps = 3.4620e-01, PNorm = 99.1516, GNorm = 0.2186
Meta loss on this task batch = 3.3544e-01, Meta loss averaged over last 500 steps = 3.4624e-01, PNorm = 99.1609, GNorm = 0.2876
Meta loss on this task batch = 3.3173e-01, Meta loss averaged over last 500 steps = 3.4628e-01, PNorm = 99.1708, GNorm = 0.2238
Meta loss on this task batch = 3.2640e-01, Meta loss averaged over last 500 steps = 3.4619e-01, PNorm = 99.1804, GNorm = 0.3621
Meta loss on this task batch = 3.3944e-01, Meta loss averaged over last 500 steps = 3.4616e-01, PNorm = 99.1904, GNorm = 0.3065
Meta loss on this task batch = 3.4298e-01, Meta loss averaged over last 500 steps = 3.4617e-01, PNorm = 99.2000, GNorm = 0.2500
Meta loss on this task batch = 3.3716e-01, Meta loss averaged over last 500 steps = 3.4625e-01, PNorm = 99.2098, GNorm = 0.3211
Took 116.4604434967041 seconds to complete one epoch of meta training
Took 124.40409970283508 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478628
Epoch 223
Meta loss on this task batch = 3.8901e-01, Meta loss averaged over last 500 steps = 3.4626e-01, PNorm = 99.2180, GNorm = 0.2790
Meta loss on this task batch = 3.9962e-01, Meta loss averaged over last 500 steps = 3.4631e-01, PNorm = 99.2262, GNorm = 0.2531
Meta loss on this task batch = 2.8743e-01, Meta loss averaged over last 500 steps = 3.4615e-01, PNorm = 99.2349, GNorm = 0.2707
Meta loss on this task batch = 3.7132e-01, Meta loss averaged over last 500 steps = 3.4627e-01, PNorm = 99.2429, GNorm = 0.2702
Meta loss on this task batch = 3.0108e-01, Meta loss averaged over last 500 steps = 3.4614e-01, PNorm = 99.2511, GNorm = 0.2459
Meta loss on this task batch = 3.4833e-01, Meta loss averaged over last 500 steps = 3.4619e-01, PNorm = 99.2587, GNorm = 0.2466
Meta loss on this task batch = 3.3306e-01, Meta loss averaged over last 500 steps = 3.4608e-01, PNorm = 99.2674, GNorm = 0.2507
Meta loss on this task batch = 3.3148e-01, Meta loss averaged over last 500 steps = 3.4608e-01, PNorm = 99.2762, GNorm = 0.3321
Meta loss on this task batch = 2.8905e-01, Meta loss averaged over last 500 steps = 3.4594e-01, PNorm = 99.2850, GNorm = 0.3609
Meta loss on this task batch = 3.0608e-01, Meta loss averaged over last 500 steps = 3.4586e-01, PNorm = 99.2950, GNorm = 0.2606
Meta loss on this task batch = 4.0134e-01, Meta loss averaged over last 500 steps = 3.4591e-01, PNorm = 99.3051, GNorm = 0.2777
Meta loss on this task batch = 3.7634e-01, Meta loss averaged over last 500 steps = 3.4607e-01, PNorm = 99.3140, GNorm = 0.3259
Meta loss on this task batch = 3.2311e-01, Meta loss averaged over last 500 steps = 3.4601e-01, PNorm = 99.3229, GNorm = 0.2562
Meta loss on this task batch = 3.1708e-01, Meta loss averaged over last 500 steps = 3.4604e-01, PNorm = 99.3320, GNorm = 0.2409
Meta loss on this task batch = 3.2917e-01, Meta loss averaged over last 500 steps = 3.4599e-01, PNorm = 99.3413, GNorm = 0.2358
Meta loss on this task batch = 3.5153e-01, Meta loss averaged over last 500 steps = 3.4602e-01, PNorm = 99.3497, GNorm = 0.3082
Meta loss on this task batch = 3.8560e-01, Meta loss averaged over last 500 steps = 3.4610e-01, PNorm = 99.3583, GNorm = 0.2900
Meta loss on this task batch = 3.1493e-01, Meta loss averaged over last 500 steps = 3.4600e-01, PNorm = 99.3678, GNorm = 0.2414
Meta loss on this task batch = 3.9869e-01, Meta loss averaged over last 500 steps = 3.4615e-01, PNorm = 99.3770, GNorm = 0.2915
Took 112.0736198425293 seconds to complete one epoch of meta training
Took 120.02524471282959 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459815
Epoch 224
Meta loss on this task batch = 3.6221e-01, Meta loss averaged over last 500 steps = 3.4609e-01, PNorm = 99.3869, GNorm = 0.2448
Meta loss on this task batch = 2.9812e-01, Meta loss averaged over last 500 steps = 3.4603e-01, PNorm = 99.3978, GNorm = 0.3319
Meta loss on this task batch = 3.2929e-01, Meta loss averaged over last 500 steps = 3.4596e-01, PNorm = 99.4092, GNorm = 0.2475
Meta loss on this task batch = 3.8265e-01, Meta loss averaged over last 500 steps = 3.4601e-01, PNorm = 99.4209, GNorm = 0.2623
Meta loss on this task batch = 3.5720e-01, Meta loss averaged over last 500 steps = 3.4591e-01, PNorm = 99.4330, GNorm = 0.2748
Meta loss on this task batch = 3.3048e-01, Meta loss averaged over last 500 steps = 3.4585e-01, PNorm = 99.4447, GNorm = 0.4052
Meta loss on this task batch = 3.1507e-01, Meta loss averaged over last 500 steps = 3.4575e-01, PNorm = 99.4567, GNorm = 0.2537
Meta loss on this task batch = 3.0287e-01, Meta loss averaged over last 500 steps = 3.4563e-01, PNorm = 99.4694, GNorm = 0.4039
Meta loss on this task batch = 4.1538e-01, Meta loss averaged over last 500 steps = 3.4567e-01, PNorm = 99.4800, GNorm = 0.3356
Meta loss on this task batch = 3.6817e-01, Meta loss averaged over last 500 steps = 3.4565e-01, PNorm = 99.4897, GNorm = 0.3148
Meta loss on this task batch = 2.7152e-01, Meta loss averaged over last 500 steps = 3.4557e-01, PNorm = 99.4994, GNorm = 0.2482
Meta loss on this task batch = 2.9894e-01, Meta loss averaged over last 500 steps = 3.4557e-01, PNorm = 99.5091, GNorm = 0.2848
Meta loss on this task batch = 3.7809e-01, Meta loss averaged over last 500 steps = 3.4559e-01, PNorm = 99.5174, GNorm = 0.3416
Meta loss on this task batch = 3.6759e-01, Meta loss averaged over last 500 steps = 3.4571e-01, PNorm = 99.5253, GNorm = 0.3387
Meta loss on this task batch = 3.1158e-01, Meta loss averaged over last 500 steps = 3.4568e-01, PNorm = 99.5327, GNorm = 0.2812
Meta loss on this task batch = 3.6467e-01, Meta loss averaged over last 500 steps = 3.4568e-01, PNorm = 99.5408, GNorm = 0.2776
Meta loss on this task batch = 3.6293e-01, Meta loss averaged over last 500 steps = 3.4566e-01, PNorm = 99.5475, GNorm = 0.3173
Meta loss on this task batch = 3.2011e-01, Meta loss averaged over last 500 steps = 3.4570e-01, PNorm = 99.5551, GNorm = 0.2556
Meta loss on this task batch = 3.4670e-01, Meta loss averaged over last 500 steps = 3.4572e-01, PNorm = 99.5631, GNorm = 0.3139
Took 113.49548864364624 seconds to complete one epoch of meta training
Took 121.01168441772461 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484140
Epoch 225
Meta loss on this task batch = 3.5077e-01, Meta loss averaged over last 500 steps = 3.4574e-01, PNorm = 99.5715, GNorm = 0.2412
Meta loss on this task batch = 3.1114e-01, Meta loss averaged over last 500 steps = 3.4568e-01, PNorm = 99.5804, GNorm = 0.2175
Meta loss on this task batch = 2.8898e-01, Meta loss averaged over last 500 steps = 3.4544e-01, PNorm = 99.5898, GNorm = 0.2308
Meta loss on this task batch = 3.3793e-01, Meta loss averaged over last 500 steps = 3.4533e-01, PNorm = 99.5992, GNorm = 0.2385
Meta loss on this task batch = 3.0455e-01, Meta loss averaged over last 500 steps = 3.4523e-01, PNorm = 99.6085, GNorm = 0.2712
Meta loss on this task batch = 3.6706e-01, Meta loss averaged over last 500 steps = 3.4528e-01, PNorm = 99.6165, GNorm = 0.2835
Meta loss on this task batch = 3.8352e-01, Meta loss averaged over last 500 steps = 3.4541e-01, PNorm = 99.6242, GNorm = 0.3830
Meta loss on this task batch = 2.5212e-01, Meta loss averaged over last 500 steps = 3.4520e-01, PNorm = 99.6323, GNorm = 0.3054
Meta loss on this task batch = 3.7000e-01, Meta loss averaged over last 500 steps = 3.4521e-01, PNorm = 99.6403, GNorm = 0.4839
Meta loss on this task batch = 3.3154e-01, Meta loss averaged over last 500 steps = 3.4506e-01, PNorm = 99.6481, GNorm = 0.3420
Meta loss on this task batch = 3.6520e-01, Meta loss averaged over last 500 steps = 3.4499e-01, PNorm = 99.6568, GNorm = 0.3000
Meta loss on this task batch = 3.7036e-01, Meta loss averaged over last 500 steps = 3.4510e-01, PNorm = 99.6658, GNorm = 0.4478
Meta loss on this task batch = 3.2134e-01, Meta loss averaged over last 500 steps = 3.4497e-01, PNorm = 99.6743, GNorm = 0.2856
Meta loss on this task batch = 3.2743e-01, Meta loss averaged over last 500 steps = 3.4491e-01, PNorm = 99.6832, GNorm = 0.2814
Meta loss on this task batch = 3.3876e-01, Meta loss averaged over last 500 steps = 3.4483e-01, PNorm = 99.6929, GNorm = 0.3117
Meta loss on this task batch = 3.7234e-01, Meta loss averaged over last 500 steps = 3.4486e-01, PNorm = 99.7025, GNorm = 0.3471
Meta loss on this task batch = 3.2966e-01, Meta loss averaged over last 500 steps = 3.4476e-01, PNorm = 99.7121, GNorm = 0.2401
Meta loss on this task batch = 3.0580e-01, Meta loss averaged over last 500 steps = 3.4463e-01, PNorm = 99.7229, GNorm = 0.2219
Meta loss on this task batch = 3.7993e-01, Meta loss averaged over last 500 steps = 3.4482e-01, PNorm = 99.7336, GNorm = 0.4060
Took 111.16682076454163 seconds to complete one epoch of meta training
Took 119.05970501899719 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490412
Epoch 226
Meta loss on this task batch = 3.7322e-01, Meta loss averaged over last 500 steps = 3.4492e-01, PNorm = 99.7444, GNorm = 0.3340
Meta loss on this task batch = 3.5027e-01, Meta loss averaged over last 500 steps = 3.4498e-01, PNorm = 99.7545, GNorm = 0.3266
Meta loss on this task batch = 3.0636e-01, Meta loss averaged over last 500 steps = 3.4483e-01, PNorm = 99.7650, GNorm = 0.2449
Meta loss on this task batch = 3.0823e-01, Meta loss averaged over last 500 steps = 3.4475e-01, PNorm = 99.7758, GNorm = 0.2869
Meta loss on this task batch = 3.6192e-01, Meta loss averaged over last 500 steps = 3.4473e-01, PNorm = 99.7860, GNorm = 0.2995
Meta loss on this task batch = 3.2178e-01, Meta loss averaged over last 500 steps = 3.4483e-01, PNorm = 99.7964, GNorm = 0.2806
Meta loss on this task batch = 3.3200e-01, Meta loss averaged over last 500 steps = 3.4483e-01, PNorm = 99.8070, GNorm = 0.3411
Meta loss on this task batch = 3.9555e-01, Meta loss averaged over last 500 steps = 3.4490e-01, PNorm = 99.8171, GNorm = 0.3453
Meta loss on this task batch = 3.7068e-01, Meta loss averaged over last 500 steps = 3.4489e-01, PNorm = 99.8267, GNorm = 0.2718
Meta loss on this task batch = 3.0723e-01, Meta loss averaged over last 500 steps = 3.4477e-01, PNorm = 99.8370, GNorm = 0.2298
Meta loss on this task batch = 2.9433e-01, Meta loss averaged over last 500 steps = 3.4456e-01, PNorm = 99.8479, GNorm = 0.3448
Meta loss on this task batch = 2.9387e-01, Meta loss averaged over last 500 steps = 3.4441e-01, PNorm = 99.8583, GNorm = 0.3269
Meta loss on this task batch = 3.6367e-01, Meta loss averaged over last 500 steps = 3.4448e-01, PNorm = 99.8679, GNorm = 0.2356
Meta loss on this task batch = 3.6224e-01, Meta loss averaged over last 500 steps = 3.4448e-01, PNorm = 99.8766, GNorm = 0.2821
Meta loss on this task batch = 3.4661e-01, Meta loss averaged over last 500 steps = 3.4444e-01, PNorm = 99.8848, GNorm = 0.3485
Meta loss on this task batch = 3.5905e-01, Meta loss averaged over last 500 steps = 3.4463e-01, PNorm = 99.8923, GNorm = 0.2671
Meta loss on this task batch = 2.8173e-01, Meta loss averaged over last 500 steps = 3.4450e-01, PNorm = 99.9009, GNorm = 0.4010
Meta loss on this task batch = 2.9926e-01, Meta loss averaged over last 500 steps = 3.4432e-01, PNorm = 99.9103, GNorm = 0.2327
Meta loss on this task batch = 3.5788e-01, Meta loss averaged over last 500 steps = 3.4444e-01, PNorm = 99.9195, GNorm = 0.3301
Took 110.86772751808167 seconds to complete one epoch of meta training
Took 118.61494612693787 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474234
Epoch 227
Meta loss on this task batch = 3.8095e-01, Meta loss averaged over last 500 steps = 3.4451e-01, PNorm = 99.9288, GNorm = 0.2296
Meta loss on this task batch = 3.3045e-01, Meta loss averaged over last 500 steps = 3.4444e-01, PNorm = 99.9388, GNorm = 0.3516
Meta loss on this task batch = 3.2100e-01, Meta loss averaged over last 500 steps = 3.4437e-01, PNorm = 99.9485, GNorm = 0.2668
Meta loss on this task batch = 2.4548e-01, Meta loss averaged over last 500 steps = 3.4415e-01, PNorm = 99.9586, GNorm = 0.2689
Meta loss on this task batch = 3.0575e-01, Meta loss averaged over last 500 steps = 3.4408e-01, PNorm = 99.9682, GNorm = 0.2596
Meta loss on this task batch = 3.3943e-01, Meta loss averaged over last 500 steps = 3.4407e-01, PNorm = 99.9770, GNorm = 0.3018
Meta loss on this task batch = 3.6719e-01, Meta loss averaged over last 500 steps = 3.4410e-01, PNorm = 99.9852, GNorm = 0.2906
Meta loss on this task batch = 3.3696e-01, Meta loss averaged over last 500 steps = 3.4411e-01, PNorm = 99.9932, GNorm = 0.3697
Meta loss on this task batch = 3.4562e-01, Meta loss averaged over last 500 steps = 3.4403e-01, PNorm = 100.0004, GNorm = 0.3442
Meta loss on this task batch = 3.6547e-01, Meta loss averaged over last 500 steps = 3.4414e-01, PNorm = 100.0066, GNorm = 0.3372
Meta loss on this task batch = 3.1868e-01, Meta loss averaged over last 500 steps = 3.4412e-01, PNorm = 100.0131, GNorm = 0.2409
Meta loss on this task batch = 3.9736e-01, Meta loss averaged over last 500 steps = 3.4420e-01, PNorm = 100.0199, GNorm = 0.2748
Meta loss on this task batch = 4.2123e-01, Meta loss averaged over last 500 steps = 3.4442e-01, PNorm = 100.0257, GNorm = 0.3513
Meta loss on this task batch = 3.1402e-01, Meta loss averaged over last 500 steps = 3.4432e-01, PNorm = 100.0323, GNorm = 0.2638
Meta loss on this task batch = 3.2554e-01, Meta loss averaged over last 500 steps = 3.4438e-01, PNorm = 100.0398, GNorm = 0.2309
Meta loss on this task batch = 3.5314e-01, Meta loss averaged over last 500 steps = 3.4442e-01, PNorm = 100.0474, GNorm = 0.2242
Meta loss on this task batch = 3.5337e-01, Meta loss averaged over last 500 steps = 3.4449e-01, PNorm = 100.0542, GNorm = 0.3097
Meta loss on this task batch = 3.2747e-01, Meta loss averaged over last 500 steps = 3.4440e-01, PNorm = 100.0620, GNorm = 0.4103
Meta loss on this task batch = 3.2879e-01, Meta loss averaged over last 500 steps = 3.4438e-01, PNorm = 100.0706, GNorm = 0.2607
Took 111.18945336341858 seconds to complete one epoch of meta training
Took 119.33844780921936 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470172
Epoch 228
Meta loss on this task batch = 3.6229e-01, Meta loss averaged over last 500 steps = 3.4436e-01, PNorm = 100.0801, GNorm = 0.3331
Meta loss on this task batch = 3.5210e-01, Meta loss averaged over last 500 steps = 3.4437e-01, PNorm = 100.0900, GNorm = 0.3542
Meta loss on this task batch = 3.1941e-01, Meta loss averaged over last 500 steps = 3.4423e-01, PNorm = 100.1004, GNorm = 0.2032
Meta loss on this task batch = 3.6754e-01, Meta loss averaged over last 500 steps = 3.4425e-01, PNorm = 100.1106, GNorm = 0.2647
Meta loss on this task batch = 3.9603e-01, Meta loss averaged over last 500 steps = 3.4431e-01, PNorm = 100.1200, GNorm = 0.2713
Meta loss on this task batch = 3.1986e-01, Meta loss averaged over last 500 steps = 3.4416e-01, PNorm = 100.1297, GNorm = 0.2476
Meta loss on this task batch = 2.9366e-01, Meta loss averaged over last 500 steps = 3.4401e-01, PNorm = 100.1395, GNorm = 0.2340
Meta loss on this task batch = 3.1512e-01, Meta loss averaged over last 500 steps = 3.4404e-01, PNorm = 100.1492, GNorm = 0.2448
Meta loss on this task batch = 3.5542e-01, Meta loss averaged over last 500 steps = 3.4406e-01, PNorm = 100.1584, GNorm = 0.3569
Meta loss on this task batch = 3.4591e-01, Meta loss averaged over last 500 steps = 3.4407e-01, PNorm = 100.1675, GNorm = 0.3633
Meta loss on this task batch = 3.8049e-01, Meta loss averaged over last 500 steps = 3.4406e-01, PNorm = 100.1752, GNorm = 0.4162
Meta loss on this task batch = 3.3617e-01, Meta loss averaged over last 500 steps = 3.4398e-01, PNorm = 100.1827, GNorm = 0.2586
Meta loss on this task batch = 3.1518e-01, Meta loss averaged over last 500 steps = 3.4397e-01, PNorm = 100.1901, GNorm = 0.3175
Meta loss on this task batch = 3.3384e-01, Meta loss averaged over last 500 steps = 3.4389e-01, PNorm = 100.1964, GNorm = 0.3183
Meta loss on this task batch = 3.5932e-01, Meta loss averaged over last 500 steps = 3.4395e-01, PNorm = 100.2036, GNorm = 0.2871
Meta loss on this task batch = 3.2812e-01, Meta loss averaged over last 500 steps = 3.4383e-01, PNorm = 100.2105, GNorm = 0.2297
Meta loss on this task batch = 3.7742e-01, Meta loss averaged over last 500 steps = 3.4393e-01, PNorm = 100.2176, GNorm = 0.2435
Meta loss on this task batch = 3.2303e-01, Meta loss averaged over last 500 steps = 3.4391e-01, PNorm = 100.2251, GNorm = 0.3457
Meta loss on this task batch = 2.6465e-01, Meta loss averaged over last 500 steps = 3.4381e-01, PNorm = 100.2339, GNorm = 0.2693
Took 111.75182700157166 seconds to complete one epoch of meta training
Took 119.19789266586304 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471566
Epoch 229
Meta loss on this task batch = 2.9143e-01, Meta loss averaged over last 500 steps = 3.4378e-01, PNorm = 100.2428, GNorm = 0.2200
Meta loss on this task batch = 3.4036e-01, Meta loss averaged over last 500 steps = 3.4376e-01, PNorm = 100.2515, GNorm = 0.2874
Meta loss on this task batch = 2.8542e-01, Meta loss averaged over last 500 steps = 3.4363e-01, PNorm = 100.2607, GNorm = 0.2446
Meta loss on this task batch = 3.6169e-01, Meta loss averaged over last 500 steps = 3.4364e-01, PNorm = 100.2700, GNorm = 0.3054
Meta loss on this task batch = 3.5379e-01, Meta loss averaged over last 500 steps = 3.4351e-01, PNorm = 100.2795, GNorm = 0.3712
Meta loss on this task batch = 3.2510e-01, Meta loss averaged over last 500 steps = 3.4341e-01, PNorm = 100.2887, GNorm = 0.2683
Meta loss on this task batch = 3.3883e-01, Meta loss averaged over last 500 steps = 3.4334e-01, PNorm = 100.2978, GNorm = 0.3435
Meta loss on this task batch = 3.6310e-01, Meta loss averaged over last 500 steps = 3.4337e-01, PNorm = 100.3063, GNorm = 0.3479
Meta loss on this task batch = 2.8982e-01, Meta loss averaged over last 500 steps = 3.4338e-01, PNorm = 100.3148, GNorm = 0.2834
Meta loss on this task batch = 3.1828e-01, Meta loss averaged over last 500 steps = 3.4327e-01, PNorm = 100.3234, GNorm = 0.3364
Meta loss on this task batch = 2.8072e-01, Meta loss averaged over last 500 steps = 3.4317e-01, PNorm = 100.3324, GNorm = 0.2749
Meta loss on this task batch = 3.7729e-01, Meta loss averaged over last 500 steps = 3.4316e-01, PNorm = 100.3415, GNorm = 0.2826
Meta loss on this task batch = 2.6959e-01, Meta loss averaged over last 500 steps = 3.4306e-01, PNorm = 100.3515, GNorm = 0.2948
Meta loss on this task batch = 3.6189e-01, Meta loss averaged over last 500 steps = 3.4305e-01, PNorm = 100.3613, GNorm = 0.2667
Meta loss on this task batch = 3.2942e-01, Meta loss averaged over last 500 steps = 3.4306e-01, PNorm = 100.3704, GNorm = 0.2581
Meta loss on this task batch = 3.6827e-01, Meta loss averaged over last 500 steps = 3.4302e-01, PNorm = 100.3793, GNorm = 0.2236
Meta loss on this task batch = 3.4116e-01, Meta loss averaged over last 500 steps = 3.4304e-01, PNorm = 100.3891, GNorm = 0.2461
Meta loss on this task batch = 3.6294e-01, Meta loss averaged over last 500 steps = 3.4311e-01, PNorm = 100.3992, GNorm = 0.3296
Meta loss on this task batch = 3.7325e-01, Meta loss averaged over last 500 steps = 3.4320e-01, PNorm = 100.4084, GNorm = 0.3475
Took 112.84018516540527 seconds to complete one epoch of meta training
Took 119.78903126716614 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472026
Epoch 230
Meta loss on this task batch = 3.3441e-01, Meta loss averaged over last 500 steps = 3.4320e-01, PNorm = 100.4174, GNorm = 0.4204
Meta loss on this task batch = 3.1000e-01, Meta loss averaged over last 500 steps = 3.4304e-01, PNorm = 100.4258, GNorm = 0.2060
Meta loss on this task batch = 3.5206e-01, Meta loss averaged over last 500 steps = 3.4317e-01, PNorm = 100.4336, GNorm = 0.2725
Meta loss on this task batch = 3.7133e-01, Meta loss averaged over last 500 steps = 3.4328e-01, PNorm = 100.4406, GNorm = 0.2927
Meta loss on this task batch = 3.7048e-01, Meta loss averaged over last 500 steps = 3.4324e-01, PNorm = 100.4480, GNorm = 0.2280
Meta loss on this task batch = 2.8746e-01, Meta loss averaged over last 500 steps = 3.4307e-01, PNorm = 100.4560, GNorm = 0.2037
Meta loss on this task batch = 3.1982e-01, Meta loss averaged over last 500 steps = 3.4297e-01, PNorm = 100.4641, GNorm = 0.3072
Meta loss on this task batch = 3.6441e-01, Meta loss averaged over last 500 steps = 3.4296e-01, PNorm = 100.4724, GNorm = 0.2178
Meta loss on this task batch = 3.2582e-01, Meta loss averaged over last 500 steps = 3.4290e-01, PNorm = 100.4813, GNorm = 0.2418
Meta loss on this task batch = 3.6771e-01, Meta loss averaged over last 500 steps = 3.4287e-01, PNorm = 100.4902, GNorm = 0.3090
Meta loss on this task batch = 3.2751e-01, Meta loss averaged over last 500 steps = 3.4284e-01, PNorm = 100.4987, GNorm = 0.2212
Meta loss on this task batch = 3.7207e-01, Meta loss averaged over last 500 steps = 3.4283e-01, PNorm = 100.5073, GNorm = 0.2913
Meta loss on this task batch = 3.0609e-01, Meta loss averaged over last 500 steps = 3.4283e-01, PNorm = 100.5164, GNorm = 0.2936
Meta loss on this task batch = 3.7403e-01, Meta loss averaged over last 500 steps = 3.4290e-01, PNorm = 100.5249, GNorm = 0.3130
Meta loss on this task batch = 3.9476e-01, Meta loss averaged over last 500 steps = 3.4302e-01, PNorm = 100.5334, GNorm = 0.2646
Meta loss on this task batch = 3.2200e-01, Meta loss averaged over last 500 steps = 3.4297e-01, PNorm = 100.5421, GNorm = 0.3149
Meta loss on this task batch = 3.3716e-01, Meta loss averaged over last 500 steps = 3.4302e-01, PNorm = 100.5508, GNorm = 0.3269
Meta loss on this task batch = 3.2653e-01, Meta loss averaged over last 500 steps = 3.4295e-01, PNorm = 100.5595, GNorm = 0.4279
Meta loss on this task batch = 3.7007e-01, Meta loss averaged over last 500 steps = 3.4301e-01, PNorm = 100.5673, GNorm = 0.3384
Took 111.77295565605164 seconds to complete one epoch of meta training
Took 119.44061255455017 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480754
Epoch 231
Meta loss on this task batch = 4.5063e-01, Meta loss averaged over last 500 steps = 3.4322e-01, PNorm = 100.5744, GNorm = 0.3902
Meta loss on this task batch = 3.3272e-01, Meta loss averaged over last 500 steps = 3.4317e-01, PNorm = 100.5821, GNorm = 0.2846
Meta loss on this task batch = 3.4208e-01, Meta loss averaged over last 500 steps = 3.4321e-01, PNorm = 100.5892, GNorm = 0.3529
Meta loss on this task batch = 3.2564e-01, Meta loss averaged over last 500 steps = 3.4322e-01, PNorm = 100.5966, GNorm = 0.2578
Meta loss on this task batch = 3.4659e-01, Meta loss averaged over last 500 steps = 3.4312e-01, PNorm = 100.6049, GNorm = 0.2271
Meta loss on this task batch = 3.5029e-01, Meta loss averaged over last 500 steps = 3.4313e-01, PNorm = 100.6135, GNorm = 0.2527
Meta loss on this task batch = 3.2954e-01, Meta loss averaged over last 500 steps = 3.4312e-01, PNorm = 100.6221, GNorm = 0.2181
Meta loss on this task batch = 3.5180e-01, Meta loss averaged over last 500 steps = 3.4322e-01, PNorm = 100.6303, GNorm = 0.2722
Meta loss on this task batch = 3.6134e-01, Meta loss averaged over last 500 steps = 3.4321e-01, PNorm = 100.6393, GNorm = 0.4097
Meta loss on this task batch = 3.1207e-01, Meta loss averaged over last 500 steps = 3.4298e-01, PNorm = 100.6485, GNorm = 0.2078
Meta loss on this task batch = 3.7294e-01, Meta loss averaged over last 500 steps = 3.4306e-01, PNorm = 100.6570, GNorm = 0.3400
Meta loss on this task batch = 3.2470e-01, Meta loss averaged over last 500 steps = 3.4293e-01, PNorm = 100.6656, GNorm = 0.2309
Meta loss on this task batch = 3.1647e-01, Meta loss averaged over last 500 steps = 3.4279e-01, PNorm = 100.6741, GNorm = 0.2488
Meta loss on this task batch = 2.9948e-01, Meta loss averaged over last 500 steps = 3.4280e-01, PNorm = 100.6835, GNorm = 0.3619
Meta loss on this task batch = 3.6357e-01, Meta loss averaged over last 500 steps = 3.4285e-01, PNorm = 100.6914, GNorm = 0.3842
Meta loss on this task batch = 3.3988e-01, Meta loss averaged over last 500 steps = 3.4289e-01, PNorm = 100.6989, GNorm = 0.2478
Meta loss on this task batch = 4.0518e-01, Meta loss averaged over last 500 steps = 3.4310e-01, PNorm = 100.7071, GNorm = 0.3470
Meta loss on this task batch = 3.2241e-01, Meta loss averaged over last 500 steps = 3.4306e-01, PNorm = 100.7157, GNorm = 0.2486
Meta loss on this task batch = 2.7965e-01, Meta loss averaged over last 500 steps = 3.4291e-01, PNorm = 100.7247, GNorm = 0.2868
Took 112.50108861923218 seconds to complete one epoch of meta training
Took 120.56797313690186 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461033
Epoch 232
Meta loss on this task batch = 3.2655e-01, Meta loss averaged over last 500 steps = 3.4298e-01, PNorm = 100.7345, GNorm = 0.4191
Meta loss on this task batch = 2.9515e-01, Meta loss averaged over last 500 steps = 3.4278e-01, PNorm = 100.7440, GNorm = 0.2478
Meta loss on this task batch = 3.4740e-01, Meta loss averaged over last 500 steps = 3.4278e-01, PNorm = 100.7535, GNorm = 0.2896
Meta loss on this task batch = 3.0183e-01, Meta loss averaged over last 500 steps = 3.4262e-01, PNorm = 100.7638, GNorm = 0.2332
Meta loss on this task batch = 3.0593e-01, Meta loss averaged over last 500 steps = 3.4247e-01, PNorm = 100.7739, GNorm = 0.2412
Meta loss on this task batch = 3.9159e-01, Meta loss averaged over last 500 steps = 3.4257e-01, PNorm = 100.7833, GNorm = 0.4539
Meta loss on this task batch = 3.4139e-01, Meta loss averaged over last 500 steps = 3.4254e-01, PNorm = 100.7925, GNorm = 0.2522
Meta loss on this task batch = 3.5446e-01, Meta loss averaged over last 500 steps = 3.4260e-01, PNorm = 100.8016, GNorm = 0.2930
Meta loss on this task batch = 3.2103e-01, Meta loss averaged over last 500 steps = 3.4254e-01, PNorm = 100.8107, GNorm = 0.2663
Meta loss on this task batch = 3.6673e-01, Meta loss averaged over last 500 steps = 3.4249e-01, PNorm = 100.8197, GNorm = 0.2357
Meta loss on this task batch = 3.6429e-01, Meta loss averaged over last 500 steps = 3.4257e-01, PNorm = 100.8286, GNorm = 0.3051
Meta loss on this task batch = 3.8050e-01, Meta loss averaged over last 500 steps = 3.4271e-01, PNorm = 100.8379, GNorm = 0.2566
Meta loss on this task batch = 3.6968e-01, Meta loss averaged over last 500 steps = 3.4286e-01, PNorm = 100.8472, GNorm = 0.2649
Meta loss on this task batch = 3.7332e-01, Meta loss averaged over last 500 steps = 3.4286e-01, PNorm = 100.8570, GNorm = 0.2961
Meta loss on this task batch = 3.3747e-01, Meta loss averaged over last 500 steps = 3.4283e-01, PNorm = 100.8666, GNorm = 0.2957
Meta loss on this task batch = 3.4266e-01, Meta loss averaged over last 500 steps = 3.4279e-01, PNorm = 100.8763, GNorm = 0.3612
Meta loss on this task batch = 3.3895e-01, Meta loss averaged over last 500 steps = 3.4291e-01, PNorm = 100.8855, GNorm = 0.3232
Meta loss on this task batch = 3.5026e-01, Meta loss averaged over last 500 steps = 3.4301e-01, PNorm = 100.8940, GNorm = 0.2810
Meta loss on this task batch = 3.1919e-01, Meta loss averaged over last 500 steps = 3.4294e-01, PNorm = 100.9027, GNorm = 0.3656
Took 113.91572451591492 seconds to complete one epoch of meta training
Took 120.95075345039368 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480703
Epoch 233
Meta loss on this task batch = 2.8873e-01, Meta loss averaged over last 500 steps = 3.4271e-01, PNorm = 100.9119, GNorm = 0.2432
Meta loss on this task batch = 3.1901e-01, Meta loss averaged over last 500 steps = 3.4274e-01, PNorm = 100.9217, GNorm = 0.2739
Meta loss on this task batch = 3.0147e-01, Meta loss averaged over last 500 steps = 3.4262e-01, PNorm = 100.9308, GNorm = 0.2184
Meta loss on this task batch = 3.8176e-01, Meta loss averaged over last 500 steps = 3.4272e-01, PNorm = 100.9388, GNorm = 0.3091
Meta loss on this task batch = 3.5456e-01, Meta loss averaged over last 500 steps = 3.4262e-01, PNorm = 100.9458, GNorm = 0.3060
Meta loss on this task batch = 3.0058e-01, Meta loss averaged over last 500 steps = 3.4231e-01, PNorm = 100.9530, GNorm = 0.4542
Meta loss on this task batch = 3.6507e-01, Meta loss averaged over last 500 steps = 3.4219e-01, PNorm = 100.9597, GNorm = 0.2454
Meta loss on this task batch = 3.4881e-01, Meta loss averaged over last 500 steps = 3.4222e-01, PNorm = 100.9670, GNorm = 0.2379
Meta loss on this task batch = 3.3471e-01, Meta loss averaged over last 500 steps = 3.4219e-01, PNorm = 100.9743, GNorm = 0.2258
Meta loss on this task batch = 3.5204e-01, Meta loss averaged over last 500 steps = 3.4225e-01, PNorm = 100.9828, GNorm = 0.3326
Meta loss on this task batch = 3.4757e-01, Meta loss averaged over last 500 steps = 3.4230e-01, PNorm = 100.9909, GNorm = 0.3424
Meta loss on this task batch = 3.4692e-01, Meta loss averaged over last 500 steps = 3.4236e-01, PNorm = 100.9988, GNorm = 0.2536
Meta loss on this task batch = 3.1815e-01, Meta loss averaged over last 500 steps = 3.4225e-01, PNorm = 101.0064, GNorm = 0.2530
Meta loss on this task batch = 3.5905e-01, Meta loss averaged over last 500 steps = 3.4228e-01, PNorm = 101.0145, GNorm = 0.2672
Meta loss on this task batch = 3.5094e-01, Meta loss averaged over last 500 steps = 3.4225e-01, PNorm = 101.0230, GNorm = 0.2481
Meta loss on this task batch = 3.2379e-01, Meta loss averaged over last 500 steps = 3.4216e-01, PNorm = 101.0316, GNorm = 0.3221
Meta loss on this task batch = 3.1550e-01, Meta loss averaged over last 500 steps = 3.4213e-01, PNorm = 101.0405, GNorm = 0.2358
Meta loss on this task batch = 3.6202e-01, Meta loss averaged over last 500 steps = 3.4218e-01, PNorm = 101.0492, GNorm = 0.2987
Meta loss on this task batch = 3.5789e-01, Meta loss averaged over last 500 steps = 3.4223e-01, PNorm = 101.0587, GNorm = 0.2911
Took 108.89416003227234 seconds to complete one epoch of meta training
Took 116.79160404205322 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478020
Epoch 234
Meta loss on this task batch = 3.4537e-01, Meta loss averaged over last 500 steps = 3.4218e-01, PNorm = 101.0677, GNorm = 0.2544
Meta loss on this task batch = 3.5206e-01, Meta loss averaged over last 500 steps = 3.4216e-01, PNorm = 101.0761, GNorm = 0.2897
Meta loss on this task batch = 3.4877e-01, Meta loss averaged over last 500 steps = 3.4222e-01, PNorm = 101.0844, GNorm = 0.2654
Meta loss on this task batch = 2.7687e-01, Meta loss averaged over last 500 steps = 3.4208e-01, PNorm = 101.0926, GNorm = 0.2388
Meta loss on this task batch = 3.1939e-01, Meta loss averaged over last 500 steps = 3.4205e-01, PNorm = 101.1017, GNorm = 0.2707
Meta loss on this task batch = 3.5930e-01, Meta loss averaged over last 500 steps = 3.4191e-01, PNorm = 101.1104, GNorm = 0.2549
Meta loss on this task batch = 3.3854e-01, Meta loss averaged over last 500 steps = 3.4171e-01, PNorm = 101.1189, GNorm = 0.2490
Meta loss on this task batch = 3.1769e-01, Meta loss averaged over last 500 steps = 3.4158e-01, PNorm = 101.1267, GNorm = 0.2236
Meta loss on this task batch = 3.4531e-01, Meta loss averaged over last 500 steps = 3.4165e-01, PNorm = 101.1348, GNorm = 0.2833
Meta loss on this task batch = 3.3107e-01, Meta loss averaged over last 500 steps = 3.4159e-01, PNorm = 101.1426, GNorm = 0.3573
Meta loss on this task batch = 3.5910e-01, Meta loss averaged over last 500 steps = 3.4164e-01, PNorm = 101.1513, GNorm = 0.4724
Meta loss on this task batch = 3.8531e-01, Meta loss averaged over last 500 steps = 3.4166e-01, PNorm = 101.1611, GNorm = 0.3513
Meta loss on this task batch = 3.9303e-01, Meta loss averaged over last 500 steps = 3.4191e-01, PNorm = 101.1705, GNorm = 0.5412
Meta loss on this task batch = 3.0214e-01, Meta loss averaged over last 500 steps = 3.4169e-01, PNorm = 101.1803, GNorm = 0.2390
Meta loss on this task batch = 3.3229e-01, Meta loss averaged over last 500 steps = 3.4179e-01, PNorm = 101.1903, GNorm = 0.2602
Meta loss on this task batch = 3.4993e-01, Meta loss averaged over last 500 steps = 3.4191e-01, PNorm = 101.1997, GNorm = 0.2481
Meta loss on this task batch = 3.3097e-01, Meta loss averaged over last 500 steps = 3.4194e-01, PNorm = 101.2101, GNorm = 0.4237
Meta loss on this task batch = 3.7528e-01, Meta loss averaged over last 500 steps = 3.4195e-01, PNorm = 101.2206, GNorm = 0.4755
Meta loss on this task batch = 3.1827e-01, Meta loss averaged over last 500 steps = 3.4198e-01, PNorm = 101.2320, GNorm = 0.5485
Took 111.59801650047302 seconds to complete one epoch of meta training
Took 119.67211961746216 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464659
Epoch 235
Meta loss on this task batch = 3.8103e-01, Meta loss averaged over last 500 steps = 3.4207e-01, PNorm = 101.2424, GNorm = 0.3599
Meta loss on this task batch = 3.3452e-01, Meta loss averaged over last 500 steps = 3.4193e-01, PNorm = 101.2516, GNorm = 0.2397
Meta loss on this task batch = 3.6677e-01, Meta loss averaged over last 500 steps = 3.4200e-01, PNorm = 101.2608, GNorm = 0.2594
Meta loss on this task batch = 3.3659e-01, Meta loss averaged over last 500 steps = 3.4193e-01, PNorm = 101.2695, GNorm = 0.2393
Meta loss on this task batch = 3.3026e-01, Meta loss averaged over last 500 steps = 3.4189e-01, PNorm = 101.2777, GNorm = 0.2758
Meta loss on this task batch = 2.9765e-01, Meta loss averaged over last 500 steps = 3.4174e-01, PNorm = 101.2858, GNorm = 0.2421
Meta loss on this task batch = 3.0120e-01, Meta loss averaged over last 500 steps = 3.4169e-01, PNorm = 101.2942, GNorm = 0.2738
Meta loss on this task batch = 3.3667e-01, Meta loss averaged over last 500 steps = 3.4166e-01, PNorm = 101.3030, GNorm = 0.2614
Meta loss on this task batch = 3.4366e-01, Meta loss averaged over last 500 steps = 3.4163e-01, PNorm = 101.3107, GNorm = 0.2759
Meta loss on this task batch = 3.9106e-01, Meta loss averaged over last 500 steps = 3.4170e-01, PNorm = 101.3178, GNorm = 0.3715
Meta loss on this task batch = 3.7075e-01, Meta loss averaged over last 500 steps = 3.4168e-01, PNorm = 101.3251, GNorm = 0.2408
Meta loss on this task batch = 3.5680e-01, Meta loss averaged over last 500 steps = 3.4169e-01, PNorm = 101.3321, GNorm = 0.2687
Meta loss on this task batch = 3.1109e-01, Meta loss averaged over last 500 steps = 3.4164e-01, PNorm = 101.3397, GNorm = 0.2123
Meta loss on this task batch = 3.7306e-01, Meta loss averaged over last 500 steps = 3.4184e-01, PNorm = 101.3471, GNorm = 0.3880
Meta loss on this task batch = 3.1783e-01, Meta loss averaged over last 500 steps = 3.4170e-01, PNorm = 101.3540, GNorm = 0.3510
Meta loss on this task batch = 3.3467e-01, Meta loss averaged over last 500 steps = 3.4179e-01, PNorm = 101.3616, GNorm = 0.2434
Meta loss on this task batch = 2.9643e-01, Meta loss averaged over last 500 steps = 3.4158e-01, PNorm = 101.3706, GNorm = 0.2461
Meta loss on this task batch = 3.2869e-01, Meta loss averaged over last 500 steps = 3.4154e-01, PNorm = 101.3805, GNorm = 0.2206
Meta loss on this task batch = 3.2204e-01, Meta loss averaged over last 500 steps = 3.4155e-01, PNorm = 101.3915, GNorm = 0.2948
Took 110.95191860198975 seconds to complete one epoch of meta training
Took 119.26815676689148 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481101
Epoch 236
Meta loss on this task batch = 3.0873e-01, Meta loss averaged over last 500 steps = 3.4154e-01, PNorm = 101.4026, GNorm = 0.2117
Meta loss on this task batch = 3.9187e-01, Meta loss averaged over last 500 steps = 3.4163e-01, PNorm = 101.4128, GNorm = 0.2556
Meta loss on this task batch = 3.5354e-01, Meta loss averaged over last 500 steps = 3.4179e-01, PNorm = 101.4227, GNorm = 0.3495
Meta loss on this task batch = 3.0646e-01, Meta loss averaged over last 500 steps = 3.4170e-01, PNorm = 101.4333, GNorm = 0.3907
Meta loss on this task batch = 3.3523e-01, Meta loss averaged over last 500 steps = 3.4162e-01, PNorm = 101.4435, GNorm = 0.2438
Meta loss on this task batch = 3.5077e-01, Meta loss averaged over last 500 steps = 3.4160e-01, PNorm = 101.4537, GNorm = 0.3232
Meta loss on this task batch = 3.7057e-01, Meta loss averaged over last 500 steps = 3.4166e-01, PNorm = 101.4626, GNorm = 0.3370
Meta loss on this task batch = 3.2180e-01, Meta loss averaged over last 500 steps = 3.4155e-01, PNorm = 101.4726, GNorm = 0.2825
Meta loss on this task batch = 3.8062e-01, Meta loss averaged over last 500 steps = 3.4163e-01, PNorm = 101.4813, GNorm = 0.4220
Meta loss on this task batch = 3.7003e-01, Meta loss averaged over last 500 steps = 3.4164e-01, PNorm = 101.4894, GNorm = 0.2620
Meta loss on this task batch = 3.3495e-01, Meta loss averaged over last 500 steps = 3.4174e-01, PNorm = 101.4977, GNorm = 0.3285
Meta loss on this task batch = 3.1766e-01, Meta loss averaged over last 500 steps = 3.4166e-01, PNorm = 101.5056, GNorm = 0.2970
Meta loss on this task batch = 3.5056e-01, Meta loss averaged over last 500 steps = 3.4171e-01, PNorm = 101.5142, GNorm = 0.3965
Meta loss on this task batch = 2.8066e-01, Meta loss averaged over last 500 steps = 3.4151e-01, PNorm = 101.5234, GNorm = 0.3711
Meta loss on this task batch = 3.6056e-01, Meta loss averaged over last 500 steps = 3.4150e-01, PNorm = 101.5324, GNorm = 0.3039
Meta loss on this task batch = 3.2919e-01, Meta loss averaged over last 500 steps = 3.4149e-01, PNorm = 101.5419, GNorm = 0.3849
Meta loss on this task batch = 3.1738e-01, Meta loss averaged over last 500 steps = 3.4145e-01, PNorm = 101.5513, GNorm = 0.2216
Meta loss on this task batch = 3.1860e-01, Meta loss averaged over last 500 steps = 3.4138e-01, PNorm = 101.5603, GNorm = 0.2746
Meta loss on this task batch = 3.1642e-01, Meta loss averaged over last 500 steps = 3.4130e-01, PNorm = 101.5696, GNorm = 0.3941
Took 113.53003764152527 seconds to complete one epoch of meta training
Took 120.48389101028442 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481788
Epoch 237
Meta loss on this task batch = 3.4074e-01, Meta loss averaged over last 500 steps = 3.4133e-01, PNorm = 101.5786, GNorm = 0.2921
Meta loss on this task batch = 3.3036e-01, Meta loss averaged over last 500 steps = 3.4126e-01, PNorm = 101.5885, GNorm = 0.2746
Meta loss on this task batch = 3.5644e-01, Meta loss averaged over last 500 steps = 3.4136e-01, PNorm = 101.5995, GNorm = 0.3962
Meta loss on this task batch = 2.8871e-01, Meta loss averaged over last 500 steps = 3.4126e-01, PNorm = 101.6108, GNorm = 0.2418
Meta loss on this task batch = 3.0086e-01, Meta loss averaged over last 500 steps = 3.4115e-01, PNorm = 101.6223, GNorm = 0.2578
Meta loss on this task batch = 3.6917e-01, Meta loss averaged over last 500 steps = 3.4113e-01, PNorm = 101.6331, GNorm = 0.2363
Meta loss on this task batch = 3.1261e-01, Meta loss averaged over last 500 steps = 3.4108e-01, PNorm = 101.6434, GNorm = 0.2254
Meta loss on this task batch = 3.1054e-01, Meta loss averaged over last 500 steps = 3.4103e-01, PNorm = 101.6553, GNorm = 0.6296
Meta loss on this task batch = 3.4043e-01, Meta loss averaged over last 500 steps = 3.4102e-01, PNorm = 101.6671, GNorm = 0.3603
Meta loss on this task batch = 2.9386e-01, Meta loss averaged over last 500 steps = 3.4084e-01, PNorm = 101.6791, GNorm = 0.2752
Meta loss on this task batch = 3.2840e-01, Meta loss averaged over last 500 steps = 3.4075e-01, PNorm = 101.6914, GNorm = 0.3068
Meta loss on this task batch = 3.1802e-01, Meta loss averaged over last 500 steps = 3.4066e-01, PNorm = 101.7040, GNorm = 0.3024
Meta loss on this task batch = 3.3130e-01, Meta loss averaged over last 500 steps = 3.4068e-01, PNorm = 101.7163, GNorm = 0.2331
Meta loss on this task batch = 3.2355e-01, Meta loss averaged over last 500 steps = 3.4058e-01, PNorm = 101.7283, GNorm = 0.3396
Meta loss on this task batch = 2.8492e-01, Meta loss averaged over last 500 steps = 3.4041e-01, PNorm = 101.7402, GNorm = 0.2256
Meta loss on this task batch = 3.5471e-01, Meta loss averaged over last 500 steps = 3.4049e-01, PNorm = 101.7508, GNorm = 0.3050
Meta loss on this task batch = 3.8025e-01, Meta loss averaged over last 500 steps = 3.4049e-01, PNorm = 101.7602, GNorm = 0.3189
Meta loss on this task batch = 3.5574e-01, Meta loss averaged over last 500 steps = 3.4052e-01, PNorm = 101.7688, GNorm = 0.2626
Meta loss on this task batch = 3.3598e-01, Meta loss averaged over last 500 steps = 3.4040e-01, PNorm = 101.7778, GNorm = 0.4365
Took 108.58420825004578 seconds to complete one epoch of meta training
Took 116.59225869178772 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491519
Epoch 238
Meta loss on this task batch = 3.0462e-01, Meta loss averaged over last 500 steps = 3.4039e-01, PNorm = 101.7862, GNorm = 0.3082
Meta loss on this task batch = 3.6939e-01, Meta loss averaged over last 500 steps = 3.4046e-01, PNorm = 101.7937, GNorm = 0.4188
Meta loss on this task batch = 3.8336e-01, Meta loss averaged over last 500 steps = 3.4052e-01, PNorm = 101.8007, GNorm = 0.3952
Meta loss on this task batch = 2.9577e-01, Meta loss averaged over last 500 steps = 3.4058e-01, PNorm = 101.8076, GNorm = 0.3329
Meta loss on this task batch = 3.3647e-01, Meta loss averaged over last 500 steps = 3.4063e-01, PNorm = 101.8150, GNorm = 0.3125
Meta loss on this task batch = 3.1390e-01, Meta loss averaged over last 500 steps = 3.4063e-01, PNorm = 101.8228, GNorm = 0.2577
Meta loss on this task batch = 3.5639e-01, Meta loss averaged over last 500 steps = 3.4065e-01, PNorm = 101.8303, GNorm = 0.2688
Meta loss on this task batch = 3.2733e-01, Meta loss averaged over last 500 steps = 3.4064e-01, PNorm = 101.8377, GNorm = 0.5184
Meta loss on this task batch = 3.4379e-01, Meta loss averaged over last 500 steps = 3.4070e-01, PNorm = 101.8452, GNorm = 0.2816
Meta loss on this task batch = 3.5795e-01, Meta loss averaged over last 500 steps = 3.4063e-01, PNorm = 101.8526, GNorm = 0.2579
Meta loss on this task batch = 3.3136e-01, Meta loss averaged over last 500 steps = 3.4072e-01, PNorm = 101.8610, GNorm = 0.3242
Meta loss on this task batch = 3.5904e-01, Meta loss averaged over last 500 steps = 3.4064e-01, PNorm = 101.8703, GNorm = 0.3005
Meta loss on this task batch = 3.5728e-01, Meta loss averaged over last 500 steps = 3.4062e-01, PNorm = 101.8793, GNorm = 0.2269
Meta loss on this task batch = 4.2216e-01, Meta loss averaged over last 500 steps = 3.4071e-01, PNorm = 101.8893, GNorm = 0.2827
Meta loss on this task batch = 3.7667e-01, Meta loss averaged over last 500 steps = 3.4076e-01, PNorm = 101.8990, GNorm = 0.3291
Meta loss on this task batch = 2.9068e-01, Meta loss averaged over last 500 steps = 3.4066e-01, PNorm = 101.9099, GNorm = 0.3012
Meta loss on this task batch = 3.6949e-01, Meta loss averaged over last 500 steps = 3.4072e-01, PNorm = 101.9221, GNorm = 0.3880
Meta loss on this task batch = 3.7131e-01, Meta loss averaged over last 500 steps = 3.4084e-01, PNorm = 101.9330, GNorm = 0.2705
Meta loss on this task batch = 3.9495e-01, Meta loss averaged over last 500 steps = 3.4102e-01, PNorm = 101.9422, GNorm = 0.3088
Took 112.22218537330627 seconds to complete one epoch of meta training
Took 119.90895414352417 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469271
Epoch 239
Meta loss on this task batch = 3.3931e-01, Meta loss averaged over last 500 steps = 3.4107e-01, PNorm = 101.9508, GNorm = 0.3367
Meta loss on this task batch = 3.7212e-01, Meta loss averaged over last 500 steps = 3.4111e-01, PNorm = 101.9590, GNorm = 0.2565
Meta loss on this task batch = 3.8967e-01, Meta loss averaged over last 500 steps = 3.4117e-01, PNorm = 101.9670, GNorm = 0.2368
Meta loss on this task batch = 3.6313e-01, Meta loss averaged over last 500 steps = 3.4119e-01, PNorm = 101.9738, GNorm = 0.3930
Meta loss on this task batch = 3.4671e-01, Meta loss averaged over last 500 steps = 3.4128e-01, PNorm = 101.9807, GNorm = 0.2320
Meta loss on this task batch = 3.6942e-01, Meta loss averaged over last 500 steps = 3.4137e-01, PNorm = 101.9882, GNorm = 0.3293
Meta loss on this task batch = 3.8430e-01, Meta loss averaged over last 500 steps = 3.4148e-01, PNorm = 101.9953, GNorm = 0.3618
Meta loss on this task batch = 3.3420e-01, Meta loss averaged over last 500 steps = 3.4152e-01, PNorm = 102.0022, GNorm = 0.2714
Meta loss on this task batch = 3.7702e-01, Meta loss averaged over last 500 steps = 3.4153e-01, PNorm = 102.0096, GNorm = 0.2940
Meta loss on this task batch = 3.1321e-01, Meta loss averaged over last 500 steps = 3.4151e-01, PNorm = 102.0167, GNorm = 0.2688
Meta loss on this task batch = 3.7871e-01, Meta loss averaged over last 500 steps = 3.4156e-01, PNorm = 102.0244, GNorm = 0.3106
Meta loss on this task batch = 3.1684e-01, Meta loss averaged over last 500 steps = 3.4144e-01, PNorm = 102.0329, GNorm = 0.2676
Meta loss on this task batch = 3.8630e-01, Meta loss averaged over last 500 steps = 3.4162e-01, PNorm = 102.0415, GNorm = 0.3763
Meta loss on this task batch = 3.5040e-01, Meta loss averaged over last 500 steps = 3.4170e-01, PNorm = 102.0509, GNorm = 0.3337
Meta loss on this task batch = 3.3329e-01, Meta loss averaged over last 500 steps = 3.4177e-01, PNorm = 102.0600, GNorm = 0.3378
Meta loss on this task batch = 2.8937e-01, Meta loss averaged over last 500 steps = 3.4163e-01, PNorm = 102.0693, GNorm = 0.3253
Meta loss on this task batch = 3.8398e-01, Meta loss averaged over last 500 steps = 3.4164e-01, PNorm = 102.0787, GNorm = 0.2417
Meta loss on this task batch = 3.4137e-01, Meta loss averaged over last 500 steps = 3.4166e-01, PNorm = 102.0883, GNorm = 0.2812
Meta loss on this task batch = 3.2308e-01, Meta loss averaged over last 500 steps = 3.4157e-01, PNorm = 102.0987, GNorm = 0.3723
Took 110.12956047058105 seconds to complete one epoch of meta training
Took 117.68760657310486 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453427
Epoch 240
Meta loss on this task batch = 3.4215e-01, Meta loss averaged over last 500 steps = 3.4150e-01, PNorm = 102.1084, GNorm = 0.3850
Meta loss on this task batch = 3.5384e-01, Meta loss averaged over last 500 steps = 3.4136e-01, PNorm = 102.1179, GNorm = 0.2543
Meta loss on this task batch = 3.7269e-01, Meta loss averaged over last 500 steps = 3.4154e-01, PNorm = 102.1277, GNorm = 0.3357
Meta loss on this task batch = 3.3128e-01, Meta loss averaged over last 500 steps = 3.4140e-01, PNorm = 102.1377, GNorm = 0.2834
Meta loss on this task batch = 3.6391e-01, Meta loss averaged over last 500 steps = 3.4151e-01, PNorm = 102.1475, GNorm = 0.3396
Meta loss on this task batch = 3.7801e-01, Meta loss averaged over last 500 steps = 3.4155e-01, PNorm = 102.1580, GNorm = 0.3825
Meta loss on this task batch = 3.3463e-01, Meta loss averaged over last 500 steps = 3.4159e-01, PNorm = 102.1683, GNorm = 0.2890
Meta loss on this task batch = 3.5530e-01, Meta loss averaged over last 500 steps = 3.4161e-01, PNorm = 102.1791, GNorm = 0.2916
Meta loss on this task batch = 3.9443e-01, Meta loss averaged over last 500 steps = 3.4165e-01, PNorm = 102.1887, GNorm = 0.2440
Meta loss on this task batch = 3.7163e-01, Meta loss averaged over last 500 steps = 3.4177e-01, PNorm = 102.1987, GNorm = 0.3003
Meta loss on this task batch = 3.1802e-01, Meta loss averaged over last 500 steps = 3.4177e-01, PNorm = 102.2093, GNorm = 0.3352
Meta loss on this task batch = 2.9618e-01, Meta loss averaged over last 500 steps = 3.4167e-01, PNorm = 102.2204, GNorm = 0.2959
Meta loss on this task batch = 3.4269e-01, Meta loss averaged over last 500 steps = 3.4169e-01, PNorm = 102.2321, GNorm = 0.2157
Meta loss on this task batch = 4.1417e-01, Meta loss averaged over last 500 steps = 3.4184e-01, PNorm = 102.2432, GNorm = 0.2608
Meta loss on this task batch = 3.2590e-01, Meta loss averaged over last 500 steps = 3.4181e-01, PNorm = 102.2535, GNorm = 0.2972
Meta loss on this task batch = 3.2053e-01, Meta loss averaged over last 500 steps = 3.4174e-01, PNorm = 102.2635, GNorm = 0.2857
Meta loss on this task batch = 2.8644e-01, Meta loss averaged over last 500 steps = 3.4152e-01, PNorm = 102.2733, GNorm = 0.1906
Meta loss on this task batch = 3.5934e-01, Meta loss averaged over last 500 steps = 3.4161e-01, PNorm = 102.2822, GNorm = 0.2488
Meta loss on this task batch = 3.7614e-01, Meta loss averaged over last 500 steps = 3.4171e-01, PNorm = 102.2905, GNorm = 0.4219
Took 113.81836748123169 seconds to complete one epoch of meta training
Took 122.09575867652893 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472999
Epoch 241
Meta loss on this task batch = 3.1819e-01, Meta loss averaged over last 500 steps = 3.4155e-01, PNorm = 102.2993, GNorm = 0.2151
Meta loss on this task batch = 3.3362e-01, Meta loss averaged over last 500 steps = 3.4151e-01, PNorm = 102.3079, GNorm = 0.3101
Meta loss on this task batch = 2.7697e-01, Meta loss averaged over last 500 steps = 3.4129e-01, PNorm = 102.3173, GNorm = 0.2088
Meta loss on this task batch = 3.8901e-01, Meta loss averaged over last 500 steps = 3.4142e-01, PNorm = 102.3259, GNorm = 0.3912
Meta loss on this task batch = 3.3712e-01, Meta loss averaged over last 500 steps = 3.4137e-01, PNorm = 102.3346, GNorm = 0.2182
Meta loss on this task batch = 3.7075e-01, Meta loss averaged over last 500 steps = 3.4152e-01, PNorm = 102.3436, GNorm = 0.4311
Meta loss on this task batch = 3.6209e-01, Meta loss averaged over last 500 steps = 3.4157e-01, PNorm = 102.3521, GNorm = 0.3213
Meta loss on this task batch = 3.2635e-01, Meta loss averaged over last 500 steps = 3.4142e-01, PNorm = 102.3609, GNorm = 0.2844
Meta loss on this task batch = 3.4696e-01, Meta loss averaged over last 500 steps = 3.4146e-01, PNorm = 102.3692, GNorm = 0.2478
Meta loss on this task batch = 3.4920e-01, Meta loss averaged over last 500 steps = 3.4137e-01, PNorm = 102.3773, GNorm = 0.3026
Meta loss on this task batch = 3.5157e-01, Meta loss averaged over last 500 steps = 3.4137e-01, PNorm = 102.3857, GNorm = 0.3434
Meta loss on this task batch = 3.1697e-01, Meta loss averaged over last 500 steps = 3.4131e-01, PNorm = 102.3943, GNorm = 0.2100
Meta loss on this task batch = 3.9206e-01, Meta loss averaged over last 500 steps = 3.4137e-01, PNorm = 102.4021, GNorm = 0.4047
Meta loss on this task batch = 3.3225e-01, Meta loss averaged over last 500 steps = 3.4128e-01, PNorm = 102.4108, GNorm = 0.5643
Meta loss on this task batch = 3.4586e-01, Meta loss averaged over last 500 steps = 3.4137e-01, PNorm = 102.4194, GNorm = 0.3237
Meta loss on this task batch = 2.9534e-01, Meta loss averaged over last 500 steps = 3.4131e-01, PNorm = 102.4283, GNorm = 0.2273
Meta loss on this task batch = 3.1128e-01, Meta loss averaged over last 500 steps = 3.4126e-01, PNorm = 102.4376, GNorm = 0.4624
Meta loss on this task batch = 3.2934e-01, Meta loss averaged over last 500 steps = 3.4120e-01, PNorm = 102.4468, GNorm = 0.2262
Meta loss on this task batch = 2.7880e-01, Meta loss averaged over last 500 steps = 3.4106e-01, PNorm = 102.4570, GNorm = 0.3642
Took 112.3754096031189 seconds to complete one epoch of meta training
Took 120.55291175842285 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468182
Epoch 242
Meta loss on this task batch = 3.6334e-01, Meta loss averaged over last 500 steps = 3.4117e-01, PNorm = 102.4661, GNorm = 0.3084
Meta loss on this task batch = 3.1780e-01, Meta loss averaged over last 500 steps = 3.4116e-01, PNorm = 102.4761, GNorm = 0.2189
Meta loss on this task batch = 2.9383e-01, Meta loss averaged over last 500 steps = 3.4107e-01, PNorm = 102.4864, GNorm = 0.2505
Meta loss on this task batch = 3.9706e-01, Meta loss averaged over last 500 steps = 3.4113e-01, PNorm = 102.4961, GNorm = 0.2690
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 3.4104e-01, PNorm = 102.5062, GNorm = 0.2579
Meta loss on this task batch = 3.4123e-01, Meta loss averaged over last 500 steps = 3.4106e-01, PNorm = 102.5167, GNorm = 0.2497
Meta loss on this task batch = 3.9493e-01, Meta loss averaged over last 500 steps = 3.4116e-01, PNorm = 102.5262, GNorm = 0.3780
Meta loss on this task batch = 2.9417e-01, Meta loss averaged over last 500 steps = 3.4103e-01, PNorm = 102.5352, GNorm = 0.2436
Meta loss on this task batch = 3.1227e-01, Meta loss averaged over last 500 steps = 3.4087e-01, PNorm = 102.5444, GNorm = 0.2359
Meta loss on this task batch = 3.6615e-01, Meta loss averaged over last 500 steps = 3.4089e-01, PNorm = 102.5539, GNorm = 0.3299
Meta loss on this task batch = 3.6425e-01, Meta loss averaged over last 500 steps = 3.4086e-01, PNorm = 102.5634, GNorm = 0.2568
Meta loss on this task batch = 3.8226e-01, Meta loss averaged over last 500 steps = 3.4100e-01, PNorm = 102.5724, GNorm = 0.2915
Meta loss on this task batch = 3.2278e-01, Meta loss averaged over last 500 steps = 3.4094e-01, PNorm = 102.5812, GNorm = 0.2414
Meta loss on this task batch = 3.7600e-01, Meta loss averaged over last 500 steps = 3.4104e-01, PNorm = 102.5886, GNorm = 0.3263
Meta loss on this task batch = 3.1982e-01, Meta loss averaged over last 500 steps = 3.4101e-01, PNorm = 102.5966, GNorm = 0.2339
Meta loss on this task batch = 3.2541e-01, Meta loss averaged over last 500 steps = 3.4101e-01, PNorm = 102.6034, GNorm = 0.2958
Meta loss on this task batch = 3.6185e-01, Meta loss averaged over last 500 steps = 3.4104e-01, PNorm = 102.6097, GNorm = 0.2589
Meta loss on this task batch = 3.5104e-01, Meta loss averaged over last 500 steps = 3.4106e-01, PNorm = 102.6157, GNorm = 0.2696
Meta loss on this task batch = 3.4022e-01, Meta loss averaged over last 500 steps = 3.4112e-01, PNorm = 102.6243, GNorm = 0.5790
Took 113.64649844169617 seconds to complete one epoch of meta training
Took 121.60991334915161 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458257
Epoch 243
Meta loss on this task batch = 3.5684e-01, Meta loss averaged over last 500 steps = 3.4106e-01, PNorm = 102.6323, GNorm = 0.4427
Meta loss on this task batch = 3.2969e-01, Meta loss averaged over last 500 steps = 3.4103e-01, PNorm = 102.6411, GNorm = 0.4523
Meta loss on this task batch = 3.3497e-01, Meta loss averaged over last 500 steps = 3.4100e-01, PNorm = 102.6508, GNorm = 0.2432
Meta loss on this task batch = 3.3600e-01, Meta loss averaged over last 500 steps = 3.4091e-01, PNorm = 102.6598, GNorm = 0.3019
Meta loss on this task batch = 3.6196e-01, Meta loss averaged over last 500 steps = 3.4096e-01, PNorm = 102.6686, GNorm = 0.3170
Meta loss on this task batch = 3.3395e-01, Meta loss averaged over last 500 steps = 3.4098e-01, PNorm = 102.6769, GNorm = 0.4064
Meta loss on this task batch = 3.1027e-01, Meta loss averaged over last 500 steps = 3.4083e-01, PNorm = 102.6853, GNorm = 0.3339
Meta loss on this task batch = 3.8259e-01, Meta loss averaged over last 500 steps = 3.4098e-01, PNorm = 102.6931, GNorm = 0.2681
Meta loss on this task batch = 3.2898e-01, Meta loss averaged over last 500 steps = 3.4099e-01, PNorm = 102.7003, GNorm = 0.2793
Meta loss on this task batch = 3.0359e-01, Meta loss averaged over last 500 steps = 3.4095e-01, PNorm = 102.7074, GNorm = 0.2761
Meta loss on this task batch = 3.3900e-01, Meta loss averaged over last 500 steps = 3.4087e-01, PNorm = 102.7151, GNorm = 0.3370
Meta loss on this task batch = 3.5005e-01, Meta loss averaged over last 500 steps = 3.4081e-01, PNorm = 102.7231, GNorm = 0.2417
Meta loss on this task batch = 3.3503e-01, Meta loss averaged over last 500 steps = 3.4085e-01, PNorm = 102.7300, GNorm = 0.2875
Meta loss on this task batch = 3.7375e-01, Meta loss averaged over last 500 steps = 3.4095e-01, PNorm = 102.7378, GNorm = 0.3403
Meta loss on this task batch = 3.4621e-01, Meta loss averaged over last 500 steps = 3.4102e-01, PNorm = 102.7455, GNorm = 0.2418
Meta loss on this task batch = 3.5820e-01, Meta loss averaged over last 500 steps = 3.4102e-01, PNorm = 102.7537, GNorm = 0.2415
Meta loss on this task batch = 3.3666e-01, Meta loss averaged over last 500 steps = 3.4094e-01, PNorm = 102.7624, GNorm = 0.4404
Meta loss on this task batch = 3.2794e-01, Meta loss averaged over last 500 steps = 3.4075e-01, PNorm = 102.7718, GNorm = 0.2364
Meta loss on this task batch = 3.5075e-01, Meta loss averaged over last 500 steps = 3.4074e-01, PNorm = 102.7806, GNorm = 0.3288
Took 109.62733745574951 seconds to complete one epoch of meta training
Took 117.4422492980957 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468788
Epoch 244
Meta loss on this task batch = 3.6716e-01, Meta loss averaged over last 500 steps = 3.4094e-01, PNorm = 102.7894, GNorm = 0.2921
Meta loss on this task batch = 3.6241e-01, Meta loss averaged over last 500 steps = 3.4110e-01, PNorm = 102.7980, GNorm = 0.2785
Meta loss on this task batch = 3.7268e-01, Meta loss averaged over last 500 steps = 3.4107e-01, PNorm = 102.8057, GNorm = 0.2411
Meta loss on this task batch = 3.4668e-01, Meta loss averaged over last 500 steps = 3.4107e-01, PNorm = 102.8125, GNorm = 0.2660
Meta loss on this task batch = 3.2914e-01, Meta loss averaged over last 500 steps = 3.4099e-01, PNorm = 102.8195, GNorm = 0.2347
Meta loss on this task batch = 3.1214e-01, Meta loss averaged over last 500 steps = 3.4090e-01, PNorm = 102.8272, GNorm = 0.2595
Meta loss on this task batch = 2.9655e-01, Meta loss averaged over last 500 steps = 3.4082e-01, PNorm = 102.8353, GNorm = 0.3406
Meta loss on this task batch = 3.0156e-01, Meta loss averaged over last 500 steps = 3.4065e-01, PNorm = 102.8439, GNorm = 0.3558
Meta loss on this task batch = 3.0049e-01, Meta loss averaged over last 500 steps = 3.4063e-01, PNorm = 102.8523, GNorm = 0.2456
Meta loss on this task batch = 3.8065e-01, Meta loss averaged over last 500 steps = 3.4081e-01, PNorm = 102.8604, GNorm = 0.2866
Meta loss on this task batch = 3.2232e-01, Meta loss averaged over last 500 steps = 3.4079e-01, PNorm = 102.8680, GNorm = 0.2497
Meta loss on this task batch = 3.1949e-01, Meta loss averaged over last 500 steps = 3.4066e-01, PNorm = 102.8759, GNorm = 0.2580
Meta loss on this task batch = 3.5042e-01, Meta loss averaged over last 500 steps = 3.4074e-01, PNorm = 102.8835, GNorm = 0.2586
Meta loss on this task batch = 3.3536e-01, Meta loss averaged over last 500 steps = 3.4070e-01, PNorm = 102.8909, GNorm = 0.2869
Meta loss on this task batch = 3.1003e-01, Meta loss averaged over last 500 steps = 3.4064e-01, PNorm = 102.8982, GNorm = 0.2684
Meta loss on this task batch = 3.9379e-01, Meta loss averaged over last 500 steps = 3.4070e-01, PNorm = 102.9050, GNorm = 0.3985
Meta loss on this task batch = 3.5455e-01, Meta loss averaged over last 500 steps = 3.4057e-01, PNorm = 102.9122, GNorm = 0.4121
Meta loss on this task batch = 3.1535e-01, Meta loss averaged over last 500 steps = 3.4061e-01, PNorm = 102.9205, GNorm = 0.2458
Meta loss on this task batch = 2.9730e-01, Meta loss averaged over last 500 steps = 3.4055e-01, PNorm = 102.9291, GNorm = 0.3158
Took 111.7989501953125 seconds to complete one epoch of meta training
Took 120.04699611663818 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488836
Epoch 245
Meta loss on this task batch = 3.4161e-01, Meta loss averaged over last 500 steps = 3.4051e-01, PNorm = 102.9374, GNorm = 0.4504
Meta loss on this task batch = 3.3858e-01, Meta loss averaged over last 500 steps = 3.4051e-01, PNorm = 102.9464, GNorm = 0.2733
Meta loss on this task batch = 2.9682e-01, Meta loss averaged over last 500 steps = 3.4043e-01, PNorm = 102.9561, GNorm = 0.2499
Meta loss on this task batch = 3.6375e-01, Meta loss averaged over last 500 steps = 3.4053e-01, PNorm = 102.9648, GNorm = 0.3294
Meta loss on this task batch = 3.1657e-01, Meta loss averaged over last 500 steps = 3.4049e-01, PNorm = 102.9741, GNorm = 0.2546
Meta loss on this task batch = 3.0044e-01, Meta loss averaged over last 500 steps = 3.4047e-01, PNorm = 102.9838, GNorm = 0.2968
Meta loss on this task batch = 3.7667e-01, Meta loss averaged over last 500 steps = 3.4056e-01, PNorm = 102.9917, GNorm = 0.3123
Meta loss on this task batch = 4.0234e-01, Meta loss averaged over last 500 steps = 3.4078e-01, PNorm = 102.9986, GNorm = 0.2765
Meta loss on this task batch = 3.2509e-01, Meta loss averaged over last 500 steps = 3.4073e-01, PNorm = 103.0053, GNorm = 0.3232
Meta loss on this task batch = 3.4630e-01, Meta loss averaged over last 500 steps = 3.4078e-01, PNorm = 103.0117, GNorm = 0.2819
Meta loss on this task batch = 2.8767e-01, Meta loss averaged over last 500 steps = 3.4072e-01, PNorm = 103.0186, GNorm = 0.2244
Meta loss on this task batch = 3.2202e-01, Meta loss averaged over last 500 steps = 3.4070e-01, PNorm = 103.0270, GNorm = 0.2273
Meta loss on this task batch = 3.1674e-01, Meta loss averaged over last 500 steps = 3.4072e-01, PNorm = 103.0360, GNorm = 0.2922
Meta loss on this task batch = 3.6994e-01, Meta loss averaged over last 500 steps = 3.4073e-01, PNorm = 103.0448, GNorm = 0.4247
Meta loss on this task batch = 3.7331e-01, Meta loss averaged over last 500 steps = 3.4091e-01, PNorm = 103.0539, GNorm = 0.2713
Meta loss on this task batch = 3.0296e-01, Meta loss averaged over last 500 steps = 3.4072e-01, PNorm = 103.0641, GNorm = 0.2531
Meta loss on this task batch = 3.7064e-01, Meta loss averaged over last 500 steps = 3.4069e-01, PNorm = 103.0742, GNorm = 0.3854
Meta loss on this task batch = 3.5316e-01, Meta loss averaged over last 500 steps = 3.4079e-01, PNorm = 103.0845, GNorm = 0.3423
Meta loss on this task batch = 2.7458e-01, Meta loss averaged over last 500 steps = 3.4065e-01, PNorm = 103.0946, GNorm = 0.2706
Took 112.97988271713257 seconds to complete one epoch of meta training
Took 120.90481996536255 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492604
Epoch 246
Meta loss on this task batch = 2.8653e-01, Meta loss averaged over last 500 steps = 3.4047e-01, PNorm = 103.1048, GNorm = 0.2508
Meta loss on this task batch = 3.0784e-01, Meta loss averaged over last 500 steps = 3.4039e-01, PNorm = 103.1148, GNorm = 0.2841
Meta loss on this task batch = 3.2873e-01, Meta loss averaged over last 500 steps = 3.4038e-01, PNorm = 103.1249, GNorm = 0.2574
Meta loss on this task batch = 3.0779e-01, Meta loss averaged over last 500 steps = 3.4037e-01, PNorm = 103.1352, GNorm = 0.3197
Meta loss on this task batch = 3.7627e-01, Meta loss averaged over last 500 steps = 3.4038e-01, PNorm = 103.1448, GNorm = 0.4009
Meta loss on this task batch = 3.7194e-01, Meta loss averaged over last 500 steps = 3.4044e-01, PNorm = 103.1528, GNorm = 0.3048
Meta loss on this task batch = 3.5161e-01, Meta loss averaged over last 500 steps = 3.4047e-01, PNorm = 103.1605, GNorm = 0.4613
Meta loss on this task batch = 3.0974e-01, Meta loss averaged over last 500 steps = 3.4041e-01, PNorm = 103.1682, GNorm = 0.2399
Meta loss on this task batch = 3.3954e-01, Meta loss averaged over last 500 steps = 3.4041e-01, PNorm = 103.1762, GNorm = 0.4223
Meta loss on this task batch = 3.3495e-01, Meta loss averaged over last 500 steps = 3.4037e-01, PNorm = 103.1845, GNorm = 0.4886
Meta loss on this task batch = 2.9426e-01, Meta loss averaged over last 500 steps = 3.4018e-01, PNorm = 103.1932, GNorm = 0.2970
Meta loss on this task batch = 3.2435e-01, Meta loss averaged over last 500 steps = 3.4017e-01, PNorm = 103.2017, GNorm = 0.2785
Meta loss on this task batch = 3.3964e-01, Meta loss averaged over last 500 steps = 3.4019e-01, PNorm = 103.2091, GNorm = 0.3366
Meta loss on this task batch = 3.2893e-01, Meta loss averaged over last 500 steps = 3.4015e-01, PNorm = 103.2153, GNorm = 0.2452
Meta loss on this task batch = 3.3379e-01, Meta loss averaged over last 500 steps = 3.4012e-01, PNorm = 103.2220, GNorm = 0.3469
Meta loss on this task batch = 3.9471e-01, Meta loss averaged over last 500 steps = 3.4016e-01, PNorm = 103.2275, GNorm = 0.5312
Meta loss on this task batch = 3.2813e-01, Meta loss averaged over last 500 steps = 3.4024e-01, PNorm = 103.2339, GNorm = 0.3169
Meta loss on this task batch = 3.3036e-01, Meta loss averaged over last 500 steps = 3.4025e-01, PNorm = 103.2401, GNorm = 0.3132
Meta loss on this task batch = 3.9092e-01, Meta loss averaged over last 500 steps = 3.4039e-01, PNorm = 103.2468, GNorm = 0.3859
Took 112.61520147323608 seconds to complete one epoch of meta training
Took 120.60795450210571 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.521901
Found better MAML checkpoint after meta validation, saving now
Epoch 247
Meta loss on this task batch = 3.3486e-01, Meta loss averaged over last 500 steps = 3.4037e-01, PNorm = 103.2535, GNorm = 0.3350
Meta loss on this task batch = 3.3000e-01, Meta loss averaged over last 500 steps = 3.4029e-01, PNorm = 103.2605, GNorm = 0.2248
Meta loss on this task batch = 3.4874e-01, Meta loss averaged over last 500 steps = 3.4027e-01, PNorm = 103.2673, GNorm = 0.3000
Meta loss on this task batch = 3.6339e-01, Meta loss averaged over last 500 steps = 3.4028e-01, PNorm = 103.2743, GNorm = 0.2583
Meta loss on this task batch = 2.6980e-01, Meta loss averaged over last 500 steps = 3.4025e-01, PNorm = 103.2826, GNorm = 0.2555
Meta loss on this task batch = 3.4320e-01, Meta loss averaged over last 500 steps = 3.4015e-01, PNorm = 103.2916, GNorm = 0.3718
Meta loss on this task batch = 3.3612e-01, Meta loss averaged over last 500 steps = 3.4020e-01, PNorm = 103.3005, GNorm = 0.2396
Meta loss on this task batch = 3.5668e-01, Meta loss averaged over last 500 steps = 3.4022e-01, PNorm = 103.3089, GNorm = 0.2884
Meta loss on this task batch = 3.5628e-01, Meta loss averaged over last 500 steps = 3.4026e-01, PNorm = 103.3164, GNorm = 0.5392
Meta loss on this task batch = 3.0111e-01, Meta loss averaged over last 500 steps = 3.4016e-01, PNorm = 103.3242, GNorm = 0.5180
Meta loss on this task batch = 3.3429e-01, Meta loss averaged over last 500 steps = 3.4026e-01, PNorm = 103.3327, GNorm = 0.2451
Meta loss on this task batch = 3.1786e-01, Meta loss averaged over last 500 steps = 3.4018e-01, PNorm = 103.3414, GNorm = 0.2244
Meta loss on this task batch = 3.5229e-01, Meta loss averaged over last 500 steps = 3.4024e-01, PNorm = 103.3505, GNorm = 0.3255
Meta loss on this task batch = 3.1535e-01, Meta loss averaged over last 500 steps = 3.4019e-01, PNorm = 103.3599, GNorm = 0.2126
Meta loss on this task batch = 3.5803e-01, Meta loss averaged over last 500 steps = 3.4017e-01, PNorm = 103.3692, GNorm = 0.3270
Meta loss on this task batch = 3.7171e-01, Meta loss averaged over last 500 steps = 3.4024e-01, PNorm = 103.3782, GNorm = 0.3327
Meta loss on this task batch = 2.9533e-01, Meta loss averaged over last 500 steps = 3.4005e-01, PNorm = 103.3870, GNorm = 0.3603
Meta loss on this task batch = 3.7951e-01, Meta loss averaged over last 500 steps = 3.4012e-01, PNorm = 103.3941, GNorm = 0.3128
Meta loss on this task batch = 3.3379e-01, Meta loss averaged over last 500 steps = 3.4007e-01, PNorm = 103.4025, GNorm = 0.3644
Took 114.99925398826599 seconds to complete one epoch of meta training
Took 122.72410440444946 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481602
Epoch 248
Meta loss on this task batch = 3.8310e-01, Meta loss averaged over last 500 steps = 3.4021e-01, PNorm = 103.4099, GNorm = 0.5305
Meta loss on this task batch = 3.9079e-01, Meta loss averaged over last 500 steps = 3.4019e-01, PNorm = 103.4163, GNorm = 0.3460
Meta loss on this task batch = 3.2702e-01, Meta loss averaged over last 500 steps = 3.4012e-01, PNorm = 103.4224, GNorm = 0.2800
Meta loss on this task batch = 3.5119e-01, Meta loss averaged over last 500 steps = 3.4016e-01, PNorm = 103.4285, GNorm = 0.2641
Meta loss on this task batch = 3.2463e-01, Meta loss averaged over last 500 steps = 3.4012e-01, PNorm = 103.4344, GNorm = 0.2414
Meta loss on this task batch = 3.5861e-01, Meta loss averaged over last 500 steps = 3.4029e-01, PNorm = 103.4403, GNorm = 0.2781
Meta loss on this task batch = 3.2281e-01, Meta loss averaged over last 500 steps = 3.4023e-01, PNorm = 103.4464, GNorm = 0.2544
Meta loss on this task batch = 3.1840e-01, Meta loss averaged over last 500 steps = 3.4031e-01, PNorm = 103.4541, GNorm = 0.2270
Meta loss on this task batch = 3.4777e-01, Meta loss averaged over last 500 steps = 3.4030e-01, PNorm = 103.4621, GNorm = 0.2430
Meta loss on this task batch = 3.2690e-01, Meta loss averaged over last 500 steps = 3.4027e-01, PNorm = 103.4704, GNorm = 0.2311
Meta loss on this task batch = 3.3389e-01, Meta loss averaged over last 500 steps = 3.4013e-01, PNorm = 103.4789, GNorm = 0.2574
Meta loss on this task batch = 3.2428e-01, Meta loss averaged over last 500 steps = 3.4006e-01, PNorm = 103.4875, GNorm = 0.2916
Meta loss on this task batch = 3.5787e-01, Meta loss averaged over last 500 steps = 3.3999e-01, PNorm = 103.4966, GNorm = 0.2459
Meta loss on this task batch = 2.8682e-01, Meta loss averaged over last 500 steps = 3.3990e-01, PNorm = 103.5060, GNorm = 0.2584
Meta loss on this task batch = 3.1683e-01, Meta loss averaged over last 500 steps = 3.3984e-01, PNorm = 103.5141, GNorm = 0.3435
Meta loss on this task batch = 3.6199e-01, Meta loss averaged over last 500 steps = 3.3981e-01, PNorm = 103.5217, GNorm = 0.2904
Meta loss on this task batch = 3.0327e-01, Meta loss averaged over last 500 steps = 3.3983e-01, PNorm = 103.5300, GNorm = 0.2295
Meta loss on this task batch = 3.3121e-01, Meta loss averaged over last 500 steps = 3.3990e-01, PNorm = 103.5382, GNorm = 0.2380
Meta loss on this task batch = 3.6529e-01, Meta loss averaged over last 500 steps = 3.4004e-01, PNorm = 103.5459, GNorm = 0.3640
Took 111.63690996170044 seconds to complete one epoch of meta training
Took 119.68019676208496 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464321
Epoch 249
Meta loss on this task batch = 3.2182e-01, Meta loss averaged over last 500 steps = 3.4001e-01, PNorm = 103.5537, GNorm = 0.3459
Meta loss on this task batch = 4.0697e-01, Meta loss averaged over last 500 steps = 3.4016e-01, PNorm = 103.5608, GNorm = 0.6233
Meta loss on this task batch = 3.0798e-01, Meta loss averaged over last 500 steps = 3.4012e-01, PNorm = 103.5692, GNorm = 0.3001
Meta loss on this task batch = 3.1879e-01, Meta loss averaged over last 500 steps = 3.4008e-01, PNorm = 103.5779, GNorm = 0.3079
Meta loss on this task batch = 3.3917e-01, Meta loss averaged over last 500 steps = 3.4007e-01, PNorm = 103.5868, GNorm = 0.4073
Meta loss on this task batch = 3.7745e-01, Meta loss averaged over last 500 steps = 3.4015e-01, PNorm = 103.5955, GNorm = 0.2731
Meta loss on this task batch = 3.2075e-01, Meta loss averaged over last 500 steps = 3.4002e-01, PNorm = 103.6043, GNorm = 0.2959
Meta loss on this task batch = 3.3378e-01, Meta loss averaged over last 500 steps = 3.3989e-01, PNorm = 103.6133, GNorm = 0.2599
Meta loss on this task batch = 3.6005e-01, Meta loss averaged over last 500 steps = 3.4003e-01, PNorm = 103.6219, GNorm = 0.2651
Meta loss on this task batch = 2.6879e-01, Meta loss averaged over last 500 steps = 3.3983e-01, PNorm = 103.6308, GNorm = 0.3152
Meta loss on this task batch = 3.4509e-01, Meta loss averaged over last 500 steps = 3.3992e-01, PNorm = 103.6400, GNorm = 0.2558
Meta loss on this task batch = 3.6510e-01, Meta loss averaged over last 500 steps = 3.3995e-01, PNorm = 103.6500, GNorm = 0.3157
Meta loss on this task batch = 3.2761e-01, Meta loss averaged over last 500 steps = 3.3994e-01, PNorm = 103.6606, GNorm = 0.3089
Meta loss on this task batch = 3.5838e-01, Meta loss averaged over last 500 steps = 3.3999e-01, PNorm = 103.6709, GNorm = 0.2567
Meta loss on this task batch = 3.3333e-01, Meta loss averaged over last 500 steps = 3.4008e-01, PNorm = 103.6809, GNorm = 0.2705
Meta loss on this task batch = 3.6849e-01, Meta loss averaged over last 500 steps = 3.4020e-01, PNorm = 103.6915, GNorm = 0.5005
Meta loss on this task batch = 3.2915e-01, Meta loss averaged over last 500 steps = 3.4006e-01, PNorm = 103.7034, GNorm = 0.3170
Meta loss on this task batch = 3.3894e-01, Meta loss averaged over last 500 steps = 3.3999e-01, PNorm = 103.7157, GNorm = 0.3597
Meta loss on this task batch = 3.9248e-01, Meta loss averaged over last 500 steps = 3.4012e-01, PNorm = 103.7276, GNorm = 0.3978
Took 112.26604795455933 seconds to complete one epoch of meta training
Took 119.93917775154114 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470679
Epoch 250
Meta loss on this task batch = 3.5408e-01, Meta loss averaged over last 500 steps = 3.4020e-01, PNorm = 103.7394, GNorm = 0.3624
Meta loss on this task batch = 3.4313e-01, Meta loss averaged over last 500 steps = 3.4023e-01, PNorm = 103.7510, GNorm = 0.2749
Meta loss on this task batch = 3.4794e-01, Meta loss averaged over last 500 steps = 3.4022e-01, PNorm = 103.7616, GNorm = 0.3441
Meta loss on this task batch = 4.1414e-01, Meta loss averaged over last 500 steps = 3.4028e-01, PNorm = 103.7703, GNorm = 0.4634
Meta loss on this task batch = 3.1083e-01, Meta loss averaged over last 500 steps = 3.4027e-01, PNorm = 103.7784, GNorm = 0.3194
Meta loss on this task batch = 3.2494e-01, Meta loss averaged over last 500 steps = 3.4012e-01, PNorm = 103.7880, GNorm = 0.4734
Meta loss on this task batch = 3.3040e-01, Meta loss averaged over last 500 steps = 3.4006e-01, PNorm = 103.7983, GNorm = 0.4274
Meta loss on this task batch = 3.5166e-01, Meta loss averaged over last 500 steps = 3.4016e-01, PNorm = 103.8092, GNorm = 0.4718
Meta loss on this task batch = 3.3007e-01, Meta loss averaged over last 500 steps = 3.4017e-01, PNorm = 103.8201, GNorm = 0.3005
Meta loss on this task batch = 2.8241e-01, Meta loss averaged over last 500 steps = 3.3997e-01, PNorm = 103.8295, GNorm = 0.2332
Meta loss on this task batch = 3.3239e-01, Meta loss averaged over last 500 steps = 3.3992e-01, PNorm = 103.8394, GNorm = 0.3663
Meta loss on this task batch = 3.2977e-01, Meta loss averaged over last 500 steps = 3.3991e-01, PNorm = 103.8495, GNorm = 0.3586
Meta loss on this task batch = 3.2108e-01, Meta loss averaged over last 500 steps = 3.3993e-01, PNorm = 103.8595, GNorm = 0.2824
Meta loss on this task batch = 3.7240e-01, Meta loss averaged over last 500 steps = 3.4007e-01, PNorm = 103.8685, GNorm = 0.3779
Meta loss on this task batch = 3.3887e-01, Meta loss averaged over last 500 steps = 3.3991e-01, PNorm = 103.8781, GNorm = 0.2673
Meta loss on this task batch = 3.8810e-01, Meta loss averaged over last 500 steps = 3.3995e-01, PNorm = 103.8851, GNorm = 0.3346
Meta loss on this task batch = 3.6036e-01, Meta loss averaged over last 500 steps = 3.4013e-01, PNorm = 103.8923, GNorm = 0.3061
Meta loss on this task batch = 3.7059e-01, Meta loss averaged over last 500 steps = 3.4027e-01, PNorm = 103.9013, GNorm = 0.3464
Meta loss on this task batch = 3.1896e-01, Meta loss averaged over last 500 steps = 3.4015e-01, PNorm = 103.9118, GNorm = 0.3481
Took 111.99920606613159 seconds to complete one epoch of meta training
Took 120.01343297958374 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452556
Epoch 251
Meta loss on this task batch = 3.0995e-01, Meta loss averaged over last 500 steps = 3.4004e-01, PNorm = 103.9232, GNorm = 0.2877
Meta loss on this task batch = 3.0670e-01, Meta loss averaged over last 500 steps = 3.4003e-01, PNorm = 103.9346, GNorm = 0.2401
Meta loss on this task batch = 3.3404e-01, Meta loss averaged over last 500 steps = 3.3997e-01, PNorm = 103.9457, GNorm = 0.2739
Meta loss on this task batch = 3.3998e-01, Meta loss averaged over last 500 steps = 3.3992e-01, PNorm = 103.9567, GNorm = 0.2519
Meta loss on this task batch = 3.2985e-01, Meta loss averaged over last 500 steps = 3.3994e-01, PNorm = 103.9673, GNorm = 0.2692
Meta loss on this task batch = 3.2702e-01, Meta loss averaged over last 500 steps = 3.3990e-01, PNorm = 103.9781, GNorm = 0.2731
Meta loss on this task batch = 3.3463e-01, Meta loss averaged over last 500 steps = 3.3987e-01, PNorm = 103.9888, GNorm = 0.2326
Meta loss on this task batch = 3.4419e-01, Meta loss averaged over last 500 steps = 3.3994e-01, PNorm = 103.9985, GNorm = 0.2669
Meta loss on this task batch = 3.4690e-01, Meta loss averaged over last 500 steps = 3.4005e-01, PNorm = 104.0083, GNorm = 0.2629
Meta loss on this task batch = 3.8243e-01, Meta loss averaged over last 500 steps = 3.4014e-01, PNorm = 104.0177, GNorm = 0.2812
Meta loss on this task batch = 3.5772e-01, Meta loss averaged over last 500 steps = 3.4025e-01, PNorm = 104.0264, GNorm = 0.2298
Meta loss on this task batch = 2.7600e-01, Meta loss averaged over last 500 steps = 3.4007e-01, PNorm = 104.0352, GNorm = 0.2451
Meta loss on this task batch = 3.6981e-01, Meta loss averaged over last 500 steps = 3.4004e-01, PNorm = 104.0433, GNorm = 0.3372
Meta loss on this task batch = 3.2351e-01, Meta loss averaged over last 500 steps = 3.4018e-01, PNorm = 104.0507, GNorm = 0.2864
Meta loss on this task batch = 3.5397e-01, Meta loss averaged over last 500 steps = 3.4015e-01, PNorm = 104.0579, GNorm = 0.2824
Meta loss on this task batch = 3.2070e-01, Meta loss averaged over last 500 steps = 3.4013e-01, PNorm = 104.0657, GNorm = 0.2553
Meta loss on this task batch = 3.4711e-01, Meta loss averaged over last 500 steps = 3.4009e-01, PNorm = 104.0730, GNorm = 0.3413
Meta loss on this task batch = 3.3196e-01, Meta loss averaged over last 500 steps = 3.4001e-01, PNorm = 104.0803, GNorm = 0.2578
Meta loss on this task batch = 3.8137e-01, Meta loss averaged over last 500 steps = 3.4013e-01, PNorm = 104.0868, GNorm = 0.4386
Took 112.78108501434326 seconds to complete one epoch of meta training
Took 120.62581944465637 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470593
Epoch 252
Meta loss on this task batch = 3.5040e-01, Meta loss averaged over last 500 steps = 3.4018e-01, PNorm = 104.0935, GNorm = 0.3086
Meta loss on this task batch = 3.5933e-01, Meta loss averaged over last 500 steps = 3.4022e-01, PNorm = 104.1001, GNorm = 0.2558
Meta loss on this task batch = 2.7733e-01, Meta loss averaged over last 500 steps = 3.4003e-01, PNorm = 104.1077, GNorm = 0.2100
Meta loss on this task batch = 3.3656e-01, Meta loss averaged over last 500 steps = 3.4005e-01, PNorm = 104.1145, GNorm = 0.3353
Meta loss on this task batch = 3.5522e-01, Meta loss averaged over last 500 steps = 3.4014e-01, PNorm = 104.1212, GNorm = 0.2939
Meta loss on this task batch = 3.0948e-01, Meta loss averaged over last 500 steps = 3.4000e-01, PNorm = 104.1282, GNorm = 0.2334
Meta loss on this task batch = 3.5319e-01, Meta loss averaged over last 500 steps = 3.3996e-01, PNorm = 104.1356, GNorm = 0.2646
Meta loss on this task batch = 2.7040e-01, Meta loss averaged over last 500 steps = 3.3980e-01, PNorm = 104.1435, GNorm = 0.2460
Meta loss on this task batch = 3.8752e-01, Meta loss averaged over last 500 steps = 3.3997e-01, PNorm = 104.1509, GNorm = 0.2597
Meta loss on this task batch = 3.2393e-01, Meta loss averaged over last 500 steps = 3.4000e-01, PNorm = 104.1584, GNorm = 0.3598
Meta loss on this task batch = 2.8092e-01, Meta loss averaged over last 500 steps = 3.3983e-01, PNorm = 104.1660, GNorm = 0.2255
Meta loss on this task batch = 4.0234e-01, Meta loss averaged over last 500 steps = 3.4000e-01, PNorm = 104.1733, GNorm = 0.3009
Meta loss on this task batch = 3.3223e-01, Meta loss averaged over last 500 steps = 3.4000e-01, PNorm = 104.1809, GNorm = 0.4312
Meta loss on this task batch = 3.1724e-01, Meta loss averaged over last 500 steps = 3.3984e-01, PNorm = 104.1887, GNorm = 0.2320
Meta loss on this task batch = 3.2705e-01, Meta loss averaged over last 500 steps = 3.3975e-01, PNorm = 104.1966, GNorm = 0.2453
Meta loss on this task batch = 4.0195e-01, Meta loss averaged over last 500 steps = 3.3994e-01, PNorm = 104.2032, GNorm = 0.3650
Meta loss on this task batch = 3.6887e-01, Meta loss averaged over last 500 steps = 3.4009e-01, PNorm = 104.2101, GNorm = 0.3249
Meta loss on this task batch = 3.1387e-01, Meta loss averaged over last 500 steps = 3.4013e-01, PNorm = 104.2169, GNorm = 0.2755
Meta loss on this task batch = 3.3671e-01, Meta loss averaged over last 500 steps = 3.4008e-01, PNorm = 104.2237, GNorm = 0.2698
Took 110.76165199279785 seconds to complete one epoch of meta training
Took 118.48719120025635 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494729
Epoch 253
Meta loss on this task batch = 3.1186e-01, Meta loss averaged over last 500 steps = 3.3998e-01, PNorm = 104.2315, GNorm = 0.3187
Meta loss on this task batch = 3.3772e-01, Meta loss averaged over last 500 steps = 3.3996e-01, PNorm = 104.2395, GNorm = 0.3039
Meta loss on this task batch = 2.8411e-01, Meta loss averaged over last 500 steps = 3.3981e-01, PNorm = 104.2483, GNorm = 0.3059
Meta loss on this task batch = 3.3104e-01, Meta loss averaged over last 500 steps = 3.3991e-01, PNorm = 104.2566, GNorm = 0.2789
Meta loss on this task batch = 3.4402e-01, Meta loss averaged over last 500 steps = 3.4000e-01, PNorm = 104.2642, GNorm = 0.2749
Meta loss on this task batch = 3.7471e-01, Meta loss averaged over last 500 steps = 3.4003e-01, PNorm = 104.2717, GNorm = 0.2466
Meta loss on this task batch = 3.3380e-01, Meta loss averaged over last 500 steps = 3.3994e-01, PNorm = 104.2794, GNorm = 0.2696
Meta loss on this task batch = 3.8096e-01, Meta loss averaged over last 500 steps = 3.4004e-01, PNorm = 104.2864, GNorm = 0.2604
Meta loss on this task batch = 3.7285e-01, Meta loss averaged over last 500 steps = 3.4014e-01, PNorm = 104.2937, GNorm = 0.3288
Meta loss on this task batch = 2.9896e-01, Meta loss averaged over last 500 steps = 3.4025e-01, PNorm = 104.3014, GNorm = 0.2907
Meta loss on this task batch = 3.6199e-01, Meta loss averaged over last 500 steps = 3.4036e-01, PNorm = 104.3083, GNorm = 0.2552
Meta loss on this task batch = 3.5600e-01, Meta loss averaged over last 500 steps = 3.4039e-01, PNorm = 104.3142, GNorm = 0.3050
Meta loss on this task batch = 3.2030e-01, Meta loss averaged over last 500 steps = 3.4030e-01, PNorm = 104.3211, GNorm = 0.2230
Meta loss on this task batch = 3.6831e-01, Meta loss averaged over last 500 steps = 3.4036e-01, PNorm = 104.3273, GNorm = 0.2666
Meta loss on this task batch = 3.4397e-01, Meta loss averaged over last 500 steps = 3.4036e-01, PNorm = 104.3339, GNorm = 0.2614
Meta loss on this task batch = 3.2635e-01, Meta loss averaged over last 500 steps = 3.4028e-01, PNorm = 104.3408, GNorm = 0.2940
Meta loss on this task batch = 3.1875e-01, Meta loss averaged over last 500 steps = 3.4028e-01, PNorm = 104.3485, GNorm = 0.2312
Meta loss on this task batch = 3.0108e-01, Meta loss averaged over last 500 steps = 3.4009e-01, PNorm = 104.3565, GNorm = 0.2280
Meta loss on this task batch = 3.2107e-01, Meta loss averaged over last 500 steps = 3.3989e-01, PNorm = 104.3649, GNorm = 0.4486
Took 109.93567728996277 seconds to complete one epoch of meta training
Took 116.81288385391235 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471749
Epoch 254
Meta loss on this task batch = 2.6446e-01, Meta loss averaged over last 500 steps = 3.3979e-01, PNorm = 104.3739, GNorm = 0.2405
Meta loss on this task batch = 3.4531e-01, Meta loss averaged over last 500 steps = 3.3983e-01, PNorm = 104.3823, GNorm = 0.2614
Meta loss on this task batch = 3.5584e-01, Meta loss averaged over last 500 steps = 3.3983e-01, PNorm = 104.3910, GNorm = 0.2382
Meta loss on this task batch = 2.9946e-01, Meta loss averaged over last 500 steps = 3.3973e-01, PNorm = 104.4005, GNorm = 0.2280
Meta loss on this task batch = 3.3775e-01, Meta loss averaged over last 500 steps = 3.3975e-01, PNorm = 104.4088, GNorm = 0.3535
Meta loss on this task batch = 3.2360e-01, Meta loss averaged over last 500 steps = 3.3974e-01, PNorm = 104.4170, GNorm = 0.3119
Meta loss on this task batch = 3.2226e-01, Meta loss averaged over last 500 steps = 3.3966e-01, PNorm = 104.4248, GNorm = 0.2390
Meta loss on this task batch = 3.3506e-01, Meta loss averaged over last 500 steps = 3.3962e-01, PNorm = 104.4315, GNorm = 0.2897
Meta loss on this task batch = 3.4254e-01, Meta loss averaged over last 500 steps = 3.3967e-01, PNorm = 104.4374, GNorm = 0.2668
Meta loss on this task batch = 3.3227e-01, Meta loss averaged over last 500 steps = 3.3960e-01, PNorm = 104.4444, GNorm = 0.2565
Meta loss on this task batch = 3.5997e-01, Meta loss averaged over last 500 steps = 3.3953e-01, PNorm = 104.4509, GNorm = 0.3133
Meta loss on this task batch = 2.8641e-01, Meta loss averaged over last 500 steps = 3.3946e-01, PNorm = 104.4585, GNorm = 0.2114
Meta loss on this task batch = 3.0900e-01, Meta loss averaged over last 500 steps = 3.3949e-01, PNorm = 104.4662, GNorm = 0.3022
Meta loss on this task batch = 3.3225e-01, Meta loss averaged over last 500 steps = 3.3952e-01, PNorm = 104.4736, GNorm = 0.2514
Meta loss on this task batch = 3.2417e-01, Meta loss averaged over last 500 steps = 3.3946e-01, PNorm = 104.4807, GNorm = 0.2559
Meta loss on this task batch = 3.4475e-01, Meta loss averaged over last 500 steps = 3.3946e-01, PNorm = 104.4876, GNorm = 0.2595
Meta loss on this task batch = 3.4089e-01, Meta loss averaged over last 500 steps = 3.3938e-01, PNorm = 104.4949, GNorm = 0.2953
Meta loss on this task batch = 3.6074e-01, Meta loss averaged over last 500 steps = 3.3943e-01, PNorm = 104.5019, GNorm = 0.2751
Meta loss on this task batch = 3.2808e-01, Meta loss averaged over last 500 steps = 3.3945e-01, PNorm = 104.5093, GNorm = 0.3430
Took 110.91670203208923 seconds to complete one epoch of meta training
Took 119.35302519798279 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468431
Epoch 255
Meta loss on this task batch = 3.2692e-01, Meta loss averaged over last 500 steps = 3.3944e-01, PNorm = 104.5165, GNorm = 0.2528
Meta loss on this task batch = 3.3316e-01, Meta loss averaged over last 500 steps = 3.3939e-01, PNorm = 104.5233, GNorm = 0.2886
Meta loss on this task batch = 3.9768e-01, Meta loss averaged over last 500 steps = 3.3953e-01, PNorm = 104.5298, GNorm = 0.2701
Meta loss on this task batch = 3.5953e-01, Meta loss averaged over last 500 steps = 3.3949e-01, PNorm = 104.5366, GNorm = 0.3816
Meta loss on this task batch = 3.1858e-01, Meta loss averaged over last 500 steps = 3.3948e-01, PNorm = 104.5436, GNorm = 0.2109
Meta loss on this task batch = 3.1409e-01, Meta loss averaged over last 500 steps = 3.3958e-01, PNorm = 104.5513, GNorm = 0.2566
Meta loss on this task batch = 3.6338e-01, Meta loss averaged over last 500 steps = 3.3973e-01, PNorm = 104.5585, GNorm = 0.3969
Meta loss on this task batch = 3.0136e-01, Meta loss averaged over last 500 steps = 3.3965e-01, PNorm = 104.5664, GNorm = 0.3198
Meta loss on this task batch = 3.2358e-01, Meta loss averaged over last 500 steps = 3.3972e-01, PNorm = 104.5741, GNorm = 0.2483
Meta loss on this task batch = 3.3525e-01, Meta loss averaged over last 500 steps = 3.3967e-01, PNorm = 104.5814, GNorm = 0.2650
Meta loss on this task batch = 3.1669e-01, Meta loss averaged over last 500 steps = 3.3960e-01, PNorm = 104.5891, GNorm = 0.2171
Meta loss on this task batch = 3.1550e-01, Meta loss averaged over last 500 steps = 3.3958e-01, PNorm = 104.5966, GNorm = 0.3135
Meta loss on this task batch = 3.0627e-01, Meta loss averaged over last 500 steps = 3.3951e-01, PNorm = 104.6045, GNorm = 0.2353
Meta loss on this task batch = 3.6077e-01, Meta loss averaged over last 500 steps = 3.3951e-01, PNorm = 104.6120, GNorm = 0.3460
Meta loss on this task batch = 3.2566e-01, Meta loss averaged over last 500 steps = 3.3958e-01, PNorm = 104.6196, GNorm = 0.2396
Meta loss on this task batch = 2.9053e-01, Meta loss averaged over last 500 steps = 3.3952e-01, PNorm = 104.6279, GNorm = 0.2668
Meta loss on this task batch = 3.1686e-01, Meta loss averaged over last 500 steps = 3.3960e-01, PNorm = 104.6365, GNorm = 0.2687
Meta loss on this task batch = 3.1157e-01, Meta loss averaged over last 500 steps = 3.3947e-01, PNorm = 104.6456, GNorm = 0.3022
Meta loss on this task batch = 2.3637e-01, Meta loss averaged over last 500 steps = 3.3940e-01, PNorm = 104.6556, GNorm = 0.2713
Took 108.69880795478821 seconds to complete one epoch of meta training
Took 116.20885610580444 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469212
Epoch 256
Meta loss on this task batch = 3.3230e-01, Meta loss averaged over last 500 steps = 3.3934e-01, PNorm = 104.6664, GNorm = 0.2312
Meta loss on this task batch = 2.8895e-01, Meta loss averaged over last 500 steps = 3.3926e-01, PNorm = 104.6776, GNorm = 0.2856
Meta loss on this task batch = 3.2483e-01, Meta loss averaged over last 500 steps = 3.3917e-01, PNorm = 104.6880, GNorm = 0.3079
Meta loss on this task batch = 3.1867e-01, Meta loss averaged over last 500 steps = 3.3913e-01, PNorm = 104.6988, GNorm = 0.3340
Meta loss on this task batch = 2.8967e-01, Meta loss averaged over last 500 steps = 3.3898e-01, PNorm = 104.7093, GNorm = 0.2507
Meta loss on this task batch = 2.8056e-01, Meta loss averaged over last 500 steps = 3.3879e-01, PNorm = 104.7200, GNorm = 0.2640
Meta loss on this task batch = 3.1672e-01, Meta loss averaged over last 500 steps = 3.3876e-01, PNorm = 104.7302, GNorm = 0.2788
Meta loss on this task batch = 3.7641e-01, Meta loss averaged over last 500 steps = 3.3889e-01, PNorm = 104.7378, GNorm = 0.5065
Meta loss on this task batch = 3.6685e-01, Meta loss averaged over last 500 steps = 3.3892e-01, PNorm = 104.7449, GNorm = 0.3096
Meta loss on this task batch = 3.4728e-01, Meta loss averaged over last 500 steps = 3.3887e-01, PNorm = 104.7515, GNorm = 0.4185
Meta loss on this task batch = 3.4770e-01, Meta loss averaged over last 500 steps = 3.3883e-01, PNorm = 104.7577, GNorm = 0.5114
Meta loss on this task batch = 3.2222e-01, Meta loss averaged over last 500 steps = 3.3890e-01, PNorm = 104.7646, GNorm = 0.2958
Meta loss on this task batch = 3.0645e-01, Meta loss averaged over last 500 steps = 3.3887e-01, PNorm = 104.7714, GNorm = 0.3060
Meta loss on this task batch = 3.3103e-01, Meta loss averaged over last 500 steps = 3.3880e-01, PNorm = 104.7777, GNorm = 0.4047
Meta loss on this task batch = 3.5687e-01, Meta loss averaged over last 500 steps = 3.3887e-01, PNorm = 104.7842, GNorm = 0.2574
Meta loss on this task batch = 3.6147e-01, Meta loss averaged over last 500 steps = 3.3885e-01, PNorm = 104.7905, GNorm = 0.2339
Meta loss on this task batch = 3.5101e-01, Meta loss averaged over last 500 steps = 3.3890e-01, PNorm = 104.7963, GNorm = 0.3549
Meta loss on this task batch = 3.7086e-01, Meta loss averaged over last 500 steps = 3.3890e-01, PNorm = 104.8035, GNorm = 0.2518
Meta loss on this task batch = 3.2833e-01, Meta loss averaged over last 500 steps = 3.3894e-01, PNorm = 104.8114, GNorm = 0.4416
Took 109.5323076248169 seconds to complete one epoch of meta training
Took 117.12212419509888 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486671
Epoch 257
Meta loss on this task batch = 3.1331e-01, Meta loss averaged over last 500 steps = 3.3882e-01, PNorm = 104.8197, GNorm = 0.3058
Meta loss on this task batch = 3.0814e-01, Meta loss averaged over last 500 steps = 3.3865e-01, PNorm = 104.8283, GNorm = 0.2301
Meta loss on this task batch = 2.9856e-01, Meta loss averaged over last 500 steps = 3.3860e-01, PNorm = 104.8367, GNorm = 0.2568
Meta loss on this task batch = 3.0268e-01, Meta loss averaged over last 500 steps = 3.3853e-01, PNorm = 104.8452, GNorm = 0.2889
Meta loss on this task batch = 2.8083e-01, Meta loss averaged over last 500 steps = 3.3844e-01, PNorm = 104.8545, GNorm = 0.2351
Meta loss on this task batch = 3.2200e-01, Meta loss averaged over last 500 steps = 3.3834e-01, PNorm = 104.8644, GNorm = 0.4718
Meta loss on this task batch = 3.9256e-01, Meta loss averaged over last 500 steps = 3.3823e-01, PNorm = 104.8739, GNorm = 0.4120
Meta loss on this task batch = 3.1679e-01, Meta loss averaged over last 500 steps = 3.3820e-01, PNorm = 104.8832, GNorm = 0.2412
Meta loss on this task batch = 3.4751e-01, Meta loss averaged over last 500 steps = 3.3821e-01, PNorm = 104.8920, GNorm = 0.3323
Meta loss on this task batch = 2.9802e-01, Meta loss averaged over last 500 steps = 3.3815e-01, PNorm = 104.9015, GNorm = 0.3047
Meta loss on this task batch = 3.4382e-01, Meta loss averaged over last 500 steps = 3.3815e-01, PNorm = 104.9102, GNorm = 0.3264
Meta loss on this task batch = 4.0177e-01, Meta loss averaged over last 500 steps = 3.3825e-01, PNorm = 104.9182, GNorm = 0.4278
Meta loss on this task batch = 3.7141e-01, Meta loss averaged over last 500 steps = 3.3833e-01, PNorm = 104.9263, GNorm = 0.2577
Meta loss on this task batch = 3.4388e-01, Meta loss averaged over last 500 steps = 3.3832e-01, PNorm = 104.9342, GNorm = 0.2980
Meta loss on this task batch = 3.4750e-01, Meta loss averaged over last 500 steps = 3.3829e-01, PNorm = 104.9414, GNorm = 0.2791
Meta loss on this task batch = 3.5883e-01, Meta loss averaged over last 500 steps = 3.3838e-01, PNorm = 104.9483, GNorm = 0.2761
Meta loss on this task batch = 3.1185e-01, Meta loss averaged over last 500 steps = 3.3826e-01, PNorm = 104.9555, GNorm = 0.2353
Meta loss on this task batch = 3.5370e-01, Meta loss averaged over last 500 steps = 3.3832e-01, PNorm = 104.9631, GNorm = 0.2562
Meta loss on this task batch = 3.1310e-01, Meta loss averaged over last 500 steps = 3.3831e-01, PNorm = 104.9709, GNorm = 0.3165
Took 111.08458828926086 seconds to complete one epoch of meta training
Took 118.56975984573364 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462132
Epoch 258
Meta loss on this task batch = 3.1059e-01, Meta loss averaged over last 500 steps = 3.3833e-01, PNorm = 104.9792, GNorm = 0.1980
Meta loss on this task batch = 3.1863e-01, Meta loss averaged over last 500 steps = 3.3825e-01, PNorm = 104.9881, GNorm = 0.2690
Meta loss on this task batch = 3.4087e-01, Meta loss averaged over last 500 steps = 3.3825e-01, PNorm = 104.9970, GNorm = 0.2572
Meta loss on this task batch = 3.4950e-01, Meta loss averaged over last 500 steps = 3.3814e-01, PNorm = 105.0062, GNorm = 0.2927
Meta loss on this task batch = 2.8228e-01, Meta loss averaged over last 500 steps = 3.3806e-01, PNorm = 105.0154, GNorm = 0.2441
Meta loss on this task batch = 3.2739e-01, Meta loss averaged over last 500 steps = 3.3815e-01, PNorm = 105.0247, GNorm = 0.2978
Meta loss on this task batch = 3.6035e-01, Meta loss averaged over last 500 steps = 3.3822e-01, PNorm = 105.0336, GNorm = 0.2683
Meta loss on this task batch = 3.1655e-01, Meta loss averaged over last 500 steps = 3.3826e-01, PNorm = 105.0426, GNorm = 0.3133
Meta loss on this task batch = 3.1230e-01, Meta loss averaged over last 500 steps = 3.3819e-01, PNorm = 105.0507, GNorm = 0.3157
Meta loss on this task batch = 4.2872e-01, Meta loss averaged over last 500 steps = 3.3844e-01, PNorm = 105.0577, GNorm = 0.3373
Meta loss on this task batch = 3.3289e-01, Meta loss averaged over last 500 steps = 3.3850e-01, PNorm = 105.0645, GNorm = 0.2986
Meta loss on this task batch = 2.7365e-01, Meta loss averaged over last 500 steps = 3.3826e-01, PNorm = 105.0713, GNorm = 0.2862
Meta loss on this task batch = 3.7024e-01, Meta loss averaged over last 500 steps = 3.3832e-01, PNorm = 105.0775, GNorm = 0.3714
Meta loss on this task batch = 3.7001e-01, Meta loss averaged over last 500 steps = 3.3835e-01, PNorm = 105.0837, GNorm = 0.4455
Meta loss on this task batch = 3.4226e-01, Meta loss averaged over last 500 steps = 3.3839e-01, PNorm = 105.0908, GNorm = 0.3744
Meta loss on this task batch = 3.5801e-01, Meta loss averaged over last 500 steps = 3.3838e-01, PNorm = 105.0983, GNorm = 0.3753
Meta loss on this task batch = 3.5805e-01, Meta loss averaged over last 500 steps = 3.3836e-01, PNorm = 105.1056, GNorm = 0.3860
Meta loss on this task batch = 2.7603e-01, Meta loss averaged over last 500 steps = 3.3816e-01, PNorm = 105.1136, GNorm = 0.2368
Meta loss on this task batch = 3.2268e-01, Meta loss averaged over last 500 steps = 3.3806e-01, PNorm = 105.1219, GNorm = 0.2967
Took 112.89459443092346 seconds to complete one epoch of meta training
Took 120.55631852149963 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453987
Epoch 259
Meta loss on this task batch = 3.2879e-01, Meta loss averaged over last 500 steps = 3.3797e-01, PNorm = 105.1303, GNorm = 0.2719
Meta loss on this task batch = 3.1334e-01, Meta loss averaged over last 500 steps = 3.3792e-01, PNorm = 105.1389, GNorm = 0.2624
Meta loss on this task batch = 3.2130e-01, Meta loss averaged over last 500 steps = 3.3788e-01, PNorm = 105.1483, GNorm = 0.2542
Meta loss on this task batch = 3.8744e-01, Meta loss averaged over last 500 steps = 3.3798e-01, PNorm = 105.1566, GNorm = 0.2901
Meta loss on this task batch = 3.8716e-01, Meta loss averaged over last 500 steps = 3.3805e-01, PNorm = 105.1640, GNorm = 0.2635
Meta loss on this task batch = 3.8022e-01, Meta loss averaged over last 500 steps = 3.3817e-01, PNorm = 105.1701, GNorm = 0.3042
Meta loss on this task batch = 2.9841e-01, Meta loss averaged over last 500 steps = 3.3819e-01, PNorm = 105.1757, GNorm = 0.4800
Meta loss on this task batch = 3.1767e-01, Meta loss averaged over last 500 steps = 3.3819e-01, PNorm = 105.1814, GNorm = 0.2470
Meta loss on this task batch = 3.1086e-01, Meta loss averaged over last 500 steps = 3.3821e-01, PNorm = 105.1876, GNorm = 0.2832
Meta loss on this task batch = 4.1322e-01, Meta loss averaged over last 500 steps = 3.3827e-01, PNorm = 105.1929, GNorm = 0.3000
Meta loss on this task batch = 3.4290e-01, Meta loss averaged over last 500 steps = 3.3825e-01, PNorm = 105.1977, GNorm = 0.2567
Meta loss on this task batch = 3.0973e-01, Meta loss averaged over last 500 steps = 3.3827e-01, PNorm = 105.2037, GNorm = 0.2316
Meta loss on this task batch = 3.3444e-01, Meta loss averaged over last 500 steps = 3.3821e-01, PNorm = 105.2094, GNorm = 0.2402
Meta loss on this task batch = 3.3032e-01, Meta loss averaged over last 500 steps = 3.3817e-01, PNorm = 105.2159, GNorm = 0.2339
Meta loss on this task batch = 3.1536e-01, Meta loss averaged over last 500 steps = 3.3813e-01, PNorm = 105.2227, GNorm = 0.3237
Meta loss on this task batch = 3.0546e-01, Meta loss averaged over last 500 steps = 3.3804e-01, PNorm = 105.2299, GNorm = 0.2240
Meta loss on this task batch = 3.3922e-01, Meta loss averaged over last 500 steps = 3.3802e-01, PNorm = 105.2369, GNorm = 0.2849
Meta loss on this task batch = 3.1879e-01, Meta loss averaged over last 500 steps = 3.3796e-01, PNorm = 105.2440, GNorm = 0.2600
Meta loss on this task batch = 3.4401e-01, Meta loss averaged over last 500 steps = 3.3802e-01, PNorm = 105.2501, GNorm = 0.3441
Took 111.81181049346924 seconds to complete one epoch of meta training
Took 119.85418510437012 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472470
Epoch 260
Meta loss on this task batch = 2.9566e-01, Meta loss averaged over last 500 steps = 3.3789e-01, PNorm = 105.2571, GNorm = 0.2765
Meta loss on this task batch = 3.8483e-01, Meta loss averaged over last 500 steps = 3.3796e-01, PNorm = 105.2643, GNorm = 0.2830
Meta loss on this task batch = 2.8534e-01, Meta loss averaged over last 500 steps = 3.3788e-01, PNorm = 105.2720, GNorm = 0.2660
Meta loss on this task batch = 3.7457e-01, Meta loss averaged over last 500 steps = 3.3800e-01, PNorm = 105.2800, GNorm = 0.2831
Meta loss on this task batch = 3.3841e-01, Meta loss averaged over last 500 steps = 3.3795e-01, PNorm = 105.2887, GNorm = 0.3299
Meta loss on this task batch = 2.9031e-01, Meta loss averaged over last 500 steps = 3.3782e-01, PNorm = 105.2977, GNorm = 0.2527
Meta loss on this task batch = 3.4744e-01, Meta loss averaged over last 500 steps = 3.3782e-01, PNorm = 105.3062, GNorm = 0.2343
Meta loss on this task batch = 3.2456e-01, Meta loss averaged over last 500 steps = 3.3777e-01, PNorm = 105.3150, GNorm = 0.2388
Meta loss on this task batch = 3.2314e-01, Meta loss averaged over last 500 steps = 3.3771e-01, PNorm = 105.3236, GNorm = 0.2451
Meta loss on this task batch = 4.1050e-01, Meta loss averaged over last 500 steps = 3.3798e-01, PNorm = 105.3321, GNorm = 0.3119
Meta loss on this task batch = 2.9678e-01, Meta loss averaged over last 500 steps = 3.3794e-01, PNorm = 105.3414, GNorm = 0.3883
Meta loss on this task batch = 3.7738e-01, Meta loss averaged over last 500 steps = 3.3797e-01, PNorm = 105.3500, GNorm = 0.2932
Meta loss on this task batch = 3.2085e-01, Meta loss averaged over last 500 steps = 3.3794e-01, PNorm = 105.3589, GNorm = 0.2955
Meta loss on this task batch = 3.1909e-01, Meta loss averaged over last 500 steps = 3.3794e-01, PNorm = 105.3683, GNorm = 0.2543
Meta loss on this task batch = 3.5115e-01, Meta loss averaged over last 500 steps = 3.3795e-01, PNorm = 105.3776, GNorm = 0.3424
Meta loss on this task batch = 2.9979e-01, Meta loss averaged over last 500 steps = 3.3789e-01, PNorm = 105.3871, GNorm = 0.2360
Meta loss on this task batch = 2.9276e-01, Meta loss averaged over last 500 steps = 3.3776e-01, PNorm = 105.3966, GNorm = 0.2535
Meta loss on this task batch = 2.9789e-01, Meta loss averaged over last 500 steps = 3.3758e-01, PNorm = 105.4060, GNorm = 0.2975
Meta loss on this task batch = 3.5230e-01, Meta loss averaged over last 500 steps = 3.3750e-01, PNorm = 105.4147, GNorm = 0.3745
Took 111.66634702682495 seconds to complete one epoch of meta training
Took 119.42467784881592 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475208
Epoch 261
Meta loss on this task batch = 3.1708e-01, Meta loss averaged over last 500 steps = 3.3753e-01, PNorm = 105.4241, GNorm = 0.4227
Meta loss on this task batch = 3.0611e-01, Meta loss averaged over last 500 steps = 3.3748e-01, PNorm = 105.4323, GNorm = 0.2591
Meta loss on this task batch = 3.1162e-01, Meta loss averaged over last 500 steps = 3.3740e-01, PNorm = 105.4397, GNorm = 0.2468
Meta loss on this task batch = 3.7391e-01, Meta loss averaged over last 500 steps = 3.3749e-01, PNorm = 105.4454, GNorm = 0.3340
Meta loss on this task batch = 3.8978e-01, Meta loss averaged over last 500 steps = 3.3752e-01, PNorm = 105.4497, GNorm = 0.3824
Meta loss on this task batch = 3.4196e-01, Meta loss averaged over last 500 steps = 3.3756e-01, PNorm = 105.4542, GNorm = 0.2609
Meta loss on this task batch = 3.3728e-01, Meta loss averaged over last 500 steps = 3.3748e-01, PNorm = 105.4592, GNorm = 0.3140
Meta loss on this task batch = 3.2392e-01, Meta loss averaged over last 500 steps = 3.3745e-01, PNorm = 105.4648, GNorm = 0.2271
Meta loss on this task batch = 3.0401e-01, Meta loss averaged over last 500 steps = 3.3733e-01, PNorm = 105.4707, GNorm = 0.2462
Meta loss on this task batch = 3.1986e-01, Meta loss averaged over last 500 steps = 3.3730e-01, PNorm = 105.4766, GNorm = 0.4144
Meta loss on this task batch = 3.2433e-01, Meta loss averaged over last 500 steps = 3.3728e-01, PNorm = 105.4832, GNorm = 0.2943
Meta loss on this task batch = 3.0875e-01, Meta loss averaged over last 500 steps = 3.3731e-01, PNorm = 105.4900, GNorm = 0.2370
Meta loss on this task batch = 3.4778e-01, Meta loss averaged over last 500 steps = 3.3740e-01, PNorm = 105.4975, GNorm = 0.3372
Meta loss on this task batch = 3.4451e-01, Meta loss averaged over last 500 steps = 3.3741e-01, PNorm = 105.5053, GNorm = 0.2599
Meta loss on this task batch = 2.8875e-01, Meta loss averaged over last 500 steps = 3.3730e-01, PNorm = 105.5129, GNorm = 0.3155
Meta loss on this task batch = 2.8483e-01, Meta loss averaged over last 500 steps = 3.3709e-01, PNorm = 105.5207, GNorm = 0.2743
Meta loss on this task batch = 3.5989e-01, Meta loss averaged over last 500 steps = 3.3707e-01, PNorm = 105.5289, GNorm = 0.2527
Meta loss on this task batch = 3.5420e-01, Meta loss averaged over last 500 steps = 3.3707e-01, PNorm = 105.5371, GNorm = 0.2530
Meta loss on this task batch = 3.7434e-01, Meta loss averaged over last 500 steps = 3.3719e-01, PNorm = 105.5444, GNorm = 0.4100
Took 113.20407962799072 seconds to complete one epoch of meta training
Took 121.08147263526917 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488233
Epoch 262
Meta loss on this task batch = 3.1268e-01, Meta loss averaged over last 500 steps = 3.3707e-01, PNorm = 105.5518, GNorm = 0.2731
Meta loss on this task batch = 3.8599e-01, Meta loss averaged over last 500 steps = 3.3721e-01, PNorm = 105.5593, GNorm = 0.3766
Meta loss on this task batch = 3.3171e-01, Meta loss averaged over last 500 steps = 3.3720e-01, PNorm = 105.5666, GNorm = 0.2928
Meta loss on this task batch = 3.5578e-01, Meta loss averaged over last 500 steps = 3.3732e-01, PNorm = 105.5747, GNorm = 0.3030
Meta loss on this task batch = 3.5018e-01, Meta loss averaged over last 500 steps = 3.3736e-01, PNorm = 105.5820, GNorm = 0.3910
Meta loss on this task batch = 3.1900e-01, Meta loss averaged over last 500 steps = 3.3736e-01, PNorm = 105.5889, GNorm = 0.2981
Meta loss on this task batch = 3.0321e-01, Meta loss averaged over last 500 steps = 3.3735e-01, PNorm = 105.5968, GNorm = 0.2413
Meta loss on this task batch = 3.5642e-01, Meta loss averaged over last 500 steps = 3.3727e-01, PNorm = 105.6051, GNorm = 0.4558
Meta loss on this task batch = 3.5228e-01, Meta loss averaged over last 500 steps = 3.3727e-01, PNorm = 105.6133, GNorm = 0.2619
Meta loss on this task batch = 3.1411e-01, Meta loss averaged over last 500 steps = 3.3729e-01, PNorm = 105.6217, GNorm = 0.3412
Meta loss on this task batch = 3.3264e-01, Meta loss averaged over last 500 steps = 3.3728e-01, PNorm = 105.6306, GNorm = 0.2993
Meta loss on this task batch = 3.7228e-01, Meta loss averaged over last 500 steps = 3.3733e-01, PNorm = 105.6389, GNorm = 0.4420
Meta loss on this task batch = 3.4569e-01, Meta loss averaged over last 500 steps = 3.3728e-01, PNorm = 105.6467, GNorm = 0.3344
Meta loss on this task batch = 3.2181e-01, Meta loss averaged over last 500 steps = 3.3728e-01, PNorm = 105.6543, GNorm = 0.3053
Meta loss on this task batch = 3.2666e-01, Meta loss averaged over last 500 steps = 3.3717e-01, PNorm = 105.6632, GNorm = 0.2813
Meta loss on this task batch = 3.7341e-01, Meta loss averaged over last 500 steps = 3.3717e-01, PNorm = 105.6708, GNorm = 0.3136
Meta loss on this task batch = 3.1880e-01, Meta loss averaged over last 500 steps = 3.3714e-01, PNorm = 105.6789, GNorm = 0.2685
Meta loss on this task batch = 3.6857e-01, Meta loss averaged over last 500 steps = 3.3724e-01, PNorm = 105.6865, GNorm = 0.2935
Meta loss on this task batch = 2.8543e-01, Meta loss averaged over last 500 steps = 3.3711e-01, PNorm = 105.6950, GNorm = 0.2989
Took 111.02445983886719 seconds to complete one epoch of meta training
Took 118.68387508392334 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482129
Epoch 263
Meta loss on this task batch = 3.9837e-01, Meta loss averaged over last 500 steps = 3.3735e-01, PNorm = 105.7038, GNorm = 0.3579
Meta loss on this task batch = 3.4690e-01, Meta loss averaged over last 500 steps = 3.3732e-01, PNorm = 105.7126, GNorm = 0.2428
Meta loss on this task batch = 3.4166e-01, Meta loss averaged over last 500 steps = 3.3735e-01, PNorm = 105.7221, GNorm = 0.2866
Meta loss on this task batch = 3.4811e-01, Meta loss averaged over last 500 steps = 3.3741e-01, PNorm = 105.7315, GNorm = 0.2417
Meta loss on this task batch = 2.9303e-01, Meta loss averaged over last 500 steps = 3.3736e-01, PNorm = 105.7419, GNorm = 0.2496
Meta loss on this task batch = 3.6256e-01, Meta loss averaged over last 500 steps = 3.3745e-01, PNorm = 105.7516, GNorm = 0.2887
Meta loss on this task batch = 2.8671e-01, Meta loss averaged over last 500 steps = 3.3734e-01, PNorm = 105.7620, GNorm = 0.2453
Meta loss on this task batch = 3.0284e-01, Meta loss averaged over last 500 steps = 3.3729e-01, PNorm = 105.7720, GNorm = 0.2395
Meta loss on this task batch = 3.3622e-01, Meta loss averaged over last 500 steps = 3.3725e-01, PNorm = 105.7820, GNorm = 0.2938
Meta loss on this task batch = 3.2516e-01, Meta loss averaged over last 500 steps = 3.3732e-01, PNorm = 105.7918, GNorm = 0.3319
Meta loss on this task batch = 3.6321e-01, Meta loss averaged over last 500 steps = 3.3744e-01, PNorm = 105.8012, GNorm = 0.2655
Meta loss on this task batch = 3.9989e-01, Meta loss averaged over last 500 steps = 3.3751e-01, PNorm = 105.8093, GNorm = 0.3102
Meta loss on this task batch = 3.1070e-01, Meta loss averaged over last 500 steps = 3.3750e-01, PNorm = 105.8163, GNorm = 0.3118
Meta loss on this task batch = 2.8984e-01, Meta loss averaged over last 500 steps = 3.3746e-01, PNorm = 105.8232, GNorm = 0.5559
Meta loss on this task batch = 3.0474e-01, Meta loss averaged over last 500 steps = 3.3739e-01, PNorm = 105.8290, GNorm = 0.2637
Meta loss on this task batch = 3.3321e-01, Meta loss averaged over last 500 steps = 3.3747e-01, PNorm = 105.8349, GNorm = 0.3415
Meta loss on this task batch = 3.6584e-01, Meta loss averaged over last 500 steps = 3.3754e-01, PNorm = 105.8403, GNorm = 0.2891
Meta loss on this task batch = 3.4410e-01, Meta loss averaged over last 500 steps = 3.3759e-01, PNorm = 105.8464, GNorm = 0.3811
Meta loss on this task batch = 3.6096e-01, Meta loss averaged over last 500 steps = 3.3765e-01, PNorm = 105.8513, GNorm = 0.3532
Took 109.27095556259155 seconds to complete one epoch of meta training
Took 116.9920654296875 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481923
Epoch 264
Meta loss on this task batch = 2.8297e-01, Meta loss averaged over last 500 steps = 3.3757e-01, PNorm = 105.8572, GNorm = 0.2070
Meta loss on this task batch = 3.2415e-01, Meta loss averaged over last 500 steps = 3.3765e-01, PNorm = 105.8637, GNorm = 0.3284
Meta loss on this task batch = 2.8971e-01, Meta loss averaged over last 500 steps = 3.3752e-01, PNorm = 105.8709, GNorm = 0.2748
Meta loss on this task batch = 3.4549e-01, Meta loss averaged over last 500 steps = 3.3745e-01, PNorm = 105.8773, GNorm = 0.2843
Meta loss on this task batch = 3.3748e-01, Meta loss averaged over last 500 steps = 3.3741e-01, PNorm = 105.8847, GNorm = 0.2274
Meta loss on this task batch = 3.0163e-01, Meta loss averaged over last 500 steps = 3.3735e-01, PNorm = 105.8919, GNorm = 0.3383
Meta loss on this task batch = 3.4291e-01, Meta loss averaged over last 500 steps = 3.3742e-01, PNorm = 105.9000, GNorm = 0.2898
Meta loss on this task batch = 3.2704e-01, Meta loss averaged over last 500 steps = 3.3734e-01, PNorm = 105.9080, GNorm = 0.2785
Meta loss on this task batch = 3.5715e-01, Meta loss averaged over last 500 steps = 3.3729e-01, PNorm = 105.9165, GNorm = 0.3179
Meta loss on this task batch = 2.7195e-01, Meta loss averaged over last 500 steps = 3.3724e-01, PNorm = 105.9249, GNorm = 0.2360
Meta loss on this task batch = 3.1896e-01, Meta loss averaged over last 500 steps = 3.3720e-01, PNorm = 105.9329, GNorm = 0.2829
Meta loss on this task batch = 3.3076e-01, Meta loss averaged over last 500 steps = 3.3724e-01, PNorm = 105.9417, GNorm = 0.3060
Meta loss on this task batch = 3.6385e-01, Meta loss averaged over last 500 steps = 3.3725e-01, PNorm = 105.9501, GNorm = 0.3040
Meta loss on this task batch = 3.3101e-01, Meta loss averaged over last 500 steps = 3.3726e-01, PNorm = 105.9581, GNorm = 0.2462
Meta loss on this task batch = 3.1181e-01, Meta loss averaged over last 500 steps = 3.3719e-01, PNorm = 105.9671, GNorm = 0.3335
Meta loss on this task batch = 3.3257e-01, Meta loss averaged over last 500 steps = 3.3714e-01, PNorm = 105.9762, GNorm = 0.3113
Meta loss on this task batch = 3.5999e-01, Meta loss averaged over last 500 steps = 3.3720e-01, PNorm = 105.9841, GNorm = 0.3863
Meta loss on this task batch = 3.0655e-01, Meta loss averaged over last 500 steps = 3.3710e-01, PNorm = 105.9913, GNorm = 0.3616
Meta loss on this task batch = 3.3003e-01, Meta loss averaged over last 500 steps = 3.3704e-01, PNorm = 105.9990, GNorm = 0.3452
Took 110.02404499053955 seconds to complete one epoch of meta training
Took 117.60126328468323 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.438723
Epoch 265
Meta loss on this task batch = 2.8111e-01, Meta loss averaged over last 500 steps = 3.3676e-01, PNorm = 106.0078, GNorm = 0.3830
Meta loss on this task batch = 3.2740e-01, Meta loss averaged over last 500 steps = 3.3666e-01, PNorm = 106.0172, GNorm = 0.3533
Meta loss on this task batch = 3.1630e-01, Meta loss averaged over last 500 steps = 3.3671e-01, PNorm = 106.0274, GNorm = 0.2328
Meta loss on this task batch = 3.5354e-01, Meta loss averaged over last 500 steps = 3.3668e-01, PNorm = 106.0377, GNorm = 0.2801
Meta loss on this task batch = 3.2740e-01, Meta loss averaged over last 500 steps = 3.3659e-01, PNorm = 106.0478, GNorm = 0.2408
Meta loss on this task batch = 3.3557e-01, Meta loss averaged over last 500 steps = 3.3647e-01, PNorm = 106.0579, GNorm = 0.2852
Meta loss on this task batch = 2.8579e-01, Meta loss averaged over last 500 steps = 3.3637e-01, PNorm = 106.0677, GNorm = 0.2455
Meta loss on this task batch = 3.3180e-01, Meta loss averaged over last 500 steps = 3.3629e-01, PNorm = 106.0777, GNorm = 0.2458
Meta loss on this task batch = 3.9194e-01, Meta loss averaged over last 500 steps = 3.3629e-01, PNorm = 106.0860, GNorm = 0.3448
Meta loss on this task batch = 2.6916e-01, Meta loss averaged over last 500 steps = 3.3610e-01, PNorm = 106.0946, GNorm = 0.4182
Meta loss on this task batch = 3.4047e-01, Meta loss averaged over last 500 steps = 3.3609e-01, PNorm = 106.1016, GNorm = 0.2762
Meta loss on this task batch = 3.5180e-01, Meta loss averaged over last 500 steps = 3.3606e-01, PNorm = 106.1084, GNorm = 0.3580
Meta loss on this task batch = 3.0314e-01, Meta loss averaged over last 500 steps = 3.3589e-01, PNorm = 106.1156, GNorm = 0.2626
Meta loss on this task batch = 2.9206e-01, Meta loss averaged over last 500 steps = 3.3581e-01, PNorm = 106.1234, GNorm = 0.2922
Meta loss on this task batch = 3.0034e-01, Meta loss averaged over last 500 steps = 3.3566e-01, PNorm = 106.1314, GNorm = 0.3477
Meta loss on this task batch = 3.4244e-01, Meta loss averaged over last 500 steps = 3.3571e-01, PNorm = 106.1390, GNorm = 0.4115
Meta loss on this task batch = 3.4999e-01, Meta loss averaged over last 500 steps = 3.3566e-01, PNorm = 106.1461, GNorm = 0.2797
Meta loss on this task batch = 2.7242e-01, Meta loss averaged over last 500 steps = 3.3557e-01, PNorm = 106.1542, GNorm = 0.2713
Meta loss on this task batch = 3.5763e-01, Meta loss averaged over last 500 steps = 3.3551e-01, PNorm = 106.1625, GNorm = 0.3456
Took 109.53041791915894 seconds to complete one epoch of meta training
Took 117.27000617980957 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456326
Epoch 266
Meta loss on this task batch = 3.5726e-01, Meta loss averaged over last 500 steps = 3.3552e-01, PNorm = 106.1709, GNorm = 0.2538
Meta loss on this task batch = 3.2454e-01, Meta loss averaged over last 500 steps = 3.3551e-01, PNorm = 106.1799, GNorm = 0.3126
Meta loss on this task batch = 3.6032e-01, Meta loss averaged over last 500 steps = 3.3565e-01, PNorm = 106.1888, GNorm = 0.2583
Meta loss on this task batch = 3.5781e-01, Meta loss averaged over last 500 steps = 3.3560e-01, PNorm = 106.1964, GNorm = 0.2577
Meta loss on this task batch = 4.0632e-01, Meta loss averaged over last 500 steps = 3.3573e-01, PNorm = 106.2037, GNorm = 0.3114
Meta loss on this task batch = 3.0852e-01, Meta loss averaged over last 500 steps = 3.3570e-01, PNorm = 106.2101, GNorm = 0.2828
Meta loss on this task batch = 2.8669e-01, Meta loss averaged over last 500 steps = 3.3559e-01, PNorm = 106.2179, GNorm = 0.2312
Meta loss on this task batch = 3.2213e-01, Meta loss averaged over last 500 steps = 3.3552e-01, PNorm = 106.2269, GNorm = 0.2790
Meta loss on this task batch = 3.3350e-01, Meta loss averaged over last 500 steps = 3.3544e-01, PNorm = 106.2369, GNorm = 0.2280
Meta loss on this task batch = 3.4213e-01, Meta loss averaged over last 500 steps = 3.3547e-01, PNorm = 106.2474, GNorm = 0.3437
Meta loss on this task batch = 3.1954e-01, Meta loss averaged over last 500 steps = 3.3538e-01, PNorm = 106.2580, GNorm = 0.2471
Meta loss on this task batch = 2.8134e-01, Meta loss averaged over last 500 steps = 3.3518e-01, PNorm = 106.2688, GNorm = 0.2744
Meta loss on this task batch = 3.1314e-01, Meta loss averaged over last 500 steps = 3.3514e-01, PNorm = 106.2801, GNorm = 0.3416
Meta loss on this task batch = 3.2512e-01, Meta loss averaged over last 500 steps = 3.3508e-01, PNorm = 106.2912, GNorm = 0.2632
Meta loss on this task batch = 3.7630e-01, Meta loss averaged over last 500 steps = 3.3504e-01, PNorm = 106.3004, GNorm = 0.2992
Meta loss on this task batch = 3.6548e-01, Meta loss averaged over last 500 steps = 3.3503e-01, PNorm = 106.3092, GNorm = 0.2874
Meta loss on this task batch = 3.1654e-01, Meta loss averaged over last 500 steps = 3.3503e-01, PNorm = 106.3168, GNorm = 0.2857
Meta loss on this task batch = 2.5938e-01, Meta loss averaged over last 500 steps = 3.3496e-01, PNorm = 106.3240, GNorm = 0.2244
Meta loss on this task batch = 3.1584e-01, Meta loss averaged over last 500 steps = 3.3490e-01, PNorm = 106.3318, GNorm = 0.2948
Took 111.58420038223267 seconds to complete one epoch of meta training
Took 119.4294376373291 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477007
Epoch 267
Meta loss on this task batch = 3.6333e-01, Meta loss averaged over last 500 steps = 3.3480e-01, PNorm = 106.3391, GNorm = 0.3421
Meta loss on this task batch = 3.5230e-01, Meta loss averaged over last 500 steps = 3.3485e-01, PNorm = 106.3469, GNorm = 0.3013
Meta loss on this task batch = 3.2695e-01, Meta loss averaged over last 500 steps = 3.3487e-01, PNorm = 106.3545, GNorm = 0.3747
Meta loss on this task batch = 2.9910e-01, Meta loss averaged over last 500 steps = 3.3489e-01, PNorm = 106.3618, GNorm = 0.2491
Meta loss on this task batch = 3.2487e-01, Meta loss averaged over last 500 steps = 3.3482e-01, PNorm = 106.3690, GNorm = 0.3436
Meta loss on this task batch = 3.6167e-01, Meta loss averaged over last 500 steps = 3.3479e-01, PNorm = 106.3763, GNorm = 0.5073
Meta loss on this task batch = 3.1516e-01, Meta loss averaged over last 500 steps = 3.3479e-01, PNorm = 106.3840, GNorm = 0.4064
Meta loss on this task batch = 2.9197e-01, Meta loss averaged over last 500 steps = 3.3470e-01, PNorm = 106.3928, GNorm = 0.2671
Meta loss on this task batch = 3.5359e-01, Meta loss averaged over last 500 steps = 3.3486e-01, PNorm = 106.4019, GNorm = 0.2676
Meta loss on this task batch = 2.9742e-01, Meta loss averaged over last 500 steps = 3.3467e-01, PNorm = 106.4105, GNorm = 0.2348
Meta loss on this task batch = 2.9582e-01, Meta loss averaged over last 500 steps = 3.3459e-01, PNorm = 106.4185, GNorm = 0.3962
Meta loss on this task batch = 3.1120e-01, Meta loss averaged over last 500 steps = 3.3447e-01, PNorm = 106.4259, GNorm = 0.2717
Meta loss on this task batch = 2.9634e-01, Meta loss averaged over last 500 steps = 3.3434e-01, PNorm = 106.4333, GNorm = 0.2414
Meta loss on this task batch = 3.6636e-01, Meta loss averaged over last 500 steps = 3.3442e-01, PNorm = 106.4404, GNorm = 0.3251
Meta loss on this task batch = 3.4596e-01, Meta loss averaged over last 500 steps = 3.3442e-01, PNorm = 106.4465, GNorm = 0.2771
Meta loss on this task batch = 3.8332e-01, Meta loss averaged over last 500 steps = 3.3449e-01, PNorm = 106.4512, GNorm = 0.5033
Meta loss on this task batch = 3.4529e-01, Meta loss averaged over last 500 steps = 3.3447e-01, PNorm = 106.4551, GNorm = 0.3121
Meta loss on this task batch = 3.0046e-01, Meta loss averaged over last 500 steps = 3.3444e-01, PNorm = 106.4617, GNorm = 0.2496
Meta loss on this task batch = 2.9410e-01, Meta loss averaged over last 500 steps = 3.3425e-01, PNorm = 106.4680, GNorm = 0.3961
Took 115.50874090194702 seconds to complete one epoch of meta training
Took 123.11279940605164 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467958
Epoch 268
Meta loss on this task batch = 3.2744e-01, Meta loss averaged over last 500 steps = 3.3424e-01, PNorm = 106.4751, GNorm = 0.3663
Meta loss on this task batch = 3.7222e-01, Meta loss averaged over last 500 steps = 3.3429e-01, PNorm = 106.4830, GNorm = 0.3616
Meta loss on this task batch = 3.1519e-01, Meta loss averaged over last 500 steps = 3.3433e-01, PNorm = 106.4918, GNorm = 0.3188
Meta loss on this task batch = 3.0762e-01, Meta loss averaged over last 500 steps = 3.3432e-01, PNorm = 106.5006, GNorm = 0.2360
Meta loss on this task batch = 3.3814e-01, Meta loss averaged over last 500 steps = 3.3434e-01, PNorm = 106.5093, GNorm = 0.3490
Meta loss on this task batch = 3.0513e-01, Meta loss averaged over last 500 steps = 3.3439e-01, PNorm = 106.5184, GNorm = 0.2849
Meta loss on this task batch = 2.9218e-01, Meta loss averaged over last 500 steps = 3.3425e-01, PNorm = 106.5278, GNorm = 0.2888
Meta loss on this task batch = 3.4972e-01, Meta loss averaged over last 500 steps = 3.3431e-01, PNorm = 106.5357, GNorm = 0.2780
Meta loss on this task batch = 3.5760e-01, Meta loss averaged over last 500 steps = 3.3444e-01, PNorm = 106.5437, GNorm = 0.2919
Meta loss on this task batch = 3.2957e-01, Meta loss averaged over last 500 steps = 3.3431e-01, PNorm = 106.5525, GNorm = 0.2304
Meta loss on this task batch = 3.1188e-01, Meta loss averaged over last 500 steps = 3.3437e-01, PNorm = 106.5618, GNorm = 0.3121
Meta loss on this task batch = 3.1962e-01, Meta loss averaged over last 500 steps = 3.3432e-01, PNorm = 106.5712, GNorm = 0.2974
Meta loss on this task batch = 3.2586e-01, Meta loss averaged over last 500 steps = 3.3418e-01, PNorm = 106.5803, GNorm = 0.2358
Meta loss on this task batch = 3.3272e-01, Meta loss averaged over last 500 steps = 3.3426e-01, PNorm = 106.5890, GNorm = 0.3096
Meta loss on this task batch = 3.3491e-01, Meta loss averaged over last 500 steps = 3.3431e-01, PNorm = 106.5977, GNorm = 0.3793
Meta loss on this task batch = 3.7125e-01, Meta loss averaged over last 500 steps = 3.3432e-01, PNorm = 106.6062, GNorm = 0.3320
Meta loss on this task batch = 3.1944e-01, Meta loss averaged over last 500 steps = 3.3423e-01, PNorm = 106.6143, GNorm = 0.3573
Meta loss on this task batch = 3.6505e-01, Meta loss averaged over last 500 steps = 3.3419e-01, PNorm = 106.6216, GNorm = 0.5024
Meta loss on this task batch = 4.1094e-01, Meta loss averaged over last 500 steps = 3.3437e-01, PNorm = 106.6275, GNorm = 0.4837
Took 109.41069626808167 seconds to complete one epoch of meta training
Took 116.85658121109009 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466725
Epoch 269
Meta loss on this task batch = 3.0738e-01, Meta loss averaged over last 500 steps = 3.3423e-01, PNorm = 106.6341, GNorm = 0.2387
Meta loss on this task batch = 4.0151e-01, Meta loss averaged over last 500 steps = 3.3440e-01, PNorm = 106.6406, GNorm = 0.3995
Meta loss on this task batch = 3.5651e-01, Meta loss averaged over last 500 steps = 3.3446e-01, PNorm = 106.6477, GNorm = 0.2387
Meta loss on this task batch = 2.8204e-01, Meta loss averaged over last 500 steps = 3.3430e-01, PNorm = 106.6555, GNorm = 0.3957
Meta loss on this task batch = 3.5971e-01, Meta loss averaged over last 500 steps = 3.3431e-01, PNorm = 106.6628, GNorm = 0.3346
Meta loss on this task batch = 2.9088e-01, Meta loss averaged over last 500 steps = 3.3422e-01, PNorm = 106.6715, GNorm = 0.2719
Meta loss on this task batch = 3.8081e-01, Meta loss averaged over last 500 steps = 3.3426e-01, PNorm = 106.6807, GNorm = 0.6266
Meta loss on this task batch = 3.4649e-01, Meta loss averaged over last 500 steps = 3.3430e-01, PNorm = 106.6903, GNorm = 0.3766
Meta loss on this task batch = 3.6098e-01, Meta loss averaged over last 500 steps = 3.3435e-01, PNorm = 106.7000, GNorm = 0.2517
Meta loss on this task batch = 3.3052e-01, Meta loss averaged over last 500 steps = 3.3434e-01, PNorm = 106.7096, GNorm = 0.2811
Meta loss on this task batch = 3.6384e-01, Meta loss averaged over last 500 steps = 3.3434e-01, PNorm = 106.7194, GNorm = 0.2726
Meta loss on this task batch = 3.1223e-01, Meta loss averaged over last 500 steps = 3.3430e-01, PNorm = 106.7274, GNorm = 0.2964
Meta loss on this task batch = 2.5385e-01, Meta loss averaged over last 500 steps = 3.3419e-01, PNorm = 106.7351, GNorm = 0.4590
Meta loss on this task batch = 3.0858e-01, Meta loss averaged over last 500 steps = 3.3404e-01, PNorm = 106.7430, GNorm = 0.2464
Meta loss on this task batch = 3.4672e-01, Meta loss averaged over last 500 steps = 3.3407e-01, PNorm = 106.7516, GNorm = 0.2673
Meta loss on this task batch = 3.5265e-01, Meta loss averaged over last 500 steps = 3.3417e-01, PNorm = 106.7592, GNorm = 0.2505
Meta loss on this task batch = 2.6711e-01, Meta loss averaged over last 500 steps = 3.3403e-01, PNorm = 106.7674, GNorm = 0.3310
Meta loss on this task batch = 3.2173e-01, Meta loss averaged over last 500 steps = 3.3397e-01, PNorm = 106.7750, GNorm = 0.2708
Meta loss on this task batch = 3.6656e-01, Meta loss averaged over last 500 steps = 3.3403e-01, PNorm = 106.7807, GNorm = 0.4415
Took 115.06152749061584 seconds to complete one epoch of meta training
Took 122.55718755722046 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490375
Epoch 270
Meta loss on this task batch = 3.1487e-01, Meta loss averaged over last 500 steps = 3.3392e-01, PNorm = 106.7869, GNorm = 0.2922
Meta loss on this task batch = 3.5632e-01, Meta loss averaged over last 500 steps = 3.3394e-01, PNorm = 106.7932, GNorm = 0.3179
Meta loss on this task batch = 3.3957e-01, Meta loss averaged over last 500 steps = 3.3390e-01, PNorm = 106.8000, GNorm = 0.5052
Meta loss on this task batch = 3.5240e-01, Meta loss averaged over last 500 steps = 3.3393e-01, PNorm = 106.8073, GNorm = 0.4645
Meta loss on this task batch = 2.8775e-01, Meta loss averaged over last 500 steps = 3.3385e-01, PNorm = 106.8155, GNorm = 0.2914
Meta loss on this task batch = 3.5269e-01, Meta loss averaged over last 500 steps = 3.3385e-01, PNorm = 106.8243, GNorm = 0.2906
Meta loss on this task batch = 3.5633e-01, Meta loss averaged over last 500 steps = 3.3383e-01, PNorm = 106.8327, GNorm = 0.2774
Meta loss on this task batch = 3.2612e-01, Meta loss averaged over last 500 steps = 3.3376e-01, PNorm = 106.8409, GNorm = 0.3904
Meta loss on this task batch = 3.0904e-01, Meta loss averaged over last 500 steps = 3.3363e-01, PNorm = 106.8504, GNorm = 0.2361
Meta loss on this task batch = 3.0225e-01, Meta loss averaged over last 500 steps = 3.3354e-01, PNorm = 106.8600, GNorm = 0.2649
Meta loss on this task batch = 3.3457e-01, Meta loss averaged over last 500 steps = 3.3356e-01, PNorm = 106.8692, GNorm = 0.2347
Meta loss on this task batch = 3.1429e-01, Meta loss averaged over last 500 steps = 3.3356e-01, PNorm = 106.8782, GNorm = 0.2323
Meta loss on this task batch = 3.6906e-01, Meta loss averaged over last 500 steps = 3.3370e-01, PNorm = 106.8877, GNorm = 0.2384
Meta loss on this task batch = 2.9556e-01, Meta loss averaged over last 500 steps = 3.3369e-01, PNorm = 106.8972, GNorm = 0.2336
Meta loss on this task batch = 3.0804e-01, Meta loss averaged over last 500 steps = 3.3371e-01, PNorm = 106.9060, GNorm = 0.3123
Meta loss on this task batch = 3.4826e-01, Meta loss averaged over last 500 steps = 3.3364e-01, PNorm = 106.9143, GNorm = 0.2485
Meta loss on this task batch = 3.3438e-01, Meta loss averaged over last 500 steps = 3.3367e-01, PNorm = 106.9231, GNorm = 0.2803
Meta loss on this task batch = 3.5531e-01, Meta loss averaged over last 500 steps = 3.3374e-01, PNorm = 106.9310, GNorm = 0.3387
Meta loss on this task batch = 2.9339e-01, Meta loss averaged over last 500 steps = 3.3362e-01, PNorm = 106.9412, GNorm = 0.6297
Took 110.62855410575867 seconds to complete one epoch of meta training
Took 118.44472527503967 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453310
Epoch 271
Meta loss on this task batch = 3.2125e-01, Meta loss averaged over last 500 steps = 3.3360e-01, PNorm = 106.9503, GNorm = 0.2934
Meta loss on this task batch = 3.2173e-01, Meta loss averaged over last 500 steps = 3.3362e-01, PNorm = 106.9599, GNorm = 0.3966
Meta loss on this task batch = 2.9052e-01, Meta loss averaged over last 500 steps = 3.3341e-01, PNorm = 106.9700, GNorm = 0.3101
Meta loss on this task batch = 3.1324e-01, Meta loss averaged over last 500 steps = 3.3333e-01, PNorm = 106.9803, GNorm = 0.2917
Meta loss on this task batch = 3.8908e-01, Meta loss averaged over last 500 steps = 3.3348e-01, PNorm = 106.9905, GNorm = 0.3354
Meta loss on this task batch = 2.7605e-01, Meta loss averaged over last 500 steps = 3.3344e-01, PNorm = 106.9998, GNorm = 0.2366
Meta loss on this task batch = 3.8766e-01, Meta loss averaged over last 500 steps = 3.3353e-01, PNorm = 107.0089, GNorm = 0.2899
Meta loss on this task batch = 3.2492e-01, Meta loss averaged over last 500 steps = 3.3350e-01, PNorm = 107.0175, GNorm = 0.3070
Meta loss on this task batch = 3.3945e-01, Meta loss averaged over last 500 steps = 3.3359e-01, PNorm = 107.0260, GNorm = 0.2779
Meta loss on this task batch = 3.2392e-01, Meta loss averaged over last 500 steps = 3.3351e-01, PNorm = 107.0346, GNorm = 0.2400
Meta loss on this task batch = 3.8005e-01, Meta loss averaged over last 500 steps = 3.3363e-01, PNorm = 107.0429, GNorm = 0.2601
Meta loss on this task batch = 3.6022e-01, Meta loss averaged over last 500 steps = 3.3375e-01, PNorm = 107.0503, GNorm = 0.3297
Meta loss on this task batch = 3.2852e-01, Meta loss averaged over last 500 steps = 3.3366e-01, PNorm = 107.0581, GNorm = 0.2897
Meta loss on this task batch = 2.9841e-01, Meta loss averaged over last 500 steps = 3.3345e-01, PNorm = 107.0659, GNorm = 0.3703
Meta loss on this task batch = 3.0344e-01, Meta loss averaged over last 500 steps = 3.3341e-01, PNorm = 107.0741, GNorm = 0.2075
Meta loss on this task batch = 3.5787e-01, Meta loss averaged over last 500 steps = 3.3343e-01, PNorm = 107.0817, GNorm = 0.4353
Meta loss on this task batch = 3.3987e-01, Meta loss averaged over last 500 steps = 3.3353e-01, PNorm = 107.0891, GNorm = 0.3864
Meta loss on this task batch = 3.0306e-01, Meta loss averaged over last 500 steps = 3.3349e-01, PNorm = 107.0967, GNorm = 0.2781
Meta loss on this task batch = 3.3034e-01, Meta loss averaged over last 500 steps = 3.3352e-01, PNorm = 107.1038, GNorm = 0.4013
Took 110.83277988433838 seconds to complete one epoch of meta training
Took 118.44372296333313 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459722
Epoch 272
Meta loss on this task batch = 3.2918e-01, Meta loss averaged over last 500 steps = 3.3344e-01, PNorm = 107.1108, GNorm = 0.2675
Meta loss on this task batch = 3.2085e-01, Meta loss averaged over last 500 steps = 3.3334e-01, PNorm = 107.1177, GNorm = 0.2775
Meta loss on this task batch = 3.1062e-01, Meta loss averaged over last 500 steps = 3.3335e-01, PNorm = 107.1248, GNorm = 0.2892
Meta loss on this task batch = 3.4163e-01, Meta loss averaged over last 500 steps = 3.3329e-01, PNorm = 107.1319, GNorm = 0.3276
Meta loss on this task batch = 3.4203e-01, Meta loss averaged over last 500 steps = 3.3327e-01, PNorm = 107.1392, GNorm = 0.2550
Meta loss on this task batch = 2.8771e-01, Meta loss averaged over last 500 steps = 3.3330e-01, PNorm = 107.1466, GNorm = 0.2358
Meta loss on this task batch = 3.9497e-01, Meta loss averaged over last 500 steps = 3.3351e-01, PNorm = 107.1543, GNorm = 0.2897
Meta loss on this task batch = 3.2628e-01, Meta loss averaged over last 500 steps = 3.3355e-01, PNorm = 107.1617, GNorm = 0.3638
Meta loss on this task batch = 2.8721e-01, Meta loss averaged over last 500 steps = 3.3347e-01, PNorm = 107.1699, GNorm = 0.3208
Meta loss on this task batch = 2.9510e-01, Meta loss averaged over last 500 steps = 3.3344e-01, PNorm = 107.1776, GNorm = 0.2296
Meta loss on this task batch = 3.5196e-01, Meta loss averaged over last 500 steps = 3.3339e-01, PNorm = 107.1844, GNorm = 0.3645
Meta loss on this task batch = 2.9387e-01, Meta loss averaged over last 500 steps = 3.3324e-01, PNorm = 107.1917, GNorm = 0.2549
Meta loss on this task batch = 4.0160e-01, Meta loss averaged over last 500 steps = 3.3334e-01, PNorm = 107.1982, GNorm = 0.4381
Meta loss on this task batch = 3.1602e-01, Meta loss averaged over last 500 steps = 3.3335e-01, PNorm = 107.2045, GNorm = 0.2520
Meta loss on this task batch = 3.6635e-01, Meta loss averaged over last 500 steps = 3.3340e-01, PNorm = 107.2102, GNorm = 0.2925
Meta loss on this task batch = 2.8910e-01, Meta loss averaged over last 500 steps = 3.3331e-01, PNorm = 107.2161, GNorm = 0.2581
Meta loss on this task batch = 3.0598e-01, Meta loss averaged over last 500 steps = 3.3334e-01, PNorm = 107.2219, GNorm = 0.2255
Meta loss on this task batch = 2.8968e-01, Meta loss averaged over last 500 steps = 3.3327e-01, PNorm = 107.2280, GNorm = 0.2332
Meta loss on this task batch = 3.1689e-01, Meta loss averaged over last 500 steps = 3.3322e-01, PNorm = 107.2342, GNorm = 0.4001
Took 111.19776511192322 seconds to complete one epoch of meta training
Took 119.01356315612793 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479236
Epoch 273
Meta loss on this task batch = 2.7301e-01, Meta loss averaged over last 500 steps = 3.3311e-01, PNorm = 107.2409, GNorm = 0.2724
Meta loss on this task batch = 3.1485e-01, Meta loss averaged over last 500 steps = 3.3307e-01, PNorm = 107.2476, GNorm = 0.2865
Meta loss on this task batch = 3.6245e-01, Meta loss averaged over last 500 steps = 3.3301e-01, PNorm = 107.2551, GNorm = 0.3717
Meta loss on this task batch = 2.8826e-01, Meta loss averaged over last 500 steps = 3.3293e-01, PNorm = 107.2627, GNorm = 0.2303
Meta loss on this task batch = 3.3530e-01, Meta loss averaged over last 500 steps = 3.3294e-01, PNorm = 107.2698, GNorm = 0.2527
Meta loss on this task batch = 3.2145e-01, Meta loss averaged over last 500 steps = 3.3280e-01, PNorm = 107.2770, GNorm = 0.3350
Meta loss on this task batch = 3.0732e-01, Meta loss averaged over last 500 steps = 3.3274e-01, PNorm = 107.2845, GNorm = 0.3450
Meta loss on this task batch = 3.6087e-01, Meta loss averaged over last 500 steps = 3.3280e-01, PNorm = 107.2917, GNorm = 0.3422
Meta loss on this task batch = 3.4268e-01, Meta loss averaged over last 500 steps = 3.3279e-01, PNorm = 107.2982, GNorm = 0.2740
Meta loss on this task batch = 3.0633e-01, Meta loss averaged over last 500 steps = 3.3268e-01, PNorm = 107.3060, GNorm = 0.3978
Meta loss on this task batch = 3.9637e-01, Meta loss averaged over last 500 steps = 3.3293e-01, PNorm = 107.3132, GNorm = 0.3291
Meta loss on this task batch = 3.0430e-01, Meta loss averaged over last 500 steps = 3.3285e-01, PNorm = 107.3207, GNorm = 0.2623
Meta loss on this task batch = 2.6879e-01, Meta loss averaged over last 500 steps = 3.3272e-01, PNorm = 107.3287, GNorm = 0.2611
Meta loss on this task batch = 3.2774e-01, Meta loss averaged over last 500 steps = 3.3266e-01, PNorm = 107.3376, GNorm = 0.3110
Meta loss on this task batch = 3.0946e-01, Meta loss averaged over last 500 steps = 3.3257e-01, PNorm = 107.3464, GNorm = 0.2688
Meta loss on this task batch = 3.5047e-01, Meta loss averaged over last 500 steps = 3.3267e-01, PNorm = 107.3547, GNorm = 0.3559
Meta loss on this task batch = 3.4126e-01, Meta loss averaged over last 500 steps = 3.3268e-01, PNorm = 107.3625, GNorm = 0.3417
Meta loss on this task batch = 2.9276e-01, Meta loss averaged over last 500 steps = 3.3263e-01, PNorm = 107.3706, GNorm = 0.2862
Meta loss on this task batch = 3.3855e-01, Meta loss averaged over last 500 steps = 3.3260e-01, PNorm = 107.3779, GNorm = 0.4871
Took 112.401211977005 seconds to complete one epoch of meta training
Took 119.67697596549988 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484677
Epoch 274
Meta loss on this task batch = 2.7619e-01, Meta loss averaged over last 500 steps = 3.3252e-01, PNorm = 107.3851, GNorm = 0.2993
Meta loss on this task batch = 3.2332e-01, Meta loss averaged over last 500 steps = 3.3245e-01, PNorm = 107.3923, GNorm = 0.2669
Meta loss on this task batch = 3.2700e-01, Meta loss averaged over last 500 steps = 3.3236e-01, PNorm = 107.3989, GNorm = 0.4122
Meta loss on this task batch = 3.6442e-01, Meta loss averaged over last 500 steps = 3.3250e-01, PNorm = 107.4045, GNorm = 0.3275
Meta loss on this task batch = 3.1845e-01, Meta loss averaged over last 500 steps = 3.3238e-01, PNorm = 107.4101, GNorm = 0.3655
Meta loss on this task batch = 3.0662e-01, Meta loss averaged over last 500 steps = 3.3233e-01, PNorm = 107.4159, GNorm = 0.2919
Meta loss on this task batch = 3.6250e-01, Meta loss averaged over last 500 steps = 3.3229e-01, PNorm = 107.4215, GNorm = 0.4456
Meta loss on this task batch = 3.4728e-01, Meta loss averaged over last 500 steps = 3.3220e-01, PNorm = 107.4281, GNorm = 0.2406
Meta loss on this task batch = 3.3351e-01, Meta loss averaged over last 500 steps = 3.3221e-01, PNorm = 107.4349, GNorm = 0.3967
Meta loss on this task batch = 3.1113e-01, Meta loss averaged over last 500 steps = 3.3213e-01, PNorm = 107.4432, GNorm = 0.3143
Meta loss on this task batch = 3.0600e-01, Meta loss averaged over last 500 steps = 3.3209e-01, PNorm = 107.4517, GNorm = 0.2814
Meta loss on this task batch = 3.7010e-01, Meta loss averaged over last 500 steps = 3.3212e-01, PNorm = 107.4606, GNorm = 0.2753
Meta loss on this task batch = 2.9728e-01, Meta loss averaged over last 500 steps = 3.3207e-01, PNorm = 107.4700, GNorm = 0.2628
Meta loss on this task batch = 3.0959e-01, Meta loss averaged over last 500 steps = 3.3205e-01, PNorm = 107.4798, GNorm = 0.2854
Meta loss on this task batch = 3.1165e-01, Meta loss averaged over last 500 steps = 3.3198e-01, PNorm = 107.4898, GNorm = 0.2731
Meta loss on this task batch = 3.1798e-01, Meta loss averaged over last 500 steps = 3.3196e-01, PNorm = 107.4993, GNorm = 0.2657
Meta loss on this task batch = 3.1034e-01, Meta loss averaged over last 500 steps = 3.3191e-01, PNorm = 107.5089, GNorm = 0.2378
Meta loss on this task batch = 3.4424e-01, Meta loss averaged over last 500 steps = 3.3195e-01, PNorm = 107.5181, GNorm = 0.3097
Meta loss on this task batch = 3.1699e-01, Meta loss averaged over last 500 steps = 3.3187e-01, PNorm = 107.5273, GNorm = 0.4474
Took 114.44584679603577 seconds to complete one epoch of meta training
Took 122.48057126998901 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472611
Epoch 275
Meta loss on this task batch = 3.0512e-01, Meta loss averaged over last 500 steps = 3.3191e-01, PNorm = 107.5373, GNorm = 0.3159
Meta loss on this task batch = 3.4202e-01, Meta loss averaged over last 500 steps = 3.3196e-01, PNorm = 107.5469, GNorm = 0.2763
Meta loss on this task batch = 3.2434e-01, Meta loss averaged over last 500 steps = 3.3188e-01, PNorm = 107.5566, GNorm = 0.4031
Meta loss on this task batch = 3.5373e-01, Meta loss averaged over last 500 steps = 3.3198e-01, PNorm = 107.5650, GNorm = 0.3194
Meta loss on this task batch = 3.4079e-01, Meta loss averaged over last 500 steps = 3.3200e-01, PNorm = 107.5734, GNorm = 0.3076
Meta loss on this task batch = 3.1000e-01, Meta loss averaged over last 500 steps = 3.3189e-01, PNorm = 107.5812, GNorm = 0.2704
Meta loss on this task batch = 3.4198e-01, Meta loss averaged over last 500 steps = 3.3193e-01, PNorm = 107.5883, GNorm = 0.3324
Meta loss on this task batch = 2.7042e-01, Meta loss averaged over last 500 steps = 3.3166e-01, PNorm = 107.5954, GNorm = 0.2481
Meta loss on this task batch = 3.1715e-01, Meta loss averaged over last 500 steps = 3.3168e-01, PNorm = 107.6033, GNorm = 0.5091
Meta loss on this task batch = 3.3850e-01, Meta loss averaged over last 500 steps = 3.3172e-01, PNorm = 107.6106, GNorm = 0.3356
Meta loss on this task batch = 2.9381e-01, Meta loss averaged over last 500 steps = 3.3162e-01, PNorm = 107.6183, GNorm = 0.2930
Meta loss on this task batch = 3.4189e-01, Meta loss averaged over last 500 steps = 3.3155e-01, PNorm = 107.6262, GNorm = 0.3861
Meta loss on this task batch = 3.0628e-01, Meta loss averaged over last 500 steps = 3.3152e-01, PNorm = 107.6335, GNorm = 0.2747
Meta loss on this task batch = 3.6840e-01, Meta loss averaged over last 500 steps = 3.3159e-01, PNorm = 107.6406, GNorm = 0.2861
Meta loss on this task batch = 3.7459e-01, Meta loss averaged over last 500 steps = 3.3162e-01, PNorm = 107.6466, GNorm = 0.3286
Meta loss on this task batch = 3.0296e-01, Meta loss averaged over last 500 steps = 3.3169e-01, PNorm = 107.6522, GNorm = 0.2460
Meta loss on this task batch = 3.5430e-01, Meta loss averaged over last 500 steps = 3.3171e-01, PNorm = 107.6582, GNorm = 0.3176
Meta loss on this task batch = 2.8339e-01, Meta loss averaged over last 500 steps = 3.3155e-01, PNorm = 107.6659, GNorm = 0.2808
Meta loss on this task batch = 3.1008e-01, Meta loss averaged over last 500 steps = 3.3151e-01, PNorm = 107.6743, GNorm = 0.6974
Took 112.12028050422668 seconds to complete one epoch of meta training
Took 118.85064101219177 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460171
Epoch 276
Meta loss on this task batch = 3.1339e-01, Meta loss averaged over last 500 steps = 3.3142e-01, PNorm = 107.6838, GNorm = 0.1963
Meta loss on this task batch = 2.9598e-01, Meta loss averaged over last 500 steps = 3.3135e-01, PNorm = 107.6943, GNorm = 0.2075
Meta loss on this task batch = 3.1795e-01, Meta loss averaged over last 500 steps = 3.3125e-01, PNorm = 107.7046, GNorm = 0.2862
Meta loss on this task batch = 3.3533e-01, Meta loss averaged over last 500 steps = 3.3126e-01, PNorm = 107.7137, GNorm = 0.3073
Meta loss on this task batch = 3.4899e-01, Meta loss averaged over last 500 steps = 3.3128e-01, PNorm = 107.7230, GNorm = 0.2786
Meta loss on this task batch = 3.5380e-01, Meta loss averaged over last 500 steps = 3.3120e-01, PNorm = 107.7320, GNorm = 0.3339
Meta loss on this task batch = 4.1003e-01, Meta loss averaged over last 500 steps = 3.3131e-01, PNorm = 107.7393, GNorm = 0.3804
Meta loss on this task batch = 2.8002e-01, Meta loss averaged over last 500 steps = 3.3119e-01, PNorm = 107.7471, GNorm = 0.2253
Meta loss on this task batch = 3.3162e-01, Meta loss averaged over last 500 steps = 3.3115e-01, PNorm = 107.7548, GNorm = 0.2875
Meta loss on this task batch = 2.8897e-01, Meta loss averaged over last 500 steps = 3.3090e-01, PNorm = 107.7629, GNorm = 0.2500
Meta loss on this task batch = 3.7347e-01, Meta loss averaged over last 500 steps = 3.3103e-01, PNorm = 107.7705, GNorm = 0.3609
Meta loss on this task batch = 3.6552e-01, Meta loss averaged over last 500 steps = 3.3111e-01, PNorm = 107.7779, GNorm = 0.4811
Meta loss on this task batch = 3.1557e-01, Meta loss averaged over last 500 steps = 3.3108e-01, PNorm = 107.7848, GNorm = 0.2418
Meta loss on this task batch = 3.4149e-01, Meta loss averaged over last 500 steps = 3.3106e-01, PNorm = 107.7916, GNorm = 0.2781
Meta loss on this task batch = 3.2753e-01, Meta loss averaged over last 500 steps = 3.3105e-01, PNorm = 107.7986, GNorm = 0.2350
Meta loss on this task batch = 3.0616e-01, Meta loss averaged over last 500 steps = 3.3110e-01, PNorm = 107.8055, GNorm = 0.3233
Meta loss on this task batch = 3.3583e-01, Meta loss averaged over last 500 steps = 3.3111e-01, PNorm = 107.8120, GNorm = 0.2532
Meta loss on this task batch = 3.3697e-01, Meta loss averaged over last 500 steps = 3.3112e-01, PNorm = 107.8192, GNorm = 0.2868
Meta loss on this task batch = 2.6391e-01, Meta loss averaged over last 500 steps = 3.3101e-01, PNorm = 107.8263, GNorm = 0.2718
Took 111.68419551849365 seconds to complete one epoch of meta training
Took 119.50594067573547 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452648
Epoch 277
Meta loss on this task batch = 3.6117e-01, Meta loss averaged over last 500 steps = 3.3099e-01, PNorm = 107.8333, GNorm = 0.3061
Meta loss on this task batch = 3.3438e-01, Meta loss averaged over last 500 steps = 3.3098e-01, PNorm = 107.8401, GNorm = 0.2458
Meta loss on this task batch = 2.6139e-01, Meta loss averaged over last 500 steps = 3.3072e-01, PNorm = 107.8474, GNorm = 0.2883
Meta loss on this task batch = 3.3345e-01, Meta loss averaged over last 500 steps = 3.3067e-01, PNorm = 107.8543, GNorm = 0.4011
Meta loss on this task batch = 3.2101e-01, Meta loss averaged over last 500 steps = 3.3057e-01, PNorm = 107.8615, GNorm = 0.3369
Meta loss on this task batch = 3.4424e-01, Meta loss averaged over last 500 steps = 3.3062e-01, PNorm = 107.8681, GNorm = 0.3718
Meta loss on this task batch = 3.4187e-01, Meta loss averaged over last 500 steps = 3.3069e-01, PNorm = 107.8753, GNorm = 0.2565
Meta loss on this task batch = 3.2687e-01, Meta loss averaged over last 500 steps = 3.3073e-01, PNorm = 107.8829, GNorm = 0.3271
Meta loss on this task batch = 3.1219e-01, Meta loss averaged over last 500 steps = 3.3068e-01, PNorm = 107.8905, GNorm = 0.3209
Meta loss on this task batch = 3.3151e-01, Meta loss averaged over last 500 steps = 3.3067e-01, PNorm = 107.8977, GNorm = 0.2682
Meta loss on this task batch = 3.1563e-01, Meta loss averaged over last 500 steps = 3.3064e-01, PNorm = 107.9059, GNorm = 0.3011
Meta loss on this task batch = 3.2823e-01, Meta loss averaged over last 500 steps = 3.3064e-01, PNorm = 107.9140, GNorm = 0.2697
Meta loss on this task batch = 4.2157e-01, Meta loss averaged over last 500 steps = 3.3081e-01, PNorm = 107.9217, GNorm = 0.3540
Meta loss on this task batch = 3.5650e-01, Meta loss averaged over last 500 steps = 3.3084e-01, PNorm = 107.9302, GNorm = 0.2615
Meta loss on this task batch = 3.3889e-01, Meta loss averaged over last 500 steps = 3.3082e-01, PNorm = 107.9392, GNorm = 0.2718
Meta loss on this task batch = 3.4358e-01, Meta loss averaged over last 500 steps = 3.3074e-01, PNorm = 107.9470, GNorm = 0.3101
Meta loss on this task batch = 2.9039e-01, Meta loss averaged over last 500 steps = 3.3061e-01, PNorm = 107.9545, GNorm = 0.2506
Meta loss on this task batch = 3.1676e-01, Meta loss averaged over last 500 steps = 3.3069e-01, PNorm = 107.9619, GNorm = 0.4685
Meta loss on this task batch = 2.5886e-01, Meta loss averaged over last 500 steps = 3.3047e-01, PNorm = 107.9704, GNorm = 0.3235
Took 110.88926959037781 seconds to complete one epoch of meta training
Took 118.72667288780212 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444434
Epoch 278
Meta loss on this task batch = 3.0784e-01, Meta loss averaged over last 500 steps = 3.3044e-01, PNorm = 107.9787, GNorm = 0.3138
Meta loss on this task batch = 3.0292e-01, Meta loss averaged over last 500 steps = 3.3034e-01, PNorm = 107.9876, GNorm = 0.2484
Meta loss on this task batch = 3.3220e-01, Meta loss averaged over last 500 steps = 3.3036e-01, PNorm = 107.9963, GNorm = 0.2555
Meta loss on this task batch = 3.3174e-01, Meta loss averaged over last 500 steps = 3.3033e-01, PNorm = 108.0051, GNorm = 0.2495
Meta loss on this task batch = 4.2133e-01, Meta loss averaged over last 500 steps = 3.3051e-01, PNorm = 108.0114, GNorm = 0.3546
Meta loss on this task batch = 3.0428e-01, Meta loss averaged over last 500 steps = 3.3035e-01, PNorm = 108.0178, GNorm = 0.4768
Meta loss on this task batch = 3.0209e-01, Meta loss averaged over last 500 steps = 3.3026e-01, PNorm = 108.0242, GNorm = 0.2273
Meta loss on this task batch = 2.7517e-01, Meta loss averaged over last 500 steps = 3.3009e-01, PNorm = 108.0316, GNorm = 0.2954
Meta loss on this task batch = 3.5217e-01, Meta loss averaged over last 500 steps = 3.3024e-01, PNorm = 108.0380, GNorm = 0.2714
Meta loss on this task batch = 3.5886e-01, Meta loss averaged over last 500 steps = 3.3028e-01, PNorm = 108.0438, GNorm = 0.3349
Meta loss on this task batch = 3.5565e-01, Meta loss averaged over last 500 steps = 3.3028e-01, PNorm = 108.0487, GNorm = 0.4004
Meta loss on this task batch = 2.6370e-01, Meta loss averaged over last 500 steps = 3.3019e-01, PNorm = 108.0536, GNorm = 0.3670
Meta loss on this task batch = 2.9985e-01, Meta loss averaged over last 500 steps = 3.3008e-01, PNorm = 108.0588, GNorm = 0.2085
Meta loss on this task batch = 3.1318e-01, Meta loss averaged over last 500 steps = 3.3017e-01, PNorm = 108.0650, GNorm = 0.3131
Meta loss on this task batch = 3.5631e-01, Meta loss averaged over last 500 steps = 3.3011e-01, PNorm = 108.0708, GNorm = 0.2769
Meta loss on this task batch = 3.4173e-01, Meta loss averaged over last 500 steps = 3.3014e-01, PNorm = 108.0772, GNorm = 0.3610
Meta loss on this task batch = 3.5756e-01, Meta loss averaged over last 500 steps = 3.3030e-01, PNorm = 108.0835, GNorm = 0.2603
Meta loss on this task batch = 4.1586e-01, Meta loss averaged over last 500 steps = 3.3032e-01, PNorm = 108.0904, GNorm = 0.4503
Meta loss on this task batch = 3.2526e-01, Meta loss averaged over last 500 steps = 3.3031e-01, PNorm = 108.0986, GNorm = 0.4010
Took 114.40434098243713 seconds to complete one epoch of meta training
Took 122.30307292938232 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.442392
Epoch 279
Meta loss on this task batch = 3.1825e-01, Meta loss averaged over last 500 steps = 3.3031e-01, PNorm = 108.1074, GNorm = 0.2614
Meta loss on this task batch = 2.6731e-01, Meta loss averaged over last 500 steps = 3.3019e-01, PNorm = 108.1177, GNorm = 0.3417
Meta loss on this task batch = 2.9663e-01, Meta loss averaged over last 500 steps = 3.2998e-01, PNorm = 108.1279, GNorm = 0.3078
Meta loss on this task batch = 3.1769e-01, Meta loss averaged over last 500 steps = 3.2988e-01, PNorm = 108.1376, GNorm = 0.2617
Meta loss on this task batch = 3.1817e-01, Meta loss averaged over last 500 steps = 3.2989e-01, PNorm = 108.1468, GNorm = 0.3478
Meta loss on this task batch = 3.7280e-01, Meta loss averaged over last 500 steps = 3.2996e-01, PNorm = 108.1546, GNorm = 0.3771
Meta loss on this task batch = 4.0332e-01, Meta loss averaged over last 500 steps = 3.3014e-01, PNorm = 108.1619, GNorm = 0.3707
Meta loss on this task batch = 3.0241e-01, Meta loss averaged over last 500 steps = 3.3007e-01, PNorm = 108.1695, GNorm = 0.2455
Meta loss on this task batch = 3.2173e-01, Meta loss averaged over last 500 steps = 3.3015e-01, PNorm = 108.1770, GNorm = 0.2584
Meta loss on this task batch = 3.4855e-01, Meta loss averaged over last 500 steps = 3.3018e-01, PNorm = 108.1839, GNorm = 0.2832
Meta loss on this task batch = 2.8064e-01, Meta loss averaged over last 500 steps = 3.3006e-01, PNorm = 108.1919, GNorm = 0.3047
Meta loss on this task batch = 3.8663e-01, Meta loss averaged over last 500 steps = 3.3008e-01, PNorm = 108.1993, GNorm = 0.3614
Meta loss on this task batch = 3.3837e-01, Meta loss averaged over last 500 steps = 3.3009e-01, PNorm = 108.2063, GNorm = 0.3048
Meta loss on this task batch = 3.3379e-01, Meta loss averaged over last 500 steps = 3.2999e-01, PNorm = 108.2115, GNorm = 0.5213
Meta loss on this task batch = 3.0290e-01, Meta loss averaged over last 500 steps = 3.2985e-01, PNorm = 108.2169, GNorm = 0.3615
Meta loss on this task batch = 3.6691e-01, Meta loss averaged over last 500 steps = 3.2999e-01, PNorm = 108.2225, GNorm = 0.4815
Meta loss on this task batch = 2.8411e-01, Meta loss averaged over last 500 steps = 3.2983e-01, PNorm = 108.2286, GNorm = 0.3945
Meta loss on this task batch = 3.4672e-01, Meta loss averaged over last 500 steps = 3.2982e-01, PNorm = 108.2356, GNorm = 0.2410
Meta loss on this task batch = 3.9040e-01, Meta loss averaged over last 500 steps = 3.2996e-01, PNorm = 108.2412, GNorm = 0.3717
Took 112.3869981765747 seconds to complete one epoch of meta training
Took 120.24679279327393 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470784
Epoch 280
Meta loss on this task batch = 3.0199e-01, Meta loss averaged over last 500 steps = 3.2982e-01, PNorm = 108.2478, GNorm = 0.3633
Meta loss on this task batch = 2.9707e-01, Meta loss averaged over last 500 steps = 3.2973e-01, PNorm = 108.2546, GNorm = 0.3474
Meta loss on this task batch = 3.2960e-01, Meta loss averaged over last 500 steps = 3.2974e-01, PNorm = 108.2616, GNorm = 0.2608
Meta loss on this task batch = 3.3633e-01, Meta loss averaged over last 500 steps = 3.2977e-01, PNorm = 108.2682, GNorm = 0.2832
Meta loss on this task batch = 2.8602e-01, Meta loss averaged over last 500 steps = 3.2974e-01, PNorm = 108.2755, GNorm = 0.2466
Meta loss on this task batch = 3.5076e-01, Meta loss averaged over last 500 steps = 3.2980e-01, PNorm = 108.2826, GNorm = 0.2183
Meta loss on this task batch = 3.0662e-01, Meta loss averaged over last 500 steps = 3.2988e-01, PNorm = 108.2902, GNorm = 0.2578
Meta loss on this task batch = 3.3816e-01, Meta loss averaged over last 500 steps = 3.2987e-01, PNorm = 108.2975, GNorm = 0.3063
Meta loss on this task batch = 3.1428e-01, Meta loss averaged over last 500 steps = 3.2979e-01, PNorm = 108.3051, GNorm = 0.2634
Meta loss on this task batch = 4.1426e-01, Meta loss averaged over last 500 steps = 3.3002e-01, PNorm = 108.3120, GNorm = 0.3139
Meta loss on this task batch = 3.5374e-01, Meta loss averaged over last 500 steps = 3.3005e-01, PNorm = 108.3191, GNorm = 0.2490
Meta loss on this task batch = 3.7485e-01, Meta loss averaged over last 500 steps = 3.3015e-01, PNorm = 108.3253, GNorm = 0.2683
Meta loss on this task batch = 3.2609e-01, Meta loss averaged over last 500 steps = 3.3016e-01, PNorm = 108.3329, GNorm = 0.3490
Meta loss on this task batch = 3.5948e-01, Meta loss averaged over last 500 steps = 3.3021e-01, PNorm = 108.3404, GNorm = 0.2415
Meta loss on this task batch = 3.7275e-01, Meta loss averaged over last 500 steps = 3.3027e-01, PNorm = 108.3478, GNorm = 0.3443
Meta loss on this task batch = 3.3015e-01, Meta loss averaged over last 500 steps = 3.3026e-01, PNorm = 108.3563, GNorm = 0.2814
Meta loss on this task batch = 3.2804e-01, Meta loss averaged over last 500 steps = 3.3020e-01, PNorm = 108.3653, GNorm = 0.3079
Meta loss on this task batch = 3.4756e-01, Meta loss averaged over last 500 steps = 3.3032e-01, PNorm = 108.3748, GNorm = 0.3193
Meta loss on this task batch = 3.2603e-01, Meta loss averaged over last 500 steps = 3.3036e-01, PNorm = 108.3843, GNorm = 0.2924
Took 109.65999174118042 seconds to complete one epoch of meta training
Took 117.69855213165283 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466497
Epoch 281
Meta loss on this task batch = 3.6684e-01, Meta loss averaged over last 500 steps = 3.3043e-01, PNorm = 108.3933, GNorm = 0.2752
Meta loss on this task batch = 2.9046e-01, Meta loss averaged over last 500 steps = 3.3036e-01, PNorm = 108.4027, GNorm = 0.2380
Meta loss on this task batch = 2.8192e-01, Meta loss averaged over last 500 steps = 3.3023e-01, PNorm = 108.4126, GNorm = 0.3087
Meta loss on this task batch = 3.5287e-01, Meta loss averaged over last 500 steps = 3.3026e-01, PNorm = 108.4220, GNorm = 0.3357
Meta loss on this task batch = 2.8906e-01, Meta loss averaged over last 500 steps = 3.3011e-01, PNorm = 108.4311, GNorm = 0.4119
Meta loss on this task batch = 3.3817e-01, Meta loss averaged over last 500 steps = 3.3013e-01, PNorm = 108.4390, GNorm = 0.4078
Meta loss on this task batch = 3.8316e-01, Meta loss averaged over last 500 steps = 3.3025e-01, PNorm = 108.4449, GNorm = 0.4506
Meta loss on this task batch = 2.7198e-01, Meta loss averaged over last 500 steps = 3.3012e-01, PNorm = 108.4517, GNorm = 0.2152
Meta loss on this task batch = 2.7528e-01, Meta loss averaged over last 500 steps = 3.2988e-01, PNorm = 108.4590, GNorm = 0.2805
Meta loss on this task batch = 3.2375e-01, Meta loss averaged over last 500 steps = 3.2981e-01, PNorm = 108.4668, GNorm = 0.3175
Meta loss on this task batch = 3.2979e-01, Meta loss averaged over last 500 steps = 3.2983e-01, PNorm = 108.4741, GNorm = 0.2790
Meta loss on this task batch = 3.1729e-01, Meta loss averaged over last 500 steps = 3.2984e-01, PNorm = 108.4813, GNorm = 0.2511
Meta loss on this task batch = 3.2543e-01, Meta loss averaged over last 500 steps = 3.2976e-01, PNorm = 108.4884, GNorm = 0.3382
Meta loss on this task batch = 3.3270e-01, Meta loss averaged over last 500 steps = 3.2982e-01, PNorm = 108.4952, GNorm = 0.2757
Meta loss on this task batch = 3.6569e-01, Meta loss averaged over last 500 steps = 3.2991e-01, PNorm = 108.5017, GNorm = 0.2592
Meta loss on this task batch = 3.1880e-01, Meta loss averaged over last 500 steps = 3.2987e-01, PNorm = 108.5081, GNorm = 0.2616
Meta loss on this task batch = 3.5582e-01, Meta loss averaged over last 500 steps = 3.2995e-01, PNorm = 108.5133, GNorm = 0.3558
Meta loss on this task batch = 3.5175e-01, Meta loss averaged over last 500 steps = 3.3002e-01, PNorm = 108.5183, GNorm = 0.2768
Meta loss on this task batch = 3.4166e-01, Meta loss averaged over last 500 steps = 3.3010e-01, PNorm = 108.5231, GNorm = 0.2873
Took 110.87353134155273 seconds to complete one epoch of meta training
Took 117.34402656555176 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491085
Epoch 282
Meta loss on this task batch = 3.2100e-01, Meta loss averaged over last 500 steps = 3.3002e-01, PNorm = 108.5282, GNorm = 0.2720
Meta loss on this task batch = 3.2062e-01, Meta loss averaged over last 500 steps = 3.3001e-01, PNorm = 108.5339, GNorm = 0.2984
Meta loss on this task batch = 2.9241e-01, Meta loss averaged over last 500 steps = 3.3001e-01, PNorm = 108.5402, GNorm = 0.2239
Meta loss on this task batch = 2.9876e-01, Meta loss averaged over last 500 steps = 3.2997e-01, PNorm = 108.5475, GNorm = 0.3072
Meta loss on this task batch = 3.0600e-01, Meta loss averaged over last 500 steps = 3.2996e-01, PNorm = 108.5547, GNorm = 0.3257
Meta loss on this task batch = 3.1744e-01, Meta loss averaged over last 500 steps = 3.3012e-01, PNorm = 108.5623, GNorm = 0.2718
Meta loss on this task batch = 3.3176e-01, Meta loss averaged over last 500 steps = 3.3012e-01, PNorm = 108.5699, GNorm = 0.4521
Meta loss on this task batch = 3.3543e-01, Meta loss averaged over last 500 steps = 3.3022e-01, PNorm = 108.5776, GNorm = 0.2832
Meta loss on this task batch = 2.7674e-01, Meta loss averaged over last 500 steps = 3.3012e-01, PNorm = 108.5859, GNorm = 0.3156
Meta loss on this task batch = 3.4483e-01, Meta loss averaged over last 500 steps = 3.3017e-01, PNorm = 108.5941, GNorm = 0.2883
Meta loss on this task batch = 3.0838e-01, Meta loss averaged over last 500 steps = 3.3021e-01, PNorm = 108.6028, GNorm = 0.2904
Meta loss on this task batch = 3.4038e-01, Meta loss averaged over last 500 steps = 3.3033e-01, PNorm = 108.6119, GNorm = 0.3782
Meta loss on this task batch = 2.9446e-01, Meta loss averaged over last 500 steps = 3.3029e-01, PNorm = 108.6206, GNorm = 0.2705
Meta loss on this task batch = 3.1162e-01, Meta loss averaged over last 500 steps = 3.3016e-01, PNorm = 108.6290, GNorm = 0.2829
Meta loss on this task batch = 2.9527e-01, Meta loss averaged over last 500 steps = 3.3001e-01, PNorm = 108.6382, GNorm = 0.2532
Meta loss on this task batch = 3.2876e-01, Meta loss averaged over last 500 steps = 3.2998e-01, PNorm = 108.6478, GNorm = 0.2729
Meta loss on this task batch = 3.2251e-01, Meta loss averaged over last 500 steps = 3.2993e-01, PNorm = 108.6559, GNorm = 0.3164
Meta loss on this task batch = 3.5371e-01, Meta loss averaged over last 500 steps = 3.2999e-01, PNorm = 108.6631, GNorm = 0.3032
Meta loss on this task batch = 3.1767e-01, Meta loss averaged over last 500 steps = 3.3001e-01, PNorm = 108.6709, GNorm = 0.3429
Took 111.10045027732849 seconds to complete one epoch of meta training
Took 118.90699434280396 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497950
Epoch 283
Meta loss on this task batch = 3.5745e-01, Meta loss averaged over last 500 steps = 3.3006e-01, PNorm = 108.6778, GNorm = 0.2835
Meta loss on this task batch = 3.1845e-01, Meta loss averaged over last 500 steps = 3.2999e-01, PNorm = 108.6855, GNorm = 0.3391
Meta loss on this task batch = 3.3248e-01, Meta loss averaged over last 500 steps = 3.2993e-01, PNorm = 108.6936, GNorm = 0.2343
Meta loss on this task batch = 3.2596e-01, Meta loss averaged over last 500 steps = 3.2988e-01, PNorm = 108.7012, GNorm = 0.3313
Meta loss on this task batch = 2.9947e-01, Meta loss averaged over last 500 steps = 3.2974e-01, PNorm = 108.7090, GNorm = 0.4002
Meta loss on this task batch = 3.5131e-01, Meta loss averaged over last 500 steps = 3.2978e-01, PNorm = 108.7163, GNorm = 0.2626
Meta loss on this task batch = 3.2386e-01, Meta loss averaged over last 500 steps = 3.2980e-01, PNorm = 108.7229, GNorm = 0.3338
Meta loss on this task batch = 3.6133e-01, Meta loss averaged over last 500 steps = 3.2991e-01, PNorm = 108.7300, GNorm = 0.2560
Meta loss on this task batch = 2.9515e-01, Meta loss averaged over last 500 steps = 3.2990e-01, PNorm = 108.7376, GNorm = 0.2693
Meta loss on this task batch = 3.6857e-01, Meta loss averaged over last 500 steps = 3.3003e-01, PNorm = 108.7447, GNorm = 0.3326
Meta loss on this task batch = 3.0933e-01, Meta loss averaged over last 500 steps = 3.3009e-01, PNorm = 108.7523, GNorm = 0.2107
Meta loss on this task batch = 3.6546e-01, Meta loss averaged over last 500 steps = 3.3018e-01, PNorm = 108.7600, GNorm = 0.3263
Meta loss on this task batch = 3.5672e-01, Meta loss averaged over last 500 steps = 3.3011e-01, PNorm = 108.7664, GNorm = 0.4218
Meta loss on this task batch = 3.5001e-01, Meta loss averaged over last 500 steps = 3.3017e-01, PNorm = 108.7720, GNorm = 0.4224
Meta loss on this task batch = 3.4162e-01, Meta loss averaged over last 500 steps = 3.3016e-01, PNorm = 108.7779, GNorm = 0.2546
Meta loss on this task batch = 2.9210e-01, Meta loss averaged over last 500 steps = 3.3015e-01, PNorm = 108.7838, GNorm = 0.2478
Meta loss on this task batch = 3.1000e-01, Meta loss averaged over last 500 steps = 3.3008e-01, PNorm = 108.7910, GNorm = 0.3462
Meta loss on this task batch = 3.6460e-01, Meta loss averaged over last 500 steps = 3.3001e-01, PNorm = 108.7958, GNorm = 0.4166
Meta loss on this task batch = 2.7238e-01, Meta loss averaged over last 500 steps = 3.2981e-01, PNorm = 108.8011, GNorm = 0.2942
Took 112.07152080535889 seconds to complete one epoch of meta training
Took 119.88985562324524 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450127
Epoch 284
Meta loss on this task batch = 3.3835e-01, Meta loss averaged over last 500 steps = 3.2980e-01, PNorm = 108.8066, GNorm = 0.2741
Meta loss on this task batch = 3.5110e-01, Meta loss averaged over last 500 steps = 3.2981e-01, PNorm = 108.8131, GNorm = 0.3930
Meta loss on this task batch = 3.8830e-01, Meta loss averaged over last 500 steps = 3.2986e-01, PNorm = 108.8192, GNorm = 0.3959
Meta loss on this task batch = 2.8476e-01, Meta loss averaged over last 500 steps = 3.2981e-01, PNorm = 108.8260, GNorm = 0.2333
Meta loss on this task batch = 3.8478e-01, Meta loss averaged over last 500 steps = 3.2987e-01, PNorm = 108.8322, GNorm = 0.2830
Meta loss on this task batch = 3.1945e-01, Meta loss averaged over last 500 steps = 3.2988e-01, PNorm = 108.8388, GNorm = 0.3035
Meta loss on this task batch = 3.0427e-01, Meta loss averaged over last 500 steps = 3.2987e-01, PNorm = 108.8457, GNorm = 0.2989
Meta loss on this task batch = 3.7962e-01, Meta loss averaged over last 500 steps = 3.2999e-01, PNorm = 108.8528, GNorm = 0.2763
Meta loss on this task batch = 3.5607e-01, Meta loss averaged over last 500 steps = 3.3002e-01, PNorm = 108.8602, GNorm = 0.3511
Meta loss on this task batch = 4.0800e-01, Meta loss averaged over last 500 steps = 3.3014e-01, PNorm = 108.8683, GNorm = 0.3726
Meta loss on this task batch = 3.4411e-01, Meta loss averaged over last 500 steps = 3.3027e-01, PNorm = 108.8766, GNorm = 0.2473
Meta loss on this task batch = 3.5947e-01, Meta loss averaged over last 500 steps = 3.3033e-01, PNorm = 108.8851, GNorm = 0.2823
Meta loss on this task batch = 3.3445e-01, Meta loss averaged over last 500 steps = 3.3028e-01, PNorm = 108.8937, GNorm = 0.2375
Meta loss on this task batch = 3.3345e-01, Meta loss averaged over last 500 steps = 3.3031e-01, PNorm = 108.9019, GNorm = 0.2517
Meta loss on this task batch = 2.7777e-01, Meta loss averaged over last 500 steps = 3.3024e-01, PNorm = 108.9095, GNorm = 0.2345
Meta loss on this task batch = 2.4862e-01, Meta loss averaged over last 500 steps = 3.2988e-01, PNorm = 108.9181, GNorm = 0.2629
Meta loss on this task batch = 3.0226e-01, Meta loss averaged over last 500 steps = 3.2982e-01, PNorm = 108.9258, GNorm = 0.2437
Meta loss on this task batch = 2.8132e-01, Meta loss averaged over last 500 steps = 3.2984e-01, PNorm = 108.9341, GNorm = 0.2507
Meta loss on this task batch = 3.0861e-01, Meta loss averaged over last 500 steps = 3.2971e-01, PNorm = 108.9429, GNorm = 0.3012
Took 112.35087609291077 seconds to complete one epoch of meta training
Took 119.65422224998474 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475194
Epoch 285
Meta loss on this task batch = 3.4452e-01, Meta loss averaged over last 500 steps = 3.2966e-01, PNorm = 108.9515, GNorm = 0.3176
Meta loss on this task batch = 3.0481e-01, Meta loss averaged over last 500 steps = 3.2959e-01, PNorm = 108.9602, GNorm = 0.3151
Meta loss on this task batch = 3.2018e-01, Meta loss averaged over last 500 steps = 3.2951e-01, PNorm = 108.9690, GNorm = 0.2993
Meta loss on this task batch = 3.5103e-01, Meta loss averaged over last 500 steps = 3.2950e-01, PNorm = 108.9775, GNorm = 0.2816
Meta loss on this task batch = 3.4984e-01, Meta loss averaged over last 500 steps = 3.2965e-01, PNorm = 108.9859, GNorm = 0.3401
Meta loss on this task batch = 3.6545e-01, Meta loss averaged over last 500 steps = 3.2973e-01, PNorm = 108.9941, GNorm = 0.4447
Meta loss on this task batch = 3.2304e-01, Meta loss averaged over last 500 steps = 3.2972e-01, PNorm = 109.0020, GNorm = 0.2748
Meta loss on this task batch = 2.9080e-01, Meta loss averaged over last 500 steps = 3.2967e-01, PNorm = 109.0104, GNorm = 0.2157
Meta loss on this task batch = 3.8429e-01, Meta loss averaged over last 500 steps = 3.2980e-01, PNorm = 109.0185, GNorm = 0.2541
Meta loss on this task batch = 3.5243e-01, Meta loss averaged over last 500 steps = 3.2973e-01, PNorm = 109.0266, GNorm = 0.3268
Meta loss on this task batch = 3.3021e-01, Meta loss averaged over last 500 steps = 3.2962e-01, PNorm = 109.0351, GNorm = 0.4366
Meta loss on this task batch = 3.4748e-01, Meta loss averaged over last 500 steps = 3.2955e-01, PNorm = 109.0440, GNorm = 0.2445
Meta loss on this task batch = 3.2569e-01, Meta loss averaged over last 500 steps = 3.2961e-01, PNorm = 109.0524, GNorm = 0.3182
Meta loss on this task batch = 3.2736e-01, Meta loss averaged over last 500 steps = 3.2962e-01, PNorm = 109.0607, GNorm = 0.2482
Meta loss on this task batch = 3.2403e-01, Meta loss averaged over last 500 steps = 3.2965e-01, PNorm = 109.0693, GNorm = 0.4810
Meta loss on this task batch = 2.8568e-01, Meta loss averaged over last 500 steps = 3.2940e-01, PNorm = 109.0780, GNorm = 0.2286
Meta loss on this task batch = 3.4622e-01, Meta loss averaged over last 500 steps = 3.2940e-01, PNorm = 109.0867, GNorm = 0.3024
Meta loss on this task batch = 2.8513e-01, Meta loss averaged over last 500 steps = 3.2935e-01, PNorm = 109.0949, GNorm = 0.2305
Meta loss on this task batch = 3.1641e-01, Meta loss averaged over last 500 steps = 3.2932e-01, PNorm = 109.1035, GNorm = 0.3420
Took 162.07234907150269 seconds to complete one epoch of meta training
Took 169.1443600654602 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463091
Epoch 286
Meta loss on this task batch = 3.6496e-01, Meta loss averaged over last 500 steps = 3.2939e-01, PNorm = 109.1105, GNorm = 0.2618
Meta loss on this task batch = 3.0463e-01, Meta loss averaged over last 500 steps = 3.2936e-01, PNorm = 109.1174, GNorm = 0.2367
Meta loss on this task batch = 3.3924e-01, Meta loss averaged over last 500 steps = 3.2943e-01, PNorm = 109.1236, GNorm = 0.3148
Meta loss on this task batch = 4.1689e-01, Meta loss averaged over last 500 steps = 3.2959e-01, PNorm = 109.1291, GNorm = 0.3159
Meta loss on this task batch = 3.2350e-01, Meta loss averaged over last 500 steps = 3.2960e-01, PNorm = 109.1354, GNorm = 0.2414
Meta loss on this task batch = 2.7990e-01, Meta loss averaged over last 500 steps = 3.2947e-01, PNorm = 109.1416, GNorm = 0.3368
Meta loss on this task batch = 3.9385e-01, Meta loss averaged over last 500 steps = 3.2967e-01, PNorm = 109.1469, GNorm = 0.3251
Meta loss on this task batch = 2.7980e-01, Meta loss averaged over last 500 steps = 3.2946e-01, PNorm = 109.1533, GNorm = 0.4040
Meta loss on this task batch = 3.1796e-01, Meta loss averaged over last 500 steps = 3.2952e-01, PNorm = 109.1603, GNorm = 0.3164
Meta loss on this task batch = 3.4813e-01, Meta loss averaged over last 500 steps = 3.2947e-01, PNorm = 109.1680, GNorm = 0.2771
Meta loss on this task batch = 3.1128e-01, Meta loss averaged over last 500 steps = 3.2941e-01, PNorm = 109.1762, GNorm = 0.2343
Meta loss on this task batch = 3.4133e-01, Meta loss averaged over last 500 steps = 3.2952e-01, PNorm = 109.1845, GNorm = 0.2705
Meta loss on this task batch = 3.4773e-01, Meta loss averaged over last 500 steps = 3.2952e-01, PNorm = 109.1927, GNorm = 0.2481
Meta loss on this task batch = 4.2197e-01, Meta loss averaged over last 500 steps = 3.2971e-01, PNorm = 109.2011, GNorm = 0.3992
Meta loss on this task batch = 3.3925e-01, Meta loss averaged over last 500 steps = 3.2974e-01, PNorm = 109.2113, GNorm = 0.4341
Meta loss on this task batch = 3.2648e-01, Meta loss averaged over last 500 steps = 3.2958e-01, PNorm = 109.2216, GNorm = 0.2199
Meta loss on this task batch = 3.3634e-01, Meta loss averaged over last 500 steps = 3.2965e-01, PNorm = 109.2331, GNorm = 0.4203
Meta loss on this task batch = 2.9024e-01, Meta loss averaged over last 500 steps = 3.2948e-01, PNorm = 109.2441, GNorm = 0.2584
Meta loss on this task batch = 3.2195e-01, Meta loss averaged over last 500 steps = 3.2948e-01, PNorm = 109.2522, GNorm = 0.4296
Took 113.57902264595032 seconds to complete one epoch of meta training
Took 121.44705700874329 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462005
Epoch 287
Meta loss on this task batch = 3.3120e-01, Meta loss averaged over last 500 steps = 3.2951e-01, PNorm = 109.2591, GNorm = 0.2727
Meta loss on this task batch = 3.5928e-01, Meta loss averaged over last 500 steps = 3.2952e-01, PNorm = 109.2657, GNorm = 0.3027
Meta loss on this task batch = 3.1659e-01, Meta loss averaged over last 500 steps = 3.2956e-01, PNorm = 109.2727, GNorm = 0.3269
Meta loss on this task batch = 3.4021e-01, Meta loss averaged over last 500 steps = 3.2965e-01, PNorm = 109.2801, GNorm = 0.2514
Meta loss on this task batch = 2.8462e-01, Meta loss averaged over last 500 steps = 3.2962e-01, PNorm = 109.2879, GNorm = 0.2695
Meta loss on this task batch = 2.7986e-01, Meta loss averaged over last 500 steps = 3.2948e-01, PNorm = 109.2958, GNorm = 0.3020
Meta loss on this task batch = 2.8728e-01, Meta loss averaged over last 500 steps = 3.2942e-01, PNorm = 109.3042, GNorm = 0.3620
Meta loss on this task batch = 3.3088e-01, Meta loss averaged over last 500 steps = 3.2947e-01, PNorm = 109.3119, GNorm = 0.3788
Meta loss on this task batch = 3.2228e-01, Meta loss averaged over last 500 steps = 3.2949e-01, PNorm = 109.3198, GNorm = 0.3297
Meta loss on this task batch = 3.4878e-01, Meta loss averaged over last 500 steps = 3.2944e-01, PNorm = 109.3268, GNorm = 0.4174
Meta loss on this task batch = 3.1641e-01, Meta loss averaged over last 500 steps = 3.2929e-01, PNorm = 109.3334, GNorm = 0.2942
Meta loss on this task batch = 3.1386e-01, Meta loss averaged over last 500 steps = 3.2924e-01, PNorm = 109.3399, GNorm = 0.2657
Meta loss on this task batch = 3.5397e-01, Meta loss averaged over last 500 steps = 3.2927e-01, PNorm = 109.3464, GNorm = 0.2789
Meta loss on this task batch = 3.4458e-01, Meta loss averaged over last 500 steps = 3.2931e-01, PNorm = 109.3536, GNorm = 0.3500
Meta loss on this task batch = 3.5206e-01, Meta loss averaged over last 500 steps = 3.2941e-01, PNorm = 109.3598, GNorm = 0.2946
Meta loss on this task batch = 3.0604e-01, Meta loss averaged over last 500 steps = 3.2938e-01, PNorm = 109.3669, GNorm = 0.2550
Meta loss on this task batch = 3.1325e-01, Meta loss averaged over last 500 steps = 3.2936e-01, PNorm = 109.3738, GNorm = 0.3028
Meta loss on this task batch = 3.5925e-01, Meta loss averaged over last 500 steps = 3.2946e-01, PNorm = 109.3810, GNorm = 0.3067
Meta loss on this task batch = 3.7461e-01, Meta loss averaged over last 500 steps = 3.2951e-01, PNorm = 109.3885, GNorm = 0.4144
Took 116.19089937210083 seconds to complete one epoch of meta training
Took 123.07374286651611 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471165
Epoch 288
Meta loss on this task batch = 3.5234e-01, Meta loss averaged over last 500 steps = 3.2953e-01, PNorm = 109.3953, GNorm = 0.2914
Meta loss on this task batch = 3.0787e-01, Meta loss averaged over last 500 steps = 3.2957e-01, PNorm = 109.4024, GNorm = 0.2749
Meta loss on this task batch = 3.7087e-01, Meta loss averaged over last 500 steps = 3.2974e-01, PNorm = 109.4096, GNorm = 0.3361
Meta loss on this task batch = 3.3280e-01, Meta loss averaged over last 500 steps = 3.2969e-01, PNorm = 109.4173, GNorm = 0.3769
Meta loss on this task batch = 3.2278e-01, Meta loss averaged over last 500 steps = 3.2962e-01, PNorm = 109.4259, GNorm = 0.2852
Meta loss on this task batch = 2.8201e-01, Meta loss averaged over last 500 steps = 3.2944e-01, PNorm = 109.4349, GNorm = 0.2406
Meta loss on this task batch = 3.4826e-01, Meta loss averaged over last 500 steps = 3.2951e-01, PNorm = 109.4438, GNorm = 0.2719
Meta loss on this task batch = 2.8593e-01, Meta loss averaged over last 500 steps = 3.2931e-01, PNorm = 109.4531, GNorm = 0.2967
Meta loss on this task batch = 3.1644e-01, Meta loss averaged over last 500 steps = 3.2928e-01, PNorm = 109.4626, GNorm = 0.6172
Meta loss on this task batch = 2.8956e-01, Meta loss averaged over last 500 steps = 3.2915e-01, PNorm = 109.4713, GNorm = 0.2537
Meta loss on this task batch = 3.2277e-01, Meta loss averaged over last 500 steps = 3.2909e-01, PNorm = 109.4788, GNorm = 0.3570
Meta loss on this task batch = 3.2970e-01, Meta loss averaged over last 500 steps = 3.2911e-01, PNorm = 109.4864, GNorm = 0.2912
Meta loss on this task batch = 3.8508e-01, Meta loss averaged over last 500 steps = 3.2928e-01, PNorm = 109.4939, GNorm = 0.3795
Meta loss on this task batch = 3.4887e-01, Meta loss averaged over last 500 steps = 3.2926e-01, PNorm = 109.5010, GNorm = 0.2883
Meta loss on this task batch = 3.6680e-01, Meta loss averaged over last 500 steps = 3.2929e-01, PNorm = 109.5077, GNorm = 0.2522
Meta loss on this task batch = 3.8084e-01, Meta loss averaged over last 500 steps = 3.2942e-01, PNorm = 109.5141, GNorm = 0.2898
Meta loss on this task batch = 3.1696e-01, Meta loss averaged over last 500 steps = 3.2939e-01, PNorm = 109.5203, GNorm = 0.2886
Meta loss on this task batch = 3.5766e-01, Meta loss averaged over last 500 steps = 3.2936e-01, PNorm = 109.5264, GNorm = 0.2578
Meta loss on this task batch = 3.2145e-01, Meta loss averaged over last 500 steps = 3.2931e-01, PNorm = 109.5330, GNorm = 0.3541
Took 112.26201510429382 seconds to complete one epoch of meta training
Took 120.18775415420532 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.435728
Epoch 289
Meta loss on this task batch = 3.5227e-01, Meta loss averaged over last 500 steps = 3.2938e-01, PNorm = 109.5391, GNorm = 0.3331
Meta loss on this task batch = 3.7433e-01, Meta loss averaged over last 500 steps = 3.2947e-01, PNorm = 109.5466, GNorm = 0.4066
Meta loss on this task batch = 3.0390e-01, Meta loss averaged over last 500 steps = 3.2933e-01, PNorm = 109.5547, GNorm = 0.3137
Meta loss on this task batch = 2.8833e-01, Meta loss averaged over last 500 steps = 3.2927e-01, PNorm = 109.5627, GNorm = 0.2592
Meta loss on this task batch = 3.3146e-01, Meta loss averaged over last 500 steps = 3.2920e-01, PNorm = 109.5709, GNorm = 0.3084
Meta loss on this task batch = 2.8445e-01, Meta loss averaged over last 500 steps = 3.2919e-01, PNorm = 109.5793, GNorm = 0.2603
Meta loss on this task batch = 3.1834e-01, Meta loss averaged over last 500 steps = 3.2903e-01, PNorm = 109.5876, GNorm = 0.3265
Meta loss on this task batch = 3.8428e-01, Meta loss averaged over last 500 steps = 3.2911e-01, PNorm = 109.5955, GNorm = 0.3042
Meta loss on this task batch = 2.9017e-01, Meta loss averaged over last 500 steps = 3.2901e-01, PNorm = 109.6043, GNorm = 0.2925
Meta loss on this task batch = 3.5652e-01, Meta loss averaged over last 500 steps = 3.2902e-01, PNorm = 109.6135, GNorm = 0.4455
Meta loss on this task batch = 3.3273e-01, Meta loss averaged over last 500 steps = 3.2910e-01, PNorm = 109.6227, GNorm = 0.3430
Meta loss on this task batch = 4.2330e-01, Meta loss averaged over last 500 steps = 3.2922e-01, PNorm = 109.6299, GNorm = 0.5025
Meta loss on this task batch = 2.9397e-01, Meta loss averaged over last 500 steps = 3.2924e-01, PNorm = 109.6379, GNorm = 0.2814
Meta loss on this task batch = 3.4215e-01, Meta loss averaged over last 500 steps = 3.2932e-01, PNorm = 109.6444, GNorm = 0.3044
Meta loss on this task batch = 3.3290e-01, Meta loss averaged over last 500 steps = 3.2931e-01, PNorm = 109.6505, GNorm = 0.3288
Meta loss on this task batch = 2.9968e-01, Meta loss averaged over last 500 steps = 3.2926e-01, PNorm = 109.6570, GNorm = 0.2124
Meta loss on this task batch = 3.2289e-01, Meta loss averaged over last 500 steps = 3.2918e-01, PNorm = 109.6644, GNorm = 0.2685
Meta loss on this task batch = 3.3049e-01, Meta loss averaged over last 500 steps = 3.2904e-01, PNorm = 109.6720, GNorm = 0.2987
Meta loss on this task batch = 2.8551e-01, Meta loss averaged over last 500 steps = 3.2899e-01, PNorm = 109.6795, GNorm = 0.4534
Took 118.99834489822388 seconds to complete one epoch of meta training
Took 126.34782147407532 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477951
Epoch 290
Meta loss on this task batch = 3.4754e-01, Meta loss averaged over last 500 steps = 3.2911e-01, PNorm = 109.6858, GNorm = 0.2711
Meta loss on this task batch = 3.2105e-01, Meta loss averaged over last 500 steps = 3.2914e-01, PNorm = 109.6924, GNorm = 0.2738
Meta loss on this task batch = 3.0608e-01, Meta loss averaged over last 500 steps = 3.2908e-01, PNorm = 109.7000, GNorm = 0.2541
Meta loss on this task batch = 3.1093e-01, Meta loss averaged over last 500 steps = 3.2897e-01, PNorm = 109.7080, GNorm = 0.3110
Meta loss on this task batch = 3.0961e-01, Meta loss averaged over last 500 steps = 3.2890e-01, PNorm = 109.7161, GNorm = 0.2807
Meta loss on this task batch = 3.3251e-01, Meta loss averaged over last 500 steps = 3.2885e-01, PNorm = 109.7245, GNorm = 0.3527
Meta loss on this task batch = 3.3210e-01, Meta loss averaged over last 500 steps = 3.2895e-01, PNorm = 109.7328, GNorm = 0.3225
Meta loss on this task batch = 3.8938e-01, Meta loss averaged over last 500 steps = 3.2908e-01, PNorm = 109.7406, GNorm = 0.3318
Meta loss on this task batch = 3.3629e-01, Meta loss averaged over last 500 steps = 3.2917e-01, PNorm = 109.7484, GNorm = 0.3160
Meta loss on this task batch = 3.2105e-01, Meta loss averaged over last 500 steps = 3.2912e-01, PNorm = 109.7563, GNorm = 0.2502
Meta loss on this task batch = 3.6655e-01, Meta loss averaged over last 500 steps = 3.2918e-01, PNorm = 109.7640, GNorm = 0.3435
Meta loss on this task batch = 2.7249e-01, Meta loss averaged over last 500 steps = 3.2912e-01, PNorm = 109.7719, GNorm = 0.2735
Meta loss on this task batch = 3.6464e-01, Meta loss averaged over last 500 steps = 3.2916e-01, PNorm = 109.7802, GNorm = 0.2976
Meta loss on this task batch = 3.8067e-01, Meta loss averaged over last 500 steps = 3.2927e-01, PNorm = 109.7871, GNorm = 0.4072
Meta loss on this task batch = 3.1061e-01, Meta loss averaged over last 500 steps = 3.2918e-01, PNorm = 109.7941, GNorm = 0.2243
Meta loss on this task batch = 2.8620e-01, Meta loss averaged over last 500 steps = 3.2921e-01, PNorm = 109.8008, GNorm = 0.2775
Meta loss on this task batch = 3.3787e-01, Meta loss averaged over last 500 steps = 3.2924e-01, PNorm = 109.8089, GNorm = 0.3228
Meta loss on this task batch = 3.3850e-01, Meta loss averaged over last 500 steps = 3.2926e-01, PNorm = 109.8165, GNorm = 0.2819
Meta loss on this task batch = 3.4378e-01, Meta loss averaged over last 500 steps = 3.2922e-01, PNorm = 109.8239, GNorm = 0.3151
Took 134.52729535102844 seconds to complete one epoch of meta training
Took 141.84414076805115 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468392
Epoch 291
Meta loss on this task batch = 3.5301e-01, Meta loss averaged over last 500 steps = 3.2926e-01, PNorm = 109.8308, GNorm = 0.2995
Meta loss on this task batch = 3.1329e-01, Meta loss averaged over last 500 steps = 3.2927e-01, PNorm = 109.8385, GNorm = 0.2393
Meta loss on this task batch = 3.2857e-01, Meta loss averaged over last 500 steps = 3.2926e-01, PNorm = 109.8463, GNorm = 0.2853
Meta loss on this task batch = 3.2124e-01, Meta loss averaged over last 500 steps = 3.2918e-01, PNorm = 109.8544, GNorm = 0.2647
Meta loss on this task batch = 3.4325e-01, Meta loss averaged over last 500 steps = 3.2925e-01, PNorm = 109.8613, GNorm = 0.2644
Meta loss on this task batch = 3.4021e-01, Meta loss averaged over last 500 steps = 3.2928e-01, PNorm = 109.8686, GNorm = 0.2983
Meta loss on this task batch = 3.3634e-01, Meta loss averaged over last 500 steps = 3.2939e-01, PNorm = 109.8758, GNorm = 0.2243
Meta loss on this task batch = 3.0865e-01, Meta loss averaged over last 500 steps = 3.2935e-01, PNorm = 109.8839, GNorm = 0.2422
Meta loss on this task batch = 3.1134e-01, Meta loss averaged over last 500 steps = 3.2934e-01, PNorm = 109.8924, GNorm = 0.2488
Meta loss on this task batch = 2.5994e-01, Meta loss averaged over last 500 steps = 3.2915e-01, PNorm = 109.9011, GNorm = 0.2187
Meta loss on this task batch = 3.5023e-01, Meta loss averaged over last 500 steps = 3.2920e-01, PNorm = 109.9093, GNorm = 0.2959
Meta loss on this task batch = 3.1380e-01, Meta loss averaged over last 500 steps = 3.2915e-01, PNorm = 109.9175, GNorm = 0.2559
Meta loss on this task batch = 3.2707e-01, Meta loss averaged over last 500 steps = 3.2924e-01, PNorm = 109.9260, GNorm = 0.2598
Meta loss on this task batch = 3.4110e-01, Meta loss averaged over last 500 steps = 3.2925e-01, PNorm = 109.9344, GNorm = 0.3706
Meta loss on this task batch = 3.2522e-01, Meta loss averaged over last 500 steps = 3.2912e-01, PNorm = 109.9424, GNorm = 0.2831
Meta loss on this task batch = 3.1694e-01, Meta loss averaged over last 500 steps = 3.2922e-01, PNorm = 109.9505, GNorm = 0.3272
Meta loss on this task batch = 3.2530e-01, Meta loss averaged over last 500 steps = 3.2919e-01, PNorm = 109.9584, GNorm = 0.2408
Meta loss on this task batch = 3.3592e-01, Meta loss averaged over last 500 steps = 3.2915e-01, PNorm = 109.9665, GNorm = 0.2780
Meta loss on this task batch = 3.8290e-01, Meta loss averaged over last 500 steps = 3.2931e-01, PNorm = 109.9745, GNorm = 0.4885
Took 114.20017290115356 seconds to complete one epoch of meta training
Took 121.62928318977356 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448469
Epoch 292
Meta loss on this task batch = 3.3443e-01, Meta loss averaged over last 500 steps = 3.2940e-01, PNorm = 109.9821, GNorm = 0.2897
Meta loss on this task batch = 3.3463e-01, Meta loss averaged over last 500 steps = 3.2947e-01, PNorm = 109.9897, GNorm = 0.3244
Meta loss on this task batch = 3.2958e-01, Meta loss averaged over last 500 steps = 3.2944e-01, PNorm = 109.9973, GNorm = 0.3954
Meta loss on this task batch = 3.3451e-01, Meta loss averaged over last 500 steps = 3.2941e-01, PNorm = 110.0055, GNorm = 0.2913
Meta loss on this task batch = 3.1448e-01, Meta loss averaged over last 500 steps = 3.2949e-01, PNorm = 110.0141, GNorm = 0.3244
Meta loss on this task batch = 3.2476e-01, Meta loss averaged over last 500 steps = 3.2943e-01, PNorm = 110.0222, GNorm = 0.3391
Meta loss on this task batch = 3.5935e-01, Meta loss averaged over last 500 steps = 3.2943e-01, PNorm = 110.0297, GNorm = 0.3110
Meta loss on this task batch = 3.3608e-01, Meta loss averaged over last 500 steps = 3.2946e-01, PNorm = 110.0370, GNorm = 0.3473
Meta loss on this task batch = 2.8314e-01, Meta loss averaged over last 500 steps = 3.2930e-01, PNorm = 110.0453, GNorm = 0.2237
Meta loss on this task batch = 3.4190e-01, Meta loss averaged over last 500 steps = 3.2927e-01, PNorm = 110.0526, GNorm = 0.2749
Meta loss on this task batch = 3.7935e-01, Meta loss averaged over last 500 steps = 3.2922e-01, PNorm = 110.0593, GNorm = 0.2542
Meta loss on this task batch = 3.1289e-01, Meta loss averaged over last 500 steps = 3.2922e-01, PNorm = 110.0664, GNorm = 0.2593
Meta loss on this task batch = 3.0552e-01, Meta loss averaged over last 500 steps = 3.2926e-01, PNorm = 110.0738, GNorm = 0.2411
Meta loss on this task batch = 3.6274e-01, Meta loss averaged over last 500 steps = 3.2934e-01, PNorm = 110.0803, GNorm = 0.2560
Meta loss on this task batch = 3.3187e-01, Meta loss averaged over last 500 steps = 3.2934e-01, PNorm = 110.0866, GNorm = 0.2834
Meta loss on this task batch = 3.0896e-01, Meta loss averaged over last 500 steps = 3.2927e-01, PNorm = 110.0935, GNorm = 0.2736
Meta loss on this task batch = 3.1989e-01, Meta loss averaged over last 500 steps = 3.2927e-01, PNorm = 110.1004, GNorm = 0.2370
Meta loss on this task batch = 3.0832e-01, Meta loss averaged over last 500 steps = 3.2933e-01, PNorm = 110.1070, GNorm = 0.3009
Meta loss on this task batch = 3.8977e-01, Meta loss averaged over last 500 steps = 3.2948e-01, PNorm = 110.1132, GNorm = 0.3238
Took 109.80698084831238 seconds to complete one epoch of meta training
Took 117.57781767845154 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466765
Epoch 293
Meta loss on this task batch = 3.4996e-01, Meta loss averaged over last 500 steps = 3.2953e-01, PNorm = 110.1187, GNorm = 0.3246
Meta loss on this task batch = 3.1539e-01, Meta loss averaged over last 500 steps = 3.2941e-01, PNorm = 110.1244, GNorm = 0.2774
Meta loss on this task batch = 3.2109e-01, Meta loss averaged over last 500 steps = 3.2932e-01, PNorm = 110.1296, GNorm = 0.2372
Meta loss on this task batch = 2.9785e-01, Meta loss averaged over last 500 steps = 3.2928e-01, PNorm = 110.1350, GNorm = 0.2296
Meta loss on this task batch = 3.3504e-01, Meta loss averaged over last 500 steps = 3.2943e-01, PNorm = 110.1407, GNorm = 0.2643
Meta loss on this task batch = 3.6111e-01, Meta loss averaged over last 500 steps = 3.2953e-01, PNorm = 110.1466, GNorm = 0.2907
Meta loss on this task batch = 3.0135e-01, Meta loss averaged over last 500 steps = 3.2940e-01, PNorm = 110.1537, GNorm = 0.2682
Meta loss on this task batch = 2.5748e-01, Meta loss averaged over last 500 steps = 3.2921e-01, PNorm = 110.1618, GNorm = 0.2485
Meta loss on this task batch = 2.9581e-01, Meta loss averaged over last 500 steps = 3.2915e-01, PNorm = 110.1697, GNorm = 0.3034
Meta loss on this task batch = 3.9154e-01, Meta loss averaged over last 500 steps = 3.2933e-01, PNorm = 110.1757, GNorm = 0.3072
Meta loss on this task batch = 3.4791e-01, Meta loss averaged over last 500 steps = 3.2938e-01, PNorm = 110.1812, GNorm = 0.3489
Meta loss on this task batch = 3.1655e-01, Meta loss averaged over last 500 steps = 3.2929e-01, PNorm = 110.1868, GNorm = 0.2984
Meta loss on this task batch = 3.4133e-01, Meta loss averaged over last 500 steps = 3.2934e-01, PNorm = 110.1923, GNorm = 0.2652
Meta loss on this task batch = 3.1619e-01, Meta loss averaged over last 500 steps = 3.2939e-01, PNorm = 110.1973, GNorm = 0.2602
Meta loss on this task batch = 2.5217e-01, Meta loss averaged over last 500 steps = 3.2919e-01, PNorm = 110.2032, GNorm = 0.5130
Meta loss on this task batch = 3.8882e-01, Meta loss averaged over last 500 steps = 3.2937e-01, PNorm = 110.2092, GNorm = 0.3906
Meta loss on this task batch = 3.1087e-01, Meta loss averaged over last 500 steps = 3.2940e-01, PNorm = 110.2153, GNorm = 0.2601
Meta loss on this task batch = 3.8018e-01, Meta loss averaged over last 500 steps = 3.2954e-01, PNorm = 110.2209, GNorm = 0.4307
Meta loss on this task batch = 3.2498e-01, Meta loss averaged over last 500 steps = 3.2960e-01, PNorm = 110.2264, GNorm = 0.2944
Took 109.70746302604675 seconds to complete one epoch of meta training
Took 118.12961792945862 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479297
Epoch 294
Meta loss on this task batch = 3.1579e-01, Meta loss averaged over last 500 steps = 3.2950e-01, PNorm = 110.2321, GNorm = 0.2389
Meta loss on this task batch = 2.9572e-01, Meta loss averaged over last 500 steps = 3.2939e-01, PNorm = 110.2381, GNorm = 0.2586
Meta loss on this task batch = 3.3033e-01, Meta loss averaged over last 500 steps = 3.2929e-01, PNorm = 110.2448, GNorm = 0.3598
Meta loss on this task batch = 3.0651e-01, Meta loss averaged over last 500 steps = 3.2921e-01, PNorm = 110.2512, GNorm = 0.3121
Meta loss on this task batch = 3.0423e-01, Meta loss averaged over last 500 steps = 3.2922e-01, PNorm = 110.2572, GNorm = 0.3174
Meta loss on this task batch = 3.2298e-01, Meta loss averaged over last 500 steps = 3.2928e-01, PNorm = 110.2640, GNorm = 0.2350
Meta loss on this task batch = 3.0908e-01, Meta loss averaged over last 500 steps = 3.2924e-01, PNorm = 110.2709, GNorm = 0.2305
Meta loss on this task batch = 3.1171e-01, Meta loss averaged over last 500 steps = 3.2912e-01, PNorm = 110.2769, GNorm = 0.2547
Meta loss on this task batch = 3.4841e-01, Meta loss averaged over last 500 steps = 3.2919e-01, PNorm = 110.2816, GNorm = 0.3640
Meta loss on this task batch = 3.4130e-01, Meta loss averaged over last 500 steps = 3.2925e-01, PNorm = 110.2871, GNorm = 0.3517
Meta loss on this task batch = 3.6405e-01, Meta loss averaged over last 500 steps = 3.2930e-01, PNorm = 110.2920, GNorm = 0.4215
Meta loss on this task batch = 3.1580e-01, Meta loss averaged over last 500 steps = 3.2933e-01, PNorm = 110.2962, GNorm = 0.2588
Meta loss on this task batch = 3.1662e-01, Meta loss averaged over last 500 steps = 3.2937e-01, PNorm = 110.3010, GNorm = 0.2512
Meta loss on this task batch = 3.0084e-01, Meta loss averaged over last 500 steps = 3.2928e-01, PNorm = 110.3068, GNorm = 0.2870
Meta loss on this task batch = 3.1909e-01, Meta loss averaged over last 500 steps = 3.2920e-01, PNorm = 110.3133, GNorm = 0.2494
Meta loss on this task batch = 3.2529e-01, Meta loss averaged over last 500 steps = 3.2919e-01, PNorm = 110.3192, GNorm = 0.2454
Meta loss on this task batch = 3.7044e-01, Meta loss averaged over last 500 steps = 3.2931e-01, PNorm = 110.3233, GNorm = 0.3310
Meta loss on this task batch = 3.3791e-01, Meta loss averaged over last 500 steps = 3.2934e-01, PNorm = 110.3289, GNorm = 0.4502
Meta loss on this task batch = 3.3170e-01, Meta loss averaged over last 500 steps = 3.2936e-01, PNorm = 110.3339, GNorm = 0.2883
Took 111.07386541366577 seconds to complete one epoch of meta training
Took 118.65901303291321 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449266
Epoch 295
Meta loss on this task batch = 2.9411e-01, Meta loss averaged over last 500 steps = 3.2928e-01, PNorm = 110.3388, GNorm = 0.3287
Meta loss on this task batch = 2.6640e-01, Meta loss averaged over last 500 steps = 3.2914e-01, PNorm = 110.3447, GNorm = 0.2236
Meta loss on this task batch = 3.8936e-01, Meta loss averaged over last 500 steps = 3.2918e-01, PNorm = 110.3506, GNorm = 0.2944
Meta loss on this task batch = 2.8221e-01, Meta loss averaged over last 500 steps = 3.2910e-01, PNorm = 110.3573, GNorm = 0.2694
Meta loss on this task batch = 3.0678e-01, Meta loss averaged over last 500 steps = 3.2899e-01, PNorm = 110.3648, GNorm = 0.3842
Meta loss on this task batch = 4.0804e-01, Meta loss averaged over last 500 steps = 3.2898e-01, PNorm = 110.3721, GNorm = 0.3044
Meta loss on this task batch = 3.5496e-01, Meta loss averaged over last 500 steps = 3.2908e-01, PNorm = 110.3796, GNorm = 0.2701
Meta loss on this task batch = 2.9324e-01, Meta loss averaged over last 500 steps = 3.2886e-01, PNorm = 110.3875, GNorm = 0.2998
Meta loss on this task batch = 3.9079e-01, Meta loss averaged over last 500 steps = 3.2893e-01, PNorm = 110.3946, GNorm = 0.3539
Meta loss on this task batch = 3.4296e-01, Meta loss averaged over last 500 steps = 3.2905e-01, PNorm = 110.4009, GNorm = 0.2794
Meta loss on this task batch = 2.7264e-01, Meta loss averaged over last 500 steps = 3.2888e-01, PNorm = 110.4075, GNorm = 0.2838
Meta loss on this task batch = 2.8196e-01, Meta loss averaged over last 500 steps = 3.2886e-01, PNorm = 110.4144, GNorm = 0.3394
Meta loss on this task batch = 2.7708e-01, Meta loss averaged over last 500 steps = 3.2865e-01, PNorm = 110.4211, GNorm = 0.3094
Meta loss on this task batch = 3.5749e-01, Meta loss averaged over last 500 steps = 3.2867e-01, PNorm = 110.4279, GNorm = 0.3076
Meta loss on this task batch = 3.5241e-01, Meta loss averaged over last 500 steps = 3.2866e-01, PNorm = 110.4346, GNorm = 0.4471
Meta loss on this task batch = 3.5387e-01, Meta loss averaged over last 500 steps = 3.2870e-01, PNorm = 110.4409, GNorm = 0.3826
Meta loss on this task batch = 3.2304e-01, Meta loss averaged over last 500 steps = 3.2862e-01, PNorm = 110.4472, GNorm = 0.3424
Meta loss on this task batch = 3.0439e-01, Meta loss averaged over last 500 steps = 3.2861e-01, PNorm = 110.4537, GNorm = 0.2861
Meta loss on this task batch = 2.7223e-01, Meta loss averaged over last 500 steps = 3.2864e-01, PNorm = 110.4609, GNorm = 0.2671
Took 111.41691279411316 seconds to complete one epoch of meta training
Took 119.05814719200134 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461269
Epoch 296
Meta loss on this task batch = 3.5040e-01, Meta loss averaged over last 500 steps = 3.2873e-01, PNorm = 110.4685, GNorm = 0.2824
Meta loss on this task batch = 2.4126e-01, Meta loss averaged over last 500 steps = 3.2852e-01, PNorm = 110.4770, GNorm = 0.2106
Meta loss on this task batch = 3.1824e-01, Meta loss averaged over last 500 steps = 3.2845e-01, PNorm = 110.4856, GNorm = 0.2882
Meta loss on this task batch = 3.2337e-01, Meta loss averaged over last 500 steps = 3.2856e-01, PNorm = 110.4946, GNorm = 0.3174
Meta loss on this task batch = 4.0441e-01, Meta loss averaged over last 500 steps = 3.2872e-01, PNorm = 110.5025, GNorm = 0.3292
Meta loss on this task batch = 3.3061e-01, Meta loss averaged over last 500 steps = 3.2865e-01, PNorm = 110.5093, GNorm = 0.2816
Meta loss on this task batch = 3.2115e-01, Meta loss averaged over last 500 steps = 3.2866e-01, PNorm = 110.5170, GNorm = 0.3462
Meta loss on this task batch = 3.2863e-01, Meta loss averaged over last 500 steps = 3.2861e-01, PNorm = 110.5247, GNorm = 0.2990
Meta loss on this task batch = 3.2790e-01, Meta loss averaged over last 500 steps = 3.2859e-01, PNorm = 110.5325, GNorm = 0.2785
Meta loss on this task batch = 3.3317e-01, Meta loss averaged over last 500 steps = 3.2855e-01, PNorm = 110.5397, GNorm = 0.2722
Meta loss on this task batch = 3.3739e-01, Meta loss averaged over last 500 steps = 3.2865e-01, PNorm = 110.5473, GNorm = 0.5231
Meta loss on this task batch = 3.1333e-01, Meta loss averaged over last 500 steps = 3.2857e-01, PNorm = 110.5552, GNorm = 0.2250
Meta loss on this task batch = 3.2197e-01, Meta loss averaged over last 500 steps = 3.2850e-01, PNorm = 110.5628, GNorm = 0.2678
Meta loss on this task batch = 2.9389e-01, Meta loss averaged over last 500 steps = 3.2844e-01, PNorm = 110.5714, GNorm = 0.5051
Meta loss on this task batch = 3.7462e-01, Meta loss averaged over last 500 steps = 3.2857e-01, PNorm = 110.5798, GNorm = 0.3028
Meta loss on this task batch = 2.7327e-01, Meta loss averaged over last 500 steps = 3.2851e-01, PNorm = 110.5883, GNorm = 0.3261
Meta loss on this task batch = 3.2781e-01, Meta loss averaged over last 500 steps = 3.2849e-01, PNorm = 110.5971, GNorm = 0.3474
Meta loss on this task batch = 2.7421e-01, Meta loss averaged over last 500 steps = 3.2841e-01, PNorm = 110.6062, GNorm = 0.2462
Meta loss on this task batch = 3.5737e-01, Meta loss averaged over last 500 steps = 3.2839e-01, PNorm = 110.6143, GNorm = 0.3798
Took 113.99219036102295 seconds to complete one epoch of meta training
Took 121.2486002445221 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478716
Epoch 297
Meta loss on this task batch = 2.8569e-01, Meta loss averaged over last 500 steps = 3.2837e-01, PNorm = 110.6223, GNorm = 0.3442
Meta loss on this task batch = 3.2577e-01, Meta loss averaged over last 500 steps = 3.2841e-01, PNorm = 110.6306, GNorm = 0.3638
Meta loss on this task batch = 2.7971e-01, Meta loss averaged over last 500 steps = 3.2827e-01, PNorm = 110.6382, GNorm = 0.2780
Meta loss on this task batch = 3.8698e-01, Meta loss averaged over last 500 steps = 3.2838e-01, PNorm = 110.6446, GNorm = 0.4729
Meta loss on this task batch = 3.4314e-01, Meta loss averaged over last 500 steps = 3.2835e-01, PNorm = 110.6506, GNorm = 0.3350
Meta loss on this task batch = 3.4377e-01, Meta loss averaged over last 500 steps = 3.2845e-01, PNorm = 110.6562, GNorm = 0.3081
Meta loss on this task batch = 3.2702e-01, Meta loss averaged over last 500 steps = 3.2846e-01, PNorm = 110.6618, GNorm = 0.2242
Meta loss on this task batch = 3.1448e-01, Meta loss averaged over last 500 steps = 3.2845e-01, PNorm = 110.6680, GNorm = 0.2213
Meta loss on this task batch = 3.6882e-01, Meta loss averaged over last 500 steps = 3.2861e-01, PNorm = 110.6748, GNorm = 0.4057
Meta loss on this task batch = 3.3579e-01, Meta loss averaged over last 500 steps = 3.2865e-01, PNorm = 110.6820, GNorm = 0.2552
Meta loss on this task batch = 3.4208e-01, Meta loss averaged over last 500 steps = 3.2856e-01, PNorm = 110.6888, GNorm = 0.2354
Meta loss on this task batch = 3.2882e-01, Meta loss averaged over last 500 steps = 3.2866e-01, PNorm = 110.6960, GNorm = 0.2623
Meta loss on this task batch = 3.4804e-01, Meta loss averaged over last 500 steps = 3.2858e-01, PNorm = 110.7030, GNorm = 0.3300
Meta loss on this task batch = 2.8107e-01, Meta loss averaged over last 500 steps = 3.2849e-01, PNorm = 110.7103, GNorm = 0.2710
Meta loss on this task batch = 3.4673e-01, Meta loss averaged over last 500 steps = 3.2851e-01, PNorm = 110.7172, GNorm = 0.3767
Meta loss on this task batch = 2.8882e-01, Meta loss averaged over last 500 steps = 3.2844e-01, PNorm = 110.7251, GNorm = 0.2452
Meta loss on this task batch = 2.7462e-01, Meta loss averaged over last 500 steps = 3.2823e-01, PNorm = 110.7331, GNorm = 0.2861
Meta loss on this task batch = 3.2671e-01, Meta loss averaged over last 500 steps = 3.2816e-01, PNorm = 110.7409, GNorm = 0.2652
Meta loss on this task batch = 3.3042e-01, Meta loss averaged over last 500 steps = 3.2817e-01, PNorm = 110.7485, GNorm = 0.4572
Took 113.0504002571106 seconds to complete one epoch of meta training
Took 120.73873805999756 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485204
Epoch 298
Meta loss on this task batch = 4.3036e-01, Meta loss averaged over last 500 steps = 3.2843e-01, PNorm = 110.7556, GNorm = 0.3668
Meta loss on this task batch = 2.8964e-01, Meta loss averaged over last 500 steps = 3.2840e-01, PNorm = 110.7624, GNorm = 0.2725
Meta loss on this task batch = 2.9272e-01, Meta loss averaged over last 500 steps = 3.2827e-01, PNorm = 110.7690, GNorm = 0.2798
Meta loss on this task batch = 3.4686e-01, Meta loss averaged over last 500 steps = 3.2829e-01, PNorm = 110.7757, GNorm = 0.4153
Meta loss on this task batch = 3.3920e-01, Meta loss averaged over last 500 steps = 3.2836e-01, PNorm = 110.7832, GNorm = 0.3754
Meta loss on this task batch = 3.3517e-01, Meta loss averaged over last 500 steps = 3.2837e-01, PNorm = 110.7905, GNorm = 0.2401
Meta loss on this task batch = 3.2813e-01, Meta loss averaged over last 500 steps = 3.2836e-01, PNorm = 110.7981, GNorm = 0.3633
Meta loss on this task batch = 3.5197e-01, Meta loss averaged over last 500 steps = 3.2843e-01, PNorm = 110.8064, GNorm = 0.5404
Meta loss on this task batch = 3.1708e-01, Meta loss averaged over last 500 steps = 3.2844e-01, PNorm = 110.8150, GNorm = 0.2625
Meta loss on this task batch = 2.9677e-01, Meta loss averaged over last 500 steps = 3.2835e-01, PNorm = 110.8237, GNorm = 0.2606
Meta loss on this task batch = 3.3913e-01, Meta loss averaged over last 500 steps = 3.2834e-01, PNorm = 110.8321, GNorm = 0.4112
Meta loss on this task batch = 3.0113e-01, Meta loss averaged over last 500 steps = 3.2837e-01, PNorm = 110.8401, GNorm = 0.2736
Meta loss on this task batch = 2.6926e-01, Meta loss averaged over last 500 steps = 3.2812e-01, PNorm = 110.8489, GNorm = 0.2759
Meta loss on this task batch = 3.7474e-01, Meta loss averaged over last 500 steps = 3.2822e-01, PNorm = 110.8580, GNorm = 0.5252
Meta loss on this task batch = 3.5737e-01, Meta loss averaged over last 500 steps = 3.2836e-01, PNorm = 110.8668, GNorm = 0.2836
Meta loss on this task batch = 2.8851e-01, Meta loss averaged over last 500 steps = 3.2834e-01, PNorm = 110.8758, GNorm = 0.2899
Meta loss on this task batch = 2.7511e-01, Meta loss averaged over last 500 steps = 3.2819e-01, PNorm = 110.8844, GNorm = 0.2472
Meta loss on this task batch = 3.2059e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 110.8923, GNorm = 0.3372
Meta loss on this task batch = 3.3330e-01, Meta loss averaged over last 500 steps = 3.2811e-01, PNorm = 110.8988, GNorm = 0.3443
Took 270.2270154953003 seconds to complete one epoch of meta training
Took 277.7439525127411 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486934
Epoch 299
Meta loss on this task batch = 2.9292e-01, Meta loss averaged over last 500 steps = 3.2806e-01, PNorm = 110.9061, GNorm = 0.2618
Meta loss on this task batch = 2.6538e-01, Meta loss averaged over last 500 steps = 3.2786e-01, PNorm = 110.9136, GNorm = 0.2953
Meta loss on this task batch = 3.3126e-01, Meta loss averaged over last 500 steps = 3.2794e-01, PNorm = 110.9213, GNorm = 0.2644
Meta loss on this task batch = 2.9949e-01, Meta loss averaged over last 500 steps = 3.2793e-01, PNorm = 110.9291, GNorm = 0.4226
Meta loss on this task batch = 3.5408e-01, Meta loss averaged over last 500 steps = 3.2806e-01, PNorm = 110.9354, GNorm = 0.2672
Meta loss on this task batch = 3.5516e-01, Meta loss averaged over last 500 steps = 3.2814e-01, PNorm = 110.9406, GNorm = 0.3140
Meta loss on this task batch = 3.0651e-01, Meta loss averaged over last 500 steps = 3.2820e-01, PNorm = 110.9463, GNorm = 0.2512
Meta loss on this task batch = 2.8366e-01, Meta loss averaged over last 500 steps = 3.2814e-01, PNorm = 110.9521, GNorm = 0.2545
Meta loss on this task batch = 3.6137e-01, Meta loss averaged over last 500 steps = 3.2814e-01, PNorm = 110.9570, GNorm = 0.4155
Meta loss on this task batch = 3.2572e-01, Meta loss averaged over last 500 steps = 3.2821e-01, PNorm = 110.9620, GNorm = 0.2825
Meta loss on this task batch = 3.3930e-01, Meta loss averaged over last 500 steps = 3.2822e-01, PNorm = 110.9667, GNorm = 0.2675
Meta loss on this task batch = 3.1186e-01, Meta loss averaged over last 500 steps = 3.2820e-01, PNorm = 110.9720, GNorm = 0.2999
Meta loss on this task batch = 3.2731e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 110.9785, GNorm = 0.3337
Meta loss on this task batch = 3.1625e-01, Meta loss averaged over last 500 steps = 3.2815e-01, PNorm = 110.9855, GNorm = 0.2516
Meta loss on this task batch = 3.9772e-01, Meta loss averaged over last 500 steps = 3.2826e-01, PNorm = 110.9924, GNorm = 0.3295
Meta loss on this task batch = 2.9732e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 111.0005, GNorm = 0.2247
Meta loss on this task batch = 3.6881e-01, Meta loss averaged over last 500 steps = 3.2819e-01, PNorm = 111.0088, GNorm = 0.2449
Meta loss on this task batch = 3.1612e-01, Meta loss averaged over last 500 steps = 3.2821e-01, PNorm = 111.0172, GNorm = 0.2810
Meta loss on this task batch = 2.8863e-01, Meta loss averaged over last 500 steps = 3.2825e-01, PNorm = 111.0254, GNorm = 0.3208
Took 271.3010368347168 seconds to complete one epoch of meta training
Took 278.1727442741394 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466980
Epoch 300
Meta loss on this task batch = 3.5341e-01, Meta loss averaged over last 500 steps = 3.2830e-01, PNorm = 111.0330, GNorm = 0.2616
Meta loss on this task batch = 3.1531e-01, Meta loss averaged over last 500 steps = 3.2832e-01, PNorm = 111.0414, GNorm = 0.2908
Meta loss on this task batch = 3.2766e-01, Meta loss averaged over last 500 steps = 3.2827e-01, PNorm = 111.0501, GNorm = 0.2634
Meta loss on this task batch = 3.8605e-01, Meta loss averaged over last 500 steps = 3.2836e-01, PNorm = 111.0571, GNorm = 0.3409
Meta loss on this task batch = 3.8768e-01, Meta loss averaged over last 500 steps = 3.2855e-01, PNorm = 111.0639, GNorm = 0.3036
Meta loss on this task batch = 3.5944e-01, Meta loss averaged over last 500 steps = 3.2859e-01, PNorm = 111.0711, GNorm = 0.3088
Meta loss on this task batch = 3.0007e-01, Meta loss averaged over last 500 steps = 3.2864e-01, PNorm = 111.0783, GNorm = 0.3225
Meta loss on this task batch = 3.5122e-01, Meta loss averaged over last 500 steps = 3.2870e-01, PNorm = 111.0841, GNorm = 0.2977
Meta loss on this task batch = 2.9722e-01, Meta loss averaged over last 500 steps = 3.2864e-01, PNorm = 111.0907, GNorm = 0.3267
Meta loss on this task batch = 2.5563e-01, Meta loss averaged over last 500 steps = 3.2842e-01, PNorm = 111.0980, GNorm = 0.3340
Meta loss on this task batch = 3.4941e-01, Meta loss averaged over last 500 steps = 3.2848e-01, PNorm = 111.1043, GNorm = 0.3329
Meta loss on this task batch = 3.4093e-01, Meta loss averaged over last 500 steps = 3.2855e-01, PNorm = 111.1107, GNorm = 0.3473
Meta loss on this task batch = 3.2068e-01, Meta loss averaged over last 500 steps = 3.2846e-01, PNorm = 111.1175, GNorm = 0.2594
Meta loss on this task batch = 2.9135e-01, Meta loss averaged over last 500 steps = 3.2835e-01, PNorm = 111.1250, GNorm = 0.2253
Meta loss on this task batch = 3.1332e-01, Meta loss averaged over last 500 steps = 3.2831e-01, PNorm = 111.1323, GNorm = 0.3807
Meta loss on this task batch = 3.4936e-01, Meta loss averaged over last 500 steps = 3.2839e-01, PNorm = 111.1396, GNorm = 0.3072
Meta loss on this task batch = 3.4341e-01, Meta loss averaged over last 500 steps = 3.2846e-01, PNorm = 111.1471, GNorm = 0.3262
Meta loss on this task batch = 2.5966e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 111.1549, GNorm = 0.2484
Meta loss on this task batch = 4.2400e-01, Meta loss averaged over last 500 steps = 3.2850e-01, PNorm = 111.1617, GNorm = 0.3141
Took 113.19361162185669 seconds to complete one epoch of meta training
Took 120.34325051307678 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450296
Epoch 301
Meta loss on this task batch = 3.0507e-01, Meta loss averaged over last 500 steps = 3.2849e-01, PNorm = 111.1686, GNorm = 0.2404
Meta loss on this task batch = 3.1306e-01, Meta loss averaged over last 500 steps = 3.2849e-01, PNorm = 111.1752, GNorm = 0.2880
Meta loss on this task batch = 3.3513e-01, Meta loss averaged over last 500 steps = 3.2852e-01, PNorm = 111.1819, GNorm = 0.3466
Meta loss on this task batch = 3.5513e-01, Meta loss averaged over last 500 steps = 3.2861e-01, PNorm = 111.1886, GNorm = 0.3632
Meta loss on this task batch = 3.1330e-01, Meta loss averaged over last 500 steps = 3.2855e-01, PNorm = 111.1954, GNorm = 0.4087
Meta loss on this task batch = 2.9317e-01, Meta loss averaged over last 500 steps = 3.2850e-01, PNorm = 111.2018, GNorm = 0.3187
Meta loss on this task batch = 2.8826e-01, Meta loss averaged over last 500 steps = 3.2847e-01, PNorm = 111.2087, GNorm = 0.3362
Meta loss on this task batch = 2.9519e-01, Meta loss averaged over last 500 steps = 3.2838e-01, PNorm = 111.2150, GNorm = 0.3235
Meta loss on this task batch = 3.5651e-01, Meta loss averaged over last 500 steps = 3.2844e-01, PNorm = 111.2215, GNorm = 0.4202
Meta loss on this task batch = 3.0791e-01, Meta loss averaged over last 500 steps = 3.2835e-01, PNorm = 111.2281, GNorm = 0.3353
Meta loss on this task batch = 3.3392e-01, Meta loss averaged over last 500 steps = 3.2834e-01, PNorm = 111.2348, GNorm = 0.2510
Meta loss on this task batch = 2.9955e-01, Meta loss averaged over last 500 steps = 3.2832e-01, PNorm = 111.2419, GNorm = 0.2507
Meta loss on this task batch = 3.6150e-01, Meta loss averaged over last 500 steps = 3.2835e-01, PNorm = 111.2500, GNorm = 0.2852
Meta loss on this task batch = 3.2870e-01, Meta loss averaged over last 500 steps = 3.2847e-01, PNorm = 111.2579, GNorm = 0.2816
Meta loss on this task batch = 3.6160e-01, Meta loss averaged over last 500 steps = 3.2856e-01, PNorm = 111.2646, GNorm = 0.3536
Meta loss on this task batch = 3.1522e-01, Meta loss averaged over last 500 steps = 3.2851e-01, PNorm = 111.2714, GNorm = 0.2869
Meta loss on this task batch = 3.7697e-01, Meta loss averaged over last 500 steps = 3.2868e-01, PNorm = 111.2775, GNorm = 0.2959
Meta loss on this task batch = 3.2058e-01, Meta loss averaged over last 500 steps = 3.2864e-01, PNorm = 111.2837, GNorm = 0.4001
Meta loss on this task batch = 3.0469e-01, Meta loss averaged over last 500 steps = 3.2863e-01, PNorm = 111.2905, GNorm = 0.3541
Took 112.62119317054749 seconds to complete one epoch of meta training
Took 120.4661557674408 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473116
Epoch 302
Meta loss on this task batch = 3.2655e-01, Meta loss averaged over last 500 steps = 3.2855e-01, PNorm = 111.2980, GNorm = 0.2714
Meta loss on this task batch = 3.0357e-01, Meta loss averaged over last 500 steps = 3.2841e-01, PNorm = 111.3049, GNorm = 0.2597
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 3.2840e-01, PNorm = 111.3120, GNorm = 0.3078
Meta loss on this task batch = 2.7769e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 111.3191, GNorm = 0.3213
Meta loss on this task batch = 3.2504e-01, Meta loss averaged over last 500 steps = 3.2833e-01, PNorm = 111.3261, GNorm = 0.2650
Meta loss on this task batch = 3.7405e-01, Meta loss averaged over last 500 steps = 3.2845e-01, PNorm = 111.3327, GNorm = 0.3908
Meta loss on this task batch = 3.5474e-01, Meta loss averaged over last 500 steps = 3.2854e-01, PNorm = 111.3392, GNorm = 0.2453
Meta loss on this task batch = 2.6910e-01, Meta loss averaged over last 500 steps = 3.2848e-01, PNorm = 111.3458, GNorm = 0.3016
Meta loss on this task batch = 3.1697e-01, Meta loss averaged over last 500 steps = 3.2848e-01, PNorm = 111.3529, GNorm = 0.4111
Meta loss on this task batch = 3.2974e-01, Meta loss averaged over last 500 steps = 3.2847e-01, PNorm = 111.3601, GNorm = 0.2796
Meta loss on this task batch = 3.2751e-01, Meta loss averaged over last 500 steps = 3.2843e-01, PNorm = 111.3668, GNorm = 0.2581
Meta loss on this task batch = 3.1177e-01, Meta loss averaged over last 500 steps = 3.2834e-01, PNorm = 111.3729, GNorm = 0.3468
Meta loss on this task batch = 3.4847e-01, Meta loss averaged over last 500 steps = 3.2822e-01, PNorm = 111.3784, GNorm = 0.2764
Meta loss on this task batch = 3.1655e-01, Meta loss averaged over last 500 steps = 3.2829e-01, PNorm = 111.3836, GNorm = 0.2985
Meta loss on this task batch = 3.7418e-01, Meta loss averaged over last 500 steps = 3.2838e-01, PNorm = 111.3887, GNorm = 0.2981
Meta loss on this task batch = 3.0948e-01, Meta loss averaged over last 500 steps = 3.2842e-01, PNorm = 111.3941, GNorm = 0.3889
Meta loss on this task batch = 3.3927e-01, Meta loss averaged over last 500 steps = 3.2835e-01, PNorm = 111.3995, GNorm = 0.3043
Meta loss on this task batch = 3.1835e-01, Meta loss averaged over last 500 steps = 3.2826e-01, PNorm = 111.4054, GNorm = 0.3459
Meta loss on this task batch = 3.0284e-01, Meta loss averaged over last 500 steps = 3.2823e-01, PNorm = 111.4115, GNorm = 0.3004
Took 113.9513189792633 seconds to complete one epoch of meta training
Took 122.03591561317444 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456099
Epoch 303
Meta loss on this task batch = 2.7145e-01, Meta loss averaged over last 500 steps = 3.2809e-01, PNorm = 111.4177, GNorm = 0.2731
Meta loss on this task batch = 3.0488e-01, Meta loss averaged over last 500 steps = 3.2805e-01, PNorm = 111.4239, GNorm = 0.2686
Meta loss on this task batch = 3.2788e-01, Meta loss averaged over last 500 steps = 3.2809e-01, PNorm = 111.4309, GNorm = 0.2661
Meta loss on this task batch = 2.8845e-01, Meta loss averaged over last 500 steps = 3.2799e-01, PNorm = 111.4386, GNorm = 0.2398
Meta loss on this task batch = 3.6975e-01, Meta loss averaged over last 500 steps = 3.2806e-01, PNorm = 111.4462, GNorm = 0.4442
Meta loss on this task batch = 3.6457e-01, Meta loss averaged over last 500 steps = 3.2826e-01, PNorm = 111.4525, GNorm = 0.2738
Meta loss on this task batch = 3.3592e-01, Meta loss averaged over last 500 steps = 3.2821e-01, PNorm = 111.4581, GNorm = 0.2613
Meta loss on this task batch = 2.5846e-01, Meta loss averaged over last 500 steps = 3.2806e-01, PNorm = 111.4652, GNorm = 0.3946
Meta loss on this task batch = 3.0625e-01, Meta loss averaged over last 500 steps = 3.2815e-01, PNorm = 111.4717, GNorm = 0.2608
Meta loss on this task batch = 3.4534e-01, Meta loss averaged over last 500 steps = 3.2817e-01, PNorm = 111.4770, GNorm = 0.4175
Meta loss on this task batch = 3.5564e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 111.4826, GNorm = 0.3413
Meta loss on this task batch = 2.6554e-01, Meta loss averaged over last 500 steps = 3.2808e-01, PNorm = 111.4895, GNorm = 0.2444
Meta loss on this task batch = 3.4304e-01, Meta loss averaged over last 500 steps = 3.2809e-01, PNorm = 111.4963, GNorm = 0.2612
Meta loss on this task batch = 2.8560e-01, Meta loss averaged over last 500 steps = 3.2800e-01, PNorm = 111.5032, GNorm = 0.3632
Meta loss on this task batch = 3.0748e-01, Meta loss averaged over last 500 steps = 3.2800e-01, PNorm = 111.5094, GNorm = 0.3020
Meta loss on this task batch = 3.1786e-01, Meta loss averaged over last 500 steps = 3.2797e-01, PNorm = 111.5159, GNorm = 0.3555
Meta loss on this task batch = 3.9289e-01, Meta loss averaged over last 500 steps = 3.2812e-01, PNorm = 111.5217, GNorm = 0.3130
Meta loss on this task batch = 3.2050e-01, Meta loss averaged over last 500 steps = 3.2811e-01, PNorm = 111.5270, GNorm = 0.3120
Meta loss on this task batch = 3.2027e-01, Meta loss averaged over last 500 steps = 3.2790e-01, PNorm = 111.5324, GNorm = 0.5381
Took 111.36334276199341 seconds to complete one epoch of meta training
Took 118.96999859809875 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483792
Epoch 304
Meta loss on this task batch = 2.7430e-01, Meta loss averaged over last 500 steps = 3.2774e-01, PNorm = 111.5384, GNorm = 0.2992
Meta loss on this task batch = 4.0313e-01, Meta loss averaged over last 500 steps = 3.2787e-01, PNorm = 111.5439, GNorm = 0.3302
Meta loss on this task batch = 2.7172e-01, Meta loss averaged over last 500 steps = 3.2772e-01, PNorm = 111.5498, GNorm = 0.2331
Meta loss on this task batch = 3.0169e-01, Meta loss averaged over last 500 steps = 3.2775e-01, PNorm = 111.5554, GNorm = 0.2886
Meta loss on this task batch = 3.3470e-01, Meta loss averaged over last 500 steps = 3.2778e-01, PNorm = 111.5610, GNorm = 0.3877
Meta loss on this task batch = 3.6902e-01, Meta loss averaged over last 500 steps = 3.2800e-01, PNorm = 111.5658, GNorm = 0.4068
Meta loss on this task batch = 3.4757e-01, Meta loss averaged over last 500 steps = 3.2808e-01, PNorm = 111.5698, GNorm = 0.3214
Meta loss on this task batch = 2.9030e-01, Meta loss averaged over last 500 steps = 3.2806e-01, PNorm = 111.5742, GNorm = 0.3752
Meta loss on this task batch = 3.2324e-01, Meta loss averaged over last 500 steps = 3.2804e-01, PNorm = 111.5784, GNorm = 0.2883
Meta loss on this task batch = 3.1478e-01, Meta loss averaged over last 500 steps = 3.2801e-01, PNorm = 111.5834, GNorm = 0.3375
Meta loss on this task batch = 3.3086e-01, Meta loss averaged over last 500 steps = 3.2782e-01, PNorm = 111.5885, GNorm = 0.2866
Meta loss on this task batch = 3.0974e-01, Meta loss averaged over last 500 steps = 3.2784e-01, PNorm = 111.5939, GNorm = 0.2772
Meta loss on this task batch = 2.7884e-01, Meta loss averaged over last 500 steps = 3.2779e-01, PNorm = 111.5997, GNorm = 0.2697
Meta loss on this task batch = 3.2318e-01, Meta loss averaged over last 500 steps = 3.2789e-01, PNorm = 111.6052, GNorm = 0.3647
Meta loss on this task batch = 3.4515e-01, Meta loss averaged over last 500 steps = 3.2787e-01, PNorm = 111.6107, GNorm = 0.2591
Meta loss on this task batch = 3.0756e-01, Meta loss averaged over last 500 steps = 3.2777e-01, PNorm = 111.6167, GNorm = 0.2126
Meta loss on this task batch = 3.3536e-01, Meta loss averaged over last 500 steps = 3.2773e-01, PNorm = 111.6228, GNorm = 0.3095
Meta loss on this task batch = 3.2567e-01, Meta loss averaged over last 500 steps = 3.2785e-01, PNorm = 111.6297, GNorm = 0.3271
Meta loss on this task batch = 3.1108e-01, Meta loss averaged over last 500 steps = 3.2787e-01, PNorm = 111.6370, GNorm = 0.2770
Took 116.61507773399353 seconds to complete one epoch of meta training
Took 124.46809530258179 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474006
Epoch 305
Meta loss on this task batch = 2.8423e-01, Meta loss averaged over last 500 steps = 3.2782e-01, PNorm = 111.6451, GNorm = 0.2566
Meta loss on this task batch = 3.4660e-01, Meta loss averaged over last 500 steps = 3.2780e-01, PNorm = 111.6533, GNorm = 0.2542
Meta loss on this task batch = 3.1053e-01, Meta loss averaged over last 500 steps = 3.2773e-01, PNorm = 111.6613, GNorm = 0.3116
Meta loss on this task batch = 2.9933e-01, Meta loss averaged over last 500 steps = 3.2762e-01, PNorm = 111.6692, GNorm = 0.2352
Meta loss on this task batch = 4.0190e-01, Meta loss averaged over last 500 steps = 3.2759e-01, PNorm = 111.6761, GNorm = 0.3588
Meta loss on this task batch = 3.1806e-01, Meta loss averaged over last 500 steps = 3.2758e-01, PNorm = 111.6832, GNorm = 0.3567
Meta loss on this task batch = 3.3338e-01, Meta loss averaged over last 500 steps = 3.2761e-01, PNorm = 111.6900, GNorm = 0.3651
Meta loss on this task batch = 3.2247e-01, Meta loss averaged over last 500 steps = 3.2772e-01, PNorm = 111.6972, GNorm = 0.2596
Meta loss on this task batch = 3.1762e-01, Meta loss averaged over last 500 steps = 3.2776e-01, PNorm = 111.7044, GNorm = 0.2737
Meta loss on this task batch = 3.5950e-01, Meta loss averaged over last 500 steps = 3.2784e-01, PNorm = 111.7120, GNorm = 0.3096
Meta loss on this task batch = 3.0307e-01, Meta loss averaged over last 500 steps = 3.2781e-01, PNorm = 111.7198, GNorm = 0.3019
Meta loss on this task batch = 3.9193e-01, Meta loss averaged over last 500 steps = 3.2785e-01, PNorm = 111.7266, GNorm = 0.4025
Meta loss on this task batch = 3.5490e-01, Meta loss averaged over last 500 steps = 3.2775e-01, PNorm = 111.7337, GNorm = 0.2711
Meta loss on this task batch = 3.0375e-01, Meta loss averaged over last 500 steps = 3.2776e-01, PNorm = 111.7413, GNorm = 0.2725
Meta loss on this task batch = 2.5584e-01, Meta loss averaged over last 500 steps = 3.2762e-01, PNorm = 111.7499, GNorm = 0.2668
Meta loss on this task batch = 2.6349e-01, Meta loss averaged over last 500 steps = 3.2745e-01, PNorm = 111.7592, GNorm = 0.2633
Meta loss on this task batch = 3.4460e-01, Meta loss averaged over last 500 steps = 3.2758e-01, PNorm = 111.7679, GNorm = 0.2952
Meta loss on this task batch = 3.2529e-01, Meta loss averaged over last 500 steps = 3.2746e-01, PNorm = 111.7765, GNorm = 0.3580
Meta loss on this task batch = 3.4826e-01, Meta loss averaged over last 500 steps = 3.2748e-01, PNorm = 111.7850, GNorm = 0.3110
Took 130.72041535377502 seconds to complete one epoch of meta training
Took 137.6677849292755 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470505
Epoch 306
Meta loss on this task batch = 3.4502e-01, Meta loss averaged over last 500 steps = 3.2750e-01, PNorm = 111.7922, GNorm = 0.3246
Meta loss on this task batch = 3.5221e-01, Meta loss averaged over last 500 steps = 3.2760e-01, PNorm = 111.7995, GNorm = 0.3210
Meta loss on this task batch = 3.0628e-01, Meta loss averaged over last 500 steps = 3.2748e-01, PNorm = 111.8069, GNorm = 0.2812
Meta loss on this task batch = 2.6786e-01, Meta loss averaged over last 500 steps = 3.2745e-01, PNorm = 111.8136, GNorm = 0.2544
Meta loss on this task batch = 2.7317e-01, Meta loss averaged over last 500 steps = 3.2730e-01, PNorm = 111.8206, GNorm = 0.2571
Meta loss on this task batch = 2.9878e-01, Meta loss averaged over last 500 steps = 3.2712e-01, PNorm = 111.8280, GNorm = 0.3049
Meta loss on this task batch = 3.4037e-01, Meta loss averaged over last 500 steps = 3.2719e-01, PNorm = 111.8349, GNorm = 0.3007
Meta loss on this task batch = 3.5463e-01, Meta loss averaged over last 500 steps = 3.2731e-01, PNorm = 111.8420, GNorm = 0.3351
Meta loss on this task batch = 3.4329e-01, Meta loss averaged over last 500 steps = 3.2734e-01, PNorm = 111.8491, GNorm = 0.2726
Meta loss on this task batch = 3.7309e-01, Meta loss averaged over last 500 steps = 3.2741e-01, PNorm = 111.8559, GNorm = 0.3629
Meta loss on this task batch = 3.3113e-01, Meta loss averaged over last 500 steps = 3.2750e-01, PNorm = 111.8631, GNorm = 0.4321
Meta loss on this task batch = 3.0743e-01, Meta loss averaged over last 500 steps = 3.2741e-01, PNorm = 111.8698, GNorm = 0.3961
Meta loss on this task batch = 3.4455e-01, Meta loss averaged over last 500 steps = 3.2749e-01, PNorm = 111.8765, GNorm = 0.3393
Meta loss on this task batch = 3.5546e-01, Meta loss averaged over last 500 steps = 3.2752e-01, PNorm = 111.8831, GNorm = 0.3452
Meta loss on this task batch = 3.4536e-01, Meta loss averaged over last 500 steps = 3.2759e-01, PNorm = 111.8893, GNorm = 0.2490
Meta loss on this task batch = 3.0083e-01, Meta loss averaged over last 500 steps = 3.2736e-01, PNorm = 111.8962, GNorm = 0.2766
Meta loss on this task batch = 3.3628e-01, Meta loss averaged over last 500 steps = 3.2732e-01, PNorm = 111.9028, GNorm = 0.2410
Meta loss on this task batch = 2.5508e-01, Meta loss averaged over last 500 steps = 3.2708e-01, PNorm = 111.9099, GNorm = 0.3161
Meta loss on this task batch = 4.0041e-01, Meta loss averaged over last 500 steps = 3.2723e-01, PNorm = 111.9160, GNorm = 0.5447
Took 123.72807288169861 seconds to complete one epoch of meta training
Took 131.7102246284485 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471683
Epoch 307
Meta loss on this task batch = 2.8717e-01, Meta loss averaged over last 500 steps = 3.2709e-01, PNorm = 111.9228, GNorm = 0.2480
Meta loss on this task batch = 3.1253e-01, Meta loss averaged over last 500 steps = 3.2697e-01, PNorm = 111.9292, GNorm = 0.4263
Meta loss on this task batch = 3.1120e-01, Meta loss averaged over last 500 steps = 3.2693e-01, PNorm = 111.9351, GNorm = 0.3984
Meta loss on this task batch = 3.7068e-01, Meta loss averaged over last 500 steps = 3.2701e-01, PNorm = 111.9413, GNorm = 0.3042
Meta loss on this task batch = 3.2706e-01, Meta loss averaged over last 500 steps = 3.2697e-01, PNorm = 111.9480, GNorm = 0.2995
Meta loss on this task batch = 3.1911e-01, Meta loss averaged over last 500 steps = 3.2696e-01, PNorm = 111.9560, GNorm = 0.2581
Meta loss on this task batch = 3.4846e-01, Meta loss averaged over last 500 steps = 3.2692e-01, PNorm = 111.9631, GNorm = 0.2715
Meta loss on this task batch = 3.6452e-01, Meta loss averaged over last 500 steps = 3.2707e-01, PNorm = 111.9698, GNorm = 0.2525
Meta loss on this task batch = 3.2757e-01, Meta loss averaged over last 500 steps = 3.2716e-01, PNorm = 111.9769, GNorm = 0.3086
Meta loss on this task batch = 3.5403e-01, Meta loss averaged over last 500 steps = 3.2716e-01, PNorm = 111.9844, GNorm = 0.2990
Meta loss on this task batch = 3.1523e-01, Meta loss averaged over last 500 steps = 3.2722e-01, PNorm = 111.9926, GNorm = 0.2383
Meta loss on this task batch = 2.8846e-01, Meta loss averaged over last 500 steps = 3.2712e-01, PNorm = 112.0007, GNorm = 0.2466
Meta loss on this task batch = 3.1708e-01, Meta loss averaged over last 500 steps = 3.2699e-01, PNorm = 112.0098, GNorm = 0.3134
Meta loss on this task batch = 3.8252e-01, Meta loss averaged over last 500 steps = 3.2721e-01, PNorm = 112.0186, GNorm = 0.3153
Meta loss on this task batch = 3.3165e-01, Meta loss averaged over last 500 steps = 3.2732e-01, PNorm = 112.0277, GNorm = 0.4644
Meta loss on this task batch = 3.0100e-01, Meta loss averaged over last 500 steps = 3.2727e-01, PNorm = 112.0366, GNorm = 0.2653
Meta loss on this task batch = 3.0479e-01, Meta loss averaged over last 500 steps = 3.2722e-01, PNorm = 112.0448, GNorm = 0.2710
Meta loss on this task batch = 3.3345e-01, Meta loss averaged over last 500 steps = 3.2726e-01, PNorm = 112.0527, GNorm = 0.4050
Meta loss on this task batch = 3.4323e-01, Meta loss averaged over last 500 steps = 3.2729e-01, PNorm = 112.0603, GNorm = 0.3023
Took 123.44268679618835 seconds to complete one epoch of meta training
Took 131.31705951690674 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466523
Epoch 308
Meta loss on this task batch = 3.4740e-01, Meta loss averaged over last 500 steps = 3.2732e-01, PNorm = 112.0670, GNorm = 0.2755
Meta loss on this task batch = 3.5110e-01, Meta loss averaged over last 500 steps = 3.2729e-01, PNorm = 112.0733, GNorm = 0.3738
Meta loss on this task batch = 2.9169e-01, Meta loss averaged over last 500 steps = 3.2724e-01, PNorm = 112.0803, GNorm = 0.2861
Meta loss on this task batch = 2.9520e-01, Meta loss averaged over last 500 steps = 3.2712e-01, PNorm = 112.0876, GNorm = 0.2894
Meta loss on this task batch = 3.3622e-01, Meta loss averaged over last 500 steps = 3.2709e-01, PNorm = 112.0939, GNorm = 0.2781
Meta loss on this task batch = 3.1161e-01, Meta loss averaged over last 500 steps = 3.2703e-01, PNorm = 112.1008, GNorm = 0.2860
Meta loss on this task batch = 3.4193e-01, Meta loss averaged over last 500 steps = 3.2707e-01, PNorm = 112.1088, GNorm = 0.2465
Meta loss on this task batch = 3.4357e-01, Meta loss averaged over last 500 steps = 3.2711e-01, PNorm = 112.1174, GNorm = 0.5119
Meta loss on this task batch = 3.4737e-01, Meta loss averaged over last 500 steps = 3.2722e-01, PNorm = 112.1251, GNorm = 0.3353
Meta loss on this task batch = 3.3667e-01, Meta loss averaged over last 500 steps = 3.2730e-01, PNorm = 112.1331, GNorm = 0.2422
Meta loss on this task batch = 3.3181e-01, Meta loss averaged over last 500 steps = 3.2735e-01, PNorm = 112.1401, GNorm = 0.2886
Meta loss on this task batch = 2.9282e-01, Meta loss averaged over last 500 steps = 3.2730e-01, PNorm = 112.1477, GNorm = 0.2425
Meta loss on this task batch = 3.0239e-01, Meta loss averaged over last 500 steps = 3.2724e-01, PNorm = 112.1557, GNorm = 0.2499
Meta loss on this task batch = 3.3340e-01, Meta loss averaged over last 500 steps = 3.2724e-01, PNorm = 112.1640, GNorm = 0.3147
Meta loss on this task batch = 2.8332e-01, Meta loss averaged over last 500 steps = 3.2725e-01, PNorm = 112.1722, GNorm = 0.3223
Meta loss on this task batch = 3.5281e-01, Meta loss averaged over last 500 steps = 3.2727e-01, PNorm = 112.1804, GNorm = 0.2829
Meta loss on this task batch = 3.3995e-01, Meta loss averaged over last 500 steps = 3.2733e-01, PNorm = 112.1882, GNorm = 0.4580
Meta loss on this task batch = 2.8326e-01, Meta loss averaged over last 500 steps = 3.2722e-01, PNorm = 112.1955, GNorm = 0.2696
Meta loss on this task batch = 3.5158e-01, Meta loss averaged over last 500 steps = 3.2733e-01, PNorm = 112.2026, GNorm = 0.4161
Took 136.3759458065033 seconds to complete one epoch of meta training
Took 143.99649024009705 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468964
Epoch 309
Meta loss on this task batch = 3.8256e-01, Meta loss averaged over last 500 steps = 3.2747e-01, PNorm = 112.2090, GNorm = 0.3038
Meta loss on this task batch = 2.6087e-01, Meta loss averaged over last 500 steps = 3.2740e-01, PNorm = 112.2162, GNorm = 0.2169
Meta loss on this task batch = 2.7823e-01, Meta loss averaged over last 500 steps = 3.2730e-01, PNorm = 112.2235, GNorm = 0.3140
Meta loss on this task batch = 3.1838e-01, Meta loss averaged over last 500 steps = 3.2729e-01, PNorm = 112.2308, GNorm = 0.2429
Meta loss on this task batch = 3.5579e-01, Meta loss averaged over last 500 steps = 3.2730e-01, PNorm = 112.2382, GNorm = 0.2660
Meta loss on this task batch = 3.7879e-01, Meta loss averaged over last 500 steps = 3.2742e-01, PNorm = 112.2454, GNorm = 0.2762
Meta loss on this task batch = 3.0275e-01, Meta loss averaged over last 500 steps = 3.2731e-01, PNorm = 112.2534, GNorm = 0.2393
Meta loss on this task batch = 3.1172e-01, Meta loss averaged over last 500 steps = 3.2730e-01, PNorm = 112.2616, GNorm = 0.2578
Meta loss on this task batch = 2.9111e-01, Meta loss averaged over last 500 steps = 3.2722e-01, PNorm = 112.2695, GNorm = 0.3975
Meta loss on this task batch = 3.5541e-01, Meta loss averaged over last 500 steps = 3.2727e-01, PNorm = 112.2766, GNorm = 0.2567
Meta loss on this task batch = 3.5853e-01, Meta loss averaged over last 500 steps = 3.2739e-01, PNorm = 112.2827, GNorm = 0.2993
Meta loss on this task batch = 3.1640e-01, Meta loss averaged over last 500 steps = 3.2732e-01, PNorm = 112.2886, GNorm = 0.2434
Meta loss on this task batch = 3.6859e-01, Meta loss averaged over last 500 steps = 3.2741e-01, PNorm = 112.2936, GNorm = 0.2974
Meta loss on this task batch = 3.3433e-01, Meta loss averaged over last 500 steps = 3.2736e-01, PNorm = 112.2987, GNorm = 0.2391
Meta loss on this task batch = 2.9359e-01, Meta loss averaged over last 500 steps = 3.2735e-01, PNorm = 112.3045, GNorm = 0.2313
Meta loss on this task batch = 3.7848e-01, Meta loss averaged over last 500 steps = 3.2737e-01, PNorm = 112.3105, GNorm = 0.2882
Meta loss on this task batch = 2.9730e-01, Meta loss averaged over last 500 steps = 3.2735e-01, PNorm = 112.3172, GNorm = 0.3032
Meta loss on this task batch = 3.1001e-01, Meta loss averaged over last 500 steps = 3.2724e-01, PNorm = 112.3242, GNorm = 0.2633
Meta loss on this task batch = 3.1387e-01, Meta loss averaged over last 500 steps = 3.2715e-01, PNorm = 112.3317, GNorm = 0.3437
Took 134.50734305381775 seconds to complete one epoch of meta training
Took 142.24551439285278 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457507
Epoch 310
Meta loss on this task batch = 2.9945e-01, Meta loss averaged over last 500 steps = 3.2705e-01, PNorm = 112.3391, GNorm = 0.3560
Meta loss on this task batch = 3.0419e-01, Meta loss averaged over last 500 steps = 3.2698e-01, PNorm = 112.3471, GNorm = 0.2561
Meta loss on this task batch = 3.7157e-01, Meta loss averaged over last 500 steps = 3.2714e-01, PNorm = 112.3566, GNorm = 0.4792
Meta loss on this task batch = 3.3059e-01, Meta loss averaged over last 500 steps = 3.2718e-01, PNorm = 112.3663, GNorm = 0.3210
Meta loss on this task batch = 2.9930e-01, Meta loss averaged over last 500 steps = 3.2705e-01, PNorm = 112.3767, GNorm = 0.3672
Meta loss on this task batch = 3.0268e-01, Meta loss averaged over last 500 steps = 3.2711e-01, PNorm = 112.3866, GNorm = 0.2912
Meta loss on this task batch = 3.0039e-01, Meta loss averaged over last 500 steps = 3.2703e-01, PNorm = 112.3961, GNorm = 0.3190
Meta loss on this task batch = 3.7214e-01, Meta loss averaged over last 500 steps = 3.2707e-01, PNorm = 112.4042, GNorm = 0.3280
Meta loss on this task batch = 3.3223e-01, Meta loss averaged over last 500 steps = 3.2696e-01, PNorm = 112.4118, GNorm = 0.2787
Meta loss on this task batch = 3.4324e-01, Meta loss averaged over last 500 steps = 3.2708e-01, PNorm = 112.4189, GNorm = 0.2542
Meta loss on this task batch = 2.8990e-01, Meta loss averaged over last 500 steps = 3.2689e-01, PNorm = 112.4255, GNorm = 0.4045
Meta loss on this task batch = 3.5272e-01, Meta loss averaged over last 500 steps = 3.2696e-01, PNorm = 112.4328, GNorm = 0.2632
Meta loss on this task batch = 2.6211e-01, Meta loss averaged over last 500 steps = 3.2687e-01, PNorm = 112.4412, GNorm = 0.3533
Meta loss on this task batch = 2.8587e-01, Meta loss averaged over last 500 steps = 3.2668e-01, PNorm = 112.4501, GNorm = 0.3148
Meta loss on this task batch = 3.0286e-01, Meta loss averaged over last 500 steps = 3.2658e-01, PNorm = 112.4593, GNorm = 0.4029
Meta loss on this task batch = 3.8533e-01, Meta loss averaged over last 500 steps = 3.2653e-01, PNorm = 112.4679, GNorm = 0.3069
Meta loss on this task batch = 3.3902e-01, Meta loss averaged over last 500 steps = 3.2652e-01, PNorm = 112.4761, GNorm = 0.2611
Meta loss on this task batch = 3.3125e-01, Meta loss averaged over last 500 steps = 3.2647e-01, PNorm = 112.4841, GNorm = 0.3977
Meta loss on this task batch = 3.4991e-01, Meta loss averaged over last 500 steps = 3.2650e-01, PNorm = 112.4915, GNorm = 0.2861
Took 128.5537073612213 seconds to complete one epoch of meta training
Took 136.1779043674469 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.442581
Epoch 311
Meta loss on this task batch = 2.6063e-01, Meta loss averaged over last 500 steps = 3.2635e-01, PNorm = 112.4985, GNorm = 0.2718
Meta loss on this task batch = 2.9651e-01, Meta loss averaged over last 500 steps = 3.2639e-01, PNorm = 112.5061, GNorm = 0.2208
Meta loss on this task batch = 3.3872e-01, Meta loss averaged over last 500 steps = 3.2657e-01, PNorm = 112.5138, GNorm = 0.3927
Meta loss on this task batch = 3.4400e-01, Meta loss averaged over last 500 steps = 3.2665e-01, PNorm = 112.5217, GNorm = 0.2512
Meta loss on this task batch = 3.8268e-01, Meta loss averaged over last 500 steps = 3.2685e-01, PNorm = 112.5296, GNorm = 0.3919
Meta loss on this task batch = 3.2305e-01, Meta loss averaged over last 500 steps = 3.2688e-01, PNorm = 112.5378, GNorm = 0.2396
Meta loss on this task batch = 3.4026e-01, Meta loss averaged over last 500 steps = 3.2688e-01, PNorm = 112.5449, GNorm = 0.2622
Meta loss on this task batch = 3.3897e-01, Meta loss averaged over last 500 steps = 3.2694e-01, PNorm = 112.5521, GNorm = 0.2503
Meta loss on this task batch = 3.1199e-01, Meta loss averaged over last 500 steps = 3.2693e-01, PNorm = 112.5593, GNorm = 0.2811
Meta loss on this task batch = 3.2806e-01, Meta loss averaged over last 500 steps = 3.2688e-01, PNorm = 112.5662, GNorm = 0.2403
Meta loss on this task batch = 2.9706e-01, Meta loss averaged over last 500 steps = 3.2678e-01, PNorm = 112.5734, GNorm = 0.2330
Meta loss on this task batch = 2.8900e-01, Meta loss averaged over last 500 steps = 3.2662e-01, PNorm = 112.5800, GNorm = 0.2713
Meta loss on this task batch = 3.2136e-01, Meta loss averaged over last 500 steps = 3.2662e-01, PNorm = 112.5867, GNorm = 0.2808
Meta loss on this task batch = 3.9116e-01, Meta loss averaged over last 500 steps = 3.2682e-01, PNorm = 112.5927, GNorm = 0.4809
Meta loss on this task batch = 3.4841e-01, Meta loss averaged over last 500 steps = 3.2675e-01, PNorm = 112.5987, GNorm = 0.3141
Meta loss on this task batch = 3.2212e-01, Meta loss averaged over last 500 steps = 3.2669e-01, PNorm = 112.6052, GNorm = 0.2625
Meta loss on this task batch = 2.8565e-01, Meta loss averaged over last 500 steps = 3.2660e-01, PNorm = 112.6114, GNorm = 0.3119
Meta loss on this task batch = 3.0608e-01, Meta loss averaged over last 500 steps = 3.2652e-01, PNorm = 112.6184, GNorm = 0.2888
Meta loss on this task batch = 3.2293e-01, Meta loss averaged over last 500 steps = 3.2651e-01, PNorm = 112.6250, GNorm = 0.2991
Took 129.84843158721924 seconds to complete one epoch of meta training
Took 137.72330045700073 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473852
Epoch 312
Meta loss on this task batch = 3.3215e-01, Meta loss averaged over last 500 steps = 3.2652e-01, PNorm = 112.6316, GNorm = 0.2997
Meta loss on this task batch = 3.1631e-01, Meta loss averaged over last 500 steps = 3.2650e-01, PNorm = 112.6378, GNorm = 0.2347
Meta loss on this task batch = 3.0040e-01, Meta loss averaged over last 500 steps = 3.2653e-01, PNorm = 112.6432, GNorm = 0.4011
Meta loss on this task batch = 3.5315e-01, Meta loss averaged over last 500 steps = 3.2655e-01, PNorm = 112.6481, GNorm = 0.2708
Meta loss on this task batch = 3.4305e-01, Meta loss averaged over last 500 steps = 3.2666e-01, PNorm = 112.6528, GNorm = 0.2943
Meta loss on this task batch = 2.8157e-01, Meta loss averaged over last 500 steps = 3.2659e-01, PNorm = 112.6584, GNorm = 0.2305
Meta loss on this task batch = 3.4174e-01, Meta loss averaged over last 500 steps = 3.2655e-01, PNorm = 112.6645, GNorm = 0.3338
Meta loss on this task batch = 3.2112e-01, Meta loss averaged over last 500 steps = 3.2658e-01, PNorm = 112.6704, GNorm = 0.3497
Meta loss on this task batch = 3.0416e-01, Meta loss averaged over last 500 steps = 3.2651e-01, PNorm = 112.6772, GNorm = 0.2530
Meta loss on this task batch = 3.8179e-01, Meta loss averaged over last 500 steps = 3.2644e-01, PNorm = 112.6839, GNorm = 0.3000
Meta loss on this task batch = 3.1004e-01, Meta loss averaged over last 500 steps = 3.2641e-01, PNorm = 112.6900, GNorm = 0.2917
Meta loss on this task batch = 3.7175e-01, Meta loss averaged over last 500 steps = 3.2660e-01, PNorm = 112.6967, GNorm = 0.3660
Meta loss on this task batch = 3.9506e-01, Meta loss averaged over last 500 steps = 3.2660e-01, PNorm = 112.7023, GNorm = 0.2798
Meta loss on this task batch = 2.9326e-01, Meta loss averaged over last 500 steps = 3.2663e-01, PNorm = 112.7084, GNorm = 0.3528
Meta loss on this task batch = 3.5012e-01, Meta loss averaged over last 500 steps = 3.2669e-01, PNorm = 112.7150, GNorm = 0.3186
Meta loss on this task batch = 3.1658e-01, Meta loss averaged over last 500 steps = 3.2663e-01, PNorm = 112.7230, GNorm = 0.3441
Meta loss on this task batch = 2.7722e-01, Meta loss averaged over last 500 steps = 3.2656e-01, PNorm = 112.7317, GNorm = 0.2327
Meta loss on this task batch = 3.5190e-01, Meta loss averaged over last 500 steps = 3.2658e-01, PNorm = 112.7396, GNorm = 0.2702
Meta loss on this task batch = 2.3448e-01, Meta loss averaged over last 500 steps = 3.2635e-01, PNorm = 112.7490, GNorm = 0.3525
Took 136.26557970046997 seconds to complete one epoch of meta training
Took 144.27698349952698 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448269
Epoch 313
Meta loss on this task batch = 3.2735e-01, Meta loss averaged over last 500 steps = 3.2616e-01, PNorm = 112.7592, GNorm = 0.2701
Meta loss on this task batch = 3.0795e-01, Meta loss averaged over last 500 steps = 3.2610e-01, PNorm = 112.7693, GNorm = 0.2383
Meta loss on this task batch = 3.9453e-01, Meta loss averaged over last 500 steps = 3.2624e-01, PNorm = 112.7783, GNorm = 0.4714
Meta loss on this task batch = 3.1980e-01, Meta loss averaged over last 500 steps = 3.2621e-01, PNorm = 112.7870, GNorm = 0.2896
Meta loss on this task batch = 3.1211e-01, Meta loss averaged over last 500 steps = 3.2625e-01, PNorm = 112.7953, GNorm = 0.2706
Meta loss on this task batch = 3.4288e-01, Meta loss averaged over last 500 steps = 3.2629e-01, PNorm = 112.8029, GNorm = 0.3437
Meta loss on this task batch = 4.0799e-01, Meta loss averaged over last 500 steps = 3.2644e-01, PNorm = 112.8086, GNorm = 0.3408
Meta loss on this task batch = 3.1146e-01, Meta loss averaged over last 500 steps = 3.2635e-01, PNorm = 112.8148, GNorm = 0.2231
Meta loss on this task batch = 3.2065e-01, Meta loss averaged over last 500 steps = 3.2636e-01, PNorm = 112.8211, GNorm = 0.2864
Meta loss on this task batch = 3.4785e-01, Meta loss averaged over last 500 steps = 3.2637e-01, PNorm = 112.8280, GNorm = 0.3511
Meta loss on this task batch = 2.7065e-01, Meta loss averaged over last 500 steps = 3.2634e-01, PNorm = 112.8361, GNorm = 0.2564
Meta loss on this task batch = 2.9802e-01, Meta loss averaged over last 500 steps = 3.2638e-01, PNorm = 112.8457, GNorm = 0.3584
Meta loss on this task batch = 2.6697e-01, Meta loss averaged over last 500 steps = 3.2634e-01, PNorm = 112.8555, GNorm = 0.2749
Meta loss on this task batch = 2.6111e-01, Meta loss averaged over last 500 steps = 3.2620e-01, PNorm = 112.8657, GNorm = 0.2293
Meta loss on this task batch = 3.6068e-01, Meta loss averaged over last 500 steps = 3.2628e-01, PNorm = 112.8750, GNorm = 0.4081
Meta loss on this task batch = 2.8519e-01, Meta loss averaged over last 500 steps = 3.2615e-01, PNorm = 112.8853, GNorm = 0.3472
Meta loss on this task batch = 3.5926e-01, Meta loss averaged over last 500 steps = 3.2624e-01, PNorm = 112.8946, GNorm = 0.3989
Meta loss on this task batch = 2.6991e-01, Meta loss averaged over last 500 steps = 3.2615e-01, PNorm = 112.9044, GNorm = 0.2954
Meta loss on this task batch = 3.0061e-01, Meta loss averaged over last 500 steps = 3.2604e-01, PNorm = 112.9142, GNorm = 0.3279
Took 127.4866156578064 seconds to complete one epoch of meta training
Took 134.5023365020752 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460570
Epoch 314
Meta loss on this task batch = 3.3194e-01, Meta loss averaged over last 500 steps = 3.2602e-01, PNorm = 112.9222, GNorm = 0.5498
Meta loss on this task batch = 2.4052e-01, Meta loss averaged over last 500 steps = 3.2579e-01, PNorm = 112.9310, GNorm = 0.3277
Meta loss on this task batch = 3.4199e-01, Meta loss averaged over last 500 steps = 3.2586e-01, PNorm = 112.9395, GNorm = 0.2877
Meta loss on this task batch = 3.1633e-01, Meta loss averaged over last 500 steps = 3.2587e-01, PNorm = 112.9472, GNorm = 0.2727
Meta loss on this task batch = 3.2906e-01, Meta loss averaged over last 500 steps = 3.2581e-01, PNorm = 112.9540, GNorm = 0.2588
Meta loss on this task batch = 3.1836e-01, Meta loss averaged over last 500 steps = 3.2570e-01, PNorm = 112.9615, GNorm = 0.3802
Meta loss on this task batch = 2.9126e-01, Meta loss averaged over last 500 steps = 3.2558e-01, PNorm = 112.9688, GNorm = 0.3077
Meta loss on this task batch = 3.7945e-01, Meta loss averaged over last 500 steps = 3.2572e-01, PNorm = 112.9750, GNorm = 0.3265
Meta loss on this task batch = 3.2557e-01, Meta loss averaged over last 500 steps = 3.2563e-01, PNorm = 112.9813, GNorm = 0.2890
Meta loss on this task batch = 3.1034e-01, Meta loss averaged over last 500 steps = 3.2558e-01, PNorm = 112.9874, GNorm = 0.2464
Meta loss on this task batch = 3.2512e-01, Meta loss averaged over last 500 steps = 3.2559e-01, PNorm = 112.9941, GNorm = 0.2970
Meta loss on this task batch = 3.5203e-01, Meta loss averaged over last 500 steps = 3.2573e-01, PNorm = 113.0009, GNorm = 0.2736
Meta loss on this task batch = 3.2158e-01, Meta loss averaged over last 500 steps = 3.2567e-01, PNorm = 113.0086, GNorm = 0.2482
Meta loss on this task batch = 2.9704e-01, Meta loss averaged over last 500 steps = 3.2570e-01, PNorm = 113.0159, GNorm = 0.3019
Meta loss on this task batch = 3.3645e-01, Meta loss averaged over last 500 steps = 3.2574e-01, PNorm = 113.0230, GNorm = 0.3117
Meta loss on this task batch = 3.4523e-01, Meta loss averaged over last 500 steps = 3.2585e-01, PNorm = 113.0303, GNorm = 0.3312
Meta loss on this task batch = 3.1516e-01, Meta loss averaged over last 500 steps = 3.2583e-01, PNorm = 113.0377, GNorm = 0.2658
Meta loss on this task batch = 3.0552e-01, Meta loss averaged over last 500 steps = 3.2578e-01, PNorm = 113.0448, GNorm = 0.3195
Meta loss on this task batch = 3.6368e-01, Meta loss averaged over last 500 steps = 3.2574e-01, PNorm = 113.0514, GNorm = 0.3334
Took 132.92256784439087 seconds to complete one epoch of meta training
Took 141.29909753799438 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463816
Epoch 315
Meta loss on this task batch = 3.3357e-01, Meta loss averaged over last 500 steps = 3.2571e-01, PNorm = 113.0587, GNorm = 0.2979
Meta loss on this task batch = 3.4209e-01, Meta loss averaged over last 500 steps = 3.2566e-01, PNorm = 113.0656, GNorm = 0.3001
Meta loss on this task batch = 3.0713e-01, Meta loss averaged over last 500 steps = 3.2551e-01, PNorm = 113.0731, GNorm = 0.2523
Meta loss on this task batch = 3.4406e-01, Meta loss averaged over last 500 steps = 3.2557e-01, PNorm = 113.0811, GNorm = 0.2798
Meta loss on this task batch = 2.7738e-01, Meta loss averaged over last 500 steps = 3.2541e-01, PNorm = 113.0891, GNorm = 0.2395
Meta loss on this task batch = 3.3152e-01, Meta loss averaged over last 500 steps = 3.2543e-01, PNorm = 113.0965, GNorm = 0.3257
Meta loss on this task batch = 3.4252e-01, Meta loss averaged over last 500 steps = 3.2541e-01, PNorm = 113.1035, GNorm = 0.2994
Meta loss on this task batch = 2.5495e-01, Meta loss averaged over last 500 steps = 3.2517e-01, PNorm = 113.1115, GNorm = 0.2746
Meta loss on this task batch = 3.3909e-01, Meta loss averaged over last 500 steps = 3.2524e-01, PNorm = 113.1175, GNorm = 0.3363
Meta loss on this task batch = 3.0840e-01, Meta loss averaged over last 500 steps = 3.2528e-01, PNorm = 113.1237, GNorm = 0.3717
Meta loss on this task batch = 3.6846e-01, Meta loss averaged over last 500 steps = 3.2535e-01, PNorm = 113.1304, GNorm = 0.2915
Meta loss on this task batch = 3.1936e-01, Meta loss averaged over last 500 steps = 3.2542e-01, PNorm = 113.1377, GNorm = 0.2743
Meta loss on this task batch = 3.6023e-01, Meta loss averaged over last 500 steps = 3.2551e-01, PNorm = 113.1456, GNorm = 0.3257
Meta loss on this task batch = 3.2461e-01, Meta loss averaged over last 500 steps = 3.2539e-01, PNorm = 113.1536, GNorm = 0.3057
Meta loss on this task batch = 3.4188e-01, Meta loss averaged over last 500 steps = 3.2549e-01, PNorm = 113.1607, GNorm = 0.2856
Meta loss on this task batch = 3.4713e-01, Meta loss averaged over last 500 steps = 3.2547e-01, PNorm = 113.1684, GNorm = 0.4647
Meta loss on this task batch = 3.3162e-01, Meta loss averaged over last 500 steps = 3.2547e-01, PNorm = 113.1765, GNorm = 0.3879
Meta loss on this task batch = 3.1189e-01, Meta loss averaged over last 500 steps = 3.2525e-01, PNorm = 113.1849, GNorm = 0.3016
Meta loss on this task batch = 2.8642e-01, Meta loss averaged over last 500 steps = 3.2523e-01, PNorm = 113.1940, GNorm = 0.3136
Took 121.89875650405884 seconds to complete one epoch of meta training
Took 129.74116611480713 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462235
Epoch 316
Meta loss on this task batch = 2.6754e-01, Meta loss averaged over last 500 steps = 3.2508e-01, PNorm = 113.2036, GNorm = 0.3116
Meta loss on this task batch = 3.0791e-01, Meta loss averaged over last 500 steps = 3.2503e-01, PNorm = 113.2134, GNorm = 0.2997
Meta loss on this task batch = 3.8889e-01, Meta loss averaged over last 500 steps = 3.2521e-01, PNorm = 113.2230, GNorm = 0.2664
Meta loss on this task batch = 3.5418e-01, Meta loss averaged over last 500 steps = 3.2527e-01, PNorm = 113.2327, GNorm = 0.4102
Meta loss on this task batch = 3.3870e-01, Meta loss averaged over last 500 steps = 3.2529e-01, PNorm = 113.2419, GNorm = 0.2542
Meta loss on this task batch = 4.1269e-01, Meta loss averaged over last 500 steps = 3.2555e-01, PNorm = 113.2486, GNorm = 0.3887
Meta loss on this task batch = 3.0210e-01, Meta loss averaged over last 500 steps = 3.2545e-01, PNorm = 113.2553, GNorm = 0.2521
Meta loss on this task batch = 3.5969e-01, Meta loss averaged over last 500 steps = 3.2553e-01, PNorm = 113.2620, GNorm = 0.3066
Meta loss on this task batch = 3.5783e-01, Meta loss averaged over last 500 steps = 3.2564e-01, PNorm = 113.2676, GNorm = 0.3163
Meta loss on this task batch = 3.7324e-01, Meta loss averaged over last 500 steps = 3.2576e-01, PNorm = 113.2730, GNorm = 0.3227
Meta loss on this task batch = 3.4779e-01, Meta loss averaged over last 500 steps = 3.2584e-01, PNorm = 113.2784, GNorm = 0.2865
Meta loss on this task batch = 2.7356e-01, Meta loss averaged over last 500 steps = 3.2572e-01, PNorm = 113.2844, GNorm = 0.4733
Meta loss on this task batch = 3.1636e-01, Meta loss averaged over last 500 steps = 3.2569e-01, PNorm = 113.2906, GNorm = 0.3638
Meta loss on this task batch = 2.9900e-01, Meta loss averaged over last 500 steps = 3.2551e-01, PNorm = 113.2966, GNorm = 0.2719
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 3.2540e-01, PNorm = 113.3030, GNorm = 0.2646
Meta loss on this task batch = 2.7216e-01, Meta loss averaged over last 500 steps = 3.2531e-01, PNorm = 113.3096, GNorm = 0.2535
Meta loss on this task batch = 2.9555e-01, Meta loss averaged over last 500 steps = 3.2516e-01, PNorm = 113.3160, GNorm = 0.2944
Meta loss on this task batch = 2.8646e-01, Meta loss averaged over last 500 steps = 3.2519e-01, PNorm = 113.3226, GNorm = 0.2293
Meta loss on this task batch = 3.8683e-01, Meta loss averaged over last 500 steps = 3.2524e-01, PNorm = 113.3284, GNorm = 0.4500
Took 118.99025964736938 seconds to complete one epoch of meta training
Took 126.7133457660675 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487274
Epoch 317
Meta loss on this task batch = 3.3837e-01, Meta loss averaged over last 500 steps = 3.2515e-01, PNorm = 113.3334, GNorm = 0.3001
Meta loss on this task batch = 3.0495e-01, Meta loss averaged over last 500 steps = 3.2514e-01, PNorm = 113.3383, GNorm = 0.2621
Meta loss on this task batch = 3.0137e-01, Meta loss averaged over last 500 steps = 3.2517e-01, PNorm = 113.3424, GNorm = 0.2916
Meta loss on this task batch = 3.1935e-01, Meta loss averaged over last 500 steps = 3.2513e-01, PNorm = 113.3468, GNorm = 0.2551
Meta loss on this task batch = 2.7022e-01, Meta loss averaged over last 500 steps = 3.2500e-01, PNorm = 113.3519, GNorm = 0.2395
Meta loss on this task batch = 3.5697e-01, Meta loss averaged over last 500 steps = 3.2502e-01, PNorm = 113.3559, GNorm = 0.3611
Meta loss on this task batch = 2.9442e-01, Meta loss averaged over last 500 steps = 3.2491e-01, PNorm = 113.3600, GNorm = 0.2809
Meta loss on this task batch = 2.9115e-01, Meta loss averaged over last 500 steps = 3.2486e-01, PNorm = 113.3646, GNorm = 0.2289
Meta loss on this task batch = 2.8830e-01, Meta loss averaged over last 500 steps = 3.2478e-01, PNorm = 113.3696, GNorm = 0.2714
Meta loss on this task batch = 3.4272e-01, Meta loss averaged over last 500 steps = 3.2482e-01, PNorm = 113.3741, GNorm = 0.3703
Meta loss on this task batch = 3.4287e-01, Meta loss averaged over last 500 steps = 3.2482e-01, PNorm = 113.3790, GNorm = 0.3016
Meta loss on this task batch = 3.1654e-01, Meta loss averaged over last 500 steps = 3.2478e-01, PNorm = 113.3851, GNorm = 0.2523
Meta loss on this task batch = 3.2047e-01, Meta loss averaged over last 500 steps = 3.2474e-01, PNorm = 113.3912, GNorm = 0.3764
Meta loss on this task batch = 3.8863e-01, Meta loss averaged over last 500 steps = 3.2490e-01, PNorm = 113.3964, GNorm = 0.3476
Meta loss on this task batch = 2.6084e-01, Meta loss averaged over last 500 steps = 3.2480e-01, PNorm = 113.4019, GNorm = 0.2154
Meta loss on this task batch = 3.6444e-01, Meta loss averaged over last 500 steps = 3.2501e-01, PNorm = 113.4081, GNorm = 0.3624
Meta loss on this task batch = 3.3209e-01, Meta loss averaged over last 500 steps = 3.2498e-01, PNorm = 113.4144, GNorm = 0.3175
Meta loss on this task batch = 2.7787e-01, Meta loss averaged over last 500 steps = 3.2490e-01, PNorm = 113.4218, GNorm = 0.2701
Meta loss on this task batch = 3.6196e-01, Meta loss averaged over last 500 steps = 3.2497e-01, PNorm = 113.4290, GNorm = 0.4150
Took 113.6509735584259 seconds to complete one epoch of meta training
Took 121.51284337043762 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482002
Epoch 318
Meta loss on this task batch = 3.2481e-01, Meta loss averaged over last 500 steps = 3.2494e-01, PNorm = 113.4370, GNorm = 0.2156
Meta loss on this task batch = 3.6218e-01, Meta loss averaged over last 500 steps = 3.2502e-01, PNorm = 113.4455, GNorm = 0.3183
Meta loss on this task batch = 2.7422e-01, Meta loss averaged over last 500 steps = 3.2493e-01, PNorm = 113.4549, GNorm = 0.3049
Meta loss on this task batch = 2.9215e-01, Meta loss averaged over last 500 steps = 3.2486e-01, PNorm = 113.4644, GNorm = 0.2817
Meta loss on this task batch = 2.6241e-01, Meta loss averaged over last 500 steps = 3.2472e-01, PNorm = 113.4743, GNorm = 0.2979
Meta loss on this task batch = 3.2481e-01, Meta loss averaged over last 500 steps = 3.2460e-01, PNorm = 113.4852, GNorm = 0.5159
Meta loss on this task batch = 3.2754e-01, Meta loss averaged over last 500 steps = 3.2459e-01, PNorm = 113.4947, GNorm = 0.3202
Meta loss on this task batch = 3.3682e-01, Meta loss averaged over last 500 steps = 3.2459e-01, PNorm = 113.5036, GNorm = 0.3028
Meta loss on this task batch = 3.6400e-01, Meta loss averaged over last 500 steps = 3.2466e-01, PNorm = 113.5112, GNorm = 0.3043
Meta loss on this task batch = 2.5403e-01, Meta loss averaged over last 500 steps = 3.2450e-01, PNorm = 113.5184, GNorm = 0.2545
Meta loss on this task batch = 3.1393e-01, Meta loss averaged over last 500 steps = 3.2450e-01, PNorm = 113.5248, GNorm = 0.2970
Meta loss on this task batch = 2.8316e-01, Meta loss averaged over last 500 steps = 3.2441e-01, PNorm = 113.5312, GNorm = 0.2960
Meta loss on this task batch = 3.6676e-01, Meta loss averaged over last 500 steps = 3.2443e-01, PNorm = 113.5357, GNorm = 0.3573
Meta loss on this task batch = 3.5799e-01, Meta loss averaged over last 500 steps = 3.2447e-01, PNorm = 113.5397, GNorm = 0.3055
Meta loss on this task batch = 3.3705e-01, Meta loss averaged over last 500 steps = 3.2458e-01, PNorm = 113.5441, GNorm = 0.5115
Meta loss on this task batch = 3.7363e-01, Meta loss averaged over last 500 steps = 3.2464e-01, PNorm = 113.5483, GNorm = 0.4673
Meta loss on this task batch = 2.9847e-01, Meta loss averaged over last 500 steps = 3.2448e-01, PNorm = 113.5527, GNorm = 0.2589
Meta loss on this task batch = 3.3942e-01, Meta loss averaged over last 500 steps = 3.2454e-01, PNorm = 113.5566, GNorm = 0.3207
Meta loss on this task batch = 2.9143e-01, Meta loss averaged over last 500 steps = 3.2451e-01, PNorm = 113.5614, GNorm = 0.2829
Took 111.62507843971252 seconds to complete one epoch of meta training
Took 119.50425839424133 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469388
Epoch 319
Meta loss on this task batch = 2.8708e-01, Meta loss averaged over last 500 steps = 3.2436e-01, PNorm = 113.5671, GNorm = 0.2115
Meta loss on this task batch = 3.8586e-01, Meta loss averaged over last 500 steps = 3.2446e-01, PNorm = 113.5720, GNorm = 0.2550
Meta loss on this task batch = 3.1087e-01, Meta loss averaged over last 500 steps = 3.2447e-01, PNorm = 113.5778, GNorm = 0.2750
Meta loss on this task batch = 2.9510e-01, Meta loss averaged over last 500 steps = 3.2442e-01, PNorm = 113.5845, GNorm = 0.2954
Meta loss on this task batch = 2.8599e-01, Meta loss averaged over last 500 steps = 3.2437e-01, PNorm = 113.5924, GNorm = 0.3115
Meta loss on this task batch = 3.1064e-01, Meta loss averaged over last 500 steps = 3.2422e-01, PNorm = 113.6004, GNorm = 0.2213
Meta loss on this task batch = 3.3746e-01, Meta loss averaged over last 500 steps = 3.2419e-01, PNorm = 113.6083, GNorm = 0.2484
Meta loss on this task batch = 3.2550e-01, Meta loss averaged over last 500 steps = 3.2421e-01, PNorm = 113.6154, GNorm = 0.2570
Meta loss on this task batch = 3.1912e-01, Meta loss averaged over last 500 steps = 3.2421e-01, PNorm = 113.6229, GNorm = 0.2692
Meta loss on this task batch = 2.8855e-01, Meta loss averaged over last 500 steps = 3.2419e-01, PNorm = 113.6319, GNorm = 0.3782
Meta loss on this task batch = 2.6942e-01, Meta loss averaged over last 500 steps = 3.2406e-01, PNorm = 113.6410, GNorm = 0.2589
Meta loss on this task batch = 3.2207e-01, Meta loss averaged over last 500 steps = 3.2398e-01, PNorm = 113.6507, GNorm = 0.2931
Meta loss on this task batch = 3.7500e-01, Meta loss averaged over last 500 steps = 3.2413e-01, PNorm = 113.6593, GNorm = 0.5401
Meta loss on this task batch = 2.9687e-01, Meta loss averaged over last 500 steps = 3.2420e-01, PNorm = 113.6679, GNorm = 0.3626
Meta loss on this task batch = 3.0855e-01, Meta loss averaged over last 500 steps = 3.2423e-01, PNorm = 113.6756, GNorm = 0.3252
Meta loss on this task batch = 3.5288e-01, Meta loss averaged over last 500 steps = 3.2415e-01, PNorm = 113.6844, GNorm = 0.3621
Meta loss on this task batch = 3.9673e-01, Meta loss averaged over last 500 steps = 3.2425e-01, PNorm = 113.6913, GNorm = 0.3205
Meta loss on this task batch = 3.4995e-01, Meta loss averaged over last 500 steps = 3.2432e-01, PNorm = 113.6970, GNorm = 0.3218
Meta loss on this task batch = 3.4671e-01, Meta loss averaged over last 500 steps = 3.2433e-01, PNorm = 113.7036, GNorm = 0.4249
Took 111.54976963996887 seconds to complete one epoch of meta training
Took 119.37377786636353 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456201
Epoch 320
Meta loss on this task batch = 2.8057e-01, Meta loss averaged over last 500 steps = 3.2426e-01, PNorm = 113.7103, GNorm = 0.3437
Meta loss on this task batch = 3.3310e-01, Meta loss averaged over last 500 steps = 3.2442e-01, PNorm = 113.7182, GNorm = 0.4064
Meta loss on this task batch = 3.3145e-01, Meta loss averaged over last 500 steps = 3.2430e-01, PNorm = 113.7260, GNorm = 0.2366
Meta loss on this task batch = 3.4519e-01, Meta loss averaged over last 500 steps = 3.2437e-01, PNorm = 113.7328, GNorm = 0.3004
Meta loss on this task batch = 2.7400e-01, Meta loss averaged over last 500 steps = 3.2416e-01, PNorm = 113.7394, GNorm = 0.2824
Meta loss on this task batch = 3.3924e-01, Meta loss averaged over last 500 steps = 3.2419e-01, PNorm = 113.7458, GNorm = 0.2629
Meta loss on this task batch = 2.7115e-01, Meta loss averaged over last 500 steps = 3.2410e-01, PNorm = 113.7532, GNorm = 0.2633
Meta loss on this task batch = 2.9458e-01, Meta loss averaged over last 500 steps = 3.2410e-01, PNorm = 113.7614, GNorm = 0.3453
Meta loss on this task batch = 3.8109e-01, Meta loss averaged over last 500 steps = 3.2420e-01, PNorm = 113.7686, GNorm = 0.3008
Meta loss on this task batch = 3.2454e-01, Meta loss averaged over last 500 steps = 3.2423e-01, PNorm = 113.7759, GNorm = 0.2421
Meta loss on this task batch = 3.3017e-01, Meta loss averaged over last 500 steps = 3.2429e-01, PNorm = 113.7828, GNorm = 0.2684
Meta loss on this task batch = 3.8830e-01, Meta loss averaged over last 500 steps = 3.2442e-01, PNorm = 113.7892, GNorm = 0.2667
Meta loss on this task batch = 3.1317e-01, Meta loss averaged over last 500 steps = 3.2443e-01, PNorm = 113.7959, GNorm = 0.3299
Meta loss on this task batch = 3.1453e-01, Meta loss averaged over last 500 steps = 3.2443e-01, PNorm = 113.8027, GNorm = 0.2421
Meta loss on this task batch = 2.3104e-01, Meta loss averaged over last 500 steps = 3.2420e-01, PNorm = 113.8097, GNorm = 0.2392
Meta loss on this task batch = 3.1089e-01, Meta loss averaged over last 500 steps = 3.2414e-01, PNorm = 113.8157, GNorm = 0.3137
Meta loss on this task batch = 3.1659e-01, Meta loss averaged over last 500 steps = 3.2404e-01, PNorm = 113.8220, GNorm = 0.2388
Meta loss on this task batch = 3.4012e-01, Meta loss averaged over last 500 steps = 3.2409e-01, PNorm = 113.8279, GNorm = 0.2707
Meta loss on this task batch = 3.9245e-01, Meta loss averaged over last 500 steps = 3.2424e-01, PNorm = 113.8327, GNorm = 0.2956
Took 113.1103515625 seconds to complete one epoch of meta training
Took 120.47300553321838 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448631
Epoch 321
Meta loss on this task batch = 2.7234e-01, Meta loss averaged over last 500 steps = 3.2418e-01, PNorm = 113.8382, GNorm = 0.2336
Meta loss on this task batch = 2.7940e-01, Meta loss averaged over last 500 steps = 3.2410e-01, PNorm = 113.8441, GNorm = 0.2370
Meta loss on this task batch = 3.4296e-01, Meta loss averaged over last 500 steps = 3.2414e-01, PNorm = 113.8504, GNorm = 0.3484
Meta loss on this task batch = 3.4153e-01, Meta loss averaged over last 500 steps = 3.2408e-01, PNorm = 113.8564, GNorm = 0.2727
Meta loss on this task batch = 2.9845e-01, Meta loss averaged over last 500 steps = 3.2400e-01, PNorm = 113.8624, GNorm = 0.2259
Meta loss on this task batch = 2.8677e-01, Meta loss averaged over last 500 steps = 3.2391e-01, PNorm = 113.8686, GNorm = 0.2543
Meta loss on this task batch = 3.1041e-01, Meta loss averaged over last 500 steps = 3.2395e-01, PNorm = 113.8742, GNorm = 0.3533
Meta loss on this task batch = 3.3394e-01, Meta loss averaged over last 500 steps = 3.2408e-01, PNorm = 113.8811, GNorm = 0.3418
Meta loss on this task batch = 3.8248e-01, Meta loss averaged over last 500 steps = 3.2407e-01, PNorm = 113.8884, GNorm = 0.3260
Meta loss on this task batch = 3.3078e-01, Meta loss averaged over last 500 steps = 3.2416e-01, PNorm = 113.8955, GNorm = 0.2943
Meta loss on this task batch = 3.2467e-01, Meta loss averaged over last 500 steps = 3.2420e-01, PNorm = 113.9032, GNorm = 0.2699
Meta loss on this task batch = 3.2252e-01, Meta loss averaged over last 500 steps = 3.2403e-01, PNorm = 113.9114, GNorm = 0.2367
Meta loss on this task batch = 3.7572e-01, Meta loss averaged over last 500 steps = 3.2407e-01, PNorm = 113.9188, GNorm = 0.2779
Meta loss on this task batch = 2.7934e-01, Meta loss averaged over last 500 steps = 3.2404e-01, PNorm = 113.9271, GNorm = 0.3261
Meta loss on this task batch = 2.7788e-01, Meta loss averaged over last 500 steps = 3.2382e-01, PNorm = 113.9364, GNorm = 0.2213
Meta loss on this task batch = 3.1184e-01, Meta loss averaged over last 500 steps = 3.2375e-01, PNorm = 113.9448, GNorm = 0.2686
Meta loss on this task batch = 3.1164e-01, Meta loss averaged over last 500 steps = 3.2383e-01, PNorm = 113.9521, GNorm = 0.2547
Meta loss on this task batch = 3.6115e-01, Meta loss averaged over last 500 steps = 3.2399e-01, PNorm = 113.9587, GNorm = 0.2779
Meta loss on this task batch = 3.4060e-01, Meta loss averaged over last 500 steps = 3.2412e-01, PNorm = 113.9652, GNorm = 0.5408
Took 113.17685079574585 seconds to complete one epoch of meta training
Took 120.92776441574097 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458937
Epoch 322
Meta loss on this task batch = 3.4274e-01, Meta loss averaged over last 500 steps = 3.2409e-01, PNorm = 113.9713, GNorm = 0.3085
Meta loss on this task batch = 2.9859e-01, Meta loss averaged over last 500 steps = 3.2398e-01, PNorm = 113.9777, GNorm = 0.2840
Meta loss on this task batch = 3.2910e-01, Meta loss averaged over last 500 steps = 3.2393e-01, PNorm = 113.9841, GNorm = 0.2322
Meta loss on this task batch = 3.7139e-01, Meta loss averaged over last 500 steps = 3.2403e-01, PNorm = 113.9907, GNorm = 0.2553
Meta loss on this task batch = 2.9310e-01, Meta loss averaged over last 500 steps = 3.2401e-01, PNorm = 113.9975, GNorm = 0.3101
Meta loss on this task batch = 3.4543e-01, Meta loss averaged over last 500 steps = 3.2415e-01, PNorm = 114.0054, GNorm = 0.3021
Meta loss on this task batch = 2.9468e-01, Meta loss averaged over last 500 steps = 3.2404e-01, PNorm = 114.0133, GNorm = 0.3161
Meta loss on this task batch = 2.9998e-01, Meta loss averaged over last 500 steps = 3.2416e-01, PNorm = 114.0213, GNorm = 0.2555
Meta loss on this task batch = 2.9555e-01, Meta loss averaged over last 500 steps = 3.2411e-01, PNorm = 114.0286, GNorm = 0.2495
Meta loss on this task batch = 2.7138e-01, Meta loss averaged over last 500 steps = 3.2401e-01, PNorm = 114.0358, GNorm = 0.3411
Meta loss on this task batch = 3.4111e-01, Meta loss averaged over last 500 steps = 3.2388e-01, PNorm = 114.0420, GNorm = 0.3153
Meta loss on this task batch = 3.8048e-01, Meta loss averaged over last 500 steps = 3.2398e-01, PNorm = 114.0475, GNorm = 0.4338
Meta loss on this task batch = 2.9366e-01, Meta loss averaged over last 500 steps = 3.2393e-01, PNorm = 114.0528, GNorm = 0.2116
Meta loss on this task batch = 3.1201e-01, Meta loss averaged over last 500 steps = 3.2389e-01, PNorm = 114.0574, GNorm = 0.3380
Meta loss on this task batch = 2.6552e-01, Meta loss averaged over last 500 steps = 3.2377e-01, PNorm = 114.0626, GNorm = 0.2466
Meta loss on this task batch = 2.6353e-01, Meta loss averaged over last 500 steps = 3.2363e-01, PNorm = 114.0692, GNorm = 0.2736
Meta loss on this task batch = 3.3417e-01, Meta loss averaged over last 500 steps = 3.2362e-01, PNorm = 114.0756, GNorm = 0.2644
Meta loss on this task batch = 3.2231e-01, Meta loss averaged over last 500 steps = 3.2364e-01, PNorm = 114.0816, GNorm = 0.2828
Meta loss on this task batch = 3.6485e-01, Meta loss averaged over last 500 steps = 3.2373e-01, PNorm = 114.0873, GNorm = 0.3598
Took 113.1595778465271 seconds to complete one epoch of meta training
Took 120.11028122901917 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450639
Epoch 323
Meta loss on this task batch = 3.5862e-01, Meta loss averaged over last 500 steps = 3.2386e-01, PNorm = 114.0938, GNorm = 0.3252
Meta loss on this task batch = 3.3806e-01, Meta loss averaged over last 500 steps = 3.2378e-01, PNorm = 114.1000, GNorm = 0.2841
Meta loss on this task batch = 3.2759e-01, Meta loss averaged over last 500 steps = 3.2389e-01, PNorm = 114.1066, GNorm = 0.4712
Meta loss on this task batch = 2.6080e-01, Meta loss averaged over last 500 steps = 3.2376e-01, PNorm = 114.1138, GNorm = 0.2344
Meta loss on this task batch = 3.1927e-01, Meta loss averaged over last 500 steps = 3.2385e-01, PNorm = 114.1211, GNorm = 0.2745
Meta loss on this task batch = 3.4922e-01, Meta loss averaged over last 500 steps = 3.2383e-01, PNorm = 114.1281, GNorm = 0.3298
Meta loss on this task batch = 3.4703e-01, Meta loss averaged over last 500 steps = 3.2395e-01, PNorm = 114.1354, GNorm = 0.2748
Meta loss on this task batch = 2.9785e-01, Meta loss averaged over last 500 steps = 3.2390e-01, PNorm = 114.1430, GNorm = 0.2569
Meta loss on this task batch = 3.3089e-01, Meta loss averaged over last 500 steps = 3.2400e-01, PNorm = 114.1507, GNorm = 0.2620
Meta loss on this task batch = 2.7866e-01, Meta loss averaged over last 500 steps = 3.2378e-01, PNorm = 114.1589, GNorm = 0.2646
Meta loss on this task batch = 2.0862e-01, Meta loss averaged over last 500 steps = 3.2352e-01, PNorm = 114.1679, GNorm = 0.3974
Meta loss on this task batch = 3.2521e-01, Meta loss averaged over last 500 steps = 3.2348e-01, PNorm = 114.1778, GNorm = 0.3869
Meta loss on this task batch = 2.7495e-01, Meta loss averaged over last 500 steps = 3.2337e-01, PNorm = 114.1874, GNorm = 0.3232
Meta loss on this task batch = 2.7461e-01, Meta loss averaged over last 500 steps = 3.2329e-01, PNorm = 114.1969, GNorm = 0.2510
Meta loss on this task batch = 3.0680e-01, Meta loss averaged over last 500 steps = 3.2317e-01, PNorm = 114.2064, GNorm = 0.2353
Meta loss on this task batch = 3.3827e-01, Meta loss averaged over last 500 steps = 3.2318e-01, PNorm = 114.2149, GNorm = 0.3085
Meta loss on this task batch = 3.3910e-01, Meta loss averaged over last 500 steps = 3.2317e-01, PNorm = 114.2225, GNorm = 0.2289
Meta loss on this task batch = 4.2009e-01, Meta loss averaged over last 500 steps = 3.2335e-01, PNorm = 114.2293, GNorm = 0.3088
Meta loss on this task batch = 3.4821e-01, Meta loss averaged over last 500 steps = 3.2335e-01, PNorm = 114.2347, GNorm = 0.3188
Took 112.85004949569702 seconds to complete one epoch of meta training
Took 120.7782084941864 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457178
Epoch 324
Meta loss on this task batch = 2.9457e-01, Meta loss averaged over last 500 steps = 3.2338e-01, PNorm = 114.2408, GNorm = 0.2634
Meta loss on this task batch = 2.5477e-01, Meta loss averaged over last 500 steps = 3.2320e-01, PNorm = 114.2487, GNorm = 0.4862
Meta loss on this task batch = 3.0602e-01, Meta loss averaged over last 500 steps = 3.2323e-01, PNorm = 114.2566, GNorm = 0.2889
Meta loss on this task batch = 3.1039e-01, Meta loss averaged over last 500 steps = 3.2330e-01, PNorm = 114.2643, GNorm = 0.2162
Meta loss on this task batch = 3.4683e-01, Meta loss averaged over last 500 steps = 3.2334e-01, PNorm = 114.2716, GNorm = 0.2432
Meta loss on this task batch = 3.1153e-01, Meta loss averaged over last 500 steps = 3.2330e-01, PNorm = 114.2788, GNorm = 0.3182
Meta loss on this task batch = 3.6584e-01, Meta loss averaged over last 500 steps = 3.2317e-01, PNorm = 114.2864, GNorm = 0.2685
Meta loss on this task batch = 3.5794e-01, Meta loss averaged over last 500 steps = 3.2331e-01, PNorm = 114.2929, GNorm = 0.3029
Meta loss on this task batch = 2.7706e-01, Meta loss averaged over last 500 steps = 3.2328e-01, PNorm = 114.3002, GNorm = 0.2556
Meta loss on this task batch = 3.4498e-01, Meta loss averaged over last 500 steps = 3.2328e-01, PNorm = 114.3070, GNorm = 0.3434
Meta loss on this task batch = 3.6503e-01, Meta loss averaged over last 500 steps = 3.2333e-01, PNorm = 114.3122, GNorm = 0.3154
Meta loss on this task batch = 3.1984e-01, Meta loss averaged over last 500 steps = 3.2330e-01, PNorm = 114.3179, GNorm = 0.4351
Meta loss on this task batch = 3.4604e-01, Meta loss averaged over last 500 steps = 3.2333e-01, PNorm = 114.3235, GNorm = 0.2829
Meta loss on this task batch = 3.5574e-01, Meta loss averaged over last 500 steps = 3.2334e-01, PNorm = 114.3287, GNorm = 0.2686
Meta loss on this task batch = 2.8367e-01, Meta loss averaged over last 500 steps = 3.2327e-01, PNorm = 114.3344, GNorm = 0.2555
Meta loss on this task batch = 3.0495e-01, Meta loss averaged over last 500 steps = 3.2329e-01, PNorm = 114.3407, GNorm = 0.4554
Meta loss on this task batch = 3.4276e-01, Meta loss averaged over last 500 steps = 3.2330e-01, PNorm = 114.3462, GNorm = 0.2511
Meta loss on this task batch = 3.0520e-01, Meta loss averaged over last 500 steps = 3.2331e-01, PNorm = 114.3523, GNorm = 0.2375
Meta loss on this task batch = 3.1731e-01, Meta loss averaged over last 500 steps = 3.2340e-01, PNorm = 114.3581, GNorm = 0.2963
Took 111.57209420204163 seconds to complete one epoch of meta training
Took 119.29700660705566 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460623
Epoch 325
Meta loss on this task batch = 3.4607e-01, Meta loss averaged over last 500 steps = 3.2334e-01, PNorm = 114.3641, GNorm = 0.2559
Meta loss on this task batch = 3.1631e-01, Meta loss averaged over last 500 steps = 3.2326e-01, PNorm = 114.3693, GNorm = 0.2577
Meta loss on this task batch = 3.3609e-01, Meta loss averaged over last 500 steps = 3.2336e-01, PNorm = 114.3751, GNorm = 0.3237
Meta loss on this task batch = 3.2385e-01, Meta loss averaged over last 500 steps = 3.2345e-01, PNorm = 114.3820, GNorm = 0.2431
Meta loss on this task batch = 3.4940e-01, Meta loss averaged over last 500 steps = 3.2351e-01, PNorm = 114.3882, GNorm = 0.2958
Meta loss on this task batch = 3.2664e-01, Meta loss averaged over last 500 steps = 3.2350e-01, PNorm = 114.3950, GNorm = 0.2915
Meta loss on this task batch = 2.7751e-01, Meta loss averaged over last 500 steps = 3.2347e-01, PNorm = 114.4020, GNorm = 0.2390
Meta loss on this task batch = 3.5394e-01, Meta loss averaged over last 500 steps = 3.2365e-01, PNorm = 114.4089, GNorm = 0.2899
Meta loss on this task batch = 3.2725e-01, Meta loss averaged over last 500 steps = 3.2364e-01, PNorm = 114.4166, GNorm = 0.3069
Meta loss on this task batch = 3.3113e-01, Meta loss averaged over last 500 steps = 3.2370e-01, PNorm = 114.4240, GNorm = 0.3176
Meta loss on this task batch = 3.0269e-01, Meta loss averaged over last 500 steps = 3.2360e-01, PNorm = 114.4319, GNorm = 0.2576
Meta loss on this task batch = 3.4605e-01, Meta loss averaged over last 500 steps = 3.2358e-01, PNorm = 114.4396, GNorm = 0.2669
Meta loss on this task batch = 3.1911e-01, Meta loss averaged over last 500 steps = 3.2360e-01, PNorm = 114.4467, GNorm = 0.2428
Meta loss on this task batch = 2.8926e-01, Meta loss averaged over last 500 steps = 3.2362e-01, PNorm = 114.4533, GNorm = 0.3053
Meta loss on this task batch = 2.7972e-01, Meta loss averaged over last 500 steps = 3.2345e-01, PNorm = 114.4601, GNorm = 0.2325
Meta loss on this task batch = 3.2450e-01, Meta loss averaged over last 500 steps = 3.2345e-01, PNorm = 114.4675, GNorm = 0.3099
Meta loss on this task batch = 3.0774e-01, Meta loss averaged over last 500 steps = 3.2339e-01, PNorm = 114.4753, GNorm = 0.2852
Meta loss on this task batch = 3.3008e-01, Meta loss averaged over last 500 steps = 3.2342e-01, PNorm = 114.4830, GNorm = 0.3559
Meta loss on this task batch = 2.9531e-01, Meta loss averaged over last 500 steps = 3.2336e-01, PNorm = 114.4910, GNorm = 0.3028
Took 110.0629243850708 seconds to complete one epoch of meta training
Took 118.10446691513062 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489355
Epoch 326
Meta loss on this task batch = 2.5068e-01, Meta loss averaged over last 500 steps = 3.2323e-01, PNorm = 114.4992, GNorm = 0.2361
Meta loss on this task batch = 2.9720e-01, Meta loss averaged over last 500 steps = 3.2303e-01, PNorm = 114.5074, GNorm = 0.2409
Meta loss on this task batch = 3.2207e-01, Meta loss averaged over last 500 steps = 3.2308e-01, PNorm = 114.5155, GNorm = 0.3137
Meta loss on this task batch = 3.8827e-01, Meta loss averaged over last 500 steps = 3.2312e-01, PNorm = 114.5227, GNorm = 0.5423
Meta loss on this task batch = 3.5946e-01, Meta loss averaged over last 500 steps = 3.2320e-01, PNorm = 114.5289, GNorm = 0.2951
Meta loss on this task batch = 2.8214e-01, Meta loss averaged over last 500 steps = 3.2319e-01, PNorm = 114.5338, GNorm = 0.3439
Meta loss on this task batch = 3.6803e-01, Meta loss averaged over last 500 steps = 3.2322e-01, PNorm = 114.5375, GNorm = 0.3044
Meta loss on this task batch = 3.5287e-01, Meta loss averaged over last 500 steps = 3.2329e-01, PNorm = 114.5413, GNorm = 0.5050
Meta loss on this task batch = 3.7154e-01, Meta loss averaged over last 500 steps = 3.2338e-01, PNorm = 114.5446, GNorm = 0.3103
Meta loss on this task batch = 2.9213e-01, Meta loss averaged over last 500 steps = 3.2319e-01, PNorm = 114.5492, GNorm = 0.2782
Meta loss on this task batch = 3.2270e-01, Meta loss averaged over last 500 steps = 3.2306e-01, PNorm = 114.5546, GNorm = 0.2483
Meta loss on this task batch = 3.4658e-01, Meta loss averaged over last 500 steps = 3.2304e-01, PNorm = 114.5605, GNorm = 0.2775
Meta loss on this task batch = 3.0371e-01, Meta loss averaged over last 500 steps = 3.2305e-01, PNorm = 114.5671, GNorm = 0.2504
Meta loss on this task batch = 2.9441e-01, Meta loss averaged over last 500 steps = 3.2293e-01, PNorm = 114.5740, GNorm = 0.2250
Meta loss on this task batch = 2.8989e-01, Meta loss averaged over last 500 steps = 3.2292e-01, PNorm = 114.5812, GNorm = 0.2464
Meta loss on this task batch = 2.9859e-01, Meta loss averaged over last 500 steps = 3.2300e-01, PNorm = 114.5887, GNorm = 0.3214
Meta loss on this task batch = 3.1496e-01, Meta loss averaged over last 500 steps = 3.2293e-01, PNorm = 114.5965, GNorm = 0.2782
Meta loss on this task batch = 3.6585e-01, Meta loss averaged over last 500 steps = 3.2298e-01, PNorm = 114.6027, GNorm = 0.3651
Meta loss on this task batch = 2.9602e-01, Meta loss averaged over last 500 steps = 3.2293e-01, PNorm = 114.6085, GNorm = 0.3142
Took 112.54709005355835 seconds to complete one epoch of meta training
Took 120.39738702774048 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466653
Epoch 327
Meta loss on this task batch = 3.3115e-01, Meta loss averaged over last 500 steps = 3.2301e-01, PNorm = 114.6143, GNorm = 0.2466
Meta loss on this task batch = 3.0958e-01, Meta loss averaged over last 500 steps = 3.2301e-01, PNorm = 114.6199, GNorm = 0.2578
Meta loss on this task batch = 3.4892e-01, Meta loss averaged over last 500 steps = 3.2301e-01, PNorm = 114.6257, GNorm = 0.4157
Meta loss on this task batch = 3.2849e-01, Meta loss averaged over last 500 steps = 3.2298e-01, PNorm = 114.6316, GNorm = 0.2768
Meta loss on this task batch = 3.2727e-01, Meta loss averaged over last 500 steps = 3.2311e-01, PNorm = 114.6381, GNorm = 0.4434
Meta loss on this task batch = 3.2651e-01, Meta loss averaged over last 500 steps = 3.2292e-01, PNorm = 114.6449, GNorm = 0.2984
Meta loss on this task batch = 2.7481e-01, Meta loss averaged over last 500 steps = 3.2286e-01, PNorm = 114.6521, GNorm = 0.2634
Meta loss on this task batch = 3.4688e-01, Meta loss averaged over last 500 steps = 3.2292e-01, PNorm = 114.6598, GNorm = 0.3051
Meta loss on this task batch = 2.6046e-01, Meta loss averaged over last 500 steps = 3.2277e-01, PNorm = 114.6675, GNorm = 0.2705
Meta loss on this task batch = 2.8865e-01, Meta loss averaged over last 500 steps = 3.2264e-01, PNorm = 114.6759, GNorm = 0.2352
Meta loss on this task batch = 3.0127e-01, Meta loss averaged over last 500 steps = 3.2262e-01, PNorm = 114.6851, GNorm = 0.2364
Meta loss on this task batch = 3.5923e-01, Meta loss averaged over last 500 steps = 3.2275e-01, PNorm = 114.6934, GNorm = 0.3054
Meta loss on this task batch = 3.0542e-01, Meta loss averaged over last 500 steps = 3.2278e-01, PNorm = 114.7022, GNorm = 0.3123
Meta loss on this task batch = 3.1333e-01, Meta loss averaged over last 500 steps = 3.2282e-01, PNorm = 114.7105, GNorm = 0.3767
Meta loss on this task batch = 3.1026e-01, Meta loss averaged over last 500 steps = 3.2273e-01, PNorm = 114.7189, GNorm = 0.2454
Meta loss on this task batch = 3.5150e-01, Meta loss averaged over last 500 steps = 3.2281e-01, PNorm = 114.7261, GNorm = 0.2671
Meta loss on this task batch = 3.1716e-01, Meta loss averaged over last 500 steps = 3.2278e-01, PNorm = 114.7329, GNorm = 0.2666
Meta loss on this task batch = 2.9966e-01, Meta loss averaged over last 500 steps = 3.2278e-01, PNorm = 114.7389, GNorm = 0.2474
Meta loss on this task batch = 2.8407e-01, Meta loss averaged over last 500 steps = 3.2263e-01, PNorm = 114.7456, GNorm = 0.2923
Took 112.21714544296265 seconds to complete one epoch of meta training
Took 119.75323534011841 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487693
Epoch 328
Meta loss on this task batch = 3.1241e-01, Meta loss averaged over last 500 steps = 3.2259e-01, PNorm = 114.7523, GNorm = 0.2910
Meta loss on this task batch = 3.3842e-01, Meta loss averaged over last 500 steps = 3.2255e-01, PNorm = 114.7587, GNorm = 0.2728
Meta loss on this task batch = 3.5931e-01, Meta loss averaged over last 500 steps = 3.2264e-01, PNorm = 114.7643, GNorm = 0.5040
Meta loss on this task batch = 3.1457e-01, Meta loss averaged over last 500 steps = 3.2251e-01, PNorm = 114.7694, GNorm = 0.2559
Meta loss on this task batch = 3.6618e-01, Meta loss averaged over last 500 steps = 3.2260e-01, PNorm = 114.7742, GNorm = 0.3086
Meta loss on this task batch = 3.1495e-01, Meta loss averaged over last 500 steps = 3.2262e-01, PNorm = 114.7799, GNorm = 0.3035
Meta loss on this task batch = 3.1606e-01, Meta loss averaged over last 500 steps = 3.2260e-01, PNorm = 114.7863, GNorm = 0.4620
Meta loss on this task batch = 2.8807e-01, Meta loss averaged over last 500 steps = 3.2257e-01, PNorm = 114.7926, GNorm = 0.3014
Meta loss on this task batch = 3.2024e-01, Meta loss averaged over last 500 steps = 3.2262e-01, PNorm = 114.7993, GNorm = 0.3142
Meta loss on this task batch = 3.1432e-01, Meta loss averaged over last 500 steps = 3.2269e-01, PNorm = 114.8058, GNorm = 0.2602
Meta loss on this task batch = 2.9610e-01, Meta loss averaged over last 500 steps = 3.2263e-01, PNorm = 114.8125, GNorm = 0.2718
Meta loss on this task batch = 3.3671e-01, Meta loss averaged over last 500 steps = 3.2256e-01, PNorm = 114.8188, GNorm = 0.2590
Meta loss on this task batch = 3.0713e-01, Meta loss averaged over last 500 steps = 3.2246e-01, PNorm = 114.8252, GNorm = 0.2707
Meta loss on this task batch = 2.9231e-01, Meta loss averaged over last 500 steps = 3.2251e-01, PNorm = 114.8318, GNorm = 0.5296
Meta loss on this task batch = 3.5593e-01, Meta loss averaged over last 500 steps = 3.2259e-01, PNorm = 114.8379, GNorm = 0.3116
Meta loss on this task batch = 3.4202e-01, Meta loss averaged over last 500 steps = 3.2261e-01, PNorm = 114.8439, GNorm = 0.2937
Meta loss on this task batch = 3.4109e-01, Meta loss averaged over last 500 steps = 3.2264e-01, PNorm = 114.8492, GNorm = 0.3518
Meta loss on this task batch = 3.0813e-01, Meta loss averaged over last 500 steps = 3.2263e-01, PNorm = 114.8545, GNorm = 0.3055
Meta loss on this task batch = 2.3993e-01, Meta loss averaged over last 500 steps = 3.2241e-01, PNorm = 114.8601, GNorm = 0.3355
Took 111.18152022361755 seconds to complete one epoch of meta training
Took 118.7430031299591 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475287
Epoch 329
Meta loss on this task batch = 3.5780e-01, Meta loss averaged over last 500 steps = 3.2250e-01, PNorm = 114.8658, GNorm = 0.2690
Meta loss on this task batch = 3.2067e-01, Meta loss averaged over last 500 steps = 3.2239e-01, PNorm = 114.8718, GNorm = 0.4049
Meta loss on this task batch = 2.9988e-01, Meta loss averaged over last 500 steps = 3.2237e-01, PNorm = 114.8784, GNorm = 0.3039
Meta loss on this task batch = 3.4923e-01, Meta loss averaged over last 500 steps = 3.2239e-01, PNorm = 114.8852, GNorm = 0.3130
Meta loss on this task batch = 3.0522e-01, Meta loss averaged over last 500 steps = 3.2236e-01, PNorm = 114.8924, GNorm = 0.3897
Meta loss on this task batch = 3.1257e-01, Meta loss averaged over last 500 steps = 3.2238e-01, PNorm = 114.8998, GNorm = 0.2718
Meta loss on this task batch = 2.7028e-01, Meta loss averaged over last 500 steps = 3.2238e-01, PNorm = 114.9077, GNorm = 0.3284
Meta loss on this task batch = 3.5457e-01, Meta loss averaged over last 500 steps = 3.2248e-01, PNorm = 114.9141, GNorm = 0.3172
Meta loss on this task batch = 3.0244e-01, Meta loss averaged over last 500 steps = 3.2243e-01, PNorm = 114.9216, GNorm = 0.2473
Meta loss on this task batch = 2.7601e-01, Meta loss averaged over last 500 steps = 3.2240e-01, PNorm = 114.9305, GNorm = 0.3312
Meta loss on this task batch = 3.9391e-01, Meta loss averaged over last 500 steps = 3.2245e-01, PNorm = 114.9380, GNorm = 0.3668
Meta loss on this task batch = 3.2727e-01, Meta loss averaged over last 500 steps = 3.2238e-01, PNorm = 114.9453, GNorm = 0.4241
Meta loss on this task batch = 3.2210e-01, Meta loss averaged over last 500 steps = 3.2235e-01, PNorm = 114.9519, GNorm = 0.2956
Meta loss on this task batch = 3.1222e-01, Meta loss averaged over last 500 steps = 3.2246e-01, PNorm = 114.9589, GNorm = 0.2361
Meta loss on this task batch = 3.1471e-01, Meta loss averaged over last 500 steps = 3.2247e-01, PNorm = 114.9657, GNorm = 0.3378
Meta loss on this task batch = 3.3456e-01, Meta loss averaged over last 500 steps = 3.2245e-01, PNorm = 114.9728, GNorm = 0.2500
Meta loss on this task batch = 2.8252e-01, Meta loss averaged over last 500 steps = 3.2231e-01, PNorm = 114.9801, GNorm = 0.2711
Meta loss on this task batch = 3.3603e-01, Meta loss averaged over last 500 steps = 3.2245e-01, PNorm = 114.9875, GNorm = 0.3340
Meta loss on this task batch = 3.6643e-01, Meta loss averaged over last 500 steps = 3.2249e-01, PNorm = 114.9945, GNorm = 0.4614
Took 112.49213767051697 seconds to complete one epoch of meta training
Took 120.69568634033203 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463428
Epoch 330
Meta loss on this task batch = 3.1375e-01, Meta loss averaged over last 500 steps = 3.2255e-01, PNorm = 115.0015, GNorm = 0.2605
Meta loss on this task batch = 3.4562e-01, Meta loss averaged over last 500 steps = 3.2263e-01, PNorm = 115.0068, GNorm = 0.3307
Meta loss on this task batch = 2.5855e-01, Meta loss averaged over last 500 steps = 3.2251e-01, PNorm = 115.0132, GNorm = 0.2997
Meta loss on this task batch = 2.9276e-01, Meta loss averaged over last 500 steps = 3.2231e-01, PNorm = 115.0193, GNorm = 0.3842
Meta loss on this task batch = 3.1076e-01, Meta loss averaged over last 500 steps = 3.2229e-01, PNorm = 115.0260, GNorm = 0.2211
Meta loss on this task batch = 2.9966e-01, Meta loss averaged over last 500 steps = 3.2225e-01, PNorm = 115.0335, GNorm = 0.3399
Meta loss on this task batch = 2.6729e-01, Meta loss averaged over last 500 steps = 3.2223e-01, PNorm = 115.0418, GNorm = 0.2056
Meta loss on this task batch = 2.6419e-01, Meta loss averaged over last 500 steps = 3.2196e-01, PNorm = 115.0510, GNorm = 0.3082
Meta loss on this task batch = 3.6565e-01, Meta loss averaged over last 500 steps = 3.2214e-01, PNorm = 115.0600, GNorm = 0.3086
Meta loss on this task batch = 3.6168e-01, Meta loss averaged over last 500 steps = 3.2226e-01, PNorm = 115.0694, GNorm = 0.2726
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 3.2218e-01, PNorm = 115.0775, GNorm = 0.2446
Meta loss on this task batch = 3.1224e-01, Meta loss averaged over last 500 steps = 3.2206e-01, PNorm = 115.0859, GNorm = 0.2853
Meta loss on this task batch = 2.9139e-01, Meta loss averaged over last 500 steps = 3.2195e-01, PNorm = 115.0944, GNorm = 0.2730
Meta loss on this task batch = 3.5531e-01, Meta loss averaged over last 500 steps = 3.2208e-01, PNorm = 115.1025, GNorm = 0.3277
Meta loss on this task batch = 2.8901e-01, Meta loss averaged over last 500 steps = 3.2201e-01, PNorm = 115.1108, GNorm = 0.2978
Meta loss on this task batch = 3.8031e-01, Meta loss averaged over last 500 steps = 3.2214e-01, PNorm = 115.1181, GNorm = 0.4741
Meta loss on this task batch = 3.1611e-01, Meta loss averaged over last 500 steps = 3.2211e-01, PNorm = 115.1246, GNorm = 0.3002
Meta loss on this task batch = 3.3745e-01, Meta loss averaged over last 500 steps = 3.2217e-01, PNorm = 115.1315, GNorm = 0.2567
Meta loss on this task batch = 3.0011e-01, Meta loss averaged over last 500 steps = 3.2221e-01, PNorm = 115.1391, GNorm = 0.3997
Took 109.75876879692078 seconds to complete one epoch of meta training
Took 117.84283232688904 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460625
Epoch 331
Meta loss on this task batch = 3.3029e-01, Meta loss averaged over last 500 steps = 3.2223e-01, PNorm = 115.1459, GNorm = 0.3788
Meta loss on this task batch = 3.6690e-01, Meta loss averaged over last 500 steps = 3.2227e-01, PNorm = 115.1514, GNorm = 0.2901
Meta loss on this task batch = 3.1578e-01, Meta loss averaged over last 500 steps = 3.2229e-01, PNorm = 115.1567, GNorm = 0.2509
Meta loss on this task batch = 3.0420e-01, Meta loss averaged over last 500 steps = 3.2222e-01, PNorm = 115.1628, GNorm = 0.2786
Meta loss on this task batch = 3.1842e-01, Meta loss averaged over last 500 steps = 3.2221e-01, PNorm = 115.1694, GNorm = 0.2469
Meta loss on this task batch = 2.6992e-01, Meta loss averaged over last 500 steps = 3.2213e-01, PNorm = 115.1774, GNorm = 0.2795
Meta loss on this task batch = 2.9716e-01, Meta loss averaged over last 500 steps = 3.2215e-01, PNorm = 115.1857, GNorm = 0.2539
Meta loss on this task batch = 3.2978e-01, Meta loss averaged over last 500 steps = 3.2212e-01, PNorm = 115.1930, GNorm = 0.3364
Meta loss on this task batch = 2.7393e-01, Meta loss averaged over last 500 steps = 3.2205e-01, PNorm = 115.2012, GNorm = 0.2396
Meta loss on this task batch = 3.3591e-01, Meta loss averaged over last 500 steps = 3.2212e-01, PNorm = 115.2102, GNorm = 0.2852
Meta loss on this task batch = 3.4084e-01, Meta loss averaged over last 500 steps = 3.2200e-01, PNorm = 115.2190, GNorm = 0.2500
Meta loss on this task batch = 3.5440e-01, Meta loss averaged over last 500 steps = 3.2207e-01, PNorm = 115.2275, GNorm = 0.2450
Meta loss on this task batch = 3.0302e-01, Meta loss averaged over last 500 steps = 3.2201e-01, PNorm = 115.2361, GNorm = 0.2701
Meta loss on this task batch = 3.2189e-01, Meta loss averaged over last 500 steps = 3.2201e-01, PNorm = 115.2455, GNorm = 0.3044
Meta loss on this task batch = 2.9794e-01, Meta loss averaged over last 500 steps = 3.2197e-01, PNorm = 115.2537, GNorm = 0.2905
Meta loss on this task batch = 3.0263e-01, Meta loss averaged over last 500 steps = 3.2186e-01, PNorm = 115.2624, GNorm = 0.3221
Meta loss on this task batch = 3.2849e-01, Meta loss averaged over last 500 steps = 3.2191e-01, PNorm = 115.2706, GNorm = 0.2878
Meta loss on this task batch = 3.1098e-01, Meta loss averaged over last 500 steps = 3.2174e-01, PNorm = 115.2783, GNorm = 0.3350
Meta loss on this task batch = 3.9626e-01, Meta loss averaged over last 500 steps = 3.2183e-01, PNorm = 115.2852, GNorm = 0.4097
Took 112.171311378479 seconds to complete one epoch of meta training
Took 119.70991778373718 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.441240
Epoch 332
Meta loss on this task batch = 3.6889e-01, Meta loss averaged over last 500 steps = 3.2196e-01, PNorm = 115.2915, GNorm = 0.3376
Meta loss on this task batch = 3.1386e-01, Meta loss averaged over last 500 steps = 3.2207e-01, PNorm = 115.2980, GNorm = 0.2689
Meta loss on this task batch = 3.2742e-01, Meta loss averaged over last 500 steps = 3.2220e-01, PNorm = 115.3042, GNorm = 0.2706
Meta loss on this task batch = 2.8750e-01, Meta loss averaged over last 500 steps = 3.2209e-01, PNorm = 115.3113, GNorm = 0.3271
Meta loss on this task batch = 3.3368e-01, Meta loss averaged over last 500 steps = 3.2210e-01, PNorm = 115.3176, GNorm = 0.3279
Meta loss on this task batch = 3.1338e-01, Meta loss averaged over last 500 steps = 3.2203e-01, PNorm = 115.3245, GNorm = 0.4109
Meta loss on this task batch = 4.2005e-01, Meta loss averaged over last 500 steps = 3.2218e-01, PNorm = 115.3305, GNorm = 0.3205
Meta loss on this task batch = 4.3042e-01, Meta loss averaged over last 500 steps = 3.2234e-01, PNorm = 115.3355, GNorm = 0.3856
Meta loss on this task batch = 3.0533e-01, Meta loss averaged over last 500 steps = 3.2234e-01, PNorm = 115.3411, GNorm = 0.3960
Meta loss on this task batch = 3.1158e-01, Meta loss averaged over last 500 steps = 3.2243e-01, PNorm = 115.3471, GNorm = 0.3832
Meta loss on this task batch = 3.3509e-01, Meta loss averaged over last 500 steps = 3.2255e-01, PNorm = 115.3528, GNorm = 0.3141
Meta loss on this task batch = 3.1878e-01, Meta loss averaged over last 500 steps = 3.2259e-01, PNorm = 115.3588, GNorm = 0.2698
Meta loss on this task batch = 2.8713e-01, Meta loss averaged over last 500 steps = 3.2248e-01, PNorm = 115.3659, GNorm = 0.3507
Meta loss on this task batch = 2.8591e-01, Meta loss averaged over last 500 steps = 3.2235e-01, PNorm = 115.3738, GNorm = 0.4432
Meta loss on this task batch = 3.4532e-01, Meta loss averaged over last 500 steps = 3.2235e-01, PNorm = 115.3811, GNorm = 0.3252
Meta loss on this task batch = 3.3115e-01, Meta loss averaged over last 500 steps = 3.2227e-01, PNorm = 115.3888, GNorm = 0.3825
Meta loss on this task batch = 3.0699e-01, Meta loss averaged over last 500 steps = 3.2222e-01, PNorm = 115.3962, GNorm = 0.3457
Meta loss on this task batch = 3.2143e-01, Meta loss averaged over last 500 steps = 3.2225e-01, PNorm = 115.4035, GNorm = 0.4656
Meta loss on this task batch = 3.0273e-01, Meta loss averaged over last 500 steps = 3.2216e-01, PNorm = 115.4109, GNorm = 0.3640
Took 110.12125825881958 seconds to complete one epoch of meta training
Took 118.10661220550537 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444728
Epoch 333
Meta loss on this task batch = 2.8674e-01, Meta loss averaged over last 500 steps = 3.2202e-01, PNorm = 115.4180, GNorm = 0.3078
Meta loss on this task batch = 3.4280e-01, Meta loss averaged over last 500 steps = 3.2202e-01, PNorm = 115.4249, GNorm = 0.4487
Meta loss on this task batch = 3.2953e-01, Meta loss averaged over last 500 steps = 3.2208e-01, PNorm = 115.4319, GNorm = 0.2539
Meta loss on this task batch = 3.1613e-01, Meta loss averaged over last 500 steps = 3.2204e-01, PNorm = 115.4393, GNorm = 0.4155
Meta loss on this task batch = 3.6916e-01, Meta loss averaged over last 500 steps = 3.2226e-01, PNorm = 115.4460, GNorm = 0.2677
Meta loss on this task batch = 3.1203e-01, Meta loss averaged over last 500 steps = 3.2209e-01, PNorm = 115.4527, GNorm = 0.2401
Meta loss on this task batch = 2.7231e-01, Meta loss averaged over last 500 steps = 3.2206e-01, PNorm = 115.4588, GNorm = 0.3181
Meta loss on this task batch = 3.4518e-01, Meta loss averaged over last 500 steps = 3.2212e-01, PNorm = 115.4646, GNorm = 0.3020
Meta loss on this task batch = 3.0878e-01, Meta loss averaged over last 500 steps = 3.2212e-01, PNorm = 115.4709, GNorm = 0.3116
Meta loss on this task batch = 3.2517e-01, Meta loss averaged over last 500 steps = 3.2203e-01, PNorm = 115.4778, GNorm = 0.3263
Meta loss on this task batch = 3.3614e-01, Meta loss averaged over last 500 steps = 3.2205e-01, PNorm = 115.4849, GNorm = 0.3475
Meta loss on this task batch = 3.3925e-01, Meta loss averaged over last 500 steps = 3.2209e-01, PNorm = 115.4918, GNorm = 0.3269
Meta loss on this task batch = 3.1031e-01, Meta loss averaged over last 500 steps = 3.2201e-01, PNorm = 115.4989, GNorm = 0.2560
Meta loss on this task batch = 3.2657e-01, Meta loss averaged over last 500 steps = 3.2193e-01, PNorm = 115.5062, GNorm = 0.3159
Meta loss on this task batch = 3.2913e-01, Meta loss averaged over last 500 steps = 3.2194e-01, PNorm = 115.5127, GNorm = 0.2612
Meta loss on this task batch = 3.1173e-01, Meta loss averaged over last 500 steps = 3.2185e-01, PNorm = 115.5185, GNorm = 0.2791
Meta loss on this task batch = 2.6849e-01, Meta loss averaged over last 500 steps = 3.2176e-01, PNorm = 115.5262, GNorm = 0.2747
Meta loss on this task batch = 3.1753e-01, Meta loss averaged over last 500 steps = 3.2182e-01, PNorm = 115.5346, GNorm = 0.3209
Meta loss on this task batch = 3.1231e-01, Meta loss averaged over last 500 steps = 3.2181e-01, PNorm = 115.5430, GNorm = 0.3497
Took 112.24991536140442 seconds to complete one epoch of meta training
Took 119.94459247589111 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455571
Epoch 334
Meta loss on this task batch = 2.9862e-01, Meta loss averaged over last 500 steps = 3.2164e-01, PNorm = 115.5521, GNorm = 0.2210
Meta loss on this task batch = 3.5173e-01, Meta loss averaged over last 500 steps = 3.2168e-01, PNorm = 115.5605, GNorm = 0.2455
Meta loss on this task batch = 3.1961e-01, Meta loss averaged over last 500 steps = 3.2172e-01, PNorm = 115.5692, GNorm = 0.2799
Meta loss on this task batch = 2.9425e-01, Meta loss averaged over last 500 steps = 3.2170e-01, PNorm = 115.5783, GNorm = 0.2557
Meta loss on this task batch = 2.6718e-01, Meta loss averaged over last 500 steps = 3.2156e-01, PNorm = 115.5872, GNorm = 0.2398
Meta loss on this task batch = 2.6327e-01, Meta loss averaged over last 500 steps = 3.2140e-01, PNorm = 115.5963, GNorm = 0.2885
Meta loss on this task batch = 3.8908e-01, Meta loss averaged over last 500 steps = 3.2149e-01, PNorm = 115.6036, GNorm = 0.3254
Meta loss on this task batch = 3.2405e-01, Meta loss averaged over last 500 steps = 3.2143e-01, PNorm = 115.6106, GNorm = 0.4836
Meta loss on this task batch = 3.6802e-01, Meta loss averaged over last 500 steps = 3.2159e-01, PNorm = 115.6167, GNorm = 0.4428
Meta loss on this task batch = 3.1134e-01, Meta loss averaged over last 500 steps = 3.2162e-01, PNorm = 115.6231, GNorm = 0.4690
Meta loss on this task batch = 3.2805e-01, Meta loss averaged over last 500 steps = 3.2160e-01, PNorm = 115.6294, GNorm = 0.3866
Meta loss on this task batch = 2.5964e-01, Meta loss averaged over last 500 steps = 3.2150e-01, PNorm = 115.6358, GNorm = 0.2467
Meta loss on this task batch = 3.6788e-01, Meta loss averaged over last 500 steps = 3.2155e-01, PNorm = 115.6415, GNorm = 0.2773
Meta loss on this task batch = 3.2252e-01, Meta loss averaged over last 500 steps = 3.2151e-01, PNorm = 115.6470, GNorm = 0.2867
Meta loss on this task batch = 3.3902e-01, Meta loss averaged over last 500 steps = 3.2149e-01, PNorm = 115.6525, GNorm = 0.2409
Meta loss on this task batch = 3.4036e-01, Meta loss averaged over last 500 steps = 3.2150e-01, PNorm = 115.6577, GNorm = 0.3101
Meta loss on this task batch = 3.4099e-01, Meta loss averaged over last 500 steps = 3.2152e-01, PNorm = 115.6631, GNorm = 0.2854
Meta loss on this task batch = 3.7693e-01, Meta loss averaged over last 500 steps = 3.2168e-01, PNorm = 115.6683, GNorm = 0.4098
Meta loss on this task batch = 2.5846e-01, Meta loss averaged over last 500 steps = 3.2160e-01, PNorm = 115.6740, GNorm = 0.2408
Took 110.35565876960754 seconds to complete one epoch of meta training
Took 118.3645989894867 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478370
Epoch 335
Meta loss on this task batch = 3.3277e-01, Meta loss averaged over last 500 steps = 3.2160e-01, PNorm = 115.6794, GNorm = 0.2889
Meta loss on this task batch = 3.7851e-01, Meta loss averaged over last 500 steps = 3.2179e-01, PNorm = 115.6846, GNorm = 0.3650
Meta loss on this task batch = 3.5779e-01, Meta loss averaged over last 500 steps = 3.2180e-01, PNorm = 115.6898, GNorm = 0.2288
Meta loss on this task batch = 3.5377e-01, Meta loss averaged over last 500 steps = 3.2182e-01, PNorm = 115.6950, GNorm = 0.2933
Meta loss on this task batch = 2.8298e-01, Meta loss averaged over last 500 steps = 3.2182e-01, PNorm = 115.7011, GNorm = 0.2481
Meta loss on this task batch = 3.1034e-01, Meta loss averaged over last 500 steps = 3.2174e-01, PNorm = 115.7071, GNorm = 0.2246
Meta loss on this task batch = 3.1584e-01, Meta loss averaged over last 500 steps = 3.2161e-01, PNorm = 115.7142, GNorm = 0.2558
Meta loss on this task batch = 2.9757e-01, Meta loss averaged over last 500 steps = 3.2168e-01, PNorm = 115.7221, GNorm = 0.2591
Meta loss on this task batch = 3.2510e-01, Meta loss averaged over last 500 steps = 3.2177e-01, PNorm = 115.7298, GNorm = 0.2508
Meta loss on this task batch = 3.4382e-01, Meta loss averaged over last 500 steps = 3.2183e-01, PNorm = 115.7371, GNorm = 0.3220
Meta loss on this task batch = 3.2427e-01, Meta loss averaged over last 500 steps = 3.2176e-01, PNorm = 115.7442, GNorm = 0.3012
Meta loss on this task batch = 3.1705e-01, Meta loss averaged over last 500 steps = 3.2164e-01, PNorm = 115.7523, GNorm = 0.2904
Meta loss on this task batch = 3.3527e-01, Meta loss averaged over last 500 steps = 3.2170e-01, PNorm = 115.7598, GNorm = 0.2565
Meta loss on this task batch = 2.6989e-01, Meta loss averaged over last 500 steps = 3.2162e-01, PNorm = 115.7678, GNorm = 0.4042
Meta loss on this task batch = 3.1045e-01, Meta loss averaged over last 500 steps = 3.2166e-01, PNorm = 115.7756, GNorm = 0.3616
Meta loss on this task batch = 2.8690e-01, Meta loss averaged over last 500 steps = 3.2152e-01, PNorm = 115.7834, GNorm = 0.2344
Meta loss on this task batch = 2.8476e-01, Meta loss averaged over last 500 steps = 3.2137e-01, PNorm = 115.7917, GNorm = 0.2453
Meta loss on this task batch = 3.0383e-01, Meta loss averaged over last 500 steps = 3.2135e-01, PNorm = 115.7999, GNorm = 0.2282
Meta loss on this task batch = 2.9943e-01, Meta loss averaged over last 500 steps = 3.2121e-01, PNorm = 115.8084, GNorm = 0.2844
Took 111.84247398376465 seconds to complete one epoch of meta training
Took 119.57130479812622 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475159
Epoch 336
Meta loss on this task batch = 2.9318e-01, Meta loss averaged over last 500 steps = 3.2113e-01, PNorm = 115.8166, GNorm = 0.2357
Meta loss on this task batch = 3.0630e-01, Meta loss averaged over last 500 steps = 3.2115e-01, PNorm = 115.8238, GNorm = 0.2945
Meta loss on this task batch = 3.2140e-01, Meta loss averaged over last 500 steps = 3.2104e-01, PNorm = 115.8295, GNorm = 0.3480
Meta loss on this task batch = 3.1214e-01, Meta loss averaged over last 500 steps = 3.2107e-01, PNorm = 115.8338, GNorm = 0.3068
Meta loss on this task batch = 3.4087e-01, Meta loss averaged over last 500 steps = 3.2113e-01, PNorm = 115.8375, GNorm = 0.3400
Meta loss on this task batch = 3.2787e-01, Meta loss averaged over last 500 steps = 3.2116e-01, PNorm = 115.8417, GNorm = 0.2740
Meta loss on this task batch = 3.5508e-01, Meta loss averaged over last 500 steps = 3.2127e-01, PNorm = 115.8456, GNorm = 0.3592
Meta loss on this task batch = 2.6146e-01, Meta loss averaged over last 500 steps = 3.2118e-01, PNorm = 115.8503, GNorm = 0.2272
Meta loss on this task batch = 2.4071e-01, Meta loss averaged over last 500 steps = 3.2092e-01, PNorm = 115.8564, GNorm = 0.2439
Meta loss on this task batch = 3.0718e-01, Meta loss averaged over last 500 steps = 3.2088e-01, PNorm = 115.8627, GNorm = 0.2781
Meta loss on this task batch = 3.1849e-01, Meta loss averaged over last 500 steps = 3.2091e-01, PNorm = 115.8696, GNorm = 0.2862
Meta loss on this task batch = 3.4054e-01, Meta loss averaged over last 500 steps = 3.2099e-01, PNorm = 115.8761, GNorm = 0.2823
Meta loss on this task batch = 3.5502e-01, Meta loss averaged over last 500 steps = 3.2110e-01, PNorm = 115.8815, GNorm = 0.3209
Meta loss on this task batch = 2.9571e-01, Meta loss averaged over last 500 steps = 3.2095e-01, PNorm = 115.8871, GNorm = 0.2369
Meta loss on this task batch = 3.3230e-01, Meta loss averaged over last 500 steps = 3.2095e-01, PNorm = 115.8923, GNorm = 0.2823
Meta loss on this task batch = 3.0144e-01, Meta loss averaged over last 500 steps = 3.2086e-01, PNorm = 115.8971, GNorm = 0.4059
Meta loss on this task batch = 3.2361e-01, Meta loss averaged over last 500 steps = 3.2093e-01, PNorm = 115.9025, GNorm = 0.3305
Meta loss on this task batch = 3.4072e-01, Meta loss averaged over last 500 steps = 3.2091e-01, PNorm = 115.9068, GNorm = 0.3820
Meta loss on this task batch = 2.8411e-01, Meta loss averaged over last 500 steps = 3.2095e-01, PNorm = 115.9111, GNorm = 0.3006
Took 109.80196952819824 seconds to complete one epoch of meta training
Took 117.46740889549255 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503269
Epoch 337
Meta loss on this task batch = 3.1245e-01, Meta loss averaged over last 500 steps = 3.2100e-01, PNorm = 115.9154, GNorm = 0.2449
Meta loss on this task batch = 3.1784e-01, Meta loss averaged over last 500 steps = 3.2103e-01, PNorm = 115.9200, GNorm = 0.3790
Meta loss on this task batch = 3.6210e-01, Meta loss averaged over last 500 steps = 3.2099e-01, PNorm = 115.9242, GNorm = 0.3385
Meta loss on this task batch = 3.3781e-01, Meta loss averaged over last 500 steps = 3.2098e-01, PNorm = 115.9282, GNorm = 0.2779
Meta loss on this task batch = 2.7117e-01, Meta loss averaged over last 500 steps = 3.2086e-01, PNorm = 115.9329, GNorm = 0.2970
Meta loss on this task batch = 3.2142e-01, Meta loss averaged over last 500 steps = 3.2081e-01, PNorm = 115.9373, GNorm = 0.2452
Meta loss on this task batch = 2.8863e-01, Meta loss averaged over last 500 steps = 3.2086e-01, PNorm = 115.9421, GNorm = 0.3013
Meta loss on this task batch = 3.2921e-01, Meta loss averaged over last 500 steps = 3.2093e-01, PNorm = 115.9474, GNorm = 0.2915
Meta loss on this task batch = 3.1741e-01, Meta loss averaged over last 500 steps = 3.2089e-01, PNorm = 115.9536, GNorm = 0.2732
Meta loss on this task batch = 3.3770e-01, Meta loss averaged over last 500 steps = 3.2087e-01, PNorm = 115.9600, GNorm = 0.2621
Meta loss on this task batch = 2.8546e-01, Meta loss averaged over last 500 steps = 3.2068e-01, PNorm = 115.9672, GNorm = 0.2615
Meta loss on this task batch = 2.8370e-01, Meta loss averaged over last 500 steps = 3.2060e-01, PNorm = 115.9757, GNorm = 0.2543
Meta loss on this task batch = 2.9434e-01, Meta loss averaged over last 500 steps = 3.2051e-01, PNorm = 115.9849, GNorm = 0.2721
Meta loss on this task batch = 3.3929e-01, Meta loss averaged over last 500 steps = 3.2051e-01, PNorm = 115.9942, GNorm = 0.3640
Meta loss on this task batch = 2.9536e-01, Meta loss averaged over last 500 steps = 3.2048e-01, PNorm = 116.0038, GNorm = 0.2346
Meta loss on this task batch = 3.1475e-01, Meta loss averaged over last 500 steps = 3.2045e-01, PNorm = 116.0119, GNorm = 0.2337
Meta loss on this task batch = 3.7546e-01, Meta loss averaged over last 500 steps = 3.2061e-01, PNorm = 116.0194, GNorm = 0.3415
Meta loss on this task batch = 3.5188e-01, Meta loss averaged over last 500 steps = 3.2073e-01, PNorm = 116.0254, GNorm = 0.3059
Meta loss on this task batch = 3.4380e-01, Meta loss averaged over last 500 steps = 3.2078e-01, PNorm = 116.0308, GNorm = 0.4027
Took 111.9468150138855 seconds to complete one epoch of meta training
Took 120.105233669281 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482837
Epoch 338
Meta loss on this task batch = 2.7443e-01, Meta loss averaged over last 500 steps = 3.2054e-01, PNorm = 116.0363, GNorm = 0.2706
Meta loss on this task batch = 3.2359e-01, Meta loss averaged over last 500 steps = 3.2049e-01, PNorm = 116.0422, GNorm = 0.3973
Meta loss on this task batch = 3.1628e-01, Meta loss averaged over last 500 steps = 3.2048e-01, PNorm = 116.0486, GNorm = 0.3159
Meta loss on this task batch = 2.5715e-01, Meta loss averaged over last 500 steps = 3.2043e-01, PNorm = 116.0558, GNorm = 0.2459
Meta loss on this task batch = 3.6443e-01, Meta loss averaged over last 500 steps = 3.2054e-01, PNorm = 116.0634, GNorm = 0.3072
Meta loss on this task batch = 2.5873e-01, Meta loss averaged over last 500 steps = 3.2041e-01, PNorm = 116.0714, GNorm = 0.2950
Meta loss on this task batch = 3.0976e-01, Meta loss averaged over last 500 steps = 3.2037e-01, PNorm = 116.0791, GNorm = 0.3735
Meta loss on this task batch = 3.0928e-01, Meta loss averaged over last 500 steps = 3.2035e-01, PNorm = 116.0861, GNorm = 0.2759
Meta loss on this task batch = 3.0714e-01, Meta loss averaged over last 500 steps = 3.2037e-01, PNorm = 116.0921, GNorm = 0.3991
Meta loss on this task batch = 3.4136e-01, Meta loss averaged over last 500 steps = 3.2034e-01, PNorm = 116.0973, GNorm = 0.2978
Meta loss on this task batch = 3.9706e-01, Meta loss averaged over last 500 steps = 3.2045e-01, PNorm = 116.1019, GNorm = 0.2945
Meta loss on this task batch = 2.8893e-01, Meta loss averaged over last 500 steps = 3.2047e-01, PNorm = 116.1074, GNorm = 0.4251
Meta loss on this task batch = 3.3549e-01, Meta loss averaged over last 500 steps = 3.2046e-01, PNorm = 116.1124, GNorm = 0.3121
Meta loss on this task batch = 3.1335e-01, Meta loss averaged over last 500 steps = 3.2044e-01, PNorm = 116.1181, GNorm = 0.3126
Meta loss on this task batch = 2.8011e-01, Meta loss averaged over last 500 steps = 3.2039e-01, PNorm = 116.1252, GNorm = 0.3551
Meta loss on this task batch = 3.0749e-01, Meta loss averaged over last 500 steps = 3.2024e-01, PNorm = 116.1326, GNorm = 0.2845
Meta loss on this task batch = 3.1023e-01, Meta loss averaged over last 500 steps = 3.2024e-01, PNorm = 116.1395, GNorm = 0.3142
Meta loss on this task batch = 3.1974e-01, Meta loss averaged over last 500 steps = 3.2014e-01, PNorm = 116.1466, GNorm = 0.2671
Meta loss on this task batch = 3.1060e-01, Meta loss averaged over last 500 steps = 3.1997e-01, PNorm = 116.1546, GNorm = 0.3147
Took 112.22947216033936 seconds to complete one epoch of meta training
Took 120.22946572303772 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463408
Epoch 339
Meta loss on this task batch = 3.6394e-01, Meta loss averaged over last 500 steps = 3.2011e-01, PNorm = 116.1626, GNorm = 0.3073
Meta loss on this task batch = 3.1179e-01, Meta loss averaged over last 500 steps = 3.2003e-01, PNorm = 116.1710, GNorm = 0.2737
Meta loss on this task batch = 3.4031e-01, Meta loss averaged over last 500 steps = 3.2008e-01, PNorm = 116.1788, GNorm = 0.2495
Meta loss on this task batch = 3.3175e-01, Meta loss averaged over last 500 steps = 3.2019e-01, PNorm = 116.1857, GNorm = 0.3160
Meta loss on this task batch = 3.5004e-01, Meta loss averaged over last 500 steps = 3.2019e-01, PNorm = 116.1916, GNorm = 0.3127
Meta loss on this task batch = 3.1962e-01, Meta loss averaged over last 500 steps = 3.2036e-01, PNorm = 116.1972, GNorm = 0.2384
Meta loss on this task batch = 2.9043e-01, Meta loss averaged over last 500 steps = 3.2028e-01, PNorm = 116.2035, GNorm = 0.3281
Meta loss on this task batch = 3.0535e-01, Meta loss averaged over last 500 steps = 3.2028e-01, PNorm = 116.2092, GNorm = 0.2537
Meta loss on this task batch = 3.1525e-01, Meta loss averaged over last 500 steps = 3.2012e-01, PNorm = 116.2140, GNorm = 0.2497
Meta loss on this task batch = 3.1810e-01, Meta loss averaged over last 500 steps = 3.2012e-01, PNorm = 116.2194, GNorm = 0.3339
Meta loss on this task batch = 2.8980e-01, Meta loss averaged over last 500 steps = 3.2007e-01, PNorm = 116.2258, GNorm = 0.4446
Meta loss on this task batch = 3.2725e-01, Meta loss averaged over last 500 steps = 3.2004e-01, PNorm = 116.2314, GNorm = 0.4343
Meta loss on this task batch = 2.7607e-01, Meta loss averaged over last 500 steps = 3.1978e-01, PNorm = 116.2373, GNorm = 0.2085
Meta loss on this task batch = 3.3493e-01, Meta loss averaged over last 500 steps = 3.1982e-01, PNorm = 116.2432, GNorm = 0.3047
Meta loss on this task batch = 3.1333e-01, Meta loss averaged over last 500 steps = 3.1981e-01, PNorm = 116.2493, GNorm = 0.3526
Meta loss on this task batch = 3.4141e-01, Meta loss averaged over last 500 steps = 3.1980e-01, PNorm = 116.2555, GNorm = 0.2939
Meta loss on this task batch = 3.7345e-01, Meta loss averaged over last 500 steps = 3.2000e-01, PNorm = 116.2595, GNorm = 0.3838
Meta loss on this task batch = 2.8837e-01, Meta loss averaged over last 500 steps = 3.1998e-01, PNorm = 116.2644, GNorm = 0.2961
Meta loss on this task batch = 3.6234e-01, Meta loss averaged over last 500 steps = 3.2017e-01, PNorm = 116.2690, GNorm = 0.2985
Took 112.35392785072327 seconds to complete one epoch of meta training
Took 118.9567391872406 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473242
Epoch 340
Meta loss on this task batch = 2.9700e-01, Meta loss averaged over last 500 steps = 3.2025e-01, PNorm = 116.2737, GNorm = 0.3931
Meta loss on this task batch = 3.2486e-01, Meta loss averaged over last 500 steps = 3.2017e-01, PNorm = 116.2788, GNorm = 0.2624
Meta loss on this task batch = 3.7684e-01, Meta loss averaged over last 500 steps = 3.2036e-01, PNorm = 116.2837, GNorm = 0.2664
Meta loss on this task batch = 3.0375e-01, Meta loss averaged over last 500 steps = 3.2025e-01, PNorm = 116.2886, GNorm = 0.2568
Meta loss on this task batch = 3.1034e-01, Meta loss averaged over last 500 steps = 3.2033e-01, PNorm = 116.2943, GNorm = 0.2568
Meta loss on this task batch = 3.2412e-01, Meta loss averaged over last 500 steps = 3.2037e-01, PNorm = 116.2999, GNorm = 0.3164
Meta loss on this task batch = 3.0864e-01, Meta loss averaged over last 500 steps = 3.2033e-01, PNorm = 116.3065, GNorm = 0.3202
Meta loss on this task batch = 2.5715e-01, Meta loss averaged over last 500 steps = 3.2036e-01, PNorm = 116.3139, GNorm = 0.2585
Meta loss on this task batch = 3.3982e-01, Meta loss averaged over last 500 steps = 3.2036e-01, PNorm = 116.3217, GNorm = 0.2510
Meta loss on this task batch = 3.6526e-01, Meta loss averaged over last 500 steps = 3.2045e-01, PNorm = 116.3293, GNorm = 0.2914
Meta loss on this task batch = 3.6213e-01, Meta loss averaged over last 500 steps = 3.2052e-01, PNorm = 116.3356, GNorm = 0.3476
Meta loss on this task batch = 2.8980e-01, Meta loss averaged over last 500 steps = 3.2046e-01, PNorm = 116.3430, GNorm = 0.2677
Meta loss on this task batch = 2.9879e-01, Meta loss averaged over last 500 steps = 3.2048e-01, PNorm = 116.3509, GNorm = 0.2575
Meta loss on this task batch = 2.6203e-01, Meta loss averaged over last 500 steps = 3.2024e-01, PNorm = 116.3595, GNorm = 0.2487
Meta loss on this task batch = 3.5208e-01, Meta loss averaged over last 500 steps = 3.2030e-01, PNorm = 116.3667, GNorm = 0.2645
Meta loss on this task batch = 2.7954e-01, Meta loss averaged over last 500 steps = 3.2023e-01, PNorm = 116.3743, GNorm = 0.2255
Meta loss on this task batch = 3.2115e-01, Meta loss averaged over last 500 steps = 3.2023e-01, PNorm = 116.3822, GNorm = 0.2585
Meta loss on this task batch = 2.3637e-01, Meta loss averaged over last 500 steps = 3.2000e-01, PNorm = 116.3901, GNorm = 0.2294
Meta loss on this task batch = 3.6823e-01, Meta loss averaged over last 500 steps = 3.2009e-01, PNorm = 116.3978, GNorm = 0.4613
Took 113.08527493476868 seconds to complete one epoch of meta training
Took 120.85772895812988 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464627
Epoch 341
Meta loss on this task batch = 3.0502e-01, Meta loss averaged over last 500 steps = 3.2010e-01, PNorm = 116.4054, GNorm = 0.2614
Meta loss on this task batch = 3.3486e-01, Meta loss averaged over last 500 steps = 3.2010e-01, PNorm = 116.4127, GNorm = 0.2531
Meta loss on this task batch = 3.1645e-01, Meta loss averaged over last 500 steps = 3.2004e-01, PNorm = 116.4202, GNorm = 0.5109
Meta loss on this task batch = 3.5620e-01, Meta loss averaged over last 500 steps = 3.2013e-01, PNorm = 116.4268, GNorm = 0.3829
Meta loss on this task batch = 3.6057e-01, Meta loss averaged over last 500 steps = 3.2024e-01, PNorm = 116.4334, GNorm = 0.3031
Meta loss on this task batch = 3.7837e-01, Meta loss averaged over last 500 steps = 3.2027e-01, PNorm = 116.4395, GNorm = 0.2889
Meta loss on this task batch = 2.5399e-01, Meta loss averaged over last 500 steps = 3.2011e-01, PNorm = 116.4454, GNorm = 0.3702
Meta loss on this task batch = 3.0236e-01, Meta loss averaged over last 500 steps = 3.2003e-01, PNorm = 116.4510, GNorm = 0.2980
Meta loss on this task batch = 2.7721e-01, Meta loss averaged over last 500 steps = 3.1997e-01, PNorm = 116.4578, GNorm = 0.3014
Meta loss on this task batch = 3.0176e-01, Meta loss averaged over last 500 steps = 3.1988e-01, PNorm = 116.4647, GNorm = 0.3440
Meta loss on this task batch = 3.8208e-01, Meta loss averaged over last 500 steps = 3.2009e-01, PNorm = 116.4702, GNorm = 0.3409
Meta loss on this task batch = 2.4096e-01, Meta loss averaged over last 500 steps = 3.1991e-01, PNorm = 116.4772, GNorm = 0.2553
Meta loss on this task batch = 3.3230e-01, Meta loss averaged over last 500 steps = 3.1989e-01, PNorm = 116.4846, GNorm = 0.2667
Meta loss on this task batch = 4.2284e-01, Meta loss averaged over last 500 steps = 3.2023e-01, PNorm = 116.4912, GNorm = 0.3399
Meta loss on this task batch = 2.8379e-01, Meta loss averaged over last 500 steps = 3.2012e-01, PNorm = 116.4982, GNorm = 0.2236
Meta loss on this task batch = 3.9217e-01, Meta loss averaged over last 500 steps = 3.2028e-01, PNorm = 116.5041, GNorm = 0.3939
Meta loss on this task batch = 3.4537e-01, Meta loss averaged over last 500 steps = 3.2024e-01, PNorm = 116.5112, GNorm = 0.3450
Meta loss on this task batch = 2.8149e-01, Meta loss averaged over last 500 steps = 3.2016e-01, PNorm = 116.5208, GNorm = 0.3857
Meta loss on this task batch = 3.4525e-01, Meta loss averaged over last 500 steps = 3.2013e-01, PNorm = 116.5291, GNorm = 0.5146
Took 112.38222098350525 seconds to complete one epoch of meta training
Took 120.00124955177307 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.426090
Epoch 342
Meta loss on this task batch = 3.2011e-01, Meta loss averaged over last 500 steps = 3.2012e-01, PNorm = 116.5366, GNorm = 0.2888
Meta loss on this task batch = 3.0705e-01, Meta loss averaged over last 500 steps = 3.2005e-01, PNorm = 116.5441, GNorm = 0.2626
Meta loss on this task batch = 3.3512e-01, Meta loss averaged over last 500 steps = 3.2003e-01, PNorm = 116.5509, GNorm = 0.3166
Meta loss on this task batch = 2.9946e-01, Meta loss averaged over last 500 steps = 3.1996e-01, PNorm = 116.5583, GNorm = 0.3007
Meta loss on this task batch = 3.0841e-01, Meta loss averaged over last 500 steps = 3.1996e-01, PNorm = 116.5658, GNorm = 0.2674
Meta loss on this task batch = 2.8720e-01, Meta loss averaged over last 500 steps = 3.1996e-01, PNorm = 116.5733, GNorm = 0.3645
Meta loss on this task batch = 3.1445e-01, Meta loss averaged over last 500 steps = 3.2005e-01, PNorm = 116.5807, GNorm = 0.2563
Meta loss on this task batch = 3.5361e-01, Meta loss averaged over last 500 steps = 3.2014e-01, PNorm = 116.5873, GNorm = 0.3791
Meta loss on this task batch = 3.1297e-01, Meta loss averaged over last 500 steps = 3.1999e-01, PNorm = 116.5947, GNorm = 0.3057
Meta loss on this task batch = 2.4800e-01, Meta loss averaged over last 500 steps = 3.1978e-01, PNorm = 116.6027, GNorm = 0.2725
Meta loss on this task batch = 3.1514e-01, Meta loss averaged over last 500 steps = 3.1973e-01, PNorm = 116.6104, GNorm = 0.2885
Meta loss on this task batch = 3.3244e-01, Meta loss averaged over last 500 steps = 3.1957e-01, PNorm = 116.6186, GNorm = 0.3769
Meta loss on this task batch = 3.3863e-01, Meta loss averaged over last 500 steps = 3.1965e-01, PNorm = 116.6266, GNorm = 0.3068
Meta loss on this task batch = 3.0003e-01, Meta loss averaged over last 500 steps = 3.1953e-01, PNorm = 116.6347, GNorm = 0.4544
Meta loss on this task batch = 3.5565e-01, Meta loss averaged over last 500 steps = 3.1952e-01, PNorm = 116.6416, GNorm = 0.3814
Meta loss on this task batch = 3.6388e-01, Meta loss averaged over last 500 steps = 3.1950e-01, PNorm = 116.6485, GNorm = 0.3033
Meta loss on this task batch = 3.1377e-01, Meta loss averaged over last 500 steps = 3.1943e-01, PNorm = 116.6562, GNorm = 0.2654
Meta loss on this task batch = 3.6659e-01, Meta loss averaged over last 500 steps = 3.1962e-01, PNorm = 116.6635, GNorm = 0.4472
Meta loss on this task batch = 3.0106e-01, Meta loss averaged over last 500 steps = 3.1959e-01, PNorm = 116.6722, GNorm = 0.3577
Took 112.05529475212097 seconds to complete one epoch of meta training
Took 120.20036745071411 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469948
Epoch 343
Meta loss on this task batch = 3.3780e-01, Meta loss averaged over last 500 steps = 3.1967e-01, PNorm = 116.6809, GNorm = 0.2522
Meta loss on this task batch = 2.9504e-01, Meta loss averaged over last 500 steps = 3.1969e-01, PNorm = 116.6898, GNorm = 0.2616
Meta loss on this task batch = 2.8653e-01, Meta loss averaged over last 500 steps = 3.1972e-01, PNorm = 116.6997, GNorm = 0.2241
Meta loss on this task batch = 3.1330e-01, Meta loss averaged over last 500 steps = 3.1975e-01, PNorm = 116.7091, GNorm = 0.3703
Meta loss on this task batch = 3.3939e-01, Meta loss averaged over last 500 steps = 3.1986e-01, PNorm = 116.7189, GNorm = 0.2355
Meta loss on this task batch = 2.6885e-01, Meta loss averaged over last 500 steps = 3.1962e-01, PNorm = 116.7290, GNorm = 0.2443
Meta loss on this task batch = 2.9534e-01, Meta loss averaged over last 500 steps = 3.1954e-01, PNorm = 116.7390, GNorm = 0.3617
Meta loss on this task batch = 3.5418e-01, Meta loss averaged over last 500 steps = 3.1964e-01, PNorm = 116.7474, GNorm = 0.4498
Meta loss on this task batch = 2.9665e-01, Meta loss averaged over last 500 steps = 3.1963e-01, PNorm = 116.7568, GNorm = 0.5190
Meta loss on this task batch = 2.9506e-01, Meta loss averaged over last 500 steps = 3.1958e-01, PNorm = 116.7648, GNorm = 0.5329
Meta loss on this task batch = 3.3025e-01, Meta loss averaged over last 500 steps = 3.1970e-01, PNorm = 116.7734, GNorm = 0.3872
Meta loss on this task batch = 3.4786e-01, Meta loss averaged over last 500 steps = 3.1968e-01, PNorm = 116.7808, GNorm = 0.2698
Meta loss on this task batch = 3.2709e-01, Meta loss averaged over last 500 steps = 3.1974e-01, PNorm = 116.7874, GNorm = 0.2819
Meta loss on this task batch = 2.6406e-01, Meta loss averaged over last 500 steps = 3.1969e-01, PNorm = 116.7932, GNorm = 0.2832
Meta loss on this task batch = 3.7574e-01, Meta loss averaged over last 500 steps = 3.1986e-01, PNorm = 116.7970, GNorm = 0.5173
Meta loss on this task batch = 3.9634e-01, Meta loss averaged over last 500 steps = 3.1997e-01, PNorm = 116.8005, GNorm = 0.4043
Meta loss on this task batch = 3.7905e-01, Meta loss averaged over last 500 steps = 3.2004e-01, PNorm = 116.8038, GNorm = 0.3135
Meta loss on this task batch = 3.2708e-01, Meta loss averaged over last 500 steps = 3.2007e-01, PNorm = 116.8073, GNorm = 0.2596
Meta loss on this task batch = 3.5670e-01, Meta loss averaged over last 500 steps = 3.2014e-01, PNorm = 116.8111, GNorm = 0.4108
Took 115.35498833656311 seconds to complete one epoch of meta training
Took 123.12042570114136 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.438326
Epoch 344
Meta loss on this task batch = 2.7384e-01, Meta loss averaged over last 500 steps = 3.1991e-01, PNorm = 116.8168, GNorm = 0.2882
Meta loss on this task batch = 2.9758e-01, Meta loss averaged over last 500 steps = 3.1998e-01, PNorm = 116.8218, GNorm = 0.2537
Meta loss on this task batch = 3.3083e-01, Meta loss averaged over last 500 steps = 3.1991e-01, PNorm = 116.8277, GNorm = 0.3096
Meta loss on this task batch = 3.2053e-01, Meta loss averaged over last 500 steps = 3.1989e-01, PNorm = 116.8354, GNorm = 0.3905
Meta loss on this task batch = 3.1691e-01, Meta loss averaged over last 500 steps = 3.1997e-01, PNorm = 116.8435, GNorm = 0.2522
Meta loss on this task batch = 3.1506e-01, Meta loss averaged over last 500 steps = 3.1988e-01, PNorm = 116.8515, GNorm = 0.3322
Meta loss on this task batch = 2.9148e-01, Meta loss averaged over last 500 steps = 3.1981e-01, PNorm = 116.8597, GNorm = 0.4296
Meta loss on this task batch = 3.4943e-01, Meta loss averaged over last 500 steps = 3.1978e-01, PNorm = 116.8671, GNorm = 0.3191
Meta loss on this task batch = 3.2670e-01, Meta loss averaged over last 500 steps = 3.1989e-01, PNorm = 116.8753, GNorm = 0.3799
Meta loss on this task batch = 3.0727e-01, Meta loss averaged over last 500 steps = 3.1992e-01, PNorm = 116.8834, GNorm = 0.5023
Meta loss on this task batch = 3.0829e-01, Meta loss averaged over last 500 steps = 3.2001e-01, PNorm = 116.8918, GNorm = 0.2846
Meta loss on this task batch = 3.8405e-01, Meta loss averaged over last 500 steps = 3.2013e-01, PNorm = 116.9001, GNorm = 0.2794
Meta loss on this task batch = 2.6812e-01, Meta loss averaged over last 500 steps = 3.2001e-01, PNorm = 116.9089, GNorm = 0.2808
Meta loss on this task batch = 3.1516e-01, Meta loss averaged over last 500 steps = 3.1997e-01, PNorm = 116.9174, GNorm = 0.2780
Meta loss on this task batch = 3.3670e-01, Meta loss averaged over last 500 steps = 3.1991e-01, PNorm = 116.9252, GNorm = 0.2953
Meta loss on this task batch = 3.9043e-01, Meta loss averaged over last 500 steps = 3.2019e-01, PNorm = 116.9322, GNorm = 0.3149
Meta loss on this task batch = 3.5243e-01, Meta loss averaged over last 500 steps = 3.2026e-01, PNorm = 116.9389, GNorm = 0.2620
Meta loss on this task batch = 2.3091e-01, Meta loss averaged over last 500 steps = 3.2016e-01, PNorm = 116.9463, GNorm = 0.2206
Meta loss on this task batch = 4.1375e-01, Meta loss averaged over last 500 steps = 3.2025e-01, PNorm = 116.9532, GNorm = 0.4138
Took 111.10330104827881 seconds to complete one epoch of meta training
Took 119.18912625312805 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450043
Epoch 345
Meta loss on this task batch = 3.7441e-01, Meta loss averaged over last 500 steps = 3.2028e-01, PNorm = 116.9591, GNorm = 0.2588
Meta loss on this task batch = 3.0005e-01, Meta loss averaged over last 500 steps = 3.2021e-01, PNorm = 116.9655, GNorm = 0.2475
Meta loss on this task batch = 3.3887e-01, Meta loss averaged over last 500 steps = 3.2014e-01, PNorm = 116.9708, GNorm = 0.3348
Meta loss on this task batch = 3.5047e-01, Meta loss averaged over last 500 steps = 3.2025e-01, PNorm = 116.9768, GNorm = 0.3222
Meta loss on this task batch = 2.8885e-01, Meta loss averaged over last 500 steps = 3.2014e-01, PNorm = 116.9834, GNorm = 0.3067
Meta loss on this task batch = 3.2798e-01, Meta loss averaged over last 500 steps = 3.2022e-01, PNorm = 116.9899, GNorm = 0.2398
Meta loss on this task batch = 3.0011e-01, Meta loss averaged over last 500 steps = 3.2024e-01, PNorm = 116.9956, GNorm = 0.4403
Meta loss on this task batch = 2.9108e-01, Meta loss averaged over last 500 steps = 3.2005e-01, PNorm = 117.0020, GNorm = 0.3561
Meta loss on this task batch = 3.1529e-01, Meta loss averaged over last 500 steps = 3.2006e-01, PNorm = 117.0084, GNorm = 0.2704
Meta loss on this task batch = 3.2384e-01, Meta loss averaged over last 500 steps = 3.2012e-01, PNorm = 117.0155, GNorm = 0.2710
Meta loss on this task batch = 3.0997e-01, Meta loss averaged over last 500 steps = 3.2017e-01, PNorm = 117.0225, GNorm = 0.2853
Meta loss on this task batch = 3.1076e-01, Meta loss averaged over last 500 steps = 3.2017e-01, PNorm = 117.0304, GNorm = 0.3210
Meta loss on this task batch = 3.3447e-01, Meta loss averaged over last 500 steps = 3.2016e-01, PNorm = 117.0379, GNorm = 0.2355
Meta loss on this task batch = 3.4177e-01, Meta loss averaged over last 500 steps = 3.2019e-01, PNorm = 117.0448, GNorm = 0.2385
Meta loss on this task batch = 3.2806e-01, Meta loss averaged over last 500 steps = 3.2021e-01, PNorm = 117.0524, GNorm = 0.2813
Meta loss on this task batch = 3.3468e-01, Meta loss averaged over last 500 steps = 3.2030e-01, PNorm = 117.0602, GNorm = 0.2705
Meta loss on this task batch = 3.2979e-01, Meta loss averaged over last 500 steps = 3.2043e-01, PNorm = 117.0676, GNorm = 0.3685
Meta loss on this task batch = 2.8138e-01, Meta loss averaged over last 500 steps = 3.2034e-01, PNorm = 117.0755, GNorm = 0.4863
Meta loss on this task batch = 3.5242e-01, Meta loss averaged over last 500 steps = 3.2030e-01, PNorm = 117.0816, GNorm = 0.3808
Took 113.83140063285828 seconds to complete one epoch of meta training
Took 121.43017745018005 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454984
Epoch 346
Meta loss on this task batch = 2.9598e-01, Meta loss averaged over last 500 steps = 3.2030e-01, PNorm = 117.0877, GNorm = 0.2763
Meta loss on this task batch = 2.9153e-01, Meta loss averaged over last 500 steps = 3.2026e-01, PNorm = 117.0944, GNorm = 0.2606
Meta loss on this task batch = 3.0217e-01, Meta loss averaged over last 500 steps = 3.2016e-01, PNorm = 117.1014, GNorm = 0.2811
Meta loss on this task batch = 3.0983e-01, Meta loss averaged over last 500 steps = 3.1999e-01, PNorm = 117.1070, GNorm = 0.3633
Meta loss on this task batch = 2.8168e-01, Meta loss averaged over last 500 steps = 3.1985e-01, PNorm = 117.1128, GNorm = 0.2705
Meta loss on this task batch = 3.2104e-01, Meta loss averaged over last 500 steps = 3.1980e-01, PNorm = 117.1184, GNorm = 0.2875
Meta loss on this task batch = 3.8950e-01, Meta loss averaged over last 500 steps = 3.2002e-01, PNorm = 117.1240, GNorm = 0.3793
Meta loss on this task batch = 3.1127e-01, Meta loss averaged over last 500 steps = 3.1997e-01, PNorm = 117.1294, GNorm = 0.3270
Meta loss on this task batch = 2.6038e-01, Meta loss averaged over last 500 steps = 3.1983e-01, PNorm = 117.1350, GNorm = 0.2831
Meta loss on this task batch = 3.2741e-01, Meta loss averaged over last 500 steps = 3.1980e-01, PNorm = 117.1408, GNorm = 0.3591
Meta loss on this task batch = 4.1024e-01, Meta loss averaged over last 500 steps = 3.2007e-01, PNorm = 117.1465, GNorm = 0.3845
Meta loss on this task batch = 3.0500e-01, Meta loss averaged over last 500 steps = 3.2000e-01, PNorm = 117.1528, GNorm = 0.2514
Meta loss on this task batch = 3.4027e-01, Meta loss averaged over last 500 steps = 3.2014e-01, PNorm = 117.1596, GNorm = 0.2928
Meta loss on this task batch = 2.6307e-01, Meta loss averaged over last 500 steps = 3.2008e-01, PNorm = 117.1666, GNorm = 0.2712
Meta loss on this task batch = 2.9911e-01, Meta loss averaged over last 500 steps = 3.1991e-01, PNorm = 117.1744, GNorm = 0.4759
Meta loss on this task batch = 3.3306e-01, Meta loss averaged over last 500 steps = 3.1993e-01, PNorm = 117.1818, GNorm = 0.2646
Meta loss on this task batch = 3.4732e-01, Meta loss averaged over last 500 steps = 3.1996e-01, PNorm = 117.1896, GNorm = 0.2747
Meta loss on this task batch = 3.2151e-01, Meta loss averaged over last 500 steps = 3.1983e-01, PNorm = 117.1973, GNorm = 0.2400
Meta loss on this task batch = 3.6370e-01, Meta loss averaged over last 500 steps = 3.1993e-01, PNorm = 117.2051, GNorm = 0.4478
Took 121.59665632247925 seconds to complete one epoch of meta training
Took 129.8563506603241 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485437
Epoch 347
Meta loss on this task batch = 2.8208e-01, Meta loss averaged over last 500 steps = 3.1987e-01, PNorm = 117.2136, GNorm = 0.2479
Meta loss on this task batch = 2.7490e-01, Meta loss averaged over last 500 steps = 3.1995e-01, PNorm = 117.2221, GNorm = 0.3020
Meta loss on this task batch = 3.0565e-01, Meta loss averaged over last 500 steps = 3.1994e-01, PNorm = 117.2305, GNorm = 0.2449
Meta loss on this task batch = 3.3936e-01, Meta loss averaged over last 500 steps = 3.1999e-01, PNorm = 117.2389, GNorm = 0.2771
Meta loss on this task batch = 3.2607e-01, Meta loss averaged over last 500 steps = 3.1996e-01, PNorm = 117.2461, GNorm = 0.2502
Meta loss on this task batch = 3.0025e-01, Meta loss averaged over last 500 steps = 3.1978e-01, PNorm = 117.2530, GNorm = 0.3569
Meta loss on this task batch = 3.5097e-01, Meta loss averaged over last 500 steps = 3.1993e-01, PNorm = 117.2595, GNorm = 0.5787
Meta loss on this task batch = 2.9184e-01, Meta loss averaged over last 500 steps = 3.1996e-01, PNorm = 117.2660, GNorm = 0.2697
Meta loss on this task batch = 3.0767e-01, Meta loss averaged over last 500 steps = 3.1989e-01, PNorm = 117.2728, GNorm = 0.3106
Meta loss on this task batch = 3.2120e-01, Meta loss averaged over last 500 steps = 3.1985e-01, PNorm = 117.2795, GNorm = 0.2587
Meta loss on this task batch = 3.0912e-01, Meta loss averaged over last 500 steps = 3.1987e-01, PNorm = 117.2869, GNorm = 0.4423
Meta loss on this task batch = 3.0071e-01, Meta loss averaged over last 500 steps = 3.1990e-01, PNorm = 117.2944, GNorm = 0.2371
Meta loss on this task batch = 3.7497e-01, Meta loss averaged over last 500 steps = 3.2003e-01, PNorm = 117.3010, GNorm = 0.3271
Meta loss on this task batch = 3.6024e-01, Meta loss averaged over last 500 steps = 3.2008e-01, PNorm = 117.3068, GNorm = 0.3570
Meta loss on this task batch = 3.0958e-01, Meta loss averaged over last 500 steps = 3.1993e-01, PNorm = 117.3127, GNorm = 0.2752
Meta loss on this task batch = 2.7063e-01, Meta loss averaged over last 500 steps = 3.1981e-01, PNorm = 117.3185, GNorm = 0.3461
Meta loss on this task batch = 2.9379e-01, Meta loss averaged over last 500 steps = 3.1975e-01, PNorm = 117.3245, GNorm = 0.3616
Meta loss on this task batch = 3.1086e-01, Meta loss averaged over last 500 steps = 3.1973e-01, PNorm = 117.3296, GNorm = 0.2553
Meta loss on this task batch = 4.0619e-01, Meta loss averaged over last 500 steps = 3.1979e-01, PNorm = 117.3351, GNorm = 0.3666
Took 116.50920033454895 seconds to complete one epoch of meta training
Took 124.48385214805603 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457992
Epoch 348
Meta loss on this task batch = 3.1833e-01, Meta loss averaged over last 500 steps = 3.1987e-01, PNorm = 117.3403, GNorm = 0.3372
Meta loss on this task batch = 3.4272e-01, Meta loss averaged over last 500 steps = 3.2000e-01, PNorm = 117.3457, GNorm = 0.3361
Meta loss on this task batch = 3.1165e-01, Meta loss averaged over last 500 steps = 3.1999e-01, PNorm = 117.3501, GNorm = 0.3168
Meta loss on this task batch = 3.5825e-01, Meta loss averaged over last 500 steps = 3.2009e-01, PNorm = 117.3537, GNorm = 0.3338
Meta loss on this task batch = 3.6288e-01, Meta loss averaged over last 500 steps = 3.2009e-01, PNorm = 117.3572, GNorm = 0.3029
Meta loss on this task batch = 3.1342e-01, Meta loss averaged over last 500 steps = 3.2004e-01, PNorm = 117.3614, GNorm = 0.2308
Meta loss on this task batch = 2.6790e-01, Meta loss averaged over last 500 steps = 3.1989e-01, PNorm = 117.3658, GNorm = 0.2410
Meta loss on this task batch = 3.3203e-01, Meta loss averaged over last 500 steps = 3.1995e-01, PNorm = 117.3709, GNorm = 0.2838
Meta loss on this task batch = 2.5591e-01, Meta loss averaged over last 500 steps = 3.1981e-01, PNorm = 117.3767, GNorm = 0.2307
Meta loss on this task batch = 3.2678e-01, Meta loss averaged over last 500 steps = 3.1972e-01, PNorm = 117.3831, GNorm = 0.3142
Meta loss on this task batch = 3.1635e-01, Meta loss averaged over last 500 steps = 3.1977e-01, PNorm = 117.3893, GNorm = 0.2368
Meta loss on this task batch = 2.7934e-01, Meta loss averaged over last 500 steps = 3.1963e-01, PNorm = 117.3957, GNorm = 0.2678
Meta loss on this task batch = 2.9539e-01, Meta loss averaged over last 500 steps = 3.1963e-01, PNorm = 117.4023, GNorm = 0.2604
Meta loss on this task batch = 3.4705e-01, Meta loss averaged over last 500 steps = 3.1973e-01, PNorm = 117.4096, GNorm = 0.4569
Meta loss on this task batch = 3.2446e-01, Meta loss averaged over last 500 steps = 3.1979e-01, PNorm = 117.4163, GNorm = 0.3311
Meta loss on this task batch = 3.7126e-01, Meta loss averaged over last 500 steps = 3.1999e-01, PNorm = 117.4234, GNorm = 0.2557
Meta loss on this task batch = 2.9672e-01, Meta loss averaged over last 500 steps = 3.1990e-01, PNorm = 117.4303, GNorm = 0.3502
Meta loss on this task batch = 3.2175e-01, Meta loss averaged over last 500 steps = 3.1978e-01, PNorm = 117.4369, GNorm = 0.3063
Meta loss on this task batch = 3.0220e-01, Meta loss averaged over last 500 steps = 3.1980e-01, PNorm = 117.4441, GNorm = 0.2531
Took 203.86570858955383 seconds to complete one epoch of meta training
Took 211.62795042991638 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476078
Epoch 349
Meta loss on this task batch = 2.8755e-01, Meta loss averaged over last 500 steps = 3.1975e-01, PNorm = 117.4521, GNorm = 0.3140
Meta loss on this task batch = 3.0044e-01, Meta loss averaged over last 500 steps = 3.1982e-01, PNorm = 117.4595, GNorm = 0.4154
Meta loss on this task batch = 3.6440e-01, Meta loss averaged over last 500 steps = 3.2002e-01, PNorm = 117.4666, GNorm = 0.2739
Meta loss on this task batch = 2.5639e-01, Meta loss averaged over last 500 steps = 3.1986e-01, PNorm = 117.4745, GNorm = 0.2740
Meta loss on this task batch = 2.4515e-01, Meta loss averaged over last 500 steps = 3.1971e-01, PNorm = 117.4824, GNorm = 0.2194
Meta loss on this task batch = 3.4962e-01, Meta loss averaged over last 500 steps = 3.1968e-01, PNorm = 117.4897, GNorm = 0.3557
Meta loss on this task batch = 3.2880e-01, Meta loss averaged over last 500 steps = 3.1962e-01, PNorm = 117.4974, GNorm = 0.3375
Meta loss on this task batch = 3.0079e-01, Meta loss averaged over last 500 steps = 3.1955e-01, PNorm = 117.5048, GNorm = 0.4105
Meta loss on this task batch = 3.2489e-01, Meta loss averaged over last 500 steps = 3.1954e-01, PNorm = 117.5118, GNorm = 0.2454
Meta loss on this task batch = 2.8863e-01, Meta loss averaged over last 500 steps = 3.1960e-01, PNorm = 117.5190, GNorm = 0.2314
Meta loss on this task batch = 2.7651e-01, Meta loss averaged over last 500 steps = 3.1951e-01, PNorm = 117.5260, GNorm = 0.2558
Meta loss on this task batch = 3.1954e-01, Meta loss averaged over last 500 steps = 3.1945e-01, PNorm = 117.5324, GNorm = 0.2873
Meta loss on this task batch = 3.5507e-01, Meta loss averaged over last 500 steps = 3.1947e-01, PNorm = 117.5377, GNorm = 0.3713
Meta loss on this task batch = 2.9471e-01, Meta loss averaged over last 500 steps = 3.1946e-01, PNorm = 117.5428, GNorm = 0.3716
Meta loss on this task batch = 3.3216e-01, Meta loss averaged over last 500 steps = 3.1946e-01, PNorm = 117.5477, GNorm = 0.3002
Meta loss on this task batch = 3.2836e-01, Meta loss averaged over last 500 steps = 3.1956e-01, PNorm = 117.5520, GNorm = 0.3241
Meta loss on this task batch = 3.6959e-01, Meta loss averaged over last 500 steps = 3.1988e-01, PNorm = 117.5559, GNorm = 0.5418
Meta loss on this task batch = 3.4124e-01, Meta loss averaged over last 500 steps = 3.1992e-01, PNorm = 117.5605, GNorm = 0.3902
Meta loss on this task batch = 2.4374e-01, Meta loss averaged over last 500 steps = 3.1985e-01, PNorm = 117.5658, GNorm = 0.2627
Took 115.74534606933594 seconds to complete one epoch of meta training
Took 123.63094902038574 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482902
Epoch 350
Meta loss on this task batch = 3.4741e-01, Meta loss averaged over last 500 steps = 3.2000e-01, PNorm = 117.5714, GNorm = 0.3226
Meta loss on this task batch = 3.4771e-01, Meta loss averaged over last 500 steps = 3.2008e-01, PNorm = 117.5768, GNorm = 0.3772
Meta loss on this task batch = 2.9134e-01, Meta loss averaged over last 500 steps = 3.1999e-01, PNorm = 117.5830, GNorm = 0.2797
Meta loss on this task batch = 3.2494e-01, Meta loss averaged over last 500 steps = 3.1996e-01, PNorm = 117.5897, GNorm = 0.2796
Meta loss on this task batch = 3.0767e-01, Meta loss averaged over last 500 steps = 3.1973e-01, PNorm = 117.5968, GNorm = 0.3230
Meta loss on this task batch = 2.9068e-01, Meta loss averaged over last 500 steps = 3.1962e-01, PNorm = 117.6047, GNorm = 0.2666
Meta loss on this task batch = 2.7473e-01, Meta loss averaged over last 500 steps = 3.1958e-01, PNorm = 117.6134, GNorm = 0.2882
Meta loss on this task batch = 3.0843e-01, Meta loss averaged over last 500 steps = 3.1969e-01, PNorm = 117.6228, GNorm = 0.3914
Meta loss on this task batch = 2.9511e-01, Meta loss averaged over last 500 steps = 3.1967e-01, PNorm = 117.6319, GNorm = 0.2328
Meta loss on this task batch = 3.0039e-01, Meta loss averaged over last 500 steps = 3.1965e-01, PNorm = 117.6408, GNorm = 0.3746
Meta loss on this task batch = 3.1562e-01, Meta loss averaged over last 500 steps = 3.1958e-01, PNorm = 117.6494, GNorm = 0.3451
Meta loss on this task batch = 2.9841e-01, Meta loss averaged over last 500 steps = 3.1956e-01, PNorm = 117.6579, GNorm = 0.2344
Meta loss on this task batch = 3.4315e-01, Meta loss averaged over last 500 steps = 3.1951e-01, PNorm = 117.6654, GNorm = 0.3016
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 3.1941e-01, PNorm = 117.6730, GNorm = 0.2608
Meta loss on this task batch = 2.7987e-01, Meta loss averaged over last 500 steps = 3.1941e-01, PNorm = 117.6811, GNorm = 0.2862
Meta loss on this task batch = 3.5492e-01, Meta loss averaged over last 500 steps = 3.1943e-01, PNorm = 117.6884, GNorm = 0.3109
Meta loss on this task batch = 3.1543e-01, Meta loss averaged over last 500 steps = 3.1934e-01, PNorm = 117.6945, GNorm = 0.2851
Meta loss on this task batch = 3.2124e-01, Meta loss averaged over last 500 steps = 3.1934e-01, PNorm = 117.7004, GNorm = 0.2461
Meta loss on this task batch = 3.3346e-01, Meta loss averaged over last 500 steps = 3.1931e-01, PNorm = 117.7059, GNorm = 0.3485
Took 147.5351860523224 seconds to complete one epoch of meta training
Took 155.03356671333313 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492818
Epoch 351
Meta loss on this task batch = 3.3555e-01, Meta loss averaged over last 500 steps = 3.1927e-01, PNorm = 117.7117, GNorm = 0.3118
Meta loss on this task batch = 3.4616e-01, Meta loss averaged over last 500 steps = 3.1940e-01, PNorm = 117.7172, GNorm = 0.2770
Meta loss on this task batch = 2.8599e-01, Meta loss averaged over last 500 steps = 3.1936e-01, PNorm = 117.7227, GNorm = 0.2794
Meta loss on this task batch = 2.5183e-01, Meta loss averaged over last 500 steps = 3.1918e-01, PNorm = 117.7287, GNorm = 0.3325
Meta loss on this task batch = 2.9108e-01, Meta loss averaged over last 500 steps = 3.1915e-01, PNorm = 117.7346, GNorm = 0.2581
Meta loss on this task batch = 2.5801e-01, Meta loss averaged over last 500 steps = 3.1903e-01, PNorm = 117.7407, GNorm = 0.2867
Meta loss on this task batch = 2.4592e-01, Meta loss averaged over last 500 steps = 3.1883e-01, PNorm = 117.7462, GNorm = 0.3781
Meta loss on this task batch = 3.3906e-01, Meta loss averaged over last 500 steps = 3.1888e-01, PNorm = 117.7513, GNorm = 0.4575
Meta loss on this task batch = 3.7429e-01, Meta loss averaged over last 500 steps = 3.1895e-01, PNorm = 117.7560, GNorm = 0.3042
Meta loss on this task batch = 3.5772e-01, Meta loss averaged over last 500 steps = 3.1902e-01, PNorm = 117.7609, GNorm = 0.2818
Meta loss on this task batch = 3.1544e-01, Meta loss averaged over last 500 steps = 3.1895e-01, PNorm = 117.7663, GNorm = 0.3763
Meta loss on this task batch = 3.6372e-01, Meta loss averaged over last 500 steps = 3.1903e-01, PNorm = 117.7715, GNorm = 0.3096
Meta loss on this task batch = 2.8391e-01, Meta loss averaged over last 500 steps = 3.1904e-01, PNorm = 117.7767, GNorm = 0.3252
Meta loss on this task batch = 3.4117e-01, Meta loss averaged over last 500 steps = 3.1901e-01, PNorm = 117.7823, GNorm = 0.2986
Meta loss on this task batch = 3.2764e-01, Meta loss averaged over last 500 steps = 3.1901e-01, PNorm = 117.7883, GNorm = 0.3839
Meta loss on this task batch = 3.0509e-01, Meta loss averaged over last 500 steps = 3.1896e-01, PNorm = 117.7940, GNorm = 0.4731
Meta loss on this task batch = 3.1451e-01, Meta loss averaged over last 500 steps = 3.1899e-01, PNorm = 117.7999, GNorm = 0.2331
Meta loss on this task batch = 3.6703e-01, Meta loss averaged over last 500 steps = 3.1903e-01, PNorm = 117.8053, GNorm = 0.3544
Meta loss on this task batch = 3.1889e-01, Meta loss averaged over last 500 steps = 3.1903e-01, PNorm = 117.8117, GNorm = 0.3814
Took 208.40822219848633 seconds to complete one epoch of meta training
Took 216.45656085014343 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457792
Epoch 352
Meta loss on this task batch = 3.0622e-01, Meta loss averaged over last 500 steps = 3.1906e-01, PNorm = 117.8179, GNorm = 0.3068
Meta loss on this task batch = 3.1516e-01, Meta loss averaged over last 500 steps = 3.1913e-01, PNorm = 117.8245, GNorm = 0.2916
Meta loss on this task batch = 3.0651e-01, Meta loss averaged over last 500 steps = 3.1910e-01, PNorm = 117.8312, GNorm = 0.2485
Meta loss on this task batch = 2.7279e-01, Meta loss averaged over last 500 steps = 3.1903e-01, PNorm = 117.8381, GNorm = 0.2609
Meta loss on this task batch = 3.1810e-01, Meta loss averaged over last 500 steps = 3.1900e-01, PNorm = 117.8445, GNorm = 0.3856
Meta loss on this task batch = 3.5515e-01, Meta loss averaged over last 500 steps = 3.1912e-01, PNorm = 117.8508, GNorm = 0.2566
Meta loss on this task batch = 2.9485e-01, Meta loss averaged over last 500 steps = 3.1921e-01, PNorm = 117.8581, GNorm = 0.3370
Meta loss on this task batch = 2.4419e-01, Meta loss averaged over last 500 steps = 3.1910e-01, PNorm = 117.8666, GNorm = 0.3455
Meta loss on this task batch = 3.2897e-01, Meta loss averaged over last 500 steps = 3.1912e-01, PNorm = 117.8747, GNorm = 0.3222
Meta loss on this task batch = 3.1081e-01, Meta loss averaged over last 500 steps = 3.1896e-01, PNorm = 117.8811, GNorm = 0.4859
Meta loss on this task batch = 2.9457e-01, Meta loss averaged over last 500 steps = 3.1883e-01, PNorm = 117.8875, GNorm = 0.2973
Meta loss on this task batch = 3.6821e-01, Meta loss averaged over last 500 steps = 3.1901e-01, PNorm = 117.8920, GNorm = 0.3304
Meta loss on this task batch = 3.0184e-01, Meta loss averaged over last 500 steps = 3.1887e-01, PNorm = 117.8960, GNorm = 0.2646
Meta loss on this task batch = 2.8321e-01, Meta loss averaged over last 500 steps = 3.1873e-01, PNorm = 117.9002, GNorm = 0.3154
Meta loss on this task batch = 3.0728e-01, Meta loss averaged over last 500 steps = 3.1861e-01, PNorm = 117.9047, GNorm = 0.3888
Meta loss on this task batch = 3.6883e-01, Meta loss averaged over last 500 steps = 3.1876e-01, PNorm = 117.9083, GNorm = 0.3530
Meta loss on this task batch = 3.2060e-01, Meta loss averaged over last 500 steps = 3.1876e-01, PNorm = 117.9126, GNorm = 0.2884
Meta loss on this task batch = 3.1698e-01, Meta loss averaged over last 500 steps = 3.1870e-01, PNorm = 117.9165, GNorm = 0.3328
Meta loss on this task batch = 3.1669e-01, Meta loss averaged over last 500 steps = 3.1872e-01, PNorm = 117.9206, GNorm = 0.3226
Took 175.82365489006042 seconds to complete one epoch of meta training
Took 183.9124801158905 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486941
Epoch 353
Meta loss on this task batch = 2.8787e-01, Meta loss averaged over last 500 steps = 3.1871e-01, PNorm = 117.9255, GNorm = 0.2939
Meta loss on this task batch = 3.0107e-01, Meta loss averaged over last 500 steps = 3.1873e-01, PNorm = 117.9320, GNorm = 0.2959
Meta loss on this task batch = 2.4959e-01, Meta loss averaged over last 500 steps = 3.1863e-01, PNorm = 117.9395, GNorm = 0.3815
Meta loss on this task batch = 3.6219e-01, Meta loss averaged over last 500 steps = 3.1873e-01, PNorm = 117.9470, GNorm = 0.4728
Meta loss on this task batch = 3.3834e-01, Meta loss averaged over last 500 steps = 3.1867e-01, PNorm = 117.9536, GNorm = 0.3102
Meta loss on this task batch = 2.9658e-01, Meta loss averaged over last 500 steps = 3.1867e-01, PNorm = 117.9606, GNorm = 0.2847
Meta loss on this task batch = 3.0213e-01, Meta loss averaged over last 500 steps = 3.1862e-01, PNorm = 117.9674, GNorm = 0.2893
Meta loss on this task batch = 3.2635e-01, Meta loss averaged over last 500 steps = 3.1865e-01, PNorm = 117.9741, GNorm = 0.2717
Meta loss on this task batch = 3.1843e-01, Meta loss averaged over last 500 steps = 3.1859e-01, PNorm = 117.9810, GNorm = 0.3438
Meta loss on this task batch = 3.6441e-01, Meta loss averaged over last 500 steps = 3.1866e-01, PNorm = 117.9868, GNorm = 0.3300
Meta loss on this task batch = 2.4518e-01, Meta loss averaged over last 500 steps = 3.1850e-01, PNorm = 117.9933, GNorm = 0.2290
Meta loss on this task batch = 3.0303e-01, Meta loss averaged over last 500 steps = 3.1845e-01, PNorm = 118.0008, GNorm = 0.2593
Meta loss on this task batch = 3.3396e-01, Meta loss averaged over last 500 steps = 3.1857e-01, PNorm = 118.0084, GNorm = 0.4298
Meta loss on this task batch = 2.9998e-01, Meta loss averaged over last 500 steps = 3.1847e-01, PNorm = 118.0160, GNorm = 0.2895
Meta loss on this task batch = 3.0122e-01, Meta loss averaged over last 500 steps = 3.1855e-01, PNorm = 118.0229, GNorm = 0.2932
Meta loss on this task batch = 3.4779e-01, Meta loss averaged over last 500 steps = 3.1867e-01, PNorm = 118.0295, GNorm = 0.2732
Meta loss on this task batch = 2.6160e-01, Meta loss averaged over last 500 steps = 3.1859e-01, PNorm = 118.0366, GNorm = 0.3479
Meta loss on this task batch = 2.7111e-01, Meta loss averaged over last 500 steps = 3.1842e-01, PNorm = 118.0442, GNorm = 0.2488
Meta loss on this task batch = 3.6593e-01, Meta loss averaged over last 500 steps = 3.1854e-01, PNorm = 118.0510, GNorm = 0.3475
Took 114.40561151504517 seconds to complete one epoch of meta training
Took 122.7483880519867 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473984
Epoch 354
Meta loss on this task batch = 3.3247e-01, Meta loss averaged over last 500 steps = 3.1858e-01, PNorm = 118.0570, GNorm = 0.3031
Meta loss on this task batch = 3.3816e-01, Meta loss averaged over last 500 steps = 3.1863e-01, PNorm = 118.0630, GNorm = 0.3156
Meta loss on this task batch = 2.3937e-01, Meta loss averaged over last 500 steps = 3.1841e-01, PNorm = 118.0704, GNorm = 0.2315
Meta loss on this task batch = 3.3777e-01, Meta loss averaged over last 500 steps = 3.1845e-01, PNorm = 118.0781, GNorm = 0.2493
Meta loss on this task batch = 3.4753e-01, Meta loss averaged over last 500 steps = 3.1855e-01, PNorm = 118.0851, GNorm = 0.2659
Meta loss on this task batch = 3.2791e-01, Meta loss averaged over last 500 steps = 3.1863e-01, PNorm = 118.0911, GNorm = 0.2448
Meta loss on this task batch = 3.2813e-01, Meta loss averaged over last 500 steps = 3.1866e-01, PNorm = 118.0982, GNorm = 0.2740
Meta loss on this task batch = 2.8808e-01, Meta loss averaged over last 500 steps = 3.1856e-01, PNorm = 118.1068, GNorm = 0.3724
Meta loss on this task batch = 3.0121e-01, Meta loss averaged over last 500 steps = 3.1845e-01, PNorm = 118.1159, GNorm = 0.2324
Meta loss on this task batch = 2.9994e-01, Meta loss averaged over last 500 steps = 3.1842e-01, PNorm = 118.1252, GNorm = 0.2392
Meta loss on this task batch = 3.1232e-01, Meta loss averaged over last 500 steps = 3.1831e-01, PNorm = 118.1354, GNorm = 0.2874
Meta loss on this task batch = 3.9981e-01, Meta loss averaged over last 500 steps = 3.1848e-01, PNorm = 118.1440, GNorm = 0.4901
Meta loss on this task batch = 2.8942e-01, Meta loss averaged over last 500 steps = 3.1843e-01, PNorm = 118.1518, GNorm = 0.3109
Meta loss on this task batch = 3.6376e-01, Meta loss averaged over last 500 steps = 3.1858e-01, PNorm = 118.1586, GNorm = 0.3733
Meta loss on this task batch = 2.9928e-01, Meta loss averaged over last 500 steps = 3.1854e-01, PNorm = 118.1650, GNorm = 0.4218
Meta loss on this task batch = 2.8975e-01, Meta loss averaged over last 500 steps = 3.1849e-01, PNorm = 118.1710, GNorm = 0.2295
Meta loss on this task batch = 2.8133e-01, Meta loss averaged over last 500 steps = 3.1846e-01, PNorm = 118.1769, GNorm = 0.3676
Meta loss on this task batch = 3.6182e-01, Meta loss averaged over last 500 steps = 3.1851e-01, PNorm = 118.1827, GNorm = 0.3851
Meta loss on this task batch = 3.3258e-01, Meta loss averaged over last 500 steps = 3.1856e-01, PNorm = 118.1886, GNorm = 0.3955
Took 111.90800619125366 seconds to complete one epoch of meta training
Took 119.67419028282166 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463648
Epoch 355
Meta loss on this task batch = 2.8121e-01, Meta loss averaged over last 500 steps = 3.1854e-01, PNorm = 118.1955, GNorm = 0.2863
Meta loss on this task batch = 3.9391e-01, Meta loss averaged over last 500 steps = 3.1861e-01, PNorm = 118.2022, GNorm = 0.3952
Meta loss on this task batch = 2.6381e-01, Meta loss averaged over last 500 steps = 3.1846e-01, PNorm = 118.2089, GNorm = 0.3593
Meta loss on this task batch = 3.2989e-01, Meta loss averaged over last 500 steps = 3.1843e-01, PNorm = 118.2151, GNorm = 0.2917
Meta loss on this task batch = 3.5598e-01, Meta loss averaged over last 500 steps = 3.1853e-01, PNorm = 118.2206, GNorm = 0.3086
Meta loss on this task batch = 2.4844e-01, Meta loss averaged over last 500 steps = 3.1855e-01, PNorm = 118.2267, GNorm = 0.2837
Meta loss on this task batch = 2.5855e-01, Meta loss averaged over last 500 steps = 3.1835e-01, PNorm = 118.2343, GNorm = 0.3136
Meta loss on this task batch = 2.4903e-01, Meta loss averaged over last 500 steps = 3.1820e-01, PNorm = 118.2422, GNorm = 0.2527
Meta loss on this task batch = 2.6200e-01, Meta loss averaged over last 500 steps = 3.1813e-01, PNorm = 118.2492, GNorm = 0.2991
Meta loss on this task batch = 3.1370e-01, Meta loss averaged over last 500 steps = 3.1806e-01, PNorm = 118.2562, GNorm = 0.2572
Meta loss on this task batch = 3.2661e-01, Meta loss averaged over last 500 steps = 3.1810e-01, PNorm = 118.2617, GNorm = 0.3643
Meta loss on this task batch = 3.1531e-01, Meta loss averaged over last 500 steps = 3.1811e-01, PNorm = 118.2661, GNorm = 0.3255
Meta loss on this task batch = 3.5591e-01, Meta loss averaged over last 500 steps = 3.1828e-01, PNorm = 118.2703, GNorm = 0.2899
Meta loss on this task batch = 3.2609e-01, Meta loss averaged over last 500 steps = 3.1822e-01, PNorm = 118.2749, GNorm = 0.2691
Meta loss on this task batch = 2.6956e-01, Meta loss averaged over last 500 steps = 3.1815e-01, PNorm = 118.2807, GNorm = 0.3022
Meta loss on this task batch = 3.2982e-01, Meta loss averaged over last 500 steps = 3.1826e-01, PNorm = 118.2871, GNorm = 0.3217
Meta loss on this task batch = 3.3911e-01, Meta loss averaged over last 500 steps = 3.1815e-01, PNorm = 118.2933, GNorm = 0.3557
Meta loss on this task batch = 3.5052e-01, Meta loss averaged over last 500 steps = 3.1820e-01, PNorm = 118.2998, GNorm = 0.3124
Meta loss on this task batch = 3.3297e-01, Meta loss averaged over last 500 steps = 3.1822e-01, PNorm = 118.3064, GNorm = 0.3730
Took 112.47788047790527 seconds to complete one epoch of meta training
Took 119.59545040130615 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483627
Epoch 356
Meta loss on this task batch = 2.9051e-01, Meta loss averaged over last 500 steps = 3.1818e-01, PNorm = 118.3129, GNorm = 0.3012
Meta loss on this task batch = 3.0081e-01, Meta loss averaged over last 500 steps = 3.1815e-01, PNorm = 118.3189, GNorm = 0.2661
Meta loss on this task batch = 3.0575e-01, Meta loss averaged over last 500 steps = 3.1809e-01, PNorm = 118.3243, GNorm = 0.2923
Meta loss on this task batch = 3.0333e-01, Meta loss averaged over last 500 steps = 3.1813e-01, PNorm = 118.3294, GNorm = 0.2571
Meta loss on this task batch = 3.2463e-01, Meta loss averaged over last 500 steps = 3.1811e-01, PNorm = 118.3342, GNorm = 0.3503
Meta loss on this task batch = 2.9363e-01, Meta loss averaged over last 500 steps = 3.1797e-01, PNorm = 118.3396, GNorm = 0.2716
Meta loss on this task batch = 3.5314e-01, Meta loss averaged over last 500 steps = 3.1804e-01, PNorm = 118.3453, GNorm = 0.3032
Meta loss on this task batch = 2.6693e-01, Meta loss averaged over last 500 steps = 3.1789e-01, PNorm = 118.3513, GNorm = 0.2265
Meta loss on this task batch = 3.3275e-01, Meta loss averaged over last 500 steps = 3.1804e-01, PNorm = 118.3575, GNorm = 0.3014
Meta loss on this task batch = 3.4796e-01, Meta loss averaged over last 500 steps = 3.1815e-01, PNorm = 118.3632, GNorm = 0.3225
Meta loss on this task batch = 2.6983e-01, Meta loss averaged over last 500 steps = 3.1806e-01, PNorm = 118.3695, GNorm = 0.2522
Meta loss on this task batch = 3.3063e-01, Meta loss averaged over last 500 steps = 3.1813e-01, PNorm = 118.3761, GNorm = 0.3328
Meta loss on this task batch = 3.6941e-01, Meta loss averaged over last 500 steps = 3.1833e-01, PNorm = 118.3817, GNorm = 0.3268
Meta loss on this task batch = 2.5343e-01, Meta loss averaged over last 500 steps = 3.1831e-01, PNorm = 118.3870, GNorm = 0.2742
Meta loss on this task batch = 3.1070e-01, Meta loss averaged over last 500 steps = 3.1820e-01, PNorm = 118.3923, GNorm = 0.2861
Meta loss on this task batch = 3.1827e-01, Meta loss averaged over last 500 steps = 3.1811e-01, PNorm = 118.3977, GNorm = 0.4267
Meta loss on this task batch = 2.8514e-01, Meta loss averaged over last 500 steps = 3.1810e-01, PNorm = 118.4032, GNorm = 0.2294
Meta loss on this task batch = 3.0842e-01, Meta loss averaged over last 500 steps = 3.1809e-01, PNorm = 118.4086, GNorm = 0.3579
Meta loss on this task batch = 3.3041e-01, Meta loss averaged over last 500 steps = 3.1817e-01, PNorm = 118.4134, GNorm = 0.3637
Took 112.30942749977112 seconds to complete one epoch of meta training
Took 118.99714946746826 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461449
Epoch 357
Meta loss on this task batch = 2.9513e-01, Meta loss averaged over last 500 steps = 3.1805e-01, PNorm = 118.4186, GNorm = 0.4033
Meta loss on this task batch = 2.9398e-01, Meta loss averaged over last 500 steps = 3.1806e-01, PNorm = 118.4237, GNorm = 0.2823
Meta loss on this task batch = 3.6403e-01, Meta loss averaged over last 500 steps = 3.1803e-01, PNorm = 118.4290, GNorm = 0.3343
Meta loss on this task batch = 2.9072e-01, Meta loss averaged over last 500 steps = 3.1798e-01, PNorm = 118.4340, GNorm = 0.2957
Meta loss on this task batch = 3.3244e-01, Meta loss averaged over last 500 steps = 3.1797e-01, PNorm = 118.4398, GNorm = 0.3462
Meta loss on this task batch = 3.7053e-01, Meta loss averaged over last 500 steps = 3.1811e-01, PNorm = 118.4439, GNorm = 0.3941
Meta loss on this task batch = 2.7220e-01, Meta loss averaged over last 500 steps = 3.1799e-01, PNorm = 118.4486, GNorm = 0.2863
Meta loss on this task batch = 2.9865e-01, Meta loss averaged over last 500 steps = 3.1785e-01, PNorm = 118.4546, GNorm = 0.3059
Meta loss on this task batch = 2.9391e-01, Meta loss averaged over last 500 steps = 3.1781e-01, PNorm = 118.4613, GNorm = 0.2609
Meta loss on this task batch = 3.6842e-01, Meta loss averaged over last 500 steps = 3.1794e-01, PNorm = 118.4690, GNorm = 0.3969
Meta loss on this task batch = 3.1212e-01, Meta loss averaged over last 500 steps = 3.1793e-01, PNorm = 118.4769, GNorm = 0.2847
Meta loss on this task batch = 2.9830e-01, Meta loss averaged over last 500 steps = 3.1798e-01, PNorm = 118.4856, GNorm = 0.2708
Meta loss on this task batch = 3.3430e-01, Meta loss averaged over last 500 steps = 3.1806e-01, PNorm = 118.4943, GNorm = 0.2782
Meta loss on this task batch = 2.8011e-01, Meta loss averaged over last 500 steps = 3.1796e-01, PNorm = 118.5035, GNorm = 0.2708
Meta loss on this task batch = 3.3677e-01, Meta loss averaged over last 500 steps = 3.1808e-01, PNorm = 118.5129, GNorm = 0.3467
Meta loss on this task batch = 3.3853e-01, Meta loss averaged over last 500 steps = 3.1809e-01, PNorm = 118.5221, GNorm = 0.3344
Meta loss on this task batch = 3.2256e-01, Meta loss averaged over last 500 steps = 3.1805e-01, PNorm = 118.5312, GNorm = 0.3408
Meta loss on this task batch = 3.4314e-01, Meta loss averaged over last 500 steps = 3.1803e-01, PNorm = 118.5392, GNorm = 0.4134
Meta loss on this task batch = 2.8178e-01, Meta loss averaged over last 500 steps = 3.1799e-01, PNorm = 118.5468, GNorm = 0.4390
Took 113.21998763084412 seconds to complete one epoch of meta training
Took 120.64786767959595 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466214
Epoch 358
Meta loss on this task batch = 3.6104e-01, Meta loss averaged over last 500 steps = 3.1807e-01, PNorm = 118.5532, GNorm = 0.5059
Meta loss on this task batch = 3.3681e-01, Meta loss averaged over last 500 steps = 3.1814e-01, PNorm = 118.5583, GNorm = 0.2977
Meta loss on this task batch = 3.8526e-01, Meta loss averaged over last 500 steps = 3.1831e-01, PNorm = 118.5629, GNorm = 0.7603
Meta loss on this task batch = 2.7315e-01, Meta loss averaged over last 500 steps = 3.1820e-01, PNorm = 118.5684, GNorm = 0.4092
Meta loss on this task batch = 3.0870e-01, Meta loss averaged over last 500 steps = 3.1819e-01, PNorm = 118.5737, GNorm = 0.3594
Meta loss on this task batch = 3.0201e-01, Meta loss averaged over last 500 steps = 3.1800e-01, PNorm = 118.5801, GNorm = 0.2834
Meta loss on this task batch = 2.3974e-01, Meta loss averaged over last 500 steps = 3.1775e-01, PNorm = 118.5875, GNorm = 0.2371
Meta loss on this task batch = 3.1064e-01, Meta loss averaged over last 500 steps = 3.1774e-01, PNorm = 118.5956, GNorm = 0.5032
Meta loss on this task batch = 2.9321e-01, Meta loss averaged over last 500 steps = 3.1767e-01, PNorm = 118.6042, GNorm = 0.2942
Meta loss on this task batch = 3.1970e-01, Meta loss averaged over last 500 steps = 3.1774e-01, PNorm = 118.6127, GNorm = 0.2879
Meta loss on this task batch = 3.4264e-01, Meta loss averaged over last 500 steps = 3.1775e-01, PNorm = 118.6203, GNorm = 0.3312
Meta loss on this task batch = 2.8653e-01, Meta loss averaged over last 500 steps = 3.1770e-01, PNorm = 118.6277, GNorm = 0.2785
Meta loss on this task batch = 2.8843e-01, Meta loss averaged over last 500 steps = 3.1744e-01, PNorm = 118.6354, GNorm = 0.2953
Meta loss on this task batch = 3.2960e-01, Meta loss averaged over last 500 steps = 3.1723e-01, PNorm = 118.6429, GNorm = 0.2938
Meta loss on this task batch = 2.7498e-01, Meta loss averaged over last 500 steps = 3.1717e-01, PNorm = 118.6511, GNorm = 0.2907
Meta loss on this task batch = 3.6785e-01, Meta loss averaged over last 500 steps = 3.1729e-01, PNorm = 118.6583, GNorm = 0.3626
Meta loss on this task batch = 3.2312e-01, Meta loss averaged over last 500 steps = 3.1726e-01, PNorm = 118.6653, GNorm = 0.2373
Meta loss on this task batch = 3.4419e-01, Meta loss averaged over last 500 steps = 3.1731e-01, PNorm = 118.6717, GNorm = 0.2963
Meta loss on this task batch = 2.2960e-01, Meta loss averaged over last 500 steps = 3.1720e-01, PNorm = 118.6787, GNorm = 0.3198
Took 110.16549372673035 seconds to complete one epoch of meta training
Took 117.96238827705383 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483329
Epoch 359
Meta loss on this task batch = 3.1429e-01, Meta loss averaged over last 500 steps = 3.1726e-01, PNorm = 118.6857, GNorm = 0.3038
Meta loss on this task batch = 3.3330e-01, Meta loss averaged over last 500 steps = 3.1723e-01, PNorm = 118.6921, GNorm = 0.2903
Meta loss on this task batch = 3.2212e-01, Meta loss averaged over last 500 steps = 3.1721e-01, PNorm = 118.6981, GNorm = 0.4146
Meta loss on this task batch = 2.9681e-01, Meta loss averaged over last 500 steps = 3.1719e-01, PNorm = 118.7046, GNorm = 0.2236
Meta loss on this task batch = 3.5432e-01, Meta loss averaged over last 500 steps = 3.1726e-01, PNorm = 118.7104, GNorm = 0.4476
Meta loss on this task batch = 2.9692e-01, Meta loss averaged over last 500 steps = 3.1725e-01, PNorm = 118.7152, GNorm = 0.3905
Meta loss on this task batch = 3.3072e-01, Meta loss averaged over last 500 steps = 3.1734e-01, PNorm = 118.7207, GNorm = 0.4010
Meta loss on this task batch = 3.0025e-01, Meta loss averaged over last 500 steps = 3.1725e-01, PNorm = 118.7272, GNorm = 0.2458
Meta loss on this task batch = 2.8360e-01, Meta loss averaged over last 500 steps = 3.1716e-01, PNorm = 118.7335, GNorm = 0.2728
Meta loss on this task batch = 3.3918e-01, Meta loss averaged over last 500 steps = 3.1720e-01, PNorm = 118.7396, GNorm = 0.2706
Meta loss on this task batch = 3.1896e-01, Meta loss averaged over last 500 steps = 3.1710e-01, PNorm = 118.7463, GNorm = 0.2871
Meta loss on this task batch = 2.8799e-01, Meta loss averaged over last 500 steps = 3.1706e-01, PNorm = 118.7538, GNorm = 0.2686
Meta loss on this task batch = 3.2345e-01, Meta loss averaged over last 500 steps = 3.1716e-01, PNorm = 118.7610, GNorm = 0.4355
Meta loss on this task batch = 2.9729e-01, Meta loss averaged over last 500 steps = 3.1706e-01, PNorm = 118.7679, GNorm = 0.2931
Meta loss on this task batch = 3.2145e-01, Meta loss averaged over last 500 steps = 3.1709e-01, PNorm = 118.7736, GNorm = 0.3393
Meta loss on this task batch = 2.9306e-01, Meta loss averaged over last 500 steps = 3.1702e-01, PNorm = 118.7801, GNorm = 0.3096
Meta loss on this task batch = 3.0798e-01, Meta loss averaged over last 500 steps = 3.1697e-01, PNorm = 118.7867, GNorm = 0.2643
Meta loss on this task batch = 3.3761e-01, Meta loss averaged over last 500 steps = 3.1696e-01, PNorm = 118.7922, GNorm = 0.2990
Meta loss on this task batch = 2.6526e-01, Meta loss averaged over last 500 steps = 3.1687e-01, PNorm = 118.7974, GNorm = 0.2835
Took 112.31050229072571 seconds to complete one epoch of meta training
Took 120.00858902931213 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487242
Epoch 360
Meta loss on this task batch = 2.9196e-01, Meta loss averaged over last 500 steps = 3.1680e-01, PNorm = 118.8022, GNorm = 0.3487
Meta loss on this task batch = 2.7652e-01, Meta loss averaged over last 500 steps = 3.1670e-01, PNorm = 118.8075, GNorm = 0.2402
Meta loss on this task batch = 2.9781e-01, Meta loss averaged over last 500 steps = 3.1667e-01, PNorm = 118.8128, GNorm = 0.2542
Meta loss on this task batch = 3.2264e-01, Meta loss averaged over last 500 steps = 3.1678e-01, PNorm = 118.8188, GNorm = 0.3608
Meta loss on this task batch = 2.9031e-01, Meta loss averaged over last 500 steps = 3.1673e-01, PNorm = 118.8255, GNorm = 0.3610
Meta loss on this task batch = 3.7628e-01, Meta loss averaged over last 500 steps = 3.1685e-01, PNorm = 118.8305, GNorm = 0.4024
Meta loss on this task batch = 2.4835e-01, Meta loss averaged over last 500 steps = 3.1675e-01, PNorm = 118.8365, GNorm = 0.2435
Meta loss on this task batch = 3.5774e-01, Meta loss averaged over last 500 steps = 3.1676e-01, PNorm = 118.8419, GNorm = 0.2969
Meta loss on this task batch = 3.4010e-01, Meta loss averaged over last 500 steps = 3.1681e-01, PNorm = 118.8476, GNorm = 0.3301
Meta loss on this task batch = 2.7307e-01, Meta loss averaged over last 500 steps = 3.1676e-01, PNorm = 118.8540, GNorm = 0.3392
Meta loss on this task batch = 2.9836e-01, Meta loss averaged over last 500 steps = 3.1683e-01, PNorm = 118.8608, GNorm = 0.2495
Meta loss on this task batch = 3.1898e-01, Meta loss averaged over last 500 steps = 3.1694e-01, PNorm = 118.8677, GNorm = 0.4733
Meta loss on this task batch = 2.9773e-01, Meta loss averaged over last 500 steps = 3.1675e-01, PNorm = 118.8745, GNorm = 0.3673
Meta loss on this task batch = 2.8968e-01, Meta loss averaged over last 500 steps = 3.1669e-01, PNorm = 118.8815, GNorm = 0.2743
Meta loss on this task batch = 3.5658e-01, Meta loss averaged over last 500 steps = 3.1666e-01, PNorm = 118.8882, GNorm = 0.4008
Meta loss on this task batch = 2.9208e-01, Meta loss averaged over last 500 steps = 3.1662e-01, PNorm = 118.8953, GNorm = 0.2710
Meta loss on this task batch = 3.4039e-01, Meta loss averaged over last 500 steps = 3.1665e-01, PNorm = 118.9023, GNorm = 0.2913
Meta loss on this task batch = 2.9617e-01, Meta loss averaged over last 500 steps = 3.1672e-01, PNorm = 118.9093, GNorm = 0.3190
Meta loss on this task batch = 3.8027e-01, Meta loss averaged over last 500 steps = 3.1675e-01, PNorm = 118.9158, GNorm = 0.3716
Took 113.59852266311646 seconds to complete one epoch of meta training
Took 121.31245946884155 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461605
Epoch 361
Meta loss on this task batch = 3.0539e-01, Meta loss averaged over last 500 steps = 3.1671e-01, PNorm = 118.9220, GNorm = 0.2838
Meta loss on this task batch = 3.0326e-01, Meta loss averaged over last 500 steps = 3.1664e-01, PNorm = 118.9284, GNorm = 0.2436
Meta loss on this task batch = 2.9020e-01, Meta loss averaged over last 500 steps = 3.1654e-01, PNorm = 118.9356, GNorm = 0.2561
Meta loss on this task batch = 3.4687e-01, Meta loss averaged over last 500 steps = 3.1655e-01, PNorm = 118.9429, GNorm = 0.2941
Meta loss on this task batch = 4.1612e-01, Meta loss averaged over last 500 steps = 3.1663e-01, PNorm = 118.9490, GNorm = 0.3052
Meta loss on this task batch = 3.2288e-01, Meta loss averaged over last 500 steps = 3.1676e-01, PNorm = 118.9546, GNorm = 0.3976
Meta loss on this task batch = 2.7709e-01, Meta loss averaged over last 500 steps = 3.1665e-01, PNorm = 118.9615, GNorm = 0.2514
Meta loss on this task batch = 2.1177e-01, Meta loss averaged over last 500 steps = 3.1631e-01, PNorm = 118.9695, GNorm = 0.4317
Meta loss on this task batch = 2.7154e-01, Meta loss averaged over last 500 steps = 3.1614e-01, PNorm = 118.9766, GNorm = 0.2528
Meta loss on this task batch = 3.1792e-01, Meta loss averaged over last 500 steps = 3.1607e-01, PNorm = 118.9840, GNorm = 0.2612
Meta loss on this task batch = 2.9826e-01, Meta loss averaged over last 500 steps = 3.1610e-01, PNorm = 118.9922, GNorm = 0.2439
Meta loss on this task batch = 3.0695e-01, Meta loss averaged over last 500 steps = 3.1609e-01, PNorm = 118.9991, GNorm = 0.3245
Meta loss on this task batch = 3.0416e-01, Meta loss averaged over last 500 steps = 3.1607e-01, PNorm = 119.0065, GNorm = 0.3202
Meta loss on this task batch = 3.1098e-01, Meta loss averaged over last 500 steps = 3.1610e-01, PNorm = 119.0136, GNorm = 0.4291
Meta loss on this task batch = 3.2889e-01, Meta loss averaged over last 500 steps = 3.1611e-01, PNorm = 119.0206, GNorm = 0.3251
Meta loss on this task batch = 3.0908e-01, Meta loss averaged over last 500 steps = 3.1604e-01, PNorm = 119.0270, GNorm = 0.2817
Meta loss on this task batch = 3.6029e-01, Meta loss averaged over last 500 steps = 3.1611e-01, PNorm = 119.0320, GNorm = 0.4274
Meta loss on this task batch = 3.1691e-01, Meta loss averaged over last 500 steps = 3.1611e-01, PNorm = 119.0373, GNorm = 0.3170
Meta loss on this task batch = 3.5272e-01, Meta loss averaged over last 500 steps = 3.1614e-01, PNorm = 119.0415, GNorm = 0.3271
Took 112.13761377334595 seconds to complete one epoch of meta training
Took 119.59650707244873 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466274
Epoch 362
Meta loss on this task batch = 3.2550e-01, Meta loss averaged over last 500 steps = 3.1625e-01, PNorm = 119.0470, GNorm = 0.2807
Meta loss on this task batch = 3.1639e-01, Meta loss averaged over last 500 steps = 3.1627e-01, PNorm = 119.0527, GNorm = 0.3959
Meta loss on this task batch = 3.3733e-01, Meta loss averaged over last 500 steps = 3.1637e-01, PNorm = 119.0588, GNorm = 0.2579
Meta loss on this task batch = 3.1426e-01, Meta loss averaged over last 500 steps = 3.1643e-01, PNorm = 119.0657, GNorm = 0.2787
Meta loss on this task batch = 2.9943e-01, Meta loss averaged over last 500 steps = 3.1642e-01, PNorm = 119.0728, GNorm = 0.2474
Meta loss on this task batch = 2.7733e-01, Meta loss averaged over last 500 steps = 3.1637e-01, PNorm = 119.0808, GNorm = 0.3017
Meta loss on this task batch = 3.2979e-01, Meta loss averaged over last 500 steps = 3.1645e-01, PNorm = 119.0887, GNorm = 0.4244
Meta loss on this task batch = 2.7039e-01, Meta loss averaged over last 500 steps = 3.1637e-01, PNorm = 119.0966, GNorm = 0.2250
Meta loss on this task batch = 3.3631e-01, Meta loss averaged over last 500 steps = 3.1640e-01, PNorm = 119.1039, GNorm = 0.5022
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 3.1635e-01, PNorm = 119.1112, GNorm = 0.2860
Meta loss on this task batch = 3.4870e-01, Meta loss averaged over last 500 steps = 3.1636e-01, PNorm = 119.1184, GNorm = 0.2930
Meta loss on this task batch = 3.6477e-01, Meta loss averaged over last 500 steps = 3.1644e-01, PNorm = 119.1250, GNorm = 0.4569
Meta loss on this task batch = 2.7723e-01, Meta loss averaged over last 500 steps = 3.1628e-01, PNorm = 119.1319, GNorm = 0.3191
Meta loss on this task batch = 3.8614e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 119.1380, GNorm = 0.4348
Meta loss on this task batch = 2.4016e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 119.1438, GNorm = 0.2385
Meta loss on this task batch = 3.0909e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 119.1492, GNorm = 0.3488
Meta loss on this task batch = 3.4401e-01, Meta loss averaged over last 500 steps = 3.1659e-01, PNorm = 119.1548, GNorm = 0.2694
Meta loss on this task batch = 3.1953e-01, Meta loss averaged over last 500 steps = 3.1654e-01, PNorm = 119.1603, GNorm = 0.3010
Meta loss on this task batch = 2.8273e-01, Meta loss averaged over last 500 steps = 3.1640e-01, PNorm = 119.1659, GNorm = 0.2994
Took 108.93714642524719 seconds to complete one epoch of meta training
Took 116.66002130508423 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461825
Epoch 363
Meta loss on this task batch = 3.1429e-01, Meta loss averaged over last 500 steps = 3.1644e-01, PNorm = 119.1723, GNorm = 0.4068
Meta loss on this task batch = 3.2057e-01, Meta loss averaged over last 500 steps = 3.1641e-01, PNorm = 119.1775, GNorm = 0.3208
Meta loss on this task batch = 2.8404e-01, Meta loss averaged over last 500 steps = 3.1638e-01, PNorm = 119.1835, GNorm = 0.2393
Meta loss on this task batch = 2.9891e-01, Meta loss averaged over last 500 steps = 3.1633e-01, PNorm = 119.1900, GNorm = 0.2571
Meta loss on this task batch = 3.6295e-01, Meta loss averaged over last 500 steps = 3.1637e-01, PNorm = 119.1971, GNorm = 0.4205
Meta loss on this task batch = 2.8981e-01, Meta loss averaged over last 500 steps = 3.1638e-01, PNorm = 119.2043, GNorm = 0.2367
Meta loss on this task batch = 3.0440e-01, Meta loss averaged over last 500 steps = 3.1637e-01, PNorm = 119.2114, GNorm = 0.2602
Meta loss on this task batch = 3.3107e-01, Meta loss averaged over last 500 steps = 3.1640e-01, PNorm = 119.2187, GNorm = 0.3421
Meta loss on this task batch = 2.7953e-01, Meta loss averaged over last 500 steps = 3.1623e-01, PNorm = 119.2272, GNorm = 0.3057
Meta loss on this task batch = 3.0894e-01, Meta loss averaged over last 500 steps = 3.1617e-01, PNorm = 119.2352, GNorm = 0.3476
Meta loss on this task batch = 3.4667e-01, Meta loss averaged over last 500 steps = 3.1632e-01, PNorm = 119.2421, GNorm = 0.3318
Meta loss on this task batch = 3.2656e-01, Meta loss averaged over last 500 steps = 3.1633e-01, PNorm = 119.2490, GNorm = 0.2523
Meta loss on this task batch = 2.5838e-01, Meta loss averaged over last 500 steps = 3.1627e-01, PNorm = 119.2565, GNorm = 0.3183
Meta loss on this task batch = 3.2411e-01, Meta loss averaged over last 500 steps = 3.1626e-01, PNorm = 119.2638, GNorm = 0.2570
Meta loss on this task batch = 3.2743e-01, Meta loss averaged over last 500 steps = 3.1628e-01, PNorm = 119.2716, GNorm = 0.3268
Meta loss on this task batch = 3.1296e-01, Meta loss averaged over last 500 steps = 3.1623e-01, PNorm = 119.2794, GNorm = 0.4167
Meta loss on this task batch = 2.9822e-01, Meta loss averaged over last 500 steps = 3.1626e-01, PNorm = 119.2869, GNorm = 0.2380
Meta loss on this task batch = 3.2920e-01, Meta loss averaged over last 500 steps = 3.1635e-01, PNorm = 119.2933, GNorm = 0.3283
Meta loss on this task batch = 2.5895e-01, Meta loss averaged over last 500 steps = 3.1628e-01, PNorm = 119.2992, GNorm = 0.2483
Took 111.43651795387268 seconds to complete one epoch of meta training
Took 119.2231969833374 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455873
Epoch 364
Meta loss on this task batch = 2.8949e-01, Meta loss averaged over last 500 steps = 3.1618e-01, PNorm = 119.3049, GNorm = 0.2560
Meta loss on this task batch = 2.9574e-01, Meta loss averaged over last 500 steps = 3.1618e-01, PNorm = 119.3090, GNorm = 0.2817
Meta loss on this task batch = 3.9039e-01, Meta loss averaged over last 500 steps = 3.1633e-01, PNorm = 119.3135, GNorm = 0.4032
Meta loss on this task batch = 2.8780e-01, Meta loss averaged over last 500 steps = 3.1616e-01, PNorm = 119.3177, GNorm = 0.2679
Meta loss on this task batch = 2.9211e-01, Meta loss averaged over last 500 steps = 3.1604e-01, PNorm = 119.3223, GNorm = 0.3241
Meta loss on this task batch = 3.0613e-01, Meta loss averaged over last 500 steps = 3.1596e-01, PNorm = 119.3279, GNorm = 0.2852
Meta loss on this task batch = 2.9772e-01, Meta loss averaged over last 500 steps = 3.1601e-01, PNorm = 119.3343, GNorm = 0.3642
Meta loss on this task batch = 2.9798e-01, Meta loss averaged over last 500 steps = 3.1596e-01, PNorm = 119.3407, GNorm = 0.3508
Meta loss on this task batch = 3.3077e-01, Meta loss averaged over last 500 steps = 3.1599e-01, PNorm = 119.3478, GNorm = 0.2687
Meta loss on this task batch = 3.0019e-01, Meta loss averaged over last 500 steps = 3.1607e-01, PNorm = 119.3545, GNorm = 0.2514
Meta loss on this task batch = 3.0340e-01, Meta loss averaged over last 500 steps = 3.1595e-01, PNorm = 119.3611, GNorm = 0.2536
Meta loss on this task batch = 2.8889e-01, Meta loss averaged over last 500 steps = 3.1601e-01, PNorm = 119.3689, GNorm = 0.3094
Meta loss on this task batch = 3.3867e-01, Meta loss averaged over last 500 steps = 3.1607e-01, PNorm = 119.3755, GNorm = 0.2818
Meta loss on this task batch = 3.4906e-01, Meta loss averaged over last 500 steps = 3.1615e-01, PNorm = 119.3815, GNorm = 0.4545
Meta loss on this task batch = 3.0692e-01, Meta loss averaged over last 500 steps = 3.1615e-01, PNorm = 119.3878, GNorm = 0.3655
Meta loss on this task batch = 3.0006e-01, Meta loss averaged over last 500 steps = 3.1606e-01, PNorm = 119.3934, GNorm = 0.3404
Meta loss on this task batch = 2.8619e-01, Meta loss averaged over last 500 steps = 3.1584e-01, PNorm = 119.3999, GNorm = 0.2970
Meta loss on this task batch = 3.7660e-01, Meta loss averaged over last 500 steps = 3.1602e-01, PNorm = 119.4060, GNorm = 0.5439
Meta loss on this task batch = 2.6886e-01, Meta loss averaged over last 500 steps = 3.1588e-01, PNorm = 119.4124, GNorm = 0.3614
Took 111.42541360855103 seconds to complete one epoch of meta training
Took 119.39012837409973 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448973
Epoch 365
Meta loss on this task batch = 2.8000e-01, Meta loss averaged over last 500 steps = 3.1582e-01, PNorm = 119.4203, GNorm = 0.2735
Meta loss on this task batch = 3.3201e-01, Meta loss averaged over last 500 steps = 3.1592e-01, PNorm = 119.4283, GNorm = 0.3163
Meta loss on this task batch = 3.8556e-01, Meta loss averaged over last 500 steps = 3.1608e-01, PNorm = 119.4347, GNorm = 0.2765
Meta loss on this task batch = 2.4494e-01, Meta loss averaged over last 500 steps = 3.1595e-01, PNorm = 119.4416, GNorm = 0.2932
Meta loss on this task batch = 3.2302e-01, Meta loss averaged over last 500 steps = 3.1595e-01, PNorm = 119.4487, GNorm = 0.2355
Meta loss on this task batch = 3.2548e-01, Meta loss averaged over last 500 steps = 3.1598e-01, PNorm = 119.4555, GNorm = 0.4178
Meta loss on this task batch = 2.5010e-01, Meta loss averaged over last 500 steps = 3.1576e-01, PNorm = 119.4630, GNorm = 0.2877
Meta loss on this task batch = 2.7501e-01, Meta loss averaged over last 500 steps = 3.1568e-01, PNorm = 119.4687, GNorm = 0.2736
Meta loss on this task batch = 3.8510e-01, Meta loss averaged over last 500 steps = 3.1577e-01, PNorm = 119.4735, GNorm = 0.2760
Meta loss on this task batch = 3.4704e-01, Meta loss averaged over last 500 steps = 3.1580e-01, PNorm = 119.4788, GNorm = 0.3044
Meta loss on this task batch = 3.3457e-01, Meta loss averaged over last 500 steps = 3.1577e-01, PNorm = 119.4830, GNorm = 0.2692
Meta loss on this task batch = 2.9462e-01, Meta loss averaged over last 500 steps = 3.1572e-01, PNorm = 119.4878, GNorm = 0.3106
Meta loss on this task batch = 3.7262e-01, Meta loss averaged over last 500 steps = 3.1589e-01, PNorm = 119.4927, GNorm = 0.2770
Meta loss on this task batch = 2.8054e-01, Meta loss averaged over last 500 steps = 3.1584e-01, PNorm = 119.4979, GNorm = 0.2762
Meta loss on this task batch = 3.6820e-01, Meta loss averaged over last 500 steps = 3.1594e-01, PNorm = 119.5028, GNorm = 0.4148
Meta loss on this task batch = 2.9619e-01, Meta loss averaged over last 500 steps = 3.1590e-01, PNorm = 119.5077, GNorm = 0.3145
Meta loss on this task batch = 3.0108e-01, Meta loss averaged over last 500 steps = 3.1592e-01, PNorm = 119.5115, GNorm = 0.3082
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 3.1588e-01, PNorm = 119.5156, GNorm = 0.3306
Meta loss on this task batch = 2.7587e-01, Meta loss averaged over last 500 steps = 3.1588e-01, PNorm = 119.5190, GNorm = 0.2915
Took 108.31641578674316 seconds to complete one epoch of meta training
Took 115.96663117408752 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452458
Epoch 366
Meta loss on this task batch = 2.8925e-01, Meta loss averaged over last 500 steps = 3.1579e-01, PNorm = 119.5234, GNorm = 0.2052
Meta loss on this task batch = 3.0527e-01, Meta loss averaged over last 500 steps = 3.1577e-01, PNorm = 119.5280, GNorm = 0.2970
Meta loss on this task batch = 3.5576e-01, Meta loss averaged over last 500 steps = 3.1580e-01, PNorm = 119.5316, GNorm = 0.2454
Meta loss on this task batch = 2.9728e-01, Meta loss averaged over last 500 steps = 3.1565e-01, PNorm = 119.5358, GNorm = 0.2796
Meta loss on this task batch = 2.8591e-01, Meta loss averaged over last 500 steps = 3.1564e-01, PNorm = 119.5396, GNorm = 0.3786
Meta loss on this task batch = 2.5431e-01, Meta loss averaged over last 500 steps = 3.1543e-01, PNorm = 119.5446, GNorm = 0.2896
Meta loss on this task batch = 3.6854e-01, Meta loss averaged over last 500 steps = 3.1557e-01, PNorm = 119.5489, GNorm = 0.2966
Meta loss on this task batch = 2.5029e-01, Meta loss averaged over last 500 steps = 3.1542e-01, PNorm = 119.5538, GNorm = 0.2935
Meta loss on this task batch = 3.0446e-01, Meta loss averaged over last 500 steps = 3.1528e-01, PNorm = 119.5588, GNorm = 0.2793
Meta loss on this task batch = 3.2416e-01, Meta loss averaged over last 500 steps = 3.1532e-01, PNorm = 119.5644, GNorm = 0.3444
Meta loss on this task batch = 2.6239e-01, Meta loss averaged over last 500 steps = 3.1522e-01, PNorm = 119.5699, GNorm = 0.3117
Meta loss on this task batch = 3.6431e-01, Meta loss averaged over last 500 steps = 3.1530e-01, PNorm = 119.5752, GNorm = 0.3635
Meta loss on this task batch = 3.0874e-01, Meta loss averaged over last 500 steps = 3.1530e-01, PNorm = 119.5798, GNorm = 0.2481
Meta loss on this task batch = 3.8905e-01, Meta loss averaged over last 500 steps = 3.1557e-01, PNorm = 119.5830, GNorm = 0.4044
Meta loss on this task batch = 2.9556e-01, Meta loss averaged over last 500 steps = 3.1548e-01, PNorm = 119.5865, GNorm = 0.3081
Meta loss on this task batch = 2.9374e-01, Meta loss averaged over last 500 steps = 3.1533e-01, PNorm = 119.5900, GNorm = 0.2562
Meta loss on this task batch = 3.0054e-01, Meta loss averaged over last 500 steps = 3.1521e-01, PNorm = 119.5941, GNorm = 0.2594
Meta loss on this task batch = 3.1896e-01, Meta loss averaged over last 500 steps = 3.1527e-01, PNorm = 119.5990, GNorm = 0.2757
Meta loss on this task batch = 2.9742e-01, Meta loss averaged over last 500 steps = 3.1527e-01, PNorm = 119.6046, GNorm = 0.3331
Took 111.0481767654419 seconds to complete one epoch of meta training
Took 118.69264030456543 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470486
Epoch 367
Meta loss on this task batch = 3.2222e-01, Meta loss averaged over last 500 steps = 3.1539e-01, PNorm = 119.6102, GNorm = 0.2666
Meta loss on this task batch = 2.9793e-01, Meta loss averaged over last 500 steps = 3.1528e-01, PNorm = 119.6169, GNorm = 0.3265
Meta loss on this task batch = 2.5399e-01, Meta loss averaged over last 500 steps = 3.1523e-01, PNorm = 119.6241, GNorm = 0.2308
Meta loss on this task batch = 3.3330e-01, Meta loss averaged over last 500 steps = 3.1525e-01, PNorm = 119.6317, GNorm = 0.2540
Meta loss on this task batch = 3.6177e-01, Meta loss averaged over last 500 steps = 3.1550e-01, PNorm = 119.6390, GNorm = 0.2786
Meta loss on this task batch = 2.6152e-01, Meta loss averaged over last 500 steps = 3.1529e-01, PNorm = 119.6472, GNorm = 0.2636
Meta loss on this task batch = 3.1988e-01, Meta loss averaged over last 500 steps = 3.1532e-01, PNorm = 119.6549, GNorm = 0.2868
Meta loss on this task batch = 2.8202e-01, Meta loss averaged over last 500 steps = 3.1521e-01, PNorm = 119.6631, GNorm = 0.3045
Meta loss on this task batch = 2.8866e-01, Meta loss averaged over last 500 steps = 3.1516e-01, PNorm = 119.6715, GNorm = 0.4357
Meta loss on this task batch = 3.5658e-01, Meta loss averaged over last 500 steps = 3.1516e-01, PNorm = 119.6799, GNorm = 0.3067
Meta loss on this task batch = 3.5152e-01, Meta loss averaged over last 500 steps = 3.1514e-01, PNorm = 119.6881, GNorm = 0.3160
Meta loss on this task batch = 3.2313e-01, Meta loss averaged over last 500 steps = 3.1503e-01, PNorm = 119.6960, GNorm = 0.2676
Meta loss on this task batch = 3.0893e-01, Meta loss averaged over last 500 steps = 3.1514e-01, PNorm = 119.7031, GNorm = 0.3553
Meta loss on this task batch = 2.9300e-01, Meta loss averaged over last 500 steps = 3.1512e-01, PNorm = 119.7101, GNorm = 0.2540
Meta loss on this task batch = 3.3624e-01, Meta loss averaged over last 500 steps = 3.1524e-01, PNorm = 119.7152, GNorm = 0.3636
Meta loss on this task batch = 3.3510e-01, Meta loss averaged over last 500 steps = 3.1531e-01, PNorm = 119.7206, GNorm = 0.3272
Meta loss on this task batch = 3.1797e-01, Meta loss averaged over last 500 steps = 3.1518e-01, PNorm = 119.7257, GNorm = 0.3408
Meta loss on this task batch = 2.7549e-01, Meta loss averaged over last 500 steps = 3.1525e-01, PNorm = 119.7313, GNorm = 0.4690
Meta loss on this task batch = 2.9001e-01, Meta loss averaged over last 500 steps = 3.1516e-01, PNorm = 119.7367, GNorm = 0.3646
Took 114.82137298583984 seconds to complete one epoch of meta training
Took 122.39429426193237 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474689
Epoch 368
Meta loss on this task batch = 3.3812e-01, Meta loss averaged over last 500 steps = 3.1499e-01, PNorm = 119.7424, GNorm = 0.5140
Meta loss on this task batch = 3.5339e-01, Meta loss averaged over last 500 steps = 3.1513e-01, PNorm = 119.7481, GNorm = 0.4400
Meta loss on this task batch = 3.3335e-01, Meta loss averaged over last 500 steps = 3.1501e-01, PNorm = 119.7536, GNorm = 0.3471
Meta loss on this task batch = 3.6252e-01, Meta loss averaged over last 500 steps = 3.1505e-01, PNorm = 119.7593, GNorm = 0.3802
Meta loss on this task batch = 3.0555e-01, Meta loss averaged over last 500 steps = 3.1510e-01, PNorm = 119.7643, GNorm = 0.2896
Meta loss on this task batch = 3.2757e-01, Meta loss averaged over last 500 steps = 3.1506e-01, PNorm = 119.7703, GNorm = 0.2607
Meta loss on this task batch = 3.2671e-01, Meta loss averaged over last 500 steps = 3.1508e-01, PNorm = 119.7770, GNorm = 0.2475
Meta loss on this task batch = 3.3236e-01, Meta loss averaged over last 500 steps = 3.1513e-01, PNorm = 119.7838, GNorm = 0.2816
Meta loss on this task batch = 3.1862e-01, Meta loss averaged over last 500 steps = 3.1509e-01, PNorm = 119.7907, GNorm = 0.2303
Meta loss on this task batch = 3.2191e-01, Meta loss averaged over last 500 steps = 3.1514e-01, PNorm = 119.7980, GNorm = 0.2672
Meta loss on this task batch = 2.9827e-01, Meta loss averaged over last 500 steps = 3.1512e-01, PNorm = 119.8047, GNorm = 0.2881
Meta loss on this task batch = 3.5139e-01, Meta loss averaged over last 500 steps = 3.1525e-01, PNorm = 119.8100, GNorm = 0.3593
Meta loss on this task batch = 2.2477e-01, Meta loss averaged over last 500 steps = 3.1507e-01, PNorm = 119.8158, GNorm = 0.2647
Meta loss on this task batch = 3.0655e-01, Meta loss averaged over last 500 steps = 3.1497e-01, PNorm = 119.8218, GNorm = 0.3366
Meta loss on this task batch = 2.7188e-01, Meta loss averaged over last 500 steps = 3.1489e-01, PNorm = 119.8289, GNorm = 0.2736
Meta loss on this task batch = 3.1852e-01, Meta loss averaged over last 500 steps = 3.1503e-01, PNorm = 119.8363, GNorm = 0.2508
Meta loss on this task batch = 3.0237e-01, Meta loss averaged over last 500 steps = 3.1501e-01, PNorm = 119.8446, GNorm = 0.3114
Meta loss on this task batch = 3.2470e-01, Meta loss averaged over last 500 steps = 3.1499e-01, PNorm = 119.8523, GNorm = 0.2921
Meta loss on this task batch = 3.0391e-01, Meta loss averaged over last 500 steps = 3.1492e-01, PNorm = 119.8595, GNorm = 0.3284
Took 111.07016706466675 seconds to complete one epoch of meta training
Took 118.45509266853333 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.425688
Epoch 369
Meta loss on this task batch = 2.9802e-01, Meta loss averaged over last 500 steps = 3.1492e-01, PNorm = 119.8669, GNorm = 0.2662
Meta loss on this task batch = 3.3390e-01, Meta loss averaged over last 500 steps = 3.1487e-01, PNorm = 119.8738, GNorm = 0.3080
Meta loss on this task batch = 2.8347e-01, Meta loss averaged over last 500 steps = 3.1471e-01, PNorm = 119.8806, GNorm = 0.3804
Meta loss on this task batch = 3.5314e-01, Meta loss averaged over last 500 steps = 3.1479e-01, PNorm = 119.8871, GNorm = 0.3752
Meta loss on this task batch = 3.1279e-01, Meta loss averaged over last 500 steps = 3.1468e-01, PNorm = 119.8946, GNorm = 0.3165
Meta loss on this task batch = 2.9553e-01, Meta loss averaged over last 500 steps = 3.1467e-01, PNorm = 119.9020, GNorm = 0.2869
Meta loss on this task batch = 2.8344e-01, Meta loss averaged over last 500 steps = 3.1456e-01, PNorm = 119.9097, GNorm = 0.3233
Meta loss on this task batch = 3.0961e-01, Meta loss averaged over last 500 steps = 3.1459e-01, PNorm = 119.9170, GNorm = 0.5638
Meta loss on this task batch = 3.1441e-01, Meta loss averaged over last 500 steps = 3.1465e-01, PNorm = 119.9246, GNorm = 0.2922
Meta loss on this task batch = 3.4742e-01, Meta loss averaged over last 500 steps = 3.1472e-01, PNorm = 119.9313, GNorm = 0.2669
Meta loss on this task batch = 2.5139e-01, Meta loss averaged over last 500 steps = 3.1454e-01, PNorm = 119.9390, GNorm = 0.2399
Meta loss on this task batch = 2.8978e-01, Meta loss averaged over last 500 steps = 3.1458e-01, PNorm = 119.9459, GNorm = 0.2768
Meta loss on this task batch = 3.1020e-01, Meta loss averaged over last 500 steps = 3.1461e-01, PNorm = 119.9530, GNorm = 0.2480
Meta loss on this task batch = 3.2277e-01, Meta loss averaged over last 500 steps = 3.1455e-01, PNorm = 119.9595, GNorm = 0.2607
Meta loss on this task batch = 3.5844e-01, Meta loss averaged over last 500 steps = 3.1467e-01, PNorm = 119.9649, GNorm = 0.2985
Meta loss on this task batch = 3.2748e-01, Meta loss averaged over last 500 steps = 3.1474e-01, PNorm = 119.9704, GNorm = 0.3166
Meta loss on this task batch = 3.6097e-01, Meta loss averaged over last 500 steps = 3.1480e-01, PNorm = 119.9744, GNorm = 0.3057
Meta loss on this task batch = 2.9828e-01, Meta loss averaged over last 500 steps = 3.1470e-01, PNorm = 119.9793, GNorm = 0.3278
Meta loss on this task batch = 2.7671e-01, Meta loss averaged over last 500 steps = 3.1460e-01, PNorm = 119.9849, GNorm = 0.3046
Took 109.23986840248108 seconds to complete one epoch of meta training
Took 117.16445684432983 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.435415
Epoch 370
Meta loss on this task batch = 3.3397e-01, Meta loss averaged over last 500 steps = 3.1474e-01, PNorm = 119.9902, GNorm = 0.3194
Meta loss on this task batch = 2.5779e-01, Meta loss averaged over last 500 steps = 3.1450e-01, PNorm = 119.9961, GNorm = 0.2256
Meta loss on this task batch = 3.2654e-01, Meta loss averaged over last 500 steps = 3.1436e-01, PNorm = 120.0020, GNorm = 0.2640
Meta loss on this task batch = 3.0010e-01, Meta loss averaged over last 500 steps = 3.1421e-01, PNorm = 120.0078, GNorm = 0.3823
Meta loss on this task batch = 3.4822e-01, Meta loss averaged over last 500 steps = 3.1425e-01, PNorm = 120.0134, GNorm = 0.3626
Meta loss on this task batch = 2.9744e-01, Meta loss averaged over last 500 steps = 3.1413e-01, PNorm = 120.0190, GNorm = 0.3690
Meta loss on this task batch = 3.1406e-01, Meta loss averaged over last 500 steps = 3.1421e-01, PNorm = 120.0249, GNorm = 0.3047
Meta loss on this task batch = 3.2797e-01, Meta loss averaged over last 500 steps = 3.1427e-01, PNorm = 120.0301, GNorm = 0.2654
Meta loss on this task batch = 3.6408e-01, Meta loss averaged over last 500 steps = 3.1434e-01, PNorm = 120.0354, GNorm = 0.2883
Meta loss on this task batch = 2.6584e-01, Meta loss averaged over last 500 steps = 3.1423e-01, PNorm = 120.0406, GNorm = 0.3001
Meta loss on this task batch = 2.4322e-01, Meta loss averaged over last 500 steps = 3.1408e-01, PNorm = 120.0467, GNorm = 0.1952
Meta loss on this task batch = 2.9872e-01, Meta loss averaged over last 500 steps = 3.1405e-01, PNorm = 120.0525, GNorm = 0.3443
Meta loss on this task batch = 3.7683e-01, Meta loss averaged over last 500 steps = 3.1422e-01, PNorm = 120.0587, GNorm = 0.5951
Meta loss on this task batch = 3.0020e-01, Meta loss averaged over last 500 steps = 3.1412e-01, PNorm = 120.0650, GNorm = 0.2511
Meta loss on this task batch = 3.2268e-01, Meta loss averaged over last 500 steps = 3.1411e-01, PNorm = 120.0716, GNorm = 0.2700
Meta loss on this task batch = 2.5655e-01, Meta loss averaged over last 500 steps = 3.1401e-01, PNorm = 120.0787, GNorm = 0.2222
Meta loss on this task batch = 2.7771e-01, Meta loss averaged over last 500 steps = 3.1395e-01, PNorm = 120.0865, GNorm = 0.2810
Meta loss on this task batch = 3.6405e-01, Meta loss averaged over last 500 steps = 3.1391e-01, PNorm = 120.0938, GNorm = 0.2861
Meta loss on this task batch = 2.7565e-01, Meta loss averaged over last 500 steps = 3.1392e-01, PNorm = 120.1000, GNorm = 0.3513
Took 109.92806458473206 seconds to complete one epoch of meta training
Took 118.13583016395569 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454357
Epoch 371
Meta loss on this task batch = 3.4291e-01, Meta loss averaged over last 500 steps = 3.1398e-01, PNorm = 120.1067, GNorm = 0.3799
Meta loss on this task batch = 2.9509e-01, Meta loss averaged over last 500 steps = 3.1390e-01, PNorm = 120.1135, GNorm = 0.3653
Meta loss on this task batch = 2.8429e-01, Meta loss averaged over last 500 steps = 3.1368e-01, PNorm = 120.1205, GNorm = 0.2838
Meta loss on this task batch = 2.8042e-01, Meta loss averaged over last 500 steps = 3.1354e-01, PNorm = 120.1277, GNorm = 0.3137
Meta loss on this task batch = 3.0208e-01, Meta loss averaged over last 500 steps = 3.1368e-01, PNorm = 120.1343, GNorm = 0.2926
Meta loss on this task batch = 3.6337e-01, Meta loss averaged over last 500 steps = 3.1358e-01, PNorm = 120.1398, GNorm = 0.3613
Meta loss on this task batch = 2.7937e-01, Meta loss averaged over last 500 steps = 3.1339e-01, PNorm = 120.1461, GNorm = 0.3043
Meta loss on this task batch = 3.0478e-01, Meta loss averaged over last 500 steps = 3.1340e-01, PNorm = 120.1534, GNorm = 0.3232
Meta loss on this task batch = 2.7543e-01, Meta loss averaged over last 500 steps = 3.1327e-01, PNorm = 120.1605, GNorm = 0.3223
Meta loss on this task batch = 3.0439e-01, Meta loss averaged over last 500 steps = 3.1318e-01, PNorm = 120.1673, GNorm = 0.3203
Meta loss on this task batch = 3.2574e-01, Meta loss averaged over last 500 steps = 3.1326e-01, PNorm = 120.1727, GNorm = 0.3425
Meta loss on this task batch = 3.7092e-01, Meta loss averaged over last 500 steps = 3.1334e-01, PNorm = 120.1779, GNorm = 0.3970
Meta loss on this task batch = 3.1121e-01, Meta loss averaged over last 500 steps = 3.1336e-01, PNorm = 120.1831, GNorm = 0.4209
Meta loss on this task batch = 2.9292e-01, Meta loss averaged over last 500 steps = 3.1337e-01, PNorm = 120.1886, GNorm = 0.2806
Meta loss on this task batch = 3.2089e-01, Meta loss averaged over last 500 steps = 3.1338e-01, PNorm = 120.1943, GNorm = 0.2706
Meta loss on this task batch = 3.0409e-01, Meta loss averaged over last 500 steps = 3.1334e-01, PNorm = 120.1996, GNorm = 0.2862
Meta loss on this task batch = 3.1150e-01, Meta loss averaged over last 500 steps = 3.1334e-01, PNorm = 120.2047, GNorm = 0.3311
Meta loss on this task batch = 2.9528e-01, Meta loss averaged over last 500 steps = 3.1331e-01, PNorm = 120.2093, GNorm = 0.2719
Meta loss on this task batch = 3.3851e-01, Meta loss averaged over last 500 steps = 3.1332e-01, PNorm = 120.2141, GNorm = 0.2999
Took 113.67136979103088 seconds to complete one epoch of meta training
Took 121.36992049217224 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463528
Epoch 372
Meta loss on this task batch = 3.6502e-01, Meta loss averaged over last 500 steps = 3.1337e-01, PNorm = 120.2194, GNorm = 0.3166
Meta loss on this task batch = 3.3677e-01, Meta loss averaged over last 500 steps = 3.1338e-01, PNorm = 120.2242, GNorm = 0.3137
Meta loss on this task batch = 3.2953e-01, Meta loss averaged over last 500 steps = 3.1337e-01, PNorm = 120.2293, GNorm = 0.2555
Meta loss on this task batch = 2.9804e-01, Meta loss averaged over last 500 steps = 3.1331e-01, PNorm = 120.2338, GNorm = 0.2760
Meta loss on this task batch = 3.0041e-01, Meta loss averaged over last 500 steps = 3.1335e-01, PNorm = 120.2399, GNorm = 0.2559
Meta loss on this task batch = 3.3391e-01, Meta loss averaged over last 500 steps = 3.1331e-01, PNorm = 120.2458, GNorm = 0.3561
Meta loss on this task batch = 3.5770e-01, Meta loss averaged over last 500 steps = 3.1343e-01, PNorm = 120.2517, GNorm = 0.2582
Meta loss on this task batch = 3.2554e-01, Meta loss averaged over last 500 steps = 3.1350e-01, PNorm = 120.2571, GNorm = 0.3795
Meta loss on this task batch = 3.2088e-01, Meta loss averaged over last 500 steps = 3.1354e-01, PNorm = 120.2625, GNorm = 0.2824
Meta loss on this task batch = 2.8486e-01, Meta loss averaged over last 500 steps = 3.1349e-01, PNorm = 120.2687, GNorm = 0.2915
Meta loss on this task batch = 2.9999e-01, Meta loss averaged over last 500 steps = 3.1353e-01, PNorm = 120.2739, GNorm = 0.3858
Meta loss on this task batch = 3.2526e-01, Meta loss averaged over last 500 steps = 3.1354e-01, PNorm = 120.2800, GNorm = 0.4296
Meta loss on this task batch = 3.1111e-01, Meta loss averaged over last 500 steps = 3.1338e-01, PNorm = 120.2868, GNorm = 0.3152
Meta loss on this task batch = 3.3497e-01, Meta loss averaged over last 500 steps = 3.1343e-01, PNorm = 120.2935, GNorm = 0.3652
Meta loss on this task batch = 2.6690e-01, Meta loss averaged over last 500 steps = 3.1344e-01, PNorm = 120.2999, GNorm = 0.3742
Meta loss on this task batch = 3.0509e-01, Meta loss averaged over last 500 steps = 3.1339e-01, PNorm = 120.3065, GNorm = 0.2467
Meta loss on this task batch = 2.8565e-01, Meta loss averaged over last 500 steps = 3.1315e-01, PNorm = 120.3132, GNorm = 0.3080
Meta loss on this task batch = 3.0621e-01, Meta loss averaged over last 500 steps = 3.1315e-01, PNorm = 120.3201, GNorm = 0.2753
Meta loss on this task batch = 2.6502e-01, Meta loss averaged over last 500 steps = 3.1300e-01, PNorm = 120.3278, GNorm = 0.2843
Took 119.21914887428284 seconds to complete one epoch of meta training
Took 127.65506076812744 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456395
Epoch 373
Meta loss on this task batch = 2.6915e-01, Meta loss averaged over last 500 steps = 3.1301e-01, PNorm = 120.3355, GNorm = 0.2467
Meta loss on this task batch = 3.0768e-01, Meta loss averaged over last 500 steps = 3.1303e-01, PNorm = 120.3431, GNorm = 0.2462
Meta loss on this task batch = 2.7125e-01, Meta loss averaged over last 500 steps = 3.1290e-01, PNorm = 120.3519, GNorm = 0.2798
Meta loss on this task batch = 2.6163e-01, Meta loss averaged over last 500 steps = 3.1273e-01, PNorm = 120.3610, GNorm = 0.3118
Meta loss on this task batch = 2.8748e-01, Meta loss averaged over last 500 steps = 3.1266e-01, PNorm = 120.3690, GNorm = 0.4182
Meta loss on this task batch = 3.1661e-01, Meta loss averaged over last 500 steps = 3.1257e-01, PNorm = 120.3769, GNorm = 0.3750
Meta loss on this task batch = 2.9364e-01, Meta loss averaged over last 500 steps = 3.1259e-01, PNorm = 120.3838, GNorm = 0.4280
Meta loss on this task batch = 2.7611e-01, Meta loss averaged over last 500 steps = 3.1259e-01, PNorm = 120.3906, GNorm = 0.3268
Meta loss on this task batch = 3.4702e-01, Meta loss averaged over last 500 steps = 3.1268e-01, PNorm = 120.3964, GNorm = 0.3171
Meta loss on this task batch = 3.0609e-01, Meta loss averaged over last 500 steps = 3.1261e-01, PNorm = 120.4024, GNorm = 0.2587
Meta loss on this task batch = 2.7244e-01, Meta loss averaged over last 500 steps = 3.1250e-01, PNorm = 120.4089, GNorm = 0.3034
Meta loss on this task batch = 3.8332e-01, Meta loss averaged over last 500 steps = 3.1267e-01, PNorm = 120.4154, GNorm = 0.3300
Meta loss on this task batch = 2.9197e-01, Meta loss averaged over last 500 steps = 3.1255e-01, PNorm = 120.4220, GNorm = 0.2957
Meta loss on this task batch = 3.3167e-01, Meta loss averaged over last 500 steps = 3.1263e-01, PNorm = 120.4287, GNorm = 0.3501
Meta loss on this task batch = 3.5255e-01, Meta loss averaged over last 500 steps = 3.1272e-01, PNorm = 120.4353, GNorm = 0.4615
Meta loss on this task batch = 2.9208e-01, Meta loss averaged over last 500 steps = 3.1266e-01, PNorm = 120.4433, GNorm = 0.4074
Meta loss on this task batch = 3.4148e-01, Meta loss averaged over last 500 steps = 3.1273e-01, PNorm = 120.4515, GNorm = 0.2717
Meta loss on this task batch = 3.4599e-01, Meta loss averaged over last 500 steps = 3.1282e-01, PNorm = 120.4603, GNorm = 0.2734
Meta loss on this task batch = 3.5500e-01, Meta loss averaged over last 500 steps = 3.1278e-01, PNorm = 120.4688, GNorm = 0.3499
Took 114.83745765686035 seconds to complete one epoch of meta training
Took 123.03773045539856 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457138
Epoch 374
Meta loss on this task batch = 2.5581e-01, Meta loss averaged over last 500 steps = 3.1257e-01, PNorm = 120.4778, GNorm = 0.2202
Meta loss on this task batch = 2.7617e-01, Meta loss averaged over last 500 steps = 3.1250e-01, PNorm = 120.4876, GNorm = 0.4211
Meta loss on this task batch = 3.7749e-01, Meta loss averaged over last 500 steps = 3.1272e-01, PNorm = 120.4966, GNorm = 0.3214
Meta loss on this task batch = 2.3143e-01, Meta loss averaged over last 500 steps = 3.1259e-01, PNorm = 120.5052, GNorm = 0.3099
Meta loss on this task batch = 2.9845e-01, Meta loss averaged over last 500 steps = 3.1257e-01, PNorm = 120.5141, GNorm = 0.2462
Meta loss on this task batch = 3.0837e-01, Meta loss averaged over last 500 steps = 3.1237e-01, PNorm = 120.5221, GNorm = 0.4141
Meta loss on this task batch = 3.3887e-01, Meta loss averaged over last 500 steps = 3.1241e-01, PNorm = 120.5291, GNorm = 0.2798
Meta loss on this task batch = 3.2693e-01, Meta loss averaged over last 500 steps = 3.1238e-01, PNorm = 120.5364, GNorm = 0.2961
Meta loss on this task batch = 3.2731e-01, Meta loss averaged over last 500 steps = 3.1241e-01, PNorm = 120.5429, GNorm = 0.3164
Meta loss on this task batch = 2.5862e-01, Meta loss averaged over last 500 steps = 3.1221e-01, PNorm = 120.5495, GNorm = 0.3134
Meta loss on this task batch = 3.1084e-01, Meta loss averaged over last 500 steps = 3.1211e-01, PNorm = 120.5566, GNorm = 0.3175
Meta loss on this task batch = 3.4995e-01, Meta loss averaged over last 500 steps = 3.1218e-01, PNorm = 120.5639, GNorm = 0.3823
Meta loss on this task batch = 3.4797e-01, Meta loss averaged over last 500 steps = 3.1234e-01, PNorm = 120.5709, GNorm = 0.3343
Meta loss on this task batch = 3.5632e-01, Meta loss averaged over last 500 steps = 3.1239e-01, PNorm = 120.5787, GNorm = 0.3033
Meta loss on this task batch = 2.9142e-01, Meta loss averaged over last 500 steps = 3.1246e-01, PNorm = 120.5869, GNorm = 0.2928
Meta loss on this task batch = 3.3291e-01, Meta loss averaged over last 500 steps = 3.1247e-01, PNorm = 120.5947, GNorm = 0.2746
Meta loss on this task batch = 2.9412e-01, Meta loss averaged over last 500 steps = 3.1243e-01, PNorm = 120.6026, GNorm = 0.4496
Meta loss on this task batch = 3.3079e-01, Meta loss averaged over last 500 steps = 3.1253e-01, PNorm = 120.6098, GNorm = 0.2800
Meta loss on this task batch = 3.3544e-01, Meta loss averaged over last 500 steps = 3.1261e-01, PNorm = 120.6175, GNorm = 0.3072
Took 116.85187101364136 seconds to complete one epoch of meta training
Took 124.83438420295715 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474082
Epoch 375
Meta loss on this task batch = 3.1236e-01, Meta loss averaged over last 500 steps = 3.1254e-01, PNorm = 120.6246, GNorm = 0.2919
Meta loss on this task batch = 3.1093e-01, Meta loss averaged over last 500 steps = 3.1252e-01, PNorm = 120.6313, GNorm = 0.2748
Meta loss on this task batch = 3.3462e-01, Meta loss averaged over last 500 steps = 3.1244e-01, PNorm = 120.6385, GNorm = 0.3049
Meta loss on this task batch = 3.2001e-01, Meta loss averaged over last 500 steps = 3.1249e-01, PNorm = 120.6447, GNorm = 0.3913
Meta loss on this task batch = 3.4612e-01, Meta loss averaged over last 500 steps = 3.1254e-01, PNorm = 120.6500, GNorm = 0.3283
Meta loss on this task batch = 3.2875e-01, Meta loss averaged over last 500 steps = 3.1259e-01, PNorm = 120.6568, GNorm = 0.2669
Meta loss on this task batch = 2.7299e-01, Meta loss averaged over last 500 steps = 3.1256e-01, PNorm = 120.6639, GNorm = 0.2393
Meta loss on this task batch = 2.5876e-01, Meta loss averaged over last 500 steps = 3.1248e-01, PNorm = 120.6709, GNorm = 0.2301
Meta loss on this task batch = 2.9302e-01, Meta loss averaged over last 500 steps = 3.1234e-01, PNorm = 120.6773, GNorm = 0.5125
Meta loss on this task batch = 3.3666e-01, Meta loss averaged over last 500 steps = 3.1250e-01, PNorm = 120.6825, GNorm = 0.3269
Meta loss on this task batch = 3.0797e-01, Meta loss averaged over last 500 steps = 3.1262e-01, PNorm = 120.6878, GNorm = 0.5738
Meta loss on this task batch = 2.9901e-01, Meta loss averaged over last 500 steps = 3.1252e-01, PNorm = 120.6938, GNorm = 0.2729
Meta loss on this task batch = 3.7736e-01, Meta loss averaged over last 500 steps = 3.1262e-01, PNorm = 120.6993, GNorm = 0.4295
Meta loss on this task batch = 3.2153e-01, Meta loss averaged over last 500 steps = 3.1266e-01, PNorm = 120.7045, GNorm = 0.2830
Meta loss on this task batch = 3.4800e-01, Meta loss averaged over last 500 steps = 3.1271e-01, PNorm = 120.7094, GNorm = 0.4066
Meta loss on this task batch = 2.6396e-01, Meta loss averaged over last 500 steps = 3.1266e-01, PNorm = 120.7152, GNorm = 0.2413
Meta loss on this task batch = 3.2152e-01, Meta loss averaged over last 500 steps = 3.1275e-01, PNorm = 120.7217, GNorm = 0.3153
Meta loss on this task batch = 3.1968e-01, Meta loss averaged over last 500 steps = 3.1275e-01, PNorm = 120.7289, GNorm = 0.3000
Meta loss on this task batch = 3.4337e-01, Meta loss averaged over last 500 steps = 3.1272e-01, PNorm = 120.7362, GNorm = 0.5061
Took 111.73786354064941 seconds to complete one epoch of meta training
Took 119.29918766021729 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460357
Epoch 376
Meta loss on this task batch = 3.5167e-01, Meta loss averaged over last 500 steps = 3.1284e-01, PNorm = 120.7433, GNorm = 0.2828
Meta loss on this task batch = 3.1163e-01, Meta loss averaged over last 500 steps = 3.1280e-01, PNorm = 120.7499, GNorm = 0.2603
Meta loss on this task batch = 2.1604e-01, Meta loss averaged over last 500 steps = 3.1257e-01, PNorm = 120.7576, GNorm = 0.2204
Meta loss on this task batch = 2.5452e-01, Meta loss averaged over last 500 steps = 3.1234e-01, PNorm = 120.7650, GNorm = 0.2416
Meta loss on this task batch = 3.0542e-01, Meta loss averaged over last 500 steps = 3.1227e-01, PNorm = 120.7716, GNorm = 0.2783
Meta loss on this task batch = 3.1893e-01, Meta loss averaged over last 500 steps = 3.1242e-01, PNorm = 120.7773, GNorm = 0.3768
Meta loss on this task batch = 3.3110e-01, Meta loss averaged over last 500 steps = 3.1239e-01, PNorm = 120.7831, GNorm = 0.4257
Meta loss on this task batch = 3.3327e-01, Meta loss averaged over last 500 steps = 3.1236e-01, PNorm = 120.7891, GNorm = 0.2592
Meta loss on this task batch = 3.9043e-01, Meta loss averaged over last 500 steps = 3.1256e-01, PNorm = 120.7935, GNorm = 0.4259
Meta loss on this task batch = 3.0961e-01, Meta loss averaged over last 500 steps = 3.1253e-01, PNorm = 120.7977, GNorm = 0.2385
Meta loss on this task batch = 2.6843e-01, Meta loss averaged over last 500 steps = 3.1245e-01, PNorm = 120.8021, GNorm = 0.2236
Meta loss on this task batch = 3.2131e-01, Meta loss averaged over last 500 steps = 3.1251e-01, PNorm = 120.8060, GNorm = 0.4131
Meta loss on this task batch = 2.9667e-01, Meta loss averaged over last 500 steps = 3.1255e-01, PNorm = 120.8102, GNorm = 0.4541
Meta loss on this task batch = 2.9607e-01, Meta loss averaged over last 500 steps = 3.1253e-01, PNorm = 120.8151, GNorm = 0.4395
Meta loss on this task batch = 3.7363e-01, Meta loss averaged over last 500 steps = 3.1268e-01, PNorm = 120.8197, GNorm = 0.2816
Meta loss on this task batch = 3.3987e-01, Meta loss averaged over last 500 steps = 3.1276e-01, PNorm = 120.8245, GNorm = 0.3443
Meta loss on this task batch = 2.6853e-01, Meta loss averaged over last 500 steps = 3.1267e-01, PNorm = 120.8301, GNorm = 0.2632
Meta loss on this task batch = 2.7804e-01, Meta loss averaged over last 500 steps = 3.1263e-01, PNorm = 120.8367, GNorm = 0.3417
Meta loss on this task batch = 3.6161e-01, Meta loss averaged over last 500 steps = 3.1267e-01, PNorm = 120.8431, GNorm = 0.3785
Took 111.45078492164612 seconds to complete one epoch of meta training
Took 119.21974062919617 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486613
Epoch 377
Meta loss on this task batch = 3.2830e-01, Meta loss averaged over last 500 steps = 3.1271e-01, PNorm = 120.8497, GNorm = 0.3412
Meta loss on this task batch = 3.1679e-01, Meta loss averaged over last 500 steps = 3.1278e-01, PNorm = 120.8575, GNorm = 0.3575
Meta loss on this task batch = 2.9561e-01, Meta loss averaged over last 500 steps = 3.1266e-01, PNorm = 120.8662, GNorm = 0.3580
Meta loss on this task batch = 3.1555e-01, Meta loss averaged over last 500 steps = 3.1266e-01, PNorm = 120.8750, GNorm = 0.3603
Meta loss on this task batch = 3.0039e-01, Meta loss averaged over last 500 steps = 3.1262e-01, PNorm = 120.8837, GNorm = 0.2655
Meta loss on this task batch = 2.4520e-01, Meta loss averaged over last 500 steps = 3.1245e-01, PNorm = 120.8926, GNorm = 0.4258
Meta loss on this task batch = 3.0648e-01, Meta loss averaged over last 500 steps = 3.1239e-01, PNorm = 120.9012, GNorm = 0.3245
Meta loss on this task batch = 2.9950e-01, Meta loss averaged over last 500 steps = 3.1229e-01, PNorm = 120.9097, GNorm = 0.3119
Meta loss on this task batch = 3.6411e-01, Meta loss averaged over last 500 steps = 3.1245e-01, PNorm = 120.9173, GNorm = 0.3380
Meta loss on this task batch = 2.9867e-01, Meta loss averaged over last 500 steps = 3.1254e-01, PNorm = 120.9241, GNorm = 0.4545
Meta loss on this task batch = 2.8104e-01, Meta loss averaged over last 500 steps = 3.1252e-01, PNorm = 120.9306, GNorm = 0.3364
Meta loss on this task batch = 2.9896e-01, Meta loss averaged over last 500 steps = 3.1261e-01, PNorm = 120.9368, GNorm = 0.3027
Meta loss on this task batch = 3.2821e-01, Meta loss averaged over last 500 steps = 3.1277e-01, PNorm = 120.9408, GNorm = 0.3084
Meta loss on this task batch = 2.9124e-01, Meta loss averaged over last 500 steps = 3.1267e-01, PNorm = 120.9451, GNorm = 0.2731
Meta loss on this task batch = 3.4729e-01, Meta loss averaged over last 500 steps = 3.1262e-01, PNorm = 120.9489, GNorm = 0.3598
Meta loss on this task batch = 3.5374e-01, Meta loss averaged over last 500 steps = 3.1261e-01, PNorm = 120.9523, GNorm = 0.2696
Meta loss on this task batch = 3.3533e-01, Meta loss averaged over last 500 steps = 3.1265e-01, PNorm = 120.9558, GNorm = 0.3032
Meta loss on this task batch = 3.3451e-01, Meta loss averaged over last 500 steps = 3.1259e-01, PNorm = 120.9589, GNorm = 0.2460
Meta loss on this task batch = 2.1238e-01, Meta loss averaged over last 500 steps = 3.1245e-01, PNorm = 120.9636, GNorm = 0.2621
Took 111.02029299736023 seconds to complete one epoch of meta training
Took 118.92461729049683 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453919
Epoch 378
Meta loss on this task batch = 2.7942e-01, Meta loss averaged over last 500 steps = 3.1233e-01, PNorm = 120.9687, GNorm = 0.2486
Meta loss on this task batch = 2.9906e-01, Meta loss averaged over last 500 steps = 3.1227e-01, PNorm = 120.9738, GNorm = 0.3113
Meta loss on this task batch = 3.0767e-01, Meta loss averaged over last 500 steps = 3.1228e-01, PNorm = 120.9792, GNorm = 0.3247
Meta loss on this task batch = 3.3428e-01, Meta loss averaged over last 500 steps = 3.1231e-01, PNorm = 120.9848, GNorm = 0.4808
Meta loss on this task batch = 3.9641e-01, Meta loss averaged over last 500 steps = 3.1237e-01, PNorm = 120.9902, GNorm = 0.4201
Meta loss on this task batch = 2.9619e-01, Meta loss averaged over last 500 steps = 3.1233e-01, PNorm = 120.9969, GNorm = 0.4713
Meta loss on this task batch = 3.1247e-01, Meta loss averaged over last 500 steps = 3.1234e-01, PNorm = 121.0035, GNorm = 0.3246
Meta loss on this task batch = 3.1060e-01, Meta loss averaged over last 500 steps = 3.1233e-01, PNorm = 121.0108, GNorm = 0.2951
Meta loss on this task batch = 3.2518e-01, Meta loss averaged over last 500 steps = 3.1237e-01, PNorm = 121.0188, GNorm = 0.3196
Meta loss on this task batch = 2.8590e-01, Meta loss averaged over last 500 steps = 3.1240e-01, PNorm = 121.0271, GNorm = 0.3899
Meta loss on this task batch = 2.9367e-01, Meta loss averaged over last 500 steps = 3.1235e-01, PNorm = 121.0352, GNorm = 0.3519
Meta loss on this task batch = 2.8610e-01, Meta loss averaged over last 500 steps = 3.1221e-01, PNorm = 121.0436, GNorm = 0.2275
Meta loss on this task batch = 2.9718e-01, Meta loss averaged over last 500 steps = 3.1221e-01, PNorm = 121.0526, GNorm = 0.2703
Meta loss on this task batch = 2.9544e-01, Meta loss averaged over last 500 steps = 3.1232e-01, PNorm = 121.0616, GNorm = 0.2906
Meta loss on this task batch = 2.9488e-01, Meta loss averaged over last 500 steps = 3.1225e-01, PNorm = 121.0707, GNorm = 0.2723
Meta loss on this task batch = 2.9439e-01, Meta loss averaged over last 500 steps = 3.1221e-01, PNorm = 121.0789, GNorm = 0.2905
Meta loss on this task batch = 3.0566e-01, Meta loss averaged over last 500 steps = 3.1224e-01, PNorm = 121.0872, GNorm = 0.3156
Meta loss on this task batch = 3.3970e-01, Meta loss averaged over last 500 steps = 3.1218e-01, PNorm = 121.0958, GNorm = 0.3818
Meta loss on this task batch = 3.3678e-01, Meta loss averaged over last 500 steps = 3.1225e-01, PNorm = 121.1047, GNorm = 0.3216
Took 108.09431600570679 seconds to complete one epoch of meta training
Took 115.75876307487488 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458292
Epoch 379
Meta loss on this task batch = 2.8905e-01, Meta loss averaged over last 500 steps = 3.1226e-01, PNorm = 121.1131, GNorm = 0.2356
Meta loss on this task batch = 3.1078e-01, Meta loss averaged over last 500 steps = 3.1227e-01, PNorm = 121.1209, GNorm = 0.2718
Meta loss on this task batch = 3.2338e-01, Meta loss averaged over last 500 steps = 3.1218e-01, PNorm = 121.1281, GNorm = 0.2818
Meta loss on this task batch = 2.8716e-01, Meta loss averaged over last 500 steps = 3.1211e-01, PNorm = 121.1348, GNorm = 0.2686
Meta loss on this task batch = 2.7096e-01, Meta loss averaged over last 500 steps = 3.1202e-01, PNorm = 121.1421, GNorm = 0.2547
Meta loss on this task batch = 3.1563e-01, Meta loss averaged over last 500 steps = 3.1202e-01, PNorm = 121.1485, GNorm = 0.2771
Meta loss on this task batch = 2.9429e-01, Meta loss averaged over last 500 steps = 3.1203e-01, PNorm = 121.1539, GNorm = 0.3586
Meta loss on this task batch = 2.6011e-01, Meta loss averaged over last 500 steps = 3.1195e-01, PNorm = 121.1595, GNorm = 0.2777
Meta loss on this task batch = 3.3133e-01, Meta loss averaged over last 500 steps = 3.1211e-01, PNorm = 121.1650, GNorm = 0.2924
Meta loss on this task batch = 3.0803e-01, Meta loss averaged over last 500 steps = 3.1200e-01, PNorm = 121.1703, GNorm = 0.2834
Meta loss on this task batch = 2.6484e-01, Meta loss averaged over last 500 steps = 3.1186e-01, PNorm = 121.1758, GNorm = 0.2405
Meta loss on this task batch = 2.7013e-01, Meta loss averaged over last 500 steps = 3.1180e-01, PNorm = 121.1830, GNorm = 0.3494
Meta loss on this task batch = 3.4242e-01, Meta loss averaged over last 500 steps = 3.1188e-01, PNorm = 121.1901, GNorm = 0.2589
Meta loss on this task batch = 2.9529e-01, Meta loss averaged over last 500 steps = 3.1182e-01, PNorm = 121.1974, GNorm = 0.3409
Meta loss on this task batch = 3.8573e-01, Meta loss averaged over last 500 steps = 3.1196e-01, PNorm = 121.2032, GNorm = 0.2808
Meta loss on this task batch = 3.3199e-01, Meta loss averaged over last 500 steps = 3.1189e-01, PNorm = 121.2083, GNorm = 0.3140
Meta loss on this task batch = 3.2599e-01, Meta loss averaged over last 500 steps = 3.1205e-01, PNorm = 121.2142, GNorm = 0.3785
Meta loss on this task batch = 3.5365e-01, Meta loss averaged over last 500 steps = 3.1215e-01, PNorm = 121.2191, GNorm = 0.3487
Meta loss on this task batch = 3.0468e-01, Meta loss averaged over last 500 steps = 3.1209e-01, PNorm = 121.2236, GNorm = 0.3128
Took 112.90891146659851 seconds to complete one epoch of meta training
Took 120.67662620544434 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.430532
Epoch 380
Meta loss on this task batch = 3.2561e-01, Meta loss averaged over last 500 steps = 3.1215e-01, PNorm = 121.2284, GNorm = 0.4770
Meta loss on this task batch = 3.1441e-01, Meta loss averaged over last 500 steps = 3.1217e-01, PNorm = 121.2329, GNorm = 0.3093
Meta loss on this task batch = 3.0014e-01, Meta loss averaged over last 500 steps = 3.1208e-01, PNorm = 121.2375, GNorm = 0.3040
Meta loss on this task batch = 3.2862e-01, Meta loss averaged over last 500 steps = 3.1221e-01, PNorm = 121.2415, GNorm = 0.3481
Meta loss on this task batch = 2.8849e-01, Meta loss averaged over last 500 steps = 3.1225e-01, PNorm = 121.2460, GNorm = 0.3158
Meta loss on this task batch = 3.2587e-01, Meta loss averaged over last 500 steps = 3.1217e-01, PNorm = 121.2507, GNorm = 0.3176
Meta loss on this task batch = 2.9415e-01, Meta loss averaged over last 500 steps = 3.1209e-01, PNorm = 121.2558, GNorm = 0.2478
Meta loss on this task batch = 3.6212e-01, Meta loss averaged over last 500 steps = 3.1214e-01, PNorm = 121.2601, GNorm = 0.2844
Meta loss on this task batch = 3.0046e-01, Meta loss averaged over last 500 steps = 3.1226e-01, PNorm = 121.2651, GNorm = 0.4014
Meta loss on this task batch = 3.2366e-01, Meta loss averaged over last 500 steps = 3.1223e-01, PNorm = 121.2705, GNorm = 0.3911
Meta loss on this task batch = 2.8477e-01, Meta loss averaged over last 500 steps = 3.1211e-01, PNorm = 121.2762, GNorm = 0.2420
Meta loss on this task batch = 2.8501e-01, Meta loss averaged over last 500 steps = 3.1202e-01, PNorm = 121.2820, GNorm = 0.2413
Meta loss on this task batch = 3.0590e-01, Meta loss averaged over last 500 steps = 3.1198e-01, PNorm = 121.2881, GNorm = 0.2929
Meta loss on this task batch = 2.8348e-01, Meta loss averaged over last 500 steps = 3.1197e-01, PNorm = 121.2945, GNorm = 0.2544
Meta loss on this task batch = 3.4604e-01, Meta loss averaged over last 500 steps = 3.1206e-01, PNorm = 121.3012, GNorm = 0.3378
Meta loss on this task batch = 3.2426e-01, Meta loss averaged over last 500 steps = 3.1210e-01, PNorm = 121.3079, GNorm = 0.2807
Meta loss on this task batch = 3.0839e-01, Meta loss averaged over last 500 steps = 3.1210e-01, PNorm = 121.3151, GNorm = 0.2670
Meta loss on this task batch = 3.1313e-01, Meta loss averaged over last 500 steps = 3.1192e-01, PNorm = 121.3227, GNorm = 0.5145
Meta loss on this task batch = 2.7064e-01, Meta loss averaged over last 500 steps = 3.1189e-01, PNorm = 121.3305, GNorm = 0.3424
Took 111.82701778411865 seconds to complete one epoch of meta training
Took 120.19079732894897 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451699
Epoch 381
Meta loss on this task batch = 3.1926e-01, Meta loss averaged over last 500 steps = 3.1180e-01, PNorm = 121.3389, GNorm = 0.3227
Meta loss on this task batch = 2.8822e-01, Meta loss averaged over last 500 steps = 3.1177e-01, PNorm = 121.3470, GNorm = 0.2634
Meta loss on this task batch = 3.4736e-01, Meta loss averaged over last 500 steps = 3.1189e-01, PNorm = 121.3538, GNorm = 0.3273
Meta loss on this task batch = 3.0379e-01, Meta loss averaged over last 500 steps = 3.1193e-01, PNorm = 121.3605, GNorm = 0.3385
Meta loss on this task batch = 3.5925e-01, Meta loss averaged over last 500 steps = 3.1193e-01, PNorm = 121.3671, GNorm = 0.3863
Meta loss on this task batch = 3.5697e-01, Meta loss averaged over last 500 steps = 3.1198e-01, PNorm = 121.3724, GNorm = 0.2627
Meta loss on this task batch = 3.5284e-01, Meta loss averaged over last 500 steps = 3.1212e-01, PNorm = 121.3770, GNorm = 0.3209
Meta loss on this task batch = 2.5479e-01, Meta loss averaged over last 500 steps = 3.1184e-01, PNorm = 121.3827, GNorm = 0.2839
Meta loss on this task batch = 2.5742e-01, Meta loss averaged over last 500 steps = 3.1183e-01, PNorm = 121.3881, GNorm = 0.2746
Meta loss on this task batch = 3.0503e-01, Meta loss averaged over last 500 steps = 3.1178e-01, PNorm = 121.3943, GNorm = 0.2713
Meta loss on this task batch = 2.6456e-01, Meta loss averaged over last 500 steps = 3.1160e-01, PNorm = 121.4017, GNorm = 0.3163
Meta loss on this task batch = 2.9292e-01, Meta loss averaged over last 500 steps = 3.1169e-01, PNorm = 121.4088, GNorm = 0.3556
Meta loss on this task batch = 3.7789e-01, Meta loss averaged over last 500 steps = 3.1193e-01, PNorm = 121.4145, GNorm = 0.3171
Meta loss on this task batch = 3.2527e-01, Meta loss averaged over last 500 steps = 3.1208e-01, PNorm = 121.4206, GNorm = 0.2836
Meta loss on this task batch = 3.3003e-01, Meta loss averaged over last 500 steps = 3.1221e-01, PNorm = 121.4265, GNorm = 0.3232
Meta loss on this task batch = 3.2838e-01, Meta loss averaged over last 500 steps = 3.1224e-01, PNorm = 121.4320, GNorm = 0.2624
Meta loss on this task batch = 3.0200e-01, Meta loss averaged over last 500 steps = 3.1219e-01, PNorm = 121.4377, GNorm = 0.2800
Meta loss on this task batch = 2.8601e-01, Meta loss averaged over last 500 steps = 3.1214e-01, PNorm = 121.4437, GNorm = 0.2611
Meta loss on this task batch = 3.1216e-01, Meta loss averaged over last 500 steps = 3.1205e-01, PNorm = 121.4506, GNorm = 0.3080
Took 109.2675347328186 seconds to complete one epoch of meta training
Took 117.12591361999512 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463486
Epoch 382
Meta loss on this task batch = 2.9039e-01, Meta loss averaged over last 500 steps = 3.1198e-01, PNorm = 121.4575, GNorm = 0.3577
Meta loss on this task batch = 2.8662e-01, Meta loss averaged over last 500 steps = 3.1201e-01, PNorm = 121.4652, GNorm = 0.4457
Meta loss on this task batch = 3.3654e-01, Meta loss averaged over last 500 steps = 3.1202e-01, PNorm = 121.4730, GNorm = 0.4670
Meta loss on this task batch = 2.7426e-01, Meta loss averaged over last 500 steps = 3.1189e-01, PNorm = 121.4808, GNorm = 0.3415
Meta loss on this task batch = 3.2536e-01, Meta loss averaged over last 500 steps = 3.1184e-01, PNorm = 121.4885, GNorm = 0.3044
Meta loss on this task batch = 3.3581e-01, Meta loss averaged over last 500 steps = 3.1185e-01, PNorm = 121.4957, GNorm = 0.5565
Meta loss on this task batch = 2.5330e-01, Meta loss averaged over last 500 steps = 3.1178e-01, PNorm = 121.5032, GNorm = 0.2653
Meta loss on this task batch = 3.8211e-01, Meta loss averaged over last 500 steps = 3.1194e-01, PNorm = 121.5094, GNorm = 0.3426
Meta loss on this task batch = 3.1998e-01, Meta loss averaged over last 500 steps = 3.1197e-01, PNorm = 121.5162, GNorm = 0.3883
Meta loss on this task batch = 3.1187e-01, Meta loss averaged over last 500 steps = 3.1198e-01, PNorm = 121.5228, GNorm = 0.3041
Meta loss on this task batch = 2.9652e-01, Meta loss averaged over last 500 steps = 3.1193e-01, PNorm = 121.5298, GNorm = 0.2988
Meta loss on this task batch = 2.7391e-01, Meta loss averaged over last 500 steps = 3.1189e-01, PNorm = 121.5370, GNorm = 0.2805
Meta loss on this task batch = 3.0845e-01, Meta loss averaged over last 500 steps = 3.1180e-01, PNorm = 121.5439, GNorm = 0.3850
Meta loss on this task batch = 2.6645e-01, Meta loss averaged over last 500 steps = 3.1180e-01, PNorm = 121.5512, GNorm = 0.2655
Meta loss on this task batch = 3.2519e-01, Meta loss averaged over last 500 steps = 3.1178e-01, PNorm = 121.5590, GNorm = 0.4162
Meta loss on this task batch = 3.5440e-01, Meta loss averaged over last 500 steps = 3.1180e-01, PNorm = 121.5673, GNorm = 0.3439
Meta loss on this task batch = 3.4134e-01, Meta loss averaged over last 500 steps = 3.1194e-01, PNorm = 121.5755, GNorm = 0.3475
Meta loss on this task batch = 3.7620e-01, Meta loss averaged over last 500 steps = 3.1203e-01, PNorm = 121.5822, GNorm = 0.4583
Meta loss on this task batch = 2.6068e-01, Meta loss averaged over last 500 steps = 3.1181e-01, PNorm = 121.5896, GNorm = 0.3318
Took 113.94202470779419 seconds to complete one epoch of meta training
Took 120.68653607368469 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481960
Epoch 383
Meta loss on this task batch = 2.5775e-01, Meta loss averaged over last 500 steps = 3.1182e-01, PNorm = 121.5971, GNorm = 0.2975
Meta loss on this task batch = 3.0653e-01, Meta loss averaged over last 500 steps = 3.1181e-01, PNorm = 121.6049, GNorm = 0.3338
Meta loss on this task batch = 3.2229e-01, Meta loss averaged over last 500 steps = 3.1182e-01, PNorm = 121.6119, GNorm = 0.3114
Meta loss on this task batch = 3.3204e-01, Meta loss averaged over last 500 steps = 3.1191e-01, PNorm = 121.6182, GNorm = 0.2871
Meta loss on this task batch = 2.7246e-01, Meta loss averaged over last 500 steps = 3.1184e-01, PNorm = 121.6231, GNorm = 0.3500
Meta loss on this task batch = 2.6041e-01, Meta loss averaged over last 500 steps = 3.1170e-01, PNorm = 121.6278, GNorm = 0.2633
Meta loss on this task batch = 3.6086e-01, Meta loss averaged over last 500 steps = 3.1183e-01, PNorm = 121.6320, GNorm = 0.3505
Meta loss on this task batch = 3.3244e-01, Meta loss averaged over last 500 steps = 3.1191e-01, PNorm = 121.6356, GNorm = 0.2815
Meta loss on this task batch = 2.9526e-01, Meta loss averaged over last 500 steps = 3.1177e-01, PNorm = 121.6395, GNorm = 0.2294
Meta loss on this task batch = 2.7217e-01, Meta loss averaged over last 500 steps = 3.1174e-01, PNorm = 121.6438, GNorm = 0.2828
Meta loss on this task batch = 3.5682e-01, Meta loss averaged over last 500 steps = 3.1178e-01, PNorm = 121.6477, GNorm = 0.3286
Meta loss on this task batch = 3.0245e-01, Meta loss averaged over last 500 steps = 3.1165e-01, PNorm = 121.6523, GNorm = 0.3158
Meta loss on this task batch = 2.9750e-01, Meta loss averaged over last 500 steps = 3.1170e-01, PNorm = 121.6581, GNorm = 0.3613
Meta loss on this task batch = 2.9459e-01, Meta loss averaged over last 500 steps = 3.1169e-01, PNorm = 121.6642, GNorm = 0.2608
Meta loss on this task batch = 3.0697e-01, Meta loss averaged over last 500 steps = 3.1172e-01, PNorm = 121.6704, GNorm = 0.3191
Meta loss on this task batch = 3.3930e-01, Meta loss averaged over last 500 steps = 3.1166e-01, PNorm = 121.6754, GNorm = 0.3221
Meta loss on this task batch = 3.2932e-01, Meta loss averaged over last 500 steps = 3.1169e-01, PNorm = 121.6816, GNorm = 0.3040
Meta loss on this task batch = 2.9879e-01, Meta loss averaged over last 500 steps = 3.1169e-01, PNorm = 121.6869, GNorm = 0.4485
Meta loss on this task batch = 3.4258e-01, Meta loss averaged over last 500 steps = 3.1171e-01, PNorm = 121.6930, GNorm = 0.4135
Took 112.25190377235413 seconds to complete one epoch of meta training
Took 120.35771536827087 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478530
Epoch 384
Meta loss on this task batch = 3.1695e-01, Meta loss averaged over last 500 steps = 3.1178e-01, PNorm = 121.6998, GNorm = 0.3040
Meta loss on this task batch = 2.7092e-01, Meta loss averaged over last 500 steps = 3.1165e-01, PNorm = 121.7076, GNorm = 0.2620
Meta loss on this task batch = 2.8874e-01, Meta loss averaged over last 500 steps = 3.1155e-01, PNorm = 121.7158, GNorm = 0.3252
Meta loss on this task batch = 3.4100e-01, Meta loss averaged over last 500 steps = 3.1159e-01, PNorm = 121.7239, GNorm = 0.2977
Meta loss on this task batch = 2.9725e-01, Meta loss averaged over last 500 steps = 3.1150e-01, PNorm = 121.7323, GNorm = 0.2781
Meta loss on this task batch = 3.5158e-01, Meta loss averaged over last 500 steps = 3.1164e-01, PNorm = 121.7398, GNorm = 0.3597
Meta loss on this task batch = 3.6015e-01, Meta loss averaged over last 500 steps = 3.1164e-01, PNorm = 121.7466, GNorm = 0.2939
Meta loss on this task batch = 2.7708e-01, Meta loss averaged over last 500 steps = 3.1152e-01, PNorm = 121.7535, GNorm = 0.2293
Meta loss on this task batch = 2.8492e-01, Meta loss averaged over last 500 steps = 3.1132e-01, PNorm = 121.7604, GNorm = 0.2442
Meta loss on this task batch = 3.4191e-01, Meta loss averaged over last 500 steps = 3.1145e-01, PNorm = 121.7670, GNorm = 0.3025
Meta loss on this task batch = 2.8607e-01, Meta loss averaged over last 500 steps = 3.1141e-01, PNorm = 121.7741, GNorm = 0.2514
Meta loss on this task batch = 2.7193e-01, Meta loss averaged over last 500 steps = 3.1135e-01, PNorm = 121.7818, GNorm = 0.2174
Meta loss on this task batch = 3.2345e-01, Meta loss averaged over last 500 steps = 3.1152e-01, PNorm = 121.7886, GNorm = 0.3069
Meta loss on this task batch = 3.0252e-01, Meta loss averaged over last 500 steps = 3.1150e-01, PNorm = 121.7950, GNorm = 0.2864
Meta loss on this task batch = 3.6865e-01, Meta loss averaged over last 500 steps = 3.1165e-01, PNorm = 121.8007, GNorm = 0.3229
Meta loss on this task batch = 3.1981e-01, Meta loss averaged over last 500 steps = 3.1165e-01, PNorm = 121.8063, GNorm = 0.3083
Meta loss on this task batch = 2.8996e-01, Meta loss averaged over last 500 steps = 3.1155e-01, PNorm = 121.8117, GNorm = 0.3742
Meta loss on this task batch = 2.9392e-01, Meta loss averaged over last 500 steps = 3.1156e-01, PNorm = 121.8178, GNorm = 0.2553
Meta loss on this task batch = 2.9134e-01, Meta loss averaged over last 500 steps = 3.1157e-01, PNorm = 121.8240, GNorm = 0.3678
Took 111.18110871315002 seconds to complete one epoch of meta training
Took 118.90624380111694 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448779
Epoch 385
Meta loss on this task batch = 3.6205e-01, Meta loss averaged over last 500 steps = 3.1163e-01, PNorm = 121.8300, GNorm = 0.3187
Meta loss on this task batch = 3.7733e-01, Meta loss averaged over last 500 steps = 3.1184e-01, PNorm = 121.8352, GNorm = 0.3442
Meta loss on this task batch = 2.7820e-01, Meta loss averaged over last 500 steps = 3.1166e-01, PNorm = 121.8414, GNorm = 0.2546
Meta loss on this task batch = 2.8782e-01, Meta loss averaged over last 500 steps = 3.1159e-01, PNorm = 121.8478, GNorm = 0.2418
Meta loss on this task batch = 2.5157e-01, Meta loss averaged over last 500 steps = 3.1140e-01, PNorm = 121.8548, GNorm = 0.2272
Meta loss on this task batch = 2.9853e-01, Meta loss averaged over last 500 steps = 3.1154e-01, PNorm = 121.8609, GNorm = 0.3090
Meta loss on this task batch = 2.9405e-01, Meta loss averaged over last 500 steps = 3.1150e-01, PNorm = 121.8665, GNorm = 0.2814
Meta loss on this task batch = 2.7174e-01, Meta loss averaged over last 500 steps = 3.1137e-01, PNorm = 121.8728, GNorm = 0.2873
Meta loss on this task batch = 3.1263e-01, Meta loss averaged over last 500 steps = 3.1136e-01, PNorm = 121.8792, GNorm = 0.3459
Meta loss on this task batch = 3.6560e-01, Meta loss averaged over last 500 steps = 3.1149e-01, PNorm = 121.8848, GNorm = 0.3220
Meta loss on this task batch = 3.2239e-01, Meta loss averaged over last 500 steps = 3.1143e-01, PNorm = 121.8903, GNorm = 0.3324
Meta loss on this task batch = 3.6411e-01, Meta loss averaged over last 500 steps = 3.1156e-01, PNorm = 121.8952, GNorm = 0.2904
Meta loss on this task batch = 3.1474e-01, Meta loss averaged over last 500 steps = 3.1153e-01, PNorm = 121.9002, GNorm = 0.2923
Meta loss on this task batch = 2.6315e-01, Meta loss averaged over last 500 steps = 3.1146e-01, PNorm = 121.9062, GNorm = 0.2963
Meta loss on this task batch = 2.6994e-01, Meta loss averaged over last 500 steps = 3.1143e-01, PNorm = 121.9131, GNorm = 0.3298
Meta loss on this task batch = 3.1300e-01, Meta loss averaged over last 500 steps = 3.1138e-01, PNorm = 121.9200, GNorm = 0.2802
Meta loss on this task batch = 3.7527e-01, Meta loss averaged over last 500 steps = 3.1149e-01, PNorm = 121.9256, GNorm = 0.3956
Meta loss on this task batch = 2.9678e-01, Meta loss averaged over last 500 steps = 3.1151e-01, PNorm = 121.9307, GNorm = 0.4127
Meta loss on this task batch = 3.7496e-01, Meta loss averaged over last 500 steps = 3.1161e-01, PNorm = 121.9363, GNorm = 0.4015
Took 111.85778474807739 seconds to complete one epoch of meta training
Took 119.59508204460144 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505461
Epoch 386
Meta loss on this task batch = 3.3135e-01, Meta loss averaged over last 500 steps = 3.1168e-01, PNorm = 121.9424, GNorm = 0.3008
Meta loss on this task batch = 3.8640e-01, Meta loss averaged over last 500 steps = 3.1181e-01, PNorm = 121.9480, GNorm = 0.4221
Meta loss on this task batch = 3.1712e-01, Meta loss averaged over last 500 steps = 3.1186e-01, PNorm = 121.9545, GNorm = 0.4495
Meta loss on this task batch = 2.7581e-01, Meta loss averaged over last 500 steps = 3.1179e-01, PNorm = 121.9612, GNorm = 0.3075
Meta loss on this task batch = 2.4414e-01, Meta loss averaged over last 500 steps = 3.1161e-01, PNorm = 121.9687, GNorm = 0.2324
Meta loss on this task batch = 3.1339e-01, Meta loss averaged over last 500 steps = 3.1170e-01, PNorm = 121.9760, GNorm = 0.2250
Meta loss on this task batch = 3.0284e-01, Meta loss averaged over last 500 steps = 3.1172e-01, PNorm = 121.9832, GNorm = 0.2474
Meta loss on this task batch = 3.2298e-01, Meta loss averaged over last 500 steps = 3.1182e-01, PNorm = 121.9894, GNorm = 0.2598
Meta loss on this task batch = 3.0611e-01, Meta loss averaged over last 500 steps = 3.1183e-01, PNorm = 121.9946, GNorm = 0.2749
Meta loss on this task batch = 3.4332e-01, Meta loss averaged over last 500 steps = 3.1187e-01, PNorm = 121.9991, GNorm = 0.3228
Meta loss on this task batch = 3.1054e-01, Meta loss averaged over last 500 steps = 3.1192e-01, PNorm = 122.0041, GNorm = 0.2662
Meta loss on this task batch = 2.6048e-01, Meta loss averaged over last 500 steps = 3.1168e-01, PNorm = 122.0098, GNorm = 0.2242
Meta loss on this task batch = 3.1148e-01, Meta loss averaged over last 500 steps = 3.1181e-01, PNorm = 122.0150, GNorm = 0.2755
Meta loss on this task batch = 2.8512e-01, Meta loss averaged over last 500 steps = 3.1166e-01, PNorm = 122.0208, GNorm = 0.3014
Meta loss on this task batch = 3.7360e-01, Meta loss averaged over last 500 steps = 3.1173e-01, PNorm = 122.0262, GNorm = 0.3122
Meta loss on this task batch = 2.7744e-01, Meta loss averaged over last 500 steps = 3.1174e-01, PNorm = 122.0317, GNorm = 0.2300
Meta loss on this task batch = 3.3697e-01, Meta loss averaged over last 500 steps = 3.1182e-01, PNorm = 122.0375, GNorm = 0.4102
Meta loss on this task batch = 3.0070e-01, Meta loss averaged over last 500 steps = 3.1178e-01, PNorm = 122.0437, GNorm = 0.3885
Meta loss on this task batch = 3.1782e-01, Meta loss averaged over last 500 steps = 3.1182e-01, PNorm = 122.0488, GNorm = 0.3492
Took 110.0931499004364 seconds to complete one epoch of meta training
Took 117.92205572128296 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451622
Epoch 387
Meta loss on this task batch = 3.0635e-01, Meta loss averaged over last 500 steps = 3.1185e-01, PNorm = 122.0539, GNorm = 0.2857
Meta loss on this task batch = 2.8190e-01, Meta loss averaged over last 500 steps = 3.1171e-01, PNorm = 122.0599, GNorm = 0.2912
Meta loss on this task batch = 3.1597e-01, Meta loss averaged over last 500 steps = 3.1175e-01, PNorm = 122.0660, GNorm = 0.3940
Meta loss on this task batch = 3.3531e-01, Meta loss averaged over last 500 steps = 3.1174e-01, PNorm = 122.0726, GNorm = 0.3222
Meta loss on this task batch = 2.8193e-01, Meta loss averaged over last 500 steps = 3.1171e-01, PNorm = 122.0788, GNorm = 0.2564
Meta loss on this task batch = 2.8615e-01, Meta loss averaged over last 500 steps = 3.1153e-01, PNorm = 122.0853, GNorm = 0.3369
Meta loss on this task batch = 2.9000e-01, Meta loss averaged over last 500 steps = 3.1150e-01, PNorm = 122.0915, GNorm = 0.2556
Meta loss on this task batch = 3.0914e-01, Meta loss averaged over last 500 steps = 3.1151e-01, PNorm = 122.0983, GNorm = 0.4460
Meta loss on this task batch = 3.3409e-01, Meta loss averaged over last 500 steps = 3.1159e-01, PNorm = 122.1049, GNorm = 0.4166
Meta loss on this task batch = 2.5004e-01, Meta loss averaged over last 500 steps = 3.1140e-01, PNorm = 122.1118, GNorm = 0.2388
Meta loss on this task batch = 2.7113e-01, Meta loss averaged over last 500 steps = 3.1111e-01, PNorm = 122.1192, GNorm = 0.2503
Meta loss on this task batch = 3.2427e-01, Meta loss averaged over last 500 steps = 3.1111e-01, PNorm = 122.1259, GNorm = 0.2671
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 3.1113e-01, PNorm = 122.1330, GNorm = 0.3528
Meta loss on this task batch = 3.3021e-01, Meta loss averaged over last 500 steps = 3.1137e-01, PNorm = 122.1400, GNorm = 0.3930
Meta loss on this task batch = 2.6160e-01, Meta loss averaged over last 500 steps = 3.1135e-01, PNorm = 122.1466, GNorm = 0.3349
Meta loss on this task batch = 3.3127e-01, Meta loss averaged over last 500 steps = 3.1138e-01, PNorm = 122.1530, GNorm = 0.3226
Meta loss on this task batch = 2.7092e-01, Meta loss averaged over last 500 steps = 3.1132e-01, PNorm = 122.1594, GNorm = 0.2582
Meta loss on this task batch = 2.8958e-01, Meta loss averaged over last 500 steps = 3.1129e-01, PNorm = 122.1664, GNorm = 0.3142
Meta loss on this task batch = 3.5886e-01, Meta loss averaged over last 500 steps = 3.1140e-01, PNorm = 122.1734, GNorm = 0.3726
Took 112.72749614715576 seconds to complete one epoch of meta training
Took 120.61285519599915 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480096
Epoch 388
Meta loss on this task batch = 2.7269e-01, Meta loss averaged over last 500 steps = 3.1132e-01, PNorm = 122.1810, GNorm = 0.2468
Meta loss on this task batch = 2.5234e-01, Meta loss averaged over last 500 steps = 3.1117e-01, PNorm = 122.1886, GNorm = 0.2524
Meta loss on this task batch = 2.8338e-01, Meta loss averaged over last 500 steps = 3.1112e-01, PNorm = 122.1953, GNorm = 0.2567
Meta loss on this task batch = 2.9016e-01, Meta loss averaged over last 500 steps = 3.1098e-01, PNorm = 122.2018, GNorm = 0.2401
Meta loss on this task batch = 3.4270e-01, Meta loss averaged over last 500 steps = 3.1103e-01, PNorm = 122.2080, GNorm = 0.3132
Meta loss on this task batch = 3.9872e-01, Meta loss averaged over last 500 steps = 3.1112e-01, PNorm = 122.2127, GNorm = 0.3294
Meta loss on this task batch = 2.6595e-01, Meta loss averaged over last 500 steps = 3.1100e-01, PNorm = 122.2178, GNorm = 0.2508
Meta loss on this task batch = 3.3876e-01, Meta loss averaged over last 500 steps = 3.1105e-01, PNorm = 122.2235, GNorm = 0.4273
Meta loss on this task batch = 3.0588e-01, Meta loss averaged over last 500 steps = 3.1098e-01, PNorm = 122.2292, GNorm = 0.2484
Meta loss on this task batch = 3.0300e-01, Meta loss averaged over last 500 steps = 3.1096e-01, PNorm = 122.2347, GNorm = 0.2500
Meta loss on this task batch = 3.4043e-01, Meta loss averaged over last 500 steps = 3.1104e-01, PNorm = 122.2404, GNorm = 0.3104
Meta loss on this task batch = 2.5743e-01, Meta loss averaged over last 500 steps = 3.1100e-01, PNorm = 122.2465, GNorm = 0.2970
Meta loss on this task batch = 3.6325e-01, Meta loss averaged over last 500 steps = 3.1107e-01, PNorm = 122.2534, GNorm = 0.2905
Meta loss on this task batch = 3.0277e-01, Meta loss averaged over last 500 steps = 3.1113e-01, PNorm = 122.2602, GNorm = 0.2517
Meta loss on this task batch = 3.2800e-01, Meta loss averaged over last 500 steps = 3.1112e-01, PNorm = 122.2664, GNorm = 0.3407
Meta loss on this task batch = 2.9239e-01, Meta loss averaged over last 500 steps = 3.1113e-01, PNorm = 122.2713, GNorm = 0.2716
Meta loss on this task batch = 2.9500e-01, Meta loss averaged over last 500 steps = 3.1103e-01, PNorm = 122.2758, GNorm = 0.2699
Meta loss on this task batch = 2.7241e-01, Meta loss averaged over last 500 steps = 3.1084e-01, PNorm = 122.2811, GNorm = 0.2830
Meta loss on this task batch = 3.6253e-01, Meta loss averaged over last 500 steps = 3.1101e-01, PNorm = 122.2869, GNorm = 0.4175
Took 109.53412508964539 seconds to complete one epoch of meta training
Took 117.41978764533997 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454867
Epoch 389
Meta loss on this task batch = 3.3747e-01, Meta loss averaged over last 500 steps = 3.1091e-01, PNorm = 122.2933, GNorm = 0.2710
Meta loss on this task batch = 3.3987e-01, Meta loss averaged over last 500 steps = 3.1111e-01, PNorm = 122.2995, GNorm = 0.4613
Meta loss on this task batch = 2.7529e-01, Meta loss averaged over last 500 steps = 3.1105e-01, PNorm = 122.3049, GNorm = 0.2509
Meta loss on this task batch = 3.0975e-01, Meta loss averaged over last 500 steps = 3.1098e-01, PNorm = 122.3105, GNorm = 0.2922
Meta loss on this task batch = 2.6211e-01, Meta loss averaged over last 500 steps = 3.1086e-01, PNorm = 122.3168, GNorm = 0.3106
Meta loss on this task batch = 2.9933e-01, Meta loss averaged over last 500 steps = 3.1090e-01, PNorm = 122.3230, GNorm = 0.4085
Meta loss on this task batch = 2.6206e-01, Meta loss averaged over last 500 steps = 3.1079e-01, PNorm = 122.3297, GNorm = 0.2649
Meta loss on this task batch = 3.3079e-01, Meta loss averaged over last 500 steps = 3.1081e-01, PNorm = 122.3364, GNorm = 0.2905
Meta loss on this task batch = 3.0607e-01, Meta loss averaged over last 500 steps = 3.1086e-01, PNorm = 122.3427, GNorm = 0.3450
Meta loss on this task batch = 3.0593e-01, Meta loss averaged over last 500 steps = 3.1087e-01, PNorm = 122.3495, GNorm = 0.2600
Meta loss on this task batch = 2.7322e-01, Meta loss averaged over last 500 steps = 3.1069e-01, PNorm = 122.3560, GNorm = 0.2481
Meta loss on this task batch = 3.7066e-01, Meta loss averaged over last 500 steps = 3.1085e-01, PNorm = 122.3621, GNorm = 0.3933
Meta loss on this task batch = 3.5945e-01, Meta loss averaged over last 500 steps = 3.1096e-01, PNorm = 122.3682, GNorm = 0.2850
Meta loss on this task batch = 3.0421e-01, Meta loss averaged over last 500 steps = 3.1091e-01, PNorm = 122.3746, GNorm = 0.5493
Meta loss on this task batch = 3.1026e-01, Meta loss averaged over last 500 steps = 3.1097e-01, PNorm = 122.3810, GNorm = 0.3263
Meta loss on this task batch = 2.8605e-01, Meta loss averaged over last 500 steps = 3.1092e-01, PNorm = 122.3870, GNorm = 0.2846
Meta loss on this task batch = 3.0490e-01, Meta loss averaged over last 500 steps = 3.1084e-01, PNorm = 122.3928, GNorm = 0.2449
Meta loss on this task batch = 3.4376e-01, Meta loss averaged over last 500 steps = 3.1088e-01, PNorm = 122.3981, GNorm = 0.2962
Meta loss on this task batch = 3.7512e-01, Meta loss averaged over last 500 steps = 3.1111e-01, PNorm = 122.4037, GNorm = 0.3611
Took 112.09184622764587 seconds to complete one epoch of meta training
Took 119.9714138507843 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454975
Epoch 390
Meta loss on this task batch = 3.0672e-01, Meta loss averaged over last 500 steps = 3.1107e-01, PNorm = 122.4102, GNorm = 0.2479
Meta loss on this task batch = 3.1671e-01, Meta loss averaged over last 500 steps = 3.1105e-01, PNorm = 122.4166, GNorm = 0.3003
Meta loss on this task batch = 3.3721e-01, Meta loss averaged over last 500 steps = 3.1110e-01, PNorm = 122.4238, GNorm = 0.2979
Meta loss on this task batch = 2.5978e-01, Meta loss averaged over last 500 steps = 3.1102e-01, PNorm = 122.4323, GNorm = 0.2401
Meta loss on this task batch = 2.9482e-01, Meta loss averaged over last 500 steps = 3.1096e-01, PNorm = 122.4415, GNorm = 0.3227
Meta loss on this task batch = 3.0598e-01, Meta loss averaged over last 500 steps = 3.1105e-01, PNorm = 122.4504, GNorm = 0.2462
Meta loss on this task batch = 2.6916e-01, Meta loss averaged over last 500 steps = 3.1101e-01, PNorm = 122.4592, GNorm = 0.2442
Meta loss on this task batch = 2.8846e-01, Meta loss averaged over last 500 steps = 3.1099e-01, PNorm = 122.4678, GNorm = 0.2377
Meta loss on this task batch = 2.8980e-01, Meta loss averaged over last 500 steps = 3.1079e-01, PNorm = 122.4755, GNorm = 0.2595
Meta loss on this task batch = 2.6291e-01, Meta loss averaged over last 500 steps = 3.1074e-01, PNorm = 122.4825, GNorm = 0.2700
Meta loss on this task batch = 3.4799e-01, Meta loss averaged over last 500 steps = 3.1086e-01, PNorm = 122.4887, GNorm = 0.2973
Meta loss on this task batch = 2.7722e-01, Meta loss averaged over last 500 steps = 3.1080e-01, PNorm = 122.4940, GNorm = 0.2498
Meta loss on this task batch = 4.0150e-01, Meta loss averaged over last 500 steps = 3.1100e-01, PNorm = 122.4978, GNorm = 0.4021
Meta loss on this task batch = 3.1782e-01, Meta loss averaged over last 500 steps = 3.1104e-01, PNorm = 122.5017, GNorm = 0.2800
Meta loss on this task batch = 3.0030e-01, Meta loss averaged over last 500 steps = 3.1098e-01, PNorm = 122.5055, GNorm = 0.4602
Meta loss on this task batch = 2.9364e-01, Meta loss averaged over last 500 steps = 3.1097e-01, PNorm = 122.5091, GNorm = 0.3871
Meta loss on this task batch = 3.1518e-01, Meta loss averaged over last 500 steps = 3.1099e-01, PNorm = 122.5131, GNorm = 0.3179
Meta loss on this task batch = 3.4666e-01, Meta loss averaged over last 500 steps = 3.1111e-01, PNorm = 122.5174, GNorm = 0.2832
Meta loss on this task batch = 3.1113e-01, Meta loss averaged over last 500 steps = 3.1105e-01, PNorm = 122.5220, GNorm = 0.3378
Took 112.21693778038025 seconds to complete one epoch of meta training
Took 120.02856492996216 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481874
Epoch 391
Meta loss on this task batch = 3.2609e-01, Meta loss averaged over last 500 steps = 3.1101e-01, PNorm = 122.5261, GNorm = 0.3006
Meta loss on this task batch = 2.8188e-01, Meta loss averaged over last 500 steps = 3.1096e-01, PNorm = 122.5308, GNorm = 0.2434
Meta loss on this task batch = 3.3954e-01, Meta loss averaged over last 500 steps = 3.1104e-01, PNorm = 122.5353, GNorm = 0.4054
Meta loss on this task batch = 3.6481e-01, Meta loss averaged over last 500 steps = 3.1119e-01, PNorm = 122.5404, GNorm = 0.2769
Meta loss on this task batch = 2.8132e-01, Meta loss averaged over last 500 steps = 3.1100e-01, PNorm = 122.5463, GNorm = 0.2659
Meta loss on this task batch = 2.4312e-01, Meta loss averaged over last 500 steps = 3.1095e-01, PNorm = 122.5536, GNorm = 0.2913
Meta loss on this task batch = 2.9253e-01, Meta loss averaged over last 500 steps = 3.1098e-01, PNorm = 122.5613, GNorm = 0.3298
Meta loss on this task batch = 2.6743e-01, Meta loss averaged over last 500 steps = 3.1085e-01, PNorm = 122.5693, GNorm = 0.3409
Meta loss on this task batch = 3.4908e-01, Meta loss averaged over last 500 steps = 3.1078e-01, PNorm = 122.5771, GNorm = 0.3714
Meta loss on this task batch = 2.8556e-01, Meta loss averaged over last 500 steps = 3.1086e-01, PNorm = 122.5850, GNorm = 0.3803
Meta loss on this task batch = 3.5091e-01, Meta loss averaged over last 500 steps = 3.1091e-01, PNorm = 122.5924, GNorm = 0.3640
Meta loss on this task batch = 3.2671e-01, Meta loss averaged over last 500 steps = 3.1092e-01, PNorm = 122.5993, GNorm = 0.2609
Meta loss on this task batch = 2.8229e-01, Meta loss averaged over last 500 steps = 3.1098e-01, PNorm = 122.6058, GNorm = 0.2891
Meta loss on this task batch = 2.9615e-01, Meta loss averaged over last 500 steps = 3.1102e-01, PNorm = 122.6104, GNorm = 0.5928
Meta loss on this task batch = 2.6702e-01, Meta loss averaged over last 500 steps = 3.1079e-01, PNorm = 122.6149, GNorm = 0.2804
Meta loss on this task batch = 3.6381e-01, Meta loss averaged over last 500 steps = 3.1082e-01, PNorm = 122.6181, GNorm = 0.3207
Meta loss on this task batch = 3.4392e-01, Meta loss averaged over last 500 steps = 3.1084e-01, PNorm = 122.6210, GNorm = 0.2912
Meta loss on this task batch = 3.1766e-01, Meta loss averaged over last 500 steps = 3.1088e-01, PNorm = 122.6237, GNorm = 0.3111
Meta loss on this task batch = 2.9691e-01, Meta loss averaged over last 500 steps = 3.1073e-01, PNorm = 122.6268, GNorm = 0.3328
Took 111.0138578414917 seconds to complete one epoch of meta training
Took 118.82997465133667 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487458
Epoch 392
Meta loss on this task batch = 3.4490e-01, Meta loss averaged over last 500 steps = 3.1086e-01, PNorm = 122.6301, GNorm = 0.2771
Meta loss on this task batch = 3.1239e-01, Meta loss averaged over last 500 steps = 3.1075e-01, PNorm = 122.6337, GNorm = 0.2594
Meta loss on this task batch = 2.9633e-01, Meta loss averaged over last 500 steps = 3.1075e-01, PNorm = 122.6368, GNorm = 0.2487
Meta loss on this task batch = 2.8346e-01, Meta loss averaged over last 500 steps = 3.1071e-01, PNorm = 122.6402, GNorm = 0.2504
Meta loss on this task batch = 2.8327e-01, Meta loss averaged over last 500 steps = 3.1067e-01, PNorm = 122.6438, GNorm = 0.2550
Meta loss on this task batch = 2.8474e-01, Meta loss averaged over last 500 steps = 3.1068e-01, PNorm = 122.6491, GNorm = 0.2477
Meta loss on this task batch = 3.7986e-01, Meta loss averaged over last 500 steps = 3.1087e-01, PNorm = 122.6549, GNorm = 0.2376
Meta loss on this task batch = 3.6063e-01, Meta loss averaged over last 500 steps = 3.1098e-01, PNorm = 122.6603, GNorm = 0.3066
Meta loss on this task batch = 3.3312e-01, Meta loss averaged over last 500 steps = 3.1093e-01, PNorm = 122.6657, GNorm = 0.2667
Meta loss on this task batch = 3.3855e-01, Meta loss averaged over last 500 steps = 3.1101e-01, PNorm = 122.6721, GNorm = 0.2939
Meta loss on this task batch = 3.3965e-01, Meta loss averaged over last 500 steps = 3.1112e-01, PNorm = 122.6780, GNorm = 0.3523
Meta loss on this task batch = 2.9989e-01, Meta loss averaged over last 500 steps = 3.1121e-01, PNorm = 122.6845, GNorm = 0.2457
Meta loss on this task batch = 2.7241e-01, Meta loss averaged over last 500 steps = 3.1102e-01, PNorm = 122.6905, GNorm = 0.2359
Meta loss on this task batch = 2.8425e-01, Meta loss averaged over last 500 steps = 3.1109e-01, PNorm = 122.6979, GNorm = 0.2691
Meta loss on this task batch = 3.1401e-01, Meta loss averaged over last 500 steps = 3.1111e-01, PNorm = 122.7046, GNorm = 0.2633
Meta loss on this task batch = 3.3919e-01, Meta loss averaged over last 500 steps = 3.1114e-01, PNorm = 122.7112, GNorm = 0.3528
Meta loss on this task batch = 2.9092e-01, Meta loss averaged over last 500 steps = 3.1119e-01, PNorm = 122.7185, GNorm = 0.4096
Meta loss on this task batch = 3.0034e-01, Meta loss averaged over last 500 steps = 3.1107e-01, PNorm = 122.7252, GNorm = 0.2806
Meta loss on this task batch = 3.0957e-01, Meta loss averaged over last 500 steps = 3.1107e-01, PNorm = 122.7310, GNorm = 0.3985
Took 110.30765724182129 seconds to complete one epoch of meta training
Took 117.99009704589844 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487433
Epoch 393
Meta loss on this task batch = 3.0067e-01, Meta loss averaged over last 500 steps = 3.1089e-01, PNorm = 122.7365, GNorm = 0.2945
Meta loss on this task batch = 2.9443e-01, Meta loss averaged over last 500 steps = 3.1089e-01, PNorm = 122.7417, GNorm = 0.2904
Meta loss on this task batch = 3.0259e-01, Meta loss averaged over last 500 steps = 3.1091e-01, PNorm = 122.7474, GNorm = 0.2200
Meta loss on this task batch = 3.2662e-01, Meta loss averaged over last 500 steps = 3.1096e-01, PNorm = 122.7530, GNorm = 0.2725
Meta loss on this task batch = 3.1625e-01, Meta loss averaged over last 500 steps = 3.1095e-01, PNorm = 122.7589, GNorm = 0.2490
Meta loss on this task batch = 2.7616e-01, Meta loss averaged over last 500 steps = 3.1091e-01, PNorm = 122.7654, GNorm = 0.2597
Meta loss on this task batch = 2.7651e-01, Meta loss averaged over last 500 steps = 3.1082e-01, PNorm = 122.7717, GNorm = 0.3032
Meta loss on this task batch = 3.4780e-01, Meta loss averaged over last 500 steps = 3.1092e-01, PNorm = 122.7779, GNorm = 0.3409
Meta loss on this task batch = 3.3124e-01, Meta loss averaged over last 500 steps = 3.1107e-01, PNorm = 122.7836, GNorm = 0.2968
Meta loss on this task batch = 3.1400e-01, Meta loss averaged over last 500 steps = 3.1104e-01, PNorm = 122.7890, GNorm = 0.3516
Meta loss on this task batch = 2.9881e-01, Meta loss averaged over last 500 steps = 3.1091e-01, PNorm = 122.7952, GNorm = 0.3612
Meta loss on this task batch = 2.6594e-01, Meta loss averaged over last 500 steps = 3.1092e-01, PNorm = 122.8008, GNorm = 0.2898
Meta loss on this task batch = 3.0748e-01, Meta loss averaged over last 500 steps = 3.1089e-01, PNorm = 122.8063, GNorm = 0.2555
Meta loss on this task batch = 2.2845e-01, Meta loss averaged over last 500 steps = 3.1079e-01, PNorm = 122.8126, GNorm = 0.3000
Meta loss on this task batch = 3.0474e-01, Meta loss averaged over last 500 steps = 3.1082e-01, PNorm = 122.8183, GNorm = 0.2730
Meta loss on this task batch = 3.2322e-01, Meta loss averaged over last 500 steps = 3.1075e-01, PNorm = 122.8233, GNorm = 0.3547
Meta loss on this task batch = 3.5465e-01, Meta loss averaged over last 500 steps = 3.1076e-01, PNorm = 122.8277, GNorm = 0.3963
Meta loss on this task batch = 3.2831e-01, Meta loss averaged over last 500 steps = 3.1077e-01, PNorm = 122.8320, GNorm = 0.5399
Meta loss on this task batch = 2.4965e-01, Meta loss averaged over last 500 steps = 3.1065e-01, PNorm = 122.8367, GNorm = 0.3780
Took 111.25953006744385 seconds to complete one epoch of meta training
Took 119.18290305137634 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492872
Epoch 394
Meta loss on this task batch = 3.5607e-01, Meta loss averaged over last 500 steps = 3.1078e-01, PNorm = 122.8414, GNorm = 0.3722
Meta loss on this task batch = 2.5857e-01, Meta loss averaged over last 500 steps = 3.1062e-01, PNorm = 122.8470, GNorm = 0.2821
Meta loss on this task batch = 2.8650e-01, Meta loss averaged over last 500 steps = 3.1052e-01, PNorm = 122.8542, GNorm = 0.3119
Meta loss on this task batch = 2.6908e-01, Meta loss averaged over last 500 steps = 3.1043e-01, PNorm = 122.8613, GNorm = 0.2735
Meta loss on this task batch = 3.1947e-01, Meta loss averaged over last 500 steps = 3.1051e-01, PNorm = 122.8684, GNorm = 0.5059
Meta loss on this task batch = 4.1522e-01, Meta loss averaged over last 500 steps = 3.1076e-01, PNorm = 122.8737, GNorm = 0.5034
Meta loss on this task batch = 3.0035e-01, Meta loss averaged over last 500 steps = 3.1069e-01, PNorm = 122.8790, GNorm = 0.3205
Meta loss on this task batch = 2.8199e-01, Meta loss averaged over last 500 steps = 3.1055e-01, PNorm = 122.8850, GNorm = 0.2615
Meta loss on this task batch = 2.9800e-01, Meta loss averaged over last 500 steps = 3.1047e-01, PNorm = 122.8910, GNorm = 0.2496
Meta loss on this task batch = 2.9232e-01, Meta loss averaged over last 500 steps = 3.1033e-01, PNorm = 122.8975, GNorm = 0.2693
Meta loss on this task batch = 3.2128e-01, Meta loss averaged over last 500 steps = 3.1037e-01, PNorm = 122.9030, GNorm = 0.3052
Meta loss on this task batch = 2.2847e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 122.9085, GNorm = 0.3758
Meta loss on this task batch = 3.2771e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 122.9137, GNorm = 0.2919
Meta loss on this task batch = 3.5624e-01, Meta loss averaged over last 500 steps = 3.1022e-01, PNorm = 122.9183, GNorm = 0.3117
Meta loss on this task batch = 2.8833e-01, Meta loss averaged over last 500 steps = 3.1016e-01, PNorm = 122.9228, GNorm = 0.3523
Meta loss on this task batch = 3.0600e-01, Meta loss averaged over last 500 steps = 3.1013e-01, PNorm = 122.9285, GNorm = 0.3688
Meta loss on this task batch = 3.1756e-01, Meta loss averaged over last 500 steps = 3.1016e-01, PNorm = 122.9343, GNorm = 0.3133
Meta loss on this task batch = 3.3598e-01, Meta loss averaged over last 500 steps = 3.1013e-01, PNorm = 122.9415, GNorm = 0.4180
Meta loss on this task batch = 3.1265e-01, Meta loss averaged over last 500 steps = 3.1031e-01, PNorm = 122.9490, GNorm = 0.4084
Took 111.06607103347778 seconds to complete one epoch of meta training
Took 118.85822057723999 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485708
Epoch 395
Meta loss on this task batch = 2.5494e-01, Meta loss averaged over last 500 steps = 3.1021e-01, PNorm = 122.9568, GNorm = 0.2892
Meta loss on this task batch = 2.8937e-01, Meta loss averaged over last 500 steps = 3.1024e-01, PNorm = 122.9654, GNorm = 0.3203
Meta loss on this task batch = 3.2435e-01, Meta loss averaged over last 500 steps = 3.1025e-01, PNorm = 122.9734, GNorm = 0.3814
Meta loss on this task batch = 2.9183e-01, Meta loss averaged over last 500 steps = 3.1023e-01, PNorm = 122.9818, GNorm = 0.2299
Meta loss on this task batch = 2.9529e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 122.9900, GNorm = 0.2963
Meta loss on this task batch = 3.3158e-01, Meta loss averaged over last 500 steps = 3.1023e-01, PNorm = 122.9971, GNorm = 0.3082
Meta loss on this task batch = 3.0010e-01, Meta loss averaged over last 500 steps = 3.1023e-01, PNorm = 123.0036, GNorm = 0.2585
Meta loss on this task batch = 3.7387e-01, Meta loss averaged over last 500 steps = 3.1031e-01, PNorm = 123.0104, GNorm = 0.4184
Meta loss on this task batch = 3.1397e-01, Meta loss averaged over last 500 steps = 3.1037e-01, PNorm = 123.0173, GNorm = 0.2332
Meta loss on this task batch = 2.7677e-01, Meta loss averaged over last 500 steps = 3.1022e-01, PNorm = 123.0244, GNorm = 0.2542
Meta loss on this task batch = 3.0270e-01, Meta loss averaged over last 500 steps = 3.1020e-01, PNorm = 123.0307, GNorm = 0.2519
Meta loss on this task batch = 3.3432e-01, Meta loss averaged over last 500 steps = 3.1028e-01, PNorm = 123.0366, GNorm = 0.3076
Meta loss on this task batch = 3.2474e-01, Meta loss averaged over last 500 steps = 3.1036e-01, PNorm = 123.0421, GNorm = 0.3000
Meta loss on this task batch = 3.0647e-01, Meta loss averaged over last 500 steps = 3.1035e-01, PNorm = 123.0478, GNorm = 0.3442
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 3.1030e-01, PNorm = 123.0536, GNorm = 0.4023
Meta loss on this task batch = 3.6590e-01, Meta loss averaged over last 500 steps = 3.1033e-01, PNorm = 123.0597, GNorm = 0.3150
Meta loss on this task batch = 2.8180e-01, Meta loss averaged over last 500 steps = 3.1039e-01, PNorm = 123.0651, GNorm = 0.2827
Meta loss on this task batch = 2.8870e-01, Meta loss averaged over last 500 steps = 3.1039e-01, PNorm = 123.0712, GNorm = 0.3259
Meta loss on this task batch = 2.8242e-01, Meta loss averaged over last 500 steps = 3.1034e-01, PNorm = 123.0780, GNorm = 0.5348
Took 111.45037841796875 seconds to complete one epoch of meta training
Took 119.66750502586365 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481643
Epoch 396
Meta loss on this task batch = 3.2935e-01, Meta loss averaged over last 500 steps = 3.1035e-01, PNorm = 123.0848, GNorm = 0.3071
Meta loss on this task batch = 3.1520e-01, Meta loss averaged over last 500 steps = 3.1026e-01, PNorm = 123.0913, GNorm = 0.3325
Meta loss on this task batch = 3.2125e-01, Meta loss averaged over last 500 steps = 3.1025e-01, PNorm = 123.0971, GNorm = 0.2936
Meta loss on this task batch = 3.4104e-01, Meta loss averaged over last 500 steps = 3.1021e-01, PNorm = 123.1034, GNorm = 0.3102
Meta loss on this task batch = 3.0060e-01, Meta loss averaged over last 500 steps = 3.1021e-01, PNorm = 123.1101, GNorm = 0.2849
Meta loss on this task batch = 3.2434e-01, Meta loss averaged over last 500 steps = 3.1031e-01, PNorm = 123.1169, GNorm = 0.2695
Meta loss on this task batch = 2.9474e-01, Meta loss averaged over last 500 steps = 3.1023e-01, PNorm = 123.1237, GNorm = 0.2915
Meta loss on this task batch = 3.2262e-01, Meta loss averaged over last 500 steps = 3.1036e-01, PNorm = 123.1297, GNorm = 0.3407
Meta loss on this task batch = 3.1654e-01, Meta loss averaged over last 500 steps = 3.1034e-01, PNorm = 123.1357, GNorm = 0.3588
Meta loss on this task batch = 2.7502e-01, Meta loss averaged over last 500 steps = 3.1029e-01, PNorm = 123.1421, GNorm = 0.2288
Meta loss on this task batch = 2.4745e-01, Meta loss averaged over last 500 steps = 3.1009e-01, PNorm = 123.1482, GNorm = 0.4074
Meta loss on this task batch = 2.7822e-01, Meta loss averaged over last 500 steps = 3.1005e-01, PNorm = 123.1543, GNorm = 0.3035
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 3.1002e-01, PNorm = 123.1601, GNorm = 0.4147
Meta loss on this task batch = 3.1122e-01, Meta loss averaged over last 500 steps = 3.0998e-01, PNorm = 123.1653, GNorm = 0.3547
Meta loss on this task batch = 2.6296e-01, Meta loss averaged over last 500 steps = 3.0978e-01, PNorm = 123.1716, GNorm = 0.2971
Meta loss on this task batch = 3.0589e-01, Meta loss averaged over last 500 steps = 3.0986e-01, PNorm = 123.1777, GNorm = 0.2682
Meta loss on this task batch = 3.9977e-01, Meta loss averaged over last 500 steps = 3.1018e-01, PNorm = 123.1826, GNorm = 0.3602
Meta loss on this task batch = 2.7468e-01, Meta loss averaged over last 500 steps = 3.1013e-01, PNorm = 123.1869, GNorm = 0.3124
Meta loss on this task batch = 3.1068e-01, Meta loss averaged over last 500 steps = 3.0999e-01, PNorm = 123.1903, GNorm = 0.3337
Took 109.78620648384094 seconds to complete one epoch of meta training
Took 117.30032134056091 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485805
Epoch 397
Meta loss on this task batch = 3.1725e-01, Meta loss averaged over last 500 steps = 3.1003e-01, PNorm = 123.1944, GNorm = 0.2976
Meta loss on this task batch = 2.9972e-01, Meta loss averaged over last 500 steps = 3.0998e-01, PNorm = 123.1990, GNorm = 0.3070
Meta loss on this task batch = 2.7922e-01, Meta loss averaged over last 500 steps = 3.1003e-01, PNorm = 123.2052, GNorm = 0.3843
Meta loss on this task batch = 2.8274e-01, Meta loss averaged over last 500 steps = 3.1004e-01, PNorm = 123.2105, GNorm = 0.2755
Meta loss on this task batch = 3.2739e-01, Meta loss averaged over last 500 steps = 3.0997e-01, PNorm = 123.2154, GNorm = 0.3319
Meta loss on this task batch = 3.2720e-01, Meta loss averaged over last 500 steps = 3.1007e-01, PNorm = 123.2220, GNorm = 0.3343
Meta loss on this task batch = 2.5774e-01, Meta loss averaged over last 500 steps = 3.0990e-01, PNorm = 123.2295, GNorm = 0.2972
Meta loss on this task batch = 3.7220e-01, Meta loss averaged over last 500 steps = 3.1005e-01, PNorm = 123.2363, GNorm = 0.2989
Meta loss on this task batch = 3.1720e-01, Meta loss averaged over last 500 steps = 3.1012e-01, PNorm = 123.2425, GNorm = 0.2911
Meta loss on this task batch = 3.0011e-01, Meta loss averaged over last 500 steps = 3.1016e-01, PNorm = 123.2486, GNorm = 0.2790
Meta loss on this task batch = 3.7893e-01, Meta loss averaged over last 500 steps = 3.1031e-01, PNorm = 123.2541, GNorm = 0.2679
Meta loss on this task batch = 2.6198e-01, Meta loss averaged over last 500 steps = 3.1011e-01, PNorm = 123.2599, GNorm = 0.2240
Meta loss on this task batch = 3.0160e-01, Meta loss averaged over last 500 steps = 3.1015e-01, PNorm = 123.2667, GNorm = 0.2951
Meta loss on this task batch = 3.6251e-01, Meta loss averaged over last 500 steps = 3.1027e-01, PNorm = 123.2733, GNorm = 0.3129
Meta loss on this task batch = 2.7971e-01, Meta loss averaged over last 500 steps = 3.1028e-01, PNorm = 123.2805, GNorm = 0.2222
Meta loss on this task batch = 3.4590e-01, Meta loss averaged over last 500 steps = 3.1036e-01, PNorm = 123.2877, GNorm = 0.2746
Meta loss on this task batch = 3.0155e-01, Meta loss averaged over last 500 steps = 3.1031e-01, PNorm = 123.2948, GNorm = 0.2668
Meta loss on this task batch = 3.1617e-01, Meta loss averaged over last 500 steps = 3.1020e-01, PNorm = 123.3024, GNorm = 0.2594
Meta loss on this task batch = 2.3713e-01, Meta loss averaged over last 500 steps = 3.1005e-01, PNorm = 123.3102, GNorm = 0.3363
Took 111.72540879249573 seconds to complete one epoch of meta training
Took 119.6120331287384 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490878
Epoch 398
Meta loss on this task batch = 3.1752e-01, Meta loss averaged over last 500 steps = 3.1010e-01, PNorm = 123.3178, GNorm = 0.3081
Meta loss on this task batch = 3.5356e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 123.3249, GNorm = 0.2944
Meta loss on this task batch = 3.2762e-01, Meta loss averaged over last 500 steps = 3.1022e-01, PNorm = 123.3307, GNorm = 0.3593
Meta loss on this task batch = 2.7909e-01, Meta loss averaged over last 500 steps = 3.1015e-01, PNorm = 123.3363, GNorm = 0.4237
Meta loss on this task batch = 2.7445e-01, Meta loss averaged over last 500 steps = 3.1011e-01, PNorm = 123.3414, GNorm = 0.2581
Meta loss on this task batch = 2.8587e-01, Meta loss averaged over last 500 steps = 3.1000e-01, PNorm = 123.3469, GNorm = 0.2672
Meta loss on this task batch = 2.7209e-01, Meta loss averaged over last 500 steps = 3.0982e-01, PNorm = 123.3517, GNorm = 0.2879
Meta loss on this task batch = 3.4202e-01, Meta loss averaged over last 500 steps = 3.0983e-01, PNorm = 123.3564, GNorm = 0.5351
Meta loss on this task batch = 2.7904e-01, Meta loss averaged over last 500 steps = 3.0973e-01, PNorm = 123.3609, GNorm = 0.2793
Meta loss on this task batch = 2.8017e-01, Meta loss averaged over last 500 steps = 3.0969e-01, PNorm = 123.3663, GNorm = 0.2878
Meta loss on this task batch = 2.9113e-01, Meta loss averaged over last 500 steps = 3.0967e-01, PNorm = 123.3718, GNorm = 0.2573
Meta loss on this task batch = 2.5260e-01, Meta loss averaged over last 500 steps = 3.0951e-01, PNorm = 123.3773, GNorm = 0.2513
Meta loss on this task batch = 3.5750e-01, Meta loss averaged over last 500 steps = 3.0951e-01, PNorm = 123.3822, GNorm = 0.2864
Meta loss on this task batch = 3.5797e-01, Meta loss averaged over last 500 steps = 3.0957e-01, PNorm = 123.3871, GNorm = 0.2765
Meta loss on this task batch = 3.3376e-01, Meta loss averaged over last 500 steps = 3.0960e-01, PNorm = 123.3912, GNorm = 0.3460
Meta loss on this task batch = 3.2643e-01, Meta loss averaged over last 500 steps = 3.0968e-01, PNorm = 123.3951, GNorm = 0.2995
Meta loss on this task batch = 3.1716e-01, Meta loss averaged over last 500 steps = 3.0972e-01, PNorm = 123.3994, GNorm = 0.2511
Meta loss on this task batch = 2.4620e-01, Meta loss averaged over last 500 steps = 3.0956e-01, PNorm = 123.4044, GNorm = 0.2607
Meta loss on this task batch = 3.3212e-01, Meta loss averaged over last 500 steps = 3.0960e-01, PNorm = 123.4101, GNorm = 0.3469
Took 110.68326449394226 seconds to complete one epoch of meta training
Took 117.59799766540527 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468038
Epoch 399
Meta loss on this task batch = 2.8346e-01, Meta loss averaged over last 500 steps = 3.0950e-01, PNorm = 123.4161, GNorm = 0.2634
Meta loss on this task batch = 3.3576e-01, Meta loss averaged over last 500 steps = 3.0964e-01, PNorm = 123.4218, GNorm = 0.4434
Meta loss on this task batch = 2.7575e-01, Meta loss averaged over last 500 steps = 3.0958e-01, PNorm = 123.4279, GNorm = 0.3025
Meta loss on this task batch = 3.2279e-01, Meta loss averaged over last 500 steps = 3.0965e-01, PNorm = 123.4334, GNorm = 0.2607
Meta loss on this task batch = 2.9229e-01, Meta loss averaged over last 500 steps = 3.0962e-01, PNorm = 123.4392, GNorm = 0.3051
Meta loss on this task batch = 3.7875e-01, Meta loss averaged over last 500 steps = 3.0985e-01, PNorm = 123.4443, GNorm = 0.3821
Meta loss on this task batch = 3.1894e-01, Meta loss averaged over last 500 steps = 3.0995e-01, PNorm = 123.4497, GNorm = 0.3084
Meta loss on this task batch = 3.2425e-01, Meta loss averaged over last 500 steps = 3.0998e-01, PNorm = 123.4552, GNorm = 0.4292
Meta loss on this task batch = 3.0272e-01, Meta loss averaged over last 500 steps = 3.1005e-01, PNorm = 123.4615, GNorm = 0.2858
Meta loss on this task batch = 2.9720e-01, Meta loss averaged over last 500 steps = 3.1012e-01, PNorm = 123.4671, GNorm = 0.2540
Meta loss on this task batch = 3.2258e-01, Meta loss averaged over last 500 steps = 3.1019e-01, PNorm = 123.4732, GNorm = 0.2818
Meta loss on this task batch = 3.0054e-01, Meta loss averaged over last 500 steps = 3.1016e-01, PNorm = 123.4798, GNorm = 0.3130
Meta loss on this task batch = 3.1343e-01, Meta loss averaged over last 500 steps = 3.1020e-01, PNorm = 123.4866, GNorm = 0.2724
Meta loss on this task batch = 3.3950e-01, Meta loss averaged over last 500 steps = 3.1032e-01, PNorm = 123.4934, GNorm = 0.2998
Meta loss on this task batch = 2.7174e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 123.5006, GNorm = 0.2642
Meta loss on this task batch = 3.4619e-01, Meta loss averaged over last 500 steps = 3.1025e-01, PNorm = 123.5071, GNorm = 0.3493
Meta loss on this task batch = 2.9891e-01, Meta loss averaged over last 500 steps = 3.1031e-01, PNorm = 123.5130, GNorm = 0.2967
Meta loss on this task batch = 3.0367e-01, Meta loss averaged over last 500 steps = 3.1015e-01, PNorm = 123.5188, GNorm = 0.3700
Meta loss on this task batch = 3.2102e-01, Meta loss averaged over last 500 steps = 3.1020e-01, PNorm = 123.5248, GNorm = 0.2639
Took 114.87140345573425 seconds to complete one epoch of meta training
Took 122.44825983047485 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484933
Epoch 400
Meta loss on this task batch = 2.9731e-01, Meta loss averaged over last 500 steps = 3.1014e-01, PNorm = 123.5311, GNorm = 0.2876
Meta loss on this task batch = 3.2989e-01, Meta loss averaged over last 500 steps = 3.1009e-01, PNorm = 123.5364, GNorm = 0.3699
Meta loss on this task batch = 3.2970e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 123.5404, GNorm = 0.4011
Meta loss on this task batch = 3.4546e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 123.5438, GNorm = 0.3128
Meta loss on this task batch = 3.0651e-01, Meta loss averaged over last 500 steps = 3.1009e-01, PNorm = 123.5471, GNorm = 0.3305
Meta loss on this task batch = 3.1545e-01, Meta loss averaged over last 500 steps = 3.1002e-01, PNorm = 123.5493, GNorm = 0.3121
Meta loss on this task batch = 3.0888e-01, Meta loss averaged over last 500 steps = 3.1012e-01, PNorm = 123.5522, GNorm = 0.3403
Meta loss on this task batch = 3.2880e-01, Meta loss averaged over last 500 steps = 3.1023e-01, PNorm = 123.5560, GNorm = 0.3708
Meta loss on this task batch = 2.7522e-01, Meta loss averaged over last 500 steps = 3.1002e-01, PNorm = 123.5610, GNorm = 0.3813
Meta loss on this task batch = 3.2580e-01, Meta loss averaged over last 500 steps = 3.1021e-01, PNorm = 123.5656, GNorm = 0.2772
Meta loss on this task batch = 3.3350e-01, Meta loss averaged over last 500 steps = 3.1028e-01, PNorm = 123.5706, GNorm = 0.2622
Meta loss on this task batch = 2.9083e-01, Meta loss averaged over last 500 steps = 3.1025e-01, PNorm = 123.5766, GNorm = 0.4018
Meta loss on this task batch = 2.9310e-01, Meta loss averaged over last 500 steps = 3.1015e-01, PNorm = 123.5834, GNorm = 0.2988
Meta loss on this task batch = 3.0806e-01, Meta loss averaged over last 500 steps = 3.1012e-01, PNorm = 123.5904, GNorm = 0.3035
Meta loss on this task batch = 2.6329e-01, Meta loss averaged over last 500 steps = 3.0999e-01, PNorm = 123.5982, GNorm = 0.2874
Meta loss on this task batch = 2.6874e-01, Meta loss averaged over last 500 steps = 3.1001e-01, PNorm = 123.6064, GNorm = 0.3803
Meta loss on this task batch = 2.7791e-01, Meta loss averaged over last 500 steps = 3.0994e-01, PNorm = 123.6155, GNorm = 0.3512
Meta loss on this task batch = 3.0969e-01, Meta loss averaged over last 500 steps = 3.0986e-01, PNorm = 123.6245, GNorm = 0.2764
Meta loss on this task batch = 3.4470e-01, Meta loss averaged over last 500 steps = 3.0986e-01, PNorm = 123.6322, GNorm = 0.3345
Took 111.26612639427185 seconds to complete one epoch of meta training
Took 118.45867347717285 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478128
Epoch 401
Meta loss on this task batch = 2.7951e-01, Meta loss averaged over last 500 steps = 3.0970e-01, PNorm = 123.6393, GNorm = 0.3043
Meta loss on this task batch = 3.0001e-01, Meta loss averaged over last 500 steps = 3.0972e-01, PNorm = 123.6458, GNorm = 0.2870
Meta loss on this task batch = 3.2459e-01, Meta loss averaged over last 500 steps = 3.0970e-01, PNorm = 123.6513, GNorm = 0.3829
Meta loss on this task batch = 2.4714e-01, Meta loss averaged over last 500 steps = 3.0961e-01, PNorm = 123.6568, GNorm = 0.2303
Meta loss on this task batch = 3.1765e-01, Meta loss averaged over last 500 steps = 3.0958e-01, PNorm = 123.6623, GNorm = 0.2988
Meta loss on this task batch = 3.1279e-01, Meta loss averaged over last 500 steps = 3.0954e-01, PNorm = 123.6674, GNorm = 0.3443
Meta loss on this task batch = 3.3200e-01, Meta loss averaged over last 500 steps = 3.0958e-01, PNorm = 123.6728, GNorm = 0.3286
Meta loss on this task batch = 3.0603e-01, Meta loss averaged over last 500 steps = 3.0957e-01, PNorm = 123.6781, GNorm = 0.2530
Meta loss on this task batch = 3.0857e-01, Meta loss averaged over last 500 steps = 3.0951e-01, PNorm = 123.6842, GNorm = 0.2954
Meta loss on this task batch = 3.3719e-01, Meta loss averaged over last 500 steps = 3.0955e-01, PNorm = 123.6897, GNorm = 0.3278
Meta loss on this task batch = 2.7365e-01, Meta loss averaged over last 500 steps = 3.0940e-01, PNorm = 123.6965, GNorm = 0.2438
Meta loss on this task batch = 2.9988e-01, Meta loss averaged over last 500 steps = 3.0935e-01, PNorm = 123.7037, GNorm = 0.5024
Meta loss on this task batch = 2.6127e-01, Meta loss averaged over last 500 steps = 3.0932e-01, PNorm = 123.7111, GNorm = 0.2707
Meta loss on this task batch = 2.5694e-01, Meta loss averaged over last 500 steps = 3.0932e-01, PNorm = 123.7189, GNorm = 0.2733
Meta loss on this task batch = 3.2577e-01, Meta loss averaged over last 500 steps = 3.0938e-01, PNorm = 123.7274, GNorm = 0.4479
Meta loss on this task batch = 3.4844e-01, Meta loss averaged over last 500 steps = 3.0941e-01, PNorm = 123.7355, GNorm = 0.3379
Meta loss on this task batch = 3.5152e-01, Meta loss averaged over last 500 steps = 3.0950e-01, PNorm = 123.7429, GNorm = 0.2977
Meta loss on this task batch = 3.1487e-01, Meta loss averaged over last 500 steps = 3.0953e-01, PNorm = 123.7501, GNorm = 0.3440
Meta loss on this task batch = 2.8445e-01, Meta loss averaged over last 500 steps = 3.0934e-01, PNorm = 123.7580, GNorm = 0.3302
Took 109.14180397987366 seconds to complete one epoch of meta training
Took 117.40194416046143 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476320
Epoch 402
Meta loss on this task batch = 2.6312e-01, Meta loss averaged over last 500 steps = 3.0922e-01, PNorm = 123.7666, GNorm = 0.2681
Meta loss on this task batch = 3.4111e-01, Meta loss averaged over last 500 steps = 3.0921e-01, PNorm = 123.7743, GNorm = 0.3955
Meta loss on this task batch = 2.9900e-01, Meta loss averaged over last 500 steps = 3.0928e-01, PNorm = 123.7817, GNorm = 0.2723
Meta loss on this task batch = 2.9062e-01, Meta loss averaged over last 500 steps = 3.0922e-01, PNorm = 123.7881, GNorm = 0.2507
Meta loss on this task batch = 3.2519e-01, Meta loss averaged over last 500 steps = 3.0923e-01, PNorm = 123.7939, GNorm = 0.3584
Meta loss on this task batch = 2.8215e-01, Meta loss averaged over last 500 steps = 3.0911e-01, PNorm = 123.7999, GNorm = 0.2515
Meta loss on this task batch = 3.4801e-01, Meta loss averaged over last 500 steps = 3.0910e-01, PNorm = 123.8049, GNorm = 0.3708
Meta loss on this task batch = 3.0069e-01, Meta loss averaged over last 500 steps = 3.0908e-01, PNorm = 123.8091, GNorm = 0.3452
Meta loss on this task batch = 3.2697e-01, Meta loss averaged over last 500 steps = 3.0930e-01, PNorm = 123.8127, GNorm = 0.3129
Meta loss on this task batch = 3.0603e-01, Meta loss averaged over last 500 steps = 3.0940e-01, PNorm = 123.8164, GNorm = 0.4734
Meta loss on this task batch = 3.0727e-01, Meta loss averaged over last 500 steps = 3.0941e-01, PNorm = 123.8195, GNorm = 0.3134
Meta loss on this task batch = 2.8489e-01, Meta loss averaged over last 500 steps = 3.0934e-01, PNorm = 123.8229, GNorm = 0.2775
Meta loss on this task batch = 2.6029e-01, Meta loss averaged over last 500 steps = 3.0920e-01, PNorm = 123.8266, GNorm = 0.2295
Meta loss on this task batch = 3.2351e-01, Meta loss averaged over last 500 steps = 3.0918e-01, PNorm = 123.8308, GNorm = 0.2795
Meta loss on this task batch = 2.9836e-01, Meta loss averaged over last 500 steps = 3.0899e-01, PNorm = 123.8352, GNorm = 0.3315
Meta loss on this task batch = 3.1002e-01, Meta loss averaged over last 500 steps = 3.0899e-01, PNorm = 123.8390, GNorm = 0.3209
Meta loss on this task batch = 2.8956e-01, Meta loss averaged over last 500 steps = 3.0904e-01, PNorm = 123.8432, GNorm = 0.4984
Meta loss on this task batch = 2.3398e-01, Meta loss averaged over last 500 steps = 3.0886e-01, PNorm = 123.8478, GNorm = 0.3045
Meta loss on this task batch = 3.0820e-01, Meta loss averaged over last 500 steps = 3.0889e-01, PNorm = 123.8527, GNorm = 0.3300
Took 112.2550859451294 seconds to complete one epoch of meta training
Took 120.16860866546631 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495714
Epoch 403
Meta loss on this task batch = 2.9710e-01, Meta loss averaged over last 500 steps = 3.0889e-01, PNorm = 123.8576, GNorm = 0.3259
Meta loss on this task batch = 3.3383e-01, Meta loss averaged over last 500 steps = 3.0881e-01, PNorm = 123.8622, GNorm = 0.3197
Meta loss on this task batch = 3.4271e-01, Meta loss averaged over last 500 steps = 3.0881e-01, PNorm = 123.8673, GNorm = 0.2811
Meta loss on this task batch = 2.9890e-01, Meta loss averaged over last 500 steps = 3.0887e-01, PNorm = 123.8726, GNorm = 0.4003
Meta loss on this task batch = 2.9535e-01, Meta loss averaged over last 500 steps = 3.0891e-01, PNorm = 123.8781, GNorm = 0.3428
Meta loss on this task batch = 3.3354e-01, Meta loss averaged over last 500 steps = 3.0885e-01, PNorm = 123.8841, GNorm = 0.2915
Meta loss on this task batch = 2.9245e-01, Meta loss averaged over last 500 steps = 3.0878e-01, PNorm = 123.8906, GNorm = 0.3435
Meta loss on this task batch = 2.7463e-01, Meta loss averaged over last 500 steps = 3.0870e-01, PNorm = 123.8975, GNorm = 0.2511
Meta loss on this task batch = 2.4973e-01, Meta loss averaged over last 500 steps = 3.0860e-01, PNorm = 123.9053, GNorm = 0.3688
Meta loss on this task batch = 2.8081e-01, Meta loss averaged over last 500 steps = 3.0854e-01, PNorm = 123.9137, GNorm = 0.3301
Meta loss on this task batch = 3.2939e-01, Meta loss averaged over last 500 steps = 3.0859e-01, PNorm = 123.9215, GNorm = 0.3515
Meta loss on this task batch = 3.0375e-01, Meta loss averaged over last 500 steps = 3.0871e-01, PNorm = 123.9298, GNorm = 0.3141
Meta loss on this task batch = 2.9354e-01, Meta loss averaged over last 500 steps = 3.0868e-01, PNorm = 123.9387, GNorm = 0.2778
Meta loss on this task batch = 3.7624e-01, Meta loss averaged over last 500 steps = 3.0884e-01, PNorm = 123.9457, GNorm = 0.4044
Meta loss on this task batch = 3.3520e-01, Meta loss averaged over last 500 steps = 3.0878e-01, PNorm = 123.9525, GNorm = 0.4197
Meta loss on this task batch = 2.8115e-01, Meta loss averaged over last 500 steps = 3.0875e-01, PNorm = 123.9590, GNorm = 0.2956
Meta loss on this task batch = 3.1887e-01, Meta loss averaged over last 500 steps = 3.0882e-01, PNorm = 123.9654, GNorm = 0.2845
Meta loss on this task batch = 2.8301e-01, Meta loss averaged over last 500 steps = 3.0879e-01, PNorm = 123.9722, GNorm = 0.2615
Meta loss on this task batch = 3.6093e-01, Meta loss averaged over last 500 steps = 3.0885e-01, PNorm = 123.9791, GNorm = 0.3374
Took 114.5912652015686 seconds to complete one epoch of meta training
Took 122.73562908172607 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502391
Epoch 404
Meta loss on this task batch = 3.0135e-01, Meta loss averaged over last 500 steps = 3.0887e-01, PNorm = 123.9856, GNorm = 0.4014
Meta loss on this task batch = 3.6269e-01, Meta loss averaged over last 500 steps = 3.0891e-01, PNorm = 123.9923, GNorm = 0.3614
Meta loss on this task batch = 3.2030e-01, Meta loss averaged over last 500 steps = 3.0884e-01, PNorm = 123.9990, GNorm = 0.3176
Meta loss on this task batch = 2.5076e-01, Meta loss averaged over last 500 steps = 3.0867e-01, PNorm = 124.0056, GNorm = 0.2895
Meta loss on this task batch = 3.1804e-01, Meta loss averaged over last 500 steps = 3.0864e-01, PNorm = 124.0114, GNorm = 0.3217
Meta loss on this task batch = 2.7876e-01, Meta loss averaged over last 500 steps = 3.0877e-01, PNorm = 124.0174, GNorm = 0.3209
Meta loss on this task batch = 3.4329e-01, Meta loss averaged over last 500 steps = 3.0890e-01, PNorm = 124.0235, GNorm = 0.3854
Meta loss on this task batch = 3.2507e-01, Meta loss averaged over last 500 steps = 3.0895e-01, PNorm = 124.0305, GNorm = 0.2742
Meta loss on this task batch = 2.8027e-01, Meta loss averaged over last 500 steps = 3.0889e-01, PNorm = 124.0368, GNorm = 0.3189
Meta loss on this task batch = 2.9544e-01, Meta loss averaged over last 500 steps = 3.0882e-01, PNorm = 124.0433, GNorm = 0.3341
Meta loss on this task batch = 3.4099e-01, Meta loss averaged over last 500 steps = 3.0871e-01, PNorm = 124.0490, GNorm = 0.2915
Meta loss on this task batch = 2.2378e-01, Meta loss averaged over last 500 steps = 3.0856e-01, PNorm = 124.0556, GNorm = 0.2314
Meta loss on this task batch = 2.6127e-01, Meta loss averaged over last 500 steps = 3.0846e-01, PNorm = 124.0620, GNorm = 0.2811
Meta loss on this task batch = 3.2173e-01, Meta loss averaged over last 500 steps = 3.0848e-01, PNorm = 124.0678, GNorm = 0.2862
Meta loss on this task batch = 3.3770e-01, Meta loss averaged over last 500 steps = 3.0851e-01, PNorm = 124.0723, GNorm = 0.2943
Meta loss on this task batch = 3.3216e-01, Meta loss averaged over last 500 steps = 3.0860e-01, PNorm = 124.0761, GNorm = 0.3706
Meta loss on this task batch = 3.6974e-01, Meta loss averaged over last 500 steps = 3.0875e-01, PNorm = 124.0794, GNorm = 0.4869
Meta loss on this task batch = 3.1792e-01, Meta loss averaged over last 500 steps = 3.0881e-01, PNorm = 124.0834, GNorm = 0.2590
Meta loss on this task batch = 3.4419e-01, Meta loss averaged over last 500 steps = 3.0891e-01, PNorm = 124.0882, GNorm = 0.3649
Took 111.20551776885986 seconds to complete one epoch of meta training
Took 119.25921845436096 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464630
Epoch 405
Meta loss on this task batch = 2.9864e-01, Meta loss averaged over last 500 steps = 3.0891e-01, PNorm = 124.0935, GNorm = 0.3313
Meta loss on this task batch = 2.7379e-01, Meta loss averaged over last 500 steps = 3.0887e-01, PNorm = 124.0994, GNorm = 0.2944
Meta loss on this task batch = 3.2021e-01, Meta loss averaged over last 500 steps = 3.0892e-01, PNorm = 124.1056, GNorm = 0.2402
Meta loss on this task batch = 3.0103e-01, Meta loss averaged over last 500 steps = 3.0891e-01, PNorm = 124.1125, GNorm = 0.4515
Meta loss on this task batch = 3.4029e-01, Meta loss averaged over last 500 steps = 3.0892e-01, PNorm = 124.1190, GNorm = 0.3983
Meta loss on this task batch = 2.9054e-01, Meta loss averaged over last 500 steps = 3.0882e-01, PNorm = 124.1256, GNorm = 0.3321
Meta loss on this task batch = 3.4177e-01, Meta loss averaged over last 500 steps = 3.0893e-01, PNorm = 124.1325, GNorm = 0.3097
Meta loss on this task batch = 2.9779e-01, Meta loss averaged over last 500 steps = 3.0890e-01, PNorm = 124.1394, GNorm = 0.2667
Meta loss on this task batch = 2.6029e-01, Meta loss averaged over last 500 steps = 3.0878e-01, PNorm = 124.1473, GNorm = 0.2689
Meta loss on this task batch = 3.3377e-01, Meta loss averaged over last 500 steps = 3.0887e-01, PNorm = 124.1545, GNorm = 0.2570
Meta loss on this task batch = 2.9143e-01, Meta loss averaged over last 500 steps = 3.0891e-01, PNorm = 124.1624, GNorm = 0.3161
Meta loss on this task batch = 2.6462e-01, Meta loss averaged over last 500 steps = 3.0881e-01, PNorm = 124.1709, GNorm = 0.2180
Meta loss on this task batch = 3.3583e-01, Meta loss averaged over last 500 steps = 3.0889e-01, PNorm = 124.1795, GNorm = 0.3295
Meta loss on this task batch = 2.4588e-01, Meta loss averaged over last 500 steps = 3.0886e-01, PNorm = 124.1879, GNorm = 0.2284
Meta loss on this task batch = 3.4107e-01, Meta loss averaged over last 500 steps = 3.0888e-01, PNorm = 124.1958, GNorm = 0.2819
Meta loss on this task batch = 3.1944e-01, Meta loss averaged over last 500 steps = 3.0891e-01, PNorm = 124.2025, GNorm = 0.4976
Meta loss on this task batch = 3.1446e-01, Meta loss averaged over last 500 steps = 3.0900e-01, PNorm = 124.2095, GNorm = 0.3574
Meta loss on this task batch = 3.2046e-01, Meta loss averaged over last 500 steps = 3.0911e-01, PNorm = 124.2161, GNorm = 0.3777
Meta loss on this task batch = 2.8919e-01, Meta loss averaged over last 500 steps = 3.0900e-01, PNorm = 124.2230, GNorm = 0.2996
Took 109.54733490943909 seconds to complete one epoch of meta training
Took 117.08528232574463 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458864
Epoch 406
Meta loss on this task batch = 2.5283e-01, Meta loss averaged over last 500 steps = 3.0891e-01, PNorm = 124.2298, GNorm = 0.2944
Meta loss on this task batch = 2.9031e-01, Meta loss averaged over last 500 steps = 3.0872e-01, PNorm = 124.2365, GNorm = 0.3046
Meta loss on this task batch = 3.0880e-01, Meta loss averaged over last 500 steps = 3.0868e-01, PNorm = 124.2433, GNorm = 0.2998
Meta loss on this task batch = 3.1198e-01, Meta loss averaged over last 500 steps = 3.0865e-01, PNorm = 124.2486, GNorm = 0.2466
Meta loss on this task batch = 3.4257e-01, Meta loss averaged over last 500 steps = 3.0863e-01, PNorm = 124.2534, GNorm = 0.4477
Meta loss on this task batch = 3.3982e-01, Meta loss averaged over last 500 steps = 3.0870e-01, PNorm = 124.2577, GNorm = 0.2525
Meta loss on this task batch = 2.7751e-01, Meta loss averaged over last 500 steps = 3.0860e-01, PNorm = 124.2629, GNorm = 0.3134
Meta loss on this task batch = 2.9413e-01, Meta loss averaged over last 500 steps = 3.0856e-01, PNorm = 124.2673, GNorm = 0.2879
Meta loss on this task batch = 2.9785e-01, Meta loss averaged over last 500 steps = 3.0856e-01, PNorm = 124.2716, GNorm = 0.4003
Meta loss on this task batch = 2.6827e-01, Meta loss averaged over last 500 steps = 3.0843e-01, PNorm = 124.2766, GNorm = 0.2808
Meta loss on this task batch = 3.3176e-01, Meta loss averaged over last 500 steps = 3.0852e-01, PNorm = 124.2815, GNorm = 0.3656
Meta loss on this task batch = 2.8615e-01, Meta loss averaged over last 500 steps = 3.0844e-01, PNorm = 124.2860, GNorm = 0.3785
Meta loss on this task batch = 2.8378e-01, Meta loss averaged over last 500 steps = 3.0842e-01, PNorm = 124.2912, GNorm = 0.2686
Meta loss on this task batch = 2.9111e-01, Meta loss averaged over last 500 steps = 3.0828e-01, PNorm = 124.2969, GNorm = 0.3395
Meta loss on this task batch = 3.0024e-01, Meta loss averaged over last 500 steps = 3.0828e-01, PNorm = 124.3022, GNorm = 0.3496
Meta loss on this task batch = 2.9655e-01, Meta loss averaged over last 500 steps = 3.0822e-01, PNorm = 124.3078, GNorm = 0.2602
Meta loss on this task batch = 3.4534e-01, Meta loss averaged over last 500 steps = 3.0835e-01, PNorm = 124.3136, GNorm = 0.2607
Meta loss on this task batch = 3.2693e-01, Meta loss averaged over last 500 steps = 3.0843e-01, PNorm = 124.3176, GNorm = 0.3063
Meta loss on this task batch = 3.3149e-01, Meta loss averaged over last 500 steps = 3.0848e-01, PNorm = 124.3221, GNorm = 0.3224
Took 107.44258236885071 seconds to complete one epoch of meta training
Took 115.92294406890869 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448388
Epoch 407
Meta loss on this task batch = 2.6180e-01, Meta loss averaged over last 500 steps = 3.0844e-01, PNorm = 124.3277, GNorm = 0.2273
Meta loss on this task batch = 2.8707e-01, Meta loss averaged over last 500 steps = 3.0832e-01, PNorm = 124.3336, GNorm = 0.2998
Meta loss on this task batch = 3.3356e-01, Meta loss averaged over last 500 steps = 3.0834e-01, PNorm = 124.3390, GNorm = 0.2779
Meta loss on this task batch = 2.7342e-01, Meta loss averaged over last 500 steps = 3.0827e-01, PNorm = 124.3448, GNorm = 0.3693
Meta loss on this task batch = 2.9810e-01, Meta loss averaged over last 500 steps = 3.0824e-01, PNorm = 124.3514, GNorm = 0.3010
Meta loss on this task batch = 3.4604e-01, Meta loss averaged over last 500 steps = 3.0839e-01, PNorm = 124.3579, GNorm = 0.4895
Meta loss on this task batch = 3.2722e-01, Meta loss averaged over last 500 steps = 3.0840e-01, PNorm = 124.3644, GNorm = 0.2892
Meta loss on this task batch = 2.8637e-01, Meta loss averaged over last 500 steps = 3.0840e-01, PNorm = 124.3706, GNorm = 0.2909
Meta loss on this task batch = 3.1099e-01, Meta loss averaged over last 500 steps = 3.0833e-01, PNorm = 124.3762, GNorm = 0.3282
Meta loss on this task batch = 3.2724e-01, Meta loss averaged over last 500 steps = 3.0838e-01, PNorm = 124.3807, GNorm = 0.2694
Meta loss on this task batch = 2.6687e-01, Meta loss averaged over last 500 steps = 3.0819e-01, PNorm = 124.3855, GNorm = 0.2985
Meta loss on this task batch = 3.1382e-01, Meta loss averaged over last 500 steps = 3.0810e-01, PNorm = 124.3903, GNorm = 0.3631
Meta loss on this task batch = 3.1773e-01, Meta loss averaged over last 500 steps = 3.0803e-01, PNorm = 124.3949, GNorm = 0.4742
Meta loss on this task batch = 3.5658e-01, Meta loss averaged over last 500 steps = 3.0824e-01, PNorm = 124.3998, GNorm = 0.3338
Meta loss on this task batch = 3.4934e-01, Meta loss averaged over last 500 steps = 3.0842e-01, PNorm = 124.4040, GNorm = 0.5459
Meta loss on this task batch = 3.4988e-01, Meta loss averaged over last 500 steps = 3.0851e-01, PNorm = 124.4078, GNorm = 0.3031
Meta loss on this task batch = 3.3361e-01, Meta loss averaged over last 500 steps = 3.0865e-01, PNorm = 124.4114, GNorm = 0.3103
Meta loss on this task batch = 2.5396e-01, Meta loss averaged over last 500 steps = 3.0857e-01, PNorm = 124.4158, GNorm = 0.3834
Meta loss on this task batch = 2.6298e-01, Meta loss averaged over last 500 steps = 3.0834e-01, PNorm = 124.4211, GNorm = 0.3402
Took 110.43871092796326 seconds to complete one epoch of meta training
Took 118.09848546981812 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452900
Epoch 408
Meta loss on this task batch = 2.9279e-01, Meta loss averaged over last 500 steps = 3.0828e-01, PNorm = 124.4274, GNorm = 0.4245
Meta loss on this task batch = 3.6943e-01, Meta loss averaged over last 500 steps = 3.0836e-01, PNorm = 124.4334, GNorm = 0.4273
Meta loss on this task batch = 3.1051e-01, Meta loss averaged over last 500 steps = 3.0832e-01, PNorm = 124.4387, GNorm = 0.3706
Meta loss on this task batch = 2.5676e-01, Meta loss averaged over last 500 steps = 3.0823e-01, PNorm = 124.4453, GNorm = 0.2994
Meta loss on this task batch = 3.2784e-01, Meta loss averaged over last 500 steps = 3.0831e-01, PNorm = 124.4519, GNorm = 0.3494
Meta loss on this task batch = 3.3832e-01, Meta loss averaged over last 500 steps = 3.0837e-01, PNorm = 124.4595, GNorm = 0.2618
Meta loss on this task batch = 3.3143e-01, Meta loss averaged over last 500 steps = 3.0845e-01, PNorm = 124.4669, GNorm = 0.2699
Meta loss on this task batch = 3.0379e-01, Meta loss averaged over last 500 steps = 3.0848e-01, PNorm = 124.4745, GNorm = 0.3351
Meta loss on this task batch = 2.8171e-01, Meta loss averaged over last 500 steps = 3.0837e-01, PNorm = 124.4820, GNorm = 0.3392
Meta loss on this task batch = 2.7129e-01, Meta loss averaged over last 500 steps = 3.0837e-01, PNorm = 124.4895, GNorm = 0.2698
Meta loss on this task batch = 2.8238e-01, Meta loss averaged over last 500 steps = 3.0828e-01, PNorm = 124.4974, GNorm = 0.3721
Meta loss on this task batch = 2.6461e-01, Meta loss averaged over last 500 steps = 3.0814e-01, PNorm = 124.5053, GNorm = 0.5184
Meta loss on this task batch = 2.9885e-01, Meta loss averaged over last 500 steps = 3.0823e-01, PNorm = 124.5131, GNorm = 0.3687
Meta loss on this task batch = 3.2544e-01, Meta loss averaged over last 500 steps = 3.0812e-01, PNorm = 124.5203, GNorm = 0.4002
Meta loss on this task batch = 2.9958e-01, Meta loss averaged over last 500 steps = 3.0807e-01, PNorm = 124.5267, GNorm = 0.2680
Meta loss on this task batch = 2.9792e-01, Meta loss averaged over last 500 steps = 3.0805e-01, PNorm = 124.5328, GNorm = 0.2672
Meta loss on this task batch = 3.6183e-01, Meta loss averaged over last 500 steps = 3.0818e-01, PNorm = 124.5383, GNorm = 0.3217
Meta loss on this task batch = 3.2708e-01, Meta loss averaged over last 500 steps = 3.0828e-01, PNorm = 124.5438, GNorm = 0.3495
Meta loss on this task batch = 3.0160e-01, Meta loss averaged over last 500 steps = 3.0827e-01, PNorm = 124.5493, GNorm = 0.4683
Took 112.80815315246582 seconds to complete one epoch of meta training
Took 120.63287878036499 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452509
Epoch 409
Meta loss on this task batch = 3.0683e-01, Meta loss averaged over last 500 steps = 3.0835e-01, PNorm = 124.5547, GNorm = 0.2944
Meta loss on this task batch = 3.2756e-01, Meta loss averaged over last 500 steps = 3.0836e-01, PNorm = 124.5595, GNorm = 0.3870
Meta loss on this task batch = 3.3283e-01, Meta loss averaged over last 500 steps = 3.0831e-01, PNorm = 124.5637, GNorm = 0.2964
Meta loss on this task batch = 2.8222e-01, Meta loss averaged over last 500 steps = 3.0819e-01, PNorm = 124.5681, GNorm = 0.2435
Meta loss on this task batch = 2.6166e-01, Meta loss averaged over last 500 steps = 3.0796e-01, PNorm = 124.5731, GNorm = 0.2412
Meta loss on this task batch = 3.0631e-01, Meta loss averaged over last 500 steps = 3.0806e-01, PNorm = 124.5788, GNorm = 0.3271
Meta loss on this task batch = 3.2801e-01, Meta loss averaged over last 500 steps = 3.0820e-01, PNorm = 124.5848, GNorm = 0.2767
Meta loss on this task batch = 3.3740e-01, Meta loss averaged over last 500 steps = 3.0826e-01, PNorm = 124.5907, GNorm = 0.3195
Meta loss on this task batch = 3.9294e-01, Meta loss averaged over last 500 steps = 3.0840e-01, PNorm = 124.5968, GNorm = 0.3946
Meta loss on this task batch = 2.8079e-01, Meta loss averaged over last 500 steps = 3.0830e-01, PNorm = 124.6029, GNorm = 0.3092
Meta loss on this task batch = 1.7771e-01, Meta loss averaged over last 500 steps = 3.0811e-01, PNorm = 124.6097, GNorm = 0.2056
Meta loss on this task batch = 3.0100e-01, Meta loss averaged over last 500 steps = 3.0819e-01, PNorm = 124.6162, GNorm = 0.2659
Meta loss on this task batch = 3.3248e-01, Meta loss averaged over last 500 steps = 3.0813e-01, PNorm = 124.6212, GNorm = 0.2746
Meta loss on this task batch = 3.1317e-01, Meta loss averaged over last 500 steps = 3.0809e-01, PNorm = 124.6262, GNorm = 0.3908
Meta loss on this task batch = 3.6249e-01, Meta loss averaged over last 500 steps = 3.0823e-01, PNorm = 124.6292, GNorm = 0.3182
Meta loss on this task batch = 2.9461e-01, Meta loss averaged over last 500 steps = 3.0827e-01, PNorm = 124.6324, GNorm = 0.2239
Meta loss on this task batch = 2.9453e-01, Meta loss averaged over last 500 steps = 3.0815e-01, PNorm = 124.6350, GNorm = 0.4804
Meta loss on this task batch = 3.1003e-01, Meta loss averaged over last 500 steps = 3.0816e-01, PNorm = 124.6384, GNorm = 0.2490
Meta loss on this task batch = 3.0718e-01, Meta loss averaged over last 500 steps = 3.0818e-01, PNorm = 124.6417, GNorm = 0.3841
Took 112.04550862312317 seconds to complete one epoch of meta training
Took 119.61888003349304 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484590
Epoch 410
Meta loss on this task batch = 3.0955e-01, Meta loss averaged over last 500 steps = 3.0821e-01, PNorm = 124.6458, GNorm = 0.2919
Meta loss on this task batch = 2.8880e-01, Meta loss averaged over last 500 steps = 3.0818e-01, PNorm = 124.6502, GNorm = 0.2497
Meta loss on this task batch = 3.7970e-01, Meta loss averaged over last 500 steps = 3.0826e-01, PNorm = 124.6543, GNorm = 0.3047
Meta loss on this task batch = 3.2074e-01, Meta loss averaged over last 500 steps = 3.0824e-01, PNorm = 124.6587, GNorm = 0.2592
Meta loss on this task batch = 2.8906e-01, Meta loss averaged over last 500 steps = 3.0822e-01, PNorm = 124.6637, GNorm = 0.3358
Meta loss on this task batch = 3.4757e-01, Meta loss averaged over last 500 steps = 3.0823e-01, PNorm = 124.6678, GNorm = 0.3405
Meta loss on this task batch = 3.1474e-01, Meta loss averaged over last 500 steps = 3.0823e-01, PNorm = 124.6722, GNorm = 0.2649
Meta loss on this task batch = 3.1480e-01, Meta loss averaged over last 500 steps = 3.0831e-01, PNorm = 124.6789, GNorm = 0.4209
Meta loss on this task batch = 2.3537e-01, Meta loss averaged over last 500 steps = 3.0821e-01, PNorm = 124.6863, GNorm = 0.2792
Meta loss on this task batch = 3.0191e-01, Meta loss averaged over last 500 steps = 3.0813e-01, PNorm = 124.6941, GNorm = 0.2551
Meta loss on this task batch = 2.8302e-01, Meta loss averaged over last 500 steps = 3.0810e-01, PNorm = 124.7028, GNorm = 0.2929
Meta loss on this task batch = 2.8047e-01, Meta loss averaged over last 500 steps = 3.0796e-01, PNorm = 124.7123, GNorm = 0.3350
Meta loss on this task batch = 3.0339e-01, Meta loss averaged over last 500 steps = 3.0784e-01, PNorm = 124.7199, GNorm = 0.3730
Meta loss on this task batch = 3.2944e-01, Meta loss averaged over last 500 steps = 3.0795e-01, PNorm = 124.7265, GNorm = 0.4455
Meta loss on this task batch = 2.9962e-01, Meta loss averaged over last 500 steps = 3.0798e-01, PNorm = 124.7339, GNorm = 0.3118
Meta loss on this task batch = 3.2860e-01, Meta loss averaged over last 500 steps = 3.0795e-01, PNorm = 124.7401, GNorm = 0.3035
Meta loss on this task batch = 3.2641e-01, Meta loss averaged over last 500 steps = 3.0803e-01, PNorm = 124.7457, GNorm = 0.3158
Meta loss on this task batch = 2.7873e-01, Meta loss averaged over last 500 steps = 3.0805e-01, PNorm = 124.7518, GNorm = 0.3912
Meta loss on this task batch = 3.3348e-01, Meta loss averaged over last 500 steps = 3.0807e-01, PNorm = 124.7575, GNorm = 0.4053
Took 111.10105395317078 seconds to complete one epoch of meta training
Took 119.35499596595764 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494277
Epoch 411
Meta loss on this task batch = 3.0863e-01, Meta loss averaged over last 500 steps = 3.0808e-01, PNorm = 124.7627, GNorm = 0.3016
Meta loss on this task batch = 3.1199e-01, Meta loss averaged over last 500 steps = 3.0797e-01, PNorm = 124.7673, GNorm = 0.2797
Meta loss on this task batch = 2.7174e-01, Meta loss averaged over last 500 steps = 3.0787e-01, PNorm = 124.7720, GNorm = 0.2826
Meta loss on this task batch = 3.2294e-01, Meta loss averaged over last 500 steps = 3.0794e-01, PNorm = 124.7776, GNorm = 0.3143
Meta loss on this task batch = 2.6888e-01, Meta loss averaged over last 500 steps = 3.0789e-01, PNorm = 124.7838, GNorm = 0.2402
Meta loss on this task batch = 2.6462e-01, Meta loss averaged over last 500 steps = 3.0783e-01, PNorm = 124.7907, GNorm = 0.3043
Meta loss on this task batch = 3.8093e-01, Meta loss averaged over last 500 steps = 3.0787e-01, PNorm = 124.7963, GNorm = 0.3430
Meta loss on this task batch = 3.1222e-01, Meta loss averaged over last 500 steps = 3.0774e-01, PNorm = 124.8028, GNorm = 0.2966
Meta loss on this task batch = 2.9087e-01, Meta loss averaged over last 500 steps = 3.0776e-01, PNorm = 124.8086, GNorm = 0.2457
Meta loss on this task batch = 3.7734e-01, Meta loss averaged over last 500 steps = 3.0794e-01, PNorm = 124.8123, GNorm = 0.5088
Meta loss on this task batch = 2.4960e-01, Meta loss averaged over last 500 steps = 3.0794e-01, PNorm = 124.8163, GNorm = 0.2133
Meta loss on this task batch = 3.3930e-01, Meta loss averaged over last 500 steps = 3.0802e-01, PNorm = 124.8210, GNorm = 0.3837
Meta loss on this task batch = 3.6403e-01, Meta loss averaged over last 500 steps = 3.0816e-01, PNorm = 124.8249, GNorm = 0.2614
Meta loss on this task batch = 2.7592e-01, Meta loss averaged over last 500 steps = 3.0817e-01, PNorm = 124.8293, GNorm = 0.2903
Meta loss on this task batch = 3.1587e-01, Meta loss averaged over last 500 steps = 3.0818e-01, PNorm = 124.8337, GNorm = 0.2399
Meta loss on this task batch = 3.0779e-01, Meta loss averaged over last 500 steps = 3.0806e-01, PNorm = 124.8387, GNorm = 0.3374
Meta loss on this task batch = 2.9652e-01, Meta loss averaged over last 500 steps = 3.0801e-01, PNorm = 124.8452, GNorm = 0.2797
Meta loss on this task batch = 3.5771e-01, Meta loss averaged over last 500 steps = 3.0800e-01, PNorm = 124.8507, GNorm = 0.5392
Meta loss on this task batch = 3.0521e-01, Meta loss averaged over last 500 steps = 3.0798e-01, PNorm = 124.8551, GNorm = 0.5067
Took 115.87556481361389 seconds to complete one epoch of meta training
Took 123.78891634941101 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456292
Epoch 412
Meta loss on this task batch = 2.8062e-01, Meta loss averaged over last 500 steps = 3.0801e-01, PNorm = 124.8592, GNorm = 0.2822
Meta loss on this task batch = 2.8821e-01, Meta loss averaged over last 500 steps = 3.0805e-01, PNorm = 124.8642, GNorm = 0.2793
Meta loss on this task batch = 3.0242e-01, Meta loss averaged over last 500 steps = 3.0803e-01, PNorm = 124.8695, GNorm = 0.2571
Meta loss on this task batch = 2.6403e-01, Meta loss averaged over last 500 steps = 3.0780e-01, PNorm = 124.8752, GNorm = 0.2319
Meta loss on this task batch = 2.8558e-01, Meta loss averaged over last 500 steps = 3.0778e-01, PNorm = 124.8813, GNorm = 0.2462
Meta loss on this task batch = 3.1631e-01, Meta loss averaged over last 500 steps = 3.0767e-01, PNorm = 124.8873, GNorm = 0.3218
Meta loss on this task batch = 3.5001e-01, Meta loss averaged over last 500 steps = 3.0770e-01, PNorm = 124.8935, GNorm = 0.3209
Meta loss on this task batch = 3.3801e-01, Meta loss averaged over last 500 steps = 3.0761e-01, PNorm = 124.8985, GNorm = 0.4387
Meta loss on this task batch = 3.0478e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 124.9033, GNorm = 0.2463
Meta loss on this task batch = 2.6933e-01, Meta loss averaged over last 500 steps = 3.0757e-01, PNorm = 124.9082, GNorm = 0.2774
Meta loss on this task batch = 2.9541e-01, Meta loss averaged over last 500 steps = 3.0767e-01, PNorm = 124.9126, GNorm = 0.2624
Meta loss on this task batch = 3.1494e-01, Meta loss averaged over last 500 steps = 3.0767e-01, PNorm = 124.9167, GNorm = 0.2693
Meta loss on this task batch = 3.0045e-01, Meta loss averaged over last 500 steps = 3.0767e-01, PNorm = 124.9209, GNorm = 0.2414
Meta loss on this task batch = 2.7456e-01, Meta loss averaged over last 500 steps = 3.0757e-01, PNorm = 124.9253, GNorm = 0.2192
Meta loss on this task batch = 3.1106e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 124.9307, GNorm = 0.3132
Meta loss on this task batch = 2.8644e-01, Meta loss averaged over last 500 steps = 3.0747e-01, PNorm = 124.9372, GNorm = 0.2526
Meta loss on this task batch = 2.8311e-01, Meta loss averaged over last 500 steps = 3.0741e-01, PNorm = 124.9445, GNorm = 0.3285
Meta loss on this task batch = 3.0440e-01, Meta loss averaged over last 500 steps = 3.0750e-01, PNorm = 124.9517, GNorm = 0.2719
Meta loss on this task batch = 3.4659e-01, Meta loss averaged over last 500 steps = 3.0757e-01, PNorm = 124.9588, GNorm = 0.3970
Took 113.26838326454163 seconds to complete one epoch of meta training
Took 120.81174921989441 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444788
Epoch 413
Meta loss on this task batch = 3.1702e-01, Meta loss averaged over last 500 steps = 3.0764e-01, PNorm = 124.9657, GNorm = 0.3129
Meta loss on this task batch = 2.9214e-01, Meta loss averaged over last 500 steps = 3.0747e-01, PNorm = 124.9728, GNorm = 0.3431
Meta loss on this task batch = 3.3283e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 124.9805, GNorm = 0.4285
Meta loss on this task batch = 3.4389e-01, Meta loss averaged over last 500 steps = 3.0760e-01, PNorm = 124.9877, GNorm = 0.2894
Meta loss on this task batch = 2.4196e-01, Meta loss averaged over last 500 steps = 3.0748e-01, PNorm = 124.9962, GNorm = 0.2484
Meta loss on this task batch = 3.2166e-01, Meta loss averaged over last 500 steps = 3.0749e-01, PNorm = 125.0046, GNorm = 0.3223
Meta loss on this task batch = 2.9120e-01, Meta loss averaged over last 500 steps = 3.0746e-01, PNorm = 125.0136, GNorm = 0.2518
Meta loss on this task batch = 3.1534e-01, Meta loss averaged over last 500 steps = 3.0752e-01, PNorm = 125.0232, GNorm = 0.2759
Meta loss on this task batch = 3.1342e-01, Meta loss averaged over last 500 steps = 3.0752e-01, PNorm = 125.0314, GNorm = 0.3450
Meta loss on this task batch = 2.9333e-01, Meta loss averaged over last 500 steps = 3.0743e-01, PNorm = 125.0384, GNorm = 0.5262
Meta loss on this task batch = 3.0479e-01, Meta loss averaged over last 500 steps = 3.0748e-01, PNorm = 125.0455, GNorm = 0.2893
Meta loss on this task batch = 3.4907e-01, Meta loss averaged over last 500 steps = 3.0761e-01, PNorm = 125.0512, GNorm = 0.2955
Meta loss on this task batch = 2.4485e-01, Meta loss averaged over last 500 steps = 3.0752e-01, PNorm = 125.0566, GNorm = 0.2269
Meta loss on this task batch = 3.2667e-01, Meta loss averaged over last 500 steps = 3.0755e-01, PNorm = 125.0623, GNorm = 0.2503
Meta loss on this task batch = 3.2956e-01, Meta loss averaged over last 500 steps = 3.0754e-01, PNorm = 125.0677, GNorm = 0.2683
Meta loss on this task batch = 3.3951e-01, Meta loss averaged over last 500 steps = 3.0772e-01, PNorm = 125.0736, GNorm = 0.3070
Meta loss on this task batch = 3.2507e-01, Meta loss averaged over last 500 steps = 3.0783e-01, PNorm = 125.0788, GNorm = 0.2589
Meta loss on this task batch = 3.1900e-01, Meta loss averaged over last 500 steps = 3.0782e-01, PNorm = 125.0832, GNorm = 0.3067
Meta loss on this task batch = 3.1235e-01, Meta loss averaged over last 500 steps = 3.0787e-01, PNorm = 125.0875, GNorm = 0.3057
Took 115.63616967201233 seconds to complete one epoch of meta training
Took 122.81575584411621 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475717
Epoch 414
Meta loss on this task batch = 3.2595e-01, Meta loss averaged over last 500 steps = 3.0786e-01, PNorm = 125.0916, GNorm = 0.2486
Meta loss on this task batch = 3.1641e-01, Meta loss averaged over last 500 steps = 3.0797e-01, PNorm = 125.0965, GNorm = 0.5799
Meta loss on this task batch = 2.6121e-01, Meta loss averaged over last 500 steps = 3.0783e-01, PNorm = 125.1023, GNorm = 0.3464
Meta loss on this task batch = 3.2514e-01, Meta loss averaged over last 500 steps = 3.0794e-01, PNorm = 125.1086, GNorm = 0.2775
Meta loss on this task batch = 2.5321e-01, Meta loss averaged over last 500 steps = 3.0786e-01, PNorm = 125.1166, GNorm = 0.3013
Meta loss on this task batch = 2.6647e-01, Meta loss averaged over last 500 steps = 3.0768e-01, PNorm = 125.1254, GNorm = 0.2508
Meta loss on this task batch = 3.8519e-01, Meta loss averaged over last 500 steps = 3.0791e-01, PNorm = 125.1337, GNorm = 0.3946
Meta loss on this task batch = 2.4333e-01, Meta loss averaged over last 500 steps = 3.0789e-01, PNorm = 125.1422, GNorm = 0.2316
Meta loss on this task batch = 3.1824e-01, Meta loss averaged over last 500 steps = 3.0796e-01, PNorm = 125.1511, GNorm = 0.3703
Meta loss on this task batch = 2.8327e-01, Meta loss averaged over last 500 steps = 3.0794e-01, PNorm = 125.1594, GNorm = 0.3362
Meta loss on this task batch = 3.0344e-01, Meta loss averaged over last 500 steps = 3.0786e-01, PNorm = 125.1683, GNorm = 0.2627
Meta loss on this task batch = 2.6629e-01, Meta loss averaged over last 500 steps = 3.0760e-01, PNorm = 125.1768, GNorm = 0.3054
Meta loss on this task batch = 3.2692e-01, Meta loss averaged over last 500 steps = 3.0772e-01, PNorm = 125.1855, GNorm = 0.2935
Meta loss on this task batch = 3.0140e-01, Meta loss averaged over last 500 steps = 3.0765e-01, PNorm = 125.1942, GNorm = 0.2524
Meta loss on this task batch = 3.4295e-01, Meta loss averaged over last 500 steps = 3.0772e-01, PNorm = 125.2019, GNorm = 0.3285
Meta loss on this task batch = 2.4452e-01, Meta loss averaged over last 500 steps = 3.0760e-01, PNorm = 125.2096, GNorm = 0.3008
Meta loss on this task batch = 2.8973e-01, Meta loss averaged over last 500 steps = 3.0750e-01, PNorm = 125.2177, GNorm = 0.3010
Meta loss on this task batch = 3.3042e-01, Meta loss averaged over last 500 steps = 3.0765e-01, PNorm = 125.2243, GNorm = 0.2985
Meta loss on this task batch = 3.3202e-01, Meta loss averaged over last 500 steps = 3.0759e-01, PNorm = 125.2295, GNorm = 0.5152
Took 111.20343494415283 seconds to complete one epoch of meta training
Took 119.15520477294922 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482800
Epoch 415
Meta loss on this task batch = 3.1718e-01, Meta loss averaged over last 500 steps = 3.0761e-01, PNorm = 125.2351, GNorm = 0.2896
Meta loss on this task batch = 3.0658e-01, Meta loss averaged over last 500 steps = 3.0757e-01, PNorm = 125.2421, GNorm = 0.4675
Meta loss on this task batch = 2.6130e-01, Meta loss averaged over last 500 steps = 3.0751e-01, PNorm = 125.2497, GNorm = 0.2625
Meta loss on this task batch = 3.3812e-01, Meta loss averaged over last 500 steps = 3.0760e-01, PNorm = 125.2565, GNorm = 0.3733
Meta loss on this task batch = 2.7140e-01, Meta loss averaged over last 500 steps = 3.0759e-01, PNorm = 125.2625, GNorm = 0.2676
Meta loss on this task batch = 3.6424e-01, Meta loss averaged over last 500 steps = 3.0760e-01, PNorm = 125.2669, GNorm = 0.3767
Meta loss on this task batch = 2.7911e-01, Meta loss averaged over last 500 steps = 3.0748e-01, PNorm = 125.2709, GNorm = 0.4407
Meta loss on this task batch = 3.5335e-01, Meta loss averaged over last 500 steps = 3.0751e-01, PNorm = 125.2752, GNorm = 0.2689
Meta loss on this task batch = 3.3516e-01, Meta loss averaged over last 500 steps = 3.0763e-01, PNorm = 125.2794, GNorm = 0.4506
Meta loss on this task batch = 2.8542e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 125.2833, GNorm = 0.3334
Meta loss on this task batch = 2.8775e-01, Meta loss averaged over last 500 steps = 3.0763e-01, PNorm = 125.2880, GNorm = 0.2662
Meta loss on this task batch = 2.6366e-01, Meta loss averaged over last 500 steps = 3.0756e-01, PNorm = 125.2938, GNorm = 0.2919
Meta loss on this task batch = 2.9544e-01, Meta loss averaged over last 500 steps = 3.0763e-01, PNorm = 125.3009, GNorm = 0.4034
Meta loss on this task batch = 2.8759e-01, Meta loss averaged over last 500 steps = 3.0754e-01, PNorm = 125.3085, GNorm = 0.2950
Meta loss on this task batch = 3.2782e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 125.3153, GNorm = 0.2732
Meta loss on this task batch = 2.8114e-01, Meta loss averaged over last 500 steps = 3.0753e-01, PNorm = 125.3223, GNorm = 0.2879
Meta loss on this task batch = 2.9858e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 125.3287, GNorm = 0.2954
Meta loss on this task batch = 3.1302e-01, Meta loss averaged over last 500 steps = 3.0747e-01, PNorm = 125.3339, GNorm = 0.3934
Meta loss on this task batch = 3.2123e-01, Meta loss averaged over last 500 steps = 3.0739e-01, PNorm = 125.3391, GNorm = 0.5907
Took 111.81134605407715 seconds to complete one epoch of meta training
Took 119.73057198524475 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480915
Epoch 416
Meta loss on this task batch = 2.8748e-01, Meta loss averaged over last 500 steps = 3.0736e-01, PNorm = 125.3446, GNorm = 0.3222
Meta loss on this task batch = 2.6245e-01, Meta loss averaged over last 500 steps = 3.0726e-01, PNorm = 125.3500, GNorm = 0.2863
Meta loss on this task batch = 3.3889e-01, Meta loss averaged over last 500 steps = 3.0737e-01, PNorm = 125.3551, GNorm = 0.3074
Meta loss on this task batch = 3.5358e-01, Meta loss averaged over last 500 steps = 3.0747e-01, PNorm = 125.3599, GNorm = 0.2741
Meta loss on this task batch = 3.3397e-01, Meta loss averaged over last 500 steps = 3.0745e-01, PNorm = 125.3654, GNorm = 0.2486
Meta loss on this task batch = 3.3468e-01, Meta loss averaged over last 500 steps = 3.0737e-01, PNorm = 125.3710, GNorm = 0.2795
Meta loss on this task batch = 2.4620e-01, Meta loss averaged over last 500 steps = 3.0724e-01, PNorm = 125.3761, GNorm = 0.2492
Meta loss on this task batch = 3.0223e-01, Meta loss averaged over last 500 steps = 3.0722e-01, PNorm = 125.3812, GNorm = 0.3095
Meta loss on this task batch = 3.0663e-01, Meta loss averaged over last 500 steps = 3.0715e-01, PNorm = 125.3859, GNorm = 0.5299
Meta loss on this task batch = 2.9548e-01, Meta loss averaged over last 500 steps = 3.0723e-01, PNorm = 125.3911, GNorm = 0.2615
Meta loss on this task batch = 3.0702e-01, Meta loss averaged over last 500 steps = 3.0725e-01, PNorm = 125.3964, GNorm = 0.3213
Meta loss on this task batch = 2.5257e-01, Meta loss averaged over last 500 steps = 3.0714e-01, PNorm = 125.4022, GNorm = 0.3014
Meta loss on this task batch = 2.9781e-01, Meta loss averaged over last 500 steps = 3.0720e-01, PNorm = 125.4088, GNorm = 0.2771
Meta loss on this task batch = 2.9386e-01, Meta loss averaged over last 500 steps = 3.0721e-01, PNorm = 125.4152, GNorm = 0.2452
Meta loss on this task batch = 2.9959e-01, Meta loss averaged over last 500 steps = 3.0723e-01, PNorm = 125.4211, GNorm = 0.3109
Meta loss on this task batch = 2.9908e-01, Meta loss averaged over last 500 steps = 3.0730e-01, PNorm = 125.4272, GNorm = 0.2716
Meta loss on this task batch = 2.9999e-01, Meta loss averaged over last 500 steps = 3.0721e-01, PNorm = 125.4334, GNorm = 0.4789
Meta loss on this task batch = 3.7901e-01, Meta loss averaged over last 500 steps = 3.0741e-01, PNorm = 125.4390, GNorm = 0.2925
Meta loss on this task batch = 3.4517e-01, Meta loss averaged over last 500 steps = 3.0730e-01, PNorm = 125.4447, GNorm = 0.4483
Took 114.09187936782837 seconds to complete one epoch of meta training
Took 121.91998863220215 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470001
Epoch 417
Meta loss on this task batch = 3.3508e-01, Meta loss averaged over last 500 steps = 3.0733e-01, PNorm = 125.4502, GNorm = 0.3016
Meta loss on this task batch = 3.4492e-01, Meta loss averaged over last 500 steps = 3.0742e-01, PNorm = 125.4563, GNorm = 0.2423
Meta loss on this task batch = 3.3860e-01, Meta loss averaged over last 500 steps = 3.0751e-01, PNorm = 125.4616, GNorm = 0.3120
Meta loss on this task batch = 3.4032e-01, Meta loss averaged over last 500 steps = 3.0756e-01, PNorm = 125.4675, GNorm = 0.2838
Meta loss on this task batch = 3.1580e-01, Meta loss averaged over last 500 steps = 3.0750e-01, PNorm = 125.4732, GNorm = 0.2753
Meta loss on this task batch = 2.7142e-01, Meta loss averaged over last 500 steps = 3.0742e-01, PNorm = 125.4802, GNorm = 0.2981
Meta loss on this task batch = 3.5531e-01, Meta loss averaged over last 500 steps = 3.0748e-01, PNorm = 125.4867, GNorm = 0.4215
Meta loss on this task batch = 2.8900e-01, Meta loss averaged over last 500 steps = 3.0749e-01, PNorm = 125.4940, GNorm = 0.2397
Meta loss on this task batch = 2.9881e-01, Meta loss averaged over last 500 steps = 3.0741e-01, PNorm = 125.5026, GNorm = 0.2961
Meta loss on this task batch = 2.8385e-01, Meta loss averaged over last 500 steps = 3.0725e-01, PNorm = 125.5112, GNorm = 0.2744
Meta loss on this task batch = 3.2309e-01, Meta loss averaged over last 500 steps = 3.0733e-01, PNorm = 125.5196, GNorm = 0.4142
Meta loss on this task batch = 2.8769e-01, Meta loss averaged over last 500 steps = 3.0742e-01, PNorm = 125.5278, GNorm = 0.2564
Meta loss on this task batch = 2.7528e-01, Meta loss averaged over last 500 steps = 3.0739e-01, PNorm = 125.5355, GNorm = 0.2813
Meta loss on this task batch = 2.6377e-01, Meta loss averaged over last 500 steps = 3.0738e-01, PNorm = 125.5437, GNorm = 0.3142
Meta loss on this task batch = 2.9965e-01, Meta loss averaged over last 500 steps = 3.0728e-01, PNorm = 125.5512, GNorm = 0.4651
Meta loss on this task batch = 2.8722e-01, Meta loss averaged over last 500 steps = 3.0729e-01, PNorm = 125.5588, GNorm = 0.3204
Meta loss on this task batch = 3.1908e-01, Meta loss averaged over last 500 steps = 3.0722e-01, PNorm = 125.5659, GNorm = 0.2732
Meta loss on this task batch = 3.0016e-01, Meta loss averaged over last 500 steps = 3.0717e-01, PNorm = 125.5725, GNorm = 0.3508
Meta loss on this task batch = 3.1289e-01, Meta loss averaged over last 500 steps = 3.0723e-01, PNorm = 125.5782, GNorm = 0.3244
Took 114.00824904441833 seconds to complete one epoch of meta training
Took 121.84892058372498 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486866
Epoch 418
Meta loss on this task batch = 3.5197e-01, Meta loss averaged over last 500 steps = 3.0734e-01, PNorm = 125.5837, GNorm = 0.5146
Meta loss on this task batch = 3.0622e-01, Meta loss averaged over last 500 steps = 3.0742e-01, PNorm = 125.5885, GNorm = 0.2998
Meta loss on this task batch = 2.9259e-01, Meta loss averaged over last 500 steps = 3.0728e-01, PNorm = 125.5938, GNorm = 0.2998
Meta loss on this task batch = 3.2425e-01, Meta loss averaged over last 500 steps = 3.0724e-01, PNorm = 125.5996, GNorm = 0.2566
Meta loss on this task batch = 3.1156e-01, Meta loss averaged over last 500 steps = 3.0723e-01, PNorm = 125.6048, GNorm = 0.2771
Meta loss on this task batch = 2.9803e-01, Meta loss averaged over last 500 steps = 3.0723e-01, PNorm = 125.6097, GNorm = 0.3082
Meta loss on this task batch = 3.2092e-01, Meta loss averaged over last 500 steps = 3.0718e-01, PNorm = 125.6147, GNorm = 0.2936
Meta loss on this task batch = 2.9952e-01, Meta loss averaged over last 500 steps = 3.0715e-01, PNorm = 125.6200, GNorm = 0.3730
Meta loss on this task batch = 3.1016e-01, Meta loss averaged over last 500 steps = 3.0718e-01, PNorm = 125.6250, GNorm = 0.2623
Meta loss on this task batch = 3.1820e-01, Meta loss averaged over last 500 steps = 3.0725e-01, PNorm = 125.6296, GNorm = 0.3226
Meta loss on this task batch = 2.6751e-01, Meta loss averaged over last 500 steps = 3.0722e-01, PNorm = 125.6337, GNorm = 0.3383
Meta loss on this task batch = 2.7945e-01, Meta loss averaged over last 500 steps = 3.0721e-01, PNorm = 125.6381, GNorm = 0.3586
Meta loss on this task batch = 2.9980e-01, Meta loss averaged over last 500 steps = 3.0705e-01, PNorm = 125.6420, GNorm = 0.2957
Meta loss on this task batch = 2.5719e-01, Meta loss averaged over last 500 steps = 3.0684e-01, PNorm = 125.6463, GNorm = 0.3060
Meta loss on this task batch = 2.6976e-01, Meta loss averaged over last 500 steps = 3.0672e-01, PNorm = 125.6509, GNorm = 0.3049
Meta loss on this task batch = 3.4309e-01, Meta loss averaged over last 500 steps = 3.0673e-01, PNorm = 125.6560, GNorm = 0.3490
Meta loss on this task batch = 3.7747e-01, Meta loss averaged over last 500 steps = 3.0680e-01, PNorm = 125.6608, GNorm = 0.3202
Meta loss on this task batch = 3.2948e-01, Meta loss averaged over last 500 steps = 3.0686e-01, PNorm = 125.6659, GNorm = 0.2373
Meta loss on this task batch = 3.0658e-01, Meta loss averaged over last 500 steps = 3.0693e-01, PNorm = 125.6715, GNorm = 0.3153
Took 116.75447130203247 seconds to complete one epoch of meta training
Took 124.56297254562378 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458503
Epoch 419
Meta loss on this task batch = 3.1558e-01, Meta loss averaged over last 500 steps = 3.0699e-01, PNorm = 125.6769, GNorm = 0.2350
Meta loss on this task batch = 2.7654e-01, Meta loss averaged over last 500 steps = 3.0692e-01, PNorm = 125.6833, GNorm = 0.3071
Meta loss on this task batch = 3.1297e-01, Meta loss averaged over last 500 steps = 3.0686e-01, PNorm = 125.6894, GNorm = 0.2519
Meta loss on this task batch = 2.8683e-01, Meta loss averaged over last 500 steps = 3.0686e-01, PNorm = 125.6960, GNorm = 0.2635
Meta loss on this task batch = 3.0469e-01, Meta loss averaged over last 500 steps = 3.0686e-01, PNorm = 125.7031, GNorm = 0.2355
Meta loss on this task batch = 2.8202e-01, Meta loss averaged over last 500 steps = 3.0681e-01, PNorm = 125.7106, GNorm = 0.2885
Meta loss on this task batch = 2.5355e-01, Meta loss averaged over last 500 steps = 3.0671e-01, PNorm = 125.7189, GNorm = 0.2163
Meta loss on this task batch = 3.9250e-01, Meta loss averaged over last 500 steps = 3.0691e-01, PNorm = 125.7255, GNorm = 0.4151
Meta loss on this task batch = 3.1699e-01, Meta loss averaged over last 500 steps = 3.0694e-01, PNorm = 125.7317, GNorm = 0.2507
Meta loss on this task batch = 3.3270e-01, Meta loss averaged over last 500 steps = 3.0695e-01, PNorm = 125.7370, GNorm = 0.3084
Meta loss on this task batch = 2.9383e-01, Meta loss averaged over last 500 steps = 3.0691e-01, PNorm = 125.7422, GNorm = 0.4274
Meta loss on this task batch = 3.5173e-01, Meta loss averaged over last 500 steps = 3.0706e-01, PNorm = 125.7473, GNorm = 0.2809
Meta loss on this task batch = 2.7789e-01, Meta loss averaged over last 500 steps = 3.0706e-01, PNorm = 125.7530, GNorm = 0.2297
Meta loss on this task batch = 3.4441e-01, Meta loss averaged over last 500 steps = 3.0705e-01, PNorm = 125.7581, GNorm = 0.3263
Meta loss on this task batch = 2.7306e-01, Meta loss averaged over last 500 steps = 3.0694e-01, PNorm = 125.7638, GNorm = 0.2396
Meta loss on this task batch = 3.6028e-01, Meta loss averaged over last 500 steps = 3.0703e-01, PNorm = 125.7686, GNorm = 0.3444
Meta loss on this task batch = 3.6911e-01, Meta loss averaged over last 500 steps = 3.0717e-01, PNorm = 125.7727, GNorm = 0.3083
Meta loss on this task batch = 2.6548e-01, Meta loss averaged over last 500 steps = 3.0717e-01, PNorm = 125.7771, GNorm = 0.2387
Meta loss on this task batch = 3.6922e-01, Meta loss averaged over last 500 steps = 3.0729e-01, PNorm = 125.7810, GNorm = 0.3446
Took 111.01135873794556 seconds to complete one epoch of meta training
Took 118.62858080863953 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489910
Epoch 420
Meta loss on this task batch = 3.4518e-01, Meta loss averaged over last 500 steps = 3.0753e-01, PNorm = 125.7854, GNorm = 0.2633
Meta loss on this task batch = 2.7117e-01, Meta loss averaged over last 500 steps = 3.0746e-01, PNorm = 125.7904, GNorm = 0.2733
Meta loss on this task batch = 2.9683e-01, Meta loss averaged over last 500 steps = 3.0741e-01, PNorm = 125.7954, GNorm = 0.3071
Meta loss on this task batch = 2.8932e-01, Meta loss averaged over last 500 steps = 3.0728e-01, PNorm = 125.8016, GNorm = 0.2675
Meta loss on this task batch = 3.2188e-01, Meta loss averaged over last 500 steps = 3.0726e-01, PNorm = 125.8076, GNorm = 0.3087
Meta loss on this task batch = 2.9031e-01, Meta loss averaged over last 500 steps = 3.0734e-01, PNorm = 125.8130, GNorm = 0.3455
Meta loss on this task batch = 2.6880e-01, Meta loss averaged over last 500 steps = 3.0717e-01, PNorm = 125.8186, GNorm = 0.2936
Meta loss on this task batch = 3.4645e-01, Meta loss averaged over last 500 steps = 3.0735e-01, PNorm = 125.8240, GNorm = 0.3090
Meta loss on this task batch = 2.9090e-01, Meta loss averaged over last 500 steps = 3.0735e-01, PNorm = 125.8299, GNorm = 0.2942
Meta loss on this task batch = 3.5361e-01, Meta loss averaged over last 500 steps = 3.0752e-01, PNorm = 125.8361, GNorm = 0.4484
Meta loss on this task batch = 2.4778e-01, Meta loss averaged over last 500 steps = 3.0738e-01, PNorm = 125.8428, GNorm = 0.2438
Meta loss on this task batch = 2.9293e-01, Meta loss averaged over last 500 steps = 3.0714e-01, PNorm = 125.8497, GNorm = 0.3416
Meta loss on this task batch = 2.7925e-01, Meta loss averaged over last 500 steps = 3.0709e-01, PNorm = 125.8573, GNorm = 0.2527
Meta loss on this task batch = 3.4026e-01, Meta loss averaged over last 500 steps = 3.0721e-01, PNorm = 125.8645, GNorm = 0.3927
Meta loss on this task batch = 3.6563e-01, Meta loss averaged over last 500 steps = 3.0735e-01, PNorm = 125.8717, GNorm = 0.5774
Meta loss on this task batch = 2.7363e-01, Meta loss averaged over last 500 steps = 3.0731e-01, PNorm = 125.8792, GNorm = 0.2608
Meta loss on this task batch = 2.7614e-01, Meta loss averaged over last 500 steps = 3.0722e-01, PNorm = 125.8859, GNorm = 0.2919
Meta loss on this task batch = 3.0246e-01, Meta loss averaged over last 500 steps = 3.0737e-01, PNorm = 125.8917, GNorm = 0.2961
Meta loss on this task batch = 2.8863e-01, Meta loss averaged over last 500 steps = 3.0729e-01, PNorm = 125.8979, GNorm = 0.3396
Took 115.50371313095093 seconds to complete one epoch of meta training
Took 123.691166639328 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470660
Epoch 421
Meta loss on this task batch = 2.2780e-01, Meta loss averaged over last 500 steps = 3.0703e-01, PNorm = 125.9049, GNorm = 0.2639
Meta loss on this task batch = 2.7983e-01, Meta loss averaged over last 500 steps = 3.0701e-01, PNorm = 125.9115, GNorm = 0.3356
Meta loss on this task batch = 3.3532e-01, Meta loss averaged over last 500 steps = 3.0707e-01, PNorm = 125.9168, GNorm = 0.3453
Meta loss on this task batch = 3.7556e-01, Meta loss averaged over last 500 steps = 3.0719e-01, PNorm = 125.9217, GNorm = 0.3414
Meta loss on this task batch = 2.6167e-01, Meta loss averaged over last 500 steps = 3.0704e-01, PNorm = 125.9269, GNorm = 0.3240
Meta loss on this task batch = 3.1876e-01, Meta loss averaged over last 500 steps = 3.0705e-01, PNorm = 125.9321, GNorm = 0.3032
Meta loss on this task batch = 2.5783e-01, Meta loss averaged over last 500 steps = 3.0706e-01, PNorm = 125.9373, GNorm = 0.2385
Meta loss on this task batch = 2.8583e-01, Meta loss averaged over last 500 steps = 3.0705e-01, PNorm = 125.9429, GNorm = 0.2932
Meta loss on this task batch = 2.8682e-01, Meta loss averaged over last 500 steps = 3.0698e-01, PNorm = 125.9480, GNorm = 0.2899
Meta loss on this task batch = 3.2319e-01, Meta loss averaged over last 500 steps = 3.0704e-01, PNorm = 125.9535, GNorm = 0.3606
Meta loss on this task batch = 2.8772e-01, Meta loss averaged over last 500 steps = 3.0702e-01, PNorm = 125.9594, GNorm = 0.2713
Meta loss on this task batch = 3.4410e-01, Meta loss averaged over last 500 steps = 3.0705e-01, PNorm = 125.9651, GNorm = 0.4033
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 3.0706e-01, PNorm = 125.9701, GNorm = 0.2886
Meta loss on this task batch = 3.3555e-01, Meta loss averaged over last 500 steps = 3.0699e-01, PNorm = 125.9761, GNorm = 0.4960
Meta loss on this task batch = 3.0694e-01, Meta loss averaged over last 500 steps = 3.0697e-01, PNorm = 125.9824, GNorm = 0.2631
Meta loss on this task batch = 2.6444e-01, Meta loss averaged over last 500 steps = 3.0695e-01, PNorm = 125.9891, GNorm = 0.2418
Meta loss on this task batch = 3.1977e-01, Meta loss averaged over last 500 steps = 3.0698e-01, PNorm = 125.9952, GNorm = 0.3731
Meta loss on this task batch = 3.0616e-01, Meta loss averaged over last 500 steps = 3.0692e-01, PNorm = 126.0016, GNorm = 0.3319
Meta loss on this task batch = 3.5484e-01, Meta loss averaged over last 500 steps = 3.0699e-01, PNorm = 126.0083, GNorm = 0.3620
Took 110.1557514667511 seconds to complete one epoch of meta training
Took 117.88656115531921 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444061
Epoch 422
Meta loss on this task batch = 3.0240e-01, Meta loss averaged over last 500 steps = 3.0698e-01, PNorm = 126.0158, GNorm = 0.2780
Meta loss on this task batch = 2.4277e-01, Meta loss averaged over last 500 steps = 3.0689e-01, PNorm = 126.0240, GNorm = 0.2979
Meta loss on this task batch = 3.7462e-01, Meta loss averaged over last 500 steps = 3.0691e-01, PNorm = 126.0306, GNorm = 0.3590
Meta loss on this task batch = 3.0983e-01, Meta loss averaged over last 500 steps = 3.0696e-01, PNorm = 126.0377, GNorm = 0.3820
Meta loss on this task batch = 2.6706e-01, Meta loss averaged over last 500 steps = 3.0692e-01, PNorm = 126.0449, GNorm = 0.2978
Meta loss on this task batch = 3.1034e-01, Meta loss averaged over last 500 steps = 3.0698e-01, PNorm = 126.0526, GNorm = 0.3516
Meta loss on this task batch = 3.6470e-01, Meta loss averaged over last 500 steps = 3.0705e-01, PNorm = 126.0602, GNorm = 0.3435
Meta loss on this task batch = 3.2198e-01, Meta loss averaged over last 500 steps = 3.0706e-01, PNorm = 126.0665, GNorm = 0.3612
Meta loss on this task batch = 3.4195e-01, Meta loss averaged over last 500 steps = 3.0710e-01, PNorm = 126.0729, GNorm = 0.3645
Meta loss on this task batch = 3.1768e-01, Meta loss averaged over last 500 steps = 3.0706e-01, PNorm = 126.0788, GNorm = 0.2323
Meta loss on this task batch = 2.9084e-01, Meta loss averaged over last 500 steps = 3.0704e-01, PNorm = 126.0841, GNorm = 0.4331
Meta loss on this task batch = 2.4450e-01, Meta loss averaged over last 500 steps = 3.0688e-01, PNorm = 126.0893, GNorm = 0.2629
Meta loss on this task batch = 2.8217e-01, Meta loss averaged over last 500 steps = 3.0685e-01, PNorm = 126.0944, GNorm = 0.2456
Meta loss on this task batch = 3.3526e-01, Meta loss averaged over last 500 steps = 3.0688e-01, PNorm = 126.0986, GNorm = 0.3194
Meta loss on this task batch = 3.3995e-01, Meta loss averaged over last 500 steps = 3.0692e-01, PNorm = 126.1028, GNorm = 0.2643
Meta loss on this task batch = 3.1073e-01, Meta loss averaged over last 500 steps = 3.0700e-01, PNorm = 126.1069, GNorm = 0.2402
Meta loss on this task batch = 2.3973e-01, Meta loss averaged over last 500 steps = 3.0698e-01, PNorm = 126.1114, GNorm = 0.2471
Meta loss on this task batch = 2.9619e-01, Meta loss averaged over last 500 steps = 3.0702e-01, PNorm = 126.1164, GNorm = 0.2810
Meta loss on this task batch = 3.1889e-01, Meta loss averaged over last 500 steps = 3.0706e-01, PNorm = 126.1206, GNorm = 0.2931
Took 109.04839181900024 seconds to complete one epoch of meta training
Took 117.0564239025116 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474168
Epoch 423
Meta loss on this task batch = 3.4202e-01, Meta loss averaged over last 500 steps = 3.0712e-01, PNorm = 126.1241, GNorm = 0.2857
Meta loss on this task batch = 3.4847e-01, Meta loss averaged over last 500 steps = 3.0729e-01, PNorm = 126.1272, GNorm = 0.3375
Meta loss on this task batch = 2.4877e-01, Meta loss averaged over last 500 steps = 3.0718e-01, PNorm = 126.1320, GNorm = 0.3039
Meta loss on this task batch = 3.1435e-01, Meta loss averaged over last 500 steps = 3.0701e-01, PNorm = 126.1376, GNorm = 0.3182
Meta loss on this task batch = 2.8054e-01, Meta loss averaged over last 500 steps = 3.0702e-01, PNorm = 126.1433, GNorm = 0.2739
Meta loss on this task batch = 3.3208e-01, Meta loss averaged over last 500 steps = 3.0706e-01, PNorm = 126.1487, GNorm = 0.3790
Meta loss on this task batch = 3.0076e-01, Meta loss averaged over last 500 steps = 3.0703e-01, PNorm = 126.1544, GNorm = 0.2777
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 3.0700e-01, PNorm = 126.1605, GNorm = 0.2453
Meta loss on this task batch = 2.9382e-01, Meta loss averaged over last 500 steps = 3.0703e-01, PNorm = 126.1676, GNorm = 0.3339
Meta loss on this task batch = 2.8803e-01, Meta loss averaged over last 500 steps = 3.0704e-01, PNorm = 126.1752, GNorm = 0.2761
Meta loss on this task batch = 2.7055e-01, Meta loss averaged over last 500 steps = 3.0692e-01, PNorm = 126.1832, GNorm = 0.3395
Meta loss on this task batch = 3.1216e-01, Meta loss averaged over last 500 steps = 3.0689e-01, PNorm = 126.1907, GNorm = 0.3234
Meta loss on this task batch = 3.3254e-01, Meta loss averaged over last 500 steps = 3.0704e-01, PNorm = 126.1980, GNorm = 0.3086
Meta loss on this task batch = 3.4859e-01, Meta loss averaged over last 500 steps = 3.0700e-01, PNorm = 126.2051, GNorm = 0.2736
Meta loss on this task batch = 3.5293e-01, Meta loss averaged over last 500 steps = 3.0707e-01, PNorm = 126.2124, GNorm = 0.4560
Meta loss on this task batch = 3.4270e-01, Meta loss averaged over last 500 steps = 3.0715e-01, PNorm = 126.2184, GNorm = 0.5829
Meta loss on this task batch = 2.6506e-01, Meta loss averaged over last 500 steps = 3.0693e-01, PNorm = 126.2244, GNorm = 0.3645
Meta loss on this task batch = 3.2083e-01, Meta loss averaged over last 500 steps = 3.0704e-01, PNorm = 126.2307, GNorm = 0.3958
Meta loss on this task batch = 3.2288e-01, Meta loss averaged over last 500 steps = 3.0709e-01, PNorm = 126.2371, GNorm = 0.2894
Took 113.16075730323792 seconds to complete one epoch of meta training
Took 120.66143584251404 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481547
Epoch 424
Meta loss on this task batch = 3.0445e-01, Meta loss averaged over last 500 steps = 3.0697e-01, PNorm = 126.2434, GNorm = 0.4320
Meta loss on this task batch = 2.4313e-01, Meta loss averaged over last 500 steps = 3.0690e-01, PNorm = 126.2506, GNorm = 0.2968
Meta loss on this task batch = 2.7839e-01, Meta loss averaged over last 500 steps = 3.0676e-01, PNorm = 126.2578, GNorm = 0.3183
Meta loss on this task batch = 3.1590e-01, Meta loss averaged over last 500 steps = 3.0679e-01, PNorm = 126.2647, GNorm = 0.3031
Meta loss on this task batch = 2.7723e-01, Meta loss averaged over last 500 steps = 3.0671e-01, PNorm = 126.2716, GNorm = 0.2696
Meta loss on this task batch = 3.4408e-01, Meta loss averaged over last 500 steps = 3.0693e-01, PNorm = 126.2781, GNorm = 0.2969
Meta loss on this task batch = 2.8802e-01, Meta loss averaged over last 500 steps = 3.0687e-01, PNorm = 126.2839, GNorm = 0.3420
Meta loss on this task batch = 3.3355e-01, Meta loss averaged over last 500 steps = 3.0683e-01, PNorm = 126.2884, GNorm = 0.3440
Meta loss on this task batch = 3.3937e-01, Meta loss averaged over last 500 steps = 3.0685e-01, PNorm = 126.2932, GNorm = 0.5215
Meta loss on this task batch = 3.4657e-01, Meta loss averaged over last 500 steps = 3.0699e-01, PNorm = 126.2970, GNorm = 0.3274
Meta loss on this task batch = 2.9873e-01, Meta loss averaged over last 500 steps = 3.0703e-01, PNorm = 126.3024, GNorm = 0.3920
Meta loss on this task batch = 3.1304e-01, Meta loss averaged over last 500 steps = 3.0709e-01, PNorm = 126.3079, GNorm = 0.2376
Meta loss on this task batch = 2.7334e-01, Meta loss averaged over last 500 steps = 3.0709e-01, PNorm = 126.3141, GNorm = 0.3263
Meta loss on this task batch = 3.6386e-01, Meta loss averaged over last 500 steps = 3.0713e-01, PNorm = 126.3203, GNorm = 0.3354
Meta loss on this task batch = 2.9666e-01, Meta loss averaged over last 500 steps = 3.0717e-01, PNorm = 126.3269, GNorm = 0.2940
Meta loss on this task batch = 3.5095e-01, Meta loss averaged over last 500 steps = 3.0731e-01, PNorm = 126.3334, GNorm = 0.3592
Meta loss on this task batch = 2.6831e-01, Meta loss averaged over last 500 steps = 3.0727e-01, PNorm = 126.3398, GNorm = 0.3022
Meta loss on this task batch = 3.1092e-01, Meta loss averaged over last 500 steps = 3.0738e-01, PNorm = 126.3463, GNorm = 0.2737
Meta loss on this task batch = 3.4786e-01, Meta loss averaged over last 500 steps = 3.0736e-01, PNorm = 126.3510, GNorm = 0.3691
Took 110.22149014472961 seconds to complete one epoch of meta training
Took 117.70589232444763 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481035
Epoch 425
Meta loss on this task batch = 3.1847e-01, Meta loss averaged over last 500 steps = 3.0728e-01, PNorm = 126.3556, GNorm = 0.2866
Meta loss on this task batch = 2.3464e-01, Meta loss averaged over last 500 steps = 3.0709e-01, PNorm = 126.3608, GNorm = 0.2506
Meta loss on this task batch = 3.7815e-01, Meta loss averaged over last 500 steps = 3.0719e-01, PNorm = 126.3662, GNorm = 0.3821
Meta loss on this task batch = 3.3143e-01, Meta loss averaged over last 500 steps = 3.0722e-01, PNorm = 126.3715, GNorm = 0.3484
Meta loss on this task batch = 3.2872e-01, Meta loss averaged over last 500 steps = 3.0738e-01, PNorm = 126.3770, GNorm = 0.3012
Meta loss on this task batch = 3.2785e-01, Meta loss averaged over last 500 steps = 3.0737e-01, PNorm = 126.3827, GNorm = 0.3014
Meta loss on this task batch = 2.8568e-01, Meta loss averaged over last 500 steps = 3.0738e-01, PNorm = 126.3887, GNorm = 0.2504
Meta loss on this task batch = 3.2049e-01, Meta loss averaged over last 500 steps = 3.0735e-01, PNorm = 126.3950, GNorm = 0.4014
Meta loss on this task batch = 3.3568e-01, Meta loss averaged over last 500 steps = 3.0747e-01, PNorm = 126.4014, GNorm = 0.3802
Meta loss on this task batch = 2.8462e-01, Meta loss averaged over last 500 steps = 3.0739e-01, PNorm = 126.4090, GNorm = 0.3231
Meta loss on this task batch = 2.7055e-01, Meta loss averaged over last 500 steps = 3.0735e-01, PNorm = 126.4166, GNorm = 0.2928
Meta loss on this task batch = 2.6183e-01, Meta loss averaged over last 500 steps = 3.0711e-01, PNorm = 126.4238, GNorm = 0.2661
Meta loss on this task batch = 3.6909e-01, Meta loss averaged over last 500 steps = 3.0722e-01, PNorm = 126.4305, GNorm = 0.3125
Meta loss on this task batch = 2.8872e-01, Meta loss averaged over last 500 steps = 3.0714e-01, PNorm = 126.4375, GNorm = 0.2820
Meta loss on this task batch = 2.6941e-01, Meta loss averaged over last 500 steps = 3.0708e-01, PNorm = 126.4440, GNorm = 0.3438
Meta loss on this task batch = 2.3505e-01, Meta loss averaged over last 500 steps = 3.0695e-01, PNorm = 126.4512, GNorm = 0.2283
Meta loss on this task batch = 2.8892e-01, Meta loss averaged over last 500 steps = 3.0689e-01, PNorm = 126.4576, GNorm = 0.2880
Meta loss on this task batch = 3.1077e-01, Meta loss averaged over last 500 steps = 3.0691e-01, PNorm = 126.4632, GNorm = 0.2735
Meta loss on this task batch = 3.2733e-01, Meta loss averaged over last 500 steps = 3.0693e-01, PNorm = 126.4686, GNorm = 0.3713
Took 113.8644495010376 seconds to complete one epoch of meta training
Took 121.87288665771484 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477229
Epoch 426
Meta loss on this task batch = 3.2586e-01, Meta loss averaged over last 500 steps = 3.0691e-01, PNorm = 126.4744, GNorm = 0.3841
Meta loss on this task batch = 3.3894e-01, Meta loss averaged over last 500 steps = 3.0704e-01, PNorm = 126.4789, GNorm = 0.3303
Meta loss on this task batch = 2.7665e-01, Meta loss averaged over last 500 steps = 3.0690e-01, PNorm = 126.4834, GNorm = 0.3331
Meta loss on this task batch = 3.2316e-01, Meta loss averaged over last 500 steps = 3.0695e-01, PNorm = 126.4874, GNorm = 0.3026
Meta loss on this task batch = 2.4770e-01, Meta loss averaged over last 500 steps = 3.0684e-01, PNorm = 126.4913, GNorm = 0.2631
Meta loss on this task batch = 2.4731e-01, Meta loss averaged over last 500 steps = 3.0669e-01, PNorm = 126.4943, GNorm = 0.2770
Meta loss on this task batch = 3.0673e-01, Meta loss averaged over last 500 steps = 3.0671e-01, PNorm = 126.4977, GNorm = 0.2623
Meta loss on this task batch = 3.0694e-01, Meta loss averaged over last 500 steps = 3.0666e-01, PNorm = 126.5010, GNorm = 0.5700
Meta loss on this task batch = 3.6535e-01, Meta loss averaged over last 500 steps = 3.0674e-01, PNorm = 126.5042, GNorm = 0.5040
Meta loss on this task batch = 2.6583e-01, Meta loss averaged over last 500 steps = 3.0658e-01, PNorm = 126.5092, GNorm = 0.3445
Meta loss on this task batch = 3.0765e-01, Meta loss averaged over last 500 steps = 3.0658e-01, PNorm = 126.5150, GNorm = 0.2729
Meta loss on this task batch = 2.7762e-01, Meta loss averaged over last 500 steps = 3.0650e-01, PNorm = 126.5215, GNorm = 0.3516
Meta loss on this task batch = 3.0385e-01, Meta loss averaged over last 500 steps = 3.0649e-01, PNorm = 126.5279, GNorm = 0.2918
Meta loss on this task batch = 3.6040e-01, Meta loss averaged over last 500 steps = 3.0656e-01, PNorm = 126.5342, GNorm = 0.2956
Meta loss on this task batch = 3.3011e-01, Meta loss averaged over last 500 steps = 3.0667e-01, PNorm = 126.5399, GNorm = 0.2669
Meta loss on this task batch = 3.1480e-01, Meta loss averaged over last 500 steps = 3.0664e-01, PNorm = 126.5457, GNorm = 0.3249
Meta loss on this task batch = 3.1197e-01, Meta loss averaged over last 500 steps = 3.0660e-01, PNorm = 126.5515, GNorm = 0.2520
Meta loss on this task batch = 3.0878e-01, Meta loss averaged over last 500 steps = 3.0664e-01, PNorm = 126.5564, GNorm = 0.2739
Meta loss on this task batch = 2.7494e-01, Meta loss averaged over last 500 steps = 3.0660e-01, PNorm = 126.5626, GNorm = 0.3300
Took 113.14390444755554 seconds to complete one epoch of meta training
Took 120.31173062324524 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495558
Epoch 427
Meta loss on this task batch = 2.3370e-01, Meta loss averaged over last 500 steps = 3.0645e-01, PNorm = 126.5687, GNorm = 0.3567
Meta loss on this task batch = 3.0763e-01, Meta loss averaged over last 500 steps = 3.0654e-01, PNorm = 126.5738, GNorm = 0.3545
Meta loss on this task batch = 2.7853e-01, Meta loss averaged over last 500 steps = 3.0656e-01, PNorm = 126.5798, GNorm = 0.3483
Meta loss on this task batch = 3.3532e-01, Meta loss averaged over last 500 steps = 3.0667e-01, PNorm = 126.5851, GNorm = 0.3708
Meta loss on this task batch = 3.2685e-01, Meta loss averaged over last 500 steps = 3.0671e-01, PNorm = 126.5897, GNorm = 0.3026
Meta loss on this task batch = 2.9432e-01, Meta loss averaged over last 500 steps = 3.0661e-01, PNorm = 126.5945, GNorm = 0.2472
Meta loss on this task batch = 2.7197e-01, Meta loss averaged over last 500 steps = 3.0659e-01, PNorm = 126.5991, GNorm = 0.2474
Meta loss on this task batch = 2.8223e-01, Meta loss averaged over last 500 steps = 3.0656e-01, PNorm = 126.6047, GNorm = 0.2768
Meta loss on this task batch = 2.7851e-01, Meta loss averaged over last 500 steps = 3.0647e-01, PNorm = 126.6103, GNorm = 0.2751
Meta loss on this task batch = 3.3360e-01, Meta loss averaged over last 500 steps = 3.0664e-01, PNorm = 126.6161, GNorm = 0.3363
Meta loss on this task batch = 2.3924e-01, Meta loss averaged over last 500 steps = 3.0648e-01, PNorm = 126.6216, GNorm = 0.2294
Meta loss on this task batch = 2.8229e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 126.6277, GNorm = 0.2835
Meta loss on this task batch = 3.1908e-01, Meta loss averaged over last 500 steps = 3.0639e-01, PNorm = 126.6336, GNorm = 0.2885
Meta loss on this task batch = 2.4813e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 126.6394, GNorm = 0.2485
Meta loss on this task batch = 3.4927e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 126.6452, GNorm = 0.2977
Meta loss on this task batch = 3.3067e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 126.6511, GNorm = 0.2747
Meta loss on this task batch = 2.8292e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 126.6570, GNorm = 0.2669
Meta loss on this task batch = 2.6831e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 126.6627, GNorm = 0.2198
Meta loss on this task batch = 2.9058e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 126.6685, GNorm = 0.2936
Took 114.72788095474243 seconds to complete one epoch of meta training
Took 122.72899055480957 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469708
Epoch 428
Meta loss on this task batch = 3.2290e-01, Meta loss averaged over last 500 steps = 3.0649e-01, PNorm = 126.6743, GNorm = 0.2852
Meta loss on this task batch = 3.3068e-01, Meta loss averaged over last 500 steps = 3.0650e-01, PNorm = 126.6793, GNorm = 0.3806
Meta loss on this task batch = 2.5650e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 126.6839, GNorm = 0.2836
Meta loss on this task batch = 2.8346e-01, Meta loss averaged over last 500 steps = 3.0618e-01, PNorm = 126.6885, GNorm = 0.2563
Meta loss on this task batch = 2.9363e-01, Meta loss averaged over last 500 steps = 3.0614e-01, PNorm = 126.6933, GNorm = 0.2695
Meta loss on this task batch = 2.8554e-01, Meta loss averaged over last 500 steps = 3.0614e-01, PNorm = 126.6978, GNorm = 0.2674
Meta loss on this task batch = 3.2138e-01, Meta loss averaged over last 500 steps = 3.0626e-01, PNorm = 126.7031, GNorm = 0.4360
Meta loss on this task batch = 2.8611e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 126.7081, GNorm = 0.2747
Meta loss on this task batch = 2.9728e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 126.7131, GNorm = 0.4041
Meta loss on this task batch = 3.2969e-01, Meta loss averaged over last 500 steps = 3.0622e-01, PNorm = 126.7181, GNorm = 0.2795
Meta loss on this task batch = 3.4119e-01, Meta loss averaged over last 500 steps = 3.0626e-01, PNorm = 126.7225, GNorm = 0.3597
Meta loss on this task batch = 3.0216e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 126.7268, GNorm = 0.2769
Meta loss on this task batch = 3.3896e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 126.7311, GNorm = 0.2903
Meta loss on this task batch = 3.0829e-01, Meta loss averaged over last 500 steps = 3.0629e-01, PNorm = 126.7351, GNorm = 0.2818
Meta loss on this task batch = 3.3195e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 126.7384, GNorm = 0.2811
Meta loss on this task batch = 2.8176e-01, Meta loss averaged over last 500 steps = 3.0625e-01, PNorm = 126.7425, GNorm = 0.2765
Meta loss on this task batch = 3.4673e-01, Meta loss averaged over last 500 steps = 3.0633e-01, PNorm = 126.7470, GNorm = 0.2640
Meta loss on this task batch = 2.9042e-01, Meta loss averaged over last 500 steps = 3.0634e-01, PNorm = 126.7515, GNorm = 0.2773
Meta loss on this task batch = 3.2904e-01, Meta loss averaged over last 500 steps = 3.0648e-01, PNorm = 126.7559, GNorm = 0.4231
Took 112.48531818389893 seconds to complete one epoch of meta training
Took 120.15364861488342 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466412
Epoch 429
Meta loss on this task batch = 2.7787e-01, Meta loss averaged over last 500 steps = 3.0639e-01, PNorm = 126.7608, GNorm = 0.2473
Meta loss on this task batch = 2.8933e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 126.7662, GNorm = 0.2456
Meta loss on this task batch = 2.6834e-01, Meta loss averaged over last 500 steps = 3.0629e-01, PNorm = 126.7715, GNorm = 0.2745
Meta loss on this task batch = 2.9018e-01, Meta loss averaged over last 500 steps = 3.0629e-01, PNorm = 126.7770, GNorm = 0.2632
Meta loss on this task batch = 3.0203e-01, Meta loss averaged over last 500 steps = 3.0643e-01, PNorm = 126.7829, GNorm = 0.2438
Meta loss on this task batch = 2.7567e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 126.7880, GNorm = 0.3600
Meta loss on this task batch = 3.6160e-01, Meta loss averaged over last 500 steps = 3.0649e-01, PNorm = 126.7919, GNorm = 0.4029
Meta loss on this task batch = 2.8716e-01, Meta loss averaged over last 500 steps = 3.0640e-01, PNorm = 126.7966, GNorm = 0.3895
Meta loss on this task batch = 3.3042e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 126.8002, GNorm = 0.2812
Meta loss on this task batch = 3.0924e-01, Meta loss averaged over last 500 steps = 3.0639e-01, PNorm = 126.8047, GNorm = 0.2605
Meta loss on this task batch = 3.3416e-01, Meta loss averaged over last 500 steps = 3.0647e-01, PNorm = 126.8095, GNorm = 0.2654
Meta loss on this task batch = 2.9744e-01, Meta loss averaged over last 500 steps = 3.0640e-01, PNorm = 126.8143, GNorm = 0.2592
Meta loss on this task batch = 3.5065e-01, Meta loss averaged over last 500 steps = 3.0652e-01, PNorm = 126.8185, GNorm = 0.3463
Meta loss on this task batch = 3.2738e-01, Meta loss averaged over last 500 steps = 3.0662e-01, PNorm = 126.8239, GNorm = 0.5805
Meta loss on this task batch = 2.9265e-01, Meta loss averaged over last 500 steps = 3.0671e-01, PNorm = 126.8293, GNorm = 0.3876
Meta loss on this task batch = 2.6117e-01, Meta loss averaged over last 500 steps = 3.0667e-01, PNorm = 126.8352, GNorm = 0.2694
Meta loss on this task batch = 3.7537e-01, Meta loss averaged over last 500 steps = 3.0676e-01, PNorm = 126.8416, GNorm = 0.2840
Meta loss on this task batch = 3.2542e-01, Meta loss averaged over last 500 steps = 3.0680e-01, PNorm = 126.8486, GNorm = 0.3515
Meta loss on this task batch = 2.5597e-01, Meta loss averaged over last 500 steps = 3.0673e-01, PNorm = 126.8565, GNorm = 0.3345
Took 107.72066330909729 seconds to complete one epoch of meta training
Took 115.74891352653503 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495631
Epoch 430
Meta loss on this task batch = 3.4591e-01, Meta loss averaged over last 500 steps = 3.0667e-01, PNorm = 126.8638, GNorm = 0.2899
Meta loss on this task batch = 2.9498e-01, Meta loss averaged over last 500 steps = 3.0659e-01, PNorm = 126.8704, GNorm = 0.2663
Meta loss on this task batch = 2.8927e-01, Meta loss averaged over last 500 steps = 3.0660e-01, PNorm = 126.8771, GNorm = 0.2680
Meta loss on this task batch = 3.1688e-01, Meta loss averaged over last 500 steps = 3.0660e-01, PNorm = 126.8845, GNorm = 0.2878
Meta loss on this task batch = 2.9960e-01, Meta loss averaged over last 500 steps = 3.0663e-01, PNorm = 126.8919, GNorm = 0.3444
Meta loss on this task batch = 3.2377e-01, Meta loss averaged over last 500 steps = 3.0656e-01, PNorm = 126.8995, GNorm = 0.3543
Meta loss on this task batch = 2.7389e-01, Meta loss averaged over last 500 steps = 3.0650e-01, PNorm = 126.9070, GNorm = 0.2766
Meta loss on this task batch = 2.8712e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 126.9143, GNorm = 0.3398
Meta loss on this task batch = 2.8833e-01, Meta loss averaged over last 500 steps = 3.0629e-01, PNorm = 126.9210, GNorm = 0.2734
Meta loss on this task batch = 3.4452e-01, Meta loss averaged over last 500 steps = 3.0648e-01, PNorm = 126.9262, GNorm = 0.3243
Meta loss on this task batch = 3.1948e-01, Meta loss averaged over last 500 steps = 3.0648e-01, PNorm = 126.9314, GNorm = 0.3113
Meta loss on this task batch = 2.7832e-01, Meta loss averaged over last 500 steps = 3.0648e-01, PNorm = 126.9365, GNorm = 0.2690
Meta loss on this task batch = 3.1270e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 126.9412, GNorm = 0.2861
Meta loss on this task batch = 3.1422e-01, Meta loss averaged over last 500 steps = 3.0639e-01, PNorm = 126.9458, GNorm = 0.2907
Meta loss on this task batch = 2.9288e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 126.9509, GNorm = 0.2277
Meta loss on this task batch = 3.3376e-01, Meta loss averaged over last 500 steps = 3.0650e-01, PNorm = 126.9561, GNorm = 0.3080
Meta loss on this task batch = 2.9492e-01, Meta loss averaged over last 500 steps = 3.0640e-01, PNorm = 126.9609, GNorm = 0.2589
Meta loss on this task batch = 2.9102e-01, Meta loss averaged over last 500 steps = 3.0654e-01, PNorm = 126.9666, GNorm = 0.2858
Meta loss on this task batch = 3.1707e-01, Meta loss averaged over last 500 steps = 3.0665e-01, PNorm = 126.9732, GNorm = 0.2984
Took 113.34526515007019 seconds to complete one epoch of meta training
Took 121.1639814376831 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474389
Epoch 431
Meta loss on this task batch = 3.1883e-01, Meta loss averaged over last 500 steps = 3.0664e-01, PNorm = 126.9790, GNorm = 0.2552
Meta loss on this task batch = 2.6359e-01, Meta loss averaged over last 500 steps = 3.0650e-01, PNorm = 126.9844, GNorm = 0.3964
Meta loss on this task batch = 3.2422e-01, Meta loss averaged over last 500 steps = 3.0648e-01, PNorm = 126.9895, GNorm = 0.3728
Meta loss on this task batch = 2.9037e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 126.9953, GNorm = 0.3126
Meta loss on this task batch = 2.7477e-01, Meta loss averaged over last 500 steps = 3.0624e-01, PNorm = 127.0010, GNorm = 0.2766
Meta loss on this task batch = 2.5849e-01, Meta loss averaged over last 500 steps = 3.0606e-01, PNorm = 127.0068, GNorm = 0.3109
Meta loss on this task batch = 3.1606e-01, Meta loss averaged over last 500 steps = 3.0610e-01, PNorm = 127.0125, GNorm = 0.3418
Meta loss on this task batch = 3.1910e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 127.0179, GNorm = 0.3330
Meta loss on this task batch = 2.9258e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 127.0232, GNorm = 0.3093
Meta loss on this task batch = 3.6144e-01, Meta loss averaged over last 500 steps = 3.0625e-01, PNorm = 127.0277, GNorm = 0.3608
Meta loss on this task batch = 3.6886e-01, Meta loss averaged over last 500 steps = 3.0631e-01, PNorm = 127.0323, GNorm = 0.3246
Meta loss on this task batch = 3.2380e-01, Meta loss averaged over last 500 steps = 3.0638e-01, PNorm = 127.0361, GNorm = 0.2669
Meta loss on this task batch = 2.9402e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 127.0400, GNorm = 0.4327
Meta loss on this task batch = 2.6243e-01, Meta loss averaged over last 500 steps = 3.0621e-01, PNorm = 127.0449, GNorm = 0.2461
Meta loss on this task batch = 3.6128e-01, Meta loss averaged over last 500 steps = 3.0641e-01, PNorm = 127.0492, GNorm = 0.3439
Meta loss on this task batch = 3.1524e-01, Meta loss averaged over last 500 steps = 3.0638e-01, PNorm = 127.0544, GNorm = 0.2719
Meta loss on this task batch = 2.7724e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 127.0603, GNorm = 0.2868
Meta loss on this task batch = 3.0549e-01, Meta loss averaged over last 500 steps = 3.0643e-01, PNorm = 127.0663, GNorm = 0.2887
Meta loss on this task batch = 2.8135e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 127.0742, GNorm = 0.4290
Took 112.85144686698914 seconds to complete one epoch of meta training
Took 120.00291180610657 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482557
Epoch 432
Meta loss on this task batch = 2.8271e-01, Meta loss averaged over last 500 steps = 3.0640e-01, PNorm = 127.0834, GNorm = 0.2954
Meta loss on this task batch = 3.0993e-01, Meta loss averaged over last 500 steps = 3.0633e-01, PNorm = 127.0931, GNorm = 0.2811
Meta loss on this task batch = 2.9180e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 127.1028, GNorm = 0.2425
Meta loss on this task batch = 3.4152e-01, Meta loss averaged over last 500 steps = 3.0633e-01, PNorm = 127.1125, GNorm = 0.2827
Meta loss on this task batch = 3.6667e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 127.1205, GNorm = 0.3932
Meta loss on this task batch = 2.7069e-01, Meta loss averaged over last 500 steps = 3.0639e-01, PNorm = 127.1281, GNorm = 0.2554
Meta loss on this task batch = 2.4406e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 127.1357, GNorm = 0.2633
Meta loss on this task batch = 3.1666e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 127.1424, GNorm = 0.4713
Meta loss on this task batch = 3.1227e-01, Meta loss averaged over last 500 steps = 3.0643e-01, PNorm = 127.1486, GNorm = 0.2688
Meta loss on this task batch = 3.2066e-01, Meta loss averaged over last 500 steps = 3.0645e-01, PNorm = 127.1534, GNorm = 0.2922
Meta loss on this task batch = 3.1020e-01, Meta loss averaged over last 500 steps = 3.0638e-01, PNorm = 127.1580, GNorm = 0.2825
Meta loss on this task batch = 3.0661e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 127.1624, GNorm = 0.4044
Meta loss on this task batch = 3.0106e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 127.1670, GNorm = 0.3773
Meta loss on this task batch = 2.7939e-01, Meta loss averaged over last 500 steps = 3.0633e-01, PNorm = 127.1704, GNorm = 0.3153
Meta loss on this task batch = 3.7991e-01, Meta loss averaged over last 500 steps = 3.0650e-01, PNorm = 127.1721, GNorm = 0.3372
Meta loss on this task batch = 3.2031e-01, Meta loss averaged over last 500 steps = 3.0660e-01, PNorm = 127.1748, GNorm = 0.2734
Meta loss on this task batch = 2.9434e-01, Meta loss averaged over last 500 steps = 3.0653e-01, PNorm = 127.1780, GNorm = 0.2588
Meta loss on this task batch = 2.8334e-01, Meta loss averaged over last 500 steps = 3.0652e-01, PNorm = 127.1825, GNorm = 0.2466
Meta loss on this task batch = 2.9241e-01, Meta loss averaged over last 500 steps = 3.0654e-01, PNorm = 127.1880, GNorm = 0.3237
Took 111.02159261703491 seconds to complete one epoch of meta training
Took 119.00987911224365 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470743
Epoch 433
Meta loss on this task batch = 2.6948e-01, Meta loss averaged over last 500 steps = 3.0649e-01, PNorm = 127.1939, GNorm = 0.4104
Meta loss on this task batch = 3.1130e-01, Meta loss averaged over last 500 steps = 3.0652e-01, PNorm = 127.2002, GNorm = 0.2877
Meta loss on this task batch = 2.8293e-01, Meta loss averaged over last 500 steps = 3.0649e-01, PNorm = 127.2063, GNorm = 0.3377
Meta loss on this task batch = 2.7061e-01, Meta loss averaged over last 500 steps = 3.0634e-01, PNorm = 127.2115, GNorm = 0.3203
Meta loss on this task batch = 3.2832e-01, Meta loss averaged over last 500 steps = 3.0634e-01, PNorm = 127.2163, GNorm = 0.2735
Meta loss on this task batch = 3.2024e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 127.2203, GNorm = 0.4956
Meta loss on this task batch = 3.1008e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 127.2241, GNorm = 0.2525
Meta loss on this task batch = 3.5586e-01, Meta loss averaged over last 500 steps = 3.0655e-01, PNorm = 127.2274, GNorm = 0.5593
Meta loss on this task batch = 3.1660e-01, Meta loss averaged over last 500 steps = 3.0652e-01, PNorm = 127.2315, GNorm = 0.2914
Meta loss on this task batch = 3.9717e-01, Meta loss averaged over last 500 steps = 3.0677e-01, PNorm = 127.2347, GNorm = 0.3323
Meta loss on this task batch = 2.8398e-01, Meta loss averaged over last 500 steps = 3.0674e-01, PNorm = 127.2388, GNorm = 0.2618
Meta loss on this task batch = 3.0202e-01, Meta loss averaged over last 500 steps = 3.0665e-01, PNorm = 127.2419, GNorm = 0.2710
Meta loss on this task batch = 2.7806e-01, Meta loss averaged over last 500 steps = 3.0655e-01, PNorm = 127.2456, GNorm = 0.3787
Meta loss on this task batch = 3.5319e-01, Meta loss averaged over last 500 steps = 3.0669e-01, PNorm = 127.2502, GNorm = 0.6653
Meta loss on this task batch = 3.5597e-01, Meta loss averaged over last 500 steps = 3.0678e-01, PNorm = 127.2547, GNorm = 0.3309
Meta loss on this task batch = 3.4045e-01, Meta loss averaged over last 500 steps = 3.0680e-01, PNorm = 127.2603, GNorm = 0.3018
Meta loss on this task batch = 2.8025e-01, Meta loss averaged over last 500 steps = 3.0683e-01, PNorm = 127.2664, GNorm = 0.2263
Meta loss on this task batch = 2.7363e-01, Meta loss averaged over last 500 steps = 3.0675e-01, PNorm = 127.2731, GNorm = 0.2478
Meta loss on this task batch = 2.8389e-01, Meta loss averaged over last 500 steps = 3.0668e-01, PNorm = 127.2810, GNorm = 0.3729
Took 110.52033877372742 seconds to complete one epoch of meta training
Took 118.19611644744873 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448963
Epoch 434
Meta loss on this task batch = 2.9276e-01, Meta loss averaged over last 500 steps = 3.0655e-01, PNorm = 127.2887, GNorm = 0.2369
Meta loss on this task batch = 2.7238e-01, Meta loss averaged over last 500 steps = 3.0640e-01, PNorm = 127.2974, GNorm = 0.2224
Meta loss on this task batch = 3.2232e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 127.3058, GNorm = 0.2662
Meta loss on this task batch = 2.7478e-01, Meta loss averaged over last 500 steps = 3.0623e-01, PNorm = 127.3143, GNorm = 0.2703
Meta loss on this task batch = 3.1778e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 127.3231, GNorm = 0.2434
Meta loss on this task batch = 3.4589e-01, Meta loss averaged over last 500 steps = 3.0652e-01, PNorm = 127.3304, GNorm = 0.2606
Meta loss on this task batch = 2.7834e-01, Meta loss averaged over last 500 steps = 3.0649e-01, PNorm = 127.3371, GNorm = 0.3156
Meta loss on this task batch = 3.4325e-01, Meta loss averaged over last 500 steps = 3.0644e-01, PNorm = 127.3426, GNorm = 0.3573
Meta loss on this task batch = 3.3230e-01, Meta loss averaged over last 500 steps = 3.0648e-01, PNorm = 127.3476, GNorm = 0.3064
Meta loss on this task batch = 2.9589e-01, Meta loss averaged over last 500 steps = 3.0656e-01, PNorm = 127.3527, GNorm = 0.3541
Meta loss on this task batch = 2.7632e-01, Meta loss averaged over last 500 steps = 3.0646e-01, PNorm = 127.3569, GNorm = 0.2658
Meta loss on this task batch = 3.4831e-01, Meta loss averaged over last 500 steps = 3.0648e-01, PNorm = 127.3601, GNorm = 0.3515
Meta loss on this task batch = 3.2566e-01, Meta loss averaged over last 500 steps = 3.0647e-01, PNorm = 127.3631, GNorm = 0.2587
Meta loss on this task batch = 3.4938e-01, Meta loss averaged over last 500 steps = 3.0656e-01, PNorm = 127.3663, GNorm = 0.4242
Meta loss on this task batch = 3.4380e-01, Meta loss averaged over last 500 steps = 3.0668e-01, PNorm = 127.3705, GNorm = 0.3399
Meta loss on this task batch = 2.7084e-01, Meta loss averaged over last 500 steps = 3.0668e-01, PNorm = 127.3755, GNorm = 0.2668
Meta loss on this task batch = 3.2830e-01, Meta loss averaged over last 500 steps = 3.0677e-01, PNorm = 127.3804, GNorm = 0.2875
Meta loss on this task batch = 2.8337e-01, Meta loss averaged over last 500 steps = 3.0681e-01, PNorm = 127.3853, GNorm = 0.2954
Meta loss on this task batch = 3.0844e-01, Meta loss averaged over last 500 steps = 3.0683e-01, PNorm = 127.3901, GNorm = 0.3389
Took 112.95531845092773 seconds to complete one epoch of meta training
Took 120.74737358093262 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451017
Epoch 435
Meta loss on this task batch = 2.4662e-01, Meta loss averaged over last 500 steps = 3.0667e-01, PNorm = 127.3963, GNorm = 0.3756
Meta loss on this task batch = 3.1535e-01, Meta loss averaged over last 500 steps = 3.0670e-01, PNorm = 127.4029, GNorm = 0.3254
Meta loss on this task batch = 3.2856e-01, Meta loss averaged over last 500 steps = 3.0677e-01, PNorm = 127.4093, GNorm = 0.2902
Meta loss on this task batch = 2.6049e-01, Meta loss averaged over last 500 steps = 3.0656e-01, PNorm = 127.4161, GNorm = 0.2878
Meta loss on this task batch = 3.0346e-01, Meta loss averaged over last 500 steps = 3.0652e-01, PNorm = 127.4228, GNorm = 0.2750
Meta loss on this task batch = 3.2837e-01, Meta loss averaged over last 500 steps = 3.0657e-01, PNorm = 127.4287, GNorm = 0.4626
Meta loss on this task batch = 2.9950e-01, Meta loss averaged over last 500 steps = 3.0655e-01, PNorm = 127.4352, GNorm = 0.3554
Meta loss on this task batch = 2.9836e-01, Meta loss averaged over last 500 steps = 3.0650e-01, PNorm = 127.4416, GNorm = 0.2708
Meta loss on this task batch = 2.5193e-01, Meta loss averaged over last 500 steps = 3.0633e-01, PNorm = 127.4488, GNorm = 0.2522
Meta loss on this task batch = 3.1698e-01, Meta loss averaged over last 500 steps = 3.0640e-01, PNorm = 127.4566, GNorm = 0.2925
Meta loss on this task batch = 3.6457e-01, Meta loss averaged over last 500 steps = 3.0661e-01, PNorm = 127.4639, GNorm = 0.4430
Meta loss on this task batch = 2.8619e-01, Meta loss averaged over last 500 steps = 3.0657e-01, PNorm = 127.4703, GNorm = 0.2683
Meta loss on this task batch = 3.2077e-01, Meta loss averaged over last 500 steps = 3.0655e-01, PNorm = 127.4763, GNorm = 0.4621
Meta loss on this task batch = 3.0262e-01, Meta loss averaged over last 500 steps = 3.0649e-01, PNorm = 127.4821, GNorm = 0.2618
Meta loss on this task batch = 2.9359e-01, Meta loss averaged over last 500 steps = 3.0629e-01, PNorm = 127.4875, GNorm = 0.2543
Meta loss on this task batch = 3.1824e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 127.4934, GNorm = 0.3355
Meta loss on this task batch = 3.1706e-01, Meta loss averaged over last 500 steps = 3.0664e-01, PNorm = 127.4991, GNorm = 0.2793
Meta loss on this task batch = 2.7263e-01, Meta loss averaged over last 500 steps = 3.0658e-01, PNorm = 127.5050, GNorm = 0.2563
Meta loss on this task batch = 3.3827e-01, Meta loss averaged over last 500 steps = 3.0660e-01, PNorm = 127.5106, GNorm = 0.5400
Took 113.30110955238342 seconds to complete one epoch of meta training
Took 121.56550931930542 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471099
Epoch 436
Meta loss on this task batch = 2.7644e-01, Meta loss averaged over last 500 steps = 3.0652e-01, PNorm = 127.5160, GNorm = 0.2856
Meta loss on this task batch = 2.9153e-01, Meta loss averaged over last 500 steps = 3.0638e-01, PNorm = 127.5208, GNorm = 0.3427
Meta loss on this task batch = 2.3480e-01, Meta loss averaged over last 500 steps = 3.0626e-01, PNorm = 127.5268, GNorm = 0.3058
Meta loss on this task batch = 3.1610e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 127.5337, GNorm = 0.3186
Meta loss on this task batch = 2.9866e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 127.5411, GNorm = 0.2444
Meta loss on this task batch = 2.7481e-01, Meta loss averaged over last 500 steps = 3.0622e-01, PNorm = 127.5485, GNorm = 0.3353
Meta loss on this task batch = 2.8625e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 127.5563, GNorm = 0.3491
Meta loss on this task batch = 2.9676e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 127.5643, GNorm = 0.3181
Meta loss on this task batch = 3.4981e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 127.5712, GNorm = 0.3180
Meta loss on this task batch = 3.1992e-01, Meta loss averaged over last 500 steps = 3.0612e-01, PNorm = 127.5767, GNorm = 0.2798
Meta loss on this task batch = 3.6897e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 127.5816, GNorm = 0.2685
Meta loss on this task batch = 3.6020e-01, Meta loss averaged over last 500 steps = 3.0631e-01, PNorm = 127.5850, GNorm = 0.3700
Meta loss on this task batch = 3.1800e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 127.5887, GNorm = 0.2510
Meta loss on this task batch = 3.0220e-01, Meta loss averaged over last 500 steps = 3.0629e-01, PNorm = 127.5922, GNorm = 0.2865
Meta loss on this task batch = 3.0185e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 127.5968, GNorm = 0.2726
Meta loss on this task batch = 2.9592e-01, Meta loss averaged over last 500 steps = 3.0641e-01, PNorm = 127.6013, GNorm = 0.2597
Meta loss on this task batch = 2.8500e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 127.6053, GNorm = 0.2908
Meta loss on this task batch = 2.8866e-01, Meta loss averaged over last 500 steps = 3.0643e-01, PNorm = 127.6089, GNorm = 0.3494
Meta loss on this task batch = 2.3716e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 127.6135, GNorm = 0.2824
Took 114.28391194343567 seconds to complete one epoch of meta training
Took 122.26768469810486 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456344
Epoch 437
Meta loss on this task batch = 2.6381e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 127.6187, GNorm = 0.3000
Meta loss on this task batch = 3.0574e-01, Meta loss averaged over last 500 steps = 3.0618e-01, PNorm = 127.6242, GNorm = 0.2854
Meta loss on this task batch = 3.7111e-01, Meta loss averaged over last 500 steps = 3.0626e-01, PNorm = 127.6294, GNorm = 0.3144
Meta loss on this task batch = 2.3395e-01, Meta loss averaged over last 500 steps = 3.0608e-01, PNorm = 127.6347, GNorm = 0.2191
Meta loss on this task batch = 2.9739e-01, Meta loss averaged over last 500 steps = 3.0612e-01, PNorm = 127.6401, GNorm = 0.3055
Meta loss on this task batch = 3.8018e-01, Meta loss averaged over last 500 steps = 3.0621e-01, PNorm = 127.6443, GNorm = 0.4090
Meta loss on this task batch = 3.1200e-01, Meta loss averaged over last 500 steps = 3.0622e-01, PNorm = 127.6494, GNorm = 0.3703
Meta loss on this task batch = 3.0843e-01, Meta loss averaged over last 500 steps = 3.0621e-01, PNorm = 127.6547, GNorm = 0.3477
Meta loss on this task batch = 2.5291e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 127.6604, GNorm = 0.2728
Meta loss on this task batch = 3.3891e-01, Meta loss averaged over last 500 steps = 3.0620e-01, PNorm = 127.6659, GNorm = 0.2650
Meta loss on this task batch = 3.1070e-01, Meta loss averaged over last 500 steps = 3.0629e-01, PNorm = 127.6717, GNorm = 0.3187
Meta loss on this task batch = 3.0367e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 127.6778, GNorm = 0.2941
Meta loss on this task batch = 2.6760e-01, Meta loss averaged over last 500 steps = 3.0614e-01, PNorm = 127.6826, GNorm = 0.3233
Meta loss on this task batch = 2.4581e-01, Meta loss averaged over last 500 steps = 3.0601e-01, PNorm = 127.6873, GNorm = 0.2234
Meta loss on this task batch = 3.5998e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 127.6908, GNorm = 0.3535
Meta loss on this task batch = 2.9994e-01, Meta loss averaged over last 500 steps = 3.0599e-01, PNorm = 127.6939, GNorm = 0.2542
Meta loss on this task batch = 2.8132e-01, Meta loss averaged over last 500 steps = 3.0605e-01, PNorm = 127.6981, GNorm = 0.2369
Meta loss on this task batch = 2.8366e-01, Meta loss averaged over last 500 steps = 3.0594e-01, PNorm = 127.7026, GNorm = 0.3321
Meta loss on this task batch = 3.6678e-01, Meta loss averaged over last 500 steps = 3.0595e-01, PNorm = 127.7071, GNorm = 0.5531
Took 111.20979046821594 seconds to complete one epoch of meta training
Took 118.3616898059845 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477010
Epoch 438
Meta loss on this task batch = 3.1454e-01, Meta loss averaged over last 500 steps = 3.0603e-01, PNorm = 127.7119, GNorm = 0.2861
Meta loss on this task batch = 2.8429e-01, Meta loss averaged over last 500 steps = 3.0596e-01, PNorm = 127.7168, GNorm = 0.2579
Meta loss on this task batch = 2.8565e-01, Meta loss averaged over last 500 steps = 3.0592e-01, PNorm = 127.7219, GNorm = 0.4265
Meta loss on this task batch = 3.1594e-01, Meta loss averaged over last 500 steps = 3.0596e-01, PNorm = 127.7271, GNorm = 0.3767
Meta loss on this task batch = 2.9737e-01, Meta loss averaged over last 500 steps = 3.0584e-01, PNorm = 127.7318, GNorm = 0.3602
Meta loss on this task batch = 3.0982e-01, Meta loss averaged over last 500 steps = 3.0585e-01, PNorm = 127.7378, GNorm = 0.3774
Meta loss on this task batch = 3.3851e-01, Meta loss averaged over last 500 steps = 3.0596e-01, PNorm = 127.7434, GNorm = 0.2846
Meta loss on this task batch = 3.1059e-01, Meta loss averaged over last 500 steps = 3.0601e-01, PNorm = 127.7492, GNorm = 0.2953
Meta loss on this task batch = 3.1381e-01, Meta loss averaged over last 500 steps = 3.0603e-01, PNorm = 127.7560, GNorm = 0.3166
Meta loss on this task batch = 2.6010e-01, Meta loss averaged over last 500 steps = 3.0602e-01, PNorm = 127.7630, GNorm = 0.2822
Meta loss on this task batch = 3.0420e-01, Meta loss averaged over last 500 steps = 3.0606e-01, PNorm = 127.7697, GNorm = 0.4037
Meta loss on this task batch = 3.0234e-01, Meta loss averaged over last 500 steps = 3.0603e-01, PNorm = 127.7765, GNorm = 0.3415
Meta loss on this task batch = 3.1777e-01, Meta loss averaged over last 500 steps = 3.0597e-01, PNorm = 127.7830, GNorm = 0.2759
Meta loss on this task batch = 3.2391e-01, Meta loss averaged over last 500 steps = 3.0594e-01, PNorm = 127.7892, GNorm = 0.3732
Meta loss on this task batch = 2.8269e-01, Meta loss averaged over last 500 steps = 3.0589e-01, PNorm = 127.7953, GNorm = 0.2352
Meta loss on this task batch = 3.2476e-01, Meta loss averaged over last 500 steps = 3.0600e-01, PNorm = 127.8019, GNorm = 0.2739
Meta loss on this task batch = 3.0236e-01, Meta loss averaged over last 500 steps = 3.0602e-01, PNorm = 127.8084, GNorm = 0.5295
Meta loss on this task batch = 3.0040e-01, Meta loss averaged over last 500 steps = 3.0599e-01, PNorm = 127.8147, GNorm = 0.3856
Meta loss on this task batch = 2.9431e-01, Meta loss averaged over last 500 steps = 3.0598e-01, PNorm = 127.8213, GNorm = 0.3452
Took 113.47938799858093 seconds to complete one epoch of meta training
Took 121.90398001670837 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466848
Epoch 439
Meta loss on this task batch = 3.5108e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 127.8286, GNorm = 0.3187
Meta loss on this task batch = 3.2111e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 127.8360, GNorm = 0.3403
Meta loss on this task batch = 3.4468e-01, Meta loss averaged over last 500 steps = 3.0627e-01, PNorm = 127.8431, GNorm = 0.2859
Meta loss on this task batch = 3.5517e-01, Meta loss averaged over last 500 steps = 3.0641e-01, PNorm = 127.8502, GNorm = 0.5418
Meta loss on this task batch = 3.1895e-01, Meta loss averaged over last 500 steps = 3.0644e-01, PNorm = 127.8571, GNorm = 0.2600
Meta loss on this task batch = 2.8930e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 127.8645, GNorm = 0.3093
Meta loss on this task batch = 2.8253e-01, Meta loss averaged over last 500 steps = 3.0626e-01, PNorm = 127.8726, GNorm = 0.2580
Meta loss on this task batch = 2.8283e-01, Meta loss averaged over last 500 steps = 3.0624e-01, PNorm = 127.8812, GNorm = 0.2712
Meta loss on this task batch = 2.7196e-01, Meta loss averaged over last 500 steps = 3.0612e-01, PNorm = 127.8901, GNorm = 0.3148
Meta loss on this task batch = 3.0696e-01, Meta loss averaged over last 500 steps = 3.0604e-01, PNorm = 127.8977, GNorm = 0.3137
Meta loss on this task batch = 2.7584e-01, Meta loss averaged over last 500 steps = 3.0611e-01, PNorm = 127.9051, GNorm = 0.2889
Meta loss on this task batch = 3.1610e-01, Meta loss averaged over last 500 steps = 3.0610e-01, PNorm = 127.9122, GNorm = 0.3663
Meta loss on this task batch = 3.4710e-01, Meta loss averaged over last 500 steps = 3.0621e-01, PNorm = 127.9187, GNorm = 0.2631
Meta loss on this task batch = 2.5387e-01, Meta loss averaged over last 500 steps = 3.0609e-01, PNorm = 127.9249, GNorm = 0.2263
Meta loss on this task batch = 3.4377e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 127.9303, GNorm = 0.3915
Meta loss on this task batch = 2.8722e-01, Meta loss averaged over last 500 steps = 3.0614e-01, PNorm = 127.9357, GNorm = 0.3949
Meta loss on this task batch = 3.3094e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 127.9417, GNorm = 0.2675
Meta loss on this task batch = 3.4127e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 127.9466, GNorm = 0.3550
Meta loss on this task batch = 2.5843e-01, Meta loss averaged over last 500 steps = 3.0620e-01, PNorm = 127.9516, GNorm = 0.2735
Took 113.50019335746765 seconds to complete one epoch of meta training
Took 121.5683240890503 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466448
Epoch 440
Meta loss on this task batch = 3.6206e-01, Meta loss averaged over last 500 steps = 3.0627e-01, PNorm = 127.9552, GNorm = 0.5927
Meta loss on this task batch = 2.3572e-01, Meta loss averaged over last 500 steps = 3.0608e-01, PNorm = 127.9603, GNorm = 0.2611
Meta loss on this task batch = 3.4141e-01, Meta loss averaged over last 500 steps = 3.0609e-01, PNorm = 127.9649, GNorm = 0.2903
Meta loss on this task batch = 3.6648e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 127.9702, GNorm = 0.4496
Meta loss on this task batch = 3.2402e-01, Meta loss averaged over last 500 steps = 3.0618e-01, PNorm = 127.9767, GNorm = 0.3625
Meta loss on this task batch = 2.7244e-01, Meta loss averaged over last 500 steps = 3.0610e-01, PNorm = 127.9842, GNorm = 0.4075
Meta loss on this task batch = 3.4707e-01, Meta loss averaged over last 500 steps = 3.0614e-01, PNorm = 127.9915, GNorm = 0.3350
Meta loss on this task batch = 3.2933e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 127.9989, GNorm = 0.3434
Meta loss on this task batch = 3.0343e-01, Meta loss averaged over last 500 steps = 3.0625e-01, PNorm = 128.0063, GNorm = 0.2948
Meta loss on this task batch = 3.2046e-01, Meta loss averaged over last 500 steps = 3.0624e-01, PNorm = 128.0138, GNorm = 0.4965
Meta loss on this task batch = 2.4321e-01, Meta loss averaged over last 500 steps = 3.0622e-01, PNorm = 128.0215, GNorm = 0.2459
Meta loss on this task batch = 3.4222e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 128.0288, GNorm = 0.3090
Meta loss on this task batch = 2.8833e-01, Meta loss averaged over last 500 steps = 3.0618e-01, PNorm = 128.0365, GNorm = 0.3160
Meta loss on this task batch = 3.5687e-01, Meta loss averaged over last 500 steps = 3.0641e-01, PNorm = 128.0430, GNorm = 0.5055
Meta loss on this task batch = 2.8379e-01, Meta loss averaged over last 500 steps = 3.0634e-01, PNorm = 128.0497, GNorm = 0.2534
Meta loss on this task batch = 3.0340e-01, Meta loss averaged over last 500 steps = 3.0638e-01, PNorm = 128.0558, GNorm = 0.4195
Meta loss on this task batch = 3.5336e-01, Meta loss averaged over last 500 steps = 3.0648e-01, PNorm = 128.0609, GNorm = 0.4570
Meta loss on this task batch = 2.7171e-01, Meta loss averaged over last 500 steps = 3.0649e-01, PNorm = 128.0672, GNorm = 0.4309
Meta loss on this task batch = 2.9648e-01, Meta loss averaged over last 500 steps = 3.0643e-01, PNorm = 128.0739, GNorm = 0.3132
Took 116.3350579738617 seconds to complete one epoch of meta training
Took 123.69321584701538 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477849
Epoch 441
Meta loss on this task batch = 3.0940e-01, Meta loss averaged over last 500 steps = 3.0644e-01, PNorm = 128.0809, GNorm = 0.3332
Meta loss on this task batch = 2.9279e-01, Meta loss averaged over last 500 steps = 3.0634e-01, PNorm = 128.0879, GNorm = 0.2377
Meta loss on this task batch = 2.9858e-01, Meta loss averaged over last 500 steps = 3.0645e-01, PNorm = 128.0947, GNorm = 0.2730
Meta loss on this task batch = 2.7604e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 128.1014, GNorm = 0.2588
Meta loss on this task batch = 3.0338e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 128.1086, GNorm = 0.2531
Meta loss on this task batch = 3.1154e-01, Meta loss averaged over last 500 steps = 3.0633e-01, PNorm = 128.1157, GNorm = 0.2943
Meta loss on this task batch = 2.8816e-01, Meta loss averaged over last 500 steps = 3.0627e-01, PNorm = 128.1224, GNorm = 0.2704
Meta loss on this task batch = 2.8641e-01, Meta loss averaged over last 500 steps = 3.0623e-01, PNorm = 128.1291, GNorm = 0.2890
Meta loss on this task batch = 3.7316e-01, Meta loss averaged over last 500 steps = 3.0646e-01, PNorm = 128.1352, GNorm = 0.3580
Meta loss on this task batch = 2.9392e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 128.1412, GNorm = 0.2475
Meta loss on this task batch = 3.0619e-01, Meta loss averaged over last 500 steps = 3.0644e-01, PNorm = 128.1467, GNorm = 0.3030
Meta loss on this task batch = 3.2564e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 128.1521, GNorm = 0.3490
Meta loss on this task batch = 2.8389e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 128.1570, GNorm = 0.3715
Meta loss on this task batch = 3.0714e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 128.1622, GNorm = 0.2694
Meta loss on this task batch = 2.6906e-01, Meta loss averaged over last 500 steps = 3.0614e-01, PNorm = 128.1680, GNorm = 0.2602
Meta loss on this task batch = 2.5422e-01, Meta loss averaged over last 500 steps = 3.0608e-01, PNorm = 128.1747, GNorm = 0.2811
Meta loss on this task batch = 3.4388e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 128.1810, GNorm = 0.3655
Meta loss on this task batch = 3.1764e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 128.1866, GNorm = 0.3060
Meta loss on this task batch = 3.4543e-01, Meta loss averaged over last 500 steps = 3.0640e-01, PNorm = 128.1921, GNorm = 0.4153
Took 114.28066086769104 seconds to complete one epoch of meta training
Took 121.74867987632751 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456813
Epoch 442
Meta loss on this task batch = 3.4043e-01, Meta loss averaged over last 500 steps = 3.0651e-01, PNorm = 128.1977, GNorm = 0.3298
Meta loss on this task batch = 2.6089e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 128.2046, GNorm = 0.2418
Meta loss on this task batch = 3.5660e-01, Meta loss averaged over last 500 steps = 3.0652e-01, PNorm = 128.2109, GNorm = 0.3392
Meta loss on this task batch = 2.8849e-01, Meta loss averaged over last 500 steps = 3.0650e-01, PNorm = 128.2167, GNorm = 0.2495
Meta loss on this task batch = 3.1314e-01, Meta loss averaged over last 500 steps = 3.0650e-01, PNorm = 128.2224, GNorm = 0.3826
Meta loss on this task batch = 3.2677e-01, Meta loss averaged over last 500 steps = 3.0652e-01, PNorm = 128.2274, GNorm = 0.3582
Meta loss on this task batch = 2.9843e-01, Meta loss averaged over last 500 steps = 3.0654e-01, PNorm = 128.2327, GNorm = 0.3413
Meta loss on this task batch = 3.0483e-01, Meta loss averaged over last 500 steps = 3.0662e-01, PNorm = 128.2379, GNorm = 0.2633
Meta loss on this task batch = 2.3067e-01, Meta loss averaged over last 500 steps = 3.0641e-01, PNorm = 128.2438, GNorm = 0.2218
Meta loss on this task batch = 2.6257e-01, Meta loss averaged over last 500 steps = 3.0622e-01, PNorm = 128.2508, GNorm = 0.3277
Meta loss on this task batch = 3.1571e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 128.2578, GNorm = 0.3322
Meta loss on this task batch = 3.2318e-01, Meta loss averaged over last 500 steps = 3.0616e-01, PNorm = 128.2645, GNorm = 0.2749
Meta loss on this task batch = 2.8445e-01, Meta loss averaged over last 500 steps = 3.0624e-01, PNorm = 128.2707, GNorm = 0.2938
Meta loss on this task batch = 2.9953e-01, Meta loss averaged over last 500 steps = 3.0624e-01, PNorm = 128.2760, GNorm = 0.3342
Meta loss on this task batch = 3.6487e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 128.2802, GNorm = 0.3884
Meta loss on this task batch = 2.9725e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 128.2840, GNorm = 0.3001
Meta loss on this task batch = 3.2315e-01, Meta loss averaged over last 500 steps = 3.0639e-01, PNorm = 128.2882, GNorm = 0.2861
Meta loss on this task batch = 3.2607e-01, Meta loss averaged over last 500 steps = 3.0653e-01, PNorm = 128.2923, GNorm = 0.4337
Meta loss on this task batch = 2.4652e-01, Meta loss averaged over last 500 steps = 3.0643e-01, PNorm = 128.2976, GNorm = 0.4108
Took 113.73317384719849 seconds to complete one epoch of meta training
Took 120.74570417404175 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483669
Epoch 443
Meta loss on this task batch = 2.6719e-01, Meta loss averaged over last 500 steps = 3.0638e-01, PNorm = 128.3036, GNorm = 0.2632
Meta loss on this task batch = 3.2845e-01, Meta loss averaged over last 500 steps = 3.0644e-01, PNorm = 128.3098, GNorm = 0.4037
Meta loss on this task batch = 3.7007e-01, Meta loss averaged over last 500 steps = 3.0658e-01, PNorm = 128.3156, GNorm = 0.3645
Meta loss on this task batch = 2.8232e-01, Meta loss averaged over last 500 steps = 3.0654e-01, PNorm = 128.3217, GNorm = 0.2750
Meta loss on this task batch = 3.4489e-01, Meta loss averaged over last 500 steps = 3.0647e-01, PNorm = 128.3276, GNorm = 0.4713
Meta loss on this task batch = 2.8991e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 128.3322, GNorm = 0.2913
Meta loss on this task batch = 2.4343e-01, Meta loss averaged over last 500 steps = 3.0618e-01, PNorm = 128.3374, GNorm = 0.3107
Meta loss on this task batch = 3.2946e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 128.3429, GNorm = 0.2898
Meta loss on this task batch = 2.8390e-01, Meta loss averaged over last 500 steps = 3.0604e-01, PNorm = 128.3481, GNorm = 0.2878
Meta loss on this task batch = 2.4864e-01, Meta loss averaged over last 500 steps = 3.0586e-01, PNorm = 128.3542, GNorm = 0.2173
Meta loss on this task batch = 3.5822e-01, Meta loss averaged over last 500 steps = 3.0594e-01, PNorm = 128.3597, GNorm = 0.3598
Meta loss on this task batch = 3.2835e-01, Meta loss averaged over last 500 steps = 3.0606e-01, PNorm = 128.3653, GNorm = 0.2556
Meta loss on this task batch = 3.1536e-01, Meta loss averaged over last 500 steps = 3.0598e-01, PNorm = 128.3697, GNorm = 0.3068
Meta loss on this task batch = 3.3949e-01, Meta loss averaged over last 500 steps = 3.0608e-01, PNorm = 128.3733, GNorm = 0.2908
Meta loss on this task batch = 3.3307e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 128.3760, GNorm = 0.3092
Meta loss on this task batch = 2.6632e-01, Meta loss averaged over last 500 steps = 3.0611e-01, PNorm = 128.3788, GNorm = 0.2612
Meta loss on this task batch = 2.6486e-01, Meta loss averaged over last 500 steps = 3.0599e-01, PNorm = 128.3828, GNorm = 0.2649
Meta loss on this task batch = 3.1581e-01, Meta loss averaged over last 500 steps = 3.0605e-01, PNorm = 128.3864, GNorm = 0.3000
Meta loss on this task batch = 3.6914e-01, Meta loss averaged over last 500 steps = 3.0624e-01, PNorm = 128.3902, GNorm = 0.3380
Took 110.76608777046204 seconds to complete one epoch of meta training
Took 118.74802994728088 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481810
Epoch 444
Meta loss on this task batch = 2.9327e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 128.3947, GNorm = 0.2941
Meta loss on this task batch = 3.1361e-01, Meta loss averaged over last 500 steps = 3.0633e-01, PNorm = 128.3993, GNorm = 0.3458
Meta loss on this task batch = 3.3620e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 128.4038, GNorm = 0.2533
Meta loss on this task batch = 3.1292e-01, Meta loss averaged over last 500 steps = 3.0641e-01, PNorm = 128.4077, GNorm = 0.2851
Meta loss on this task batch = 3.1404e-01, Meta loss averaged over last 500 steps = 3.0644e-01, PNorm = 128.4111, GNorm = 0.4366
Meta loss on this task batch = 2.9911e-01, Meta loss averaged over last 500 steps = 3.0641e-01, PNorm = 128.4143, GNorm = 0.2686
Meta loss on this task batch = 3.1410e-01, Meta loss averaged over last 500 steps = 3.0634e-01, PNorm = 128.4176, GNorm = 0.3380
Meta loss on this task batch = 2.8613e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 128.4211, GNorm = 0.2647
Meta loss on this task batch = 3.3608e-01, Meta loss averaged over last 500 steps = 3.0638e-01, PNorm = 128.4253, GNorm = 0.3746
Meta loss on this task batch = 2.9971e-01, Meta loss averaged over last 500 steps = 3.0633e-01, PNorm = 128.4306, GNorm = 0.2889
Meta loss on this task batch = 3.1326e-01, Meta loss averaged over last 500 steps = 3.0634e-01, PNorm = 128.4362, GNorm = 0.5545
Meta loss on this task batch = 2.8883e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 128.4424, GNorm = 0.2693
Meta loss on this task batch = 2.6511e-01, Meta loss averaged over last 500 steps = 3.0621e-01, PNorm = 128.4493, GNorm = 0.2257
Meta loss on this task batch = 2.9413e-01, Meta loss averaged over last 500 steps = 3.0620e-01, PNorm = 128.4565, GNorm = 0.2923
Meta loss on this task batch = 3.2727e-01, Meta loss averaged over last 500 steps = 3.0623e-01, PNorm = 128.4635, GNorm = 0.2709
Meta loss on this task batch = 2.4032e-01, Meta loss averaged over last 500 steps = 3.0607e-01, PNorm = 128.4708, GNorm = 0.2742
Meta loss on this task batch = 3.0299e-01, Meta loss averaged over last 500 steps = 3.0614e-01, PNorm = 128.4783, GNorm = 0.3424
Meta loss on this task batch = 3.4333e-01, Meta loss averaged over last 500 steps = 3.0627e-01, PNorm = 128.4853, GNorm = 0.3228
Meta loss on this task batch = 3.5774e-01, Meta loss averaged over last 500 steps = 3.0639e-01, PNorm = 128.4914, GNorm = 0.5557
Took 113.95534443855286 seconds to complete one epoch of meta training
Took 122.18912696838379 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489363
Epoch 445
Meta loss on this task batch = 2.9700e-01, Meta loss averaged over last 500 steps = 3.0647e-01, PNorm = 128.4969, GNorm = 0.3290
Meta loss on this task batch = 3.6564e-01, Meta loss averaged over last 500 steps = 3.0666e-01, PNorm = 128.5027, GNorm = 0.3895
Meta loss on this task batch = 3.8051e-01, Meta loss averaged over last 500 steps = 3.0673e-01, PNorm = 128.5076, GNorm = 0.3218
Meta loss on this task batch = 3.2886e-01, Meta loss averaged over last 500 steps = 3.0664e-01, PNorm = 128.5129, GNorm = 0.2625
Meta loss on this task batch = 2.7366e-01, Meta loss averaged over last 500 steps = 3.0653e-01, PNorm = 128.5181, GNorm = 0.2251
Meta loss on this task batch = 3.1232e-01, Meta loss averaged over last 500 steps = 3.0654e-01, PNorm = 128.5236, GNorm = 0.2448
Meta loss on this task batch = 3.1901e-01, Meta loss averaged over last 500 steps = 3.0654e-01, PNorm = 128.5286, GNorm = 0.3283
Meta loss on this task batch = 2.9590e-01, Meta loss averaged over last 500 steps = 3.0658e-01, PNorm = 128.5339, GNorm = 0.2583
Meta loss on this task batch = 2.9657e-01, Meta loss averaged over last 500 steps = 3.0655e-01, PNorm = 128.5397, GNorm = 0.2876
Meta loss on this task batch = 3.3182e-01, Meta loss averaged over last 500 steps = 3.0664e-01, PNorm = 128.5449, GNorm = 0.2890
Meta loss on this task batch = 2.6342e-01, Meta loss averaged over last 500 steps = 3.0656e-01, PNorm = 128.5505, GNorm = 0.2478
Meta loss on this task batch = 3.2310e-01, Meta loss averaged over last 500 steps = 3.0664e-01, PNorm = 128.5565, GNorm = 0.2976
Meta loss on this task batch = 2.8701e-01, Meta loss averaged over last 500 steps = 3.0671e-01, PNorm = 128.5629, GNorm = 0.3129
Meta loss on this task batch = 3.5369e-01, Meta loss averaged over last 500 steps = 3.0663e-01, PNorm = 128.5700, GNorm = 0.3648
Meta loss on this task batch = 2.4210e-01, Meta loss averaged over last 500 steps = 3.0648e-01, PNorm = 128.5766, GNorm = 0.2928
Meta loss on this task batch = 3.4941e-01, Meta loss averaged over last 500 steps = 3.0651e-01, PNorm = 128.5823, GNorm = 0.3280
Meta loss on this task batch = 2.4672e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 128.5895, GNorm = 0.2422
Meta loss on this task batch = 2.5341e-01, Meta loss averaged over last 500 steps = 3.0622e-01, PNorm = 128.5969, GNorm = 0.2857
Meta loss on this task batch = 2.7029e-01, Meta loss averaged over last 500 steps = 3.0621e-01, PNorm = 128.6042, GNorm = 0.3980
Took 111.98498392105103 seconds to complete one epoch of meta training
Took 119.93086814880371 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490254
Epoch 446
Meta loss on this task batch = 3.4351e-01, Meta loss averaged over last 500 steps = 3.0620e-01, PNorm = 128.6109, GNorm = 0.2997
Meta loss on this task batch = 2.5345e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 128.6170, GNorm = 0.2484
Meta loss on this task batch = 2.7795e-01, Meta loss averaged over last 500 steps = 3.0600e-01, PNorm = 128.6228, GNorm = 0.3301
Meta loss on this task batch = 3.0743e-01, Meta loss averaged over last 500 steps = 3.0588e-01, PNorm = 128.6285, GNorm = 0.5187
Meta loss on this task batch = 3.3814e-01, Meta loss averaged over last 500 steps = 3.0602e-01, PNorm = 128.6347, GNorm = 0.4084
Meta loss on this task batch = 3.3331e-01, Meta loss averaged over last 500 steps = 3.0595e-01, PNorm = 128.6412, GNorm = 0.2544
Meta loss on this task batch = 3.3471e-01, Meta loss averaged over last 500 steps = 3.0593e-01, PNorm = 128.6475, GNorm = 0.2967
Meta loss on this task batch = 3.2990e-01, Meta loss averaged over last 500 steps = 3.0605e-01, PNorm = 128.6553, GNorm = 0.3892
Meta loss on this task batch = 3.1486e-01, Meta loss averaged over last 500 steps = 3.0608e-01, PNorm = 128.6623, GNorm = 0.3299
Meta loss on this task batch = 3.2257e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 128.6690, GNorm = 0.2947
Meta loss on this task batch = 2.6568e-01, Meta loss averaged over last 500 steps = 3.0604e-01, PNorm = 128.6766, GNorm = 0.4716
Meta loss on this task batch = 2.8615e-01, Meta loss averaged over last 500 steps = 3.0603e-01, PNorm = 128.6845, GNorm = 0.2566
Meta loss on this task batch = 3.0158e-01, Meta loss averaged over last 500 steps = 3.0610e-01, PNorm = 128.6924, GNorm = 0.2408
Meta loss on this task batch = 2.8268e-01, Meta loss averaged over last 500 steps = 3.0597e-01, PNorm = 128.7005, GNorm = 0.3349
Meta loss on this task batch = 3.1572e-01, Meta loss averaged over last 500 steps = 3.0602e-01, PNorm = 128.7088, GNorm = 0.3108
Meta loss on this task batch = 2.5257e-01, Meta loss averaged over last 500 steps = 3.0582e-01, PNorm = 128.7172, GNorm = 0.2326
Meta loss on this task batch = 3.1672e-01, Meta loss averaged over last 500 steps = 3.0595e-01, PNorm = 128.7254, GNorm = 0.3168
Meta loss on this task batch = 2.9952e-01, Meta loss averaged over last 500 steps = 3.0597e-01, PNorm = 128.7334, GNorm = 0.2825
Meta loss on this task batch = 3.2884e-01, Meta loss averaged over last 500 steps = 3.0607e-01, PNorm = 128.7411, GNorm = 0.3104
Took 115.22713565826416 seconds to complete one epoch of meta training
Took 123.18158888816833 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459577
Epoch 447
Meta loss on this task batch = 2.8749e-01, Meta loss averaged over last 500 steps = 3.0596e-01, PNorm = 128.7485, GNorm = 0.4373
Meta loss on this task batch = 3.2977e-01, Meta loss averaged over last 500 steps = 3.0589e-01, PNorm = 128.7558, GNorm = 0.2834
Meta loss on this task batch = 2.7842e-01, Meta loss averaged over last 500 steps = 3.0590e-01, PNorm = 128.7623, GNorm = 0.2625
Meta loss on this task batch = 3.3125e-01, Meta loss averaged over last 500 steps = 3.0601e-01, PNorm = 128.7682, GNorm = 0.3148
Meta loss on this task batch = 3.2095e-01, Meta loss averaged over last 500 steps = 3.0604e-01, PNorm = 128.7741, GNorm = 0.3056
Meta loss on this task batch = 2.6349e-01, Meta loss averaged over last 500 steps = 3.0599e-01, PNorm = 128.7806, GNorm = 0.2737
Meta loss on this task batch = 3.5345e-01, Meta loss averaged over last 500 steps = 3.0625e-01, PNorm = 128.7864, GNorm = 0.2960
Meta loss on this task batch = 2.9412e-01, Meta loss averaged over last 500 steps = 3.0627e-01, PNorm = 128.7920, GNorm = 0.2895
Meta loss on this task batch = 2.8482e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 128.7976, GNorm = 0.2740
Meta loss on this task batch = 3.1958e-01, Meta loss averaged over last 500 steps = 3.0606e-01, PNorm = 128.8027, GNorm = 0.2855
Meta loss on this task batch = 2.7610e-01, Meta loss averaged over last 500 steps = 3.0609e-01, PNorm = 128.8087, GNorm = 0.4753
Meta loss on this task batch = 2.8648e-01, Meta loss averaged over last 500 steps = 3.0603e-01, PNorm = 128.8151, GNorm = 0.3769
Meta loss on this task batch = 3.0776e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 128.8209, GNorm = 0.2906
Meta loss on this task batch = 2.6600e-01, Meta loss averaged over last 500 steps = 3.0609e-01, PNorm = 128.8270, GNorm = 0.2281
Meta loss on this task batch = 2.9059e-01, Meta loss averaged over last 500 steps = 3.0609e-01, PNorm = 128.8332, GNorm = 0.2594
Meta loss on this task batch = 3.0743e-01, Meta loss averaged over last 500 steps = 3.0606e-01, PNorm = 128.8399, GNorm = 0.3491
Meta loss on this task batch = 3.2753e-01, Meta loss averaged over last 500 steps = 3.0614e-01, PNorm = 128.8463, GNorm = 0.2680
Meta loss on this task batch = 3.2421e-01, Meta loss averaged over last 500 steps = 3.0610e-01, PNorm = 128.8523, GNorm = 0.3350
Meta loss on this task batch = 2.9103e-01, Meta loss averaged over last 500 steps = 3.0607e-01, PNorm = 128.8587, GNorm = 0.3738
Took 114.62010550498962 seconds to complete one epoch of meta training
Took 122.60427403450012 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462891
Epoch 448
Meta loss on this task batch = 2.7844e-01, Meta loss averaged over last 500 steps = 3.0596e-01, PNorm = 128.8659, GNorm = 0.2936
Meta loss on this task batch = 2.8616e-01, Meta loss averaged over last 500 steps = 3.0591e-01, PNorm = 128.8730, GNorm = 0.2503
Meta loss on this task batch = 2.8288e-01, Meta loss averaged over last 500 steps = 3.0595e-01, PNorm = 128.8799, GNorm = 0.2576
Meta loss on this task batch = 3.1287e-01, Meta loss averaged over last 500 steps = 3.0594e-01, PNorm = 128.8861, GNorm = 0.3606
Meta loss on this task batch = 2.8534e-01, Meta loss averaged over last 500 steps = 3.0590e-01, PNorm = 128.8921, GNorm = 0.2458
Meta loss on this task batch = 3.5828e-01, Meta loss averaged over last 500 steps = 3.0590e-01, PNorm = 128.8969, GNorm = 0.3832
Meta loss on this task batch = 2.5668e-01, Meta loss averaged over last 500 steps = 3.0581e-01, PNorm = 128.9019, GNorm = 0.2608
Meta loss on this task batch = 3.4530e-01, Meta loss averaged over last 500 steps = 3.0602e-01, PNorm = 128.9056, GNorm = 0.3099
Meta loss on this task batch = 2.1256e-01, Meta loss averaged over last 500 steps = 3.0569e-01, PNorm = 128.9096, GNorm = 0.2087
Meta loss on this task batch = 3.7240e-01, Meta loss averaged over last 500 steps = 3.0582e-01, PNorm = 128.9127, GNorm = 0.3612
Meta loss on this task batch = 2.7509e-01, Meta loss averaged over last 500 steps = 3.0583e-01, PNorm = 128.9168, GNorm = 0.2915
Meta loss on this task batch = 3.3694e-01, Meta loss averaged over last 500 steps = 3.0589e-01, PNorm = 128.9206, GNorm = 0.3031
Meta loss on this task batch = 2.8991e-01, Meta loss averaged over last 500 steps = 3.0574e-01, PNorm = 128.9246, GNorm = 0.2747
Meta loss on this task batch = 3.2746e-01, Meta loss averaged over last 500 steps = 3.0575e-01, PNorm = 128.9293, GNorm = 0.3172
Meta loss on this task batch = 3.1553e-01, Meta loss averaged over last 500 steps = 3.0569e-01, PNorm = 128.9342, GNorm = 0.2599
Meta loss on this task batch = 3.0170e-01, Meta loss averaged over last 500 steps = 3.0566e-01, PNorm = 128.9399, GNorm = 0.2994
Meta loss on this task batch = 3.6896e-01, Meta loss averaged over last 500 steps = 3.0582e-01, PNorm = 128.9459, GNorm = 0.3274
Meta loss on this task batch = 3.2059e-01, Meta loss averaged over last 500 steps = 3.0597e-01, PNorm = 128.9520, GNorm = 0.3396
Meta loss on this task batch = 3.4994e-01, Meta loss averaged over last 500 steps = 3.0611e-01, PNorm = 128.9576, GNorm = 0.4139
Took 111.52716016769409 seconds to complete one epoch of meta training
Took 119.6459493637085 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.438110
Epoch 449
Meta loss on this task batch = 2.5243e-01, Meta loss averaged over last 500 steps = 3.0594e-01, PNorm = 128.9642, GNorm = 0.2603
Meta loss on this task batch = 2.9088e-01, Meta loss averaged over last 500 steps = 3.0584e-01, PNorm = 128.9708, GNorm = 0.3121
Meta loss on this task batch = 2.4793e-01, Meta loss averaged over last 500 steps = 3.0572e-01, PNorm = 128.9785, GNorm = 0.3764
Meta loss on this task batch = 2.7655e-01, Meta loss averaged over last 500 steps = 3.0579e-01, PNorm = 128.9868, GNorm = 0.2351
Meta loss on this task batch = 3.2926e-01, Meta loss averaged over last 500 steps = 3.0586e-01, PNorm = 128.9941, GNorm = 0.3972
Meta loss on this task batch = 3.0567e-01, Meta loss averaged over last 500 steps = 3.0583e-01, PNorm = 129.0008, GNorm = 0.2685
Meta loss on this task batch = 3.0529e-01, Meta loss averaged over last 500 steps = 3.0576e-01, PNorm = 129.0067, GNorm = 0.2858
Meta loss on this task batch = 3.4424e-01, Meta loss averaged over last 500 steps = 3.0575e-01, PNorm = 129.0118, GNorm = 0.3228
Meta loss on this task batch = 3.3185e-01, Meta loss averaged over last 500 steps = 3.0591e-01, PNorm = 129.0158, GNorm = 0.3984
Meta loss on this task batch = 2.9457e-01, Meta loss averaged over last 500 steps = 3.0588e-01, PNorm = 129.0189, GNorm = 0.3195
Meta loss on this task batch = 3.1173e-01, Meta loss averaged over last 500 steps = 3.0594e-01, PNorm = 129.0220, GNorm = 0.3701
Meta loss on this task batch = 3.4885e-01, Meta loss averaged over last 500 steps = 3.0597e-01, PNorm = 129.0242, GNorm = 0.3025
Meta loss on this task batch = 3.0143e-01, Meta loss averaged over last 500 steps = 3.0597e-01, PNorm = 129.0271, GNorm = 0.3664
Meta loss on this task batch = 3.2579e-01, Meta loss averaged over last 500 steps = 3.0605e-01, PNorm = 129.0301, GNorm = 0.3613
Meta loss on this task batch = 2.8017e-01, Meta loss averaged over last 500 steps = 3.0603e-01, PNorm = 129.0334, GNorm = 0.2814
Meta loss on this task batch = 3.3310e-01, Meta loss averaged over last 500 steps = 3.0612e-01, PNorm = 129.0372, GNorm = 0.5452
Meta loss on this task batch = 3.2248e-01, Meta loss averaged over last 500 steps = 3.0622e-01, PNorm = 129.0421, GNorm = 0.3795
Meta loss on this task batch = 3.1732e-01, Meta loss averaged over last 500 steps = 3.0623e-01, PNorm = 129.0471, GNorm = 0.2995
Meta loss on this task batch = 2.4447e-01, Meta loss averaged over last 500 steps = 3.0606e-01, PNorm = 129.0521, GNorm = 0.3048
Took 112.73295640945435 seconds to complete one epoch of meta training
Took 120.83320426940918 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448818
Epoch 450
Meta loss on this task batch = 3.1482e-01, Meta loss averaged over last 500 steps = 3.0599e-01, PNorm = 129.0569, GNorm = 0.2653
Meta loss on this task batch = 2.5130e-01, Meta loss averaged over last 500 steps = 3.0578e-01, PNorm = 129.0630, GNorm = 0.2488
Meta loss on this task batch = 2.8571e-01, Meta loss averaged over last 500 steps = 3.0567e-01, PNorm = 129.0698, GNorm = 0.2547
Meta loss on this task batch = 3.0816e-01, Meta loss averaged over last 500 steps = 3.0576e-01, PNorm = 129.0760, GNorm = 0.3141
Meta loss on this task batch = 3.4870e-01, Meta loss averaged over last 500 steps = 3.0581e-01, PNorm = 129.0822, GNorm = 0.4082
Meta loss on this task batch = 3.1721e-01, Meta loss averaged over last 500 steps = 3.0580e-01, PNorm = 129.0889, GNorm = 0.2593
Meta loss on this task batch = 3.1880e-01, Meta loss averaged over last 500 steps = 3.0583e-01, PNorm = 129.0949, GNorm = 0.2432
Meta loss on this task batch = 3.7383e-01, Meta loss averaged over last 500 steps = 3.0609e-01, PNorm = 129.1007, GNorm = 0.2707
Meta loss on this task batch = 2.8945e-01, Meta loss averaged over last 500 steps = 3.0611e-01, PNorm = 129.1072, GNorm = 0.2494
Meta loss on this task batch = 3.0099e-01, Meta loss averaged over last 500 steps = 3.0608e-01, PNorm = 129.1131, GNorm = 0.3238
Meta loss on this task batch = 2.7835e-01, Meta loss averaged over last 500 steps = 3.0609e-01, PNorm = 129.1193, GNorm = 0.3298
Meta loss on this task batch = 2.8035e-01, Meta loss averaged over last 500 steps = 3.0596e-01, PNorm = 129.1258, GNorm = 0.3258
Meta loss on this task batch = 2.8661e-01, Meta loss averaged over last 500 steps = 3.0596e-01, PNorm = 129.1322, GNorm = 0.3467
Meta loss on this task batch = 2.6195e-01, Meta loss averaged over last 500 steps = 3.0581e-01, PNorm = 129.1386, GNorm = 0.2480
Meta loss on this task batch = 2.9596e-01, Meta loss averaged over last 500 steps = 3.0573e-01, PNorm = 129.1445, GNorm = 0.5424
Meta loss on this task batch = 3.0978e-01, Meta loss averaged over last 500 steps = 3.0565e-01, PNorm = 129.1481, GNorm = 0.3999
Meta loss on this task batch = 3.1380e-01, Meta loss averaged over last 500 steps = 3.0568e-01, PNorm = 129.1516, GNorm = 0.3543
Meta loss on this task batch = 3.0260e-01, Meta loss averaged over last 500 steps = 3.0566e-01, PNorm = 129.1542, GNorm = 0.4119
Meta loss on this task batch = 3.1463e-01, Meta loss averaged over last 500 steps = 3.0574e-01, PNorm = 129.1564, GNorm = 0.4095
Took 113.5020580291748 seconds to complete one epoch of meta training
Took 121.57785177230835 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484595
Epoch 451
Meta loss on this task batch = 2.6065e-01, Meta loss averaged over last 500 steps = 3.0554e-01, PNorm = 129.1592, GNorm = 0.2022
Meta loss on this task batch = 3.2763e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 129.1618, GNorm = 0.3115
Meta loss on this task batch = 3.1990e-01, Meta loss averaged over last 500 steps = 3.0554e-01, PNorm = 129.1640, GNorm = 0.3016
Meta loss on this task batch = 2.8584e-01, Meta loss averaged over last 500 steps = 3.0557e-01, PNorm = 129.1662, GNorm = 0.2942
Meta loss on this task batch = 2.9951e-01, Meta loss averaged over last 500 steps = 3.0555e-01, PNorm = 129.1688, GNorm = 0.3691
Meta loss on this task batch = 2.9243e-01, Meta loss averaged over last 500 steps = 3.0544e-01, PNorm = 129.1718, GNorm = 0.3053
Meta loss on this task batch = 3.0577e-01, Meta loss averaged over last 500 steps = 3.0541e-01, PNorm = 129.1753, GNorm = 0.4283
Meta loss on this task batch = 3.0493e-01, Meta loss averaged over last 500 steps = 3.0555e-01, PNorm = 129.1795, GNorm = 0.2595
Meta loss on this task batch = 3.0242e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 129.1849, GNorm = 0.2658
Meta loss on this task batch = 3.2055e-01, Meta loss averaged over last 500 steps = 3.0538e-01, PNorm = 129.1906, GNorm = 0.3218
Meta loss on this task batch = 3.1555e-01, Meta loss averaged over last 500 steps = 3.0535e-01, PNorm = 129.1964, GNorm = 0.2637
Meta loss on this task batch = 2.7971e-01, Meta loss averaged over last 500 steps = 3.0526e-01, PNorm = 129.2034, GNorm = 0.2537
Meta loss on this task batch = 2.9650e-01, Meta loss averaged over last 500 steps = 3.0528e-01, PNorm = 129.2111, GNorm = 0.3114
Meta loss on this task batch = 2.7160e-01, Meta loss averaged over last 500 steps = 3.0518e-01, PNorm = 129.2190, GNorm = 0.2322
Meta loss on this task batch = 3.1668e-01, Meta loss averaged over last 500 steps = 3.0514e-01, PNorm = 129.2261, GNorm = 0.3202
Meta loss on this task batch = 2.8522e-01, Meta loss averaged over last 500 steps = 3.0515e-01, PNorm = 129.2333, GNorm = 0.3353
Meta loss on this task batch = 3.4285e-01, Meta loss averaged over last 500 steps = 3.0529e-01, PNorm = 129.2398, GNorm = 0.3568
Meta loss on this task batch = 3.1810e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 129.2468, GNorm = 0.5160
Meta loss on this task batch = 3.1196e-01, Meta loss averaged over last 500 steps = 3.0529e-01, PNorm = 129.2528, GNorm = 0.2767
Took 115.41064810752869 seconds to complete one epoch of meta training
Took 123.41649150848389 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471629
Epoch 452
Meta loss on this task batch = 3.0539e-01, Meta loss averaged over last 500 steps = 3.0532e-01, PNorm = 129.2576, GNorm = 0.3729
Meta loss on this task batch = 3.0424e-01, Meta loss averaged over last 500 steps = 3.0539e-01, PNorm = 129.2631, GNorm = 0.3349
Meta loss on this task batch = 3.0586e-01, Meta loss averaged over last 500 steps = 3.0553e-01, PNorm = 129.2684, GNorm = 0.2721
Meta loss on this task batch = 3.1757e-01, Meta loss averaged over last 500 steps = 3.0559e-01, PNorm = 129.2733, GNorm = 0.4605
Meta loss on this task batch = 2.6398e-01, Meta loss averaged over last 500 steps = 3.0550e-01, PNorm = 129.2787, GNorm = 0.3653
Meta loss on this task batch = 2.9532e-01, Meta loss averaged over last 500 steps = 3.0543e-01, PNorm = 129.2841, GNorm = 0.3238
Meta loss on this task batch = 3.8476e-01, Meta loss averaged over last 500 steps = 3.0555e-01, PNorm = 129.2896, GNorm = 0.3712
Meta loss on this task batch = 2.9535e-01, Meta loss averaged over last 500 steps = 3.0546e-01, PNorm = 129.2954, GNorm = 0.2301
Meta loss on this task batch = 2.8854e-01, Meta loss averaged over last 500 steps = 3.0549e-01, PNorm = 129.3003, GNorm = 0.2879
Meta loss on this task batch = 3.0530e-01, Meta loss averaged over last 500 steps = 3.0545e-01, PNorm = 129.3056, GNorm = 0.2511
Meta loss on this task batch = 2.9197e-01, Meta loss averaged over last 500 steps = 3.0554e-01, PNorm = 129.3109, GNorm = 0.2705
Meta loss on this task batch = 3.3426e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 129.3163, GNorm = 0.3119
Meta loss on this task batch = 3.4759e-01, Meta loss averaged over last 500 steps = 3.0580e-01, PNorm = 129.3216, GNorm = 0.3176
Meta loss on this task batch = 3.0331e-01, Meta loss averaged over last 500 steps = 3.0579e-01, PNorm = 129.3277, GNorm = 0.3037
Meta loss on this task batch = 2.7532e-01, Meta loss averaged over last 500 steps = 3.0561e-01, PNorm = 129.3336, GNorm = 0.3098
Meta loss on this task batch = 2.3669e-01, Meta loss averaged over last 500 steps = 3.0555e-01, PNorm = 129.3404, GNorm = 0.2852
Meta loss on this task batch = 3.5128e-01, Meta loss averaged over last 500 steps = 3.0564e-01, PNorm = 129.3470, GNorm = 0.3123
Meta loss on this task batch = 2.8662e-01, Meta loss averaged over last 500 steps = 3.0565e-01, PNorm = 129.3538, GNorm = 0.4145
Meta loss on this task batch = 3.1366e-01, Meta loss averaged over last 500 steps = 3.0567e-01, PNorm = 129.3607, GNorm = 0.3208
Took 176.48462009429932 seconds to complete one epoch of meta training
Took 183.28735041618347 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446371
Epoch 453
Meta loss on this task batch = 3.3334e-01, Meta loss averaged over last 500 steps = 3.0562e-01, PNorm = 129.3675, GNorm = 0.3320
Meta loss on this task batch = 3.6166e-01, Meta loss averaged over last 500 steps = 3.0568e-01, PNorm = 129.3742, GNorm = 0.2891
Meta loss on this task batch = 3.1685e-01, Meta loss averaged over last 500 steps = 3.0569e-01, PNorm = 129.3817, GNorm = 0.2868
Meta loss on this task batch = 2.8931e-01, Meta loss averaged over last 500 steps = 3.0564e-01, PNorm = 129.3888, GNorm = 0.3632
Meta loss on this task batch = 2.8409e-01, Meta loss averaged over last 500 steps = 3.0559e-01, PNorm = 129.3963, GNorm = 0.2579
Meta loss on this task batch = 3.3908e-01, Meta loss averaged over last 500 steps = 3.0572e-01, PNorm = 129.4025, GNorm = 0.2813
Meta loss on this task batch = 2.7573e-01, Meta loss averaged over last 500 steps = 3.0581e-01, PNorm = 129.4088, GNorm = 0.2201
Meta loss on this task batch = 2.9846e-01, Meta loss averaged over last 500 steps = 3.0579e-01, PNorm = 129.4152, GNorm = 0.4628
Meta loss on this task batch = 3.1338e-01, Meta loss averaged over last 500 steps = 3.0586e-01, PNorm = 129.4211, GNorm = 0.3637
Meta loss on this task batch = 3.1435e-01, Meta loss averaged over last 500 steps = 3.0581e-01, PNorm = 129.4261, GNorm = 0.2632
Meta loss on this task batch = 2.5616e-01, Meta loss averaged over last 500 steps = 3.0567e-01, PNorm = 129.4310, GNorm = 0.2207
Meta loss on this task batch = 3.1268e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 129.4351, GNorm = 0.2511
Meta loss on this task batch = 3.5120e-01, Meta loss averaged over last 500 steps = 3.0587e-01, PNorm = 129.4384, GNorm = 0.2478
Meta loss on this task batch = 2.9037e-01, Meta loss averaged over last 500 steps = 3.0588e-01, PNorm = 129.4422, GNorm = 0.2358
Meta loss on this task batch = 2.6211e-01, Meta loss averaged over last 500 steps = 3.0585e-01, PNorm = 129.4463, GNorm = 0.2485
Meta loss on this task batch = 3.1477e-01, Meta loss averaged over last 500 steps = 3.0581e-01, PNorm = 129.4500, GNorm = 0.3334
Meta loss on this task batch = 2.7114e-01, Meta loss averaged over last 500 steps = 3.0588e-01, PNorm = 129.4542, GNorm = 0.2878
Meta loss on this task batch = 2.8109e-01, Meta loss averaged over last 500 steps = 3.0588e-01, PNorm = 129.4585, GNorm = 0.2466
Meta loss on this task batch = 3.0432e-01, Meta loss averaged over last 500 steps = 3.0585e-01, PNorm = 129.4636, GNorm = 0.3107
Took 295.3613920211792 seconds to complete one epoch of meta training
Took 303.39657521247864 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464102
Epoch 454
Meta loss on this task batch = 2.9088e-01, Meta loss averaged over last 500 steps = 3.0593e-01, PNorm = 129.4686, GNorm = 0.3219
Meta loss on this task batch = 3.5303e-01, Meta loss averaged over last 500 steps = 3.0594e-01, PNorm = 129.4728, GNorm = 0.3645
Meta loss on this task batch = 2.9699e-01, Meta loss averaged over last 500 steps = 3.0587e-01, PNorm = 129.4781, GNorm = 0.2845
Meta loss on this task batch = 3.5944e-01, Meta loss averaged over last 500 steps = 3.0602e-01, PNorm = 129.4816, GNorm = 0.4483
Meta loss on this task batch = 3.5215e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 129.4855, GNorm = 0.2892
Meta loss on this task batch = 3.7300e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 129.4899, GNorm = 0.2765
Meta loss on this task batch = 2.6994e-01, Meta loss averaged over last 500 steps = 3.0625e-01, PNorm = 129.4948, GNorm = 0.2752
Meta loss on this task batch = 2.6100e-01, Meta loss averaged over last 500 steps = 3.0611e-01, PNorm = 129.5006, GNorm = 0.2871
Meta loss on this task batch = 2.9401e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 129.5061, GNorm = 0.3837
Meta loss on this task batch = 2.7641e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 129.5114, GNorm = 0.2508
Meta loss on this task batch = 3.2281e-01, Meta loss averaged over last 500 steps = 3.0623e-01, PNorm = 129.5176, GNorm = 0.2823
Meta loss on this task batch = 2.5687e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 129.5244, GNorm = 0.4082
Meta loss on this task batch = 2.9521e-01, Meta loss averaged over last 500 steps = 3.0612e-01, PNorm = 129.5316, GNorm = 0.3318
Meta loss on this task batch = 2.6670e-01, Meta loss averaged over last 500 steps = 3.0608e-01, PNorm = 129.5387, GNorm = 0.2566
Meta loss on this task batch = 2.4890e-01, Meta loss averaged over last 500 steps = 3.0599e-01, PNorm = 129.5456, GNorm = 0.3337
Meta loss on this task batch = 2.8174e-01, Meta loss averaged over last 500 steps = 3.0589e-01, PNorm = 129.5517, GNorm = 0.3260
Meta loss on this task batch = 2.9365e-01, Meta loss averaged over last 500 steps = 3.0580e-01, PNorm = 129.5574, GNorm = 0.3454
Meta loss on this task batch = 2.6544e-01, Meta loss averaged over last 500 steps = 3.0572e-01, PNorm = 129.5642, GNorm = 0.2809
Meta loss on this task batch = 3.2929e-01, Meta loss averaged over last 500 steps = 3.0570e-01, PNorm = 129.5702, GNorm = 0.4789
Took 146.12783360481262 seconds to complete one epoch of meta training
Took 153.91254425048828 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486461
Epoch 455
Meta loss on this task batch = 3.1516e-01, Meta loss averaged over last 500 steps = 3.0572e-01, PNorm = 129.5752, GNorm = 0.2888
Meta loss on this task batch = 3.2200e-01, Meta loss averaged over last 500 steps = 3.0570e-01, PNorm = 129.5794, GNorm = 0.3406
Meta loss on this task batch = 2.4491e-01, Meta loss averaged over last 500 steps = 3.0562e-01, PNorm = 129.5843, GNorm = 0.3711
Meta loss on this task batch = 3.0044e-01, Meta loss averaged over last 500 steps = 3.0553e-01, PNorm = 129.5885, GNorm = 0.3286
Meta loss on this task batch = 2.9379e-01, Meta loss averaged over last 500 steps = 3.0554e-01, PNorm = 129.5924, GNorm = 0.3286
Meta loss on this task batch = 2.8858e-01, Meta loss averaged over last 500 steps = 3.0546e-01, PNorm = 129.5970, GNorm = 0.2808
Meta loss on this task batch = 3.3655e-01, Meta loss averaged over last 500 steps = 3.0557e-01, PNorm = 129.6014, GNorm = 0.2866
Meta loss on this task batch = 3.1680e-01, Meta loss averaged over last 500 steps = 3.0563e-01, PNorm = 129.6057, GNorm = 0.4215
Meta loss on this task batch = 2.8737e-01, Meta loss averaged over last 500 steps = 3.0567e-01, PNorm = 129.6108, GNorm = 0.3467
Meta loss on this task batch = 2.9182e-01, Meta loss averaged over last 500 steps = 3.0567e-01, PNorm = 129.6161, GNorm = 0.2486
Meta loss on this task batch = 2.8006e-01, Meta loss averaged over last 500 steps = 3.0563e-01, PNorm = 129.6219, GNorm = 0.3643
Meta loss on this task batch = 3.1959e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 129.6272, GNorm = 0.2603
Meta loss on this task batch = 3.3634e-01, Meta loss averaged over last 500 steps = 3.0566e-01, PNorm = 129.6321, GNorm = 0.2708
Meta loss on this task batch = 3.4455e-01, Meta loss averaged over last 500 steps = 3.0578e-01, PNorm = 129.6377, GNorm = 0.2906
Meta loss on this task batch = 2.9734e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 129.6434, GNorm = 0.2415
Meta loss on this task batch = 3.6879e-01, Meta loss averaged over last 500 steps = 3.0583e-01, PNorm = 129.6494, GNorm = 0.4000
Meta loss on this task batch = 2.7647e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 129.6561, GNorm = 0.2452
Meta loss on this task batch = 3.0235e-01, Meta loss averaged over last 500 steps = 3.0572e-01, PNorm = 129.6628, GNorm = 0.3320
Meta loss on this task batch = 2.9547e-01, Meta loss averaged over last 500 steps = 3.0561e-01, PNorm = 129.6694, GNorm = 0.3540
Took 114.00761795043945 seconds to complete one epoch of meta training
Took 122.46589159965515 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469435
Epoch 456
Meta loss on this task batch = 2.9939e-01, Meta loss averaged over last 500 steps = 3.0556e-01, PNorm = 129.6761, GNorm = 0.3184
Meta loss on this task batch = 3.1263e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 129.6825, GNorm = 0.2803
Meta loss on this task batch = 3.4961e-01, Meta loss averaged over last 500 steps = 3.0578e-01, PNorm = 129.6900, GNorm = 0.3376
Meta loss on this task batch = 2.8853e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 129.6973, GNorm = 0.2776
Meta loss on this task batch = 3.2481e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 129.7038, GNorm = 0.3233
Meta loss on this task batch = 2.8088e-01, Meta loss averaged over last 500 steps = 3.0565e-01, PNorm = 129.7102, GNorm = 0.2373
Meta loss on this task batch = 2.3174e-01, Meta loss averaged over last 500 steps = 3.0542e-01, PNorm = 129.7166, GNorm = 0.2424
Meta loss on this task batch = 3.1752e-01, Meta loss averaged over last 500 steps = 3.0547e-01, PNorm = 129.7231, GNorm = 0.2741
Meta loss on this task batch = 3.0391e-01, Meta loss averaged over last 500 steps = 3.0550e-01, PNorm = 129.7294, GNorm = 0.3322
Meta loss on this task batch = 2.9605e-01, Meta loss averaged over last 500 steps = 3.0545e-01, PNorm = 129.7341, GNorm = 0.4415
Meta loss on this task batch = 2.9217e-01, Meta loss averaged over last 500 steps = 3.0544e-01, PNorm = 129.7390, GNorm = 0.3157
Meta loss on this task batch = 2.7434e-01, Meta loss averaged over last 500 steps = 3.0534e-01, PNorm = 129.7440, GNorm = 0.3340
Meta loss on this task batch = 3.1161e-01, Meta loss averaged over last 500 steps = 3.0542e-01, PNorm = 129.7494, GNorm = 0.3669
Meta loss on this task batch = 2.7099e-01, Meta loss averaged over last 500 steps = 3.0538e-01, PNorm = 129.7551, GNorm = 0.2742
Meta loss on this task batch = 3.1731e-01, Meta loss averaged over last 500 steps = 3.0544e-01, PNorm = 129.7610, GNorm = 0.3215
Meta loss on this task batch = 3.0588e-01, Meta loss averaged over last 500 steps = 3.0536e-01, PNorm = 129.7664, GNorm = 0.2758
Meta loss on this task batch = 3.2503e-01, Meta loss averaged over last 500 steps = 3.0538e-01, PNorm = 129.7712, GNorm = 0.4673
Meta loss on this task batch = 2.6304e-01, Meta loss averaged over last 500 steps = 3.0535e-01, PNorm = 129.7764, GNorm = 0.2768
Meta loss on this task batch = 3.5224e-01, Meta loss averaged over last 500 steps = 3.0542e-01, PNorm = 129.7812, GNorm = 0.3203
Took 113.8341498374939 seconds to complete one epoch of meta training
Took 121.60100722312927 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462742
Epoch 457
Meta loss on this task batch = 3.1882e-01, Meta loss averaged over last 500 steps = 3.0543e-01, PNorm = 129.7851, GNorm = 0.3272
Meta loss on this task batch = 2.8548e-01, Meta loss averaged over last 500 steps = 3.0542e-01, PNorm = 129.7902, GNorm = 0.2529
Meta loss on this task batch = 3.2260e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 129.7954, GNorm = 0.3653
Meta loss on this task batch = 2.7840e-01, Meta loss averaged over last 500 steps = 3.0536e-01, PNorm = 129.8015, GNorm = 0.2679
Meta loss on this task batch = 3.2534e-01, Meta loss averaged over last 500 steps = 3.0543e-01, PNorm = 129.8077, GNorm = 0.2772
Meta loss on this task batch = 2.9990e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 129.8152, GNorm = 0.3033
Meta loss on this task batch = 3.0583e-01, Meta loss averaged over last 500 steps = 3.0537e-01, PNorm = 129.8230, GNorm = 0.2548
Meta loss on this task batch = 3.0622e-01, Meta loss averaged over last 500 steps = 3.0546e-01, PNorm = 129.8312, GNorm = 0.3242
Meta loss on this task batch = 3.0376e-01, Meta loss averaged over last 500 steps = 3.0542e-01, PNorm = 129.8402, GNorm = 0.2759
Meta loss on this task batch = 2.7110e-01, Meta loss averaged over last 500 steps = 3.0538e-01, PNorm = 129.8486, GNorm = 0.2397
Meta loss on this task batch = 2.7745e-01, Meta loss averaged over last 500 steps = 3.0538e-01, PNorm = 129.8576, GNorm = 0.2649
Meta loss on this task batch = 2.7826e-01, Meta loss averaged over last 500 steps = 3.0542e-01, PNorm = 129.8661, GNorm = 0.2595
Meta loss on this task batch = 3.0215e-01, Meta loss averaged over last 500 steps = 3.0539e-01, PNorm = 129.8741, GNorm = 0.2842
Meta loss on this task batch = 3.2548e-01, Meta loss averaged over last 500 steps = 3.0541e-01, PNorm = 129.8811, GNorm = 0.3376
Meta loss on this task batch = 3.2985e-01, Meta loss averaged over last 500 steps = 3.0548e-01, PNorm = 129.8863, GNorm = 0.3428
Meta loss on this task batch = 3.0769e-01, Meta loss averaged over last 500 steps = 3.0537e-01, PNorm = 129.8908, GNorm = 0.4195
Meta loss on this task batch = 2.8196e-01, Meta loss averaged over last 500 steps = 3.0520e-01, PNorm = 129.8943, GNorm = 0.3519
Meta loss on this task batch = 3.3668e-01, Meta loss averaged over last 500 steps = 3.0523e-01, PNorm = 129.8973, GNorm = 0.5341
Meta loss on this task batch = 3.6483e-01, Meta loss averaged over last 500 steps = 3.0537e-01, PNorm = 129.9003, GNorm = 0.6182
Took 114.68768858909607 seconds to complete one epoch of meta training
Took 121.53532075881958 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466303
Epoch 458
Meta loss on this task batch = 2.8053e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 129.9045, GNorm = 0.2588
Meta loss on this task batch = 3.2386e-01, Meta loss averaged over last 500 steps = 3.0533e-01, PNorm = 129.9084, GNorm = 0.3010
Meta loss on this task batch = 2.8910e-01, Meta loss averaged over last 500 steps = 3.0528e-01, PNorm = 129.9128, GNorm = 0.2326
Meta loss on this task batch = 2.6048e-01, Meta loss averaged over last 500 steps = 3.0524e-01, PNorm = 129.9178, GNorm = 0.2904
Meta loss on this task batch = 2.8369e-01, Meta loss averaged over last 500 steps = 3.0520e-01, PNorm = 129.9234, GNorm = 0.2678
Meta loss on this task batch = 3.5557e-01, Meta loss averaged over last 500 steps = 3.0535e-01, PNorm = 129.9290, GNorm = 0.2702
Meta loss on this task batch = 2.9150e-01, Meta loss averaged over last 500 steps = 3.0537e-01, PNorm = 129.9350, GNorm = 0.3380
Meta loss on this task batch = 3.1183e-01, Meta loss averaged over last 500 steps = 3.0537e-01, PNorm = 129.9416, GNorm = 0.2888
Meta loss on this task batch = 3.0614e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 129.9477, GNorm = 0.4115
Meta loss on this task batch = 2.5680e-01, Meta loss averaged over last 500 steps = 3.0523e-01, PNorm = 129.9543, GNorm = 0.2762
Meta loss on this task batch = 3.5058e-01, Meta loss averaged over last 500 steps = 3.0520e-01, PNorm = 129.9609, GNorm = 0.2918
Meta loss on this task batch = 2.7553e-01, Meta loss averaged over last 500 steps = 3.0521e-01, PNorm = 129.9672, GNorm = 0.2681
Meta loss on this task batch = 3.1552e-01, Meta loss averaged over last 500 steps = 3.0535e-01, PNorm = 129.9733, GNorm = 0.3081
Meta loss on this task batch = 2.5455e-01, Meta loss averaged over last 500 steps = 3.0523e-01, PNorm = 129.9794, GNorm = 0.3030
Meta loss on this task batch = 3.1912e-01, Meta loss averaged over last 500 steps = 3.0524e-01, PNorm = 129.9848, GNorm = 0.3984
Meta loss on this task batch = 2.8517e-01, Meta loss averaged over last 500 steps = 3.0517e-01, PNorm = 129.9905, GNorm = 0.2871
Meta loss on this task batch = 3.5312e-01, Meta loss averaged over last 500 steps = 3.0525e-01, PNorm = 129.9956, GNorm = 0.3081
Meta loss on this task batch = 2.8525e-01, Meta loss averaged over last 500 steps = 3.0521e-01, PNorm = 130.0004, GNorm = 0.2540
Meta loss on this task batch = 3.2296e-01, Meta loss averaged over last 500 steps = 3.0525e-01, PNorm = 130.0045, GNorm = 0.3794
Took 110.20308017730713 seconds to complete one epoch of meta training
Took 118.18888759613037 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474644
Epoch 459
Meta loss on this task batch = 2.6884e-01, Meta loss averaged over last 500 steps = 3.0523e-01, PNorm = 130.0088, GNorm = 0.3205
Meta loss on this task batch = 2.8913e-01, Meta loss averaged over last 500 steps = 3.0505e-01, PNorm = 130.0138, GNorm = 0.2739
Meta loss on this task batch = 3.2485e-01, Meta loss averaged over last 500 steps = 3.0506e-01, PNorm = 130.0179, GNorm = 0.3342
Meta loss on this task batch = 3.2441e-01, Meta loss averaged over last 500 steps = 3.0512e-01, PNorm = 130.0219, GNorm = 0.2913
Meta loss on this task batch = 3.2291e-01, Meta loss averaged over last 500 steps = 3.0520e-01, PNorm = 130.0261, GNorm = 0.3328
Meta loss on this task batch = 3.6557e-01, Meta loss averaged over last 500 steps = 3.0535e-01, PNorm = 130.0297, GNorm = 0.3332
Meta loss on this task batch = 2.9518e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 130.0338, GNorm = 0.3232
Meta loss on this task batch = 3.0341e-01, Meta loss averaged over last 500 steps = 3.0538e-01, PNorm = 130.0392, GNorm = 0.3723
Meta loss on this task batch = 2.9813e-01, Meta loss averaged over last 500 steps = 3.0541e-01, PNorm = 130.0451, GNorm = 0.3790
Meta loss on this task batch = 3.5117e-01, Meta loss averaged over last 500 steps = 3.0557e-01, PNorm = 130.0504, GNorm = 0.3341
Meta loss on this task batch = 2.9641e-01, Meta loss averaged over last 500 steps = 3.0551e-01, PNorm = 130.0558, GNorm = 0.2954
Meta loss on this task batch = 2.5654e-01, Meta loss averaged over last 500 steps = 3.0538e-01, PNorm = 130.0616, GNorm = 0.2693
Meta loss on this task batch = 2.8906e-01, Meta loss averaged over last 500 steps = 3.0534e-01, PNorm = 130.0677, GNorm = 0.3649
Meta loss on this task batch = 2.9638e-01, Meta loss averaged over last 500 steps = 3.0522e-01, PNorm = 130.0739, GNorm = 0.3419
Meta loss on this task batch = 3.0075e-01, Meta loss averaged over last 500 steps = 3.0519e-01, PNorm = 130.0804, GNorm = 0.2415
Meta loss on this task batch = 2.8143e-01, Meta loss averaged over last 500 steps = 3.0496e-01, PNorm = 130.0879, GNorm = 0.4944
Meta loss on this task batch = 2.4224e-01, Meta loss averaged over last 500 steps = 3.0488e-01, PNorm = 130.0950, GNorm = 0.2778
Meta loss on this task batch = 3.0912e-01, Meta loss averaged over last 500 steps = 3.0489e-01, PNorm = 130.1020, GNorm = 0.3460
Meta loss on this task batch = 3.7187e-01, Meta loss averaged over last 500 steps = 3.0508e-01, PNorm = 130.1073, GNorm = 0.4389
Took 115.12344312667847 seconds to complete one epoch of meta training
Took 122.73409104347229 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487101
Epoch 460
Meta loss on this task batch = 2.8724e-01, Meta loss averaged over last 500 steps = 3.0494e-01, PNorm = 130.1132, GNorm = 0.2309
Meta loss on this task batch = 3.0760e-01, Meta loss averaged over last 500 steps = 3.0485e-01, PNorm = 130.1194, GNorm = 0.3099
Meta loss on this task batch = 2.9587e-01, Meta loss averaged over last 500 steps = 3.0476e-01, PNorm = 130.1262, GNorm = 0.5811
Meta loss on this task batch = 2.7157e-01, Meta loss averaged over last 500 steps = 3.0474e-01, PNorm = 130.1333, GNorm = 0.2750
Meta loss on this task batch = 3.3404e-01, Meta loss averaged over last 500 steps = 3.0486e-01, PNorm = 130.1394, GNorm = 0.2645
Meta loss on this task batch = 3.3292e-01, Meta loss averaged over last 500 steps = 3.0496e-01, PNorm = 130.1442, GNorm = 0.3467
Meta loss on this task batch = 3.3483e-01, Meta loss averaged over last 500 steps = 3.0504e-01, PNorm = 130.1486, GNorm = 0.3765
Meta loss on this task batch = 2.8216e-01, Meta loss averaged over last 500 steps = 3.0506e-01, PNorm = 130.1534, GNorm = 0.3295
Meta loss on this task batch = 2.7567e-01, Meta loss averaged over last 500 steps = 3.0497e-01, PNorm = 130.1586, GNorm = 0.2786
Meta loss on this task batch = 3.0278e-01, Meta loss averaged over last 500 steps = 3.0503e-01, PNorm = 130.1639, GNorm = 0.3291
Meta loss on this task batch = 3.2654e-01, Meta loss averaged over last 500 steps = 3.0504e-01, PNorm = 130.1682, GNorm = 0.2736
Meta loss on this task batch = 2.7244e-01, Meta loss averaged over last 500 steps = 3.0490e-01, PNorm = 130.1728, GNorm = 0.2697
Meta loss on this task batch = 2.7012e-01, Meta loss averaged over last 500 steps = 3.0488e-01, PNorm = 130.1784, GNorm = 0.3037
Meta loss on this task batch = 2.8598e-01, Meta loss averaged over last 500 steps = 3.0477e-01, PNorm = 130.1833, GNorm = 0.2891
Meta loss on this task batch = 3.2124e-01, Meta loss averaged over last 500 steps = 3.0474e-01, PNorm = 130.1883, GNorm = 0.3561
Meta loss on this task batch = 2.8749e-01, Meta loss averaged over last 500 steps = 3.0473e-01, PNorm = 130.1937, GNorm = 0.3068
Meta loss on this task batch = 3.1211e-01, Meta loss averaged over last 500 steps = 3.0480e-01, PNorm = 130.1995, GNorm = 0.2877
Meta loss on this task batch = 3.3271e-01, Meta loss averaged over last 500 steps = 3.0477e-01, PNorm = 130.2049, GNorm = 0.2630
Meta loss on this task batch = 3.2187e-01, Meta loss averaged over last 500 steps = 3.0476e-01, PNorm = 130.2114, GNorm = 0.3959
Took 109.72323536872864 seconds to complete one epoch of meta training
Took 117.69964599609375 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482853
Epoch 461
Meta loss on this task batch = 3.4224e-01, Meta loss averaged over last 500 steps = 3.0475e-01, PNorm = 130.2171, GNorm = 0.3012
Meta loss on this task batch = 2.6451e-01, Meta loss averaged over last 500 steps = 3.0459e-01, PNorm = 130.2235, GNorm = 0.2460
Meta loss on this task batch = 2.9423e-01, Meta loss averaged over last 500 steps = 3.0463e-01, PNorm = 130.2302, GNorm = 0.3206
Meta loss on this task batch = 3.4214e-01, Meta loss averaged over last 500 steps = 3.0466e-01, PNorm = 130.2361, GNorm = 0.2939
Meta loss on this task batch = 2.8227e-01, Meta loss averaged over last 500 steps = 3.0466e-01, PNorm = 130.2422, GNorm = 0.2645
Meta loss on this task batch = 2.8239e-01, Meta loss averaged over last 500 steps = 3.0461e-01, PNorm = 130.2482, GNorm = 0.3206
Meta loss on this task batch = 3.0709e-01, Meta loss averaged over last 500 steps = 3.0473e-01, PNorm = 130.2538, GNorm = 0.2901
Meta loss on this task batch = 3.5730e-01, Meta loss averaged over last 500 steps = 3.0481e-01, PNorm = 130.2582, GNorm = 0.3675
Meta loss on this task batch = 3.0720e-01, Meta loss averaged over last 500 steps = 3.0477e-01, PNorm = 130.2634, GNorm = 0.4142
Meta loss on this task batch = 2.9088e-01, Meta loss averaged over last 500 steps = 3.0483e-01, PNorm = 130.2691, GNorm = 0.3334
Meta loss on this task batch = 3.0724e-01, Meta loss averaged over last 500 steps = 3.0484e-01, PNorm = 130.2741, GNorm = 0.2940
Meta loss on this task batch = 3.2829e-01, Meta loss averaged over last 500 steps = 3.0484e-01, PNorm = 130.2783, GNorm = 0.2902
Meta loss on this task batch = 2.7377e-01, Meta loss averaged over last 500 steps = 3.0479e-01, PNorm = 130.2829, GNorm = 0.2994
Meta loss on this task batch = 3.5041e-01, Meta loss averaged over last 500 steps = 3.0489e-01, PNorm = 130.2873, GNorm = 0.2797
Meta loss on this task batch = 2.3852e-01, Meta loss averaged over last 500 steps = 3.0486e-01, PNorm = 130.2928, GNorm = 0.3148
Meta loss on this task batch = 3.2981e-01, Meta loss averaged over last 500 steps = 3.0489e-01, PNorm = 130.2972, GNorm = 0.3920
Meta loss on this task batch = 3.4619e-01, Meta loss averaged over last 500 steps = 3.0485e-01, PNorm = 130.3025, GNorm = 0.3962
Meta loss on this task batch = 2.8062e-01, Meta loss averaged over last 500 steps = 3.0484e-01, PNorm = 130.3087, GNorm = 0.3587
Meta loss on this task batch = 3.1418e-01, Meta loss averaged over last 500 steps = 3.0483e-01, PNorm = 130.3150, GNorm = 0.4439
Took 110.64577436447144 seconds to complete one epoch of meta training
Took 118.18227481842041 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449036
Epoch 462
Meta loss on this task batch = 3.2564e-01, Meta loss averaged over last 500 steps = 3.0487e-01, PNorm = 130.3212, GNorm = 0.2578
Meta loss on this task batch = 3.2597e-01, Meta loss averaged over last 500 steps = 3.0494e-01, PNorm = 130.3280, GNorm = 0.2478
Meta loss on this task batch = 2.9281e-01, Meta loss averaged over last 500 steps = 3.0489e-01, PNorm = 130.3349, GNorm = 0.2347
Meta loss on this task batch = 3.3459e-01, Meta loss averaged over last 500 steps = 3.0492e-01, PNorm = 130.3422, GNorm = 0.3683
Meta loss on this task batch = 2.9267e-01, Meta loss averaged over last 500 steps = 3.0496e-01, PNorm = 130.3502, GNorm = 0.3297
Meta loss on this task batch = 3.0651e-01, Meta loss averaged over last 500 steps = 3.0490e-01, PNorm = 130.3565, GNorm = 0.2878
Meta loss on this task batch = 2.9183e-01, Meta loss averaged over last 500 steps = 3.0493e-01, PNorm = 130.3640, GNorm = 0.3021
Meta loss on this task batch = 2.9211e-01, Meta loss averaged over last 500 steps = 3.0493e-01, PNorm = 130.3716, GNorm = 0.2535
Meta loss on this task batch = 2.6578e-01, Meta loss averaged over last 500 steps = 3.0499e-01, PNorm = 130.3799, GNorm = 0.2582
Meta loss on this task batch = 3.0885e-01, Meta loss averaged over last 500 steps = 3.0498e-01, PNorm = 130.3891, GNorm = 0.2789
Meta loss on this task batch = 2.6511e-01, Meta loss averaged over last 500 steps = 3.0491e-01, PNorm = 130.3984, GNorm = 0.2317
Meta loss on this task batch = 2.9409e-01, Meta loss averaged over last 500 steps = 3.0495e-01, PNorm = 130.4068, GNorm = 0.2565
Meta loss on this task batch = 2.9611e-01, Meta loss averaged over last 500 steps = 3.0497e-01, PNorm = 130.4149, GNorm = 0.2474
Meta loss on this task batch = 2.5456e-01, Meta loss averaged over last 500 steps = 3.0489e-01, PNorm = 130.4232, GNorm = 0.3719
Meta loss on this task batch = 3.7889e-01, Meta loss averaged over last 500 steps = 3.0494e-01, PNorm = 130.4297, GNorm = 0.5587
Meta loss on this task batch = 2.9058e-01, Meta loss averaged over last 500 steps = 3.0489e-01, PNorm = 130.4347, GNorm = 0.4999
Meta loss on this task batch = 3.6881e-01, Meta loss averaged over last 500 steps = 3.0489e-01, PNorm = 130.4387, GNorm = 0.3186
Meta loss on this task batch = 2.9891e-01, Meta loss averaged over last 500 steps = 3.0476e-01, PNorm = 130.4426, GNorm = 0.2746
Meta loss on this task batch = 3.0459e-01, Meta loss averaged over last 500 steps = 3.0474e-01, PNorm = 130.4464, GNorm = 0.3262
Took 112.37402033805847 seconds to complete one epoch of meta training
Took 120.06572604179382 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483808
Epoch 463
Meta loss on this task batch = 3.0006e-01, Meta loss averaged over last 500 steps = 3.0473e-01, PNorm = 130.4504, GNorm = 0.3722
Meta loss on this task batch = 2.8653e-01, Meta loss averaged over last 500 steps = 3.0470e-01, PNorm = 130.4550, GNorm = 0.2712
Meta loss on this task batch = 3.1788e-01, Meta loss averaged over last 500 steps = 3.0475e-01, PNorm = 130.4590, GNorm = 0.3767
Meta loss on this task batch = 3.7395e-01, Meta loss averaged over last 500 steps = 3.0492e-01, PNorm = 130.4625, GNorm = 0.4676
Meta loss on this task batch = 3.4419e-01, Meta loss averaged over last 500 steps = 3.0503e-01, PNorm = 130.4660, GNorm = 0.2460
Meta loss on this task batch = 3.5042e-01, Meta loss averaged over last 500 steps = 3.0526e-01, PNorm = 130.4691, GNorm = 0.3537
Meta loss on this task batch = 3.2414e-01, Meta loss averaged over last 500 steps = 3.0538e-01, PNorm = 130.4737, GNorm = 0.3586
Meta loss on this task batch = 3.6409e-01, Meta loss averaged over last 500 steps = 3.0550e-01, PNorm = 130.4783, GNorm = 0.2889
Meta loss on this task batch = 2.9925e-01, Meta loss averaged over last 500 steps = 3.0535e-01, PNorm = 130.4833, GNorm = 0.2852
Meta loss on this task batch = 3.2304e-01, Meta loss averaged over last 500 steps = 3.0553e-01, PNorm = 130.4890, GNorm = 0.3009
Meta loss on this task batch = 3.1177e-01, Meta loss averaged over last 500 steps = 3.0556e-01, PNorm = 130.4945, GNorm = 0.3157
Meta loss on this task batch = 2.8942e-01, Meta loss averaged over last 500 steps = 3.0538e-01, PNorm = 130.5002, GNorm = 0.4198
Meta loss on this task batch = 3.0671e-01, Meta loss averaged over last 500 steps = 3.0537e-01, PNorm = 130.5055, GNorm = 0.3089
Meta loss on this task batch = 2.6200e-01, Meta loss averaged over last 500 steps = 3.0528e-01, PNorm = 130.5112, GNorm = 0.2565
Meta loss on this task batch = 2.9373e-01, Meta loss averaged over last 500 steps = 3.0536e-01, PNorm = 130.5170, GNorm = 0.3184
Meta loss on this task batch = 2.9127e-01, Meta loss averaged over last 500 steps = 3.0526e-01, PNorm = 130.5230, GNorm = 0.2421
Meta loss on this task batch = 2.8643e-01, Meta loss averaged over last 500 steps = 3.0521e-01, PNorm = 130.5295, GNorm = 0.3674
Meta loss on this task batch = 2.6884e-01, Meta loss averaged over last 500 steps = 3.0514e-01, PNorm = 130.5352, GNorm = 0.3042
Meta loss on this task batch = 2.6092e-01, Meta loss averaged over last 500 steps = 3.0513e-01, PNorm = 130.5404, GNorm = 0.3233
Took 111.30694818496704 seconds to complete one epoch of meta training
Took 119.14178562164307 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490366
Epoch 464
Meta loss on this task batch = 3.0242e-01, Meta loss averaged over last 500 steps = 3.0524e-01, PNorm = 130.5456, GNorm = 0.2780
Meta loss on this task batch = 3.1604e-01, Meta loss averaged over last 500 steps = 3.0516e-01, PNorm = 130.5499, GNorm = 0.4768
Meta loss on this task batch = 3.2728e-01, Meta loss averaged over last 500 steps = 3.0521e-01, PNorm = 130.5538, GNorm = 0.4481
Meta loss on this task batch = 2.8899e-01, Meta loss averaged over last 500 steps = 3.0523e-01, PNorm = 130.5580, GNorm = 0.2927
Meta loss on this task batch = 2.9497e-01, Meta loss averaged over last 500 steps = 3.0525e-01, PNorm = 130.5624, GNorm = 0.3758
Meta loss on this task batch = 2.6453e-01, Meta loss averaged over last 500 steps = 3.0504e-01, PNorm = 130.5664, GNorm = 0.3410
Meta loss on this task batch = 3.0913e-01, Meta loss averaged over last 500 steps = 3.0503e-01, PNorm = 130.5702, GNorm = 0.4347
Meta loss on this task batch = 3.0483e-01, Meta loss averaged over last 500 steps = 3.0507e-01, PNorm = 130.5736, GNorm = 0.2949
Meta loss on this task batch = 4.0505e-01, Meta loss averaged over last 500 steps = 3.0531e-01, PNorm = 130.5760, GNorm = 0.3144
Meta loss on this task batch = 3.1851e-01, Meta loss averaged over last 500 steps = 3.0532e-01, PNorm = 130.5795, GNorm = 0.3567
Meta loss on this task batch = 3.0914e-01, Meta loss averaged over last 500 steps = 3.0534e-01, PNorm = 130.5839, GNorm = 0.3083
Meta loss on this task batch = 2.8100e-01, Meta loss averaged over last 500 steps = 3.0528e-01, PNorm = 130.5890, GNorm = 0.2632
Meta loss on this task batch = 2.7130e-01, Meta loss averaged over last 500 steps = 3.0515e-01, PNorm = 130.5949, GNorm = 0.2568
Meta loss on this task batch = 3.6173e-01, Meta loss averaged over last 500 steps = 3.0525e-01, PNorm = 130.6003, GNorm = 0.3480
Meta loss on this task batch = 3.2154e-01, Meta loss averaged over last 500 steps = 3.0527e-01, PNorm = 130.6065, GNorm = 0.2700
Meta loss on this task batch = 2.4962e-01, Meta loss averaged over last 500 steps = 3.0525e-01, PNorm = 130.6130, GNorm = 0.2520
Meta loss on this task batch = 3.3230e-01, Meta loss averaged over last 500 steps = 3.0530e-01, PNorm = 130.6193, GNorm = 0.3297
Meta loss on this task batch = 2.4112e-01, Meta loss averaged over last 500 steps = 3.0518e-01, PNorm = 130.6260, GNorm = 0.2868
Meta loss on this task batch = 2.9718e-01, Meta loss averaged over last 500 steps = 3.0514e-01, PNorm = 130.6322, GNorm = 0.3539
Took 110.74372577667236 seconds to complete one epoch of meta training
Took 118.38351583480835 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474087
Epoch 465
Meta loss on this task batch = 2.9170e-01, Meta loss averaged over last 500 steps = 3.0508e-01, PNorm = 130.6387, GNorm = 0.2497
Meta loss on this task batch = 3.0711e-01, Meta loss averaged over last 500 steps = 3.0512e-01, PNorm = 130.6453, GNorm = 0.2455
Meta loss on this task batch = 2.9256e-01, Meta loss averaged over last 500 steps = 3.0506e-01, PNorm = 130.6522, GNorm = 0.2080
Meta loss on this task batch = 3.3611e-01, Meta loss averaged over last 500 steps = 3.0513e-01, PNorm = 130.6593, GNorm = 0.5071
Meta loss on this task batch = 4.0060e-01, Meta loss averaged over last 500 steps = 3.0533e-01, PNorm = 130.6647, GNorm = 0.3155
Meta loss on this task batch = 2.9151e-01, Meta loss averaged over last 500 steps = 3.0532e-01, PNorm = 130.6699, GNorm = 0.4873
Meta loss on this task batch = 2.8559e-01, Meta loss averaged over last 500 steps = 3.0519e-01, PNorm = 130.6760, GNorm = 0.2952
Meta loss on this task batch = 2.8021e-01, Meta loss averaged over last 500 steps = 3.0511e-01, PNorm = 130.6825, GNorm = 0.2672
Meta loss on this task batch = 3.0871e-01, Meta loss averaged over last 500 steps = 3.0504e-01, PNorm = 130.6894, GNorm = 0.2436
Meta loss on this task batch = 2.9354e-01, Meta loss averaged over last 500 steps = 3.0491e-01, PNorm = 130.6969, GNorm = 0.3065
Meta loss on this task batch = 2.6583e-01, Meta loss averaged over last 500 steps = 3.0481e-01, PNorm = 130.7048, GNorm = 0.4282
Meta loss on this task batch = 2.3110e-01, Meta loss averaged over last 500 steps = 3.0469e-01, PNorm = 130.7134, GNorm = 0.2196
Meta loss on this task batch = 3.0149e-01, Meta loss averaged over last 500 steps = 3.0473e-01, PNorm = 130.7205, GNorm = 0.3041
Meta loss on this task batch = 3.0493e-01, Meta loss averaged over last 500 steps = 3.0477e-01, PNorm = 130.7266, GNorm = 0.2777
Meta loss on this task batch = 3.1130e-01, Meta loss averaged over last 500 steps = 3.0485e-01, PNorm = 130.7314, GNorm = 0.2701
Meta loss on this task batch = 3.1201e-01, Meta loss averaged over last 500 steps = 3.0486e-01, PNorm = 130.7370, GNorm = 0.3619
Meta loss on this task batch = 2.8950e-01, Meta loss averaged over last 500 steps = 3.0489e-01, PNorm = 130.7428, GNorm = 0.2322
Meta loss on this task batch = 2.9352e-01, Meta loss averaged over last 500 steps = 3.0484e-01, PNorm = 130.7486, GNorm = 0.4587
Meta loss on this task batch = 3.2226e-01, Meta loss averaged over last 500 steps = 3.0479e-01, PNorm = 130.7528, GNorm = 0.4062
Took 111.97206664085388 seconds to complete one epoch of meta training
Took 119.98364281654358 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497659
Epoch 466
Meta loss on this task batch = 3.4728e-01, Meta loss averaged over last 500 steps = 3.0498e-01, PNorm = 130.7569, GNorm = 0.4414
Meta loss on this task batch = 3.2182e-01, Meta loss averaged over last 500 steps = 3.0494e-01, PNorm = 130.7617, GNorm = 0.3733
Meta loss on this task batch = 3.0217e-01, Meta loss averaged over last 500 steps = 3.0497e-01, PNorm = 130.7674, GNorm = 0.3304
Meta loss on this task batch = 3.2755e-01, Meta loss averaged over last 500 steps = 3.0496e-01, PNorm = 130.7714, GNorm = 0.2886
Meta loss on this task batch = 2.8995e-01, Meta loss averaged over last 500 steps = 3.0486e-01, PNorm = 130.7752, GNorm = 0.2797
Meta loss on this task batch = 2.6431e-01, Meta loss averaged over last 500 steps = 3.0487e-01, PNorm = 130.7794, GNorm = 0.3008
Meta loss on this task batch = 2.5974e-01, Meta loss averaged over last 500 steps = 3.0467e-01, PNorm = 130.7846, GNorm = 0.2703
Meta loss on this task batch = 3.2646e-01, Meta loss averaged over last 500 steps = 3.0485e-01, PNorm = 130.7901, GNorm = 0.3240
Meta loss on this task batch = 2.7350e-01, Meta loss averaged over last 500 steps = 3.0471e-01, PNorm = 130.7962, GNorm = 0.2564
Meta loss on this task batch = 2.6898e-01, Meta loss averaged over last 500 steps = 3.0452e-01, PNorm = 130.8028, GNorm = 0.4466
Meta loss on this task batch = 2.9259e-01, Meta loss averaged over last 500 steps = 3.0445e-01, PNorm = 130.8101, GNorm = 0.2921
Meta loss on this task batch = 2.4180e-01, Meta loss averaged over last 500 steps = 3.0439e-01, PNorm = 130.8179, GNorm = 0.2534
Meta loss on this task batch = 2.7079e-01, Meta loss averaged over last 500 steps = 3.0424e-01, PNorm = 130.8254, GNorm = 0.2577
Meta loss on this task batch = 2.9778e-01, Meta loss averaged over last 500 steps = 3.0418e-01, PNorm = 130.8334, GNorm = 0.2935
Meta loss on this task batch = 3.0005e-01, Meta loss averaged over last 500 steps = 3.0417e-01, PNorm = 130.8414, GNorm = 0.2888
Meta loss on this task batch = 3.3820e-01, Meta loss averaged over last 500 steps = 3.0420e-01, PNorm = 130.8482, GNorm = 0.3340
Meta loss on this task batch = 2.8006e-01, Meta loss averaged over last 500 steps = 3.0428e-01, PNorm = 130.8549, GNorm = 0.2647
Meta loss on this task batch = 3.1897e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 130.8613, GNorm = 0.2652
Meta loss on this task batch = 3.4688e-01, Meta loss averaged over last 500 steps = 3.0435e-01, PNorm = 130.8677, GNorm = 0.3354
Took 111.86276316642761 seconds to complete one epoch of meta training
Took 119.5027666091919 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480566
Epoch 467
Meta loss on this task batch = 3.0889e-01, Meta loss averaged over last 500 steps = 3.0425e-01, PNorm = 130.8743, GNorm = 0.3266
Meta loss on this task batch = 3.6250e-01, Meta loss averaged over last 500 steps = 3.0441e-01, PNorm = 130.8796, GNorm = 0.3451
Meta loss on this task batch = 3.2583e-01, Meta loss averaged over last 500 steps = 3.0446e-01, PNorm = 130.8850, GNorm = 0.3944
Meta loss on this task batch = 3.0632e-01, Meta loss averaged over last 500 steps = 3.0436e-01, PNorm = 130.8904, GNorm = 0.3520
Meta loss on this task batch = 2.9945e-01, Meta loss averaged over last 500 steps = 3.0442e-01, PNorm = 130.8961, GNorm = 0.2392
Meta loss on this task batch = 2.3792e-01, Meta loss averaged over last 500 steps = 3.0430e-01, PNorm = 130.9015, GNorm = 0.2835
Meta loss on this task batch = 2.9790e-01, Meta loss averaged over last 500 steps = 3.0428e-01, PNorm = 130.9065, GNorm = 0.2921
Meta loss on this task batch = 2.5835e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 130.9118, GNorm = 0.3298
Meta loss on this task batch = 2.7764e-01, Meta loss averaged over last 500 steps = 3.0417e-01, PNorm = 130.9176, GNorm = 0.2661
Meta loss on this task batch = 3.3745e-01, Meta loss averaged over last 500 steps = 3.0429e-01, PNorm = 130.9229, GNorm = 0.3455
Meta loss on this task batch = 2.9237e-01, Meta loss averaged over last 500 steps = 3.0427e-01, PNorm = 130.9287, GNorm = 0.2660
Meta loss on this task batch = 3.2698e-01, Meta loss averaged over last 500 steps = 3.0430e-01, PNorm = 130.9344, GNorm = 0.3933
Meta loss on this task batch = 2.4642e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 130.9400, GNorm = 0.3322
Meta loss on this task batch = 2.5689e-01, Meta loss averaged over last 500 steps = 3.0416e-01, PNorm = 130.9457, GNorm = 0.2869
Meta loss on this task batch = 2.9633e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 130.9518, GNorm = 0.3262
Meta loss on this task batch = 2.9981e-01, Meta loss averaged over last 500 steps = 3.0401e-01, PNorm = 130.9580, GNorm = 0.2782
Meta loss on this task batch = 2.8339e-01, Meta loss averaged over last 500 steps = 3.0397e-01, PNorm = 130.9645, GNorm = 0.3134
Meta loss on this task batch = 3.0404e-01, Meta loss averaged over last 500 steps = 3.0392e-01, PNorm = 130.9712, GNorm = 0.3160
Meta loss on this task batch = 2.7738e-01, Meta loss averaged over last 500 steps = 3.0391e-01, PNorm = 130.9780, GNorm = 0.4301
Took 109.50425434112549 seconds to complete one epoch of meta training
Took 117.5097279548645 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488980
Epoch 468
Meta loss on this task batch = 3.1293e-01, Meta loss averaged over last 500 steps = 3.0392e-01, PNorm = 130.9841, GNorm = 0.4083
Meta loss on this task batch = 3.0698e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 130.9902, GNorm = 0.3106
Meta loss on this task batch = 2.9689e-01, Meta loss averaged over last 500 steps = 3.0408e-01, PNorm = 130.9960, GNorm = 0.2960
Meta loss on this task batch = 2.9076e-01, Meta loss averaged over last 500 steps = 3.0398e-01, PNorm = 131.0019, GNorm = 0.2802
Meta loss on this task batch = 2.8000e-01, Meta loss averaged over last 500 steps = 3.0390e-01, PNorm = 131.0074, GNorm = 0.3459
Meta loss on this task batch = 2.9721e-01, Meta loss averaged over last 500 steps = 3.0381e-01, PNorm = 131.0124, GNorm = 0.3664
Meta loss on this task batch = 2.7609e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 131.0174, GNorm = 0.3112
Meta loss on this task batch = 3.0351e-01, Meta loss averaged over last 500 steps = 3.0376e-01, PNorm = 131.0214, GNorm = 0.3427
Meta loss on this task batch = 3.0680e-01, Meta loss averaged over last 500 steps = 3.0366e-01, PNorm = 131.0255, GNorm = 0.4715
Meta loss on this task batch = 3.1855e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 131.0296, GNorm = 0.2878
Meta loss on this task batch = 3.2976e-01, Meta loss averaged over last 500 steps = 3.0376e-01, PNorm = 131.0343, GNorm = 0.4692
Meta loss on this task batch = 2.9414e-01, Meta loss averaged over last 500 steps = 3.0369e-01, PNorm = 131.0394, GNorm = 0.2774
Meta loss on this task batch = 2.7135e-01, Meta loss averaged over last 500 steps = 3.0364e-01, PNorm = 131.0440, GNorm = 0.3617
Meta loss on this task batch = 2.4253e-01, Meta loss averaged over last 500 steps = 3.0351e-01, PNorm = 131.0489, GNorm = 0.2503
Meta loss on this task batch = 3.1108e-01, Meta loss averaged over last 500 steps = 3.0367e-01, PNorm = 131.0538, GNorm = 0.3303
Meta loss on this task batch = 2.9967e-01, Meta loss averaged over last 500 steps = 3.0375e-01, PNorm = 131.0583, GNorm = 0.3894
Meta loss on this task batch = 3.1600e-01, Meta loss averaged over last 500 steps = 3.0375e-01, PNorm = 131.0631, GNorm = 0.2977
Meta loss on this task batch = 3.0770e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 131.0691, GNorm = 0.4314
Meta loss on this task batch = 3.0994e-01, Meta loss averaged over last 500 steps = 3.0377e-01, PNorm = 131.0749, GNorm = 0.3433
Took 115.02391600608826 seconds to complete one epoch of meta training
Took 121.90308117866516 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463456
Epoch 469
Meta loss on this task batch = 2.8696e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 131.0814, GNorm = 0.2514
Meta loss on this task batch = 2.7124e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 131.0879, GNorm = 0.3262
Meta loss on this task batch = 2.9082e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 131.0932, GNorm = 0.2674
Meta loss on this task batch = 2.7363e-01, Meta loss averaged over last 500 steps = 3.0344e-01, PNorm = 131.0985, GNorm = 0.2467
Meta loss on this task batch = 3.0130e-01, Meta loss averaged over last 500 steps = 3.0339e-01, PNorm = 131.1029, GNorm = 0.2870
Meta loss on this task batch = 3.0328e-01, Meta loss averaged over last 500 steps = 3.0351e-01, PNorm = 131.1076, GNorm = 0.2677
Meta loss on this task batch = 3.4100e-01, Meta loss averaged over last 500 steps = 3.0366e-01, PNorm = 131.1126, GNorm = 0.3138
Meta loss on this task batch = 3.5872e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 131.1170, GNorm = 0.3219
Meta loss on this task batch = 2.6405e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 131.1216, GNorm = 0.2379
Meta loss on this task batch = 3.5199e-01, Meta loss averaged over last 500 steps = 3.0364e-01, PNorm = 131.1263, GNorm = 0.3111
Meta loss on this task batch = 2.8269e-01, Meta loss averaged over last 500 steps = 3.0352e-01, PNorm = 131.1308, GNorm = 0.2535
Meta loss on this task batch = 2.8525e-01, Meta loss averaged over last 500 steps = 3.0351e-01, PNorm = 131.1355, GNorm = 0.2898
Meta loss on this task batch = 3.0520e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 131.1402, GNorm = 0.2656
Meta loss on this task batch = 2.8357e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 131.1455, GNorm = 0.2600
Meta loss on this task batch = 3.5551e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 131.1503, GNorm = 0.3147
Meta loss on this task batch = 3.0062e-01, Meta loss averaged over last 500 steps = 3.0379e-01, PNorm = 131.1564, GNorm = 0.3136
Meta loss on this task batch = 3.0750e-01, Meta loss averaged over last 500 steps = 3.0369e-01, PNorm = 131.1625, GNorm = 0.3932
Meta loss on this task batch = 2.9463e-01, Meta loss averaged over last 500 steps = 3.0362e-01, PNorm = 131.1687, GNorm = 0.2794
Meta loss on this task batch = 3.8743e-01, Meta loss averaged over last 500 steps = 3.0376e-01, PNorm = 131.1731, GNorm = 0.4467
Took 112.71217751502991 seconds to complete one epoch of meta training
Took 119.51202535629272 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454674
Epoch 470
Meta loss on this task batch = 2.6338e-01, Meta loss averaged over last 500 steps = 3.0361e-01, PNorm = 131.1777, GNorm = 0.3029
Meta loss on this task batch = 3.1316e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 131.1831, GNorm = 0.3738
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 3.0362e-01, PNorm = 131.1896, GNorm = 0.3857
Meta loss on this task batch = 3.1134e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 131.1960, GNorm = 0.4053
Meta loss on this task batch = 3.1877e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 131.2019, GNorm = 0.3361
Meta loss on this task batch = 2.7234e-01, Meta loss averaged over last 500 steps = 3.0353e-01, PNorm = 131.2083, GNorm = 0.3347
Meta loss on this task batch = 2.5850e-01, Meta loss averaged over last 500 steps = 3.0346e-01, PNorm = 131.2151, GNorm = 0.3132
Meta loss on this task batch = 2.6160e-01, Meta loss averaged over last 500 steps = 3.0335e-01, PNorm = 131.2220, GNorm = 0.2488
Meta loss on this task batch = 3.4968e-01, Meta loss averaged over last 500 steps = 3.0338e-01, PNorm = 131.2285, GNorm = 0.3029
Meta loss on this task batch = 2.9614e-01, Meta loss averaged over last 500 steps = 3.0335e-01, PNorm = 131.2344, GNorm = 0.3220
Meta loss on this task batch = 2.8628e-01, Meta loss averaged over last 500 steps = 3.0329e-01, PNorm = 131.2403, GNorm = 0.3003
Meta loss on this task batch = 3.1848e-01, Meta loss averaged over last 500 steps = 3.0333e-01, PNorm = 131.2458, GNorm = 0.2968
Meta loss on this task batch = 3.8285e-01, Meta loss averaged over last 500 steps = 3.0347e-01, PNorm = 131.2495, GNorm = 0.5384
Meta loss on this task batch = 3.6789e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 131.2525, GNorm = 0.3457
Meta loss on this task batch = 3.1680e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 131.2548, GNorm = 0.2716
Meta loss on this task batch = 2.9075e-01, Meta loss averaged over last 500 steps = 3.0358e-01, PNorm = 131.2578, GNorm = 0.3728
Meta loss on this task batch = 2.9789e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 131.2607, GNorm = 0.3624
Meta loss on this task batch = 3.1482e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 131.2631, GNorm = 0.2795
Meta loss on this task batch = 3.2586e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 131.2670, GNorm = 0.4388
Took 111.15120220184326 seconds to complete one epoch of meta training
Took 118.96022820472717 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456663
Epoch 471
Meta loss on this task batch = 2.7065e-01, Meta loss averaged over last 500 steps = 3.0367e-01, PNorm = 131.2713, GNorm = 0.2838
Meta loss on this task batch = 2.5522e-01, Meta loss averaged over last 500 steps = 3.0353e-01, PNorm = 131.2762, GNorm = 0.3140
Meta loss on this task batch = 3.3106e-01, Meta loss averaged over last 500 steps = 3.0371e-01, PNorm = 131.2808, GNorm = 0.5302
Meta loss on this task batch = 3.1992e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 131.2853, GNorm = 0.2933
Meta loss on this task batch = 2.9825e-01, Meta loss averaged over last 500 steps = 3.0365e-01, PNorm = 131.2911, GNorm = 0.2732
Meta loss on this task batch = 3.1755e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 131.2971, GNorm = 0.2775
Meta loss on this task batch = 2.9455e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 131.3039, GNorm = 0.2841
Meta loss on this task batch = 2.8914e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 131.3113, GNorm = 0.2970
Meta loss on this task batch = 2.8242e-01, Meta loss averaged over last 500 steps = 3.0322e-01, PNorm = 131.3195, GNorm = 0.2587
Meta loss on this task batch = 3.0370e-01, Meta loss averaged over last 500 steps = 3.0317e-01, PNorm = 131.3277, GNorm = 0.2811
Meta loss on this task batch = 3.0876e-01, Meta loss averaged over last 500 steps = 3.0324e-01, PNorm = 131.3353, GNorm = 0.2937
Meta loss on this task batch = 3.5376e-01, Meta loss averaged over last 500 steps = 3.0332e-01, PNorm = 131.3422, GNorm = 0.2849
Meta loss on this task batch = 3.2513e-01, Meta loss averaged over last 500 steps = 3.0333e-01, PNorm = 131.3487, GNorm = 0.2869
Meta loss on this task batch = 2.5475e-01, Meta loss averaged over last 500 steps = 3.0325e-01, PNorm = 131.3546, GNorm = 0.2975
Meta loss on this task batch = 3.1942e-01, Meta loss averaged over last 500 steps = 3.0330e-01, PNorm = 131.3599, GNorm = 0.2784
Meta loss on this task batch = 3.1795e-01, Meta loss averaged over last 500 steps = 3.0327e-01, PNorm = 131.3645, GNorm = 0.3969
Meta loss on this task batch = 3.2652e-01, Meta loss averaged over last 500 steps = 3.0339e-01, PNorm = 131.3683, GNorm = 0.3243
Meta loss on this task batch = 2.8367e-01, Meta loss averaged over last 500 steps = 3.0332e-01, PNorm = 131.3726, GNorm = 0.2320
Meta loss on this task batch = 2.4269e-01, Meta loss averaged over last 500 steps = 3.0323e-01, PNorm = 131.3782, GNorm = 0.3451
Took 111.98733019828796 seconds to complete one epoch of meta training
Took 119.75837302207947 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492283
Epoch 472
Meta loss on this task batch = 3.0114e-01, Meta loss averaged over last 500 steps = 3.0312e-01, PNorm = 131.3839, GNorm = 0.2922
Meta loss on this task batch = 2.8457e-01, Meta loss averaged over last 500 steps = 3.0321e-01, PNorm = 131.3893, GNorm = 0.2583
Meta loss on this task batch = 2.2161e-01, Meta loss averaged over last 500 steps = 3.0295e-01, PNorm = 131.3950, GNorm = 0.2564
Meta loss on this task batch = 2.9258e-01, Meta loss averaged over last 500 steps = 3.0304e-01, PNorm = 131.4011, GNorm = 0.2527
Meta loss on this task batch = 3.1774e-01, Meta loss averaged over last 500 steps = 3.0317e-01, PNorm = 131.4077, GNorm = 0.4227
Meta loss on this task batch = 2.6980e-01, Meta loss averaged over last 500 steps = 3.0317e-01, PNorm = 131.4145, GNorm = 0.4198
Meta loss on this task batch = 2.8888e-01, Meta loss averaged over last 500 steps = 3.0306e-01, PNorm = 131.4207, GNorm = 0.5876
Meta loss on this task batch = 3.6864e-01, Meta loss averaged over last 500 steps = 3.0329e-01, PNorm = 131.4268, GNorm = 0.3827
Meta loss on this task batch = 3.3857e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 131.4323, GNorm = 0.3657
Meta loss on this task batch = 3.0496e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 131.4382, GNorm = 0.4389
Meta loss on this task batch = 2.4803e-01, Meta loss averaged over last 500 steps = 3.0323e-01, PNorm = 131.4440, GNorm = 0.2589
Meta loss on this task batch = 3.4635e-01, Meta loss averaged over last 500 steps = 3.0325e-01, PNorm = 131.4492, GNorm = 0.3803
Meta loss on this task batch = 3.0796e-01, Meta loss averaged over last 500 steps = 3.0320e-01, PNorm = 131.4541, GNorm = 0.3243
Meta loss on this task batch = 2.8732e-01, Meta loss averaged over last 500 steps = 3.0312e-01, PNorm = 131.4597, GNorm = 0.3626
Meta loss on this task batch = 3.4121e-01, Meta loss averaged over last 500 steps = 3.0317e-01, PNorm = 131.4651, GNorm = 0.4382
Meta loss on this task batch = 3.0048e-01, Meta loss averaged over last 500 steps = 3.0312e-01, PNorm = 131.4709, GNorm = 0.3157
Meta loss on this task batch = 2.6034e-01, Meta loss averaged over last 500 steps = 3.0311e-01, PNorm = 131.4768, GNorm = 0.2453
Meta loss on this task batch = 3.8596e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 131.4811, GNorm = 0.3412
Meta loss on this task batch = 2.9929e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 131.4853, GNorm = 0.3346
Took 111.6372480392456 seconds to complete one epoch of meta training
Took 119.50285029411316 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483363
Epoch 473
Meta loss on this task batch = 3.2093e-01, Meta loss averaged over last 500 steps = 3.0338e-01, PNorm = 131.4901, GNorm = 0.4219
Meta loss on this task batch = 2.8001e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 131.4957, GNorm = 0.2589
Meta loss on this task batch = 3.1908e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 131.5011, GNorm = 0.3467
Meta loss on this task batch = 2.9538e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 131.5067, GNorm = 0.2463
Meta loss on this task batch = 3.0824e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 131.5114, GNorm = 0.3834
Meta loss on this task batch = 2.8890e-01, Meta loss averaged over last 500 steps = 3.0334e-01, PNorm = 131.5163, GNorm = 0.3559
Meta loss on this task batch = 3.0924e-01, Meta loss averaged over last 500 steps = 3.0338e-01, PNorm = 131.5211, GNorm = 0.2627
Meta loss on this task batch = 4.2466e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 131.5251, GNorm = 0.4102
Meta loss on this task batch = 3.1590e-01, Meta loss averaged over last 500 steps = 3.0365e-01, PNorm = 131.5292, GNorm = 0.3509
Meta loss on this task batch = 3.0966e-01, Meta loss averaged over last 500 steps = 3.0361e-01, PNorm = 131.5334, GNorm = 0.2845
Meta loss on this task batch = 3.1285e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 131.5380, GNorm = 0.2486
Meta loss on this task batch = 2.3577e-01, Meta loss averaged over last 500 steps = 3.0353e-01, PNorm = 131.5432, GNorm = 0.2754
Meta loss on this task batch = 2.9141e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 131.5488, GNorm = 0.2741
Meta loss on this task batch = 3.0476e-01, Meta loss averaged over last 500 steps = 3.0343e-01, PNorm = 131.5547, GNorm = 0.2796
Meta loss on this task batch = 3.0491e-01, Meta loss averaged over last 500 steps = 3.0347e-01, PNorm = 131.5605, GNorm = 0.3178
Meta loss on this task batch = 3.3171e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 131.5658, GNorm = 0.3099
Meta loss on this task batch = 2.7238e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 131.5713, GNorm = 0.2805
Meta loss on this task batch = 2.8728e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 131.5773, GNorm = 0.3126
Meta loss on this task batch = 3.2382e-01, Meta loss averaged over last 500 steps = 3.0352e-01, PNorm = 131.5834, GNorm = 0.3806
Took 112.89456081390381 seconds to complete one epoch of meta training
Took 120.43181324005127 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461264
Epoch 474
Meta loss on this task batch = 2.7372e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 131.5905, GNorm = 0.2580
Meta loss on this task batch = 2.6460e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 131.5985, GNorm = 0.2960
Meta loss on this task batch = 3.0365e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 131.6055, GNorm = 0.3975
Meta loss on this task batch = 3.8113e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 131.6115, GNorm = 0.3484
Meta loss on this task batch = 2.9091e-01, Meta loss averaged over last 500 steps = 3.0352e-01, PNorm = 131.6178, GNorm = 0.3190
Meta loss on this task batch = 3.1271e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 131.6235, GNorm = 0.2493
Meta loss on this task batch = 2.8243e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 131.6281, GNorm = 0.3374
Meta loss on this task batch = 2.9662e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 131.6329, GNorm = 0.3422
Meta loss on this task batch = 3.1835e-01, Meta loss averaged over last 500 steps = 3.0366e-01, PNorm = 131.6377, GNorm = 0.4378
Meta loss on this task batch = 3.5413e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 131.6422, GNorm = 0.3032
Meta loss on this task batch = 3.5161e-01, Meta loss averaged over last 500 steps = 3.0388e-01, PNorm = 131.6472, GNorm = 0.4179
Meta loss on this task batch = 2.7846e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 131.6517, GNorm = 0.2825
Meta loss on this task batch = 3.1508e-01, Meta loss averaged over last 500 steps = 3.0383e-01, PNorm = 131.6558, GNorm = 0.2819
Meta loss on this task batch = 2.2784e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 131.6600, GNorm = 0.2812
Meta loss on this task batch = 3.2373e-01, Meta loss averaged over last 500 steps = 3.0382e-01, PNorm = 131.6642, GNorm = 0.3006
Meta loss on this task batch = 3.0359e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 131.6680, GNorm = 0.2951
Meta loss on this task batch = 3.2821e-01, Meta loss averaged over last 500 steps = 3.0379e-01, PNorm = 131.6718, GNorm = 0.3934
Meta loss on this task batch = 3.2324e-01, Meta loss averaged over last 500 steps = 3.0376e-01, PNorm = 131.6753, GNorm = 0.3482
Meta loss on this task batch = 2.4447e-01, Meta loss averaged over last 500 steps = 3.0367e-01, PNorm = 131.6800, GNorm = 0.2784
Took 112.57976174354553 seconds to complete one epoch of meta training
Took 119.4810631275177 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478339
Epoch 475
Meta loss on this task batch = 3.2828e-01, Meta loss averaged over last 500 steps = 3.0367e-01, PNorm = 131.6849, GNorm = 0.6126
Meta loss on this task batch = 3.3123e-01, Meta loss averaged over last 500 steps = 3.0371e-01, PNorm = 131.6888, GNorm = 0.3103
Meta loss on this task batch = 3.1636e-01, Meta loss averaged over last 500 steps = 3.0373e-01, PNorm = 131.6929, GNorm = 0.3775
Meta loss on this task batch = 3.2454e-01, Meta loss averaged over last 500 steps = 3.0365e-01, PNorm = 131.6976, GNorm = 0.2715
Meta loss on this task batch = 3.3242e-01, Meta loss averaged over last 500 steps = 3.0367e-01, PNorm = 131.7019, GNorm = 0.2778
Meta loss on this task batch = 3.1349e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 131.7078, GNorm = 0.4104
Meta loss on this task batch = 3.3893e-01, Meta loss averaged over last 500 steps = 3.0377e-01, PNorm = 131.7131, GNorm = 0.3375
Meta loss on this task batch = 3.0932e-01, Meta loss averaged over last 500 steps = 3.0381e-01, PNorm = 131.7195, GNorm = 0.2859
Meta loss on this task batch = 2.8966e-01, Meta loss averaged over last 500 steps = 3.0389e-01, PNorm = 131.7270, GNorm = 0.4668
Meta loss on this task batch = 2.8927e-01, Meta loss averaged over last 500 steps = 3.0392e-01, PNorm = 131.7349, GNorm = 0.2551
Meta loss on this task batch = 3.3807e-01, Meta loss averaged over last 500 steps = 3.0393e-01, PNorm = 131.7421, GNorm = 0.3686
Meta loss on this task batch = 2.8892e-01, Meta loss averaged over last 500 steps = 3.0390e-01, PNorm = 131.7494, GNorm = 0.2824
Meta loss on this task batch = 3.0909e-01, Meta loss averaged over last 500 steps = 3.0391e-01, PNorm = 131.7575, GNorm = 0.3663
Meta loss on this task batch = 3.0584e-01, Meta loss averaged over last 500 steps = 3.0383e-01, PNorm = 131.7658, GNorm = 0.2688
Meta loss on this task batch = 3.0068e-01, Meta loss averaged over last 500 steps = 3.0377e-01, PNorm = 131.7739, GNorm = 0.2695
Meta loss on this task batch = 3.0056e-01, Meta loss averaged over last 500 steps = 3.0378e-01, PNorm = 131.7821, GNorm = 0.3181
Meta loss on this task batch = 2.6958e-01, Meta loss averaged over last 500 steps = 3.0370e-01, PNorm = 131.7894, GNorm = 0.2704
Meta loss on this task batch = 2.5145e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 131.7970, GNorm = 0.2808
Meta loss on this task batch = 2.9894e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 131.8047, GNorm = 0.3550
Took 109.59262084960938 seconds to complete one epoch of meta training
Took 117.11468577384949 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482766
Epoch 476
Meta loss on this task batch = 3.0111e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 131.8123, GNorm = 0.3877
Meta loss on this task batch = 3.4743e-01, Meta loss averaged over last 500 steps = 3.0358e-01, PNorm = 131.8200, GNorm = 0.3737
Meta loss on this task batch = 3.2070e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 131.8270, GNorm = 0.3483
Meta loss on this task batch = 3.0098e-01, Meta loss averaged over last 500 steps = 3.0351e-01, PNorm = 131.8342, GNorm = 0.3422
Meta loss on this task batch = 2.7920e-01, Meta loss averaged over last 500 steps = 3.0344e-01, PNorm = 131.8418, GNorm = 0.3796
Meta loss on this task batch = 2.9595e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 131.8478, GNorm = 0.4727
Meta loss on this task batch = 2.4919e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 131.8542, GNorm = 0.3603
Meta loss on this task batch = 2.5652e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 131.8606, GNorm = 0.2476
Meta loss on this task batch = 2.9878e-01, Meta loss averaged over last 500 steps = 3.0344e-01, PNorm = 131.8663, GNorm = 0.3237
Meta loss on this task batch = 3.5226e-01, Meta loss averaged over last 500 steps = 3.0353e-01, PNorm = 131.8716, GNorm = 0.3078
Meta loss on this task batch = 3.0296e-01, Meta loss averaged over last 500 steps = 3.0344e-01, PNorm = 131.8768, GNorm = 0.3273
Meta loss on this task batch = 3.2483e-01, Meta loss averaged over last 500 steps = 3.0346e-01, PNorm = 131.8804, GNorm = 0.3837
Meta loss on this task batch = 3.2341e-01, Meta loss averaged over last 500 steps = 3.0347e-01, PNorm = 131.8841, GNorm = 0.2623
Meta loss on this task batch = 3.2571e-01, Meta loss averaged over last 500 steps = 3.0337e-01, PNorm = 131.8877, GNorm = 0.2979
Meta loss on this task batch = 3.4646e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 131.8921, GNorm = 0.3001
Meta loss on this task batch = 3.7518e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 131.8957, GNorm = 0.3361
Meta loss on this task batch = 2.4255e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 131.9003, GNorm = 0.3740
Meta loss on this task batch = 2.6384e-01, Meta loss averaged over last 500 steps = 3.0353e-01, PNorm = 131.9045, GNorm = 0.2221
Meta loss on this task batch = 3.0579e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 131.9085, GNorm = 0.3148
Took 112.54753923416138 seconds to complete one epoch of meta training
Took 120.3916015625 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452898
Epoch 477
Meta loss on this task batch = 2.9661e-01, Meta loss averaged over last 500 steps = 3.0364e-01, PNorm = 131.9128, GNorm = 0.3687
Meta loss on this task batch = 3.5787e-01, Meta loss averaged over last 500 steps = 3.0376e-01, PNorm = 131.9171, GNorm = 0.2833
Meta loss on this task batch = 3.1509e-01, Meta loss averaged over last 500 steps = 3.0377e-01, PNorm = 131.9219, GNorm = 0.3032
Meta loss on this task batch = 2.7195e-01, Meta loss averaged over last 500 steps = 3.0369e-01, PNorm = 131.9273, GNorm = 0.3518
Meta loss on this task batch = 2.8650e-01, Meta loss averaged over last 500 steps = 3.0365e-01, PNorm = 131.9326, GNorm = 0.3190
Meta loss on this task batch = 2.9355e-01, Meta loss averaged over last 500 steps = 3.0361e-01, PNorm = 131.9378, GNorm = 0.2892
Meta loss on this task batch = 3.0808e-01, Meta loss averaged over last 500 steps = 3.0371e-01, PNorm = 131.9431, GNorm = 0.2921
Meta loss on this task batch = 3.0995e-01, Meta loss averaged over last 500 steps = 3.0367e-01, PNorm = 131.9488, GNorm = 0.4602
Meta loss on this task batch = 2.9914e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 131.9548, GNorm = 0.2947
Meta loss on this task batch = 2.6859e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 131.9600, GNorm = 0.2822
Meta loss on this task batch = 2.8660e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 131.9643, GNorm = 0.3253
Meta loss on this task batch = 3.1990e-01, Meta loss averaged over last 500 steps = 3.0362e-01, PNorm = 131.9682, GNorm = 0.2602
Meta loss on this task batch = 3.4908e-01, Meta loss averaged over last 500 steps = 3.0371e-01, PNorm = 131.9724, GNorm = 0.2848
Meta loss on this task batch = 2.6032e-01, Meta loss averaged over last 500 steps = 3.0362e-01, PNorm = 131.9766, GNorm = 0.3072
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 131.9809, GNorm = 0.2663
Meta loss on this task batch = 3.0919e-01, Meta loss averaged over last 500 steps = 3.0351e-01, PNorm = 131.9844, GNorm = 0.2912
Meta loss on this task batch = 3.0073e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 131.9880, GNorm = 0.2892
Meta loss on this task batch = 2.4282e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 131.9922, GNorm = 0.2942
Meta loss on this task batch = 3.2598e-01, Meta loss averaged over last 500 steps = 3.0347e-01, PNorm = 131.9964, GNorm = 0.5887
Took 109.34240913391113 seconds to complete one epoch of meta training
Took 116.8395094871521 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482617
Epoch 478
Meta loss on this task batch = 2.7553e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 131.9998, GNorm = 0.3702
Meta loss on this task batch = 2.7079e-01, Meta loss averaged over last 500 steps = 3.0338e-01, PNorm = 132.0039, GNorm = 0.2439
Meta loss on this task batch = 3.0503e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 132.0071, GNorm = 0.3889
Meta loss on this task batch = 3.1044e-01, Meta loss averaged over last 500 steps = 3.0336e-01, PNorm = 132.0107, GNorm = 0.2757
Meta loss on this task batch = 2.6961e-01, Meta loss averaged over last 500 steps = 3.0326e-01, PNorm = 132.0150, GNorm = 0.2658
Meta loss on this task batch = 3.2491e-01, Meta loss averaged over last 500 steps = 3.0329e-01, PNorm = 132.0197, GNorm = 0.3142
Meta loss on this task batch = 2.8784e-01, Meta loss averaged over last 500 steps = 3.0325e-01, PNorm = 132.0246, GNorm = 0.2919
Meta loss on this task batch = 3.3121e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 132.0304, GNorm = 0.6110
Meta loss on this task batch = 2.5330e-01, Meta loss averaged over last 500 steps = 3.0320e-01, PNorm = 132.0370, GNorm = 0.2893
Meta loss on this task batch = 2.8216e-01, Meta loss averaged over last 500 steps = 3.0313e-01, PNorm = 132.0442, GNorm = 0.2813
Meta loss on this task batch = 3.2156e-01, Meta loss averaged over last 500 steps = 3.0325e-01, PNorm = 132.0504, GNorm = 0.2970
Meta loss on this task batch = 3.5329e-01, Meta loss averaged over last 500 steps = 3.0336e-01, PNorm = 132.0566, GNorm = 0.3163
Meta loss on this task batch = 3.8620e-01, Meta loss averaged over last 500 steps = 3.0336e-01, PNorm = 132.0616, GNorm = 0.3393
Meta loss on this task batch = 3.4832e-01, Meta loss averaged over last 500 steps = 3.0347e-01, PNorm = 132.0669, GNorm = 0.3101
Meta loss on this task batch = 2.8922e-01, Meta loss averaged over last 500 steps = 3.0347e-01, PNorm = 132.0723, GNorm = 0.3244
Meta loss on this task batch = 3.2171e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 132.0772, GNorm = 0.3242
Meta loss on this task batch = 2.7359e-01, Meta loss averaged over last 500 steps = 3.0347e-01, PNorm = 132.0824, GNorm = 0.2399
Meta loss on this task batch = 2.9765e-01, Meta loss averaged over last 500 steps = 3.0339e-01, PNorm = 132.0880, GNorm = 0.2695
Meta loss on this task batch = 2.6354e-01, Meta loss averaged over last 500 steps = 3.0323e-01, PNorm = 132.0936, GNorm = 0.2856
Took 110.50000667572021 seconds to complete one epoch of meta training
Took 118.46604776382446 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467739
Epoch 479
Meta loss on this task batch = 3.0078e-01, Meta loss averaged over last 500 steps = 3.0322e-01, PNorm = 132.0997, GNorm = 0.3763
Meta loss on this task batch = 2.6558e-01, Meta loss averaged over last 500 steps = 3.0320e-01, PNorm = 132.1052, GNorm = 0.2409
Meta loss on this task batch = 2.7571e-01, Meta loss averaged over last 500 steps = 3.0328e-01, PNorm = 132.1122, GNorm = 0.3420
Meta loss on this task batch = 2.9046e-01, Meta loss averaged over last 500 steps = 3.0316e-01, PNorm = 132.1191, GNorm = 0.2686
Meta loss on this task batch = 3.4521e-01, Meta loss averaged over last 500 steps = 3.0328e-01, PNorm = 132.1247, GNorm = 0.3196
Meta loss on this task batch = 2.8832e-01, Meta loss averaged over last 500 steps = 3.0322e-01, PNorm = 132.1303, GNorm = 0.3294
Meta loss on this task batch = 3.4195e-01, Meta loss averaged over last 500 steps = 3.0324e-01, PNorm = 132.1358, GNorm = 0.4011
Meta loss on this task batch = 3.0256e-01, Meta loss averaged over last 500 steps = 3.0312e-01, PNorm = 132.1408, GNorm = 0.2663
Meta loss on this task batch = 2.8706e-01, Meta loss averaged over last 500 steps = 3.0306e-01, PNorm = 132.1452, GNorm = 0.2564
Meta loss on this task batch = 3.0228e-01, Meta loss averaged over last 500 steps = 3.0309e-01, PNorm = 132.1498, GNorm = 0.3925
Meta loss on this task batch = 3.1743e-01, Meta loss averaged over last 500 steps = 3.0316e-01, PNorm = 132.1535, GNorm = 0.2566
Meta loss on this task batch = 3.5174e-01, Meta loss averaged over last 500 steps = 3.0318e-01, PNorm = 132.1565, GNorm = 0.3664
Meta loss on this task batch = 2.6167e-01, Meta loss averaged over last 500 steps = 3.0315e-01, PNorm = 132.1602, GNorm = 0.2934
Meta loss on this task batch = 3.3379e-01, Meta loss averaged over last 500 steps = 3.0322e-01, PNorm = 132.1645, GNorm = 0.2761
Meta loss on this task batch = 3.3060e-01, Meta loss averaged over last 500 steps = 3.0326e-01, PNorm = 132.1686, GNorm = 0.4466
Meta loss on this task batch = 3.3326e-01, Meta loss averaged over last 500 steps = 3.0330e-01, PNorm = 132.1738, GNorm = 0.3854
Meta loss on this task batch = 3.3653e-01, Meta loss averaged over last 500 steps = 3.0346e-01, PNorm = 132.1791, GNorm = 0.3806
Meta loss on this task batch = 2.9121e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 132.1855, GNorm = 0.3174
Meta loss on this task batch = 2.8313e-01, Meta loss averaged over last 500 steps = 3.0328e-01, PNorm = 132.1922, GNorm = 0.5186
Took 110.3797459602356 seconds to complete one epoch of meta training
Took 118.15363717079163 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487742
Epoch 480
Meta loss on this task batch = 2.9317e-01, Meta loss averaged over last 500 steps = 3.0328e-01, PNorm = 132.1990, GNorm = 0.2628
Meta loss on this task batch = 2.6179e-01, Meta loss averaged over last 500 steps = 3.0328e-01, PNorm = 132.2057, GNorm = 0.2553
Meta loss on this task batch = 2.6831e-01, Meta loss averaged over last 500 steps = 3.0319e-01, PNorm = 132.2126, GNorm = 0.2348
Meta loss on this task batch = 2.9978e-01, Meta loss averaged over last 500 steps = 3.0325e-01, PNorm = 132.2189, GNorm = 0.2947
Meta loss on this task batch = 2.9976e-01, Meta loss averaged over last 500 steps = 3.0329e-01, PNorm = 132.2253, GNorm = 0.3716
Meta loss on this task batch = 3.2867e-01, Meta loss averaged over last 500 steps = 3.0333e-01, PNorm = 132.2309, GNorm = 0.3489
Meta loss on this task batch = 2.9198e-01, Meta loss averaged over last 500 steps = 3.0334e-01, PNorm = 132.2363, GNorm = 0.3962
Meta loss on this task batch = 3.2078e-01, Meta loss averaged over last 500 steps = 3.0327e-01, PNorm = 132.2416, GNorm = 0.2867
Meta loss on this task batch = 2.9234e-01, Meta loss averaged over last 500 steps = 3.0326e-01, PNorm = 132.2468, GNorm = 0.2453
Meta loss on this task batch = 3.2673e-01, Meta loss averaged over last 500 steps = 3.0320e-01, PNorm = 132.2517, GNorm = 0.2953
Meta loss on this task batch = 3.8238e-01, Meta loss averaged over last 500 steps = 3.0326e-01, PNorm = 132.2551, GNorm = 0.4694
Meta loss on this task batch = 2.8681e-01, Meta loss averaged over last 500 steps = 3.0309e-01, PNorm = 132.2592, GNorm = 0.2405
Meta loss on this task batch = 2.6174e-01, Meta loss averaged over last 500 steps = 3.0307e-01, PNorm = 132.2630, GNorm = 0.2675
Meta loss on this task batch = 3.3379e-01, Meta loss averaged over last 500 steps = 3.0321e-01, PNorm = 132.2663, GNorm = 0.2904
Meta loss on this task batch = 3.2296e-01, Meta loss averaged over last 500 steps = 3.0327e-01, PNorm = 132.2687, GNorm = 0.4352
Meta loss on this task batch = 3.0524e-01, Meta loss averaged over last 500 steps = 3.0333e-01, PNorm = 132.2709, GNorm = 0.2845
Meta loss on this task batch = 2.8516e-01, Meta loss averaged over last 500 steps = 3.0325e-01, PNorm = 132.2733, GNorm = 0.2737
Meta loss on this task batch = 3.0769e-01, Meta loss averaged over last 500 steps = 3.0336e-01, PNorm = 132.2769, GNorm = 0.3456
Meta loss on this task batch = 3.6660e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 132.2791, GNorm = 0.3852
Took 109.1647515296936 seconds to complete one epoch of meta training
Took 117.11770701408386 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459113
Epoch 481
Meta loss on this task batch = 3.2987e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 132.2814, GNorm = 0.2524
Meta loss on this task batch = 2.9126e-01, Meta loss averaged over last 500 steps = 3.0371e-01, PNorm = 132.2845, GNorm = 0.2309
Meta loss on this task batch = 2.9612e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 132.2890, GNorm = 0.2955
Meta loss on this task batch = 3.0300e-01, Meta loss averaged over last 500 steps = 3.0376e-01, PNorm = 132.2944, GNorm = 0.2971
Meta loss on this task batch = 2.8942e-01, Meta loss averaged over last 500 steps = 3.0381e-01, PNorm = 132.3006, GNorm = 0.2497
Meta loss on this task batch = 3.2868e-01, Meta loss averaged over last 500 steps = 3.0380e-01, PNorm = 132.3069, GNorm = 0.3143
Meta loss on this task batch = 2.9158e-01, Meta loss averaged over last 500 steps = 3.0376e-01, PNorm = 132.3139, GNorm = 0.2838
Meta loss on this task batch = 2.9605e-01, Meta loss averaged over last 500 steps = 3.0371e-01, PNorm = 132.3205, GNorm = 0.2584
Meta loss on this task batch = 3.0029e-01, Meta loss averaged over last 500 steps = 3.0382e-01, PNorm = 132.3279, GNorm = 0.3781
Meta loss on this task batch = 3.2654e-01, Meta loss averaged over last 500 steps = 3.0387e-01, PNorm = 132.3347, GNorm = 0.3019
Meta loss on this task batch = 2.2322e-01, Meta loss averaged over last 500 steps = 3.0373e-01, PNorm = 132.3422, GNorm = 0.2526
Meta loss on this task batch = 3.2269e-01, Meta loss averaged over last 500 steps = 3.0380e-01, PNorm = 132.3498, GNorm = 0.2878
Meta loss on this task batch = 2.8124e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 132.3567, GNorm = 0.2835
Meta loss on this task batch = 3.0145e-01, Meta loss averaged over last 500 steps = 3.0365e-01, PNorm = 132.3635, GNorm = 0.2455
Meta loss on this task batch = 2.8590e-01, Meta loss averaged over last 500 steps = 3.0365e-01, PNorm = 132.3687, GNorm = 0.3169
Meta loss on this task batch = 2.5815e-01, Meta loss averaged over last 500 steps = 3.0358e-01, PNorm = 132.3739, GNorm = 0.2134
Meta loss on this task batch = 3.1673e-01, Meta loss averaged over last 500 steps = 3.0366e-01, PNorm = 132.3788, GNorm = 0.3461
Meta loss on this task batch = 3.4835e-01, Meta loss averaged over last 500 steps = 3.0371e-01, PNorm = 132.3831, GNorm = 0.2902
Meta loss on this task batch = 2.7362e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 132.3885, GNorm = 0.3382
Took 111.21649050712585 seconds to complete one epoch of meta training
Took 118.84397196769714 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.445182
Epoch 482
Meta loss on this task batch = 3.0687e-01, Meta loss averaged over last 500 steps = 3.0351e-01, PNorm = 132.3927, GNorm = 0.2997
Meta loss on this task batch = 3.1994e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 132.3966, GNorm = 0.2912
Meta loss on this task batch = 2.9884e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 132.4006, GNorm = 0.3034
Meta loss on this task batch = 2.7069e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 132.4047, GNorm = 0.2677
Meta loss on this task batch = 3.4335e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 132.4083, GNorm = 0.2910
Meta loss on this task batch = 3.1886e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 132.4122, GNorm = 0.3266
Meta loss on this task batch = 3.1448e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 132.4164, GNorm = 0.3716
Meta loss on this task batch = 2.9829e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 132.4205, GNorm = 0.2991
Meta loss on this task batch = 2.8215e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 132.4252, GNorm = 0.2933
Meta loss on this task batch = 3.0618e-01, Meta loss averaged over last 500 steps = 3.0344e-01, PNorm = 132.4294, GNorm = 0.2503
Meta loss on this task batch = 3.0995e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 132.4335, GNorm = 0.4335
Meta loss on this task batch = 2.9765e-01, Meta loss averaged over last 500 steps = 3.0344e-01, PNorm = 132.4380, GNorm = 0.2990
Meta loss on this task batch = 3.2280e-01, Meta loss averaged over last 500 steps = 3.0362e-01, PNorm = 132.4430, GNorm = 0.4459
Meta loss on this task batch = 2.9959e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 132.4479, GNorm = 0.3560
Meta loss on this task batch = 3.0738e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 132.4521, GNorm = 0.2859
Meta loss on this task batch = 3.5808e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 132.4563, GNorm = 0.4605
Meta loss on this task batch = 2.5970e-01, Meta loss averaged over last 500 steps = 3.0365e-01, PNorm = 132.4611, GNorm = 0.2489
Meta loss on this task batch = 2.4411e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 132.4656, GNorm = 0.3894
Meta loss on this task batch = 2.5875e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 132.4708, GNorm = 0.3723
Took 111.2506856918335 seconds to complete one epoch of meta training
Took 117.96142411231995 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.425062
Epoch 483
Meta loss on this task batch = 3.2140e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 132.4758, GNorm = 0.2514
Meta loss on this task batch = 3.0129e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 132.4813, GNorm = 0.3837
Meta loss on this task batch = 3.1287e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 132.4878, GNorm = 0.3360
Meta loss on this task batch = 2.9756e-01, Meta loss averaged over last 500 steps = 3.0352e-01, PNorm = 132.4941, GNorm = 0.2956
Meta loss on this task batch = 3.1379e-01, Meta loss averaged over last 500 steps = 3.0362e-01, PNorm = 132.4998, GNorm = 0.3136
Meta loss on this task batch = 3.5806e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 132.5043, GNorm = 0.3729
Meta loss on this task batch = 3.3227e-01, Meta loss averaged over last 500 steps = 3.0366e-01, PNorm = 132.5092, GNorm = 0.2864
Meta loss on this task batch = 3.2069e-01, Meta loss averaged over last 500 steps = 3.0373e-01, PNorm = 132.5138, GNorm = 0.4556
Meta loss on this task batch = 3.3690e-01, Meta loss averaged over last 500 steps = 3.0375e-01, PNorm = 132.5184, GNorm = 0.2764
Meta loss on this task batch = 2.8653e-01, Meta loss averaged over last 500 steps = 3.0377e-01, PNorm = 132.5232, GNorm = 0.2631
Meta loss on this task batch = 2.2066e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 132.5288, GNorm = 0.2295
Meta loss on this task batch = 2.6608e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 132.5339, GNorm = 0.3063
Meta loss on this task batch = 3.0914e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 132.5395, GNorm = 0.2972
Meta loss on this task batch = 3.7321e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 132.5446, GNorm = 0.4113
Meta loss on this task batch = 2.8274e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 132.5503, GNorm = 0.2762
Meta loss on this task batch = 3.3574e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 132.5544, GNorm = 0.5585
Meta loss on this task batch = 2.9727e-01, Meta loss averaged over last 500 steps = 3.0376e-01, PNorm = 132.5595, GNorm = 0.2674
Meta loss on this task batch = 3.5161e-01, Meta loss averaged over last 500 steps = 3.0391e-01, PNorm = 132.5652, GNorm = 0.5179
Meta loss on this task batch = 2.4734e-01, Meta loss averaged over last 500 steps = 3.0380e-01, PNorm = 132.5711, GNorm = 0.4047
Took 113.8354663848877 seconds to complete one epoch of meta training
Took 121.6425359249115 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459890
Epoch 484
Meta loss on this task batch = 3.6079e-01, Meta loss averaged over last 500 steps = 3.0387e-01, PNorm = 132.5760, GNorm = 0.3975
Meta loss on this task batch = 2.8348e-01, Meta loss averaged over last 500 steps = 3.0378e-01, PNorm = 132.5815, GNorm = 0.3048
Meta loss on this task batch = 2.7209e-01, Meta loss averaged over last 500 steps = 3.0371e-01, PNorm = 132.5865, GNorm = 0.3746
Meta loss on this task batch = 3.3934e-01, Meta loss averaged over last 500 steps = 3.0382e-01, PNorm = 132.5908, GNorm = 0.3384
Meta loss on this task batch = 3.2668e-01, Meta loss averaged over last 500 steps = 3.0380e-01, PNorm = 132.5955, GNorm = 0.2354
Meta loss on this task batch = 2.6668e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 132.6006, GNorm = 0.2919
Meta loss on this task batch = 3.4951e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 132.6038, GNorm = 0.3613
Meta loss on this task batch = 3.2681e-01, Meta loss averaged over last 500 steps = 3.0375e-01, PNorm = 132.6074, GNorm = 0.2812
Meta loss on this task batch = 3.0141e-01, Meta loss averaged over last 500 steps = 3.0377e-01, PNorm = 132.6105, GNorm = 0.2649
Meta loss on this task batch = 2.9535e-01, Meta loss averaged over last 500 steps = 3.0384e-01, PNorm = 132.6137, GNorm = 0.3046
Meta loss on this task batch = 3.3909e-01, Meta loss averaged over last 500 steps = 3.0395e-01, PNorm = 132.6173, GNorm = 0.2456
Meta loss on this task batch = 3.1950e-01, Meta loss averaged over last 500 steps = 3.0388e-01, PNorm = 132.6206, GNorm = 0.3315
Meta loss on this task batch = 3.0630e-01, Meta loss averaged over last 500 steps = 3.0391e-01, PNorm = 132.6238, GNorm = 0.3137
Meta loss on this task batch = 3.2377e-01, Meta loss averaged over last 500 steps = 3.0393e-01, PNorm = 132.6278, GNorm = 0.3993
Meta loss on this task batch = 3.0995e-01, Meta loss averaged over last 500 steps = 3.0394e-01, PNorm = 132.6341, GNorm = 0.6182
Meta loss on this task batch = 2.8414e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 132.6404, GNorm = 0.3006
Meta loss on this task batch = 3.2602e-01, Meta loss averaged over last 500 steps = 3.0395e-01, PNorm = 132.6462, GNorm = 0.2781
Meta loss on this task batch = 3.6097e-01, Meta loss averaged over last 500 steps = 3.0412e-01, PNorm = 132.6522, GNorm = 0.3462
Meta loss on this task batch = 2.5762e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 132.6584, GNorm = 0.4218
Took 112.14333534240723 seconds to complete one epoch of meta training
Took 119.52869439125061 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452720
Epoch 485
Meta loss on this task batch = 3.7065e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 132.6647, GNorm = 0.3223
Meta loss on this task batch = 3.0232e-01, Meta loss averaged over last 500 steps = 3.0420e-01, PNorm = 132.6718, GNorm = 0.3171
Meta loss on this task batch = 3.5765e-01, Meta loss averaged over last 500 steps = 3.0435e-01, PNorm = 132.6786, GNorm = 0.4094
Meta loss on this task batch = 3.3637e-01, Meta loss averaged over last 500 steps = 3.0431e-01, PNorm = 132.6853, GNorm = 0.3275
Meta loss on this task batch = 3.0269e-01, Meta loss averaged over last 500 steps = 3.0435e-01, PNorm = 132.6921, GNorm = 0.2749
Meta loss on this task batch = 2.8107e-01, Meta loss averaged over last 500 steps = 3.0426e-01, PNorm = 132.6987, GNorm = 0.2626
Meta loss on this task batch = 3.1355e-01, Meta loss averaged over last 500 steps = 3.0435e-01, PNorm = 132.7054, GNorm = 0.2809
Meta loss on this task batch = 3.0010e-01, Meta loss averaged over last 500 steps = 3.0438e-01, PNorm = 132.7120, GNorm = 0.3688
Meta loss on this task batch = 2.9135e-01, Meta loss averaged over last 500 steps = 3.0431e-01, PNorm = 132.7185, GNorm = 0.2822
Meta loss on this task batch = 3.1970e-01, Meta loss averaged over last 500 steps = 3.0430e-01, PNorm = 132.7254, GNorm = 0.3291
Meta loss on this task batch = 4.0486e-01, Meta loss averaged over last 500 steps = 3.0446e-01, PNorm = 132.7308, GNorm = 0.3639
Meta loss on this task batch = 2.7564e-01, Meta loss averaged over last 500 steps = 3.0428e-01, PNorm = 132.7366, GNorm = 0.2699
Meta loss on this task batch = 2.9134e-01, Meta loss averaged over last 500 steps = 3.0428e-01, PNorm = 132.7417, GNorm = 0.2643
Meta loss on this task batch = 2.7889e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 132.7477, GNorm = 0.2225
Meta loss on this task batch = 2.9942e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 132.7540, GNorm = 0.2924
Meta loss on this task batch = 2.6965e-01, Meta loss averaged over last 500 steps = 3.0407e-01, PNorm = 132.7606, GNorm = 0.2643
Meta loss on this task batch = 2.8946e-01, Meta loss averaged over last 500 steps = 3.0405e-01, PNorm = 132.7669, GNorm = 0.2659
Meta loss on this task batch = 2.9175e-01, Meta loss averaged over last 500 steps = 3.0412e-01, PNorm = 132.7731, GNorm = 0.2811
Meta loss on this task batch = 3.0708e-01, Meta loss averaged over last 500 steps = 3.0416e-01, PNorm = 132.7791, GNorm = 0.4093
Took 112.26327466964722 seconds to complete one epoch of meta training
Took 120.64793586730957 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459414
Epoch 486
Meta loss on this task batch = 3.2169e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 132.7852, GNorm = 0.3094
Meta loss on this task batch = 3.7989e-01, Meta loss averaged over last 500 steps = 3.0437e-01, PNorm = 132.7899, GNorm = 0.3426
Meta loss on this task batch = 2.7236e-01, Meta loss averaged over last 500 steps = 3.0435e-01, PNorm = 132.7956, GNorm = 0.2418
Meta loss on this task batch = 2.9288e-01, Meta loss averaged over last 500 steps = 3.0445e-01, PNorm = 132.8012, GNorm = 0.2937
Meta loss on this task batch = 3.2594e-01, Meta loss averaged over last 500 steps = 3.0448e-01, PNorm = 132.8063, GNorm = 0.3600
Meta loss on this task batch = 3.1358e-01, Meta loss averaged over last 500 steps = 3.0437e-01, PNorm = 132.8112, GNorm = 0.2789
Meta loss on this task batch = 2.7636e-01, Meta loss averaged over last 500 steps = 3.0435e-01, PNorm = 132.8160, GNorm = 0.2862
Meta loss on this task batch = 3.0785e-01, Meta loss averaged over last 500 steps = 3.0435e-01, PNorm = 132.8215, GNorm = 0.3623
Meta loss on this task batch = 3.0862e-01, Meta loss averaged over last 500 steps = 3.0437e-01, PNorm = 132.8268, GNorm = 0.2723
Meta loss on this task batch = 2.3466e-01, Meta loss averaged over last 500 steps = 3.0430e-01, PNorm = 132.8327, GNorm = 0.2338
Meta loss on this task batch = 2.7676e-01, Meta loss averaged over last 500 steps = 3.0418e-01, PNorm = 132.8382, GNorm = 0.2514
Meta loss on this task batch = 3.4745e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 132.8431, GNorm = 0.3425
Meta loss on this task batch = 3.2842e-01, Meta loss averaged over last 500 steps = 3.0420e-01, PNorm = 132.8474, GNorm = 0.3290
Meta loss on this task batch = 3.1575e-01, Meta loss averaged over last 500 steps = 3.0427e-01, PNorm = 132.8517, GNorm = 0.3641
Meta loss on this task batch = 2.8439e-01, Meta loss averaged over last 500 steps = 3.0428e-01, PNorm = 132.8571, GNorm = 0.2611
Meta loss on this task batch = 3.2835e-01, Meta loss averaged over last 500 steps = 3.0433e-01, PNorm = 132.8623, GNorm = 0.2560
Meta loss on this task batch = 2.7560e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 132.8680, GNorm = 0.3147
Meta loss on this task batch = 3.4297e-01, Meta loss averaged over last 500 steps = 3.0437e-01, PNorm = 132.8729, GNorm = 0.3731
Meta loss on this task batch = 2.6458e-01, Meta loss averaged over last 500 steps = 3.0436e-01, PNorm = 132.8783, GNorm = 0.4108
Took 113.91398167610168 seconds to complete one epoch of meta training
Took 121.57852745056152 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474229
Epoch 487
Meta loss on this task batch = 3.1875e-01, Meta loss averaged over last 500 steps = 3.0443e-01, PNorm = 132.8833, GNorm = 0.3049
Meta loss on this task batch = 3.0602e-01, Meta loss averaged over last 500 steps = 3.0440e-01, PNorm = 132.8878, GNorm = 0.2599
Meta loss on this task batch = 2.6528e-01, Meta loss averaged over last 500 steps = 3.0435e-01, PNorm = 132.8923, GNorm = 0.2233
Meta loss on this task batch = 3.2320e-01, Meta loss averaged over last 500 steps = 3.0438e-01, PNorm = 132.8958, GNorm = 0.3512
Meta loss on this task batch = 2.4652e-01, Meta loss averaged over last 500 steps = 3.0420e-01, PNorm = 132.9003, GNorm = 0.2739
Meta loss on this task batch = 2.8260e-01, Meta loss averaged over last 500 steps = 3.0413e-01, PNorm = 132.9058, GNorm = 0.2995
Meta loss on this task batch = 3.5260e-01, Meta loss averaged over last 500 steps = 3.0415e-01, PNorm = 132.9115, GNorm = 0.3178
Meta loss on this task batch = 2.9232e-01, Meta loss averaged over last 500 steps = 3.0420e-01, PNorm = 132.9174, GNorm = 0.3223
Meta loss on this task batch = 3.1250e-01, Meta loss averaged over last 500 steps = 3.0424e-01, PNorm = 132.9231, GNorm = 0.2988
Meta loss on this task batch = 2.8749e-01, Meta loss averaged over last 500 steps = 3.0413e-01, PNorm = 132.9280, GNorm = 0.2480
Meta loss on this task batch = 2.8780e-01, Meta loss averaged over last 500 steps = 3.0414e-01, PNorm = 132.9333, GNorm = 0.2782
Meta loss on this task batch = 3.2331e-01, Meta loss averaged over last 500 steps = 3.0422e-01, PNorm = 132.9376, GNorm = 0.3097
Meta loss on this task batch = 3.1358e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 132.9419, GNorm = 0.2825
Meta loss on this task batch = 3.2657e-01, Meta loss averaged over last 500 steps = 3.0417e-01, PNorm = 132.9458, GNorm = 0.3472
Meta loss on this task batch = 2.9389e-01, Meta loss averaged over last 500 steps = 3.0415e-01, PNorm = 132.9504, GNorm = 0.2413
Meta loss on this task batch = 2.8797e-01, Meta loss averaged over last 500 steps = 3.0414e-01, PNorm = 132.9558, GNorm = 0.2435
Meta loss on this task batch = 2.7951e-01, Meta loss averaged over last 500 steps = 3.0409e-01, PNorm = 132.9611, GNorm = 0.3105
Meta loss on this task batch = 2.9386e-01, Meta loss averaged over last 500 steps = 3.0402e-01, PNorm = 132.9666, GNorm = 0.2825
Meta loss on this task batch = 2.9956e-01, Meta loss averaged over last 500 steps = 3.0407e-01, PNorm = 132.9728, GNorm = 0.3106
Took 111.28694868087769 seconds to complete one epoch of meta training
Took 118.23299670219421 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453497
Epoch 488
Meta loss on this task batch = 2.7092e-01, Meta loss averaged over last 500 steps = 3.0391e-01, PNorm = 132.9792, GNorm = 0.2551
Meta loss on this task batch = 3.1816e-01, Meta loss averaged over last 500 steps = 3.0407e-01, PNorm = 132.9858, GNorm = 0.3348
Meta loss on this task batch = 2.4053e-01, Meta loss averaged over last 500 steps = 3.0389e-01, PNorm = 132.9928, GNorm = 0.2068
Meta loss on this task batch = 3.0025e-01, Meta loss averaged over last 500 steps = 3.0380e-01, PNorm = 132.9999, GNorm = 0.3597
Meta loss on this task batch = 2.5908e-01, Meta loss averaged over last 500 steps = 3.0375e-01, PNorm = 133.0068, GNorm = 0.3040
Meta loss on this task batch = 3.1887e-01, Meta loss averaged over last 500 steps = 3.0376e-01, PNorm = 133.0138, GNorm = 0.3208
Meta loss on this task batch = 3.0717e-01, Meta loss averaged over last 500 steps = 3.0373e-01, PNorm = 133.0208, GNorm = 0.2501
Meta loss on this task batch = 2.6984e-01, Meta loss averaged over last 500 steps = 3.0361e-01, PNorm = 133.0274, GNorm = 0.3123
Meta loss on this task batch = 3.2985e-01, Meta loss averaged over last 500 steps = 3.0369e-01, PNorm = 133.0330, GNorm = 0.3099
Meta loss on this task batch = 2.9175e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 133.0384, GNorm = 0.3951
Meta loss on this task batch = 2.9945e-01, Meta loss averaged over last 500 steps = 3.0362e-01, PNorm = 133.0436, GNorm = 0.3082
Meta loss on this task batch = 3.3428e-01, Meta loss averaged over last 500 steps = 3.0367e-01, PNorm = 133.0489, GNorm = 0.2817
Meta loss on this task batch = 3.1662e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 133.0527, GNorm = 0.4690
Meta loss on this task batch = 3.0423e-01, Meta loss averaged over last 500 steps = 3.0375e-01, PNorm = 133.0569, GNorm = 0.2727
Meta loss on this task batch = 3.6699e-01, Meta loss averaged over last 500 steps = 3.0395e-01, PNorm = 133.0615, GNorm = 0.2745
Meta loss on this task batch = 3.2268e-01, Meta loss averaged over last 500 steps = 3.0398e-01, PNorm = 133.0666, GNorm = 0.3423
Meta loss on this task batch = 3.1142e-01, Meta loss averaged over last 500 steps = 3.0407e-01, PNorm = 133.0719, GNorm = 0.3365
Meta loss on this task batch = 3.0634e-01, Meta loss averaged over last 500 steps = 3.0409e-01, PNorm = 133.0778, GNorm = 0.2416
Meta loss on this task batch = 3.6102e-01, Meta loss averaged over last 500 steps = 3.0422e-01, PNorm = 133.0828, GNorm = 0.3058
Took 107.80200600624084 seconds to complete one epoch of meta training
Took 115.59139609336853 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452302
Epoch 489
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 3.0429e-01, PNorm = 133.0877, GNorm = 0.2626
Meta loss on this task batch = 2.9321e-01, Meta loss averaged over last 500 steps = 3.0411e-01, PNorm = 133.0926, GNorm = 0.3903
Meta loss on this task batch = 3.2405e-01, Meta loss averaged over last 500 steps = 3.0418e-01, PNorm = 133.0977, GNorm = 0.3367
Meta loss on this task batch = 3.0768e-01, Meta loss averaged over last 500 steps = 3.0406e-01, PNorm = 133.1034, GNorm = 0.2660
Meta loss on this task batch = 2.9762e-01, Meta loss averaged over last 500 steps = 3.0406e-01, PNorm = 133.1089, GNorm = 0.3196
Meta loss on this task batch = 2.8672e-01, Meta loss averaged over last 500 steps = 3.0402e-01, PNorm = 133.1147, GNorm = 0.4162
Meta loss on this task batch = 2.9174e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 133.1199, GNorm = 0.3590
Meta loss on this task batch = 2.7337e-01, Meta loss averaged over last 500 steps = 3.0398e-01, PNorm = 133.1254, GNorm = 0.3593
Meta loss on this task batch = 2.9413e-01, Meta loss averaged over last 500 steps = 3.0393e-01, PNorm = 133.1310, GNorm = 0.2464
Meta loss on this task batch = 2.8475e-01, Meta loss averaged over last 500 steps = 3.0375e-01, PNorm = 133.1362, GNorm = 0.2539
Meta loss on this task batch = 3.1892e-01, Meta loss averaged over last 500 steps = 3.0370e-01, PNorm = 133.1407, GNorm = 0.2523
Meta loss on this task batch = 3.2262e-01, Meta loss averaged over last 500 steps = 3.0365e-01, PNorm = 133.1448, GNorm = 0.3063
Meta loss on this task batch = 2.3891e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 133.1494, GNorm = 0.2647
Meta loss on this task batch = 2.9922e-01, Meta loss averaged over last 500 steps = 3.0335e-01, PNorm = 133.1547, GNorm = 0.2926
Meta loss on this task batch = 3.0925e-01, Meta loss averaged over last 500 steps = 3.0337e-01, PNorm = 133.1597, GNorm = 0.2443
Meta loss on this task batch = 3.3571e-01, Meta loss averaged over last 500 steps = 3.0339e-01, PNorm = 133.1641, GNorm = 0.3114
Meta loss on this task batch = 3.2880e-01, Meta loss averaged over last 500 steps = 3.0343e-01, PNorm = 133.1683, GNorm = 0.2859
Meta loss on this task batch = 3.1345e-01, Meta loss averaged over last 500 steps = 3.0347e-01, PNorm = 133.1725, GNorm = 0.3718
Meta loss on this task batch = 2.6595e-01, Meta loss averaged over last 500 steps = 3.0339e-01, PNorm = 133.1772, GNorm = 0.3596
Took 110.68499636650085 seconds to complete one epoch of meta training
Took 118.47309589385986 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459847
Epoch 490
Meta loss on this task batch = 2.8977e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 133.1821, GNorm = 0.2848
Meta loss on this task batch = 3.1827e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 133.1863, GNorm = 0.2630
Meta loss on this task batch = 3.1651e-01, Meta loss averaged over last 500 steps = 3.0355e-01, PNorm = 133.1906, GNorm = 0.2533
Meta loss on this task batch = 3.2791e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 133.1952, GNorm = 0.3230
Meta loss on this task batch = 2.6344e-01, Meta loss averaged over last 500 steps = 3.0362e-01, PNorm = 133.1997, GNorm = 0.3350
Meta loss on this task batch = 2.9001e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 133.2044, GNorm = 0.3878
Meta loss on this task batch = 2.6170e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 133.2096, GNorm = 0.2379
Meta loss on this task batch = 3.2181e-01, Meta loss averaged over last 500 steps = 3.0361e-01, PNorm = 133.2145, GNorm = 0.4608
Meta loss on this task batch = 2.1877e-01, Meta loss averaged over last 500 steps = 3.0339e-01, PNorm = 133.2197, GNorm = 0.2070
Meta loss on this task batch = 3.0741e-01, Meta loss averaged over last 500 steps = 3.0343e-01, PNorm = 133.2250, GNorm = 0.3637
Meta loss on this task batch = 3.2484e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 133.2301, GNorm = 0.3656
Meta loss on this task batch = 3.3794e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 133.2346, GNorm = 0.3336
Meta loss on this task batch = 3.0475e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 133.2383, GNorm = 0.3065
Meta loss on this task batch = 3.3330e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 133.2412, GNorm = 0.3141
Meta loss on this task batch = 3.3819e-01, Meta loss averaged over last 500 steps = 3.0355e-01, PNorm = 133.2446, GNorm = 0.3491
Meta loss on this task batch = 3.0881e-01, Meta loss averaged over last 500 steps = 3.0353e-01, PNorm = 133.2478, GNorm = 0.2513
Meta loss on this task batch = 2.7092e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 133.2517, GNorm = 0.2710
Meta loss on this task batch = 3.1393e-01, Meta loss averaged over last 500 steps = 3.0352e-01, PNorm = 133.2555, GNorm = 0.3740
Meta loss on this task batch = 2.6622e-01, Meta loss averaged over last 500 steps = 3.0351e-01, PNorm = 133.2594, GNorm = 0.2649
Took 109.8403148651123 seconds to complete one epoch of meta training
Took 117.36143374443054 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489633
Epoch 491
Meta loss on this task batch = 2.7432e-01, Meta loss averaged over last 500 steps = 3.0333e-01, PNorm = 133.2637, GNorm = 0.2710
Meta loss on this task batch = 2.9804e-01, Meta loss averaged over last 500 steps = 3.0329e-01, PNorm = 133.2676, GNorm = 0.4005
Meta loss on this task batch = 2.8954e-01, Meta loss averaged over last 500 steps = 3.0337e-01, PNorm = 133.2721, GNorm = 0.3130
Meta loss on this task batch = 3.3068e-01, Meta loss averaged over last 500 steps = 3.0336e-01, PNorm = 133.2769, GNorm = 0.3439
Meta loss on this task batch = 3.0314e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 133.2823, GNorm = 0.2655
Meta loss on this task batch = 2.6921e-01, Meta loss averaged over last 500 steps = 3.0343e-01, PNorm = 133.2877, GNorm = 0.2183
Meta loss on this task batch = 3.2152e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 133.2936, GNorm = 0.3002
Meta loss on this task batch = 2.6386e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 133.3004, GNorm = 0.2378
Meta loss on this task batch = 2.8961e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 133.3070, GNorm = 0.2569
Meta loss on this task batch = 3.1109e-01, Meta loss averaged over last 500 steps = 3.0335e-01, PNorm = 133.3135, GNorm = 0.2676
Meta loss on this task batch = 3.0879e-01, Meta loss averaged over last 500 steps = 3.0316e-01, PNorm = 133.3192, GNorm = 0.2959
Meta loss on this task batch = 2.8737e-01, Meta loss averaged over last 500 steps = 3.0316e-01, PNorm = 133.3253, GNorm = 0.3496
Meta loss on this task batch = 2.8334e-01, Meta loss averaged over last 500 steps = 3.0315e-01, PNorm = 133.3311, GNorm = 0.2986
Meta loss on this task batch = 2.7646e-01, Meta loss averaged over last 500 steps = 3.0314e-01, PNorm = 133.3372, GNorm = 0.3112
Meta loss on this task batch = 2.8020e-01, Meta loss averaged over last 500 steps = 3.0309e-01, PNorm = 133.3432, GNorm = 0.2650
Meta loss on this task batch = 3.4267e-01, Meta loss averaged over last 500 steps = 3.0319e-01, PNorm = 133.3478, GNorm = 0.3153
Meta loss on this task batch = 2.5333e-01, Meta loss averaged over last 500 steps = 3.0316e-01, PNorm = 133.3521, GNorm = 0.2804
Meta loss on this task batch = 3.2382e-01, Meta loss averaged over last 500 steps = 3.0335e-01, PNorm = 133.3559, GNorm = 0.2722
Meta loss on this task batch = 3.0930e-01, Meta loss averaged over last 500 steps = 3.0336e-01, PNorm = 133.3604, GNorm = 0.3505
Took 112.26756048202515 seconds to complete one epoch of meta training
Took 120.38537096977234 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460235
Epoch 492
Meta loss on this task batch = 3.6536e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 133.3642, GNorm = 0.2755
Meta loss on this task batch = 2.9489e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 133.3683, GNorm = 0.3912
Meta loss on this task batch = 3.0547e-01, Meta loss averaged over last 500 steps = 3.0344e-01, PNorm = 133.3729, GNorm = 0.2749
Meta loss on this task batch = 3.7694e-01, Meta loss averaged over last 500 steps = 3.0361e-01, PNorm = 133.3771, GNorm = 0.2886
Meta loss on this task batch = 2.6561e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 133.3825, GNorm = 0.2820
Meta loss on this task batch = 2.5648e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 133.3897, GNorm = 0.2390
Meta loss on this task batch = 2.5812e-01, Meta loss averaged over last 500 steps = 3.0325e-01, PNorm = 133.3974, GNorm = 0.2407
Meta loss on this task batch = 3.1994e-01, Meta loss averaged over last 500 steps = 3.0324e-01, PNorm = 133.4043, GNorm = 0.3945
Meta loss on this task batch = 3.0686e-01, Meta loss averaged over last 500 steps = 3.0325e-01, PNorm = 133.4108, GNorm = 0.2945
Meta loss on this task batch = 3.4769e-01, Meta loss averaged over last 500 steps = 3.0329e-01, PNorm = 133.4163, GNorm = 0.3155
Meta loss on this task batch = 2.9416e-01, Meta loss averaged over last 500 steps = 3.0330e-01, PNorm = 133.4217, GNorm = 0.3057
Meta loss on this task batch = 2.2311e-01, Meta loss averaged over last 500 steps = 3.0322e-01, PNorm = 133.4272, GNorm = 0.2603
Meta loss on this task batch = 2.7559e-01, Meta loss averaged over last 500 steps = 3.0325e-01, PNorm = 133.4329, GNorm = 0.3247
Meta loss on this task batch = 2.7796e-01, Meta loss averaged over last 500 steps = 3.0315e-01, PNorm = 133.4385, GNorm = 0.4064
Meta loss on this task batch = 3.3383e-01, Meta loss averaged over last 500 steps = 3.0327e-01, PNorm = 133.4440, GNorm = 0.2729
Meta loss on this task batch = 2.6832e-01, Meta loss averaged over last 500 steps = 3.0327e-01, PNorm = 133.4496, GNorm = 0.4211
Meta loss on this task batch = 2.8004e-01, Meta loss averaged over last 500 steps = 3.0325e-01, PNorm = 133.4552, GNorm = 0.2610
Meta loss on this task batch = 3.9205e-01, Meta loss averaged over last 500 steps = 3.0355e-01, PNorm = 133.4598, GNorm = 0.3801
Meta loss on this task batch = 2.6154e-01, Meta loss averaged over last 500 steps = 3.0353e-01, PNorm = 133.4639, GNorm = 0.3479
Took 113.95087027549744 seconds to complete one epoch of meta training
Took 121.73610138893127 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458452
Epoch 493
Meta loss on this task batch = 2.7458e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 133.4677, GNorm = 0.2884
Meta loss on this task batch = 2.8316e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 133.4718, GNorm = 0.2663
Meta loss on this task batch = 2.8031e-01, Meta loss averaged over last 500 steps = 3.0333e-01, PNorm = 133.4762, GNorm = 0.2567
Meta loss on this task batch = 3.4309e-01, Meta loss averaged over last 500 steps = 3.0346e-01, PNorm = 133.4802, GNorm = 0.3419
Meta loss on this task batch = 3.2781e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 133.4833, GNorm = 0.3715
Meta loss on this task batch = 3.5078e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 133.4858, GNorm = 0.3251
Meta loss on this task batch = 2.6926e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 133.4881, GNorm = 0.3025
Meta loss on this task batch = 2.9090e-01, Meta loss averaged over last 500 steps = 3.0326e-01, PNorm = 133.4906, GNorm = 0.4239
Meta loss on this task batch = 3.1351e-01, Meta loss averaged over last 500 steps = 3.0324e-01, PNorm = 133.4937, GNorm = 0.2930
Meta loss on this task batch = 3.0310e-01, Meta loss averaged over last 500 steps = 3.0323e-01, PNorm = 133.4974, GNorm = 0.2916
Meta loss on this task batch = 2.5298e-01, Meta loss averaged over last 500 steps = 3.0314e-01, PNorm = 133.5016, GNorm = 0.3394
Meta loss on this task batch = 3.0217e-01, Meta loss averaged over last 500 steps = 3.0327e-01, PNorm = 133.5063, GNorm = 0.2623
Meta loss on this task batch = 2.6858e-01, Meta loss averaged over last 500 steps = 3.0321e-01, PNorm = 133.5119, GNorm = 0.2511
Meta loss on this task batch = 3.1107e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 133.5182, GNorm = 0.2659
Meta loss on this task batch = 3.0864e-01, Meta loss averaged over last 500 steps = 3.0338e-01, PNorm = 133.5247, GNorm = 0.3969
Meta loss on this task batch = 3.0519e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 133.5317, GNorm = 0.3148
Meta loss on this task batch = 2.6914e-01, Meta loss averaged over last 500 steps = 3.0326e-01, PNorm = 133.5393, GNorm = 0.2375
Meta loss on this task batch = 2.9593e-01, Meta loss averaged over last 500 steps = 3.0320e-01, PNorm = 133.5468, GNorm = 0.2938
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 3.0328e-01, PNorm = 133.5539, GNorm = 0.3767
Took 112.83379983901978 seconds to complete one epoch of meta training
Took 119.95138096809387 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448242
Epoch 494
Meta loss on this task batch = 3.2036e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 133.5601, GNorm = 0.2926
Meta loss on this task batch = 2.7767e-01, Meta loss averaged over last 500 steps = 3.0337e-01, PNorm = 133.5663, GNorm = 0.2298
Meta loss on this task batch = 3.2233e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 133.5720, GNorm = 0.3014
Meta loss on this task batch = 2.9579e-01, Meta loss averaged over last 500 steps = 3.0344e-01, PNorm = 133.5781, GNorm = 0.2783
Meta loss on this task batch = 2.3574e-01, Meta loss averaged over last 500 steps = 3.0330e-01, PNorm = 133.5844, GNorm = 0.2272
Meta loss on this task batch = 3.4651e-01, Meta loss averaged over last 500 steps = 3.0344e-01, PNorm = 133.5900, GNorm = 0.3853
Meta loss on this task batch = 2.8385e-01, Meta loss averaged over last 500 steps = 3.0338e-01, PNorm = 133.5955, GNorm = 0.2675
Meta loss on this task batch = 3.0941e-01, Meta loss averaged over last 500 steps = 3.0339e-01, PNorm = 133.6004, GNorm = 0.3469
Meta loss on this task batch = 3.5458e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 133.6043, GNorm = 0.3393
Meta loss on this task batch = 3.1172e-01, Meta loss averaged over last 500 steps = 3.0355e-01, PNorm = 133.6069, GNorm = 0.3107
Meta loss on this task batch = 3.0664e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 133.6103, GNorm = 0.2918
Meta loss on this task batch = 3.1814e-01, Meta loss averaged over last 500 steps = 3.0364e-01, PNorm = 133.6139, GNorm = 0.3401
Meta loss on this task batch = 2.5413e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 133.6180, GNorm = 0.2954
Meta loss on this task batch = 3.1564e-01, Meta loss averaged over last 500 steps = 3.0362e-01, PNorm = 133.6224, GNorm = 0.3451
Meta loss on this task batch = 3.4414e-01, Meta loss averaged over last 500 steps = 3.0370e-01, PNorm = 133.6268, GNorm = 0.3948
Meta loss on this task batch = 3.0319e-01, Meta loss averaged over last 500 steps = 3.0366e-01, PNorm = 133.6308, GNorm = 0.2855
Meta loss on this task batch = 3.5139e-01, Meta loss averaged over last 500 steps = 3.0371e-01, PNorm = 133.6358, GNorm = 0.2797
Meta loss on this task batch = 2.9009e-01, Meta loss averaged over last 500 steps = 3.0370e-01, PNorm = 133.6419, GNorm = 0.3656
Meta loss on this task batch = 3.1479e-01, Meta loss averaged over last 500 steps = 3.0379e-01, PNorm = 133.6478, GNorm = 0.3320
Took 112.47333836555481 seconds to complete one epoch of meta training
Took 120.48066091537476 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.437401
Epoch 495
Meta loss on this task batch = 3.1672e-01, Meta loss averaged over last 500 steps = 3.0394e-01, PNorm = 133.6536, GNorm = 0.2931
Meta loss on this task batch = 2.5950e-01, Meta loss averaged over last 500 steps = 3.0383e-01, PNorm = 133.6608, GNorm = 0.2348
Meta loss on this task batch = 3.2738e-01, Meta loss averaged over last 500 steps = 3.0389e-01, PNorm = 133.6683, GNorm = 0.3877
Meta loss on this task batch = 2.8705e-01, Meta loss averaged over last 500 steps = 3.0383e-01, PNorm = 133.6760, GNorm = 0.2450
Meta loss on this task batch = 3.0038e-01, Meta loss averaged over last 500 steps = 3.0381e-01, PNorm = 133.6835, GNorm = 0.2475
Meta loss on this task batch = 2.5698e-01, Meta loss averaged over last 500 steps = 3.0371e-01, PNorm = 133.6918, GNorm = 0.2385
Meta loss on this task batch = 2.7129e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 133.7000, GNorm = 0.2558
Meta loss on this task batch = 3.0187e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 133.7078, GNorm = 0.2690
Meta loss on this task batch = 3.1627e-01, Meta loss averaged over last 500 steps = 3.0379e-01, PNorm = 133.7146, GNorm = 0.3468
Meta loss on this task batch = 3.3316e-01, Meta loss averaged over last 500 steps = 3.0391e-01, PNorm = 133.7209, GNorm = 0.2551
Meta loss on this task batch = 3.3692e-01, Meta loss averaged over last 500 steps = 3.0398e-01, PNorm = 133.7267, GNorm = 0.2827
Meta loss on this task batch = 2.5456e-01, Meta loss averaged over last 500 steps = 3.0388e-01, PNorm = 133.7342, GNorm = 0.2492
Meta loss on this task batch = 3.0560e-01, Meta loss averaged over last 500 steps = 3.0381e-01, PNorm = 133.7408, GNorm = 0.5285
Meta loss on this task batch = 2.6622e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 133.7474, GNorm = 0.3099
Meta loss on this task batch = 2.8841e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 133.7522, GNorm = 0.3945
Meta loss on this task batch = 3.0936e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 133.7572, GNorm = 0.3566
Meta loss on this task batch = 3.9364e-01, Meta loss averaged over last 500 steps = 3.0381e-01, PNorm = 133.7613, GNorm = 0.3231
Meta loss on this task batch = 2.8788e-01, Meta loss averaged over last 500 steps = 3.0382e-01, PNorm = 133.7652, GNorm = 0.2697
Meta loss on this task batch = 2.9714e-01, Meta loss averaged over last 500 steps = 3.0380e-01, PNorm = 133.7694, GNorm = 0.4995
Took 113.34164452552795 seconds to complete one epoch of meta training
Took 121.29220509529114 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471347
Epoch 496
Meta loss on this task batch = 2.7313e-01, Meta loss averaged over last 500 steps = 3.0378e-01, PNorm = 133.7731, GNorm = 0.2674
Meta loss on this task batch = 2.5627e-01, Meta loss averaged over last 500 steps = 3.0358e-01, PNorm = 133.7779, GNorm = 0.4072
Meta loss on this task batch = 3.5306e-01, Meta loss averaged over last 500 steps = 3.0369e-01, PNorm = 133.7820, GNorm = 0.4311
Meta loss on this task batch = 2.8787e-01, Meta loss averaged over last 500 steps = 3.0365e-01, PNorm = 133.7864, GNorm = 0.2833
Meta loss on this task batch = 3.1369e-01, Meta loss averaged over last 500 steps = 3.0369e-01, PNorm = 133.7915, GNorm = 0.2789
Meta loss on this task batch = 3.1145e-01, Meta loss averaged over last 500 steps = 3.0353e-01, PNorm = 133.7970, GNorm = 0.2896
Meta loss on this task batch = 3.6230e-01, Meta loss averaged over last 500 steps = 3.0373e-01, PNorm = 133.8034, GNorm = 0.2914
Meta loss on this task batch = 2.7526e-01, Meta loss averaged over last 500 steps = 3.0366e-01, PNorm = 133.8097, GNorm = 0.2210
Meta loss on this task batch = 3.3331e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 133.8150, GNorm = 0.2786
Meta loss on this task batch = 2.9384e-01, Meta loss averaged over last 500 steps = 3.0370e-01, PNorm = 133.8199, GNorm = 0.3461
Meta loss on this task batch = 3.0874e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 133.8249, GNorm = 0.2718
Meta loss on this task batch = 2.8944e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 133.8292, GNorm = 0.3444
Meta loss on this task batch = 2.8242e-01, Meta loss averaged over last 500 steps = 3.0377e-01, PNorm = 133.8337, GNorm = 0.2373
Meta loss on this task batch = 3.2092e-01, Meta loss averaged over last 500 steps = 3.0388e-01, PNorm = 133.8383, GNorm = 0.3703
Meta loss on this task batch = 3.4051e-01, Meta loss averaged over last 500 steps = 3.0387e-01, PNorm = 133.8421, GNorm = 0.3656
Meta loss on this task batch = 3.2758e-01, Meta loss averaged over last 500 steps = 3.0393e-01, PNorm = 133.8471, GNorm = 0.3738
Meta loss on this task batch = 2.7326e-01, Meta loss averaged over last 500 steps = 3.0390e-01, PNorm = 133.8530, GNorm = 0.2308
Meta loss on this task batch = 2.7512e-01, Meta loss averaged over last 500 steps = 3.0382e-01, PNorm = 133.8592, GNorm = 0.3284
Meta loss on this task batch = 3.2235e-01, Meta loss averaged over last 500 steps = 3.0370e-01, PNorm = 133.8650, GNorm = 0.3246
Took 110.833331823349 seconds to complete one epoch of meta training
Took 118.43885016441345 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453306
Epoch 497
Meta loss on this task batch = 3.4791e-01, Meta loss averaged over last 500 steps = 3.0366e-01, PNorm = 133.8700, GNorm = 0.2531
Meta loss on this task batch = 3.2222e-01, Meta loss averaged over last 500 steps = 3.0367e-01, PNorm = 133.8749, GNorm = 0.4632
Meta loss on this task batch = 3.2413e-01, Meta loss averaged over last 500 steps = 3.0373e-01, PNorm = 133.8799, GNorm = 0.2935
Meta loss on this task batch = 2.6917e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 133.8856, GNorm = 0.2435
Meta loss on this task batch = 2.8590e-01, Meta loss averaged over last 500 steps = 3.0362e-01, PNorm = 133.8911, GNorm = 0.2646
Meta loss on this task batch = 3.0099e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 133.8977, GNorm = 0.2555
Meta loss on this task batch = 3.3143e-01, Meta loss averaged over last 500 steps = 3.0369e-01, PNorm = 133.9044, GNorm = 0.4241
Meta loss on this task batch = 2.7938e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 133.9109, GNorm = 0.3028
Meta loss on this task batch = 2.8089e-01, Meta loss averaged over last 500 steps = 3.0364e-01, PNorm = 133.9165, GNorm = 0.3949
Meta loss on this task batch = 3.4801e-01, Meta loss averaged over last 500 steps = 3.0369e-01, PNorm = 133.9222, GNorm = 0.3576
Meta loss on this task batch = 3.3025e-01, Meta loss averaged over last 500 steps = 3.0376e-01, PNorm = 133.9279, GNorm = 0.2659
Meta loss on this task batch = 2.7731e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 133.9337, GNorm = 0.2684
Meta loss on this task batch = 3.1945e-01, Meta loss averaged over last 500 steps = 3.0373e-01, PNorm = 133.9385, GNorm = 0.2801
Meta loss on this task batch = 3.3391e-01, Meta loss averaged over last 500 steps = 3.0382e-01, PNorm = 133.9433, GNorm = 0.2817
Meta loss on this task batch = 3.0050e-01, Meta loss averaged over last 500 steps = 3.0385e-01, PNorm = 133.9476, GNorm = 0.2769
Meta loss on this task batch = 2.5636e-01, Meta loss averaged over last 500 steps = 3.0376e-01, PNorm = 133.9526, GNorm = 0.3758
Meta loss on this task batch = 2.8962e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 133.9577, GNorm = 0.2956
Meta loss on this task batch = 2.6306e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 133.9636, GNorm = 0.2775
Meta loss on this task batch = 3.1125e-01, Meta loss averaged over last 500 steps = 3.0351e-01, PNorm = 133.9694, GNorm = 0.6070
Took 114.02895474433899 seconds to complete one epoch of meta training
Took 121.99657559394836 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455069
Epoch 498
Meta loss on this task batch = 2.7308e-01, Meta loss averaged over last 500 steps = 3.0355e-01, PNorm = 133.9754, GNorm = 0.3106
Meta loss on this task batch = 3.4415e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 133.9805, GNorm = 0.3936
Meta loss on this task batch = 3.1486e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 133.9854, GNorm = 0.2947
Meta loss on this task batch = 3.2542e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 133.9899, GNorm = 0.2882
Meta loss on this task batch = 3.5299e-01, Meta loss averaged over last 500 steps = 3.0373e-01, PNorm = 133.9942, GNorm = 0.2952
Meta loss on this task batch = 2.9667e-01, Meta loss averaged over last 500 steps = 3.0384e-01, PNorm = 133.9986, GNorm = 0.2592
Meta loss on this task batch = 2.9705e-01, Meta loss averaged over last 500 steps = 3.0383e-01, PNorm = 134.0038, GNorm = 0.4514
Meta loss on this task batch = 2.4695e-01, Meta loss averaged over last 500 steps = 3.0375e-01, PNorm = 134.0097, GNorm = 0.2881
Meta loss on this task batch = 2.8637e-01, Meta loss averaged over last 500 steps = 3.0388e-01, PNorm = 134.0151, GNorm = 0.3767
Meta loss on this task batch = 3.2174e-01, Meta loss averaged over last 500 steps = 3.0394e-01, PNorm = 134.0206, GNorm = 0.3947
Meta loss on this task batch = 2.7428e-01, Meta loss averaged over last 500 steps = 3.0385e-01, PNorm = 134.0266, GNorm = 0.3047
Meta loss on this task batch = 3.4521e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 134.0329, GNorm = 0.3137
Meta loss on this task batch = 3.1239e-01, Meta loss averaged over last 500 steps = 3.0405e-01, PNorm = 134.0398, GNorm = 0.2810
Meta loss on this task batch = 2.7882e-01, Meta loss averaged over last 500 steps = 3.0387e-01, PNorm = 134.0470, GNorm = 0.4018
Meta loss on this task batch = 2.7423e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 134.0542, GNorm = 0.2395
Meta loss on this task batch = 2.9782e-01, Meta loss averaged over last 500 steps = 3.0373e-01, PNorm = 134.0605, GNorm = 0.3131
Meta loss on this task batch = 2.8910e-01, Meta loss averaged over last 500 steps = 3.0381e-01, PNorm = 134.0677, GNorm = 0.2820
Meta loss on this task batch = 3.0053e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 134.0747, GNorm = 0.3014
Meta loss on this task batch = 2.5253e-01, Meta loss averaged over last 500 steps = 3.0361e-01, PNorm = 134.0817, GNorm = 0.2583
Took 114.83489155769348 seconds to complete one epoch of meta training
Took 122.53471064567566 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447057
Epoch 499
Meta loss on this task batch = 3.4515e-01, Meta loss averaged over last 500 steps = 3.0372e-01, PNorm = 134.0885, GNorm = 0.3177
Meta loss on this task batch = 2.9560e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 134.0946, GNorm = 0.3374
Meta loss on this task batch = 2.6129e-01, Meta loss averaged over last 500 steps = 3.0355e-01, PNorm = 134.1010, GNorm = 0.3775
Meta loss on this task batch = 2.7904e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 134.1076, GNorm = 0.2482
Meta loss on this task batch = 3.2528e-01, Meta loss averaged over last 500 steps = 3.0347e-01, PNorm = 134.1135, GNorm = 0.2899
Meta loss on this task batch = 2.7444e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 134.1199, GNorm = 0.2396
Meta loss on this task batch = 2.6960e-01, Meta loss averaged over last 500 steps = 3.0332e-01, PNorm = 134.1265, GNorm = 0.2464
Meta loss on this task batch = 3.0032e-01, Meta loss averaged over last 500 steps = 3.0336e-01, PNorm = 134.1334, GNorm = 0.2660
Meta loss on this task batch = 3.1727e-01, Meta loss averaged over last 500 steps = 3.0335e-01, PNorm = 134.1399, GNorm = 0.2928
Meta loss on this task batch = 3.1732e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 134.1453, GNorm = 0.2635
Meta loss on this task batch = 3.1436e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 134.1497, GNorm = 0.3644
Meta loss on this task batch = 3.1350e-01, Meta loss averaged over last 500 steps = 3.0346e-01, PNorm = 134.1534, GNorm = 0.3101
Meta loss on this task batch = 3.2558e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 134.1576, GNorm = 0.3487
Meta loss on this task batch = 2.8397e-01, Meta loss averaged over last 500 steps = 3.0321e-01, PNorm = 134.1623, GNorm = 0.3497
Meta loss on this task batch = 2.7633e-01, Meta loss averaged over last 500 steps = 3.0313e-01, PNorm = 134.1675, GNorm = 0.3684
Meta loss on this task batch = 2.7327e-01, Meta loss averaged over last 500 steps = 3.0306e-01, PNorm = 134.1733, GNorm = 0.3566
Meta loss on this task batch = 2.8284e-01, Meta loss averaged over last 500 steps = 3.0300e-01, PNorm = 134.1796, GNorm = 0.2721
Meta loss on this task batch = 2.9837e-01, Meta loss averaged over last 500 steps = 3.0312e-01, PNorm = 134.1863, GNorm = 0.2499
Meta loss on this task batch = 3.3466e-01, Meta loss averaged over last 500 steps = 3.0321e-01, PNorm = 134.1935, GNorm = 0.5072
Took 110.74669361114502 seconds to complete one epoch of meta training
Took 118.47077775001526 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458261
Epoch 500
Meta loss on this task batch = 2.7255e-01, Meta loss averaged over last 500 steps = 3.0315e-01, PNorm = 134.2009, GNorm = 0.2876
Meta loss on this task batch = 2.3231e-01, Meta loss averaged over last 500 steps = 3.0300e-01, PNorm = 134.2091, GNorm = 0.2569
Meta loss on this task batch = 2.9841e-01, Meta loss averaged over last 500 steps = 3.0293e-01, PNorm = 134.2171, GNorm = 0.2916
Meta loss on this task batch = 3.0076e-01, Meta loss averaged over last 500 steps = 3.0299e-01, PNorm = 134.2248, GNorm = 0.2501
Meta loss on this task batch = 3.5511e-01, Meta loss averaged over last 500 steps = 3.0313e-01, PNorm = 134.2311, GNorm = 0.3504
Meta loss on this task batch = 3.2135e-01, Meta loss averaged over last 500 steps = 3.0312e-01, PNorm = 134.2362, GNorm = 0.3014
Meta loss on this task batch = 2.8581e-01, Meta loss averaged over last 500 steps = 3.0315e-01, PNorm = 134.2409, GNorm = 0.4035
Meta loss on this task batch = 2.5859e-01, Meta loss averaged over last 500 steps = 3.0313e-01, PNorm = 134.2456, GNorm = 0.3068
Meta loss on this task batch = 2.6559e-01, Meta loss averaged over last 500 steps = 3.0306e-01, PNorm = 134.2496, GNorm = 0.2684
Meta loss on this task batch = 3.3380e-01, Meta loss averaged over last 500 steps = 3.0296e-01, PNorm = 134.2536, GNorm = 0.3337
Meta loss on this task batch = 3.1187e-01, Meta loss averaged over last 500 steps = 3.0301e-01, PNorm = 134.2575, GNorm = 0.2495
Meta loss on this task batch = 2.8290e-01, Meta loss averaged over last 500 steps = 3.0295e-01, PNorm = 134.2617, GNorm = 0.2463
Meta loss on this task batch = 2.8449e-01, Meta loss averaged over last 500 steps = 3.0295e-01, PNorm = 134.2660, GNorm = 0.3354
Meta loss on this task batch = 3.0698e-01, Meta loss averaged over last 500 steps = 3.0297e-01, PNorm = 134.2697, GNorm = 0.2861
Meta loss on this task batch = 2.5605e-01, Meta loss averaged over last 500 steps = 3.0285e-01, PNorm = 134.2724, GNorm = 0.3127
Meta loss on this task batch = 3.4267e-01, Meta loss averaged over last 500 steps = 3.0282e-01, PNorm = 134.2757, GNorm = 0.3322
Meta loss on this task batch = 3.1210e-01, Meta loss averaged over last 500 steps = 3.0274e-01, PNorm = 134.2789, GNorm = 0.3047
Meta loss on this task batch = 3.3995e-01, Meta loss averaged over last 500 steps = 3.0287e-01, PNorm = 134.2820, GNorm = 0.3505
Meta loss on this task batch = 2.8742e-01, Meta loss averaged over last 500 steps = 3.0281e-01, PNorm = 134.2852, GNorm = 0.3932
Took 112.70727753639221 seconds to complete one epoch of meta training
Took 120.65299701690674 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451081
Epoch 501
Meta loss on this task batch = 3.2478e-01, Meta loss averaged over last 500 steps = 3.0301e-01, PNorm = 134.2890, GNorm = 0.4817
Meta loss on this task batch = 3.3116e-01, Meta loss averaged over last 500 steps = 3.0302e-01, PNorm = 134.2934, GNorm = 0.2718
Meta loss on this task batch = 2.6707e-01, Meta loss averaged over last 500 steps = 3.0295e-01, PNorm = 134.2989, GNorm = 0.2705
Meta loss on this task batch = 2.9479e-01, Meta loss averaged over last 500 steps = 3.0288e-01, PNorm = 134.3035, GNorm = 0.4030
Meta loss on this task batch = 2.7913e-01, Meta loss averaged over last 500 steps = 3.0279e-01, PNorm = 134.3079, GNorm = 0.2480
Meta loss on this task batch = 2.4911e-01, Meta loss averaged over last 500 steps = 3.0280e-01, PNorm = 134.3117, GNorm = 0.2446
Meta loss on this task batch = 2.7538e-01, Meta loss averaged over last 500 steps = 3.0270e-01, PNorm = 134.3167, GNorm = 0.2410
Meta loss on this task batch = 3.5031e-01, Meta loss averaged over last 500 steps = 3.0273e-01, PNorm = 134.3209, GNorm = 0.3475
Meta loss on this task batch = 2.9038e-01, Meta loss averaged over last 500 steps = 3.0268e-01, PNorm = 134.3252, GNorm = 0.2328
Meta loss on this task batch = 3.0489e-01, Meta loss averaged over last 500 steps = 3.0264e-01, PNorm = 134.3295, GNorm = 0.2850
Meta loss on this task batch = 3.0398e-01, Meta loss averaged over last 500 steps = 3.0259e-01, PNorm = 134.3341, GNorm = 0.3326
Meta loss on this task batch = 3.1244e-01, Meta loss averaged over last 500 steps = 3.0258e-01, PNorm = 134.3389, GNorm = 0.2271
Meta loss on this task batch = 3.3957e-01, Meta loss averaged over last 500 steps = 3.0258e-01, PNorm = 134.3434, GNorm = 0.2951
Meta loss on this task batch = 2.7344e-01, Meta loss averaged over last 500 steps = 3.0251e-01, PNorm = 134.3484, GNorm = 0.2959
Meta loss on this task batch = 2.8756e-01, Meta loss averaged over last 500 steps = 3.0251e-01, PNorm = 134.3540, GNorm = 0.2641
Meta loss on this task batch = 2.9954e-01, Meta loss averaged over last 500 steps = 3.0253e-01, PNorm = 134.3591, GNorm = 0.2625
Meta loss on this task batch = 2.9073e-01, Meta loss averaged over last 500 steps = 3.0243e-01, PNorm = 134.3649, GNorm = 0.3134
Meta loss on this task batch = 3.0238e-01, Meta loss averaged over last 500 steps = 3.0246e-01, PNorm = 134.3712, GNorm = 0.2725
Meta loss on this task batch = 2.2590e-01, Meta loss averaged over last 500 steps = 3.0230e-01, PNorm = 134.3773, GNorm = 0.2775
Took 111.04648637771606 seconds to complete one epoch of meta training
Took 118.0523452758789 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.433655
Epoch 502
Meta loss on this task batch = 3.5271e-01, Meta loss averaged over last 500 steps = 3.0239e-01, PNorm = 134.3835, GNorm = 0.3768
Meta loss on this task batch = 3.6334e-01, Meta loss averaged over last 500 steps = 3.0251e-01, PNorm = 134.3885, GNorm = 0.4064
Meta loss on this task batch = 2.9128e-01, Meta loss averaged over last 500 steps = 3.0250e-01, PNorm = 134.3935, GNorm = 0.2812
Meta loss on this task batch = 2.8290e-01, Meta loss averaged over last 500 steps = 3.0252e-01, PNorm = 134.3988, GNorm = 0.2776
Meta loss on this task batch = 3.1059e-01, Meta loss averaged over last 500 steps = 3.0264e-01, PNorm = 134.4037, GNorm = 0.2767
Meta loss on this task batch = 3.0056e-01, Meta loss averaged over last 500 steps = 3.0264e-01, PNorm = 134.4091, GNorm = 0.3572
Meta loss on this task batch = 3.1117e-01, Meta loss averaged over last 500 steps = 3.0266e-01, PNorm = 134.4142, GNorm = 0.4093
Meta loss on this task batch = 3.0547e-01, Meta loss averaged over last 500 steps = 3.0258e-01, PNorm = 134.4187, GNorm = 0.3794
Meta loss on this task batch = 2.6391e-01, Meta loss averaged over last 500 steps = 3.0247e-01, PNorm = 134.4232, GNorm = 0.2791
Meta loss on this task batch = 2.7465e-01, Meta loss averaged over last 500 steps = 3.0241e-01, PNorm = 134.4279, GNorm = 0.2452
Meta loss on this task batch = 3.0352e-01, Meta loss averaged over last 500 steps = 3.0246e-01, PNorm = 134.4325, GNorm = 0.3325
Meta loss on this task batch = 3.1958e-01, Meta loss averaged over last 500 steps = 3.0251e-01, PNorm = 134.4364, GNorm = 0.3233
Meta loss on this task batch = 2.6785e-01, Meta loss averaged over last 500 steps = 3.0255e-01, PNorm = 134.4413, GNorm = 0.3369
Meta loss on this task batch = 3.0572e-01, Meta loss averaged over last 500 steps = 3.0265e-01, PNorm = 134.4458, GNorm = 0.3095
Meta loss on this task batch = 2.7085e-01, Meta loss averaged over last 500 steps = 3.0259e-01, PNorm = 134.4505, GNorm = 0.2467
Meta loss on this task batch = 3.4388e-01, Meta loss averaged over last 500 steps = 3.0257e-01, PNorm = 134.4552, GNorm = 0.3056
Meta loss on this task batch = 2.6142e-01, Meta loss averaged over last 500 steps = 3.0249e-01, PNorm = 134.4603, GNorm = 0.3354
Meta loss on this task batch = 2.6374e-01, Meta loss averaged over last 500 steps = 3.0237e-01, PNorm = 134.4657, GNorm = 0.2308
Meta loss on this task batch = 3.1688e-01, Meta loss averaged over last 500 steps = 3.0235e-01, PNorm = 134.4718, GNorm = 0.4378
Took 112.15600323677063 seconds to complete one epoch of meta training
Took 120.01777768135071 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.441674
Epoch 503
Meta loss on this task batch = 3.2582e-01, Meta loss averaged over last 500 steps = 3.0235e-01, PNorm = 134.4767, GNorm = 0.3267
Meta loss on this task batch = 2.5119e-01, Meta loss averaged over last 500 steps = 3.0216e-01, PNorm = 134.4819, GNorm = 0.2618
Meta loss on this task batch = 2.7157e-01, Meta loss averaged over last 500 steps = 3.0196e-01, PNorm = 134.4877, GNorm = 0.3099
Meta loss on this task batch = 3.4559e-01, Meta loss averaged over last 500 steps = 3.0216e-01, PNorm = 134.4926, GNorm = 0.3414
Meta loss on this task batch = 3.0758e-01, Meta loss averaged over last 500 steps = 3.0225e-01, PNorm = 134.4975, GNorm = 0.3166
Meta loss on this task batch = 3.0390e-01, Meta loss averaged over last 500 steps = 3.0225e-01, PNorm = 134.5022, GNorm = 0.4209
Meta loss on this task batch = 3.0853e-01, Meta loss averaged over last 500 steps = 3.0227e-01, PNorm = 134.5069, GNorm = 0.3520
Meta loss on this task batch = 2.6233e-01, Meta loss averaged over last 500 steps = 3.0208e-01, PNorm = 134.5119, GNorm = 0.2291
Meta loss on this task batch = 3.0462e-01, Meta loss averaged over last 500 steps = 3.0206e-01, PNorm = 134.5166, GNorm = 0.5099
Meta loss on this task batch = 3.2052e-01, Meta loss averaged over last 500 steps = 3.0216e-01, PNorm = 134.5218, GNorm = 0.3242
Meta loss on this task batch = 2.7728e-01, Meta loss averaged over last 500 steps = 3.0214e-01, PNorm = 134.5276, GNorm = 0.2820
Meta loss on this task batch = 3.0896e-01, Meta loss averaged over last 500 steps = 3.0217e-01, PNorm = 134.5330, GNorm = 0.3215
Meta loss on this task batch = 3.2151e-01, Meta loss averaged over last 500 steps = 3.0220e-01, PNorm = 134.5387, GNorm = 0.3119
Meta loss on this task batch = 3.6640e-01, Meta loss averaged over last 500 steps = 3.0231e-01, PNorm = 134.5437, GNorm = 0.3399
Meta loss on this task batch = 2.9976e-01, Meta loss averaged over last 500 steps = 3.0231e-01, PNorm = 134.5493, GNorm = 0.2956
Meta loss on this task batch = 3.5081e-01, Meta loss averaged over last 500 steps = 3.0247e-01, PNorm = 134.5545, GNorm = 0.3019
Meta loss on this task batch = 2.5861e-01, Meta loss averaged over last 500 steps = 3.0242e-01, PNorm = 134.5601, GNorm = 0.2976
Meta loss on this task batch = 3.2916e-01, Meta loss averaged over last 500 steps = 3.0244e-01, PNorm = 134.5658, GNorm = 0.2747
Meta loss on this task batch = 2.8765e-01, Meta loss averaged over last 500 steps = 3.0231e-01, PNorm = 134.5724, GNorm = 0.3173
Took 112.88686966896057 seconds to complete one epoch of meta training
Took 120.51383996009827 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456401
Epoch 504
Meta loss on this task batch = 3.4573e-01, Meta loss averaged over last 500 steps = 3.0248e-01, PNorm = 134.5790, GNorm = 0.2918
Meta loss on this task batch = 2.8072e-01, Meta loss averaged over last 500 steps = 3.0253e-01, PNorm = 134.5855, GNorm = 0.2951
Meta loss on this task batch = 3.3521e-01, Meta loss averaged over last 500 steps = 3.0258e-01, PNorm = 134.5913, GNorm = 0.3140
Meta loss on this task batch = 2.9376e-01, Meta loss averaged over last 500 steps = 3.0257e-01, PNorm = 134.5964, GNorm = 0.2500
Meta loss on this task batch = 2.7565e-01, Meta loss averaged over last 500 steps = 3.0263e-01, PNorm = 134.6015, GNorm = 0.3703
Meta loss on this task batch = 3.2375e-01, Meta loss averaged over last 500 steps = 3.0263e-01, PNorm = 134.6070, GNorm = 0.6398
Meta loss on this task batch = 3.1387e-01, Meta loss averaged over last 500 steps = 3.0270e-01, PNorm = 134.6129, GNorm = 0.2463
Meta loss on this task batch = 3.4141e-01, Meta loss averaged over last 500 steps = 3.0284e-01, PNorm = 134.6184, GNorm = 0.2603
Meta loss on this task batch = 3.1317e-01, Meta loss averaged over last 500 steps = 3.0286e-01, PNorm = 134.6243, GNorm = 0.4171
Meta loss on this task batch = 2.6126e-01, Meta loss averaged over last 500 steps = 3.0276e-01, PNorm = 134.6303, GNorm = 0.3724
Meta loss on this task batch = 2.8919e-01, Meta loss averaged over last 500 steps = 3.0280e-01, PNorm = 134.6363, GNorm = 0.3552
Meta loss on this task batch = 2.7298e-01, Meta loss averaged over last 500 steps = 3.0270e-01, PNorm = 134.6426, GNorm = 0.2824
Meta loss on this task batch = 3.1174e-01, Meta loss averaged over last 500 steps = 3.0275e-01, PNorm = 134.6484, GNorm = 0.2665
Meta loss on this task batch = 3.3550e-01, Meta loss averaged over last 500 steps = 3.0275e-01, PNorm = 134.6530, GNorm = 0.3013
Meta loss on this task batch = 2.5908e-01, Meta loss averaged over last 500 steps = 3.0277e-01, PNorm = 134.6577, GNorm = 0.2585
Meta loss on this task batch = 2.6596e-01, Meta loss averaged over last 500 steps = 3.0273e-01, PNorm = 134.6630, GNorm = 0.3367
Meta loss on this task batch = 2.6462e-01, Meta loss averaged over last 500 steps = 3.0262e-01, PNorm = 134.6682, GNorm = 0.2926
Meta loss on this task batch = 2.4895e-01, Meta loss averaged over last 500 steps = 3.0241e-01, PNorm = 134.6733, GNorm = 0.2788
Meta loss on this task batch = 2.8586e-01, Meta loss averaged over last 500 steps = 3.0221e-01, PNorm = 134.6788, GNorm = 0.4219
Took 109.58148217201233 seconds to complete one epoch of meta training
Took 117.64856934547424 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452954
Epoch 505
Meta loss on this task batch = 3.0101e-01, Meta loss averaged over last 500 steps = 3.0212e-01, PNorm = 134.6839, GNorm = 0.2417
Meta loss on this task batch = 2.7935e-01, Meta loss averaged over last 500 steps = 3.0210e-01, PNorm = 134.6883, GNorm = 0.3722
Meta loss on this task batch = 2.8815e-01, Meta loss averaged over last 500 steps = 3.0203e-01, PNorm = 134.6930, GNorm = 0.2982
Meta loss on this task batch = 3.0239e-01, Meta loss averaged over last 500 steps = 3.0209e-01, PNorm = 134.6975, GNorm = 0.4567
Meta loss on this task batch = 3.3547e-01, Meta loss averaged over last 500 steps = 3.0216e-01, PNorm = 134.7022, GNorm = 0.3096
Meta loss on this task batch = 3.2666e-01, Meta loss averaged over last 500 steps = 3.0229e-01, PNorm = 134.7061, GNorm = 0.2407
Meta loss on this task batch = 2.9592e-01, Meta loss averaged over last 500 steps = 3.0228e-01, PNorm = 134.7102, GNorm = 0.3561
Meta loss on this task batch = 2.4143e-01, Meta loss averaged over last 500 steps = 3.0223e-01, PNorm = 134.7149, GNorm = 0.2606
Meta loss on this task batch = 3.1450e-01, Meta loss averaged over last 500 steps = 3.0231e-01, PNorm = 134.7207, GNorm = 0.2868
Meta loss on this task batch = 3.1802e-01, Meta loss averaged over last 500 steps = 3.0236e-01, PNorm = 134.7247, GNorm = 0.5104
Meta loss on this task batch = 3.1329e-01, Meta loss averaged over last 500 steps = 3.0230e-01, PNorm = 134.7292, GNorm = 0.3192
Meta loss on this task batch = 2.9292e-01, Meta loss averaged over last 500 steps = 3.0231e-01, PNorm = 134.7339, GNorm = 0.2518
Meta loss on this task batch = 2.3504e-01, Meta loss averaged over last 500 steps = 3.0209e-01, PNorm = 134.7392, GNorm = 0.2280
Meta loss on this task batch = 3.0826e-01, Meta loss averaged over last 500 steps = 3.0211e-01, PNorm = 134.7447, GNorm = 0.2783
Meta loss on this task batch = 3.1552e-01, Meta loss averaged over last 500 steps = 3.0216e-01, PNorm = 134.7512, GNorm = 0.3058
Meta loss on this task batch = 2.8943e-01, Meta loss averaged over last 500 steps = 3.0214e-01, PNorm = 134.7582, GNorm = 0.3342
Meta loss on this task batch = 2.9099e-01, Meta loss averaged over last 500 steps = 3.0208e-01, PNorm = 134.7653, GNorm = 0.2796
Meta loss on this task batch = 3.4879e-01, Meta loss averaged over last 500 steps = 3.0208e-01, PNorm = 134.7729, GNorm = 0.3625
Meta loss on this task batch = 3.2873e-01, Meta loss averaged over last 500 steps = 3.0221e-01, PNorm = 134.7807, GNorm = 0.5255
Took 109.15530323982239 seconds to complete one epoch of meta training
Took 117.0327079296112 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461075
Epoch 506
Meta loss on this task batch = 2.7088e-01, Meta loss averaged over last 500 steps = 3.0209e-01, PNorm = 134.7886, GNorm = 0.3127
Meta loss on this task batch = 2.7817e-01, Meta loss averaged over last 500 steps = 3.0198e-01, PNorm = 134.7961, GNorm = 0.2673
Meta loss on this task batch = 3.2783e-01, Meta loss averaged over last 500 steps = 3.0197e-01, PNorm = 134.8031, GNorm = 0.3128
Meta loss on this task batch = 2.6106e-01, Meta loss averaged over last 500 steps = 3.0182e-01, PNorm = 134.8098, GNorm = 0.3054
Meta loss on this task batch = 3.0348e-01, Meta loss averaged over last 500 steps = 3.0184e-01, PNorm = 134.8171, GNorm = 0.3330
Meta loss on this task batch = 3.0222e-01, Meta loss averaged over last 500 steps = 3.0188e-01, PNorm = 134.8243, GNorm = 0.3169
Meta loss on this task batch = 3.0318e-01, Meta loss averaged over last 500 steps = 3.0190e-01, PNorm = 134.8310, GNorm = 0.5756
Meta loss on this task batch = 3.0253e-01, Meta loss averaged over last 500 steps = 3.0198e-01, PNorm = 134.8375, GNorm = 0.3061
Meta loss on this task batch = 2.7344e-01, Meta loss averaged over last 500 steps = 3.0199e-01, PNorm = 134.8442, GNorm = 0.2532
Meta loss on this task batch = 2.2139e-01, Meta loss averaged over last 500 steps = 3.0184e-01, PNorm = 134.8503, GNorm = 0.3659
Meta loss on this task batch = 3.0404e-01, Meta loss averaged over last 500 steps = 3.0185e-01, PNorm = 134.8562, GNorm = 0.3360
Meta loss on this task batch = 3.0715e-01, Meta loss averaged over last 500 steps = 3.0180e-01, PNorm = 134.8612, GNorm = 0.3137
Meta loss on this task batch = 2.9636e-01, Meta loss averaged over last 500 steps = 3.0181e-01, PNorm = 134.8651, GNorm = 0.2755
Meta loss on this task batch = 3.2996e-01, Meta loss averaged over last 500 steps = 3.0183e-01, PNorm = 134.8687, GNorm = 0.3176
Meta loss on this task batch = 3.6333e-01, Meta loss averaged over last 500 steps = 3.0197e-01, PNorm = 134.8725, GNorm = 0.4353
Meta loss on this task batch = 3.2142e-01, Meta loss averaged over last 500 steps = 3.0196e-01, PNorm = 134.8763, GNorm = 0.3138
Meta loss on this task batch = 3.1870e-01, Meta loss averaged over last 500 steps = 3.0183e-01, PNorm = 134.8800, GNorm = 0.2385
Meta loss on this task batch = 2.7609e-01, Meta loss averaged over last 500 steps = 3.0181e-01, PNorm = 134.8844, GNorm = 0.2471
Meta loss on this task batch = 3.3564e-01, Meta loss averaged over last 500 steps = 3.0196e-01, PNorm = 134.8886, GNorm = 0.3522
Took 111.49608302116394 seconds to complete one epoch of meta training
Took 119.44779253005981 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483945
Epoch 507
Meta loss on this task batch = 2.8759e-01, Meta loss averaged over last 500 steps = 3.0187e-01, PNorm = 134.8935, GNorm = 0.2712
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 3.0174e-01, PNorm = 134.8991, GNorm = 0.2844
Meta loss on this task batch = 2.9831e-01, Meta loss averaged over last 500 steps = 3.0172e-01, PNorm = 134.9047, GNorm = 0.3680
Meta loss on this task batch = 2.5382e-01, Meta loss averaged over last 500 steps = 3.0166e-01, PNorm = 134.9106, GNorm = 0.2647
Meta loss on this task batch = 3.4706e-01, Meta loss averaged over last 500 steps = 3.0174e-01, PNorm = 134.9160, GNorm = 0.3444
Meta loss on this task batch = 3.0876e-01, Meta loss averaged over last 500 steps = 3.0162e-01, PNorm = 134.9213, GNorm = 0.2744
Meta loss on this task batch = 3.0326e-01, Meta loss averaged over last 500 steps = 3.0157e-01, PNorm = 134.9257, GNorm = 0.4872
Meta loss on this task batch = 2.5251e-01, Meta loss averaged over last 500 steps = 3.0149e-01, PNorm = 134.9306, GNorm = 0.2738
Meta loss on this task batch = 2.8310e-01, Meta loss averaged over last 500 steps = 3.0147e-01, PNorm = 134.9369, GNorm = 0.3525
Meta loss on this task batch = 3.3640e-01, Meta loss averaged over last 500 steps = 3.0153e-01, PNorm = 134.9432, GNorm = 0.2645
Meta loss on this task batch = 3.0060e-01, Meta loss averaged over last 500 steps = 3.0156e-01, PNorm = 134.9488, GNorm = 0.2785
Meta loss on this task batch = 3.0300e-01, Meta loss averaged over last 500 steps = 3.0150e-01, PNorm = 134.9543, GNorm = 0.2753
Meta loss on this task batch = 3.0778e-01, Meta loss averaged over last 500 steps = 3.0154e-01, PNorm = 134.9590, GNorm = 0.3090
Meta loss on this task batch = 2.7223e-01, Meta loss averaged over last 500 steps = 3.0149e-01, PNorm = 134.9645, GNorm = 0.2437
Meta loss on this task batch = 2.8781e-01, Meta loss averaged over last 500 steps = 3.0146e-01, PNorm = 134.9705, GNorm = 0.2684
Meta loss on this task batch = 3.3374e-01, Meta loss averaged over last 500 steps = 3.0148e-01, PNorm = 134.9768, GNorm = 0.2906
Meta loss on this task batch = 3.8275e-01, Meta loss averaged over last 500 steps = 3.0180e-01, PNorm = 134.9832, GNorm = 0.3809
Meta loss on this task batch = 2.2742e-01, Meta loss averaged over last 500 steps = 3.0161e-01, PNorm = 134.9896, GNorm = 0.2346
Meta loss on this task batch = 2.7238e-01, Meta loss averaged over last 500 steps = 3.0159e-01, PNorm = 134.9963, GNorm = 0.7179
Took 109.67850804328918 seconds to complete one epoch of meta training
Took 117.59701418876648 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446259
Epoch 508
Meta loss on this task batch = 3.2385e-01, Meta loss averaged over last 500 steps = 3.0163e-01, PNorm = 135.0024, GNorm = 0.2883
Meta loss on this task batch = 2.7542e-01, Meta loss averaged over last 500 steps = 3.0161e-01, PNorm = 135.0084, GNorm = 0.3384
Meta loss on this task batch = 3.0957e-01, Meta loss averaged over last 500 steps = 3.0172e-01, PNorm = 135.0135, GNorm = 0.2699
Meta loss on this task batch = 2.4057e-01, Meta loss averaged over last 500 steps = 3.0156e-01, PNorm = 135.0190, GNorm = 0.2644
Meta loss on this task batch = 2.9964e-01, Meta loss averaged over last 500 steps = 3.0147e-01, PNorm = 135.0244, GNorm = 0.3119
Meta loss on this task batch = 2.8366e-01, Meta loss averaged over last 500 steps = 3.0149e-01, PNorm = 135.0299, GNorm = 0.2419
Meta loss on this task batch = 3.4267e-01, Meta loss averaged over last 500 steps = 3.0156e-01, PNorm = 135.0355, GNorm = 0.3287
Meta loss on this task batch = 2.8052e-01, Meta loss averaged over last 500 steps = 3.0148e-01, PNorm = 135.0417, GNorm = 0.2567
Meta loss on this task batch = 2.6107e-01, Meta loss averaged over last 500 steps = 3.0140e-01, PNorm = 135.0473, GNorm = 0.2371
Meta loss on this task batch = 2.8757e-01, Meta loss averaged over last 500 steps = 3.0144e-01, PNorm = 135.0532, GNorm = 0.2963
Meta loss on this task batch = 3.6293e-01, Meta loss averaged over last 500 steps = 3.0148e-01, PNorm = 135.0582, GNorm = 0.2896
Meta loss on this task batch = 2.1606e-01, Meta loss averaged over last 500 steps = 3.0127e-01, PNorm = 135.0645, GNorm = 0.2039
Meta loss on this task batch = 3.1579e-01, Meta loss averaged over last 500 steps = 3.0127e-01, PNorm = 135.0704, GNorm = 0.2888
Meta loss on this task batch = 3.3061e-01, Meta loss averaged over last 500 steps = 3.0134e-01, PNorm = 135.0755, GNorm = 0.4905
Meta loss on this task batch = 3.1299e-01, Meta loss averaged over last 500 steps = 3.0140e-01, PNorm = 135.0796, GNorm = 0.2983
Meta loss on this task batch = 3.1410e-01, Meta loss averaged over last 500 steps = 3.0142e-01, PNorm = 135.0839, GNorm = 0.3149
Meta loss on this task batch = 2.9251e-01, Meta loss averaged over last 500 steps = 3.0138e-01, PNorm = 135.0875, GNorm = 0.4772
Meta loss on this task batch = 3.3425e-01, Meta loss averaged over last 500 steps = 3.0145e-01, PNorm = 135.0913, GNorm = 0.2901
Meta loss on this task batch = 3.6909e-01, Meta loss averaged over last 500 steps = 3.0155e-01, PNorm = 135.0948, GNorm = 0.4444
Took 107.87352347373962 seconds to complete one epoch of meta training
Took 115.34969735145569 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467950
Epoch 509
Meta loss on this task batch = 3.0552e-01, Meta loss averaged over last 500 steps = 3.0156e-01, PNorm = 135.0975, GNorm = 0.3723
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 3.0151e-01, PNorm = 135.0991, GNorm = 0.2677
Meta loss on this task batch = 2.6000e-01, Meta loss averaged over last 500 steps = 3.0132e-01, PNorm = 135.1008, GNorm = 0.2417
Meta loss on this task batch = 3.3469e-01, Meta loss averaged over last 500 steps = 3.0147e-01, PNorm = 135.1029, GNorm = 0.4092
Meta loss on this task batch = 2.4222e-01, Meta loss averaged over last 500 steps = 3.0146e-01, PNorm = 135.1055, GNorm = 0.2911
Meta loss on this task batch = 2.9884e-01, Meta loss averaged over last 500 steps = 3.0154e-01, PNorm = 135.1083, GNorm = 0.2855
Meta loss on this task batch = 3.0369e-01, Meta loss averaged over last 500 steps = 3.0151e-01, PNorm = 135.1105, GNorm = 0.2483
Meta loss on this task batch = 2.4445e-01, Meta loss averaged over last 500 steps = 3.0139e-01, PNorm = 135.1130, GNorm = 0.2240
Meta loss on this task batch = 3.2743e-01, Meta loss averaged over last 500 steps = 3.0142e-01, PNorm = 135.1158, GNorm = 0.2628
Meta loss on this task batch = 3.0737e-01, Meta loss averaged over last 500 steps = 3.0144e-01, PNorm = 135.1190, GNorm = 0.2991
Meta loss on this task batch = 3.0923e-01, Meta loss averaged over last 500 steps = 3.0143e-01, PNorm = 135.1221, GNorm = 0.2931
Meta loss on this task batch = 3.1962e-01, Meta loss averaged over last 500 steps = 3.0136e-01, PNorm = 135.1257, GNorm = 0.2696
Meta loss on this task batch = 3.0547e-01, Meta loss averaged over last 500 steps = 3.0130e-01, PNorm = 135.1295, GNorm = 0.3290
Meta loss on this task batch = 2.8434e-01, Meta loss averaged over last 500 steps = 3.0123e-01, PNorm = 135.1332, GNorm = 0.2753
Meta loss on this task batch = 3.1804e-01, Meta loss averaged over last 500 steps = 3.0119e-01, PNorm = 135.1377, GNorm = 0.3149
Meta loss on this task batch = 2.5485e-01, Meta loss averaged over last 500 steps = 3.0113e-01, PNorm = 135.1427, GNorm = 0.2793
Meta loss on this task batch = 2.8278e-01, Meta loss averaged over last 500 steps = 3.0125e-01, PNorm = 135.1477, GNorm = 0.2576
Meta loss on this task batch = 3.0886e-01, Meta loss averaged over last 500 steps = 3.0134e-01, PNorm = 135.1523, GNorm = 0.2853
Meta loss on this task batch = 2.9754e-01, Meta loss averaged over last 500 steps = 3.0132e-01, PNorm = 135.1572, GNorm = 0.4043
Took 111.83122897148132 seconds to complete one epoch of meta training
Took 118.69737887382507 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487776
Epoch 510
Meta loss on this task batch = 3.0279e-01, Meta loss averaged over last 500 steps = 3.0118e-01, PNorm = 135.1620, GNorm = 0.2797
Meta loss on this task batch = 2.8967e-01, Meta loss averaged over last 500 steps = 3.0119e-01, PNorm = 135.1675, GNorm = 0.2984
Meta loss on this task batch = 2.8216e-01, Meta loss averaged over last 500 steps = 3.0108e-01, PNorm = 135.1729, GNorm = 0.2625
Meta loss on this task batch = 2.4859e-01, Meta loss averaged over last 500 steps = 3.0099e-01, PNorm = 135.1788, GNorm = 0.2343
Meta loss on this task batch = 3.2916e-01, Meta loss averaged over last 500 steps = 3.0094e-01, PNorm = 135.1843, GNorm = 0.3864
Meta loss on this task batch = 3.4382e-01, Meta loss averaged over last 500 steps = 3.0113e-01, PNorm = 135.1894, GNorm = 0.2990
Meta loss on this task batch = 2.8670e-01, Meta loss averaged over last 500 steps = 3.0099e-01, PNorm = 135.1943, GNorm = 0.3820
Meta loss on this task batch = 2.9555e-01, Meta loss averaged over last 500 steps = 3.0101e-01, PNorm = 135.1991, GNorm = 0.2862
Meta loss on this task batch = 2.7250e-01, Meta loss averaged over last 500 steps = 3.0101e-01, PNorm = 135.2035, GNorm = 0.2452
Meta loss on this task batch = 3.1349e-01, Meta loss averaged over last 500 steps = 3.0096e-01, PNorm = 135.2078, GNorm = 0.2680
Meta loss on this task batch = 2.5981e-01, Meta loss averaged over last 500 steps = 3.0082e-01, PNorm = 135.2130, GNorm = 0.2363
Meta loss on this task batch = 3.0710e-01, Meta loss averaged over last 500 steps = 3.0091e-01, PNorm = 135.2187, GNorm = 0.3196
Meta loss on this task batch = 2.6454e-01, Meta loss averaged over last 500 steps = 3.0074e-01, PNorm = 135.2243, GNorm = 0.3932
Meta loss on this task batch = 3.6967e-01, Meta loss averaged over last 500 steps = 3.0082e-01, PNorm = 135.2286, GNorm = 0.2919
Meta loss on this task batch = 2.6586e-01, Meta loss averaged over last 500 steps = 3.0075e-01, PNorm = 135.2328, GNorm = 0.2932
Meta loss on this task batch = 2.6382e-01, Meta loss averaged over last 500 steps = 3.0069e-01, PNorm = 135.2379, GNorm = 0.4344
Meta loss on this task batch = 2.7519e-01, Meta loss averaged over last 500 steps = 3.0056e-01, PNorm = 135.2429, GNorm = 0.2390
Meta loss on this task batch = 2.9986e-01, Meta loss averaged over last 500 steps = 3.0052e-01, PNorm = 135.2478, GNorm = 0.5790
Meta loss on this task batch = 3.1323e-01, Meta loss averaged over last 500 steps = 3.0053e-01, PNorm = 135.2525, GNorm = 0.3437
Took 111.17312335968018 seconds to complete one epoch of meta training
Took 118.56613802909851 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470060
Epoch 511
Meta loss on this task batch = 2.2925e-01, Meta loss averaged over last 500 steps = 3.0034e-01, PNorm = 135.2577, GNorm = 0.2479
Meta loss on this task batch = 2.8581e-01, Meta loss averaged over last 500 steps = 3.0030e-01, PNorm = 135.2629, GNorm = 0.3439
Meta loss on this task batch = 3.5190e-01, Meta loss averaged over last 500 steps = 3.0043e-01, PNorm = 135.2683, GNorm = 0.4913
Meta loss on this task batch = 3.0943e-01, Meta loss averaged over last 500 steps = 3.0040e-01, PNorm = 135.2741, GNorm = 0.3024
Meta loss on this task batch = 2.9782e-01, Meta loss averaged over last 500 steps = 3.0027e-01, PNorm = 135.2802, GNorm = 0.3345
Meta loss on this task batch = 3.3354e-01, Meta loss averaged over last 500 steps = 3.0042e-01, PNorm = 135.2853, GNorm = 0.3541
Meta loss on this task batch = 2.5241e-01, Meta loss averaged over last 500 steps = 3.0019e-01, PNorm = 135.2911, GNorm = 0.2469
Meta loss on this task batch = 2.7371e-01, Meta loss averaged over last 500 steps = 3.0013e-01, PNorm = 135.2966, GNorm = 0.2999
Meta loss on this task batch = 3.3180e-01, Meta loss averaged over last 500 steps = 3.0008e-01, PNorm = 135.3021, GNorm = 0.3813
Meta loss on this task batch = 2.9449e-01, Meta loss averaged over last 500 steps = 3.0000e-01, PNorm = 135.3072, GNorm = 0.4124
Meta loss on this task batch = 3.4636e-01, Meta loss averaged over last 500 steps = 3.0008e-01, PNorm = 135.3110, GNorm = 0.3392
Meta loss on this task batch = 3.7259e-01, Meta loss averaged over last 500 steps = 3.0027e-01, PNorm = 135.3154, GNorm = 0.6691
Meta loss on this task batch = 3.9364e-01, Meta loss averaged over last 500 steps = 3.0043e-01, PNorm = 135.3193, GNorm = 0.4728
Meta loss on this task batch = 2.7697e-01, Meta loss averaged over last 500 steps = 3.0038e-01, PNorm = 135.3241, GNorm = 0.3197
Meta loss on this task batch = 2.6706e-01, Meta loss averaged over last 500 steps = 3.0033e-01, PNorm = 135.3296, GNorm = 0.2689
Meta loss on this task batch = 2.8112e-01, Meta loss averaged over last 500 steps = 3.0025e-01, PNorm = 135.3354, GNorm = 0.2302
Meta loss on this task batch = 2.8409e-01, Meta loss averaged over last 500 steps = 3.0001e-01, PNorm = 135.3411, GNorm = 0.2524
Meta loss on this task batch = 3.0919e-01, Meta loss averaged over last 500 steps = 3.0008e-01, PNorm = 135.3470, GNorm = 0.2801
Meta loss on this task batch = 3.4634e-01, Meta loss averaged over last 500 steps = 3.0019e-01, PNorm = 135.3529, GNorm = 0.3444
Took 108.02976417541504 seconds to complete one epoch of meta training
Took 116.01675939559937 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452396
Epoch 512
Meta loss on this task batch = 3.0858e-01, Meta loss averaged over last 500 steps = 3.0025e-01, PNorm = 135.3590, GNorm = 0.2697
Meta loss on this task batch = 3.4284e-01, Meta loss averaged over last 500 steps = 3.0034e-01, PNorm = 135.3652, GNorm = 0.3795
Meta loss on this task batch = 2.8210e-01, Meta loss averaged over last 500 steps = 3.0036e-01, PNorm = 135.3712, GNorm = 0.3533
Meta loss on this task batch = 3.0469e-01, Meta loss averaged over last 500 steps = 3.0039e-01, PNorm = 135.3768, GNorm = 0.3201
Meta loss on this task batch = 2.8845e-01, Meta loss averaged over last 500 steps = 3.0038e-01, PNorm = 135.3824, GNorm = 0.2576
Meta loss on this task batch = 3.2926e-01, Meta loss averaged over last 500 steps = 3.0043e-01, PNorm = 135.3885, GNorm = 0.3922
Meta loss on this task batch = 3.1123e-01, Meta loss averaged over last 500 steps = 3.0041e-01, PNorm = 135.3942, GNorm = 0.2921
Meta loss on this task batch = 2.0946e-01, Meta loss averaged over last 500 steps = 3.0007e-01, PNorm = 135.4002, GNorm = 0.2090
Meta loss on this task batch = 3.5971e-01, Meta loss averaged over last 500 steps = 3.0024e-01, PNorm = 135.4053, GNorm = 0.2979
Meta loss on this task batch = 2.8817e-01, Meta loss averaged over last 500 steps = 3.0023e-01, PNorm = 135.4109, GNorm = 0.2927
Meta loss on this task batch = 2.6253e-01, Meta loss averaged over last 500 steps = 3.0011e-01, PNorm = 135.4165, GNorm = 0.2611
Meta loss on this task batch = 3.0632e-01, Meta loss averaged over last 500 steps = 3.0009e-01, PNorm = 135.4215, GNorm = 0.2902
Meta loss on this task batch = 3.1704e-01, Meta loss averaged over last 500 steps = 3.0017e-01, PNorm = 135.4262, GNorm = 0.4218
Meta loss on this task batch = 3.0557e-01, Meta loss averaged over last 500 steps = 3.0017e-01, PNorm = 135.4315, GNorm = 0.2919
Meta loss on this task batch = 2.9801e-01, Meta loss averaged over last 500 steps = 3.0015e-01, PNorm = 135.4364, GNorm = 0.2625
Meta loss on this task batch = 2.9763e-01, Meta loss averaged over last 500 steps = 3.0027e-01, PNorm = 135.4416, GNorm = 0.4305
Meta loss on this task batch = 2.9653e-01, Meta loss averaged over last 500 steps = 3.0031e-01, PNorm = 135.4470, GNorm = 0.4720
Meta loss on this task batch = 3.4728e-01, Meta loss averaged over last 500 steps = 3.0031e-01, PNorm = 135.4522, GNorm = 0.2837
Meta loss on this task batch = 2.6515e-01, Meta loss averaged over last 500 steps = 3.0018e-01, PNorm = 135.4573, GNorm = 0.3680
Took 110.22998428344727 seconds to complete one epoch of meta training
Took 117.78071284294128 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458541
Epoch 513
Meta loss on this task batch = 2.6892e-01, Meta loss averaged over last 500 steps = 3.0009e-01, PNorm = 135.4630, GNorm = 0.3144
Meta loss on this task batch = 3.0359e-01, Meta loss averaged over last 500 steps = 3.0013e-01, PNorm = 135.4686, GNorm = 0.2590
Meta loss on this task batch = 2.2615e-01, Meta loss averaged over last 500 steps = 2.9993e-01, PNorm = 135.4741, GNorm = 0.2943
Meta loss on this task batch = 2.4114e-01, Meta loss averaged over last 500 steps = 2.9986e-01, PNorm = 135.4806, GNorm = 0.4053
Meta loss on this task batch = 2.7105e-01, Meta loss averaged over last 500 steps = 2.9971e-01, PNorm = 135.4870, GNorm = 0.2554
Meta loss on this task batch = 3.8858e-01, Meta loss averaged over last 500 steps = 2.9996e-01, PNorm = 135.4925, GNorm = 0.3512
Meta loss on this task batch = 3.2466e-01, Meta loss averaged over last 500 steps = 2.9997e-01, PNorm = 135.4972, GNorm = 0.2554
Meta loss on this task batch = 3.2470e-01, Meta loss averaged over last 500 steps = 3.0001e-01, PNorm = 135.5015, GNorm = 0.3715
Meta loss on this task batch = 2.9527e-01, Meta loss averaged over last 500 steps = 3.0007e-01, PNorm = 135.5054, GNorm = 0.2642
Meta loss on this task batch = 2.3186e-01, Meta loss averaged over last 500 steps = 2.9989e-01, PNorm = 135.5103, GNorm = 0.2758
Meta loss on this task batch = 3.0167e-01, Meta loss averaged over last 500 steps = 3.0000e-01, PNorm = 135.5155, GNorm = 0.3800
Meta loss on this task batch = 3.0426e-01, Meta loss averaged over last 500 steps = 3.0004e-01, PNorm = 135.5207, GNorm = 0.2741
Meta loss on this task batch = 3.5388e-01, Meta loss averaged over last 500 steps = 3.0004e-01, PNorm = 135.5247, GNorm = 0.4196
Meta loss on this task batch = 2.7457e-01, Meta loss averaged over last 500 steps = 3.0001e-01, PNorm = 135.5285, GNorm = 0.3027
Meta loss on this task batch = 3.1877e-01, Meta loss averaged over last 500 steps = 3.0002e-01, PNorm = 135.5319, GNorm = 0.3279
Meta loss on this task batch = 3.6234e-01, Meta loss averaged over last 500 steps = 3.0017e-01, PNorm = 135.5350, GNorm = 0.3077
Meta loss on this task batch = 2.8514e-01, Meta loss averaged over last 500 steps = 3.0016e-01, PNorm = 135.5388, GNorm = 0.2453
Meta loss on this task batch = 2.7922e-01, Meta loss averaged over last 500 steps = 3.0008e-01, PNorm = 135.5431, GNorm = 0.2799
Meta loss on this task batch = 3.4014e-01, Meta loss averaged over last 500 steps = 3.0013e-01, PNorm = 135.5474, GNorm = 0.3815
Took 112.14363121986389 seconds to complete one epoch of meta training
Took 119.83174467086792 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464537
Epoch 514
Meta loss on this task batch = 2.7542e-01, Meta loss averaged over last 500 steps = 3.0003e-01, PNorm = 135.5519, GNorm = 0.2966
Meta loss on this task batch = 2.7547e-01, Meta loss averaged over last 500 steps = 2.9999e-01, PNorm = 135.5569, GNorm = 0.2585
Meta loss on this task batch = 3.1392e-01, Meta loss averaged over last 500 steps = 3.0004e-01, PNorm = 135.5616, GNorm = 0.3189
Meta loss on this task batch = 2.9686e-01, Meta loss averaged over last 500 steps = 3.0008e-01, PNorm = 135.5663, GNorm = 0.2731
Meta loss on this task batch = 2.5636e-01, Meta loss averaged over last 500 steps = 3.0000e-01, PNorm = 135.5716, GNorm = 0.2900
Meta loss on this task batch = 3.0690e-01, Meta loss averaged over last 500 steps = 3.0002e-01, PNorm = 135.5769, GNorm = 0.2938
Meta loss on this task batch = 2.8091e-01, Meta loss averaged over last 500 steps = 3.0004e-01, PNorm = 135.5821, GNorm = 0.2940
Meta loss on this task batch = 3.1445e-01, Meta loss averaged over last 500 steps = 3.0003e-01, PNorm = 135.5867, GNorm = 0.2805
Meta loss on this task batch = 3.2978e-01, Meta loss averaged over last 500 steps = 3.0021e-01, PNorm = 135.5918, GNorm = 0.3075
Meta loss on this task batch = 3.1018e-01, Meta loss averaged over last 500 steps = 3.0023e-01, PNorm = 135.5964, GNorm = 0.2804
Meta loss on this task batch = 2.6865e-01, Meta loss averaged over last 500 steps = 3.0025e-01, PNorm = 135.6009, GNorm = 0.2393
Meta loss on this task batch = 2.8896e-01, Meta loss averaged over last 500 steps = 3.0019e-01, PNorm = 135.6060, GNorm = 0.3162
Meta loss on this task batch = 2.8439e-01, Meta loss averaged over last 500 steps = 3.0014e-01, PNorm = 135.6111, GNorm = 0.3310
Meta loss on this task batch = 3.6573e-01, Meta loss averaged over last 500 steps = 3.0033e-01, PNorm = 135.6135, GNorm = 0.5133
Meta loss on this task batch = 2.7742e-01, Meta loss averaged over last 500 steps = 3.0023e-01, PNorm = 135.6167, GNorm = 0.3797
Meta loss on this task batch = 3.3491e-01, Meta loss averaged over last 500 steps = 3.0031e-01, PNorm = 135.6197, GNorm = 0.2925
Meta loss on this task batch = 2.6136e-01, Meta loss averaged over last 500 steps = 3.0024e-01, PNorm = 135.6228, GNorm = 0.2708
Meta loss on this task batch = 2.5178e-01, Meta loss averaged over last 500 steps = 3.0007e-01, PNorm = 135.6260, GNorm = 0.2208
Meta loss on this task batch = 2.6613e-01, Meta loss averaged over last 500 steps = 2.9997e-01, PNorm = 135.6298, GNorm = 0.3099
Took 111.82450199127197 seconds to complete one epoch of meta training
Took 119.79734182357788 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.440155
Epoch 515
Meta loss on this task batch = 3.1172e-01, Meta loss averaged over last 500 steps = 2.9999e-01, PNorm = 135.6334, GNorm = 0.3306
Meta loss on this task batch = 3.0417e-01, Meta loss averaged over last 500 steps = 2.9986e-01, PNorm = 135.6374, GNorm = 0.3501
Meta loss on this task batch = 2.4844e-01, Meta loss averaged over last 500 steps = 2.9971e-01, PNorm = 135.6416, GNorm = 0.2631
Meta loss on this task batch = 3.0647e-01, Meta loss averaged over last 500 steps = 2.9970e-01, PNorm = 135.6472, GNorm = 0.2789
Meta loss on this task batch = 2.3707e-01, Meta loss averaged over last 500 steps = 2.9956e-01, PNorm = 135.6537, GNorm = 0.3423
Meta loss on this task batch = 3.3197e-01, Meta loss averaged over last 500 steps = 2.9951e-01, PNorm = 135.6598, GNorm = 0.2810
Meta loss on this task batch = 2.3471e-01, Meta loss averaged over last 500 steps = 2.9940e-01, PNorm = 135.6667, GNorm = 0.3748
Meta loss on this task batch = 3.1068e-01, Meta loss averaged over last 500 steps = 2.9944e-01, PNorm = 135.6736, GNorm = 0.2421
Meta loss on this task batch = 2.8720e-01, Meta loss averaged over last 500 steps = 2.9937e-01, PNorm = 135.6801, GNorm = 0.2692
Meta loss on this task batch = 2.6592e-01, Meta loss averaged over last 500 steps = 2.9928e-01, PNorm = 135.6867, GNorm = 0.3058
Meta loss on this task batch = 3.0217e-01, Meta loss averaged over last 500 steps = 2.9929e-01, PNorm = 135.6930, GNorm = 0.3424
Meta loss on this task batch = 2.9712e-01, Meta loss averaged over last 500 steps = 2.9931e-01, PNorm = 135.6993, GNorm = 0.3887
Meta loss on this task batch = 2.9751e-01, Meta loss averaged over last 500 steps = 2.9932e-01, PNorm = 135.7061, GNorm = 0.3047
Meta loss on this task batch = 2.7512e-01, Meta loss averaged over last 500 steps = 2.9933e-01, PNorm = 135.7129, GNorm = 0.2938
Meta loss on this task batch = 3.1444e-01, Meta loss averaged over last 500 steps = 2.9937e-01, PNorm = 135.7191, GNorm = 0.4503
Meta loss on this task batch = 3.0250e-01, Meta loss averaged over last 500 steps = 2.9940e-01, PNorm = 135.7247, GNorm = 0.3277
Meta loss on this task batch = 3.2905e-01, Meta loss averaged over last 500 steps = 2.9942e-01, PNorm = 135.7300, GNorm = 0.3052
Meta loss on this task batch = 2.9532e-01, Meta loss averaged over last 500 steps = 2.9937e-01, PNorm = 135.7345, GNorm = 0.3244
Meta loss on this task batch = 3.1676e-01, Meta loss averaged over last 500 steps = 2.9952e-01, PNorm = 135.7373, GNorm = 0.4243
Took 113.37311506271362 seconds to complete one epoch of meta training
Took 121.2578535079956 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449332
Epoch 516
Meta loss on this task batch = 3.1770e-01, Meta loss averaged over last 500 steps = 2.9956e-01, PNorm = 135.7404, GNorm = 0.3096
Meta loss on this task batch = 2.8530e-01, Meta loss averaged over last 500 steps = 2.9951e-01, PNorm = 135.7443, GNorm = 0.4688
Meta loss on this task batch = 2.3619e-01, Meta loss averaged over last 500 steps = 2.9931e-01, PNorm = 135.7478, GNorm = 0.2676
Meta loss on this task batch = 2.6978e-01, Meta loss averaged over last 500 steps = 2.9920e-01, PNorm = 135.7522, GNorm = 0.3060
Meta loss on this task batch = 3.2918e-01, Meta loss averaged over last 500 steps = 2.9923e-01, PNorm = 135.7564, GNorm = 0.2984
Meta loss on this task batch = 2.8656e-01, Meta loss averaged over last 500 steps = 2.9927e-01, PNorm = 135.7603, GNorm = 0.3595
Meta loss on this task batch = 2.9948e-01, Meta loss averaged over last 500 steps = 2.9929e-01, PNorm = 135.7641, GNorm = 0.2858
Meta loss on this task batch = 2.6856e-01, Meta loss averaged over last 500 steps = 2.9919e-01, PNorm = 135.7682, GNorm = 0.4437
Meta loss on this task batch = 3.0567e-01, Meta loss averaged over last 500 steps = 2.9917e-01, PNorm = 135.7719, GNorm = 0.4184
Meta loss on this task batch = 2.9445e-01, Meta loss averaged over last 500 steps = 2.9910e-01, PNorm = 135.7750, GNorm = 0.2593
Meta loss on this task batch = 3.0020e-01, Meta loss averaged over last 500 steps = 2.9917e-01, PNorm = 135.7787, GNorm = 0.3011
Meta loss on this task batch = 3.6771e-01, Meta loss averaged over last 500 steps = 2.9933e-01, PNorm = 135.7820, GNorm = 0.2861
Meta loss on this task batch = 3.1182e-01, Meta loss averaged over last 500 steps = 2.9943e-01, PNorm = 135.7864, GNorm = 0.3915
Meta loss on this task batch = 2.7842e-01, Meta loss averaged over last 500 steps = 2.9934e-01, PNorm = 135.7919, GNorm = 0.2976
Meta loss on this task batch = 3.3747e-01, Meta loss averaged over last 500 steps = 2.9958e-01, PNorm = 135.7979, GNorm = 0.2982
Meta loss on this task batch = 3.0586e-01, Meta loss averaged over last 500 steps = 2.9958e-01, PNorm = 135.8037, GNorm = 0.2793
Meta loss on this task batch = 2.9990e-01, Meta loss averaged over last 500 steps = 2.9953e-01, PNorm = 135.8098, GNorm = 0.2894
Meta loss on this task batch = 3.5738e-01, Meta loss averaged over last 500 steps = 2.9957e-01, PNorm = 135.8157, GNorm = 0.2746
Meta loss on this task batch = 2.7367e-01, Meta loss averaged over last 500 steps = 2.9950e-01, PNorm = 135.8218, GNorm = 0.2758
Took 108.55551433563232 seconds to complete one epoch of meta training
Took 115.87191033363342 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459086
Epoch 517
Meta loss on this task batch = 2.5124e-01, Meta loss averaged over last 500 steps = 2.9934e-01, PNorm = 135.8282, GNorm = 0.3059
Meta loss on this task batch = 3.1817e-01, Meta loss averaged over last 500 steps = 2.9930e-01, PNorm = 135.8341, GNorm = 0.2686
Meta loss on this task batch = 3.0042e-01, Meta loss averaged over last 500 steps = 2.9928e-01, PNorm = 135.8396, GNorm = 0.3011
Meta loss on this task batch = 2.9548e-01, Meta loss averaged over last 500 steps = 2.9933e-01, PNorm = 135.8450, GNorm = 0.5048
Meta loss on this task batch = 3.3696e-01, Meta loss averaged over last 500 steps = 2.9938e-01, PNorm = 135.8501, GNorm = 0.4244
Meta loss on this task batch = 2.9946e-01, Meta loss averaged over last 500 steps = 2.9944e-01, PNorm = 135.8553, GNorm = 0.2730
Meta loss on this task batch = 2.7707e-01, Meta loss averaged over last 500 steps = 2.9945e-01, PNorm = 135.8611, GNorm = 0.2677
Meta loss on this task batch = 2.7148e-01, Meta loss averaged over last 500 steps = 2.9940e-01, PNorm = 135.8677, GNorm = 0.2828
Meta loss on this task batch = 2.9156e-01, Meta loss averaged over last 500 steps = 2.9940e-01, PNorm = 135.8743, GNorm = 0.3224
Meta loss on this task batch = 2.6849e-01, Meta loss averaged over last 500 steps = 2.9928e-01, PNorm = 135.8806, GNorm = 0.3752
Meta loss on this task batch = 3.3879e-01, Meta loss averaged over last 500 steps = 2.9935e-01, PNorm = 135.8865, GNorm = 0.2937
Meta loss on this task batch = 2.6850e-01, Meta loss averaged over last 500 steps = 2.9935e-01, PNorm = 135.8922, GNorm = 0.2582
Meta loss on this task batch = 2.8592e-01, Meta loss averaged over last 500 steps = 2.9928e-01, PNorm = 135.8974, GNorm = 0.2981
Meta loss on this task batch = 3.5563e-01, Meta loss averaged over last 500 steps = 2.9946e-01, PNorm = 135.9022, GNorm = 0.3368
Meta loss on this task batch = 3.1407e-01, Meta loss averaged over last 500 steps = 2.9951e-01, PNorm = 135.9055, GNorm = 0.3225
Meta loss on this task batch = 3.3464e-01, Meta loss averaged over last 500 steps = 2.9955e-01, PNorm = 135.9093, GNorm = 0.4036
Meta loss on this task batch = 2.7328e-01, Meta loss averaged over last 500 steps = 2.9948e-01, PNorm = 135.9132, GNorm = 0.2877
Meta loss on this task batch = 3.2596e-01, Meta loss averaged over last 500 steps = 2.9956e-01, PNorm = 135.9168, GNorm = 0.3635
Meta loss on this task batch = 2.7970e-01, Meta loss averaged over last 500 steps = 2.9955e-01, PNorm = 135.9222, GNorm = 0.5353
Took 112.48510313034058 seconds to complete one epoch of meta training
Took 120.61466550827026 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470753
Epoch 518
Meta loss on this task batch = 2.5576e-01, Meta loss averaged over last 500 steps = 2.9951e-01, PNorm = 135.9277, GNorm = 0.3320
Meta loss on this task batch = 2.9023e-01, Meta loss averaged over last 500 steps = 2.9953e-01, PNorm = 135.9326, GNorm = 0.3036
Meta loss on this task batch = 2.8070e-01, Meta loss averaged over last 500 steps = 2.9941e-01, PNorm = 135.9372, GNorm = 0.2352
Meta loss on this task batch = 2.7600e-01, Meta loss averaged over last 500 steps = 2.9945e-01, PNorm = 135.9420, GNorm = 0.2301
Meta loss on this task batch = 2.5013e-01, Meta loss averaged over last 500 steps = 2.9931e-01, PNorm = 135.9470, GNorm = 0.3931
Meta loss on this task batch = 2.6730e-01, Meta loss averaged over last 500 steps = 2.9922e-01, PNorm = 135.9515, GNorm = 0.2327
Meta loss on this task batch = 2.9119e-01, Meta loss averaged over last 500 steps = 2.9907e-01, PNorm = 135.9560, GNorm = 0.2958
Meta loss on this task batch = 2.9231e-01, Meta loss averaged over last 500 steps = 2.9907e-01, PNorm = 135.9611, GNorm = 0.3262
Meta loss on this task batch = 2.9748e-01, Meta loss averaged over last 500 steps = 2.9905e-01, PNorm = 135.9659, GNorm = 0.2994
Meta loss on this task batch = 2.9488e-01, Meta loss averaged over last 500 steps = 2.9889e-01, PNorm = 135.9707, GNorm = 0.3123
Meta loss on this task batch = 2.8044e-01, Meta loss averaged over last 500 steps = 2.9892e-01, PNorm = 135.9756, GNorm = 0.2847
Meta loss on this task batch = 3.3717e-01, Meta loss averaged over last 500 steps = 2.9908e-01, PNorm = 135.9808, GNorm = 0.3213
Meta loss on this task batch = 3.6979e-01, Meta loss averaged over last 500 steps = 2.9930e-01, PNorm = 135.9852, GNorm = 0.3022
Meta loss on this task batch = 3.2404e-01, Meta loss averaged over last 500 steps = 2.9931e-01, PNorm = 135.9885, GNorm = 0.2866
Meta loss on this task batch = 2.7009e-01, Meta loss averaged over last 500 steps = 2.9924e-01, PNorm = 135.9922, GNorm = 0.2943
Meta loss on this task batch = 2.6990e-01, Meta loss averaged over last 500 steps = 2.9908e-01, PNorm = 135.9966, GNorm = 0.2369
Meta loss on this task batch = 3.0517e-01, Meta loss averaged over last 500 steps = 2.9910e-01, PNorm = 136.0007, GNorm = 0.3836
Meta loss on this task batch = 2.9825e-01, Meta loss averaged over last 500 steps = 2.9925e-01, PNorm = 136.0056, GNorm = 0.2365
Meta loss on this task batch = 2.8944e-01, Meta loss averaged over last 500 steps = 2.9928e-01, PNorm = 136.0100, GNorm = 0.2855
Took 122.75043058395386 seconds to complete one epoch of meta training
Took 130.6598916053772 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446600
Epoch 519
Meta loss on this task batch = 2.3619e-01, Meta loss averaged over last 500 steps = 2.9920e-01, PNorm = 136.0145, GNorm = 0.2528
Meta loss on this task batch = 3.1702e-01, Meta loss averaged over last 500 steps = 2.9916e-01, PNorm = 136.0191, GNorm = 0.2792
Meta loss on this task batch = 3.1109e-01, Meta loss averaged over last 500 steps = 2.9925e-01, PNorm = 136.0238, GNorm = 0.2951
Meta loss on this task batch = 2.8206e-01, Meta loss averaged over last 500 steps = 2.9925e-01, PNorm = 136.0288, GNorm = 0.2958
Meta loss on this task batch = 3.0195e-01, Meta loss averaged over last 500 steps = 2.9907e-01, PNorm = 136.0348, GNorm = 0.2722
Meta loss on this task batch = 2.4397e-01, Meta loss averaged over last 500 steps = 2.9904e-01, PNorm = 136.0417, GNorm = 0.2403
Meta loss on this task batch = 3.3581e-01, Meta loss averaged over last 500 steps = 2.9916e-01, PNorm = 136.0492, GNorm = 0.3225
Meta loss on this task batch = 2.3296e-01, Meta loss averaged over last 500 steps = 2.9906e-01, PNorm = 136.0570, GNorm = 0.2747
Meta loss on this task batch = 3.3098e-01, Meta loss averaged over last 500 steps = 2.9916e-01, PNorm = 136.0648, GNorm = 0.2991
Meta loss on this task batch = 2.9093e-01, Meta loss averaged over last 500 steps = 2.9906e-01, PNorm = 136.0724, GNorm = 0.3814
Meta loss on this task batch = 3.2022e-01, Meta loss averaged over last 500 steps = 2.9904e-01, PNorm = 136.0783, GNorm = 0.3418
Meta loss on this task batch = 2.6787e-01, Meta loss averaged over last 500 steps = 2.9888e-01, PNorm = 136.0843, GNorm = 0.2758
Meta loss on this task batch = 2.8610e-01, Meta loss averaged over last 500 steps = 2.9891e-01, PNorm = 136.0902, GNorm = 0.2823
Meta loss on this task batch = 3.3314e-01, Meta loss averaged over last 500 steps = 2.9900e-01, PNorm = 136.0948, GNorm = 0.3583
Meta loss on this task batch = 2.6896e-01, Meta loss averaged over last 500 steps = 2.9891e-01, PNorm = 136.0996, GNorm = 0.2451
Meta loss on this task batch = 2.5749e-01, Meta loss averaged over last 500 steps = 2.9882e-01, PNorm = 136.1048, GNorm = 0.2960
Meta loss on this task batch = 2.8925e-01, Meta loss averaged over last 500 steps = 2.9889e-01, PNorm = 136.1101, GNorm = 0.3218
Meta loss on this task batch = 2.7159e-01, Meta loss averaged over last 500 steps = 2.9883e-01, PNorm = 136.1155, GNorm = 0.2453
Meta loss on this task batch = 3.3934e-01, Meta loss averaged over last 500 steps = 2.9897e-01, PNorm = 136.1213, GNorm = 0.4687
Took 110.0601806640625 seconds to complete one epoch of meta training
Took 117.87206768989563 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453844
Epoch 520
Meta loss on this task batch = 2.8062e-01, Meta loss averaged over last 500 steps = 2.9891e-01, PNorm = 136.1268, GNorm = 0.2886
Meta loss on this task batch = 2.6992e-01, Meta loss averaged over last 500 steps = 2.9883e-01, PNorm = 136.1326, GNorm = 0.3088
Meta loss on this task batch = 3.0242e-01, Meta loss averaged over last 500 steps = 2.9882e-01, PNorm = 136.1378, GNorm = 0.3322
Meta loss on this task batch = 2.7801e-01, Meta loss averaged over last 500 steps = 2.9884e-01, PNorm = 136.1428, GNorm = 0.2798
Meta loss on this task batch = 2.7877e-01, Meta loss averaged over last 500 steps = 2.9881e-01, PNorm = 136.1476, GNorm = 0.3134
Meta loss on this task batch = 3.2980e-01, Meta loss averaged over last 500 steps = 2.9890e-01, PNorm = 136.1522, GNorm = 0.2958
Meta loss on this task batch = 3.2498e-01, Meta loss averaged over last 500 steps = 2.9891e-01, PNorm = 136.1573, GNorm = 0.4762
Meta loss on this task batch = 3.0448e-01, Meta loss averaged over last 500 steps = 2.9896e-01, PNorm = 136.1624, GNorm = 0.2943
Meta loss on this task batch = 2.9927e-01, Meta loss averaged over last 500 steps = 2.9891e-01, PNorm = 136.1675, GNorm = 0.2925
Meta loss on this task batch = 2.5116e-01, Meta loss averaged over last 500 steps = 2.9882e-01, PNorm = 136.1736, GNorm = 0.2637
Meta loss on this task batch = 3.2413e-01, Meta loss averaged over last 500 steps = 2.9900e-01, PNorm = 136.1791, GNorm = 0.3457
Meta loss on this task batch = 2.4644e-01, Meta loss averaged over last 500 steps = 2.9880e-01, PNorm = 136.1855, GNorm = 0.2706
Meta loss on this task batch = 3.0810e-01, Meta loss averaged over last 500 steps = 2.9885e-01, PNorm = 136.1917, GNorm = 0.2676
Meta loss on this task batch = 3.0599e-01, Meta loss averaged over last 500 steps = 2.9884e-01, PNorm = 136.1983, GNorm = 0.2783
Meta loss on this task batch = 3.9791e-01, Meta loss averaged over last 500 steps = 2.9893e-01, PNorm = 136.2043, GNorm = 0.3078
Meta loss on this task batch = 2.9818e-01, Meta loss averaged over last 500 steps = 2.9890e-01, PNorm = 136.2104, GNorm = 0.2741
Meta loss on this task batch = 3.1405e-01, Meta loss averaged over last 500 steps = 2.9892e-01, PNorm = 136.2160, GNorm = 0.2991
Meta loss on this task batch = 3.0513e-01, Meta loss averaged over last 500 steps = 2.9889e-01, PNorm = 136.2219, GNorm = 0.2821
Meta loss on this task batch = 2.6102e-01, Meta loss averaged over last 500 steps = 2.9890e-01, PNorm = 136.2285, GNorm = 0.3707
Took 112.04499793052673 seconds to complete one epoch of meta training
Took 119.71784090995789 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447673
Epoch 521
Meta loss on this task batch = 2.6877e-01, Meta loss averaged over last 500 steps = 2.9881e-01, PNorm = 136.2357, GNorm = 0.3271
Meta loss on this task batch = 2.5116e-01, Meta loss averaged over last 500 steps = 2.9862e-01, PNorm = 136.2436, GNorm = 0.3000
Meta loss on this task batch = 3.2455e-01, Meta loss averaged over last 500 steps = 2.9867e-01, PNorm = 136.2515, GNorm = 0.3230
Meta loss on this task batch = 2.9785e-01, Meta loss averaged over last 500 steps = 2.9856e-01, PNorm = 136.2587, GNorm = 0.2925
Meta loss on this task batch = 2.8572e-01, Meta loss averaged over last 500 steps = 2.9855e-01, PNorm = 136.2660, GNorm = 0.3414
Meta loss on this task batch = 3.5524e-01, Meta loss averaged over last 500 steps = 2.9863e-01, PNorm = 136.2734, GNorm = 0.7047
Meta loss on this task batch = 2.7688e-01, Meta loss averaged over last 500 steps = 2.9855e-01, PNorm = 136.2797, GNorm = 0.3164
Meta loss on this task batch = 2.9977e-01, Meta loss averaged over last 500 steps = 2.9863e-01, PNorm = 136.2852, GNorm = 0.2838
Meta loss on this task batch = 3.4177e-01, Meta loss averaged over last 500 steps = 2.9866e-01, PNorm = 136.2892, GNorm = 0.3227
Meta loss on this task batch = 2.9047e-01, Meta loss averaged over last 500 steps = 2.9867e-01, PNorm = 136.2930, GNorm = 0.2984
Meta loss on this task batch = 3.5581e-01, Meta loss averaged over last 500 steps = 2.9878e-01, PNorm = 136.2963, GNorm = 0.3678
Meta loss on this task batch = 3.5198e-01, Meta loss averaged over last 500 steps = 2.9897e-01, PNorm = 136.2998, GNorm = 0.2894
Meta loss on this task batch = 3.1805e-01, Meta loss averaged over last 500 steps = 2.9906e-01, PNorm = 136.3026, GNorm = 0.4829
Meta loss on this task batch = 3.4675e-01, Meta loss averaged over last 500 steps = 2.9915e-01, PNorm = 136.3057, GNorm = 0.4019
Meta loss on this task batch = 2.5369e-01, Meta loss averaged over last 500 steps = 2.9903e-01, PNorm = 136.3099, GNorm = 0.2886
Meta loss on this task batch = 2.8766e-01, Meta loss averaged over last 500 steps = 2.9894e-01, PNorm = 136.3136, GNorm = 0.2566
Meta loss on this task batch = 3.3488e-01, Meta loss averaged over last 500 steps = 2.9893e-01, PNorm = 136.3176, GNorm = 0.3082
Meta loss on this task batch = 3.0331e-01, Meta loss averaged over last 500 steps = 2.9903e-01, PNorm = 136.3216, GNorm = 0.3656
Meta loss on this task batch = 2.7847e-01, Meta loss averaged over last 500 steps = 2.9898e-01, PNorm = 136.3271, GNorm = 0.3493
Took 110.98263263702393 seconds to complete one epoch of meta training
Took 118.87199425697327 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447451
Epoch 522
Meta loss on this task batch = 2.7671e-01, Meta loss averaged over last 500 steps = 2.9900e-01, PNorm = 136.3333, GNorm = 0.3239
Meta loss on this task batch = 2.9251e-01, Meta loss averaged over last 500 steps = 2.9901e-01, PNorm = 136.3393, GNorm = 0.2927
Meta loss on this task batch = 2.8522e-01, Meta loss averaged over last 500 steps = 2.9896e-01, PNorm = 136.3457, GNorm = 0.3062
Meta loss on this task batch = 2.9069e-01, Meta loss averaged over last 500 steps = 2.9875e-01, PNorm = 136.3520, GNorm = 0.2616
Meta loss on this task batch = 1.8637e-01, Meta loss averaged over last 500 steps = 2.9855e-01, PNorm = 136.3596, GNorm = 0.2369
Meta loss on this task batch = 2.6136e-01, Meta loss averaged over last 500 steps = 2.9848e-01, PNorm = 136.3669, GNorm = 0.2193
Meta loss on this task batch = 3.3078e-01, Meta loss averaged over last 500 steps = 2.9859e-01, PNorm = 136.3743, GNorm = 0.5048
Meta loss on this task batch = 2.9721e-01, Meta loss averaged over last 500 steps = 2.9867e-01, PNorm = 136.3812, GNorm = 0.2852
Meta loss on this task batch = 3.2131e-01, Meta loss averaged over last 500 steps = 2.9861e-01, PNorm = 136.3875, GNorm = 0.2975
Meta loss on this task batch = 3.3639e-01, Meta loss averaged over last 500 steps = 2.9871e-01, PNorm = 136.3930, GNorm = 0.2992
Meta loss on this task batch = 2.7710e-01, Meta loss averaged over last 500 steps = 2.9863e-01, PNorm = 136.3990, GNorm = 0.4224
Meta loss on this task batch = 2.9095e-01, Meta loss averaged over last 500 steps = 2.9859e-01, PNorm = 136.4041, GNorm = 0.3680
Meta loss on this task batch = 3.0808e-01, Meta loss averaged over last 500 steps = 2.9848e-01, PNorm = 136.4089, GNorm = 0.2723
Meta loss on this task batch = 3.2247e-01, Meta loss averaged over last 500 steps = 2.9858e-01, PNorm = 136.4141, GNorm = 0.4658
Meta loss on this task batch = 3.2740e-01, Meta loss averaged over last 500 steps = 2.9857e-01, PNorm = 136.4199, GNorm = 0.4024
Meta loss on this task batch = 2.4635e-01, Meta loss averaged over last 500 steps = 2.9847e-01, PNorm = 136.4260, GNorm = 0.2352
Meta loss on this task batch = 2.7649e-01, Meta loss averaged over last 500 steps = 2.9841e-01, PNorm = 136.4327, GNorm = 0.3553
Meta loss on this task batch = 3.6593e-01, Meta loss averaged over last 500 steps = 2.9856e-01, PNorm = 136.4387, GNorm = 0.3384
Meta loss on this task batch = 3.1780e-01, Meta loss averaged over last 500 steps = 2.9863e-01, PNorm = 136.4442, GNorm = 0.3813
Took 111.19979691505432 seconds to complete one epoch of meta training
Took 118.93545460700989 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471231
Epoch 523
Meta loss on this task batch = 3.5139e-01, Meta loss averaged over last 500 steps = 2.9869e-01, PNorm = 136.4483, GNorm = 0.2689
Meta loss on this task batch = 3.3244e-01, Meta loss averaged over last 500 steps = 2.9868e-01, PNorm = 136.4517, GNorm = 0.3812
Meta loss on this task batch = 2.3894e-01, Meta loss averaged over last 500 steps = 2.9850e-01, PNorm = 136.4561, GNorm = 0.2442
Meta loss on this task batch = 3.3639e-01, Meta loss averaged over last 500 steps = 2.9863e-01, PNorm = 136.4606, GNorm = 0.4670
Meta loss on this task batch = 2.1860e-01, Meta loss averaged over last 500 steps = 2.9851e-01, PNorm = 136.4657, GNorm = 0.4403
Meta loss on this task batch = 2.7764e-01, Meta loss averaged over last 500 steps = 2.9842e-01, PNorm = 136.4711, GNorm = 0.2745
Meta loss on this task batch = 2.6292e-01, Meta loss averaged over last 500 steps = 2.9825e-01, PNorm = 136.4763, GNorm = 0.4294
Meta loss on this task batch = 3.0723e-01, Meta loss averaged over last 500 steps = 2.9822e-01, PNorm = 136.4811, GNorm = 0.3071
Meta loss on this task batch = 3.4393e-01, Meta loss averaged over last 500 steps = 2.9826e-01, PNorm = 136.4858, GNorm = 0.3150
Meta loss on this task batch = 3.4964e-01, Meta loss averaged over last 500 steps = 2.9842e-01, PNorm = 136.4903, GNorm = 0.2995
Meta loss on this task batch = 2.7230e-01, Meta loss averaged over last 500 steps = 2.9840e-01, PNorm = 136.4952, GNorm = 0.2514
Meta loss on this task batch = 2.9924e-01, Meta loss averaged over last 500 steps = 2.9839e-01, PNorm = 136.4995, GNorm = 0.2721
Meta loss on this task batch = 2.5965e-01, Meta loss averaged over last 500 steps = 2.9825e-01, PNorm = 136.5044, GNorm = 0.2458
Meta loss on this task batch = 2.9897e-01, Meta loss averaged over last 500 steps = 2.9829e-01, PNorm = 136.5090, GNorm = 0.2497
Meta loss on this task batch = 2.9873e-01, Meta loss averaged over last 500 steps = 2.9832e-01, PNorm = 136.5132, GNorm = 0.3837
Meta loss on this task batch = 2.7790e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 136.5182, GNorm = 0.2471
Meta loss on this task batch = 3.1098e-01, Meta loss averaged over last 500 steps = 2.9815e-01, PNorm = 136.5230, GNorm = 0.2951
Meta loss on this task batch = 2.9242e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 136.5278, GNorm = 0.3434
Meta loss on this task batch = 2.9921e-01, Meta loss averaged over last 500 steps = 2.9813e-01, PNorm = 136.5319, GNorm = 0.3211
Took 109.25783395767212 seconds to complete one epoch of meta training
Took 116.77935028076172 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.438886
Epoch 524
Meta loss on this task batch = 3.1375e-01, Meta loss averaged over last 500 steps = 2.9809e-01, PNorm = 136.5363, GNorm = 0.2925
Meta loss on this task batch = 2.9086e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 136.5405, GNorm = 0.2826
Meta loss on this task batch = 2.8088e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 136.5454, GNorm = 0.3116
Meta loss on this task batch = 3.1548e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 136.5506, GNorm = 0.3851
Meta loss on this task batch = 3.0244e-01, Meta loss averaged over last 500 steps = 2.9825e-01, PNorm = 136.5561, GNorm = 0.2847
Meta loss on this task batch = 3.0132e-01, Meta loss averaged over last 500 steps = 2.9823e-01, PNorm = 136.5620, GNorm = 0.3108
Meta loss on this task batch = 2.7792e-01, Meta loss averaged over last 500 steps = 2.9824e-01, PNorm = 136.5685, GNorm = 0.2607
Meta loss on this task batch = 3.0320e-01, Meta loss averaged over last 500 steps = 2.9816e-01, PNorm = 136.5752, GNorm = 0.3414
Meta loss on this task batch = 3.3989e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 136.5810, GNorm = 0.2844
Meta loss on this task batch = 2.5247e-01, Meta loss averaged over last 500 steps = 2.9807e-01, PNorm = 136.5874, GNorm = 0.2718
Meta loss on this task batch = 3.0465e-01, Meta loss averaged over last 500 steps = 2.9797e-01, PNorm = 136.5938, GNorm = 0.4067
Meta loss on this task batch = 2.1678e-01, Meta loss averaged over last 500 steps = 2.9781e-01, PNorm = 136.6007, GNorm = 0.3170
Meta loss on this task batch = 3.3903e-01, Meta loss averaged over last 500 steps = 2.9789e-01, PNorm = 136.6069, GNorm = 0.2719
Meta loss on this task batch = 3.2554e-01, Meta loss averaged over last 500 steps = 2.9805e-01, PNorm = 136.6131, GNorm = 0.2602
Meta loss on this task batch = 3.3085e-01, Meta loss averaged over last 500 steps = 2.9814e-01, PNorm = 136.6195, GNorm = 0.2859
Meta loss on this task batch = 2.8338e-01, Meta loss averaged over last 500 steps = 2.9806e-01, PNorm = 136.6256, GNorm = 0.3267
Meta loss on this task batch = 3.1872e-01, Meta loss averaged over last 500 steps = 2.9815e-01, PNorm = 136.6314, GNorm = 0.3300
Meta loss on this task batch = 2.7046e-01, Meta loss averaged over last 500 steps = 2.9800e-01, PNorm = 136.6371, GNorm = 0.2736
Meta loss on this task batch = 2.7038e-01, Meta loss averaged over last 500 steps = 2.9792e-01, PNorm = 136.6434, GNorm = 0.4257
Took 112.98102235794067 seconds to complete one epoch of meta training
Took 120.67318487167358 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455579
Epoch 525
Meta loss on this task batch = 2.3750e-01, Meta loss averaged over last 500 steps = 2.9784e-01, PNorm = 136.6492, GNorm = 0.2542
Meta loss on this task batch = 2.9163e-01, Meta loss averaged over last 500 steps = 2.9787e-01, PNorm = 136.6556, GNorm = 0.3416
Meta loss on this task batch = 3.0690e-01, Meta loss averaged over last 500 steps = 2.9789e-01, PNorm = 136.6616, GNorm = 0.3429
Meta loss on this task batch = 3.0785e-01, Meta loss averaged over last 500 steps = 2.9793e-01, PNorm = 136.6672, GNorm = 0.4540
Meta loss on this task batch = 2.8159e-01, Meta loss averaged over last 500 steps = 2.9789e-01, PNorm = 136.6731, GNorm = 0.3171
Meta loss on this task batch = 2.5605e-01, Meta loss averaged over last 500 steps = 2.9790e-01, PNorm = 136.6784, GNorm = 0.3246
Meta loss on this task batch = 3.1263e-01, Meta loss averaged over last 500 steps = 2.9783e-01, PNorm = 136.6843, GNorm = 0.2672
Meta loss on this task batch = 2.8010e-01, Meta loss averaged over last 500 steps = 2.9780e-01, PNorm = 136.6904, GNorm = 0.3405
Meta loss on this task batch = 2.9844e-01, Meta loss averaged over last 500 steps = 2.9787e-01, PNorm = 136.6964, GNorm = 0.4464
Meta loss on this task batch = 3.1760e-01, Meta loss averaged over last 500 steps = 2.9795e-01, PNorm = 136.7017, GNorm = 0.3255
Meta loss on this task batch = 2.7291e-01, Meta loss averaged over last 500 steps = 2.9785e-01, PNorm = 136.7062, GNorm = 0.2862
Meta loss on this task batch = 3.0378e-01, Meta loss averaged over last 500 steps = 2.9791e-01, PNorm = 136.7096, GNorm = 0.3370
Meta loss on this task batch = 2.6529e-01, Meta loss averaged over last 500 steps = 2.9790e-01, PNorm = 136.7140, GNorm = 0.2657
Meta loss on this task batch = 2.9492e-01, Meta loss averaged over last 500 steps = 2.9789e-01, PNorm = 136.7178, GNorm = 0.2853
Meta loss on this task batch = 3.1106e-01, Meta loss averaged over last 500 steps = 2.9787e-01, PNorm = 136.7218, GNorm = 0.3212
Meta loss on this task batch = 3.7226e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 136.7248, GNorm = 0.5637
Meta loss on this task batch = 2.9818e-01, Meta loss averaged over last 500 steps = 2.9795e-01, PNorm = 136.7283, GNorm = 0.2655
Meta loss on this task batch = 2.9502e-01, Meta loss averaged over last 500 steps = 2.9791e-01, PNorm = 136.7317, GNorm = 0.2824
Meta loss on this task batch = 3.3138e-01, Meta loss averaged over last 500 steps = 2.9793e-01, PNorm = 136.7361, GNorm = 0.4456
Took 108.67115092277527 seconds to complete one epoch of meta training
Took 115.69510340690613 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456753
Epoch 526
Meta loss on this task batch = 3.3177e-01, Meta loss averaged over last 500 steps = 2.9802e-01, PNorm = 136.7400, GNorm = 0.3182
Meta loss on this task batch = 3.0972e-01, Meta loss averaged over last 500 steps = 2.9809e-01, PNorm = 136.7448, GNorm = 0.2890
Meta loss on this task batch = 2.8270e-01, Meta loss averaged over last 500 steps = 2.9811e-01, PNorm = 136.7504, GNorm = 0.2592
Meta loss on this task batch = 2.2012e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 136.7564, GNorm = 0.1924
Meta loss on this task batch = 3.0377e-01, Meta loss averaged over last 500 steps = 2.9799e-01, PNorm = 136.7619, GNorm = 0.2232
Meta loss on this task batch = 3.2812e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 136.7670, GNorm = 0.2406
Meta loss on this task batch = 3.1061e-01, Meta loss averaged over last 500 steps = 2.9806e-01, PNorm = 136.7721, GNorm = 0.2732
Meta loss on this task batch = 2.9637e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 136.7776, GNorm = 0.3122
Meta loss on this task batch = 3.3811e-01, Meta loss averaged over last 500 steps = 2.9826e-01, PNorm = 136.7823, GNorm = 0.3258
Meta loss on this task batch = 3.4134e-01, Meta loss averaged over last 500 steps = 2.9834e-01, PNorm = 136.7860, GNorm = 0.3850
Meta loss on this task batch = 2.8265e-01, Meta loss averaged over last 500 steps = 2.9820e-01, PNorm = 136.7893, GNorm = 0.4806
Meta loss on this task batch = 2.9080e-01, Meta loss averaged over last 500 steps = 2.9814e-01, PNorm = 136.7935, GNorm = 0.2985
Meta loss on this task batch = 2.8507e-01, Meta loss averaged over last 500 steps = 2.9814e-01, PNorm = 136.7978, GNorm = 0.4930
Meta loss on this task batch = 2.5190e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 136.8034, GNorm = 0.2593
Meta loss on this task batch = 2.6552e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 136.8085, GNorm = 0.2410
Meta loss on this task batch = 3.0727e-01, Meta loss averaged over last 500 steps = 2.9807e-01, PNorm = 136.8129, GNorm = 0.2849
Meta loss on this task batch = 3.0893e-01, Meta loss averaged over last 500 steps = 2.9806e-01, PNorm = 136.8177, GNorm = 0.3147
Meta loss on this task batch = 2.2771e-01, Meta loss averaged over last 500 steps = 2.9795e-01, PNorm = 136.8226, GNorm = 0.2883
Meta loss on this task batch = 3.0411e-01, Meta loss averaged over last 500 steps = 2.9799e-01, PNorm = 136.8280, GNorm = 0.7856
Took 111.75475811958313 seconds to complete one epoch of meta training
Took 119.40857434272766 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465943
Epoch 527
Meta loss on this task batch = 3.0601e-01, Meta loss averaged over last 500 steps = 2.9799e-01, PNorm = 136.8338, GNorm = 0.2564
Meta loss on this task batch = 3.0063e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 136.8402, GNorm = 0.4245
Meta loss on this task batch = 3.7298e-01, Meta loss averaged over last 500 steps = 2.9814e-01, PNorm = 136.8456, GNorm = 0.3790
Meta loss on this task batch = 2.6577e-01, Meta loss averaged over last 500 steps = 2.9805e-01, PNorm = 136.8513, GNorm = 0.2378
Meta loss on this task batch = 2.9311e-01, Meta loss averaged over last 500 steps = 2.9795e-01, PNorm = 136.8574, GNorm = 0.2438
Meta loss on this task batch = 2.5300e-01, Meta loss averaged over last 500 steps = 2.9789e-01, PNorm = 136.8638, GNorm = 0.3150
Meta loss on this task batch = 2.9542e-01, Meta loss averaged over last 500 steps = 2.9783e-01, PNorm = 136.8695, GNorm = 0.2867
Meta loss on this task batch = 3.0075e-01, Meta loss averaged over last 500 steps = 2.9777e-01, PNorm = 136.8762, GNorm = 0.3721
Meta loss on this task batch = 3.1056e-01, Meta loss averaged over last 500 steps = 2.9785e-01, PNorm = 136.8837, GNorm = 0.4330
Meta loss on this task batch = 2.2716e-01, Meta loss averaged over last 500 steps = 2.9772e-01, PNorm = 136.8917, GNorm = 0.3427
Meta loss on this task batch = 3.4657e-01, Meta loss averaged over last 500 steps = 2.9785e-01, PNorm = 136.8994, GNorm = 0.2480
Meta loss on this task batch = 3.1520e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 136.9068, GNorm = 0.3798
Meta loss on this task batch = 2.6728e-01, Meta loss averaged over last 500 steps = 2.9797e-01, PNorm = 136.9146, GNorm = 0.2240
Meta loss on this task batch = 2.7739e-01, Meta loss averaged over last 500 steps = 2.9782e-01, PNorm = 136.9222, GNorm = 0.3911
Meta loss on this task batch = 2.7799e-01, Meta loss averaged over last 500 steps = 2.9780e-01, PNorm = 136.9291, GNorm = 0.2656
Meta loss on this task batch = 2.9780e-01, Meta loss averaged over last 500 steps = 2.9778e-01, PNorm = 136.9357, GNorm = 0.3007
Meta loss on this task batch = 3.2456e-01, Meta loss averaged over last 500 steps = 2.9783e-01, PNorm = 136.9419, GNorm = 0.3388
Meta loss on this task batch = 2.4179e-01, Meta loss averaged over last 500 steps = 2.9768e-01, PNorm = 136.9488, GNorm = 0.3785
Meta loss on this task batch = 2.9678e-01, Meta loss averaged over last 500 steps = 2.9760e-01, PNorm = 136.9561, GNorm = 0.3921
Took 111.17741799354553 seconds to complete one epoch of meta training
Took 119.12052512168884 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459351
Epoch 528
Meta loss on this task batch = 3.4622e-01, Meta loss averaged over last 500 steps = 2.9774e-01, PNorm = 136.9603, GNorm = 0.4527
Meta loss on this task batch = 2.6393e-01, Meta loss averaged over last 500 steps = 2.9770e-01, PNorm = 136.9636, GNorm = 0.2586
Meta loss on this task batch = 3.3621e-01, Meta loss averaged over last 500 steps = 2.9777e-01, PNorm = 136.9660, GNorm = 0.3089
Meta loss on this task batch = 2.7485e-01, Meta loss averaged over last 500 steps = 2.9774e-01, PNorm = 136.9680, GNorm = 0.2292
Meta loss on this task batch = 2.8550e-01, Meta loss averaged over last 500 steps = 2.9770e-01, PNorm = 136.9710, GNorm = 0.2562
Meta loss on this task batch = 3.2351e-01, Meta loss averaged over last 500 steps = 2.9790e-01, PNorm = 136.9739, GNorm = 0.3488
Meta loss on this task batch = 2.6597e-01, Meta loss averaged over last 500 steps = 2.9773e-01, PNorm = 136.9772, GNorm = 0.2456
Meta loss on this task batch = 2.6540e-01, Meta loss averaged over last 500 steps = 2.9753e-01, PNorm = 136.9806, GNorm = 0.2603
Meta loss on this task batch = 2.4456e-01, Meta loss averaged over last 500 steps = 2.9744e-01, PNorm = 136.9849, GNorm = 0.2406
Meta loss on this task batch = 3.0333e-01, Meta loss averaged over last 500 steps = 2.9748e-01, PNorm = 136.9893, GNorm = 0.4084
Meta loss on this task batch = 2.8100e-01, Meta loss averaged over last 500 steps = 2.9742e-01, PNorm = 136.9937, GNorm = 0.3131
Meta loss on this task batch = 2.9050e-01, Meta loss averaged over last 500 steps = 2.9740e-01, PNorm = 136.9983, GNorm = 0.3118
Meta loss on this task batch = 3.2724e-01, Meta loss averaged over last 500 steps = 2.9743e-01, PNorm = 137.0024, GNorm = 0.3106
Meta loss on this task batch = 3.2494e-01, Meta loss averaged over last 500 steps = 2.9747e-01, PNorm = 137.0060, GNorm = 0.3051
Meta loss on this task batch = 3.5168e-01, Meta loss averaged over last 500 steps = 2.9764e-01, PNorm = 137.0102, GNorm = 0.3043
Meta loss on this task batch = 2.7307e-01, Meta loss averaged over last 500 steps = 2.9764e-01, PNorm = 137.0144, GNorm = 0.2834
Meta loss on this task batch = 3.3500e-01, Meta loss averaged over last 500 steps = 2.9770e-01, PNorm = 137.0194, GNorm = 0.2722
Meta loss on this task batch = 2.5497e-01, Meta loss averaged over last 500 steps = 2.9758e-01, PNorm = 137.0240, GNorm = 0.4039
Meta loss on this task batch = 2.7977e-01, Meta loss averaged over last 500 steps = 2.9760e-01, PNorm = 137.0293, GNorm = 0.3809
Took 112.95168590545654 seconds to complete one epoch of meta training
Took 120.92915797233582 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464861
Epoch 529
Meta loss on this task batch = 3.0822e-01, Meta loss averaged over last 500 steps = 2.9760e-01, PNorm = 137.0351, GNorm = 0.3577
Meta loss on this task batch = 2.7332e-01, Meta loss averaged over last 500 steps = 2.9761e-01, PNorm = 137.0406, GNorm = 0.3039
Meta loss on this task batch = 2.3461e-01, Meta loss averaged over last 500 steps = 2.9739e-01, PNorm = 137.0463, GNorm = 0.2603
Meta loss on this task batch = 2.7949e-01, Meta loss averaged over last 500 steps = 2.9743e-01, PNorm = 137.0520, GNorm = 0.2376
Meta loss on this task batch = 3.8074e-01, Meta loss averaged over last 500 steps = 2.9766e-01, PNorm = 137.0550, GNorm = 0.5199
Meta loss on this task batch = 3.0130e-01, Meta loss averaged over last 500 steps = 2.9763e-01, PNorm = 137.0583, GNorm = 0.3382
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.9755e-01, PNorm = 137.0611, GNorm = 0.2883
Meta loss on this task batch = 2.1078e-01, Meta loss averaged over last 500 steps = 2.9747e-01, PNorm = 137.0643, GNorm = 0.2489
Meta loss on this task batch = 3.4611e-01, Meta loss averaged over last 500 steps = 2.9762e-01, PNorm = 137.0671, GNorm = 0.3133
Meta loss on this task batch = 3.2298e-01, Meta loss averaged over last 500 steps = 2.9757e-01, PNorm = 137.0710, GNorm = 0.3969
Meta loss on this task batch = 3.0439e-01, Meta loss averaged over last 500 steps = 2.9757e-01, PNorm = 137.0748, GNorm = 0.2965
Meta loss on this task batch = 2.8943e-01, Meta loss averaged over last 500 steps = 2.9754e-01, PNorm = 137.0794, GNorm = 0.4579
Meta loss on this task batch = 2.7322e-01, Meta loss averaged over last 500 steps = 2.9747e-01, PNorm = 137.0836, GNorm = 0.5011
Meta loss on this task batch = 2.9837e-01, Meta loss averaged over last 500 steps = 2.9754e-01, PNorm = 137.0874, GNorm = 0.3249
Meta loss on this task batch = 3.2655e-01, Meta loss averaged over last 500 steps = 2.9758e-01, PNorm = 137.0918, GNorm = 0.4933
Meta loss on this task batch = 3.7159e-01, Meta loss averaged over last 500 steps = 2.9769e-01, PNorm = 137.0961, GNorm = 0.5479
Meta loss on this task batch = 2.8576e-01, Meta loss averaged over last 500 steps = 2.9770e-01, PNorm = 137.1006, GNorm = 0.2627
Meta loss on this task batch = 2.9283e-01, Meta loss averaged over last 500 steps = 2.9767e-01, PNorm = 137.1062, GNorm = 0.3312
Meta loss on this task batch = 3.0822e-01, Meta loss averaged over last 500 steps = 2.9764e-01, PNorm = 137.1117, GNorm = 0.3914
Took 149.85103726387024 seconds to complete one epoch of meta training
Took 157.75903511047363 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450434
Epoch 530
Meta loss on this task batch = 3.2312e-01, Meta loss averaged over last 500 steps = 2.9756e-01, PNorm = 137.1175, GNorm = 0.3632
Meta loss on this task batch = 3.1873e-01, Meta loss averaged over last 500 steps = 2.9759e-01, PNorm = 137.1225, GNorm = 0.3704
Meta loss on this task batch = 2.9745e-01, Meta loss averaged over last 500 steps = 2.9749e-01, PNorm = 137.1272, GNorm = 0.2446
Meta loss on this task batch = 3.5879e-01, Meta loss averaged over last 500 steps = 2.9769e-01, PNorm = 137.1318, GNorm = 0.2835
Meta loss on this task batch = 3.6057e-01, Meta loss averaged over last 500 steps = 2.9775e-01, PNorm = 137.1361, GNorm = 0.3410
Meta loss on this task batch = 3.1307e-01, Meta loss averaged over last 500 steps = 2.9780e-01, PNorm = 137.1408, GNorm = 0.3215
Meta loss on this task batch = 3.7954e-01, Meta loss averaged over last 500 steps = 2.9787e-01, PNorm = 137.1440, GNorm = 0.2822
Meta loss on this task batch = 2.9039e-01, Meta loss averaged over last 500 steps = 2.9789e-01, PNorm = 137.1473, GNorm = 0.3456
Meta loss on this task batch = 3.3272e-01, Meta loss averaged over last 500 steps = 2.9788e-01, PNorm = 137.1506, GNorm = 0.3124
Meta loss on this task batch = 2.8165e-01, Meta loss averaged over last 500 steps = 2.9786e-01, PNorm = 137.1545, GNorm = 0.2555
Meta loss on this task batch = 2.8663e-01, Meta loss averaged over last 500 steps = 2.9788e-01, PNorm = 137.1583, GNorm = 0.3114
Meta loss on this task batch = 2.6888e-01, Meta loss averaged over last 500 steps = 2.9777e-01, PNorm = 137.1633, GNorm = 0.3092
Meta loss on this task batch = 3.2904e-01, Meta loss averaged over last 500 steps = 2.9780e-01, PNorm = 137.1688, GNorm = 0.3136
Meta loss on this task batch = 2.9343e-01, Meta loss averaged over last 500 steps = 2.9771e-01, PNorm = 137.1757, GNorm = 0.2902
Meta loss on this task batch = 2.5080e-01, Meta loss averaged over last 500 steps = 2.9758e-01, PNorm = 137.1839, GNorm = 0.4311
Meta loss on this task batch = 3.7555e-01, Meta loss averaged over last 500 steps = 2.9781e-01, PNorm = 137.1907, GNorm = 0.4329
Meta loss on this task batch = 2.8277e-01, Meta loss averaged over last 500 steps = 2.9780e-01, PNorm = 137.1973, GNorm = 0.2747
Meta loss on this task batch = 3.6238e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 137.2029, GNorm = 0.3781
Meta loss on this task batch = 3.0117e-01, Meta loss averaged over last 500 steps = 2.9796e-01, PNorm = 137.2073, GNorm = 0.5910
Took 112.50598168373108 seconds to complete one epoch of meta training
Took 120.40641951560974 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470153
Epoch 531
Meta loss on this task batch = 2.7957e-01, Meta loss averaged over last 500 steps = 2.9784e-01, PNorm = 137.2121, GNorm = 0.2630
Meta loss on this task batch = 2.9376e-01, Meta loss averaged over last 500 steps = 2.9791e-01, PNorm = 137.2165, GNorm = 0.3180
Meta loss on this task batch = 2.4038e-01, Meta loss averaged over last 500 steps = 2.9786e-01, PNorm = 137.2211, GNorm = 0.2487
Meta loss on this task batch = 3.0002e-01, Meta loss averaged over last 500 steps = 2.9793e-01, PNorm = 137.2249, GNorm = 0.2553
Meta loss on this task batch = 2.4324e-01, Meta loss averaged over last 500 steps = 2.9792e-01, PNorm = 137.2297, GNorm = 0.4763
Meta loss on this task batch = 2.8011e-01, Meta loss averaged over last 500 steps = 2.9791e-01, PNorm = 137.2342, GNorm = 0.3446
Meta loss on this task batch = 2.7662e-01, Meta loss averaged over last 500 steps = 2.9786e-01, PNorm = 137.2394, GNorm = 0.2601
Meta loss on this task batch = 3.1158e-01, Meta loss averaged over last 500 steps = 2.9793e-01, PNorm = 137.2448, GNorm = 0.3636
Meta loss on this task batch = 2.8840e-01, Meta loss averaged over last 500 steps = 2.9793e-01, PNorm = 137.2497, GNorm = 0.2677
Meta loss on this task batch = 2.7253e-01, Meta loss averaged over last 500 steps = 2.9787e-01, PNorm = 137.2542, GNorm = 0.3525
Meta loss on this task batch = 3.0104e-01, Meta loss averaged over last 500 steps = 2.9780e-01, PNorm = 137.2585, GNorm = 0.2682
Meta loss on this task batch = 3.1172e-01, Meta loss averaged over last 500 steps = 2.9777e-01, PNorm = 137.2634, GNorm = 0.3633
Meta loss on this task batch = 3.1222e-01, Meta loss averaged over last 500 steps = 2.9780e-01, PNorm = 137.2676, GNorm = 0.3678
Meta loss on this task batch = 3.0714e-01, Meta loss averaged over last 500 steps = 2.9793e-01, PNorm = 137.2709, GNorm = 0.3433
Meta loss on this task batch = 3.1957e-01, Meta loss averaged over last 500 steps = 2.9794e-01, PNorm = 137.2738, GNorm = 0.2737
Meta loss on this task batch = 3.1337e-01, Meta loss averaged over last 500 steps = 2.9793e-01, PNorm = 137.2770, GNorm = 0.2814
Meta loss on this task batch = 3.1148e-01, Meta loss averaged over last 500 steps = 2.9793e-01, PNorm = 137.2804, GNorm = 0.3448
Meta loss on this task batch = 3.3878e-01, Meta loss averaged over last 500 steps = 2.9802e-01, PNorm = 137.2833, GNorm = 0.2583
Meta loss on this task batch = 3.8942e-01, Meta loss averaged over last 500 steps = 2.9833e-01, PNorm = 137.2848, GNorm = 0.4002
Took 111.25177216529846 seconds to complete one epoch of meta training
Took 119.2328577041626 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474137
Epoch 532
Meta loss on this task batch = 2.9786e-01, Meta loss averaged over last 500 steps = 2.9831e-01, PNorm = 137.2871, GNorm = 0.3730
Meta loss on this task batch = 3.0268e-01, Meta loss averaged over last 500 steps = 2.9828e-01, PNorm = 137.2898, GNorm = 0.3055
Meta loss on this task batch = 3.2787e-01, Meta loss averaged over last 500 steps = 2.9836e-01, PNorm = 137.2927, GNorm = 0.4076
Meta loss on this task batch = 2.2352e-01, Meta loss averaged over last 500 steps = 2.9822e-01, PNorm = 137.2965, GNorm = 0.3122
Meta loss on this task batch = 2.8185e-01, Meta loss averaged over last 500 steps = 2.9809e-01, PNorm = 137.3006, GNorm = 0.2767
Meta loss on this task batch = 2.8975e-01, Meta loss averaged over last 500 steps = 2.9801e-01, PNorm = 137.3048, GNorm = 0.3004
Meta loss on this task batch = 2.7534e-01, Meta loss averaged over last 500 steps = 2.9802e-01, PNorm = 137.3093, GNorm = 0.3542
Meta loss on this task batch = 2.3598e-01, Meta loss averaged over last 500 steps = 2.9794e-01, PNorm = 137.3139, GNorm = 0.2704
Meta loss on this task batch = 2.9651e-01, Meta loss averaged over last 500 steps = 2.9787e-01, PNorm = 137.3182, GNorm = 0.3791
Meta loss on this task batch = 3.0642e-01, Meta loss averaged over last 500 steps = 2.9797e-01, PNorm = 137.3223, GNorm = 0.2821
Meta loss on this task batch = 3.0829e-01, Meta loss averaged over last 500 steps = 2.9797e-01, PNorm = 137.3266, GNorm = 0.2916
Meta loss on this task batch = 2.8760e-01, Meta loss averaged over last 500 steps = 2.9795e-01, PNorm = 137.3310, GNorm = 0.2928
Meta loss on this task batch = 2.7910e-01, Meta loss averaged over last 500 steps = 2.9790e-01, PNorm = 137.3356, GNorm = 0.2967
Meta loss on this task batch = 3.4042e-01, Meta loss averaged over last 500 steps = 2.9797e-01, PNorm = 137.3401, GNorm = 0.2954
Meta loss on this task batch = 2.9627e-01, Meta loss averaged over last 500 steps = 2.9802e-01, PNorm = 137.3447, GNorm = 0.2881
Meta loss on this task batch = 2.7263e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 137.3489, GNorm = 0.2667
Meta loss on this task batch = 3.5458e-01, Meta loss averaged over last 500 steps = 2.9822e-01, PNorm = 137.3531, GNorm = 0.4480
Meta loss on this task batch = 3.5509e-01, Meta loss averaged over last 500 steps = 2.9832e-01, PNorm = 137.3570, GNorm = 0.3320
Meta loss on this task batch = 2.3878e-01, Meta loss averaged over last 500 steps = 2.9820e-01, PNorm = 137.3621, GNorm = 0.2861
Took 115.93155336380005 seconds to complete one epoch of meta training
Took 124.042973279953 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455568
Epoch 533
Meta loss on this task batch = 3.4617e-01, Meta loss averaged over last 500 steps = 2.9824e-01, PNorm = 137.3660, GNorm = 0.3063
Meta loss on this task batch = 3.0549e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 137.3705, GNorm = 0.3803
Meta loss on this task batch = 2.7572e-01, Meta loss averaged over last 500 steps = 2.9803e-01, PNorm = 137.3755, GNorm = 0.2220
Meta loss on this task batch = 3.4019e-01, Meta loss averaged over last 500 steps = 2.9807e-01, PNorm = 137.3805, GNorm = 0.2391
Meta loss on this task batch = 3.0262e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 137.3849, GNorm = 0.2766
Meta loss on this task batch = 2.7972e-01, Meta loss averaged over last 500 steps = 2.9801e-01, PNorm = 137.3892, GNorm = 0.2628
Meta loss on this task batch = 2.8080e-01, Meta loss averaged over last 500 steps = 2.9800e-01, PNorm = 137.3938, GNorm = 0.2449
Meta loss on this task batch = 2.9926e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 137.3978, GNorm = 0.2683
Meta loss on this task batch = 2.7096e-01, Meta loss averaged over last 500 steps = 2.9803e-01, PNorm = 137.4027, GNorm = 0.2918
Meta loss on this task batch = 3.0512e-01, Meta loss averaged over last 500 steps = 2.9813e-01, PNorm = 137.4075, GNorm = 0.2629
Meta loss on this task batch = 3.0839e-01, Meta loss averaged over last 500 steps = 2.9805e-01, PNorm = 137.4121, GNorm = 0.2916
Meta loss on this task batch = 2.8490e-01, Meta loss averaged over last 500 steps = 2.9801e-01, PNorm = 137.4169, GNorm = 0.2696
Meta loss on this task batch = 2.5063e-01, Meta loss averaged over last 500 steps = 2.9790e-01, PNorm = 137.4223, GNorm = 0.2676
Meta loss on this task batch = 3.1795e-01, Meta loss averaged over last 500 steps = 2.9803e-01, PNorm = 137.4270, GNorm = 0.2490
Meta loss on this task batch = 2.7457e-01, Meta loss averaged over last 500 steps = 2.9801e-01, PNorm = 137.4317, GNorm = 0.2702
Meta loss on this task batch = 2.4646e-01, Meta loss averaged over last 500 steps = 2.9783e-01, PNorm = 137.4362, GNorm = 0.2362
Meta loss on this task batch = 3.5318e-01, Meta loss averaged over last 500 steps = 2.9794e-01, PNorm = 137.4396, GNorm = 0.3109
Meta loss on this task batch = 2.7548e-01, Meta loss averaged over last 500 steps = 2.9788e-01, PNorm = 137.4428, GNorm = 0.3441
Meta loss on this task batch = 2.8079e-01, Meta loss averaged over last 500 steps = 2.9783e-01, PNorm = 137.4455, GNorm = 0.2935
Took 113.2503273487091 seconds to complete one epoch of meta training
Took 121.32526731491089 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461924
Epoch 534
Meta loss on this task batch = 3.0769e-01, Meta loss averaged over last 500 steps = 2.9790e-01, PNorm = 137.4488, GNorm = 0.2450
Meta loss on this task batch = 3.4246e-01, Meta loss averaged over last 500 steps = 2.9801e-01, PNorm = 137.4519, GNorm = 0.2950
Meta loss on this task batch = 2.8508e-01, Meta loss averaged over last 500 steps = 2.9791e-01, PNorm = 137.4554, GNorm = 0.2793
Meta loss on this task batch = 2.4118e-01, Meta loss averaged over last 500 steps = 2.9763e-01, PNorm = 137.4591, GNorm = 0.2629
Meta loss on this task batch = 3.0779e-01, Meta loss averaged over last 500 steps = 2.9779e-01, PNorm = 137.4636, GNorm = 0.3058
Meta loss on this task batch = 3.3654e-01, Meta loss averaged over last 500 steps = 2.9792e-01, PNorm = 137.4680, GNorm = 0.3136
Meta loss on this task batch = 2.8309e-01, Meta loss averaged over last 500 steps = 2.9784e-01, PNorm = 137.4728, GNorm = 0.2486
Meta loss on this task batch = 2.7522e-01, Meta loss averaged over last 500 steps = 2.9784e-01, PNorm = 137.4780, GNorm = 0.3133
Meta loss on this task batch = 2.8071e-01, Meta loss averaged over last 500 steps = 2.9778e-01, PNorm = 137.4830, GNorm = 0.2617
Meta loss on this task batch = 2.7327e-01, Meta loss averaged over last 500 steps = 2.9784e-01, PNorm = 137.4877, GNorm = 0.3236
Meta loss on this task batch = 3.4096e-01, Meta loss averaged over last 500 steps = 2.9793e-01, PNorm = 137.4921, GNorm = 0.4497
Meta loss on this task batch = 2.8116e-01, Meta loss averaged over last 500 steps = 2.9792e-01, PNorm = 137.4966, GNorm = 0.3111
Meta loss on this task batch = 3.7549e-01, Meta loss averaged over last 500 steps = 2.9799e-01, PNorm = 137.5016, GNorm = 0.3391
Meta loss on this task batch = 2.3603e-01, Meta loss averaged over last 500 steps = 2.9790e-01, PNorm = 137.5070, GNorm = 0.2160
Meta loss on this task batch = 3.4379e-01, Meta loss averaged over last 500 steps = 2.9806e-01, PNorm = 137.5124, GNorm = 0.3658
Meta loss on this task batch = 2.7989e-01, Meta loss averaged over last 500 steps = 2.9805e-01, PNorm = 137.5179, GNorm = 0.2811
Meta loss on this task batch = 2.5747e-01, Meta loss averaged over last 500 steps = 2.9784e-01, PNorm = 137.5237, GNorm = 0.3592
Meta loss on this task batch = 2.8750e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 137.5297, GNorm = 0.2812
Meta loss on this task batch = 3.4336e-01, Meta loss averaged over last 500 steps = 2.9804e-01, PNorm = 137.5352, GNorm = 0.3713
Took 116.27920079231262 seconds to complete one epoch of meta training
Took 124.60855555534363 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455905
Epoch 535
Meta loss on this task batch = 3.1750e-01, Meta loss averaged over last 500 steps = 2.9801e-01, PNorm = 137.5408, GNorm = 0.2556
Meta loss on this task batch = 2.7198e-01, Meta loss averaged over last 500 steps = 2.9793e-01, PNorm = 137.5461, GNorm = 0.2920
Meta loss on this task batch = 3.0882e-01, Meta loss averaged over last 500 steps = 2.9792e-01, PNorm = 137.5511, GNorm = 0.2931
Meta loss on this task batch = 2.7819e-01, Meta loss averaged over last 500 steps = 2.9789e-01, PNorm = 137.5565, GNorm = 0.3515
Meta loss on this task batch = 3.2620e-01, Meta loss averaged over last 500 steps = 2.9787e-01, PNorm = 137.5619, GNorm = 0.3163
Meta loss on this task batch = 2.9496e-01, Meta loss averaged over last 500 steps = 2.9772e-01, PNorm = 137.5676, GNorm = 0.2547
Meta loss on this task batch = 3.0259e-01, Meta loss averaged over last 500 steps = 2.9772e-01, PNorm = 137.5738, GNorm = 0.2842
Meta loss on this task batch = 2.5822e-01, Meta loss averaged over last 500 steps = 2.9767e-01, PNorm = 137.5799, GNorm = 0.2176
Meta loss on this task batch = 2.9630e-01, Meta loss averaged over last 500 steps = 2.9774e-01, PNorm = 137.5854, GNorm = 0.3976
Meta loss on this task batch = 2.8911e-01, Meta loss averaged over last 500 steps = 2.9765e-01, PNorm = 137.5908, GNorm = 0.2406
Meta loss on this task batch = 2.8998e-01, Meta loss averaged over last 500 steps = 2.9774e-01, PNorm = 137.5969, GNorm = 0.3011
Meta loss on this task batch = 2.6004e-01, Meta loss averaged over last 500 steps = 2.9766e-01, PNorm = 137.6028, GNorm = 0.2669
Meta loss on this task batch = 2.9466e-01, Meta loss averaged over last 500 steps = 2.9765e-01, PNorm = 137.6077, GNorm = 0.3129
Meta loss on this task batch = 2.5395e-01, Meta loss averaged over last 500 steps = 2.9767e-01, PNorm = 137.6128, GNorm = 0.2346
Meta loss on this task batch = 2.8961e-01, Meta loss averaged over last 500 steps = 2.9759e-01, PNorm = 137.6183, GNorm = 0.4015
Meta loss on this task batch = 2.9286e-01, Meta loss averaged over last 500 steps = 2.9756e-01, PNorm = 137.6233, GNorm = 0.3234
Meta loss on this task batch = 2.7707e-01, Meta loss averaged over last 500 steps = 2.9750e-01, PNorm = 137.6284, GNorm = 0.4276
Meta loss on this task batch = 2.6647e-01, Meta loss averaged over last 500 steps = 2.9739e-01, PNorm = 137.6324, GNorm = 0.3754
Meta loss on this task batch = 2.9134e-01, Meta loss averaged over last 500 steps = 2.9736e-01, PNorm = 137.6367, GNorm = 0.4411
Took 124.81894564628601 seconds to complete one epoch of meta training
Took 132.06253361701965 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465081
Epoch 536
Meta loss on this task batch = 2.2701e-01, Meta loss averaged over last 500 steps = 2.9725e-01, PNorm = 137.6412, GNorm = 0.2378
Meta loss on this task batch = 3.0309e-01, Meta loss averaged over last 500 steps = 2.9722e-01, PNorm = 137.6462, GNorm = 0.3191
Meta loss on this task batch = 3.0559e-01, Meta loss averaged over last 500 steps = 2.9732e-01, PNorm = 137.6512, GNorm = 0.2908
Meta loss on this task batch = 3.0503e-01, Meta loss averaged over last 500 steps = 2.9736e-01, PNorm = 137.6558, GNorm = 0.3387
Meta loss on this task batch = 2.3946e-01, Meta loss averaged over last 500 steps = 2.9722e-01, PNorm = 137.6603, GNorm = 0.2902
Meta loss on this task batch = 3.1504e-01, Meta loss averaged over last 500 steps = 2.9726e-01, PNorm = 137.6642, GNorm = 0.3026
Meta loss on this task batch = 2.4064e-01, Meta loss averaged over last 500 steps = 2.9714e-01, PNorm = 137.6684, GNorm = 0.2527
Meta loss on this task batch = 3.4000e-01, Meta loss averaged over last 500 steps = 2.9724e-01, PNorm = 137.6720, GNorm = 0.3920
Meta loss on this task batch = 2.7942e-01, Meta loss averaged over last 500 steps = 2.9723e-01, PNorm = 137.6759, GNorm = 0.3270
Meta loss on this task batch = 3.0674e-01, Meta loss averaged over last 500 steps = 2.9735e-01, PNorm = 137.6805, GNorm = 0.3245
Meta loss on this task batch = 3.1954e-01, Meta loss averaged over last 500 steps = 2.9733e-01, PNorm = 137.6854, GNorm = 0.2665
Meta loss on this task batch = 3.0115e-01, Meta loss averaged over last 500 steps = 2.9724e-01, PNorm = 137.6897, GNorm = 0.3710
Meta loss on this task batch = 3.0359e-01, Meta loss averaged over last 500 steps = 2.9728e-01, PNorm = 137.6942, GNorm = 0.2533
Meta loss on this task batch = 3.2746e-01, Meta loss averaged over last 500 steps = 2.9734e-01, PNorm = 137.6986, GNorm = 0.3400
Meta loss on this task batch = 2.8322e-01, Meta loss averaged over last 500 steps = 2.9736e-01, PNorm = 137.7029, GNorm = 0.2740
Meta loss on this task batch = 3.1353e-01, Meta loss averaged over last 500 steps = 2.9736e-01, PNorm = 137.7076, GNorm = 0.2447
Meta loss on this task batch = 3.4211e-01, Meta loss averaged over last 500 steps = 2.9753e-01, PNorm = 137.7120, GNorm = 0.3528
Meta loss on this task batch = 3.1572e-01, Meta loss averaged over last 500 steps = 2.9754e-01, PNorm = 137.7168, GNorm = 0.2956
Meta loss on this task batch = 2.3684e-01, Meta loss averaged over last 500 steps = 2.9749e-01, PNorm = 137.7222, GNorm = 0.3228
Took 114.95264744758606 seconds to complete one epoch of meta training
Took 122.66246461868286 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457465
Epoch 537
Meta loss on this task batch = 2.8357e-01, Meta loss averaged over last 500 steps = 2.9732e-01, PNorm = 137.7277, GNorm = 0.2876
Meta loss on this task batch = 2.8840e-01, Meta loss averaged over last 500 steps = 2.9736e-01, PNorm = 137.7334, GNorm = 0.2911
Meta loss on this task batch = 2.9715e-01, Meta loss averaged over last 500 steps = 2.9743e-01, PNorm = 137.7383, GNorm = 0.2713
Meta loss on this task batch = 2.5676e-01, Meta loss averaged over last 500 steps = 2.9739e-01, PNorm = 137.7442, GNorm = 0.2452
Meta loss on this task batch = 2.6113e-01, Meta loss averaged over last 500 steps = 2.9731e-01, PNorm = 137.7494, GNorm = 0.2743
Meta loss on this task batch = 3.4157e-01, Meta loss averaged over last 500 steps = 2.9737e-01, PNorm = 137.7543, GNorm = 0.3839
Meta loss on this task batch = 3.3343e-01, Meta loss averaged over last 500 steps = 2.9758e-01, PNorm = 137.7590, GNorm = 0.5857
Meta loss on this task batch = 2.3687e-01, Meta loss averaged over last 500 steps = 2.9748e-01, PNorm = 137.7642, GNorm = 0.3012
Meta loss on this task batch = 2.5620e-01, Meta loss averaged over last 500 steps = 2.9729e-01, PNorm = 137.7695, GNorm = 0.3668
Meta loss on this task batch = 3.2872e-01, Meta loss averaged over last 500 steps = 2.9733e-01, PNorm = 137.7751, GNorm = 0.2922
Meta loss on this task batch = 3.3010e-01, Meta loss averaged over last 500 steps = 2.9739e-01, PNorm = 137.7805, GNorm = 0.2411
Meta loss on this task batch = 3.2960e-01, Meta loss averaged over last 500 steps = 2.9738e-01, PNorm = 137.7865, GNorm = 0.2442
Meta loss on this task batch = 2.8004e-01, Meta loss averaged over last 500 steps = 2.9744e-01, PNorm = 137.7925, GNorm = 0.2752
Meta loss on this task batch = 2.8426e-01, Meta loss averaged over last 500 steps = 2.9746e-01, PNorm = 137.7987, GNorm = 0.3042
Meta loss on this task batch = 2.8007e-01, Meta loss averaged over last 500 steps = 2.9736e-01, PNorm = 137.8056, GNorm = 0.2536
Meta loss on this task batch = 2.8665e-01, Meta loss averaged over last 500 steps = 2.9734e-01, PNorm = 137.8119, GNorm = 0.2911
Meta loss on this task batch = 3.1338e-01, Meta loss averaged over last 500 steps = 2.9728e-01, PNorm = 137.8179, GNorm = 0.2808
Meta loss on this task batch = 2.7797e-01, Meta loss averaged over last 500 steps = 2.9709e-01, PNorm = 137.8249, GNorm = 0.2843
Meta loss on this task batch = 3.0208e-01, Meta loss averaged over last 500 steps = 2.9690e-01, PNorm = 137.8324, GNorm = 0.4973
Took 113.20380282402039 seconds to complete one epoch of meta training
Took 120.9252119064331 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.423110
Epoch 538
Meta loss on this task batch = 3.5192e-01, Meta loss averaged over last 500 steps = 2.9705e-01, PNorm = 137.8386, GNorm = 0.3022
Meta loss on this task batch = 2.2583e-01, Meta loss averaged over last 500 steps = 2.9697e-01, PNorm = 137.8457, GNorm = 0.2397
Meta loss on this task batch = 2.6867e-01, Meta loss averaged over last 500 steps = 2.9695e-01, PNorm = 137.8521, GNorm = 0.3122
Meta loss on this task batch = 3.4529e-01, Meta loss averaged over last 500 steps = 2.9707e-01, PNorm = 137.8587, GNorm = 0.3346
Meta loss on this task batch = 2.6920e-01, Meta loss averaged over last 500 steps = 2.9699e-01, PNorm = 137.8650, GNorm = 0.2680
Meta loss on this task batch = 3.4894e-01, Meta loss averaged over last 500 steps = 2.9699e-01, PNorm = 137.8706, GNorm = 0.3518
Meta loss on this task batch = 3.3942e-01, Meta loss averaged over last 500 steps = 2.9706e-01, PNorm = 137.8745, GNorm = 0.4369
Meta loss on this task batch = 3.0591e-01, Meta loss averaged over last 500 steps = 2.9698e-01, PNorm = 137.8788, GNorm = 0.4841
Meta loss on this task batch = 3.1481e-01, Meta loss averaged over last 500 steps = 2.9705e-01, PNorm = 137.8831, GNorm = 0.3127
Meta loss on this task batch = 3.1101e-01, Meta loss averaged over last 500 steps = 2.9706e-01, PNorm = 137.8873, GNorm = 0.3346
Meta loss on this task batch = 3.0210e-01, Meta loss averaged over last 500 steps = 2.9709e-01, PNorm = 137.8908, GNorm = 0.2661
Meta loss on this task batch = 2.8032e-01, Meta loss averaged over last 500 steps = 2.9699e-01, PNorm = 137.8943, GNorm = 0.3541
Meta loss on this task batch = 3.0813e-01, Meta loss averaged over last 500 steps = 2.9698e-01, PNorm = 137.8981, GNorm = 0.3752
Meta loss on this task batch = 2.7017e-01, Meta loss averaged over last 500 steps = 2.9710e-01, PNorm = 137.9019, GNorm = 0.2968
Meta loss on this task batch = 2.8797e-01, Meta loss averaged over last 500 steps = 2.9696e-01, PNorm = 137.9060, GNorm = 0.2762
Meta loss on this task batch = 2.7610e-01, Meta loss averaged over last 500 steps = 2.9694e-01, PNorm = 137.9100, GNorm = 0.2348
Meta loss on this task batch = 2.7632e-01, Meta loss averaged over last 500 steps = 2.9696e-01, PNorm = 137.9142, GNorm = 0.2827
Meta loss on this task batch = 2.4247e-01, Meta loss averaged over last 500 steps = 2.9684e-01, PNorm = 137.9194, GNorm = 0.3132
Meta loss on this task batch = 3.0917e-01, Meta loss averaged over last 500 steps = 2.9682e-01, PNorm = 137.9244, GNorm = 0.3064
Took 111.8135826587677 seconds to complete one epoch of meta training
Took 119.12399554252625 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.445772
Epoch 539
Meta loss on this task batch = 2.8864e-01, Meta loss averaged over last 500 steps = 2.9679e-01, PNorm = 137.9298, GNorm = 0.3137
Meta loss on this task batch = 2.8045e-01, Meta loss averaged over last 500 steps = 2.9675e-01, PNorm = 137.9356, GNorm = 0.5138
Meta loss on this task batch = 2.4380e-01, Meta loss averaged over last 500 steps = 2.9664e-01, PNorm = 137.9415, GNorm = 0.2766
Meta loss on this task batch = 2.4409e-01, Meta loss averaged over last 500 steps = 2.9654e-01, PNorm = 137.9477, GNorm = 0.2328
Meta loss on this task batch = 2.5820e-01, Meta loss averaged over last 500 steps = 2.9636e-01, PNorm = 137.9529, GNorm = 0.2739
Meta loss on this task batch = 2.5825e-01, Meta loss averaged over last 500 steps = 2.9635e-01, PNorm = 137.9585, GNorm = 0.2783
Meta loss on this task batch = 2.9780e-01, Meta loss averaged over last 500 steps = 2.9640e-01, PNorm = 137.9647, GNorm = 0.2948
Meta loss on this task batch = 3.2842e-01, Meta loss averaged over last 500 steps = 2.9645e-01, PNorm = 137.9705, GNorm = 0.2484
Meta loss on this task batch = 2.8180e-01, Meta loss averaged over last 500 steps = 2.9657e-01, PNorm = 137.9762, GNorm = 0.2540
Meta loss on this task batch = 3.2890e-01, Meta loss averaged over last 500 steps = 2.9674e-01, PNorm = 137.9810, GNorm = 0.4640
Meta loss on this task batch = 3.0276e-01, Meta loss averaged over last 500 steps = 2.9680e-01, PNorm = 137.9857, GNorm = 0.3660
Meta loss on this task batch = 3.4216e-01, Meta loss averaged over last 500 steps = 2.9671e-01, PNorm = 137.9900, GNorm = 0.5309
Meta loss on this task batch = 3.5266e-01, Meta loss averaged over last 500 steps = 2.9677e-01, PNorm = 137.9934, GNorm = 0.2876
Meta loss on this task batch = 2.6866e-01, Meta loss averaged over last 500 steps = 2.9666e-01, PNorm = 137.9967, GNorm = 0.3019
Meta loss on this task batch = 3.0434e-01, Meta loss averaged over last 500 steps = 2.9667e-01, PNorm = 137.9994, GNorm = 0.2998
Meta loss on this task batch = 2.7638e-01, Meta loss averaged over last 500 steps = 2.9676e-01, PNorm = 138.0018, GNorm = 0.2985
Meta loss on this task batch = 3.5180e-01, Meta loss averaged over last 500 steps = 2.9686e-01, PNorm = 138.0040, GNorm = 0.3242
Meta loss on this task batch = 3.1731e-01, Meta loss averaged over last 500 steps = 2.9689e-01, PNorm = 138.0060, GNorm = 0.4783
Meta loss on this task batch = 2.9035e-01, Meta loss averaged over last 500 steps = 2.9676e-01, PNorm = 138.0088, GNorm = 0.2645
Took 111.76835036277771 seconds to complete one epoch of meta training
Took 119.82559156417847 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449613
Epoch 540
Meta loss on this task batch = 2.9778e-01, Meta loss averaged over last 500 steps = 2.9681e-01, PNorm = 138.0119, GNorm = 0.3072
Meta loss on this task batch = 3.0960e-01, Meta loss averaged over last 500 steps = 2.9679e-01, PNorm = 138.0153, GNorm = 0.5072
Meta loss on this task batch = 3.2843e-01, Meta loss averaged over last 500 steps = 2.9672e-01, PNorm = 138.0193, GNorm = 0.2714
Meta loss on this task batch = 2.6664e-01, Meta loss averaged over last 500 steps = 2.9669e-01, PNorm = 138.0238, GNorm = 0.2355
Meta loss on this task batch = 3.2839e-01, Meta loss averaged over last 500 steps = 2.9678e-01, PNorm = 138.0290, GNorm = 0.3075
Meta loss on this task batch = 2.8475e-01, Meta loss averaged over last 500 steps = 2.9667e-01, PNorm = 138.0344, GNorm = 0.2175
Meta loss on this task batch = 3.5207e-01, Meta loss averaged over last 500 steps = 2.9683e-01, PNorm = 138.0400, GNorm = 0.5670
Meta loss on this task batch = 2.6229e-01, Meta loss averaged over last 500 steps = 2.9680e-01, PNorm = 138.0461, GNorm = 0.2446
Meta loss on this task batch = 3.2003e-01, Meta loss averaged over last 500 steps = 2.9681e-01, PNorm = 138.0524, GNorm = 0.2977
Meta loss on this task batch = 3.3695e-01, Meta loss averaged over last 500 steps = 2.9689e-01, PNorm = 138.0577, GNorm = 0.3707
Meta loss on this task batch = 3.3366e-01, Meta loss averaged over last 500 steps = 2.9705e-01, PNorm = 138.0614, GNorm = 0.3914
Meta loss on this task batch = 3.0455e-01, Meta loss averaged over last 500 steps = 2.9704e-01, PNorm = 138.0657, GNorm = 0.3499
Meta loss on this task batch = 2.7907e-01, Meta loss averaged over last 500 steps = 2.9704e-01, PNorm = 138.0705, GNorm = 0.2879
Meta loss on this task batch = 2.5874e-01, Meta loss averaged over last 500 steps = 2.9693e-01, PNorm = 138.0751, GNorm = 0.3931
Meta loss on this task batch = 3.2189e-01, Meta loss averaged over last 500 steps = 2.9691e-01, PNorm = 138.0795, GNorm = 0.3264
Meta loss on this task batch = 3.2700e-01, Meta loss averaged over last 500 steps = 2.9695e-01, PNorm = 138.0836, GNorm = 0.2782
Meta loss on this task batch = 2.4870e-01, Meta loss averaged over last 500 steps = 2.9691e-01, PNorm = 138.0879, GNorm = 0.2631
Meta loss on this task batch = 2.9589e-01, Meta loss averaged over last 500 steps = 2.9692e-01, PNorm = 138.0914, GNorm = 0.3664
Meta loss on this task batch = 2.4170e-01, Meta loss averaged over last 500 steps = 2.9683e-01, PNorm = 138.0954, GNorm = 0.3928
Took 111.55975770950317 seconds to complete one epoch of meta training
Took 119.6053774356842 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486111
Epoch 541
Meta loss on this task batch = 3.3963e-01, Meta loss averaged over last 500 steps = 2.9678e-01, PNorm = 138.0988, GNorm = 0.3018
Meta loss on this task batch = 3.1744e-01, Meta loss averaged over last 500 steps = 2.9686e-01, PNorm = 138.1022, GNorm = 0.2787
Meta loss on this task batch = 2.9790e-01, Meta loss averaged over last 500 steps = 2.9679e-01, PNorm = 138.1059, GNorm = 0.3192
Meta loss on this task batch = 3.2882e-01, Meta loss averaged over last 500 steps = 2.9692e-01, PNorm = 138.1097, GNorm = 0.2717
Meta loss on this task batch = 2.8560e-01, Meta loss averaged over last 500 steps = 2.9699e-01, PNorm = 138.1140, GNorm = 0.2636
Meta loss on this task batch = 2.4210e-01, Meta loss averaged over last 500 steps = 2.9694e-01, PNorm = 138.1187, GNorm = 0.2225
Meta loss on this task batch = 2.8636e-01, Meta loss averaged over last 500 steps = 2.9689e-01, PNorm = 138.1241, GNorm = 0.2886
Meta loss on this task batch = 2.6549e-01, Meta loss averaged over last 500 steps = 2.9681e-01, PNorm = 138.1297, GNorm = 0.2475
Meta loss on this task batch = 2.7651e-01, Meta loss averaged over last 500 steps = 2.9687e-01, PNorm = 138.1363, GNorm = 0.3210
Meta loss on this task batch = 2.6059e-01, Meta loss averaged over last 500 steps = 2.9678e-01, PNorm = 138.1432, GNorm = 0.2263
Meta loss on this task batch = 3.1832e-01, Meta loss averaged over last 500 steps = 2.9694e-01, PNorm = 138.1503, GNorm = 0.3868
Meta loss on this task batch = 3.8038e-01, Meta loss averaged over last 500 steps = 2.9704e-01, PNorm = 138.1559, GNorm = 0.6403
Meta loss on this task batch = 2.9367e-01, Meta loss averaged over last 500 steps = 2.9716e-01, PNorm = 138.1614, GNorm = 0.3475
Meta loss on this task batch = 2.4860e-01, Meta loss averaged over last 500 steps = 2.9703e-01, PNorm = 138.1671, GNorm = 0.2119
Meta loss on this task batch = 2.9853e-01, Meta loss averaged over last 500 steps = 2.9705e-01, PNorm = 138.1734, GNorm = 0.3372
Meta loss on this task batch = 2.7904e-01, Meta loss averaged over last 500 steps = 2.9708e-01, PNorm = 138.1797, GNorm = 0.3162
Meta loss on this task batch = 3.0883e-01, Meta loss averaged over last 500 steps = 2.9709e-01, PNorm = 138.1854, GNorm = 0.2759
Meta loss on this task batch = 3.4340e-01, Meta loss averaged over last 500 steps = 2.9719e-01, PNorm = 138.1903, GNorm = 0.2696
Meta loss on this task batch = 2.4020e-01, Meta loss averaged over last 500 steps = 2.9707e-01, PNorm = 138.1960, GNorm = 0.2635
Took 110.18764138221741 seconds to complete one epoch of meta training
Took 117.9445104598999 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475066
Epoch 542
Meta loss on this task batch = 3.2031e-01, Meta loss averaged over last 500 steps = 2.9716e-01, PNorm = 138.2014, GNorm = 0.3309
Meta loss on this task batch = 2.3677e-01, Meta loss averaged over last 500 steps = 2.9701e-01, PNorm = 138.2074, GNorm = 0.3171
Meta loss on this task batch = 3.0619e-01, Meta loss averaged over last 500 steps = 2.9701e-01, PNorm = 138.2130, GNorm = 0.3169
Meta loss on this task batch = 2.5942e-01, Meta loss averaged over last 500 steps = 2.9687e-01, PNorm = 138.2177, GNorm = 0.3295
Meta loss on this task batch = 2.7672e-01, Meta loss averaged over last 500 steps = 2.9684e-01, PNorm = 138.2224, GNorm = 0.2766
Meta loss on this task batch = 2.6079e-01, Meta loss averaged over last 500 steps = 2.9673e-01, PNorm = 138.2275, GNorm = 0.2340
Meta loss on this task batch = 3.1218e-01, Meta loss averaged over last 500 steps = 2.9671e-01, PNorm = 138.2313, GNorm = 0.5807
Meta loss on this task batch = 2.3933e-01, Meta loss averaged over last 500 steps = 2.9662e-01, PNorm = 138.2358, GNorm = 0.3183
Meta loss on this task batch = 2.7165e-01, Meta loss averaged over last 500 steps = 2.9669e-01, PNorm = 138.2406, GNorm = 0.2726
Meta loss on this task batch = 3.4081e-01, Meta loss averaged over last 500 steps = 2.9684e-01, PNorm = 138.2452, GNorm = 0.3483
Meta loss on this task batch = 3.3533e-01, Meta loss averaged over last 500 steps = 2.9685e-01, PNorm = 138.2497, GNorm = 0.4153
Meta loss on this task batch = 3.5919e-01, Meta loss averaged over last 500 steps = 2.9699e-01, PNorm = 138.2530, GNorm = 0.3980
Meta loss on this task batch = 2.4663e-01, Meta loss averaged over last 500 steps = 2.9689e-01, PNorm = 138.2568, GNorm = 0.3532
Meta loss on this task batch = 3.5546e-01, Meta loss averaged over last 500 steps = 2.9706e-01, PNorm = 138.2605, GNorm = 0.3928
Meta loss on this task batch = 3.4523e-01, Meta loss averaged over last 500 steps = 2.9714e-01, PNorm = 138.2641, GNorm = 0.4422
Meta loss on this task batch = 3.0783e-01, Meta loss averaged over last 500 steps = 2.9717e-01, PNorm = 138.2682, GNorm = 0.4246
Meta loss on this task batch = 2.9988e-01, Meta loss averaged over last 500 steps = 2.9717e-01, PNorm = 138.2732, GNorm = 0.3637
Meta loss on this task batch = 2.7185e-01, Meta loss averaged over last 500 steps = 2.9697e-01, PNorm = 138.2785, GNorm = 0.2461
Meta loss on this task batch = 2.7667e-01, Meta loss averaged over last 500 steps = 2.9690e-01, PNorm = 138.2849, GNorm = 0.5458
Took 112.85849976539612 seconds to complete one epoch of meta training
Took 120.93106842041016 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.441265
Epoch 543
Meta loss on this task batch = 2.7431e-01, Meta loss averaged over last 500 steps = 2.9690e-01, PNorm = 138.2912, GNorm = 0.2890
Meta loss on this task batch = 2.9777e-01, Meta loss averaged over last 500 steps = 2.9682e-01, PNorm = 138.2976, GNorm = 0.2878
Meta loss on this task batch = 2.7657e-01, Meta loss averaged over last 500 steps = 2.9676e-01, PNorm = 138.3044, GNorm = 0.2579
Meta loss on this task batch = 2.2442e-01, Meta loss averaged over last 500 steps = 2.9661e-01, PNorm = 138.3103, GNorm = 0.3797
Meta loss on this task batch = 2.7326e-01, Meta loss averaged over last 500 steps = 2.9644e-01, PNorm = 138.3164, GNorm = 0.2761
Meta loss on this task batch = 2.9229e-01, Meta loss averaged over last 500 steps = 2.9648e-01, PNorm = 138.3224, GNorm = 0.2695
Meta loss on this task batch = 3.5331e-01, Meta loss averaged over last 500 steps = 2.9668e-01, PNorm = 138.3275, GNorm = 0.4642
Meta loss on this task batch = 2.9146e-01, Meta loss averaged over last 500 steps = 2.9663e-01, PNorm = 138.3322, GNorm = 0.3016
Meta loss on this task batch = 2.7905e-01, Meta loss averaged over last 500 steps = 2.9658e-01, PNorm = 138.3367, GNorm = 0.2619
Meta loss on this task batch = 2.6860e-01, Meta loss averaged over last 500 steps = 2.9653e-01, PNorm = 138.3417, GNorm = 0.2533
Meta loss on this task batch = 3.1199e-01, Meta loss averaged over last 500 steps = 2.9648e-01, PNorm = 138.3466, GNorm = 0.3003
Meta loss on this task batch = 2.9707e-01, Meta loss averaged over last 500 steps = 2.9648e-01, PNorm = 138.3512, GNorm = 0.3128
Meta loss on this task batch = 3.0662e-01, Meta loss averaged over last 500 steps = 2.9653e-01, PNorm = 138.3569, GNorm = 0.2938
Meta loss on this task batch = 3.2102e-01, Meta loss averaged over last 500 steps = 2.9663e-01, PNorm = 138.3624, GNorm = 0.4297
Meta loss on this task batch = 3.2512e-01, Meta loss averaged over last 500 steps = 2.9670e-01, PNorm = 138.3685, GNorm = 0.5414
Meta loss on this task batch = 2.9830e-01, Meta loss averaged over last 500 steps = 2.9676e-01, PNorm = 138.3755, GNorm = 0.3938
Meta loss on this task batch = 2.5100e-01, Meta loss averaged over last 500 steps = 2.9659e-01, PNorm = 138.3832, GNorm = 0.2956
Meta loss on this task batch = 3.2262e-01, Meta loss averaged over last 500 steps = 2.9669e-01, PNorm = 138.3902, GNorm = 0.3078
Meta loss on this task batch = 3.7383e-01, Meta loss averaged over last 500 steps = 2.9687e-01, PNorm = 138.3978, GNorm = 0.4153
Took 115.68413472175598 seconds to complete one epoch of meta training
Took 123.71595931053162 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452332
Epoch 544
Meta loss on this task batch = 2.4450e-01, Meta loss averaged over last 500 steps = 2.9665e-01, PNorm = 138.4063, GNorm = 0.2532
Meta loss on this task batch = 2.6093e-01, Meta loss averaged over last 500 steps = 2.9654e-01, PNorm = 138.4152, GNorm = 0.3251
Meta loss on this task batch = 2.4473e-01, Meta loss averaged over last 500 steps = 2.9636e-01, PNorm = 138.4239, GNorm = 0.2981
Meta loss on this task batch = 2.8090e-01, Meta loss averaged over last 500 steps = 2.9638e-01, PNorm = 138.4330, GNorm = 0.4272
Meta loss on this task batch = 3.1173e-01, Meta loss averaged over last 500 steps = 2.9635e-01, PNorm = 138.4418, GNorm = 0.4462
Meta loss on this task batch = 2.9361e-01, Meta loss averaged over last 500 steps = 2.9638e-01, PNorm = 138.4496, GNorm = 0.2694
Meta loss on this task batch = 3.0927e-01, Meta loss averaged over last 500 steps = 2.9648e-01, PNorm = 138.4575, GNorm = 0.2815
Meta loss on this task batch = 3.2404e-01, Meta loss averaged over last 500 steps = 2.9655e-01, PNorm = 138.4652, GNorm = 0.4264
Meta loss on this task batch = 3.2577e-01, Meta loss averaged over last 500 steps = 2.9664e-01, PNorm = 138.4721, GNorm = 0.3130
Meta loss on this task batch = 2.5771e-01, Meta loss averaged over last 500 steps = 2.9660e-01, PNorm = 138.4784, GNorm = 0.2908
Meta loss on this task batch = 3.4545e-01, Meta loss averaged over last 500 steps = 2.9679e-01, PNorm = 138.4840, GNorm = 0.3003
Meta loss on this task batch = 3.0244e-01, Meta loss averaged over last 500 steps = 2.9686e-01, PNorm = 138.4887, GNorm = 0.3009
Meta loss on this task batch = 2.9021e-01, Meta loss averaged over last 500 steps = 2.9686e-01, PNorm = 138.4929, GNorm = 0.3092
Meta loss on this task batch = 3.7879e-01, Meta loss averaged over last 500 steps = 2.9704e-01, PNorm = 138.4957, GNorm = 0.3181
Meta loss on this task batch = 2.3954e-01, Meta loss averaged over last 500 steps = 2.9692e-01, PNorm = 138.4990, GNorm = 0.2362
Meta loss on this task batch = 3.4381e-01, Meta loss averaged over last 500 steps = 2.9702e-01, PNorm = 138.5020, GNorm = 0.3227
Meta loss on this task batch = 2.6295e-01, Meta loss averaged over last 500 steps = 2.9698e-01, PNorm = 138.5053, GNorm = 0.2617
Meta loss on this task batch = 2.9633e-01, Meta loss averaged over last 500 steps = 2.9690e-01, PNorm = 138.5092, GNorm = 0.3094
Meta loss on this task batch = 3.2464e-01, Meta loss averaged over last 500 steps = 2.9681e-01, PNorm = 138.5131, GNorm = 0.3838
Took 113.98642826080322 seconds to complete one epoch of meta training
Took 121.04512572288513 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462323
Epoch 545
Meta loss on this task batch = 2.9486e-01, Meta loss averaged over last 500 steps = 2.9675e-01, PNorm = 138.5176, GNorm = 0.3605
Meta loss on this task batch = 2.9003e-01, Meta loss averaged over last 500 steps = 2.9679e-01, PNorm = 138.5220, GNorm = 0.3894
Meta loss on this task batch = 2.8890e-01, Meta loss averaged over last 500 steps = 2.9683e-01, PNorm = 138.5270, GNorm = 0.2957
Meta loss on this task batch = 3.1851e-01, Meta loss averaged over last 500 steps = 2.9686e-01, PNorm = 138.5321, GNorm = 0.3413
Meta loss on this task batch = 2.9278e-01, Meta loss averaged over last 500 steps = 2.9685e-01, PNorm = 138.5370, GNorm = 0.2816
Meta loss on this task batch = 3.2537e-01, Meta loss averaged over last 500 steps = 2.9692e-01, PNorm = 138.5420, GNorm = 0.2745
Meta loss on this task batch = 3.2335e-01, Meta loss averaged over last 500 steps = 2.9709e-01, PNorm = 138.5471, GNorm = 0.3343
Meta loss on this task batch = 2.9540e-01, Meta loss averaged over last 500 steps = 2.9705e-01, PNorm = 138.5522, GNorm = 0.2868
Meta loss on this task batch = 2.9675e-01, Meta loss averaged over last 500 steps = 2.9702e-01, PNorm = 138.5570, GNorm = 0.2582
Meta loss on this task batch = 2.7138e-01, Meta loss averaged over last 500 steps = 2.9700e-01, PNorm = 138.5618, GNorm = 0.3548
Meta loss on this task batch = 2.4222e-01, Meta loss averaged over last 500 steps = 2.9688e-01, PNorm = 138.5665, GNorm = 0.3870
Meta loss on this task batch = 2.8303e-01, Meta loss averaged over last 500 steps = 2.9696e-01, PNorm = 138.5722, GNorm = 0.2717
Meta loss on this task batch = 2.7558e-01, Meta loss averaged over last 500 steps = 2.9684e-01, PNorm = 138.5782, GNorm = 0.3363
Meta loss on this task batch = 2.9372e-01, Meta loss averaged over last 500 steps = 2.9696e-01, PNorm = 138.5842, GNorm = 0.3158
Meta loss on this task batch = 2.1371e-01, Meta loss averaged over last 500 steps = 2.9672e-01, PNorm = 138.5905, GNorm = 0.2256
Meta loss on this task batch = 3.1049e-01, Meta loss averaged over last 500 steps = 2.9676e-01, PNorm = 138.5964, GNorm = 0.4459
Meta loss on this task batch = 3.0616e-01, Meta loss averaged over last 500 steps = 2.9673e-01, PNorm = 138.6028, GNorm = 0.3267
Meta loss on this task batch = 3.3273e-01, Meta loss averaged over last 500 steps = 2.9686e-01, PNorm = 138.6083, GNorm = 0.3822
Meta loss on this task batch = 3.2099e-01, Meta loss averaged over last 500 steps = 2.9693e-01, PNorm = 138.6144, GNorm = 0.3836
Took 114.78712201118469 seconds to complete one epoch of meta training
Took 123.48374652862549 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447766
Epoch 546
Meta loss on this task batch = 3.1258e-01, Meta loss averaged over last 500 steps = 2.9689e-01, PNorm = 138.6207, GNorm = 0.2950
Meta loss on this task batch = 3.1343e-01, Meta loss averaged over last 500 steps = 2.9698e-01, PNorm = 138.6272, GNorm = 0.2769
Meta loss on this task batch = 2.8432e-01, Meta loss averaged over last 500 steps = 2.9704e-01, PNorm = 138.6349, GNorm = 0.3019
Meta loss on this task batch = 3.1387e-01, Meta loss averaged over last 500 steps = 2.9708e-01, PNorm = 138.6429, GNorm = 0.3090
Meta loss on this task batch = 2.8725e-01, Meta loss averaged over last 500 steps = 2.9712e-01, PNorm = 138.6503, GNorm = 0.2664
Meta loss on this task batch = 2.9143e-01, Meta loss averaged over last 500 steps = 2.9702e-01, PNorm = 138.6572, GNorm = 0.2534
Meta loss on this task batch = 3.1628e-01, Meta loss averaged over last 500 steps = 2.9709e-01, PNorm = 138.6636, GNorm = 0.3387
Meta loss on this task batch = 2.3664e-01, Meta loss averaged over last 500 steps = 2.9703e-01, PNorm = 138.6703, GNorm = 0.2434
Meta loss on this task batch = 2.7244e-01, Meta loss averaged over last 500 steps = 2.9697e-01, PNorm = 138.6773, GNorm = 0.3343
Meta loss on this task batch = 3.0067e-01, Meta loss averaged over last 500 steps = 2.9701e-01, PNorm = 138.6830, GNorm = 0.3498
Meta loss on this task batch = 3.2227e-01, Meta loss averaged over last 500 steps = 2.9710e-01, PNorm = 138.6877, GNorm = 0.4839
Meta loss on this task batch = 2.8070e-01, Meta loss averaged over last 500 steps = 2.9700e-01, PNorm = 138.6927, GNorm = 0.2371
Meta loss on this task batch = 2.7817e-01, Meta loss averaged over last 500 steps = 2.9691e-01, PNorm = 138.6973, GNorm = 0.2965
Meta loss on this task batch = 2.9406e-01, Meta loss averaged over last 500 steps = 2.9688e-01, PNorm = 138.7014, GNorm = 0.3357
Meta loss on this task batch = 3.4030e-01, Meta loss averaged over last 500 steps = 2.9697e-01, PNorm = 138.7054, GNorm = 0.5553
Meta loss on this task batch = 3.1134e-01, Meta loss averaged over last 500 steps = 2.9709e-01, PNorm = 138.7094, GNorm = 0.2948
Meta loss on this task batch = 2.9352e-01, Meta loss averaged over last 500 steps = 2.9703e-01, PNorm = 138.7133, GNorm = 0.2852
Meta loss on this task batch = 3.0436e-01, Meta loss averaged over last 500 steps = 2.9714e-01, PNorm = 138.7172, GNorm = 0.4219
Meta loss on this task batch = 2.9958e-01, Meta loss averaged over last 500 steps = 2.9712e-01, PNorm = 138.7214, GNorm = 0.2997
Took 114.11241269111633 seconds to complete one epoch of meta training
Took 121.90690398216248 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448115
Epoch 547
Meta loss on this task batch = 2.5642e-01, Meta loss averaged over last 500 steps = 2.9703e-01, PNorm = 138.7255, GNorm = 0.2717
Meta loss on this task batch = 2.9077e-01, Meta loss averaged over last 500 steps = 2.9681e-01, PNorm = 138.7305, GNorm = 0.2863
Meta loss on this task batch = 2.8496e-01, Meta loss averaged over last 500 steps = 2.9679e-01, PNorm = 138.7358, GNorm = 0.2382
Meta loss on this task batch = 2.7070e-01, Meta loss averaged over last 500 steps = 2.9670e-01, PNorm = 138.7410, GNorm = 0.3088
Meta loss on this task batch = 2.9302e-01, Meta loss averaged over last 500 steps = 2.9667e-01, PNorm = 138.7461, GNorm = 0.3324
Meta loss on this task batch = 3.0521e-01, Meta loss averaged over last 500 steps = 2.9676e-01, PNorm = 138.7517, GNorm = 0.2627
Meta loss on this task batch = 2.7760e-01, Meta loss averaged over last 500 steps = 2.9678e-01, PNorm = 138.7578, GNorm = 0.2600
Meta loss on this task batch = 3.2997e-01, Meta loss averaged over last 500 steps = 2.9694e-01, PNorm = 138.7633, GNorm = 0.3149
Meta loss on this task batch = 2.9690e-01, Meta loss averaged over last 500 steps = 2.9688e-01, PNorm = 138.7683, GNorm = 0.2699
Meta loss on this task batch = 3.2456e-01, Meta loss averaged over last 500 steps = 2.9694e-01, PNorm = 138.7726, GNorm = 0.3617
Meta loss on this task batch = 3.1801e-01, Meta loss averaged over last 500 steps = 2.9700e-01, PNorm = 138.7772, GNorm = 0.3429
Meta loss on this task batch = 3.1098e-01, Meta loss averaged over last 500 steps = 2.9691e-01, PNorm = 138.7816, GNorm = 0.4122
Meta loss on this task batch = 2.2884e-01, Meta loss averaged over last 500 steps = 2.9682e-01, PNorm = 138.7865, GNorm = 0.2367
Meta loss on this task batch = 2.9299e-01, Meta loss averaged over last 500 steps = 2.9680e-01, PNorm = 138.7916, GNorm = 0.2565
Meta loss on this task batch = 2.6724e-01, Meta loss averaged over last 500 steps = 2.9665e-01, PNorm = 138.7975, GNorm = 0.3123
Meta loss on this task batch = 2.3391e-01, Meta loss averaged over last 500 steps = 2.9654e-01, PNorm = 138.8041, GNorm = 0.2406
Meta loss on this task batch = 2.7135e-01, Meta loss averaged over last 500 steps = 2.9637e-01, PNorm = 138.8103, GNorm = 0.3938
Meta loss on this task batch = 3.6081e-01, Meta loss averaged over last 500 steps = 2.9639e-01, PNorm = 138.8159, GNorm = 0.3180
Meta loss on this task batch = 2.7625e-01, Meta loss averaged over last 500 steps = 2.9631e-01, PNorm = 138.8217, GNorm = 0.3823
Took 113.10369873046875 seconds to complete one epoch of meta training
Took 119.88812828063965 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465684
Epoch 548
Meta loss on this task batch = 2.9248e-01, Meta loss averaged over last 500 steps = 2.9620e-01, PNorm = 138.8275, GNorm = 0.2561
Meta loss on this task batch = 2.7681e-01, Meta loss averaged over last 500 steps = 2.9624e-01, PNorm = 138.8336, GNorm = 0.3931
Meta loss on this task batch = 2.8152e-01, Meta loss averaged over last 500 steps = 2.9623e-01, PNorm = 138.8395, GNorm = 0.3119
Meta loss on this task batch = 3.0848e-01, Meta loss averaged over last 500 steps = 2.9618e-01, PNorm = 138.8451, GNorm = 0.2815
Meta loss on this task batch = 3.5456e-01, Meta loss averaged over last 500 steps = 2.9628e-01, PNorm = 138.8495, GNorm = 0.2951
Meta loss on this task batch = 2.3472e-01, Meta loss averaged over last 500 steps = 2.9619e-01, PNorm = 138.8539, GNorm = 0.2561
Meta loss on this task batch = 2.6227e-01, Meta loss averaged over last 500 steps = 2.9616e-01, PNorm = 138.8594, GNorm = 0.3484
Meta loss on this task batch = 2.7767e-01, Meta loss averaged over last 500 steps = 2.9613e-01, PNorm = 138.8648, GNorm = 0.2794
Meta loss on this task batch = 2.3366e-01, Meta loss averaged over last 500 steps = 2.9603e-01, PNorm = 138.8701, GNorm = 0.2353
Meta loss on this task batch = 3.0474e-01, Meta loss averaged over last 500 steps = 2.9606e-01, PNorm = 138.8752, GNorm = 0.3064
Meta loss on this task batch = 3.2638e-01, Meta loss averaged over last 500 steps = 2.9634e-01, PNorm = 138.8802, GNorm = 0.2707
Meta loss on this task batch = 3.1721e-01, Meta loss averaged over last 500 steps = 2.9645e-01, PNorm = 138.8846, GNorm = 0.3318
Meta loss on this task batch = 3.6662e-01, Meta loss averaged over last 500 steps = 2.9652e-01, PNorm = 138.8889, GNorm = 0.3549
Meta loss on this task batch = 2.8968e-01, Meta loss averaged over last 500 steps = 2.9651e-01, PNorm = 138.8933, GNorm = 0.3273
Meta loss on this task batch = 2.6576e-01, Meta loss averaged over last 500 steps = 2.9640e-01, PNorm = 138.8979, GNorm = 0.2585
Meta loss on this task batch = 2.7541e-01, Meta loss averaged over last 500 steps = 2.9627e-01, PNorm = 138.9029, GNorm = 0.3428
Meta loss on this task batch = 3.1477e-01, Meta loss averaged over last 500 steps = 2.9635e-01, PNorm = 138.9080, GNorm = 0.3041
Meta loss on this task batch = 2.8264e-01, Meta loss averaged over last 500 steps = 2.9633e-01, PNorm = 138.9140, GNorm = 0.3433
Meta loss on this task batch = 3.1928e-01, Meta loss averaged over last 500 steps = 2.9636e-01, PNorm = 138.9210, GNorm = 0.3775
Took 111.8535897731781 seconds to complete one epoch of meta training
Took 119.60990595817566 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467877
Epoch 549
Meta loss on this task batch = 3.1272e-01, Meta loss averaged over last 500 steps = 2.9634e-01, PNorm = 138.9270, GNorm = 0.2595
Meta loss on this task batch = 2.9445e-01, Meta loss averaged over last 500 steps = 2.9627e-01, PNorm = 138.9333, GNorm = 0.2695
Meta loss on this task batch = 3.3078e-01, Meta loss averaged over last 500 steps = 2.9644e-01, PNorm = 138.9398, GNorm = 0.3173
Meta loss on this task batch = 2.7027e-01, Meta loss averaged over last 500 steps = 2.9643e-01, PNorm = 138.9468, GNorm = 0.2763
Meta loss on this task batch = 2.7343e-01, Meta loss averaged over last 500 steps = 2.9624e-01, PNorm = 138.9534, GNorm = 0.3915
Meta loss on this task batch = 2.8933e-01, Meta loss averaged over last 500 steps = 2.9618e-01, PNorm = 138.9599, GNorm = 0.3832
Meta loss on this task batch = 2.8320e-01, Meta loss averaged over last 500 steps = 2.9605e-01, PNorm = 138.9663, GNorm = 0.3323
Meta loss on this task batch = 2.7718e-01, Meta loss averaged over last 500 steps = 2.9594e-01, PNorm = 138.9734, GNorm = 0.3416
Meta loss on this task batch = 3.0904e-01, Meta loss averaged over last 500 steps = 2.9608e-01, PNorm = 138.9794, GNorm = 0.3029
Meta loss on this task batch = 2.8087e-01, Meta loss averaged over last 500 steps = 2.9597e-01, PNorm = 138.9853, GNorm = 0.3168
Meta loss on this task batch = 3.6251e-01, Meta loss averaged over last 500 steps = 2.9625e-01, PNorm = 138.9907, GNorm = 0.3452
Meta loss on this task batch = 3.0444e-01, Meta loss averaged over last 500 steps = 2.9631e-01, PNorm = 138.9955, GNorm = 0.2634
Meta loss on this task batch = 3.0615e-01, Meta loss averaged over last 500 steps = 2.9639e-01, PNorm = 139.0002, GNorm = 0.4103
Meta loss on this task batch = 2.8185e-01, Meta loss averaged over last 500 steps = 2.9634e-01, PNorm = 139.0059, GNorm = 0.3678
Meta loss on this task batch = 3.0617e-01, Meta loss averaged over last 500 steps = 2.9627e-01, PNorm = 139.0112, GNorm = 0.3958
Meta loss on this task batch = 2.8820e-01, Meta loss averaged over last 500 steps = 2.9615e-01, PNorm = 139.0154, GNorm = 0.3267
Meta loss on this task batch = 2.6534e-01, Meta loss averaged over last 500 steps = 2.9613e-01, PNorm = 139.0201, GNorm = 0.2808
Meta loss on this task batch = 2.4048e-01, Meta loss averaged over last 500 steps = 2.9601e-01, PNorm = 139.0249, GNorm = 0.2880
Meta loss on this task batch = 2.7951e-01, Meta loss averaged over last 500 steps = 2.9605e-01, PNorm = 139.0300, GNorm = 0.2843
Took 110.30136394500732 seconds to complete one epoch of meta training
Took 118.13443040847778 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465589
Epoch 550
Meta loss on this task batch = 2.8625e-01, Meta loss averaged over last 500 steps = 2.9603e-01, PNorm = 139.0350, GNorm = 0.2848
Meta loss on this task batch = 2.8604e-01, Meta loss averaged over last 500 steps = 2.9600e-01, PNorm = 139.0402, GNorm = 0.2889
Meta loss on this task batch = 2.9870e-01, Meta loss averaged over last 500 steps = 2.9604e-01, PNorm = 139.0454, GNorm = 0.2637
Meta loss on this task batch = 3.3147e-01, Meta loss averaged over last 500 steps = 2.9609e-01, PNorm = 139.0494, GNorm = 0.2915
Meta loss on this task batch = 3.1891e-01, Meta loss averaged over last 500 steps = 2.9614e-01, PNorm = 139.0528, GNorm = 0.3380
Meta loss on this task batch = 2.6854e-01, Meta loss averaged over last 500 steps = 2.9608e-01, PNorm = 139.0570, GNorm = 0.2521
Meta loss on this task batch = 3.8987e-01, Meta loss averaged over last 500 steps = 2.9623e-01, PNorm = 139.0598, GNorm = 0.3324
Meta loss on this task batch = 2.1558e-01, Meta loss averaged over last 500 steps = 2.9608e-01, PNorm = 139.0626, GNorm = 0.4228
Meta loss on this task batch = 2.8715e-01, Meta loss averaged over last 500 steps = 2.9609e-01, PNorm = 139.0666, GNorm = 0.4457
Meta loss on this task batch = 2.8928e-01, Meta loss averaged over last 500 steps = 2.9604e-01, PNorm = 139.0702, GNorm = 0.3026
Meta loss on this task batch = 3.0600e-01, Meta loss averaged over last 500 steps = 2.9605e-01, PNorm = 139.0743, GNorm = 0.3471
Meta loss on this task batch = 3.0932e-01, Meta loss averaged over last 500 steps = 2.9606e-01, PNorm = 139.0780, GNorm = 0.4114
Meta loss on this task batch = 2.7151e-01, Meta loss averaged over last 500 steps = 2.9605e-01, PNorm = 139.0818, GNorm = 0.3021
Meta loss on this task batch = 3.5574e-01, Meta loss averaged over last 500 steps = 2.9615e-01, PNorm = 139.0845, GNorm = 0.5049
Meta loss on this task batch = 2.4800e-01, Meta loss averaged over last 500 steps = 2.9597e-01, PNorm = 139.0877, GNorm = 0.2800
Meta loss on this task batch = 2.4343e-01, Meta loss averaged over last 500 steps = 2.9595e-01, PNorm = 139.0913, GNorm = 0.2616
Meta loss on this task batch = 2.9357e-01, Meta loss averaged over last 500 steps = 2.9593e-01, PNorm = 139.0954, GNorm = 0.2571
Meta loss on this task batch = 2.3478e-01, Meta loss averaged over last 500 steps = 2.9597e-01, PNorm = 139.1006, GNorm = 0.3296
Meta loss on this task batch = 2.8945e-01, Meta loss averaged over last 500 steps = 2.9587e-01, PNorm = 139.1057, GNorm = 0.3216
Took 113.20812582969666 seconds to complete one epoch of meta training
Took 120.86759567260742 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474371
Epoch 551
Meta loss on this task batch = 2.9324e-01, Meta loss averaged over last 500 steps = 2.9580e-01, PNorm = 139.1107, GNorm = 0.2841
Meta loss on this task batch = 2.6801e-01, Meta loss averaged over last 500 steps = 2.9568e-01, PNorm = 139.1162, GNorm = 0.3155
Meta loss on this task batch = 2.5222e-01, Meta loss averaged over last 500 steps = 2.9561e-01, PNorm = 139.1223, GNorm = 0.2966
Meta loss on this task batch = 3.3420e-01, Meta loss averaged over last 500 steps = 2.9565e-01, PNorm = 139.1279, GNorm = 0.3096
Meta loss on this task batch = 2.5992e-01, Meta loss averaged over last 500 steps = 2.9562e-01, PNorm = 139.1334, GNorm = 0.3754
Meta loss on this task batch = 3.0545e-01, Meta loss averaged over last 500 steps = 2.9569e-01, PNorm = 139.1388, GNorm = 0.2922
Meta loss on this task batch = 3.0584e-01, Meta loss averaged over last 500 steps = 2.9583e-01, PNorm = 139.1444, GNorm = 0.2926
Meta loss on this task batch = 2.9190e-01, Meta loss averaged over last 500 steps = 2.9583e-01, PNorm = 139.1501, GNorm = 0.4529
Meta loss on this task batch = 2.8417e-01, Meta loss averaged over last 500 steps = 2.9579e-01, PNorm = 139.1557, GNorm = 0.3038
Meta loss on this task batch = 3.2288e-01, Meta loss averaged over last 500 steps = 2.9582e-01, PNorm = 139.1607, GNorm = 0.2928
Meta loss on this task batch = 3.3996e-01, Meta loss averaged over last 500 steps = 2.9593e-01, PNorm = 139.1642, GNorm = 0.6312
Meta loss on this task batch = 2.8257e-01, Meta loss averaged over last 500 steps = 2.9599e-01, PNorm = 139.1678, GNorm = 0.3023
Meta loss on this task batch = 2.4050e-01, Meta loss averaged over last 500 steps = 2.9584e-01, PNorm = 139.1717, GNorm = 0.2815
Meta loss on this task batch = 3.3242e-01, Meta loss averaged over last 500 steps = 2.9595e-01, PNorm = 139.1759, GNorm = 0.3346
Meta loss on this task batch = 3.0975e-01, Meta loss averaged over last 500 steps = 2.9597e-01, PNorm = 139.1801, GNorm = 0.2497
Meta loss on this task batch = 3.7587e-01, Meta loss averaged over last 500 steps = 2.9609e-01, PNorm = 139.1833, GNorm = 0.3901
Meta loss on this task batch = 2.9872e-01, Meta loss averaged over last 500 steps = 2.9614e-01, PNorm = 139.1875, GNorm = 0.4531
Meta loss on this task batch = 2.7145e-01, Meta loss averaged over last 500 steps = 2.9607e-01, PNorm = 139.1912, GNorm = 0.3403
Meta loss on this task batch = 2.7155e-01, Meta loss averaged over last 500 steps = 2.9609e-01, PNorm = 139.1950, GNorm = 0.3674
Took 111.77901244163513 seconds to complete one epoch of meta training
Took 119.68139600753784 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484495
Epoch 552
Meta loss on this task batch = 3.3975e-01, Meta loss averaged over last 500 steps = 2.9618e-01, PNorm = 139.1992, GNorm = 0.4217
Meta loss on this task batch = 2.7196e-01, Meta loss averaged over last 500 steps = 2.9610e-01, PNorm = 139.2042, GNorm = 0.3916
Meta loss on this task batch = 3.2016e-01, Meta loss averaged over last 500 steps = 2.9599e-01, PNorm = 139.2085, GNorm = 0.4054
Meta loss on this task batch = 2.8254e-01, Meta loss averaged over last 500 steps = 2.9596e-01, PNorm = 139.2131, GNorm = 0.3908
Meta loss on this task batch = 2.7022e-01, Meta loss averaged over last 500 steps = 2.9591e-01, PNorm = 139.2178, GNorm = 0.2534
Meta loss on this task batch = 2.7965e-01, Meta loss averaged over last 500 steps = 2.9581e-01, PNorm = 139.2232, GNorm = 0.2323
Meta loss on this task batch = 3.2224e-01, Meta loss averaged over last 500 steps = 2.9579e-01, PNorm = 139.2291, GNorm = 0.3332
Meta loss on this task batch = 2.3339e-01, Meta loss averaged over last 500 steps = 2.9564e-01, PNorm = 139.2357, GNorm = 0.3390
Meta loss on this task batch = 3.4020e-01, Meta loss averaged over last 500 steps = 2.9575e-01, PNorm = 139.2424, GNorm = 0.3629
Meta loss on this task batch = 3.2872e-01, Meta loss averaged over last 500 steps = 2.9597e-01, PNorm = 139.2492, GNorm = 0.2912
Meta loss on this task batch = 2.8520e-01, Meta loss averaged over last 500 steps = 2.9593e-01, PNorm = 139.2571, GNorm = 0.2833
Meta loss on this task batch = 3.1310e-01, Meta loss averaged over last 500 steps = 2.9590e-01, PNorm = 139.2648, GNorm = 0.2769
Meta loss on this task batch = 2.4618e-01, Meta loss averaged over last 500 steps = 2.9577e-01, PNorm = 139.2733, GNorm = 0.2563
Meta loss on this task batch = 2.7564e-01, Meta loss averaged over last 500 steps = 2.9573e-01, PNorm = 139.2818, GNorm = 0.2787
Meta loss on this task batch = 2.7674e-01, Meta loss averaged over last 500 steps = 2.9561e-01, PNorm = 139.2903, GNorm = 0.2336
Meta loss on this task batch = 3.7037e-01, Meta loss averaged over last 500 steps = 2.9567e-01, PNorm = 139.2977, GNorm = 0.3300
Meta loss on this task batch = 2.9091e-01, Meta loss averaged over last 500 steps = 2.9568e-01, PNorm = 139.3047, GNorm = 0.3092
Meta loss on this task batch = 2.9053e-01, Meta loss averaged over last 500 steps = 2.9568e-01, PNorm = 139.3119, GNorm = 0.3175
Meta loss on this task batch = 3.3268e-01, Meta loss averaged over last 500 steps = 2.9578e-01, PNorm = 139.3180, GNorm = 0.3283
Took 113.08518719673157 seconds to complete one epoch of meta training
Took 119.92785096168518 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466143
Epoch 553
Meta loss on this task batch = 2.8587e-01, Meta loss averaged over last 500 steps = 2.9585e-01, PNorm = 139.3238, GNorm = 0.2525
Meta loss on this task batch = 3.0498e-01, Meta loss averaged over last 500 steps = 2.9592e-01, PNorm = 139.3304, GNorm = 0.2908
Meta loss on this task batch = 2.4774e-01, Meta loss averaged over last 500 steps = 2.9581e-01, PNorm = 139.3364, GNorm = 0.2481
Meta loss on this task batch = 3.2564e-01, Meta loss averaged over last 500 steps = 2.9584e-01, PNorm = 139.3422, GNorm = 0.3207
Meta loss on this task batch = 3.4965e-01, Meta loss averaged over last 500 steps = 2.9608e-01, PNorm = 139.3468, GNorm = 0.4194
Meta loss on this task batch = 2.5030e-01, Meta loss averaged over last 500 steps = 2.9598e-01, PNorm = 139.3516, GNorm = 0.3215
Meta loss on this task batch = 2.4684e-01, Meta loss averaged over last 500 steps = 2.9586e-01, PNorm = 139.3557, GNorm = 0.2544
Meta loss on this task batch = 3.1323e-01, Meta loss averaged over last 500 steps = 2.9588e-01, PNorm = 139.3591, GNorm = 0.3877
Meta loss on this task batch = 3.5993e-01, Meta loss averaged over last 500 steps = 2.9586e-01, PNorm = 139.3623, GNorm = 0.3293
Meta loss on this task batch = 2.8277e-01, Meta loss averaged over last 500 steps = 2.9589e-01, PNorm = 139.3657, GNorm = 0.3415
Meta loss on this task batch = 2.8851e-01, Meta loss averaged over last 500 steps = 2.9588e-01, PNorm = 139.3684, GNorm = 0.3592
Meta loss on this task batch = 3.0219e-01, Meta loss averaged over last 500 steps = 2.9598e-01, PNorm = 139.3708, GNorm = 0.3996
Meta loss on this task batch = 2.9156e-01, Meta loss averaged over last 500 steps = 2.9597e-01, PNorm = 139.3728, GNorm = 0.2363
Meta loss on this task batch = 3.5456e-01, Meta loss averaged over last 500 steps = 2.9608e-01, PNorm = 139.3756, GNorm = 0.4708
Meta loss on this task batch = 2.7769e-01, Meta loss averaged over last 500 steps = 2.9601e-01, PNorm = 139.3789, GNorm = 0.2881
Meta loss on this task batch = 2.6015e-01, Meta loss averaged over last 500 steps = 2.9608e-01, PNorm = 139.3824, GNorm = 0.2733
Meta loss on this task batch = 3.0621e-01, Meta loss averaged over last 500 steps = 2.9600e-01, PNorm = 139.3858, GNorm = 0.3521
Meta loss on this task batch = 2.8260e-01, Meta loss averaged over last 500 steps = 2.9593e-01, PNorm = 139.3889, GNorm = 0.2954
Meta loss on this task batch = 3.0599e-01, Meta loss averaged over last 500 steps = 2.9601e-01, PNorm = 139.3927, GNorm = 0.3677
Took 111.39076828956604 seconds to complete one epoch of meta training
Took 119.27635741233826 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465703
Epoch 554
Meta loss on this task batch = 2.2264e-01, Meta loss averaged over last 500 steps = 2.9590e-01, PNorm = 139.3964, GNorm = 0.3116
Meta loss on this task batch = 3.6104e-01, Meta loss averaged over last 500 steps = 2.9607e-01, PNorm = 139.3990, GNorm = 0.3332
Meta loss on this task batch = 3.0805e-01, Meta loss averaged over last 500 steps = 2.9609e-01, PNorm = 139.4015, GNorm = 0.3505
Meta loss on this task batch = 2.7758e-01, Meta loss averaged over last 500 steps = 2.9599e-01, PNorm = 139.4051, GNorm = 0.2706
Meta loss on this task batch = 2.7596e-01, Meta loss averaged over last 500 steps = 2.9606e-01, PNorm = 139.4094, GNorm = 0.3090
Meta loss on this task batch = 2.7503e-01, Meta loss averaged over last 500 steps = 2.9602e-01, PNorm = 139.4138, GNorm = 0.2515
Meta loss on this task batch = 2.8246e-01, Meta loss averaged over last 500 steps = 2.9589e-01, PNorm = 139.4179, GNorm = 0.2600
Meta loss on this task batch = 2.6891e-01, Meta loss averaged over last 500 steps = 2.9590e-01, PNorm = 139.4228, GNorm = 0.2675
Meta loss on this task batch = 2.4361e-01, Meta loss averaged over last 500 steps = 2.9572e-01, PNorm = 139.4283, GNorm = 0.2125
Meta loss on this task batch = 3.4625e-01, Meta loss averaged over last 500 steps = 2.9586e-01, PNorm = 139.4327, GNorm = 0.2778
Meta loss on this task batch = 3.2393e-01, Meta loss averaged over last 500 steps = 2.9594e-01, PNorm = 139.4368, GNorm = 0.3241
Meta loss on this task batch = 3.0198e-01, Meta loss averaged over last 500 steps = 2.9589e-01, PNorm = 139.4412, GNorm = 0.3338
Meta loss on this task batch = 3.2579e-01, Meta loss averaged over last 500 steps = 2.9601e-01, PNorm = 139.4457, GNorm = 0.3676
Meta loss on this task batch = 2.8064e-01, Meta loss averaged over last 500 steps = 2.9604e-01, PNorm = 139.4503, GNorm = 0.2781
Meta loss on this task batch = 3.0372e-01, Meta loss averaged over last 500 steps = 2.9616e-01, PNorm = 139.4546, GNorm = 0.3317
Meta loss on this task batch = 2.9975e-01, Meta loss averaged over last 500 steps = 2.9615e-01, PNorm = 139.4586, GNorm = 0.2836
Meta loss on this task batch = 2.9927e-01, Meta loss averaged over last 500 steps = 2.9619e-01, PNorm = 139.4625, GNorm = 0.2817
Meta loss on this task batch = 2.9537e-01, Meta loss averaged over last 500 steps = 2.9620e-01, PNorm = 139.4677, GNorm = 0.2858
Meta loss on this task batch = 3.4508e-01, Meta loss averaged over last 500 steps = 2.9624e-01, PNorm = 139.4721, GNorm = 0.3489
Took 109.6812105178833 seconds to complete one epoch of meta training
Took 117.42179012298584 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459625
Epoch 555
Meta loss on this task batch = 3.1636e-01, Meta loss averaged over last 500 steps = 2.9622e-01, PNorm = 139.4760, GNorm = 0.3050
Meta loss on this task batch = 2.9960e-01, Meta loss averaged over last 500 steps = 2.9611e-01, PNorm = 139.4793, GNorm = 0.2561
Meta loss on this task batch = 3.0419e-01, Meta loss averaged over last 500 steps = 2.9618e-01, PNorm = 139.4827, GNorm = 0.2480
Meta loss on this task batch = 2.9443e-01, Meta loss averaged over last 500 steps = 2.9610e-01, PNorm = 139.4865, GNorm = 0.2744
Meta loss on this task batch = 2.9324e-01, Meta loss averaged over last 500 steps = 2.9617e-01, PNorm = 139.4904, GNorm = 0.3084
Meta loss on this task batch = 2.9990e-01, Meta loss averaged over last 500 steps = 2.9621e-01, PNorm = 139.4951, GNorm = 0.3018
Meta loss on this task batch = 2.9316e-01, Meta loss averaged over last 500 steps = 2.9618e-01, PNorm = 139.5000, GNorm = 0.2905
Meta loss on this task batch = 2.7752e-01, Meta loss averaged over last 500 steps = 2.9619e-01, PNorm = 139.5058, GNorm = 0.3106
Meta loss on this task batch = 2.6374e-01, Meta loss averaged over last 500 steps = 2.9625e-01, PNorm = 139.5122, GNorm = 0.2547
Meta loss on this task batch = 2.4215e-01, Meta loss averaged over last 500 steps = 2.9617e-01, PNorm = 139.5186, GNorm = 0.3695
Meta loss on this task batch = 3.1029e-01, Meta loss averaged over last 500 steps = 2.9603e-01, PNorm = 139.5247, GNorm = 0.2708
Meta loss on this task batch = 2.4800e-01, Meta loss averaged over last 500 steps = 2.9593e-01, PNorm = 139.5305, GNorm = 0.2303
Meta loss on this task batch = 3.1485e-01, Meta loss averaged over last 500 steps = 2.9598e-01, PNorm = 139.5359, GNorm = 0.2809
Meta loss on this task batch = 2.6244e-01, Meta loss averaged over last 500 steps = 2.9609e-01, PNorm = 139.5407, GNorm = 0.2729
Meta loss on this task batch = 2.4387e-01, Meta loss averaged over last 500 steps = 2.9588e-01, PNorm = 139.5461, GNorm = 0.2737
Meta loss on this task batch = 3.3006e-01, Meta loss averaged over last 500 steps = 2.9590e-01, PNorm = 139.5509, GNorm = 0.2674
Meta loss on this task batch = 2.8544e-01, Meta loss averaged over last 500 steps = 2.9586e-01, PNorm = 139.5553, GNorm = 0.3309
Meta loss on this task batch = 3.7628e-01, Meta loss averaged over last 500 steps = 2.9603e-01, PNorm = 139.5589, GNorm = 0.2976
Meta loss on this task batch = 3.2210e-01, Meta loss averaged over last 500 steps = 2.9613e-01, PNorm = 139.5618, GNorm = 0.3771
Took 109.69291090965271 seconds to complete one epoch of meta training
Took 117.41797041893005 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.445854
Epoch 556
Meta loss on this task batch = 2.6114e-01, Meta loss averaged over last 500 steps = 2.9606e-01, PNorm = 139.5653, GNorm = 0.3058
Meta loss on this task batch = 2.7951e-01, Meta loss averaged over last 500 steps = 2.9596e-01, PNorm = 139.5688, GNorm = 0.2343
Meta loss on this task batch = 2.9876e-01, Meta loss averaged over last 500 steps = 2.9582e-01, PNorm = 139.5726, GNorm = 0.2742
Meta loss on this task batch = 3.0882e-01, Meta loss averaged over last 500 steps = 2.9586e-01, PNorm = 139.5759, GNorm = 0.4251
Meta loss on this task batch = 3.1525e-01, Meta loss averaged over last 500 steps = 2.9591e-01, PNorm = 139.5797, GNorm = 0.4210
Meta loss on this task batch = 3.0733e-01, Meta loss averaged over last 500 steps = 2.9591e-01, PNorm = 139.5833, GNorm = 0.3320
Meta loss on this task batch = 2.7556e-01, Meta loss averaged over last 500 steps = 2.9581e-01, PNorm = 139.5874, GNorm = 0.2794
Meta loss on this task batch = 3.2736e-01, Meta loss averaged over last 500 steps = 2.9583e-01, PNorm = 139.5906, GNorm = 0.2918
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 2.9575e-01, PNorm = 139.5944, GNorm = 0.2530
Meta loss on this task batch = 2.9173e-01, Meta loss averaged over last 500 steps = 2.9561e-01, PNorm = 139.5984, GNorm = 0.2548
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 2.9546e-01, PNorm = 139.6032, GNorm = 0.2999
Meta loss on this task batch = 3.1410e-01, Meta loss averaged over last 500 steps = 2.9546e-01, PNorm = 139.6076, GNorm = 0.5516
Meta loss on this task batch = 2.8597e-01, Meta loss averaged over last 500 steps = 2.9528e-01, PNorm = 139.6118, GNorm = 0.3351
Meta loss on this task batch = 2.5040e-01, Meta loss averaged over last 500 steps = 2.9520e-01, PNorm = 139.6161, GNorm = 0.3129
Meta loss on this task batch = 2.7726e-01, Meta loss averaged over last 500 steps = 2.9509e-01, PNorm = 139.6201, GNorm = 0.3657
Meta loss on this task batch = 2.6964e-01, Meta loss averaged over last 500 steps = 2.9506e-01, PNorm = 139.6245, GNorm = 0.3014
Meta loss on this task batch = 2.7801e-01, Meta loss averaged over last 500 steps = 2.9505e-01, PNorm = 139.6288, GNorm = 0.3073
Meta loss on this task batch = 2.5077e-01, Meta loss averaged over last 500 steps = 2.9501e-01, PNorm = 139.6326, GNorm = 0.2764
Meta loss on this task batch = 4.1614e-01, Meta loss averaged over last 500 steps = 2.9518e-01, PNorm = 139.6349, GNorm = 0.4714
Took 111.11039590835571 seconds to complete one epoch of meta training
Took 119.3513195514679 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467083
Epoch 557
Meta loss on this task batch = 3.3220e-01, Meta loss averaged over last 500 steps = 2.9526e-01, PNorm = 139.6374, GNorm = 0.2954
Meta loss on this task batch = 3.4462e-01, Meta loss averaged over last 500 steps = 2.9545e-01, PNorm = 139.6384, GNorm = 0.3721
Meta loss on this task batch = 2.8389e-01, Meta loss averaged over last 500 steps = 2.9527e-01, PNorm = 139.6389, GNorm = 0.3105
Meta loss on this task batch = 3.1291e-01, Meta loss averaged over last 500 steps = 2.9533e-01, PNorm = 139.6392, GNorm = 0.4128
Meta loss on this task batch = 2.9022e-01, Meta loss averaged over last 500 steps = 2.9518e-01, PNorm = 139.6395, GNorm = 0.4182
Meta loss on this task batch = 2.6983e-01, Meta loss averaged over last 500 steps = 2.9512e-01, PNorm = 139.6405, GNorm = 0.2679
Meta loss on this task batch = 2.9391e-01, Meta loss averaged over last 500 steps = 2.9515e-01, PNorm = 139.6426, GNorm = 0.2669
Meta loss on this task batch = 3.0671e-01, Meta loss averaged over last 500 steps = 2.9517e-01, PNorm = 139.6456, GNorm = 0.3915
Meta loss on this task batch = 2.8510e-01, Meta loss averaged over last 500 steps = 2.9526e-01, PNorm = 139.6492, GNorm = 0.3208
Meta loss on this task batch = 2.8428e-01, Meta loss averaged over last 500 steps = 2.9523e-01, PNorm = 139.6536, GNorm = 0.2630
Meta loss on this task batch = 2.8165e-01, Meta loss averaged over last 500 steps = 2.9531e-01, PNorm = 139.6585, GNorm = 0.2918
Meta loss on this task batch = 3.3385e-01, Meta loss averaged over last 500 steps = 2.9542e-01, PNorm = 139.6639, GNorm = 0.5431
Meta loss on this task batch = 3.1432e-01, Meta loss averaged over last 500 steps = 2.9549e-01, PNorm = 139.6691, GNorm = 0.2752
Meta loss on this task batch = 2.7562e-01, Meta loss averaged over last 500 steps = 2.9542e-01, PNorm = 139.6750, GNorm = 0.2487
Meta loss on this task batch = 2.9815e-01, Meta loss averaged over last 500 steps = 2.9544e-01, PNorm = 139.6812, GNorm = 0.2743
Meta loss on this task batch = 2.7656e-01, Meta loss averaged over last 500 steps = 2.9545e-01, PNorm = 139.6876, GNorm = 0.2337
Meta loss on this task batch = 3.1522e-01, Meta loss averaged over last 500 steps = 2.9547e-01, PNorm = 139.6937, GNorm = 0.2976
Meta loss on this task batch = 3.1166e-01, Meta loss averaged over last 500 steps = 2.9547e-01, PNorm = 139.6997, GNorm = 0.5654
Meta loss on this task batch = 2.4804e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 139.7067, GNorm = 0.3258
Took 110.55595231056213 seconds to complete one epoch of meta training
Took 118.53929686546326 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461062
Epoch 558
Meta loss on this task batch = 2.5403e-01, Meta loss averaged over last 500 steps = 2.9524e-01, PNorm = 139.7141, GNorm = 0.2783
Meta loss on this task batch = 2.7634e-01, Meta loss averaged over last 500 steps = 2.9515e-01, PNorm = 139.7214, GNorm = 0.2954
Meta loss on this task batch = 2.7569e-01, Meta loss averaged over last 500 steps = 2.9508e-01, PNorm = 139.7282, GNorm = 0.3093
Meta loss on this task batch = 3.2349e-01, Meta loss averaged over last 500 steps = 2.9510e-01, PNorm = 139.7345, GNorm = 0.2708
Meta loss on this task batch = 3.0510e-01, Meta loss averaged over last 500 steps = 2.9503e-01, PNorm = 139.7402, GNorm = 0.2821
Meta loss on this task batch = 2.8479e-01, Meta loss averaged over last 500 steps = 2.9483e-01, PNorm = 139.7461, GNorm = 0.3841
Meta loss on this task batch = 3.0019e-01, Meta loss averaged over last 500 steps = 2.9483e-01, PNorm = 139.7512, GNorm = 0.2988
Meta loss on this task batch = 3.7003e-01, Meta loss averaged over last 500 steps = 2.9496e-01, PNorm = 139.7562, GNorm = 0.3210
Meta loss on this task batch = 3.3805e-01, Meta loss averaged over last 500 steps = 2.9499e-01, PNorm = 139.7603, GNorm = 0.3999
Meta loss on this task batch = 3.1866e-01, Meta loss averaged over last 500 steps = 2.9518e-01, PNorm = 139.7651, GNorm = 0.2791
Meta loss on this task batch = 3.1522e-01, Meta loss averaged over last 500 steps = 2.9524e-01, PNorm = 139.7693, GNorm = 0.3725
Meta loss on this task batch = 2.7497e-01, Meta loss averaged over last 500 steps = 2.9521e-01, PNorm = 139.7732, GNorm = 0.2510
Meta loss on this task batch = 2.8609e-01, Meta loss averaged over last 500 steps = 2.9523e-01, PNorm = 139.7774, GNorm = 0.2484
Meta loss on this task batch = 3.3244e-01, Meta loss averaged over last 500 steps = 2.9543e-01, PNorm = 139.7814, GNorm = 0.3376
Meta loss on this task batch = 3.6215e-01, Meta loss averaged over last 500 steps = 2.9556e-01, PNorm = 139.7849, GNorm = 0.3174
Meta loss on this task batch = 3.4417e-01, Meta loss averaged over last 500 steps = 2.9563e-01, PNorm = 139.7886, GNorm = 0.3217
Meta loss on this task batch = 2.7425e-01, Meta loss averaged over last 500 steps = 2.9557e-01, PNorm = 139.7932, GNorm = 0.2486
Meta loss on this task batch = 2.9374e-01, Meta loss averaged over last 500 steps = 2.9558e-01, PNorm = 139.7980, GNorm = 0.2905
Meta loss on this task batch = 2.9495e-01, Meta loss averaged over last 500 steps = 2.9561e-01, PNorm = 139.8029, GNorm = 0.4164
Took 112.3213951587677 seconds to complete one epoch of meta training
Took 120.39571952819824 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.437620
Epoch 559
Meta loss on this task batch = 2.9617e-01, Meta loss averaged over last 500 steps = 2.9552e-01, PNorm = 139.8077, GNorm = 0.2544
Meta loss on this task batch = 2.1472e-01, Meta loss averaged over last 500 steps = 2.9536e-01, PNorm = 139.8133, GNorm = 0.2368
Meta loss on this task batch = 3.0498e-01, Meta loss averaged over last 500 steps = 2.9542e-01, PNorm = 139.8186, GNorm = 0.3253
Meta loss on this task batch = 2.2125e-01, Meta loss averaged over last 500 steps = 2.9516e-01, PNorm = 139.8252, GNorm = 0.2315
Meta loss on this task batch = 2.7735e-01, Meta loss averaged over last 500 steps = 2.9500e-01, PNorm = 139.8319, GNorm = 0.2439
Meta loss on this task batch = 2.8689e-01, Meta loss averaged over last 500 steps = 2.9510e-01, PNorm = 139.8383, GNorm = 0.2753
Meta loss on this task batch = 2.9078e-01, Meta loss averaged over last 500 steps = 2.9499e-01, PNorm = 139.8448, GNorm = 0.3082
Meta loss on this task batch = 2.9042e-01, Meta loss averaged over last 500 steps = 2.9496e-01, PNorm = 139.8511, GNorm = 0.2635
Meta loss on this task batch = 3.3233e-01, Meta loss averaged over last 500 steps = 2.9507e-01, PNorm = 139.8567, GNorm = 0.3459
Meta loss on this task batch = 2.6472e-01, Meta loss averaged over last 500 steps = 2.9492e-01, PNorm = 139.8625, GNorm = 0.2360
Meta loss on this task batch = 3.3536e-01, Meta loss averaged over last 500 steps = 2.9498e-01, PNorm = 139.8668, GNorm = 0.4395
Meta loss on this task batch = 2.9247e-01, Meta loss averaged over last 500 steps = 2.9501e-01, PNorm = 139.8710, GNorm = 0.3316
Meta loss on this task batch = 3.0385e-01, Meta loss averaged over last 500 steps = 2.9506e-01, PNorm = 139.8754, GNorm = 0.2558
Meta loss on this task batch = 3.6906e-01, Meta loss averaged over last 500 steps = 2.9519e-01, PNorm = 139.8795, GNorm = 0.3078
Meta loss on this task batch = 3.4854e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 139.8823, GNorm = 0.3302
Meta loss on this task batch = 2.8653e-01, Meta loss averaged over last 500 steps = 2.9531e-01, PNorm = 139.8852, GNorm = 0.2853
Meta loss on this task batch = 2.3384e-01, Meta loss averaged over last 500 steps = 2.9516e-01, PNorm = 139.8889, GNorm = 0.2386
Meta loss on this task batch = 3.5012e-01, Meta loss averaged over last 500 steps = 2.9529e-01, PNorm = 139.8922, GNorm = 0.3201
Meta loss on this task batch = 2.7988e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 139.8963, GNorm = 0.5175
Took 111.16146850585938 seconds to complete one epoch of meta training
Took 119.12594079971313 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453394
Epoch 560
Meta loss on this task batch = 2.8888e-01, Meta loss averaged over last 500 steps = 2.9529e-01, PNorm = 139.9011, GNorm = 0.2681
Meta loss on this task batch = 2.7750e-01, Meta loss averaged over last 500 steps = 2.9530e-01, PNorm = 139.9065, GNorm = 0.3001
Meta loss on this task batch = 2.6896e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 139.9124, GNorm = 0.2488
Meta loss on this task batch = 2.9635e-01, Meta loss averaged over last 500 steps = 2.9523e-01, PNorm = 139.9183, GNorm = 0.2449
Meta loss on this task batch = 2.8361e-01, Meta loss averaged over last 500 steps = 2.9525e-01, PNorm = 139.9245, GNorm = 0.2975
Meta loss on this task batch = 2.5772e-01, Meta loss averaged over last 500 steps = 2.9520e-01, PNorm = 139.9310, GNorm = 0.2682
Meta loss on this task batch = 3.2610e-01, Meta loss averaged over last 500 steps = 2.9524e-01, PNorm = 139.9373, GNorm = 0.2737
Meta loss on this task batch = 3.1938e-01, Meta loss averaged over last 500 steps = 2.9519e-01, PNorm = 139.9437, GNorm = 0.3031
Meta loss on this task batch = 2.9291e-01, Meta loss averaged over last 500 steps = 2.9521e-01, PNorm = 139.9506, GNorm = 0.3537
Meta loss on this task batch = 2.9435e-01, Meta loss averaged over last 500 steps = 2.9531e-01, PNorm = 139.9569, GNorm = 0.3429
Meta loss on this task batch = 2.7671e-01, Meta loss averaged over last 500 steps = 2.9525e-01, PNorm = 139.9630, GNorm = 0.3615
Meta loss on this task batch = 2.8746e-01, Meta loss averaged over last 500 steps = 2.9515e-01, PNorm = 139.9698, GNorm = 0.3593
Meta loss on this task batch = 2.8648e-01, Meta loss averaged over last 500 steps = 2.9516e-01, PNorm = 139.9765, GNorm = 0.2863
Meta loss on this task batch = 2.8656e-01, Meta loss averaged over last 500 steps = 2.9518e-01, PNorm = 139.9827, GNorm = 0.2687
Meta loss on this task batch = 3.0820e-01, Meta loss averaged over last 500 steps = 2.9524e-01, PNorm = 139.9888, GNorm = 0.3025
Meta loss on this task batch = 2.6781e-01, Meta loss averaged over last 500 steps = 2.9523e-01, PNorm = 139.9945, GNorm = 0.3434
Meta loss on this task batch = 2.8497e-01, Meta loss averaged over last 500 steps = 2.9512e-01, PNorm = 139.9996, GNorm = 0.2595
Meta loss on this task batch = 3.4792e-01, Meta loss averaged over last 500 steps = 2.9525e-01, PNorm = 140.0044, GNorm = 0.2992
Meta loss on this task batch = 2.2674e-01, Meta loss averaged over last 500 steps = 2.9495e-01, PNorm = 140.0099, GNorm = 0.2825
Took 111.7729434967041 seconds to complete one epoch of meta training
Took 118.35092258453369 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452703
Epoch 561
Meta loss on this task batch = 3.1508e-01, Meta loss averaged over last 500 steps = 2.9511e-01, PNorm = 140.0150, GNorm = 0.3034
Meta loss on this task batch = 2.8155e-01, Meta loss averaged over last 500 steps = 2.9499e-01, PNorm = 140.0196, GNorm = 0.2846
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 2.9500e-01, PNorm = 140.0246, GNorm = 0.2415
Meta loss on this task batch = 3.4298e-01, Meta loss averaged over last 500 steps = 2.9517e-01, PNorm = 140.0290, GNorm = 0.5002
Meta loss on this task batch = 2.5644e-01, Meta loss averaged over last 500 steps = 2.9510e-01, PNorm = 140.0335, GNorm = 0.2680
Meta loss on this task batch = 2.8049e-01, Meta loss averaged over last 500 steps = 2.9498e-01, PNorm = 140.0380, GNorm = 0.3545
Meta loss on this task batch = 3.4830e-01, Meta loss averaged over last 500 steps = 2.9504e-01, PNorm = 140.0421, GNorm = 0.3372
Meta loss on this task batch = 2.6248e-01, Meta loss averaged over last 500 steps = 2.9502e-01, PNorm = 140.0456, GNorm = 0.2477
Meta loss on this task batch = 2.8361e-01, Meta loss averaged over last 500 steps = 2.9497e-01, PNorm = 140.0486, GNorm = 0.2853
Meta loss on this task batch = 2.9972e-01, Meta loss averaged over last 500 steps = 2.9501e-01, PNorm = 140.0527, GNorm = 0.2928
Meta loss on this task batch = 2.9073e-01, Meta loss averaged over last 500 steps = 2.9494e-01, PNorm = 140.0566, GNorm = 0.2946
Meta loss on this task batch = 2.8080e-01, Meta loss averaged over last 500 steps = 2.9491e-01, PNorm = 140.0603, GNorm = 0.2811
Meta loss on this task batch = 2.7378e-01, Meta loss averaged over last 500 steps = 2.9486e-01, PNorm = 140.0638, GNorm = 0.2315
Meta loss on this task batch = 3.2059e-01, Meta loss averaged over last 500 steps = 2.9498e-01, PNorm = 140.0664, GNorm = 0.3888
Meta loss on this task batch = 3.1466e-01, Meta loss averaged over last 500 steps = 2.9502e-01, PNorm = 140.0687, GNorm = 0.3912
Meta loss on this task batch = 3.0249e-01, Meta loss averaged over last 500 steps = 2.9504e-01, PNorm = 140.0717, GNorm = 0.2941
Meta loss on this task batch = 3.2906e-01, Meta loss averaged over last 500 steps = 2.9512e-01, PNorm = 140.0756, GNorm = 0.3066
Meta loss on this task batch = 2.9442e-01, Meta loss averaged over last 500 steps = 2.9519e-01, PNorm = 140.0804, GNorm = 0.3070
Meta loss on this task batch = 3.0904e-01, Meta loss averaged over last 500 steps = 2.9522e-01, PNorm = 140.0859, GNorm = 0.3333
Took 111.40005707740784 seconds to complete one epoch of meta training
Took 119.50766277313232 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.445942
Epoch 562
Meta loss on this task batch = 2.9811e-01, Meta loss averaged over last 500 steps = 2.9531e-01, PNorm = 140.0916, GNorm = 0.2617
Meta loss on this task batch = 3.1006e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 140.0970, GNorm = 0.3077
Meta loss on this task batch = 3.2559e-01, Meta loss averaged over last 500 steps = 2.9542e-01, PNorm = 140.1029, GNorm = 0.3149
Meta loss on this task batch = 2.8689e-01, Meta loss averaged over last 500 steps = 2.9543e-01, PNorm = 140.1091, GNorm = 0.3013
Meta loss on this task batch = 2.8655e-01, Meta loss averaged over last 500 steps = 2.9547e-01, PNorm = 140.1157, GNorm = 0.2640
Meta loss on this task batch = 2.8645e-01, Meta loss averaged over last 500 steps = 2.9547e-01, PNorm = 140.1218, GNorm = 0.2963
Meta loss on this task batch = 3.0635e-01, Meta loss averaged over last 500 steps = 2.9562e-01, PNorm = 140.1281, GNorm = 0.2665
Meta loss on this task batch = 3.6454e-01, Meta loss averaged over last 500 steps = 2.9575e-01, PNorm = 140.1339, GNorm = 0.5404
Meta loss on this task batch = 3.1028e-01, Meta loss averaged over last 500 steps = 2.9576e-01, PNorm = 140.1403, GNorm = 0.2807
Meta loss on this task batch = 2.4155e-01, Meta loss averaged over last 500 steps = 2.9563e-01, PNorm = 140.1474, GNorm = 0.3190
Meta loss on this task batch = 3.4013e-01, Meta loss averaged over last 500 steps = 2.9583e-01, PNorm = 140.1538, GNorm = 0.2890
Meta loss on this task batch = 2.8082e-01, Meta loss averaged over last 500 steps = 2.9576e-01, PNorm = 140.1613, GNorm = 0.3079
Meta loss on this task batch = 3.0265e-01, Meta loss averaged over last 500 steps = 2.9589e-01, PNorm = 140.1689, GNorm = 0.2713
Meta loss on this task batch = 2.6177e-01, Meta loss averaged over last 500 steps = 2.9573e-01, PNorm = 140.1764, GNorm = 0.3614
Meta loss on this task batch = 2.8788e-01, Meta loss averaged over last 500 steps = 2.9575e-01, PNorm = 140.1841, GNorm = 0.3143
Meta loss on this task batch = 2.5865e-01, Meta loss averaged over last 500 steps = 2.9565e-01, PNorm = 140.1915, GNorm = 0.2514
Meta loss on this task batch = 2.9878e-01, Meta loss averaged over last 500 steps = 2.9561e-01, PNorm = 140.1991, GNorm = 0.3184
Meta loss on this task batch = 3.1580e-01, Meta loss averaged over last 500 steps = 2.9564e-01, PNorm = 140.2064, GNorm = 0.2682
Meta loss on this task batch = 2.9469e-01, Meta loss averaged over last 500 steps = 2.9562e-01, PNorm = 140.2137, GNorm = 0.3847
Took 111.76621079444885 seconds to complete one epoch of meta training
Took 119.83168077468872 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477986
Epoch 563
Meta loss on this task batch = 2.9803e-01, Meta loss averaged over last 500 steps = 2.9556e-01, PNorm = 140.2201, GNorm = 0.2539
Meta loss on this task batch = 3.2046e-01, Meta loss averaged over last 500 steps = 2.9564e-01, PNorm = 140.2261, GNorm = 0.2623
Meta loss on this task batch = 3.2438e-01, Meta loss averaged over last 500 steps = 2.9566e-01, PNorm = 140.2315, GNorm = 0.3870
Meta loss on this task batch = 2.6598e-01, Meta loss averaged over last 500 steps = 2.9551e-01, PNorm = 140.2365, GNorm = 0.3511
Meta loss on this task batch = 2.9415e-01, Meta loss averaged over last 500 steps = 2.9546e-01, PNorm = 140.2412, GNorm = 0.3249
Meta loss on this task batch = 2.9776e-01, Meta loss averaged over last 500 steps = 2.9558e-01, PNorm = 140.2451, GNorm = 0.3906
Meta loss on this task batch = 3.3443e-01, Meta loss averaged over last 500 steps = 2.9569e-01, PNorm = 140.2475, GNorm = 0.5375
Meta loss on this task batch = 2.3010e-01, Meta loss averaged over last 500 steps = 2.9557e-01, PNorm = 140.2503, GNorm = 0.2443
Meta loss on this task batch = 3.2664e-01, Meta loss averaged over last 500 steps = 2.9563e-01, PNorm = 140.2529, GNorm = 0.3889
Meta loss on this task batch = 2.6071e-01, Meta loss averaged over last 500 steps = 2.9564e-01, PNorm = 140.2560, GNorm = 0.2523
Meta loss on this task batch = 2.8297e-01, Meta loss averaged over last 500 steps = 2.9568e-01, PNorm = 140.2590, GNorm = 0.2445
Meta loss on this task batch = 2.9815e-01, Meta loss averaged over last 500 steps = 2.9559e-01, PNorm = 140.2616, GNorm = 0.3326
Meta loss on this task batch = 2.5775e-01, Meta loss averaged over last 500 steps = 2.9544e-01, PNorm = 140.2637, GNorm = 0.2163
Meta loss on this task batch = 3.3257e-01, Meta loss averaged over last 500 steps = 2.9563e-01, PNorm = 140.2665, GNorm = 0.2754
Meta loss on this task batch = 3.1059e-01, Meta loss averaged over last 500 steps = 2.9574e-01, PNorm = 140.2700, GNorm = 0.2744
Meta loss on this task batch = 3.1141e-01, Meta loss averaged over last 500 steps = 2.9571e-01, PNorm = 140.2744, GNorm = 0.2640
Meta loss on this task batch = 2.8324e-01, Meta loss averaged over last 500 steps = 2.9561e-01, PNorm = 140.2795, GNorm = 0.2708
Meta loss on this task batch = 2.7238e-01, Meta loss averaged over last 500 steps = 2.9550e-01, PNorm = 140.2840, GNorm = 0.2637
Meta loss on this task batch = 2.4077e-01, Meta loss averaged over last 500 steps = 2.9542e-01, PNorm = 140.2899, GNorm = 0.2868
Took 110.10518288612366 seconds to complete one epoch of meta training
Took 117.50869131088257 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447172
Epoch 564
Meta loss on this task batch = 2.6379e-01, Meta loss averaged over last 500 steps = 2.9538e-01, PNorm = 140.2961, GNorm = 0.2400
Meta loss on this task batch = 3.3788e-01, Meta loss averaged over last 500 steps = 2.9550e-01, PNorm = 140.3016, GNorm = 0.3400
Meta loss on this task batch = 2.8713e-01, Meta loss averaged over last 500 steps = 2.9550e-01, PNorm = 140.3080, GNorm = 0.2429
Meta loss on this task batch = 3.1241e-01, Meta loss averaged over last 500 steps = 2.9549e-01, PNorm = 140.3143, GNorm = 0.3298
Meta loss on this task batch = 2.6841e-01, Meta loss averaged over last 500 steps = 2.9548e-01, PNorm = 140.3206, GNorm = 0.3049
Meta loss on this task batch = 2.8167e-01, Meta loss averaged over last 500 steps = 2.9543e-01, PNorm = 140.3266, GNorm = 0.2561
Meta loss on this task batch = 3.2796e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 140.3323, GNorm = 0.3117
Meta loss on this task batch = 3.2998e-01, Meta loss averaged over last 500 steps = 2.9559e-01, PNorm = 140.3376, GNorm = 0.2940
Meta loss on this task batch = 3.0366e-01, Meta loss averaged over last 500 steps = 2.9566e-01, PNorm = 140.3431, GNorm = 0.3379
Meta loss on this task batch = 3.3404e-01, Meta loss averaged over last 500 steps = 2.9564e-01, PNorm = 140.3487, GNorm = 0.2934
Meta loss on this task batch = 2.6712e-01, Meta loss averaged over last 500 steps = 2.9564e-01, PNorm = 140.3545, GNorm = 0.2526
Meta loss on this task batch = 2.9689e-01, Meta loss averaged over last 500 steps = 2.9553e-01, PNorm = 140.3597, GNorm = 0.2265
Meta loss on this task batch = 3.2110e-01, Meta loss averaged over last 500 steps = 2.9550e-01, PNorm = 140.3653, GNorm = 0.2903
Meta loss on this task batch = 2.9319e-01, Meta loss averaged over last 500 steps = 2.9547e-01, PNorm = 140.3703, GNorm = 0.3525
Meta loss on this task batch = 3.1138e-01, Meta loss averaged over last 500 steps = 2.9546e-01, PNorm = 140.3755, GNorm = 0.2559
Meta loss on this task batch = 2.8453e-01, Meta loss averaged over last 500 steps = 2.9541e-01, PNorm = 140.3802, GNorm = 0.2824
Meta loss on this task batch = 2.9295e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 140.3848, GNorm = 0.2541
Meta loss on this task batch = 2.8829e-01, Meta loss averaged over last 500 steps = 2.9541e-01, PNorm = 140.3895, GNorm = 0.2772
Meta loss on this task batch = 3.5240e-01, Meta loss averaged over last 500 steps = 2.9550e-01, PNorm = 140.3937, GNorm = 0.4126
Took 114.16541075706482 seconds to complete one epoch of meta training
Took 122.17678141593933 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464312
Epoch 565
Meta loss on this task batch = 2.9316e-01, Meta loss averaged over last 500 steps = 2.9554e-01, PNorm = 140.3979, GNorm = 0.2557
Meta loss on this task batch = 2.9435e-01, Meta loss averaged over last 500 steps = 2.9556e-01, PNorm = 140.4035, GNorm = 0.2679
Meta loss on this task batch = 2.1980e-01, Meta loss averaged over last 500 steps = 2.9544e-01, PNorm = 140.4089, GNorm = 0.2391
Meta loss on this task batch = 2.9855e-01, Meta loss averaged over last 500 steps = 2.9549e-01, PNorm = 140.4143, GNorm = 0.2633
Meta loss on this task batch = 2.9934e-01, Meta loss averaged over last 500 steps = 2.9560e-01, PNorm = 140.4187, GNorm = 0.3711
Meta loss on this task batch = 3.5759e-01, Meta loss averaged over last 500 steps = 2.9570e-01, PNorm = 140.4225, GNorm = 0.3644
Meta loss on this task batch = 3.3611e-01, Meta loss averaged over last 500 steps = 2.9579e-01, PNorm = 140.4265, GNorm = 0.4485
Meta loss on this task batch = 3.1126e-01, Meta loss averaged over last 500 steps = 2.9586e-01, PNorm = 140.4308, GNorm = 0.2740
Meta loss on this task batch = 3.1673e-01, Meta loss averaged over last 500 steps = 2.9600e-01, PNorm = 140.4352, GNorm = 0.4343
Meta loss on this task batch = 3.2246e-01, Meta loss averaged over last 500 steps = 2.9616e-01, PNorm = 140.4399, GNorm = 0.3265
Meta loss on this task batch = 2.4529e-01, Meta loss averaged over last 500 steps = 2.9613e-01, PNorm = 140.4452, GNorm = 0.3926
Meta loss on this task batch = 2.7555e-01, Meta loss averaged over last 500 steps = 2.9617e-01, PNorm = 140.4513, GNorm = 0.3143
Meta loss on this task batch = 2.6065e-01, Meta loss averaged over last 500 steps = 2.9609e-01, PNorm = 140.4577, GNorm = 0.3623
Meta loss on this task batch = 3.2306e-01, Meta loss averaged over last 500 steps = 2.9608e-01, PNorm = 140.4636, GNorm = 0.3501
Meta loss on this task batch = 2.6376e-01, Meta loss averaged over last 500 steps = 2.9605e-01, PNorm = 140.4698, GNorm = 0.3020
Meta loss on this task batch = 3.1352e-01, Meta loss averaged over last 500 steps = 2.9602e-01, PNorm = 140.4758, GNorm = 0.2675
Meta loss on this task batch = 3.3308e-01, Meta loss averaged over last 500 steps = 2.9608e-01, PNorm = 140.4817, GNorm = 0.2916
Meta loss on this task batch = 2.7202e-01, Meta loss averaged over last 500 steps = 2.9594e-01, PNorm = 140.4877, GNorm = 0.2929
Meta loss on this task batch = 2.5092e-01, Meta loss averaged over last 500 steps = 2.9573e-01, PNorm = 140.4933, GNorm = 0.4267
Took 114.67119598388672 seconds to complete one epoch of meta training
Took 122.75253176689148 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478940
Epoch 566
Meta loss on this task batch = 2.7245e-01, Meta loss averaged over last 500 steps = 2.9574e-01, PNorm = 140.4992, GNorm = 0.2606
Meta loss on this task batch = 3.3031e-01, Meta loss averaged over last 500 steps = 2.9579e-01, PNorm = 140.5049, GNorm = 0.2992
Meta loss on this task batch = 3.4636e-01, Meta loss averaged over last 500 steps = 2.9593e-01, PNorm = 140.5106, GNorm = 0.3457
Meta loss on this task batch = 2.7970e-01, Meta loss averaged over last 500 steps = 2.9579e-01, PNorm = 140.5157, GNorm = 0.3157
Meta loss on this task batch = 2.3276e-01, Meta loss averaged over last 500 steps = 2.9562e-01, PNorm = 140.5210, GNorm = 0.2482
Meta loss on this task batch = 2.7109e-01, Meta loss averaged over last 500 steps = 2.9558e-01, PNorm = 140.5264, GNorm = 0.2630
Meta loss on this task batch = 3.0461e-01, Meta loss averaged over last 500 steps = 2.9559e-01, PNorm = 140.5321, GNorm = 0.2908
Meta loss on this task batch = 3.2399e-01, Meta loss averaged over last 500 steps = 2.9562e-01, PNorm = 140.5379, GNorm = 0.2636
Meta loss on this task batch = 2.9905e-01, Meta loss averaged over last 500 steps = 2.9556e-01, PNorm = 140.5433, GNorm = 0.3166
Meta loss on this task batch = 2.5209e-01, Meta loss averaged over last 500 steps = 2.9553e-01, PNorm = 140.5483, GNorm = 0.3111
Meta loss on this task batch = 2.8554e-01, Meta loss averaged over last 500 steps = 2.9545e-01, PNorm = 140.5526, GNorm = 0.3172
Meta loss on this task batch = 3.0435e-01, Meta loss averaged over last 500 steps = 2.9549e-01, PNorm = 140.5573, GNorm = 0.2747
Meta loss on this task batch = 2.9405e-01, Meta loss averaged over last 500 steps = 2.9537e-01, PNorm = 140.5619, GNorm = 0.3177
Meta loss on this task batch = 2.6952e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 140.5662, GNorm = 0.2253
Meta loss on this task batch = 3.0931e-01, Meta loss averaged over last 500 steps = 2.9537e-01, PNorm = 140.5702, GNorm = 0.3078
Meta loss on this task batch = 2.9738e-01, Meta loss averaged over last 500 steps = 2.9529e-01, PNorm = 140.5734, GNorm = 0.2674
Meta loss on this task batch = 3.3059e-01, Meta loss averaged over last 500 steps = 2.9528e-01, PNorm = 140.5764, GNorm = 0.5439
Meta loss on this task batch = 2.9211e-01, Meta loss averaged over last 500 steps = 2.9525e-01, PNorm = 140.5796, GNorm = 0.2659
Meta loss on this task batch = 2.6672e-01, Meta loss averaged over last 500 steps = 2.9523e-01, PNorm = 140.5830, GNorm = 0.4460
Took 110.0774941444397 seconds to complete one epoch of meta training
Took 117.9201877117157 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450453
Epoch 567
Meta loss on this task batch = 2.4938e-01, Meta loss averaged over last 500 steps = 2.9521e-01, PNorm = 140.5869, GNorm = 0.3092
Meta loss on this task batch = 2.6625e-01, Meta loss averaged over last 500 steps = 2.9510e-01, PNorm = 140.5909, GNorm = 0.3479
Meta loss on this task batch = 2.2826e-01, Meta loss averaged over last 500 steps = 2.9490e-01, PNorm = 140.5950, GNorm = 0.2120
Meta loss on this task batch = 2.7308e-01, Meta loss averaged over last 500 steps = 2.9495e-01, PNorm = 140.6000, GNorm = 0.2559
Meta loss on this task batch = 2.8266e-01, Meta loss averaged over last 500 steps = 2.9492e-01, PNorm = 140.6051, GNorm = 0.2647
Meta loss on this task batch = 2.7580e-01, Meta loss averaged over last 500 steps = 2.9499e-01, PNorm = 140.6105, GNorm = 0.2628
Meta loss on this task batch = 3.4232e-01, Meta loss averaged over last 500 steps = 2.9500e-01, PNorm = 140.6156, GNorm = 0.2701
Meta loss on this task batch = 3.5619e-01, Meta loss averaged over last 500 steps = 2.9508e-01, PNorm = 140.6208, GNorm = 0.3334
Meta loss on this task batch = 3.5712e-01, Meta loss averaged over last 500 steps = 2.9519e-01, PNorm = 140.6244, GNorm = 0.3455
Meta loss on this task batch = 3.3476e-01, Meta loss averaged over last 500 steps = 2.9521e-01, PNorm = 140.6279, GNorm = 0.4180
Meta loss on this task batch = 2.8415e-01, Meta loss averaged over last 500 steps = 2.9520e-01, PNorm = 140.6319, GNorm = 0.2327
Meta loss on this task batch = 2.8734e-01, Meta loss averaged over last 500 steps = 2.9529e-01, PNorm = 140.6358, GNorm = 0.2696
Meta loss on this task batch = 3.0427e-01, Meta loss averaged over last 500 steps = 2.9533e-01, PNorm = 140.6395, GNorm = 0.4067
Meta loss on this task batch = 2.9667e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 140.6435, GNorm = 0.2646
Meta loss on this task batch = 2.3683e-01, Meta loss averaged over last 500 steps = 2.9531e-01, PNorm = 140.6479, GNorm = 0.2966
Meta loss on this task batch = 3.1217e-01, Meta loss averaged over last 500 steps = 2.9542e-01, PNorm = 140.6522, GNorm = 0.2602
Meta loss on this task batch = 2.6062e-01, Meta loss averaged over last 500 steps = 2.9530e-01, PNorm = 140.6566, GNorm = 0.2876
Meta loss on this task batch = 2.9502e-01, Meta loss averaged over last 500 steps = 2.9513e-01, PNorm = 140.6611, GNorm = 0.3420
Meta loss on this task batch = 2.8234e-01, Meta loss averaged over last 500 steps = 2.9511e-01, PNorm = 140.6670, GNorm = 0.5311
Took 112.84485340118408 seconds to complete one epoch of meta training
Took 120.47882676124573 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473243
Epoch 568
Meta loss on this task batch = 2.7094e-01, Meta loss averaged over last 500 steps = 2.9515e-01, PNorm = 140.6725, GNorm = 0.3158
Meta loss on this task batch = 2.8288e-01, Meta loss averaged over last 500 steps = 2.9512e-01, PNorm = 140.6787, GNorm = 0.3355
Meta loss on this task batch = 2.6299e-01, Meta loss averaged over last 500 steps = 2.9509e-01, PNorm = 140.6841, GNorm = 0.2968
Meta loss on this task batch = 3.1632e-01, Meta loss averaged over last 500 steps = 2.9510e-01, PNorm = 140.6893, GNorm = 0.3190
Meta loss on this task batch = 3.0050e-01, Meta loss averaged over last 500 steps = 2.9502e-01, PNorm = 140.6935, GNorm = 0.3136
Meta loss on this task batch = 2.9064e-01, Meta loss averaged over last 500 steps = 2.9512e-01, PNorm = 140.6977, GNorm = 0.3366
Meta loss on this task batch = 3.1321e-01, Meta loss averaged over last 500 steps = 2.9510e-01, PNorm = 140.7017, GNorm = 0.2923
Meta loss on this task batch = 3.2441e-01, Meta loss averaged over last 500 steps = 2.9528e-01, PNorm = 140.7050, GNorm = 0.3255
Meta loss on this task batch = 3.6981e-01, Meta loss averaged over last 500 steps = 2.9541e-01, PNorm = 140.7076, GNorm = 0.3204
Meta loss on this task batch = 2.8262e-01, Meta loss averaged over last 500 steps = 2.9545e-01, PNorm = 140.7106, GNorm = 0.4410
Meta loss on this task batch = 2.4143e-01, Meta loss averaged over last 500 steps = 2.9538e-01, PNorm = 140.7138, GNorm = 0.2564
Meta loss on this task batch = 2.6242e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 140.7173, GNorm = 0.3170
Meta loss on this task batch = 2.8357e-01, Meta loss averaged over last 500 steps = 2.9533e-01, PNorm = 140.7211, GNorm = 0.2393
Meta loss on this task batch = 2.6264e-01, Meta loss averaged over last 500 steps = 2.9538e-01, PNorm = 140.7249, GNorm = 0.2439
Meta loss on this task batch = 2.4127e-01, Meta loss averaged over last 500 steps = 2.9531e-01, PNorm = 140.7294, GNorm = 0.2926
Meta loss on this task batch = 2.7207e-01, Meta loss averaged over last 500 steps = 2.9518e-01, PNorm = 140.7343, GNorm = 0.2403
Meta loss on this task batch = 2.9195e-01, Meta loss averaged over last 500 steps = 2.9509e-01, PNorm = 140.7387, GNorm = 0.4405
Meta loss on this task batch = 3.5364e-01, Meta loss averaged over last 500 steps = 2.9508e-01, PNorm = 140.7429, GNorm = 0.3021
Meta loss on this task batch = 2.5139e-01, Meta loss averaged over last 500 steps = 2.9509e-01, PNorm = 140.7474, GNorm = 0.4118
Took 110.47097826004028 seconds to complete one epoch of meta training
Took 117.93174719810486 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455668
Epoch 569
Meta loss on this task batch = 3.1621e-01, Meta loss averaged over last 500 steps = 2.9501e-01, PNorm = 140.7518, GNorm = 0.3019
Meta loss on this task batch = 2.6682e-01, Meta loss averaged over last 500 steps = 2.9485e-01, PNorm = 140.7570, GNorm = 0.3148
Meta loss on this task batch = 2.9684e-01, Meta loss averaged over last 500 steps = 2.9483e-01, PNorm = 140.7623, GNorm = 0.2969
Meta loss on this task batch = 2.9680e-01, Meta loss averaged over last 500 steps = 2.9483e-01, PNorm = 140.7673, GNorm = 0.4730
Meta loss on this task batch = 3.0307e-01, Meta loss averaged over last 500 steps = 2.9489e-01, PNorm = 140.7721, GNorm = 0.3637
Meta loss on this task batch = 2.7122e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 140.7772, GNorm = 0.3135
Meta loss on this task batch = 2.7419e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 140.7819, GNorm = 0.3045
Meta loss on this task batch = 2.3768e-01, Meta loss averaged over last 500 steps = 2.9476e-01, PNorm = 140.7873, GNorm = 0.2775
Meta loss on this task batch = 2.7543e-01, Meta loss averaged over last 500 steps = 2.9475e-01, PNorm = 140.7935, GNorm = 0.2709
Meta loss on this task batch = 3.1353e-01, Meta loss averaged over last 500 steps = 2.9493e-01, PNorm = 140.8005, GNorm = 0.3380
Meta loss on this task batch = 2.9980e-01, Meta loss averaged over last 500 steps = 2.9499e-01, PNorm = 140.8070, GNorm = 0.3142
Meta loss on this task batch = 2.9247e-01, Meta loss averaged over last 500 steps = 2.9499e-01, PNorm = 140.8139, GNorm = 0.3377
Meta loss on this task batch = 3.2660e-01, Meta loss averaged over last 500 steps = 2.9493e-01, PNorm = 140.8209, GNorm = 0.3312
Meta loss on this task batch = 2.6601e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 140.8281, GNorm = 0.2811
Meta loss on this task batch = 2.7508e-01, Meta loss averaged over last 500 steps = 2.9487e-01, PNorm = 140.8352, GNorm = 0.2441
Meta loss on this task batch = 2.9053e-01, Meta loss averaged over last 500 steps = 2.9492e-01, PNorm = 140.8415, GNorm = 0.4232
Meta loss on this task batch = 2.7873e-01, Meta loss averaged over last 500 steps = 2.9485e-01, PNorm = 140.8477, GNorm = 0.2493
Meta loss on this task batch = 3.0131e-01, Meta loss averaged over last 500 steps = 2.9486e-01, PNorm = 140.8541, GNorm = 0.3993
Meta loss on this task batch = 2.9139e-01, Meta loss averaged over last 500 steps = 2.9483e-01, PNorm = 140.8615, GNorm = 0.4478
Took 113.96191215515137 seconds to complete one epoch of meta training
Took 122.09038615226746 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459242
Epoch 570
Meta loss on this task batch = 3.5292e-01, Meta loss averaged over last 500 steps = 2.9489e-01, PNorm = 140.8681, GNorm = 0.3156
Meta loss on this task batch = 3.0980e-01, Meta loss averaged over last 500 steps = 2.9486e-01, PNorm = 140.8750, GNorm = 0.3707
Meta loss on this task batch = 3.0544e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 140.8818, GNorm = 0.3658
Meta loss on this task batch = 2.3161e-01, Meta loss averaged over last 500 steps = 2.9484e-01, PNorm = 140.8880, GNorm = 0.3448
Meta loss on this task batch = 2.4993e-01, Meta loss averaged over last 500 steps = 2.9469e-01, PNorm = 140.8940, GNorm = 0.2783
Meta loss on this task batch = 2.5271e-01, Meta loss averaged over last 500 steps = 2.9445e-01, PNorm = 140.9004, GNorm = 0.2426
Meta loss on this task batch = 3.0318e-01, Meta loss averaged over last 500 steps = 2.9457e-01, PNorm = 140.9067, GNorm = 0.2597
Meta loss on this task batch = 2.9130e-01, Meta loss averaged over last 500 steps = 2.9463e-01, PNorm = 140.9125, GNorm = 0.3078
Meta loss on this task batch = 3.0026e-01, Meta loss averaged over last 500 steps = 2.9474e-01, PNorm = 140.9178, GNorm = 0.3069
Meta loss on this task batch = 3.2334e-01, Meta loss averaged over last 500 steps = 2.9482e-01, PNorm = 140.9235, GNorm = 0.4609
Meta loss on this task batch = 2.6196e-01, Meta loss averaged over last 500 steps = 2.9472e-01, PNorm = 140.9296, GNorm = 0.3661
Meta loss on this task batch = 2.9701e-01, Meta loss averaged over last 500 steps = 2.9473e-01, PNorm = 140.9352, GNorm = 0.2740
Meta loss on this task batch = 3.3691e-01, Meta loss averaged over last 500 steps = 2.9479e-01, PNorm = 140.9403, GNorm = 0.3129
Meta loss on this task batch = 2.3491e-01, Meta loss averaged over last 500 steps = 2.9461e-01, PNorm = 140.9457, GNorm = 0.2797
Meta loss on this task batch = 2.8891e-01, Meta loss averaged over last 500 steps = 2.9453e-01, PNorm = 140.9508, GNorm = 0.3052
Meta loss on this task batch = 2.8031e-01, Meta loss averaged over last 500 steps = 2.9458e-01, PNorm = 140.9552, GNorm = 0.3189
Meta loss on this task batch = 3.4050e-01, Meta loss averaged over last 500 steps = 2.9457e-01, PNorm = 140.9585, GNorm = 0.3058
Meta loss on this task batch = 3.1502e-01, Meta loss averaged over last 500 steps = 2.9460e-01, PNorm = 140.9618, GNorm = 0.3391
Meta loss on this task batch = 3.0102e-01, Meta loss averaged over last 500 steps = 2.9462e-01, PNorm = 140.9655, GNorm = 0.2855
Took 109.97478699684143 seconds to complete one epoch of meta training
Took 117.692462682724 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447060
Epoch 571
Meta loss on this task batch = 3.0516e-01, Meta loss averaged over last 500 steps = 2.9447e-01, PNorm = 140.9694, GNorm = 0.3034
Meta loss on this task batch = 3.6502e-01, Meta loss averaged over last 500 steps = 2.9472e-01, PNorm = 140.9722, GNorm = 0.3695
Meta loss on this task batch = 2.7041e-01, Meta loss averaged over last 500 steps = 2.9457e-01, PNorm = 140.9757, GNorm = 0.2190
Meta loss on this task batch = 2.1500e-01, Meta loss averaged over last 500 steps = 2.9448e-01, PNorm = 140.9802, GNorm = 0.2521
Meta loss on this task batch = 2.4416e-01, Meta loss averaged over last 500 steps = 2.9437e-01, PNorm = 140.9855, GNorm = 0.3131
Meta loss on this task batch = 2.7526e-01, Meta loss averaged over last 500 steps = 2.9427e-01, PNorm = 140.9913, GNorm = 0.2886
Meta loss on this task batch = 3.0686e-01, Meta loss averaged over last 500 steps = 2.9430e-01, PNorm = 140.9971, GNorm = 0.3398
Meta loss on this task batch = 2.7338e-01, Meta loss averaged over last 500 steps = 2.9427e-01, PNorm = 141.0026, GNorm = 0.5581
Meta loss on this task batch = 2.5644e-01, Meta loss averaged over last 500 steps = 2.9420e-01, PNorm = 141.0092, GNorm = 0.2246
Meta loss on this task batch = 3.1363e-01, Meta loss averaged over last 500 steps = 2.9419e-01, PNorm = 141.0155, GNorm = 0.2445
Meta loss on this task batch = 2.8905e-01, Meta loss averaged over last 500 steps = 2.9418e-01, PNorm = 141.0216, GNorm = 0.2537
Meta loss on this task batch = 2.9612e-01, Meta loss averaged over last 500 steps = 2.9412e-01, PNorm = 141.0280, GNorm = 0.5399
Meta loss on this task batch = 2.9401e-01, Meta loss averaged over last 500 steps = 2.9407e-01, PNorm = 141.0341, GNorm = 0.2749
Meta loss on this task batch = 2.6711e-01, Meta loss averaged over last 500 steps = 2.9401e-01, PNorm = 141.0402, GNorm = 0.3372
Meta loss on this task batch = 3.7324e-01, Meta loss averaged over last 500 steps = 2.9416e-01, PNorm = 141.0456, GNorm = 0.3804
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 2.9419e-01, PNorm = 141.0508, GNorm = 0.2614
Meta loss on this task batch = 2.9531e-01, Meta loss averaged over last 500 steps = 2.9430e-01, PNorm = 141.0562, GNorm = 0.2997
Meta loss on this task batch = 2.9932e-01, Meta loss averaged over last 500 steps = 2.9433e-01, PNorm = 141.0611, GNorm = 0.3206
Meta loss on this task batch = 3.1190e-01, Meta loss averaged over last 500 steps = 2.9441e-01, PNorm = 141.0651, GNorm = 0.4363
Took 110.40044379234314 seconds to complete one epoch of meta training
Took 118.07026624679565 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467451
Epoch 572
Meta loss on this task batch = 2.5653e-01, Meta loss averaged over last 500 steps = 2.9433e-01, PNorm = 141.0702, GNorm = 0.2988
Meta loss on this task batch = 3.0425e-01, Meta loss averaged over last 500 steps = 2.9451e-01, PNorm = 141.0745, GNorm = 0.4863
Meta loss on this task batch = 3.3774e-01, Meta loss averaged over last 500 steps = 2.9457e-01, PNorm = 141.0787, GNorm = 0.3568
Meta loss on this task batch = 2.5998e-01, Meta loss averaged over last 500 steps = 2.9447e-01, PNorm = 141.0832, GNorm = 0.2604
Meta loss on this task batch = 3.1155e-01, Meta loss averaged over last 500 steps = 2.9443e-01, PNorm = 141.0884, GNorm = 0.3654
Meta loss on this task batch = 2.4531e-01, Meta loss averaged over last 500 steps = 2.9428e-01, PNorm = 141.0943, GNorm = 0.3119
Meta loss on this task batch = 3.0766e-01, Meta loss averaged over last 500 steps = 2.9427e-01, PNorm = 141.1000, GNorm = 0.2696
Meta loss on this task batch = 3.3931e-01, Meta loss averaged over last 500 steps = 2.9432e-01, PNorm = 141.1051, GNorm = 0.3011
Meta loss on this task batch = 3.5128e-01, Meta loss averaged over last 500 steps = 2.9446e-01, PNorm = 141.1097, GNorm = 0.4232
Meta loss on this task batch = 2.5043e-01, Meta loss averaged over last 500 steps = 2.9433e-01, PNorm = 141.1150, GNorm = 0.3872
Meta loss on this task batch = 2.6417e-01, Meta loss averaged over last 500 steps = 2.9428e-01, PNorm = 141.1204, GNorm = 0.2800
Meta loss on this task batch = 2.8222e-01, Meta loss averaged over last 500 steps = 2.9426e-01, PNorm = 141.1257, GNorm = 0.3164
Meta loss on this task batch = 3.0451e-01, Meta loss averaged over last 500 steps = 2.9424e-01, PNorm = 141.1315, GNorm = 0.2549
Meta loss on this task batch = 3.0235e-01, Meta loss averaged over last 500 steps = 2.9437e-01, PNorm = 141.1367, GNorm = 0.2975
Meta loss on this task batch = 2.9350e-01, Meta loss averaged over last 500 steps = 2.9441e-01, PNorm = 141.1418, GNorm = 0.2658
Meta loss on this task batch = 2.8553e-01, Meta loss averaged over last 500 steps = 2.9438e-01, PNorm = 141.1473, GNorm = 0.2621
Meta loss on this task batch = 3.0776e-01, Meta loss averaged over last 500 steps = 2.9436e-01, PNorm = 141.1527, GNorm = 0.2793
Meta loss on this task batch = 2.8207e-01, Meta loss averaged over last 500 steps = 2.9436e-01, PNorm = 141.1581, GNorm = 0.2568
Meta loss on this task batch = 3.3612e-01, Meta loss averaged over last 500 steps = 2.9447e-01, PNorm = 141.1611, GNorm = 0.5741
Took 113.86509585380554 seconds to complete one epoch of meta training
Took 121.56311774253845 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468730
Epoch 573
Meta loss on this task batch = 2.7825e-01, Meta loss averaged over last 500 steps = 2.9444e-01, PNorm = 141.1643, GNorm = 0.2582
Meta loss on this task batch = 2.8574e-01, Meta loss averaged over last 500 steps = 2.9433e-01, PNorm = 141.1676, GNorm = 0.2884
Meta loss on this task batch = 2.7148e-01, Meta loss averaged over last 500 steps = 2.9425e-01, PNorm = 141.1709, GNorm = 0.3100
Meta loss on this task batch = 2.7123e-01, Meta loss averaged over last 500 steps = 2.9421e-01, PNorm = 141.1743, GNorm = 0.2498
Meta loss on this task batch = 2.9992e-01, Meta loss averaged over last 500 steps = 2.9420e-01, PNorm = 141.1776, GNorm = 0.3378
Meta loss on this task batch = 2.7347e-01, Meta loss averaged over last 500 steps = 2.9415e-01, PNorm = 141.1809, GNorm = 0.2738
Meta loss on this task batch = 3.6534e-01, Meta loss averaged over last 500 steps = 2.9437e-01, PNorm = 141.1840, GNorm = 0.3679
Meta loss on this task batch = 3.4075e-01, Meta loss averaged over last 500 steps = 2.9447e-01, PNorm = 141.1870, GNorm = 0.3431
Meta loss on this task batch = 2.5830e-01, Meta loss averaged over last 500 steps = 2.9441e-01, PNorm = 141.1908, GNorm = 0.2989
Meta loss on this task batch = 2.9026e-01, Meta loss averaged over last 500 steps = 2.9445e-01, PNorm = 141.1946, GNorm = 0.2863
Meta loss on this task batch = 2.8090e-01, Meta loss averaged over last 500 steps = 2.9443e-01, PNorm = 141.1988, GNorm = 0.2918
Meta loss on this task batch = 3.2429e-01, Meta loss averaged over last 500 steps = 2.9447e-01, PNorm = 141.2032, GNorm = 0.2976
Meta loss on this task batch = 2.8251e-01, Meta loss averaged over last 500 steps = 2.9448e-01, PNorm = 141.2078, GNorm = 0.3337
Meta loss on this task batch = 2.8220e-01, Meta loss averaged over last 500 steps = 2.9438e-01, PNorm = 141.2131, GNorm = 0.3072
Meta loss on this task batch = 3.3233e-01, Meta loss averaged over last 500 steps = 2.9445e-01, PNorm = 141.2182, GNorm = 0.3116
Meta loss on this task batch = 2.6681e-01, Meta loss averaged over last 500 steps = 2.9434e-01, PNorm = 141.2239, GNorm = 0.3331
Meta loss on this task batch = 2.2403e-01, Meta loss averaged over last 500 steps = 2.9415e-01, PNorm = 141.2297, GNorm = 0.2491
Meta loss on this task batch = 2.7466e-01, Meta loss averaged over last 500 steps = 2.9407e-01, PNorm = 141.2358, GNorm = 0.3313
Meta loss on this task batch = 2.7218e-01, Meta loss averaged over last 500 steps = 2.9416e-01, PNorm = 141.2429, GNorm = 0.4089
Took 113.32978868484497 seconds to complete one epoch of meta training
Took 121.29322481155396 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450240
Epoch 574
Meta loss on this task batch = 2.9234e-01, Meta loss averaged over last 500 steps = 2.9416e-01, PNorm = 141.2494, GNorm = 0.2801
Meta loss on this task batch = 2.7774e-01, Meta loss averaged over last 500 steps = 2.9418e-01, PNorm = 141.2565, GNorm = 0.2771
Meta loss on this task batch = 2.5125e-01, Meta loss averaged over last 500 steps = 2.9422e-01, PNorm = 141.2633, GNorm = 0.2527
Meta loss on this task batch = 2.9074e-01, Meta loss averaged over last 500 steps = 2.9425e-01, PNorm = 141.2699, GNorm = 0.3970
Meta loss on this task batch = 3.1119e-01, Meta loss averaged over last 500 steps = 2.9416e-01, PNorm = 141.2760, GNorm = 0.2987
Meta loss on this task batch = 2.9031e-01, Meta loss averaged over last 500 steps = 2.9418e-01, PNorm = 141.2815, GNorm = 0.2593
Meta loss on this task batch = 2.9027e-01, Meta loss averaged over last 500 steps = 2.9418e-01, PNorm = 141.2869, GNorm = 0.3110
Meta loss on this task batch = 2.5662e-01, Meta loss averaged over last 500 steps = 2.9414e-01, PNorm = 141.2920, GNorm = 0.2337
Meta loss on this task batch = 3.3716e-01, Meta loss averaged over last 500 steps = 2.9425e-01, PNorm = 141.2961, GNorm = 0.3470
Meta loss on this task batch = 3.0987e-01, Meta loss averaged over last 500 steps = 2.9425e-01, PNorm = 141.3005, GNorm = 0.2846
Meta loss on this task batch = 3.5735e-01, Meta loss averaged over last 500 steps = 2.9426e-01, PNorm = 141.3049, GNorm = 0.3901
Meta loss on this task batch = 3.0972e-01, Meta loss averaged over last 500 steps = 2.9441e-01, PNorm = 141.3099, GNorm = 0.3202
Meta loss on this task batch = 2.9035e-01, Meta loss averaged over last 500 steps = 2.9446e-01, PNorm = 141.3152, GNorm = 0.3318
Meta loss on this task batch = 2.9639e-01, Meta loss averaged over last 500 steps = 2.9450e-01, PNorm = 141.3202, GNorm = 0.2687
Meta loss on this task batch = 3.2720e-01, Meta loss averaged over last 500 steps = 2.9469e-01, PNorm = 141.3253, GNorm = 0.2805
Meta loss on this task batch = 2.8508e-01, Meta loss averaged over last 500 steps = 2.9465e-01, PNorm = 141.3303, GNorm = 0.3135
Meta loss on this task batch = 3.1495e-01, Meta loss averaged over last 500 steps = 2.9463e-01, PNorm = 141.3363, GNorm = 0.3253
Meta loss on this task batch = 2.7581e-01, Meta loss averaged over last 500 steps = 2.9454e-01, PNorm = 141.3427, GNorm = 0.2845
Meta loss on this task batch = 3.1464e-01, Meta loss averaged over last 500 steps = 2.9444e-01, PNorm = 141.3485, GNorm = 0.3145
Took 109.88637566566467 seconds to complete one epoch of meta training
Took 117.48397421836853 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.435566
Epoch 575
Meta loss on this task batch = 3.1753e-01, Meta loss averaged over last 500 steps = 2.9450e-01, PNorm = 141.3543, GNorm = 0.4084
Meta loss on this task batch = 2.7882e-01, Meta loss averaged over last 500 steps = 2.9452e-01, PNorm = 141.3605, GNorm = 0.2612
Meta loss on this task batch = 3.1038e-01, Meta loss averaged over last 500 steps = 2.9459e-01, PNorm = 141.3659, GNorm = 0.4538
Meta loss on this task batch = 2.5808e-01, Meta loss averaged over last 500 steps = 2.9448e-01, PNorm = 141.3713, GNorm = 0.2376
Meta loss on this task batch = 2.5194e-01, Meta loss averaged over last 500 steps = 2.9442e-01, PNorm = 141.3768, GNorm = 0.3474
Meta loss on this task batch = 2.7207e-01, Meta loss averaged over last 500 steps = 2.9432e-01, PNorm = 141.3822, GNorm = 0.3593
Meta loss on this task batch = 2.7637e-01, Meta loss averaged over last 500 steps = 2.9425e-01, PNorm = 141.3879, GNorm = 0.3255
Meta loss on this task batch = 3.1776e-01, Meta loss averaged over last 500 steps = 2.9430e-01, PNorm = 141.3944, GNorm = 0.4949
Meta loss on this task batch = 2.9374e-01, Meta loss averaged over last 500 steps = 2.9422e-01, PNorm = 141.4005, GNorm = 0.2965
Meta loss on this task batch = 2.8107e-01, Meta loss averaged over last 500 steps = 2.9424e-01, PNorm = 141.4065, GNorm = 0.2549
Meta loss on this task batch = 2.6834e-01, Meta loss averaged over last 500 steps = 2.9423e-01, PNorm = 141.4128, GNorm = 0.2559
Meta loss on this task batch = 3.3039e-01, Meta loss averaged over last 500 steps = 2.9432e-01, PNorm = 141.4189, GNorm = 0.3136
Meta loss on this task batch = 3.0237e-01, Meta loss averaged over last 500 steps = 2.9435e-01, PNorm = 141.4244, GNorm = 0.2825
Meta loss on this task batch = 3.3806e-01, Meta loss averaged over last 500 steps = 2.9448e-01, PNorm = 141.4298, GNorm = 0.4102
Meta loss on this task batch = 3.2139e-01, Meta loss averaged over last 500 steps = 2.9450e-01, PNorm = 141.4357, GNorm = 0.2920
Meta loss on this task batch = 2.7438e-01, Meta loss averaged over last 500 steps = 2.9449e-01, PNorm = 141.4413, GNorm = 0.2563
Meta loss on this task batch = 2.3198e-01, Meta loss averaged over last 500 steps = 2.9423e-01, PNorm = 141.4477, GNorm = 0.2658
Meta loss on this task batch = 2.6225e-01, Meta loss averaged over last 500 steps = 2.9414e-01, PNorm = 141.4533, GNorm = 0.2811
Meta loss on this task batch = 2.8665e-01, Meta loss averaged over last 500 steps = 2.9410e-01, PNorm = 141.4591, GNorm = 0.3410
Took 112.14771723747253 seconds to complete one epoch of meta training
Took 119.7274067401886 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452817
Epoch 576
Meta loss on this task batch = 2.9526e-01, Meta loss averaged over last 500 steps = 2.9413e-01, PNorm = 141.4648, GNorm = 0.4067
Meta loss on this task batch = 3.4505e-01, Meta loss averaged over last 500 steps = 2.9421e-01, PNorm = 141.4701, GNorm = 0.3260
Meta loss on this task batch = 2.1379e-01, Meta loss averaged over last 500 steps = 2.9406e-01, PNorm = 141.4761, GNorm = 0.2107
Meta loss on this task batch = 3.1021e-01, Meta loss averaged over last 500 steps = 2.9415e-01, PNorm = 141.4820, GNorm = 0.3786
Meta loss on this task batch = 3.4452e-01, Meta loss averaged over last 500 steps = 2.9436e-01, PNorm = 141.4871, GNorm = 0.3512
Meta loss on this task batch = 2.8422e-01, Meta loss averaged over last 500 steps = 2.9437e-01, PNorm = 141.4926, GNorm = 0.2859
Meta loss on this task batch = 3.3273e-01, Meta loss averaged over last 500 steps = 2.9446e-01, PNorm = 141.4972, GNorm = 0.3067
Meta loss on this task batch = 2.9838e-01, Meta loss averaged over last 500 steps = 2.9448e-01, PNorm = 141.5021, GNorm = 0.2651
Meta loss on this task batch = 2.6719e-01, Meta loss averaged over last 500 steps = 2.9442e-01, PNorm = 141.5074, GNorm = 0.2725
Meta loss on this task batch = 2.9142e-01, Meta loss averaged over last 500 steps = 2.9434e-01, PNorm = 141.5129, GNorm = 0.3280
Meta loss on this task batch = 2.9196e-01, Meta loss averaged over last 500 steps = 2.9429e-01, PNorm = 141.5194, GNorm = 0.2978
Meta loss on this task batch = 2.7303e-01, Meta loss averaged over last 500 steps = 2.9430e-01, PNorm = 141.5261, GNorm = 0.2930
Meta loss on this task batch = 2.7691e-01, Meta loss averaged over last 500 steps = 2.9407e-01, PNorm = 141.5330, GNorm = 0.4222
Meta loss on this task batch = 2.7863e-01, Meta loss averaged over last 500 steps = 2.9420e-01, PNorm = 141.5389, GNorm = 0.2760
Meta loss on this task batch = 3.0058e-01, Meta loss averaged over last 500 steps = 2.9422e-01, PNorm = 141.5447, GNorm = 0.3823
Meta loss on this task batch = 3.4091e-01, Meta loss averaged over last 500 steps = 2.9433e-01, PNorm = 141.5493, GNorm = 0.3170
Meta loss on this task batch = 2.2751e-01, Meta loss averaged over last 500 steps = 2.9417e-01, PNorm = 141.5535, GNorm = 0.2912
Meta loss on this task batch = 2.8482e-01, Meta loss averaged over last 500 steps = 2.9412e-01, PNorm = 141.5569, GNorm = 0.2768
Meta loss on this task batch = 3.0056e-01, Meta loss averaged over last 500 steps = 2.9418e-01, PNorm = 141.5602, GNorm = 0.4337
Took 111.46836423873901 seconds to complete one epoch of meta training
Took 119.38973450660706 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463800
Epoch 577
Meta loss on this task batch = 2.8720e-01, Meta loss averaged over last 500 steps = 2.9404e-01, PNorm = 141.5639, GNorm = 0.3765
Meta loss on this task batch = 2.9777e-01, Meta loss averaged over last 500 steps = 2.9414e-01, PNorm = 141.5671, GNorm = 0.2957
Meta loss on this task batch = 3.1963e-01, Meta loss averaged over last 500 steps = 2.9429e-01, PNorm = 141.5709, GNorm = 0.2694
Meta loss on this task batch = 3.0967e-01, Meta loss averaged over last 500 steps = 2.9433e-01, PNorm = 141.5744, GNorm = 0.3388
Meta loss on this task batch = 2.6314e-01, Meta loss averaged over last 500 steps = 2.9438e-01, PNorm = 141.5785, GNorm = 0.2448
Meta loss on this task batch = 2.8421e-01, Meta loss averaged over last 500 steps = 2.9437e-01, PNorm = 141.5822, GNorm = 0.2584
Meta loss on this task batch = 2.7587e-01, Meta loss averaged over last 500 steps = 2.9434e-01, PNorm = 141.5865, GNorm = 0.2511
Meta loss on this task batch = 2.4551e-01, Meta loss averaged over last 500 steps = 2.9429e-01, PNorm = 141.5915, GNorm = 0.2842
Meta loss on this task batch = 3.2470e-01, Meta loss averaged over last 500 steps = 2.9444e-01, PNorm = 141.5967, GNorm = 0.2620
Meta loss on this task batch = 2.4721e-01, Meta loss averaged over last 500 steps = 2.9426e-01, PNorm = 141.6019, GNorm = 0.4980
Meta loss on this task batch = 2.6439e-01, Meta loss averaged over last 500 steps = 2.9427e-01, PNorm = 141.6073, GNorm = 0.2822
Meta loss on this task batch = 3.0197e-01, Meta loss averaged over last 500 steps = 2.9427e-01, PNorm = 141.6126, GNorm = 0.4120
Meta loss on this task batch = 2.9248e-01, Meta loss averaged over last 500 steps = 2.9424e-01, PNorm = 141.6175, GNorm = 0.2842
Meta loss on this task batch = 3.0924e-01, Meta loss averaged over last 500 steps = 2.9427e-01, PNorm = 141.6215, GNorm = 0.3059
Meta loss on this task batch = 3.1575e-01, Meta loss averaged over last 500 steps = 2.9434e-01, PNorm = 141.6255, GNorm = 0.2908
Meta loss on this task batch = 3.0522e-01, Meta loss averaged over last 500 steps = 2.9430e-01, PNorm = 141.6296, GNorm = 0.2807
Meta loss on this task batch = 2.0834e-01, Meta loss averaged over last 500 steps = 2.9404e-01, PNorm = 141.6340, GNorm = 0.2230
Meta loss on this task batch = 3.3614e-01, Meta loss averaged over last 500 steps = 2.9414e-01, PNorm = 141.6380, GNorm = 0.4034
Meta loss on this task batch = 3.4332e-01, Meta loss averaged over last 500 steps = 2.9435e-01, PNorm = 141.6411, GNorm = 0.3941
Took 111.63096499443054 seconds to complete one epoch of meta training
Took 118.56278228759766 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.445993
Epoch 578
Meta loss on this task batch = 3.0357e-01, Meta loss averaged over last 500 steps = 2.9429e-01, PNorm = 141.6437, GNorm = 0.3094
Meta loss on this task batch = 2.5417e-01, Meta loss averaged over last 500 steps = 2.9418e-01, PNorm = 141.6469, GNorm = 0.2658
Meta loss on this task batch = 2.6837e-01, Meta loss averaged over last 500 steps = 2.9397e-01, PNorm = 141.6503, GNorm = 0.2890
Meta loss on this task batch = 2.9206e-01, Meta loss averaged over last 500 steps = 2.9395e-01, PNorm = 141.6533, GNorm = 0.2865
Meta loss on this task batch = 3.3498e-01, Meta loss averaged over last 500 steps = 2.9408e-01, PNorm = 141.6572, GNorm = 0.2875
Meta loss on this task batch = 2.8545e-01, Meta loss averaged over last 500 steps = 2.9411e-01, PNorm = 141.6606, GNorm = 0.2500
Meta loss on this task batch = 2.8499e-01, Meta loss averaged over last 500 steps = 2.9400e-01, PNorm = 141.6642, GNorm = 0.2933
Meta loss on this task batch = 2.8261e-01, Meta loss averaged over last 500 steps = 2.9402e-01, PNorm = 141.6685, GNorm = 0.2886
Meta loss on this task batch = 2.5602e-01, Meta loss averaged over last 500 steps = 2.9389e-01, PNorm = 141.6733, GNorm = 0.2705
Meta loss on this task batch = 3.3217e-01, Meta loss averaged over last 500 steps = 2.9399e-01, PNorm = 141.6776, GNorm = 0.4125
Meta loss on this task batch = 3.2323e-01, Meta loss averaged over last 500 steps = 2.9410e-01, PNorm = 141.6813, GNorm = 0.2986
Meta loss on this task batch = 3.2308e-01, Meta loss averaged over last 500 steps = 2.9418e-01, PNorm = 141.6855, GNorm = 0.2499
Meta loss on this task batch = 2.2341e-01, Meta loss averaged over last 500 steps = 2.9399e-01, PNorm = 141.6908, GNorm = 0.2226
Meta loss on this task batch = 2.8647e-01, Meta loss averaged over last 500 steps = 2.9409e-01, PNorm = 141.6962, GNorm = 0.2926
Meta loss on this task batch = 2.6560e-01, Meta loss averaged over last 500 steps = 2.9394e-01, PNorm = 141.7018, GNorm = 0.2601
Meta loss on this task batch = 2.6750e-01, Meta loss averaged over last 500 steps = 2.9382e-01, PNorm = 141.7084, GNorm = 0.2861
Meta loss on this task batch = 3.1780e-01, Meta loss averaged over last 500 steps = 2.9389e-01, PNorm = 141.7146, GNorm = 0.2653
Meta loss on this task batch = 3.0305e-01, Meta loss averaged over last 500 steps = 2.9387e-01, PNorm = 141.7210, GNorm = 0.2991
Meta loss on this task batch = 2.9831e-01, Meta loss averaged over last 500 steps = 2.9397e-01, PNorm = 141.7278, GNorm = 0.3673
Took 111.47003483772278 seconds to complete one epoch of meta training
Took 119.14337134361267 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453575
Epoch 579
Meta loss on this task batch = 2.5897e-01, Meta loss averaged over last 500 steps = 2.9394e-01, PNorm = 141.7346, GNorm = 0.3312
Meta loss on this task batch = 3.3555e-01, Meta loss averaged over last 500 steps = 2.9405e-01, PNorm = 141.7405, GNorm = 0.3053
Meta loss on this task batch = 2.7757e-01, Meta loss averaged over last 500 steps = 2.9387e-01, PNorm = 141.7457, GNorm = 0.2879
Meta loss on this task batch = 2.6490e-01, Meta loss averaged over last 500 steps = 2.9382e-01, PNorm = 141.7501, GNorm = 0.4202
Meta loss on this task batch = 3.1199e-01, Meta loss averaged over last 500 steps = 2.9386e-01, PNorm = 141.7537, GNorm = 0.3289
Meta loss on this task batch = 2.9404e-01, Meta loss averaged over last 500 steps = 2.9378e-01, PNorm = 141.7576, GNorm = 0.2967
Meta loss on this task batch = 2.7742e-01, Meta loss averaged over last 500 steps = 2.9377e-01, PNorm = 141.7616, GNorm = 0.6351
Meta loss on this task batch = 2.7197e-01, Meta loss averaged over last 500 steps = 2.9370e-01, PNorm = 141.7655, GNorm = 0.2915
Meta loss on this task batch = 2.9217e-01, Meta loss averaged over last 500 steps = 2.9379e-01, PNorm = 141.7698, GNorm = 0.4245
Meta loss on this task batch = 3.0583e-01, Meta loss averaged over last 500 steps = 2.9375e-01, PNorm = 141.7739, GNorm = 0.3746
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 2.9362e-01, PNorm = 141.7787, GNorm = 0.3958
Meta loss on this task batch = 2.7829e-01, Meta loss averaged over last 500 steps = 2.9368e-01, PNorm = 141.7835, GNorm = 0.3148
Meta loss on this task batch = 2.9781e-01, Meta loss averaged over last 500 steps = 2.9378e-01, PNorm = 141.7897, GNorm = 0.2544
Meta loss on this task batch = 2.9372e-01, Meta loss averaged over last 500 steps = 2.9374e-01, PNorm = 141.7958, GNorm = 0.3299
Meta loss on this task batch = 2.8211e-01, Meta loss averaged over last 500 steps = 2.9358e-01, PNorm = 141.8017, GNorm = 0.3978
Meta loss on this task batch = 2.6904e-01, Meta loss averaged over last 500 steps = 2.9356e-01, PNorm = 141.8076, GNorm = 0.3008
Meta loss on this task batch = 2.6739e-01, Meta loss averaged over last 500 steps = 2.9351e-01, PNorm = 141.8138, GNorm = 0.2998
Meta loss on this task batch = 2.6685e-01, Meta loss averaged over last 500 steps = 2.9344e-01, PNorm = 141.8196, GNorm = 0.3221
Meta loss on this task batch = 3.3915e-01, Meta loss averaged over last 500 steps = 2.9354e-01, PNorm = 141.8252, GNorm = 0.4538
Took 111.54661226272583 seconds to complete one epoch of meta training
Took 119.34838557243347 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455742
Epoch 580
Meta loss on this task batch = 3.0032e-01, Meta loss averaged over last 500 steps = 2.9343e-01, PNorm = 141.8303, GNorm = 0.2559
Meta loss on this task batch = 3.2986e-01, Meta loss averaged over last 500 steps = 2.9353e-01, PNorm = 141.8353, GNorm = 0.3121
Meta loss on this task batch = 2.9848e-01, Meta loss averaged over last 500 steps = 2.9361e-01, PNorm = 141.8396, GNorm = 0.2545
Meta loss on this task batch = 2.4753e-01, Meta loss averaged over last 500 steps = 2.9349e-01, PNorm = 141.8436, GNorm = 0.3304
Meta loss on this task batch = 2.4622e-01, Meta loss averaged over last 500 steps = 2.9342e-01, PNorm = 141.8474, GNorm = 0.2606
Meta loss on this task batch = 2.8446e-01, Meta loss averaged over last 500 steps = 2.9338e-01, PNorm = 141.8507, GNorm = 0.3257
Meta loss on this task batch = 3.2516e-01, Meta loss averaged over last 500 steps = 2.9358e-01, PNorm = 141.8546, GNorm = 0.3319
Meta loss on this task batch = 2.7161e-01, Meta loss averaged over last 500 steps = 2.9340e-01, PNorm = 141.8584, GNorm = 0.2657
Meta loss on this task batch = 2.9888e-01, Meta loss averaged over last 500 steps = 2.9339e-01, PNorm = 141.8632, GNorm = 0.3854
Meta loss on this task batch = 2.5881e-01, Meta loss averaged over last 500 steps = 2.9335e-01, PNorm = 141.8684, GNorm = 0.2787
Meta loss on this task batch = 2.8238e-01, Meta loss averaged over last 500 steps = 2.9336e-01, PNorm = 141.8737, GNorm = 0.3089
Meta loss on this task batch = 2.3975e-01, Meta loss averaged over last 500 steps = 2.9329e-01, PNorm = 141.8788, GNorm = 0.2224
Meta loss on this task batch = 2.6789e-01, Meta loss averaged over last 500 steps = 2.9326e-01, PNorm = 141.8840, GNorm = 0.2827
Meta loss on this task batch = 3.6720e-01, Meta loss averaged over last 500 steps = 2.9346e-01, PNorm = 141.8881, GNorm = 0.3619
Meta loss on this task batch = 2.8822e-01, Meta loss averaged over last 500 steps = 2.9355e-01, PNorm = 141.8926, GNorm = 0.2711
Meta loss on this task batch = 2.3209e-01, Meta loss averaged over last 500 steps = 2.9332e-01, PNorm = 141.8987, GNorm = 0.2720
Meta loss on this task batch = 3.3174e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 141.9041, GNorm = 0.4399
Meta loss on this task batch = 2.6079e-01, Meta loss averaged over last 500 steps = 2.9325e-01, PNorm = 141.9101, GNorm = 0.2998
Meta loss on this task batch = 3.0558e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 141.9159, GNorm = 0.3018
Took 110.57508897781372 seconds to complete one epoch of meta training
Took 118.25884771347046 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453788
Epoch 581
Meta loss on this task batch = 2.5662e-01, Meta loss averaged over last 500 steps = 2.9316e-01, PNorm = 141.9215, GNorm = 0.2749
Meta loss on this task batch = 2.8825e-01, Meta loss averaged over last 500 steps = 2.9313e-01, PNorm = 141.9271, GNorm = 0.2729
Meta loss on this task batch = 2.8446e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 141.9332, GNorm = 0.2897
Meta loss on this task batch = 3.2711e-01, Meta loss averaged over last 500 steps = 2.9316e-01, PNorm = 141.9392, GNorm = 0.2760
Meta loss on this task batch = 3.0126e-01, Meta loss averaged over last 500 steps = 2.9317e-01, PNorm = 141.9450, GNorm = 0.3371
Meta loss on this task batch = 2.8214e-01, Meta loss averaged over last 500 steps = 2.9304e-01, PNorm = 141.9505, GNorm = 0.2381
Meta loss on this task batch = 3.1687e-01, Meta loss averaged over last 500 steps = 2.9304e-01, PNorm = 141.9558, GNorm = 0.3654
Meta loss on this task batch = 3.3913e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 141.9603, GNorm = 0.2841
Meta loss on this task batch = 3.3765e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 141.9641, GNorm = 0.2942
Meta loss on this task batch = 3.4984e-01, Meta loss averaged over last 500 steps = 2.9330e-01, PNorm = 141.9677, GNorm = 0.2991
Meta loss on this task batch = 2.6801e-01, Meta loss averaged over last 500 steps = 2.9325e-01, PNorm = 141.9729, GNorm = 0.3766
Meta loss on this task batch = 2.5360e-01, Meta loss averaged over last 500 steps = 2.9316e-01, PNorm = 141.9776, GNorm = 0.3568
Meta loss on this task batch = 2.7028e-01, Meta loss averaged over last 500 steps = 2.9311e-01, PNorm = 141.9832, GNorm = 0.3357
Meta loss on this task batch = 2.8972e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 141.9894, GNorm = 0.4526
Meta loss on this task batch = 3.0032e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 141.9951, GNorm = 0.3139
Meta loss on this task batch = 2.5834e-01, Meta loss averaged over last 500 steps = 2.9324e-01, PNorm = 142.0002, GNorm = 0.2753
Meta loss on this task batch = 2.7603e-01, Meta loss averaged over last 500 steps = 2.9317e-01, PNorm = 142.0050, GNorm = 0.2401
Meta loss on this task batch = 3.2467e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 142.0099, GNorm = 0.3227
Meta loss on this task batch = 2.8506e-01, Meta loss averaged over last 500 steps = 2.9327e-01, PNorm = 142.0152, GNorm = 0.5441
Took 111.66298508644104 seconds to complete one epoch of meta training
Took 119.52106881141663 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464575
Epoch 582
Meta loss on this task batch = 2.8201e-01, Meta loss averaged over last 500 steps = 2.9331e-01, PNorm = 142.0202, GNorm = 0.3583
Meta loss on this task batch = 2.6339e-01, Meta loss averaged over last 500 steps = 2.9335e-01, PNorm = 142.0253, GNorm = 0.2614
Meta loss on this task batch = 2.5397e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 142.0308, GNorm = 0.2521
Meta loss on this task batch = 2.5957e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 142.0370, GNorm = 0.2838
Meta loss on this task batch = 3.0681e-01, Meta loss averaged over last 500 steps = 2.9300e-01, PNorm = 142.0430, GNorm = 0.2584
Meta loss on this task batch = 3.2583e-01, Meta loss averaged over last 500 steps = 2.9301e-01, PNorm = 142.0483, GNorm = 0.3426
Meta loss on this task batch = 3.0183e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 142.0538, GNorm = 0.2727
Meta loss on this task batch = 2.9187e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 142.0597, GNorm = 0.3658
Meta loss on this task batch = 2.9245e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 142.0653, GNorm = 0.3015
Meta loss on this task batch = 2.1571e-01, Meta loss averaged over last 500 steps = 2.9292e-01, PNorm = 142.0709, GNorm = 0.2376
Meta loss on this task batch = 3.3483e-01, Meta loss averaged over last 500 steps = 2.9296e-01, PNorm = 142.0758, GNorm = 0.2416
Meta loss on this task batch = 2.4057e-01, Meta loss averaged over last 500 steps = 2.9282e-01, PNorm = 142.0817, GNorm = 0.4022
Meta loss on this task batch = 3.6464e-01, Meta loss averaged over last 500 steps = 2.9300e-01, PNorm = 142.0878, GNorm = 0.5191
Meta loss on this task batch = 2.6612e-01, Meta loss averaged over last 500 steps = 2.9288e-01, PNorm = 142.0947, GNorm = 0.3550
Meta loss on this task batch = 3.2825e-01, Meta loss averaged over last 500 steps = 2.9302e-01, PNorm = 142.1013, GNorm = 0.2804
Meta loss on this task batch = 2.7451e-01, Meta loss averaged over last 500 steps = 2.9299e-01, PNorm = 142.1075, GNorm = 0.2915
Meta loss on this task batch = 2.8180e-01, Meta loss averaged over last 500 steps = 2.9298e-01, PNorm = 142.1135, GNorm = 0.2311
Meta loss on this task batch = 3.2785e-01, Meta loss averaged over last 500 steps = 2.9301e-01, PNorm = 142.1188, GNorm = 0.2532
Meta loss on this task batch = 2.8302e-01, Meta loss averaged over last 500 steps = 2.9300e-01, PNorm = 142.1240, GNorm = 0.5304
Took 113.75810194015503 seconds to complete one epoch of meta training
Took 121.78826975822449 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469959
Epoch 583
Meta loss on this task batch = 2.4106e-01, Meta loss averaged over last 500 steps = 2.9298e-01, PNorm = 142.1290, GNorm = 0.2749
Meta loss on this task batch = 2.4296e-01, Meta loss averaged over last 500 steps = 2.9291e-01, PNorm = 142.1342, GNorm = 0.2110
Meta loss on this task batch = 3.1121e-01, Meta loss averaged over last 500 steps = 2.9300e-01, PNorm = 142.1396, GNorm = 0.2874
Meta loss on this task batch = 3.1866e-01, Meta loss averaged over last 500 steps = 2.9308e-01, PNorm = 142.1444, GNorm = 0.4383
Meta loss on this task batch = 2.8244e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 142.1494, GNorm = 0.2698
Meta loss on this task batch = 2.4989e-01, Meta loss averaged over last 500 steps = 2.9281e-01, PNorm = 142.1552, GNorm = 0.2706
Meta loss on this task batch = 3.1712e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 142.1602, GNorm = 0.2773
Meta loss on this task batch = 2.8177e-01, Meta loss averaged over last 500 steps = 2.9265e-01, PNorm = 142.1658, GNorm = 0.2973
Meta loss on this task batch = 3.1582e-01, Meta loss averaged over last 500 steps = 2.9272e-01, PNorm = 142.1716, GNorm = 0.2805
Meta loss on this task batch = 2.9218e-01, Meta loss averaged over last 500 steps = 2.9268e-01, PNorm = 142.1772, GNorm = 0.3013
Meta loss on this task batch = 2.8928e-01, Meta loss averaged over last 500 steps = 2.9267e-01, PNorm = 142.1824, GNorm = 0.3097
Meta loss on this task batch = 3.0262e-01, Meta loss averaged over last 500 steps = 2.9274e-01, PNorm = 142.1862, GNorm = 0.4799
Meta loss on this task batch = 3.0973e-01, Meta loss averaged over last 500 steps = 2.9277e-01, PNorm = 142.1899, GNorm = 0.2741
Meta loss on this task batch = 2.6706e-01, Meta loss averaged over last 500 steps = 2.9269e-01, PNorm = 142.1932, GNorm = 0.3089
Meta loss on this task batch = 2.8228e-01, Meta loss averaged over last 500 steps = 2.9269e-01, PNorm = 142.1973, GNorm = 0.3315
Meta loss on this task batch = 3.1119e-01, Meta loss averaged over last 500 steps = 2.9274e-01, PNorm = 142.2020, GNorm = 0.4750
Meta loss on this task batch = 3.5496e-01, Meta loss averaged over last 500 steps = 2.9289e-01, PNorm = 142.2058, GNorm = 0.3442
Meta loss on this task batch = 2.8460e-01, Meta loss averaged over last 500 steps = 2.9279e-01, PNorm = 142.2095, GNorm = 0.2812
Meta loss on this task batch = 3.1096e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 142.2131, GNorm = 0.4998
Took 113.31364965438843 seconds to complete one epoch of meta training
Took 120.27986788749695 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460261
Epoch 584
Meta loss on this task batch = 3.6035e-01, Meta loss averaged over last 500 steps = 2.9295e-01, PNorm = 142.2167, GNorm = 0.3321
Meta loss on this task batch = 2.7580e-01, Meta loss averaged over last 500 steps = 2.9291e-01, PNorm = 142.2208, GNorm = 0.2668
Meta loss on this task batch = 2.8376e-01, Meta loss averaged over last 500 steps = 2.9292e-01, PNorm = 142.2244, GNorm = 0.2504
Meta loss on this task batch = 3.2026e-01, Meta loss averaged over last 500 steps = 2.9293e-01, PNorm = 142.2280, GNorm = 0.2841
Meta loss on this task batch = 3.0668e-01, Meta loss averaged over last 500 steps = 2.9292e-01, PNorm = 142.2322, GNorm = 0.2764
Meta loss on this task batch = 2.5756e-01, Meta loss averaged over last 500 steps = 2.9294e-01, PNorm = 142.2374, GNorm = 0.2902
Meta loss on this task batch = 3.2682e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 142.2425, GNorm = 0.2903
Meta loss on this task batch = 3.2692e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 142.2482, GNorm = 0.2821
Meta loss on this task batch = 3.1494e-01, Meta loss averaged over last 500 steps = 2.9327e-01, PNorm = 142.2544, GNorm = 0.3118
Meta loss on this task batch = 2.8443e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 142.2584, GNorm = 0.6011
Meta loss on this task batch = 2.5504e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 142.2632, GNorm = 0.3160
Meta loss on this task batch = 3.1050e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 142.2684, GNorm = 0.2938
Meta loss on this task batch = 3.2001e-01, Meta loss averaged over last 500 steps = 2.9318e-01, PNorm = 142.2732, GNorm = 0.3160
Meta loss on this task batch = 2.7309e-01, Meta loss averaged over last 500 steps = 2.9298e-01, PNorm = 142.2786, GNorm = 0.3273
Meta loss on this task batch = 3.1324e-01, Meta loss averaged over last 500 steps = 2.9293e-01, PNorm = 142.2834, GNorm = 0.2641
Meta loss on this task batch = 2.1660e-01, Meta loss averaged over last 500 steps = 2.9273e-01, PNorm = 142.2891, GNorm = 0.2757
Meta loss on this task batch = 3.3490e-01, Meta loss averaged over last 500 steps = 2.9277e-01, PNorm = 142.2935, GNorm = 0.2998
Meta loss on this task batch = 2.9343e-01, Meta loss averaged over last 500 steps = 2.9281e-01, PNorm = 142.2983, GNorm = 0.2655
Meta loss on this task batch = 2.8197e-01, Meta loss averaged over last 500 steps = 2.9280e-01, PNorm = 142.3029, GNorm = 0.3726
Took 113.16703295707703 seconds to complete one epoch of meta training
Took 120.43443274497986 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481761
Epoch 585
Meta loss on this task batch = 2.7550e-01, Meta loss averaged over last 500 steps = 2.9268e-01, PNorm = 142.3076, GNorm = 0.2787
Meta loss on this task batch = 2.7252e-01, Meta loss averaged over last 500 steps = 2.9251e-01, PNorm = 142.3120, GNorm = 0.2906
Meta loss on this task batch = 2.6575e-01, Meta loss averaged over last 500 steps = 2.9235e-01, PNorm = 142.3168, GNorm = 0.2415
Meta loss on this task batch = 2.9080e-01, Meta loss averaged over last 500 steps = 2.9238e-01, PNorm = 142.3215, GNorm = 0.3013
Meta loss on this task batch = 3.1314e-01, Meta loss averaged over last 500 steps = 2.9242e-01, PNorm = 142.3259, GNorm = 0.3131
Meta loss on this task batch = 2.5629e-01, Meta loss averaged over last 500 steps = 2.9234e-01, PNorm = 142.3307, GNorm = 0.2475
Meta loss on this task batch = 2.7873e-01, Meta loss averaged over last 500 steps = 2.9231e-01, PNorm = 142.3356, GNorm = 0.2642
Meta loss on this task batch = 3.5061e-01, Meta loss averaged over last 500 steps = 2.9258e-01, PNorm = 142.3401, GNorm = 0.2815
Meta loss on this task batch = 3.0331e-01, Meta loss averaged over last 500 steps = 2.9258e-01, PNorm = 142.3446, GNorm = 0.2638
Meta loss on this task batch = 2.3419e-01, Meta loss averaged over last 500 steps = 2.9260e-01, PNorm = 142.3494, GNorm = 0.2341
Meta loss on this task batch = 3.6552e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 142.3540, GNorm = 0.6164
Meta loss on this task batch = 3.1801e-01, Meta loss averaged over last 500 steps = 2.9284e-01, PNorm = 142.3589, GNorm = 0.2544
Meta loss on this task batch = 2.6186e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 142.3642, GNorm = 0.2638
Meta loss on this task batch = 2.4261e-01, Meta loss averaged over last 500 steps = 2.9269e-01, PNorm = 142.3690, GNorm = 0.3256
Meta loss on this task batch = 3.1168e-01, Meta loss averaged over last 500 steps = 2.9265e-01, PNorm = 142.3729, GNorm = 0.2648
Meta loss on this task batch = 3.5175e-01, Meta loss averaged over last 500 steps = 2.9282e-01, PNorm = 142.3755, GNorm = 0.4136
Meta loss on this task batch = 3.0810e-01, Meta loss averaged over last 500 steps = 2.9277e-01, PNorm = 142.3788, GNorm = 0.2671
Meta loss on this task batch = 3.0250e-01, Meta loss averaged over last 500 steps = 2.9279e-01, PNorm = 142.3825, GNorm = 0.3059
Meta loss on this task batch = 2.8979e-01, Meta loss averaged over last 500 steps = 2.9276e-01, PNorm = 142.3872, GNorm = 0.4636
Took 112.47344946861267 seconds to complete one epoch of meta training
Took 120.1816942691803 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.440271
Epoch 586
Meta loss on this task batch = 2.9266e-01, Meta loss averaged over last 500 steps = 2.9261e-01, PNorm = 142.3915, GNorm = 0.3137
Meta loss on this task batch = 2.6734e-01, Meta loss averaged over last 500 steps = 2.9244e-01, PNorm = 142.3965, GNorm = 0.2556
Meta loss on this task batch = 3.0125e-01, Meta loss averaged over last 500 steps = 2.9247e-01, PNorm = 142.4024, GNorm = 0.2962
Meta loss on this task batch = 2.7004e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 142.4075, GNorm = 0.2443
Meta loss on this task batch = 3.1569e-01, Meta loss averaged over last 500 steps = 2.9248e-01, PNorm = 142.4123, GNorm = 0.3026
Meta loss on this task batch = 2.3806e-01, Meta loss averaged over last 500 steps = 2.9239e-01, PNorm = 142.4172, GNorm = 0.2421
Meta loss on this task batch = 2.7636e-01, Meta loss averaged over last 500 steps = 2.9237e-01, PNorm = 142.4219, GNorm = 0.3105
Meta loss on this task batch = 3.2890e-01, Meta loss averaged over last 500 steps = 2.9247e-01, PNorm = 142.4264, GNorm = 0.2530
Meta loss on this task batch = 3.1423e-01, Meta loss averaged over last 500 steps = 2.9256e-01, PNorm = 142.4305, GNorm = 0.2960
Meta loss on this task batch = 3.3483e-01, Meta loss averaged over last 500 steps = 2.9264e-01, PNorm = 142.4355, GNorm = 0.3277
Meta loss on this task batch = 2.3511e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 142.4415, GNorm = 0.3266
Meta loss on this task batch = 3.3573e-01, Meta loss averaged over last 500 steps = 2.9270e-01, PNorm = 142.4476, GNorm = 0.3217
Meta loss on this task batch = 2.8101e-01, Meta loss averaged over last 500 steps = 2.9261e-01, PNorm = 142.4536, GNorm = 0.2514
Meta loss on this task batch = 3.0991e-01, Meta loss averaged over last 500 steps = 2.9259e-01, PNorm = 142.4592, GNorm = 0.2689
Meta loss on this task batch = 3.5342e-01, Meta loss averaged over last 500 steps = 2.9271e-01, PNorm = 142.4635, GNorm = 0.3333
Meta loss on this task batch = 3.0725e-01, Meta loss averaged over last 500 steps = 2.9273e-01, PNorm = 142.4669, GNorm = 0.3156
Meta loss on this task batch = 2.7817e-01, Meta loss averaged over last 500 steps = 2.9274e-01, PNorm = 142.4699, GNorm = 0.3088
Meta loss on this task batch = 2.7988e-01, Meta loss averaged over last 500 steps = 2.9272e-01, PNorm = 142.4726, GNorm = 0.2875
Meta loss on this task batch = 3.0950e-01, Meta loss averaged over last 500 steps = 2.9277e-01, PNorm = 142.4740, GNorm = 0.3994
Took 111.48302626609802 seconds to complete one epoch of meta training
Took 119.12021565437317 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453775
Epoch 587
Meta loss on this task batch = 3.2902e-01, Meta loss averaged over last 500 steps = 2.9285e-01, PNorm = 142.4752, GNorm = 0.4148
Meta loss on this task batch = 3.3570e-01, Meta loss averaged over last 500 steps = 2.9291e-01, PNorm = 142.4767, GNorm = 0.3024
Meta loss on this task batch = 2.9972e-01, Meta loss averaged over last 500 steps = 2.9297e-01, PNorm = 142.4792, GNorm = 0.3089
Meta loss on this task batch = 2.7434e-01, Meta loss averaged over last 500 steps = 2.9295e-01, PNorm = 142.4815, GNorm = 0.3266
Meta loss on this task batch = 2.6191e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 142.4839, GNorm = 0.3236
Meta loss on this task batch = 3.0974e-01, Meta loss averaged over last 500 steps = 2.9294e-01, PNorm = 142.4868, GNorm = 0.2684
Meta loss on this task batch = 3.2189e-01, Meta loss averaged over last 500 steps = 2.9296e-01, PNorm = 142.4903, GNorm = 0.2780
Meta loss on this task batch = 3.1550e-01, Meta loss averaged over last 500 steps = 2.9303e-01, PNorm = 142.4938, GNorm = 0.2705
Meta loss on this task batch = 2.8470e-01, Meta loss averaged over last 500 steps = 2.9303e-01, PNorm = 142.4979, GNorm = 0.3249
Meta loss on this task batch = 3.1284e-01, Meta loss averaged over last 500 steps = 2.9297e-01, PNorm = 142.5012, GNorm = 0.2870
Meta loss on this task batch = 3.2586e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 142.5041, GNorm = 0.3369
Meta loss on this task batch = 2.7754e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 142.5079, GNorm = 0.3618
Meta loss on this task batch = 3.4083e-01, Meta loss averaged over last 500 steps = 2.9308e-01, PNorm = 142.5131, GNorm = 0.3598
Meta loss on this task batch = 3.1180e-01, Meta loss averaged over last 500 steps = 2.9318e-01, PNorm = 142.5184, GNorm = 0.2685
Meta loss on this task batch = 2.7294e-01, Meta loss averaged over last 500 steps = 2.9316e-01, PNorm = 142.5235, GNorm = 0.4027
Meta loss on this task batch = 2.8747e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 142.5289, GNorm = 0.2713
Meta loss on this task batch = 2.2862e-01, Meta loss averaged over last 500 steps = 2.9301e-01, PNorm = 142.5343, GNorm = 0.2992
Meta loss on this task batch = 2.6469e-01, Meta loss averaged over last 500 steps = 2.9298e-01, PNorm = 142.5398, GNorm = 0.2793
Meta loss on this task batch = 2.7272e-01, Meta loss averaged over last 500 steps = 2.9298e-01, PNorm = 142.5466, GNorm = 0.5309
Took 112.43068265914917 seconds to complete one epoch of meta training
Took 119.50637221336365 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477999
Epoch 588
Meta loss on this task batch = 2.6882e-01, Meta loss averaged over last 500 steps = 2.9287e-01, PNorm = 142.5534, GNorm = 0.2402
Meta loss on this task batch = 3.0379e-01, Meta loss averaged over last 500 steps = 2.9285e-01, PNorm = 142.5598, GNorm = 0.2890
Meta loss on this task batch = 2.9210e-01, Meta loss averaged over last 500 steps = 2.9283e-01, PNorm = 142.5664, GNorm = 0.2747
Meta loss on this task batch = 3.2105e-01, Meta loss averaged over last 500 steps = 2.9282e-01, PNorm = 142.5729, GNorm = 0.4475
Meta loss on this task batch = 3.1206e-01, Meta loss averaged over last 500 steps = 2.9285e-01, PNorm = 142.5791, GNorm = 0.3673
Meta loss on this task batch = 2.7253e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 142.5854, GNorm = 0.3225
Meta loss on this task batch = 3.7062e-01, Meta loss averaged over last 500 steps = 2.9292e-01, PNorm = 142.5900, GNorm = 0.6633
Meta loss on this task batch = 2.7201e-01, Meta loss averaged over last 500 steps = 2.9285e-01, PNorm = 142.5954, GNorm = 0.3117
Meta loss on this task batch = 2.9485e-01, Meta loss averaged over last 500 steps = 2.9279e-01, PNorm = 142.5997, GNorm = 0.3201
Meta loss on this task batch = 3.0358e-01, Meta loss averaged over last 500 steps = 2.9282e-01, PNorm = 142.6043, GNorm = 0.3202
Meta loss on this task batch = 3.2007e-01, Meta loss averaged over last 500 steps = 2.9289e-01, PNorm = 142.6090, GNorm = 0.4088
Meta loss on this task batch = 2.7692e-01, Meta loss averaged over last 500 steps = 2.9287e-01, PNorm = 142.6147, GNorm = 0.3819
Meta loss on this task batch = 3.4028e-01, Meta loss averaged over last 500 steps = 2.9293e-01, PNorm = 142.6198, GNorm = 0.3719
Meta loss on this task batch = 2.3500e-01, Meta loss averaged over last 500 steps = 2.9268e-01, PNorm = 142.6249, GNorm = 0.2758
Meta loss on this task batch = 3.0062e-01, Meta loss averaged over last 500 steps = 2.9266e-01, PNorm = 142.6304, GNorm = 0.2964
Meta loss on this task batch = 2.4568e-01, Meta loss averaged over last 500 steps = 2.9266e-01, PNorm = 142.6363, GNorm = 0.2160
Meta loss on this task batch = 3.3947e-01, Meta loss averaged over last 500 steps = 2.9266e-01, PNorm = 142.6420, GNorm = 0.2931
Meta loss on this task batch = 3.3716e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 142.6465, GNorm = 0.2942
Meta loss on this task batch = 2.3886e-01, Meta loss averaged over last 500 steps = 2.9265e-01, PNorm = 142.6514, GNorm = 0.2979
Took 115.11476707458496 seconds to complete one epoch of meta training
Took 123.4937949180603 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475016
Epoch 589
Meta loss on this task batch = 3.0848e-01, Meta loss averaged over last 500 steps = 2.9274e-01, PNorm = 142.6568, GNorm = 0.2927
Meta loss on this task batch = 2.4686e-01, Meta loss averaged over last 500 steps = 2.9266e-01, PNorm = 142.6628, GNorm = 0.2351
Meta loss on this task batch = 3.1365e-01, Meta loss averaged over last 500 steps = 2.9277e-01, PNorm = 142.6681, GNorm = 0.2741
Meta loss on this task batch = 2.4877e-01, Meta loss averaged over last 500 steps = 2.9267e-01, PNorm = 142.6738, GNorm = 0.2284
Meta loss on this task batch = 2.6511e-01, Meta loss averaged over last 500 steps = 2.9257e-01, PNorm = 142.6792, GNorm = 0.2481
Meta loss on this task batch = 2.9926e-01, Meta loss averaged over last 500 steps = 2.9258e-01, PNorm = 142.6842, GNorm = 0.2916
Meta loss on this task batch = 3.1805e-01, Meta loss averaged over last 500 steps = 2.9262e-01, PNorm = 142.6881, GNorm = 0.3357
Meta loss on this task batch = 2.5166e-01, Meta loss averaged over last 500 steps = 2.9248e-01, PNorm = 142.6922, GNorm = 0.2602
Meta loss on this task batch = 2.7806e-01, Meta loss averaged over last 500 steps = 2.9239e-01, PNorm = 142.6963, GNorm = 0.2409
Meta loss on this task batch = 2.8977e-01, Meta loss averaged over last 500 steps = 2.9243e-01, PNorm = 142.7000, GNorm = 0.4402
Meta loss on this task batch = 3.1773e-01, Meta loss averaged over last 500 steps = 2.9248e-01, PNorm = 142.7029, GNorm = 0.3417
Meta loss on this task batch = 2.8382e-01, Meta loss averaged over last 500 steps = 2.9245e-01, PNorm = 142.7058, GNorm = 0.2798
Meta loss on this task batch = 3.8507e-01, Meta loss averaged over last 500 steps = 2.9256e-01, PNorm = 142.7087, GNorm = 0.3942
Meta loss on this task batch = 3.0696e-01, Meta loss averaged over last 500 steps = 2.9271e-01, PNorm = 142.7116, GNorm = 0.3025
Meta loss on this task batch = 2.2024e-01, Meta loss averaged over last 500 steps = 2.9250e-01, PNorm = 142.7152, GNorm = 0.3154
Meta loss on this task batch = 3.1986e-01, Meta loss averaged over last 500 steps = 2.9261e-01, PNorm = 142.7196, GNorm = 0.3065
Meta loss on this task batch = 2.6226e-01, Meta loss averaged over last 500 steps = 2.9257e-01, PNorm = 142.7248, GNorm = 0.2331
Meta loss on this task batch = 2.3545e-01, Meta loss averaged over last 500 steps = 2.9245e-01, PNorm = 142.7306, GNorm = 0.4532
Meta loss on this task batch = 3.5290e-01, Meta loss averaged over last 500 steps = 2.9264e-01, PNorm = 142.7355, GNorm = 0.3656
Took 112.00222158432007 seconds to complete one epoch of meta training
Took 119.60249376296997 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451180
Epoch 590
Meta loss on this task batch = 2.8416e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 142.7402, GNorm = 0.2850
Meta loss on this task batch = 3.1481e-01, Meta loss averaged over last 500 steps = 2.9255e-01, PNorm = 142.7442, GNorm = 0.2907
Meta loss on this task batch = 2.8665e-01, Meta loss averaged over last 500 steps = 2.9250e-01, PNorm = 142.7487, GNorm = 0.2808
Meta loss on this task batch = 2.5795e-01, Meta loss averaged over last 500 steps = 2.9245e-01, PNorm = 142.7538, GNorm = 0.2691
Meta loss on this task batch = 3.0283e-01, Meta loss averaged over last 500 steps = 2.9251e-01, PNorm = 142.7585, GNorm = 0.3220
Meta loss on this task batch = 2.4628e-01, Meta loss averaged over last 500 steps = 2.9252e-01, PNorm = 142.7636, GNorm = 0.3781
Meta loss on this task batch = 3.2883e-01, Meta loss averaged over last 500 steps = 2.9265e-01, PNorm = 142.7684, GNorm = 0.2969
Meta loss on this task batch = 2.5810e-01, Meta loss averaged over last 500 steps = 2.9249e-01, PNorm = 142.7732, GNorm = 0.3356
Meta loss on this task batch = 2.8121e-01, Meta loss averaged over last 500 steps = 2.9248e-01, PNorm = 142.7783, GNorm = 0.2772
Meta loss on this task batch = 3.2032e-01, Meta loss averaged over last 500 steps = 2.9250e-01, PNorm = 142.7828, GNorm = 0.3211
Meta loss on this task batch = 2.9960e-01, Meta loss averaged over last 500 steps = 2.9256e-01, PNorm = 142.7877, GNorm = 0.2803
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 2.9251e-01, PNorm = 142.7922, GNorm = 0.3813
Meta loss on this task batch = 2.3072e-01, Meta loss averaged over last 500 steps = 2.9232e-01, PNorm = 142.7967, GNorm = 0.2672
Meta loss on this task batch = 2.8371e-01, Meta loss averaged over last 500 steps = 2.9223e-01, PNorm = 142.8006, GNorm = 0.2874
Meta loss on this task batch = 2.7098e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 142.8042, GNorm = 0.3088
Meta loss on this task batch = 3.8131e-01, Meta loss averaged over last 500 steps = 2.9226e-01, PNorm = 142.8074, GNorm = 0.4737
Meta loss on this task batch = 2.8511e-01, Meta loss averaged over last 500 steps = 2.9229e-01, PNorm = 142.8111, GNorm = 0.4503
Meta loss on this task batch = 3.0271e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 142.8148, GNorm = 0.2500
Meta loss on this task batch = 2.6230e-01, Meta loss averaged over last 500 steps = 2.9219e-01, PNorm = 142.8188, GNorm = 0.3316
Took 110.35893249511719 seconds to complete one epoch of meta training
Took 117.96433234214783 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467026
Epoch 591
Meta loss on this task batch = 2.9057e-01, Meta loss averaged over last 500 steps = 2.9218e-01, PNorm = 142.8225, GNorm = 0.3091
Meta loss on this task batch = 2.7987e-01, Meta loss averaged over last 500 steps = 2.9212e-01, PNorm = 142.8264, GNorm = 0.3733
Meta loss on this task batch = 2.9671e-01, Meta loss averaged over last 500 steps = 2.9214e-01, PNorm = 142.8311, GNorm = 0.2465
Meta loss on this task batch = 2.6535e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 142.8362, GNorm = 0.2246
Meta loss on this task batch = 2.6216e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 142.8422, GNorm = 0.3004
Meta loss on this task batch = 2.6482e-01, Meta loss averaged over last 500 steps = 2.9186e-01, PNorm = 142.8484, GNorm = 0.3638
Meta loss on this task batch = 3.0000e-01, Meta loss averaged over last 500 steps = 2.9187e-01, PNorm = 142.8544, GNorm = 0.2861
Meta loss on this task batch = 2.8469e-01, Meta loss averaged over last 500 steps = 2.9185e-01, PNorm = 142.8603, GNorm = 0.2582
Meta loss on this task batch = 3.4053e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 142.8652, GNorm = 0.3253
Meta loss on this task batch = 2.6330e-01, Meta loss averaged over last 500 steps = 2.9202e-01, PNorm = 142.8711, GNorm = 0.2480
Meta loss on this task batch = 2.8329e-01, Meta loss averaged over last 500 steps = 2.9199e-01, PNorm = 142.8769, GNorm = 0.3054
Meta loss on this task batch = 2.6372e-01, Meta loss averaged over last 500 steps = 2.9180e-01, PNorm = 142.8827, GNorm = 0.3811
Meta loss on this task batch = 2.3364e-01, Meta loss averaged over last 500 steps = 2.9160e-01, PNorm = 142.8896, GNorm = 0.3556
Meta loss on this task batch = 3.0812e-01, Meta loss averaged over last 500 steps = 2.9159e-01, PNorm = 142.8952, GNorm = 0.4127
Meta loss on this task batch = 3.2014e-01, Meta loss averaged over last 500 steps = 2.9160e-01, PNorm = 142.9004, GNorm = 0.2941
Meta loss on this task batch = 3.3239e-01, Meta loss averaged over last 500 steps = 2.9162e-01, PNorm = 142.9049, GNorm = 0.2820
Meta loss on this task batch = 3.5816e-01, Meta loss averaged over last 500 steps = 2.9185e-01, PNorm = 142.9089, GNorm = 0.3156
Meta loss on this task batch = 3.0100e-01, Meta loss averaged over last 500 steps = 2.9190e-01, PNorm = 142.9133, GNorm = 0.3330
Meta loss on this task batch = 3.7274e-01, Meta loss averaged over last 500 steps = 2.9212e-01, PNorm = 142.9172, GNorm = 0.5253
Took 112.01586174964905 seconds to complete one epoch of meta training
Took 119.93936228752136 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460083
Epoch 592
Meta loss on this task batch = 2.8157e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 142.9217, GNorm = 0.2893
Meta loss on this task batch = 3.7270e-01, Meta loss averaged over last 500 steps = 2.9226e-01, PNorm = 142.9254, GNorm = 0.3612
Meta loss on this task batch = 2.6394e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 142.9297, GNorm = 0.3096
Meta loss on this task batch = 3.3364e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 142.9341, GNorm = 0.3282
Meta loss on this task batch = 2.7225e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 142.9386, GNorm = 0.3205
Meta loss on this task batch = 2.5694e-01, Meta loss averaged over last 500 steps = 2.9217e-01, PNorm = 142.9432, GNorm = 0.2770
Meta loss on this task batch = 2.8545e-01, Meta loss averaged over last 500 steps = 2.9220e-01, PNorm = 142.9479, GNorm = 0.3044
Meta loss on this task batch = 2.8909e-01, Meta loss averaged over last 500 steps = 2.9211e-01, PNorm = 142.9530, GNorm = 0.3827
Meta loss on this task batch = 2.9255e-01, Meta loss averaged over last 500 steps = 2.9201e-01, PNorm = 142.9587, GNorm = 0.4262
Meta loss on this task batch = 3.2750e-01, Meta loss averaged over last 500 steps = 2.9210e-01, PNorm = 142.9647, GNorm = 0.2941
Meta loss on this task batch = 2.8651e-01, Meta loss averaged over last 500 steps = 2.9221e-01, PNorm = 142.9705, GNorm = 0.2553
Meta loss on this task batch = 2.9384e-01, Meta loss averaged over last 500 steps = 2.9225e-01, PNorm = 142.9766, GNorm = 0.3240
Meta loss on this task batch = 2.8560e-01, Meta loss averaged over last 500 steps = 2.9222e-01, PNorm = 142.9823, GNorm = 0.2628
Meta loss on this task batch = 2.1428e-01, Meta loss averaged over last 500 steps = 2.9200e-01, PNorm = 142.9878, GNorm = 0.2195
Meta loss on this task batch = 3.0073e-01, Meta loss averaged over last 500 steps = 2.9200e-01, PNorm = 142.9929, GNorm = 0.3565
Meta loss on this task batch = 2.6868e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 142.9972, GNorm = 0.4587
Meta loss on this task batch = 3.1770e-01, Meta loss averaged over last 500 steps = 2.9210e-01, PNorm = 143.0008, GNorm = 0.3698
Meta loss on this task batch = 2.9457e-01, Meta loss averaged over last 500 steps = 2.9208e-01, PNorm = 143.0040, GNorm = 0.7026
Meta loss on this task batch = 2.8196e-01, Meta loss averaged over last 500 steps = 2.9205e-01, PNorm = 143.0080, GNorm = 0.3101
Took 113.67721390724182 seconds to complete one epoch of meta training
Took 121.41807508468628 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464930
Epoch 593
Meta loss on this task batch = 2.4934e-01, Meta loss averaged over last 500 steps = 2.9201e-01, PNorm = 143.0114, GNorm = 0.2786
Meta loss on this task batch = 3.0311e-01, Meta loss averaged over last 500 steps = 2.9200e-01, PNorm = 143.0144, GNorm = 0.2910
Meta loss on this task batch = 3.3364e-01, Meta loss averaged over last 500 steps = 2.9207e-01, PNorm = 143.0177, GNorm = 0.3080
Meta loss on this task batch = 2.5212e-01, Meta loss averaged over last 500 steps = 2.9192e-01, PNorm = 143.0214, GNorm = 0.2283
Meta loss on this task batch = 3.2462e-01, Meta loss averaged over last 500 steps = 2.9198e-01, PNorm = 143.0252, GNorm = 0.4231
Meta loss on this task batch = 2.7912e-01, Meta loss averaged over last 500 steps = 2.9201e-01, PNorm = 143.0290, GNorm = 0.3197
Meta loss on this task batch = 2.6709e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 143.0334, GNorm = 0.2832
Meta loss on this task batch = 3.0102e-01, Meta loss averaged over last 500 steps = 2.9211e-01, PNorm = 143.0372, GNorm = 0.2993
Meta loss on this task batch = 3.3475e-01, Meta loss averaged over last 500 steps = 2.9232e-01, PNorm = 143.0403, GNorm = 0.3621
Meta loss on this task batch = 2.8574e-01, Meta loss averaged over last 500 steps = 2.9235e-01, PNorm = 143.0436, GNorm = 0.2666
Meta loss on this task batch = 3.6696e-01, Meta loss averaged over last 500 steps = 2.9252e-01, PNorm = 143.0469, GNorm = 0.3680
Meta loss on this task batch = 2.9644e-01, Meta loss averaged over last 500 steps = 2.9256e-01, PNorm = 143.0503, GNorm = 0.3402
Meta loss on this task batch = 2.8277e-01, Meta loss averaged over last 500 steps = 2.9244e-01, PNorm = 143.0542, GNorm = 0.2763
Meta loss on this task batch = 2.9281e-01, Meta loss averaged over last 500 steps = 2.9231e-01, PNorm = 143.0580, GNorm = 0.3198
Meta loss on this task batch = 2.8776e-01, Meta loss averaged over last 500 steps = 2.9218e-01, PNorm = 143.0622, GNorm = 0.2430
Meta loss on this task batch = 2.5068e-01, Meta loss averaged over last 500 steps = 2.9201e-01, PNorm = 143.0669, GNorm = 0.2375
Meta loss on this task batch = 3.1165e-01, Meta loss averaged over last 500 steps = 2.9206e-01, PNorm = 143.0725, GNorm = 0.3688
Meta loss on this task batch = 3.1640e-01, Meta loss averaged over last 500 steps = 2.9212e-01, PNorm = 143.0785, GNorm = 0.3721
Meta loss on this task batch = 2.9028e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 143.0843, GNorm = 0.2798
Took 110.19630098342896 seconds to complete one epoch of meta training
Took 117.93068170547485 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463125
Epoch 594
Meta loss on this task batch = 2.6289e-01, Meta loss averaged over last 500 steps = 2.9202e-01, PNorm = 143.0900, GNorm = 0.2799
Meta loss on this task batch = 2.8805e-01, Meta loss averaged over last 500 steps = 2.9213e-01, PNorm = 143.0948, GNorm = 0.5409
Meta loss on this task batch = 2.6653e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 143.0994, GNorm = 0.2673
Meta loss on this task batch = 3.0286e-01, Meta loss averaged over last 500 steps = 2.9212e-01, PNorm = 143.1045, GNorm = 0.2733
Meta loss on this task batch = 3.2774e-01, Meta loss averaged over last 500 steps = 2.9219e-01, PNorm = 143.1097, GNorm = 0.3438
Meta loss on this task batch = 2.9437e-01, Meta loss averaged over last 500 steps = 2.9221e-01, PNorm = 143.1154, GNorm = 0.3856
Meta loss on this task batch = 2.7067e-01, Meta loss averaged over last 500 steps = 2.9221e-01, PNorm = 143.1207, GNorm = 0.2773
Meta loss on this task batch = 2.4738e-01, Meta loss averaged over last 500 steps = 2.9214e-01, PNorm = 143.1262, GNorm = 0.4508
Meta loss on this task batch = 3.4837e-01, Meta loss averaged over last 500 steps = 2.9231e-01, PNorm = 143.1309, GNorm = 0.3328
Meta loss on this task batch = 2.7346e-01, Meta loss averaged over last 500 steps = 2.9222e-01, PNorm = 143.1359, GNorm = 0.3697
Meta loss on this task batch = 2.8880e-01, Meta loss averaged over last 500 steps = 2.9220e-01, PNorm = 143.1410, GNorm = 0.3052
Meta loss on this task batch = 3.2769e-01, Meta loss averaged over last 500 steps = 2.9227e-01, PNorm = 143.1460, GNorm = 0.3490
Meta loss on this task batch = 3.3086e-01, Meta loss averaged over last 500 steps = 2.9231e-01, PNorm = 143.1515, GNorm = 0.4004
Meta loss on this task batch = 2.3100e-01, Meta loss averaged over last 500 steps = 2.9212e-01, PNorm = 143.1578, GNorm = 0.2570
Meta loss on this task batch = 2.8894e-01, Meta loss averaged over last 500 steps = 2.9196e-01, PNorm = 143.1630, GNorm = 0.3748
Meta loss on this task batch = 3.0878e-01, Meta loss averaged over last 500 steps = 2.9201e-01, PNorm = 143.1682, GNorm = 0.4738
Meta loss on this task batch = 3.1356e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 143.1734, GNorm = 0.4868
Meta loss on this task batch = 3.0879e-01, Meta loss averaged over last 500 steps = 2.9225e-01, PNorm = 143.1788, GNorm = 0.2643
Meta loss on this task batch = 3.3466e-01, Meta loss averaged over last 500 steps = 2.9235e-01, PNorm = 143.1835, GNorm = 0.3660
Took 111.43200707435608 seconds to complete one epoch of meta training
Took 118.21890759468079 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483783
Epoch 595
Meta loss on this task batch = 2.6171e-01, Meta loss averaged over last 500 steps = 2.9235e-01, PNorm = 143.1880, GNorm = 0.2939
Meta loss on this task batch = 2.6356e-01, Meta loss averaged over last 500 steps = 2.9239e-01, PNorm = 143.1921, GNorm = 0.3013
Meta loss on this task batch = 2.7876e-01, Meta loss averaged over last 500 steps = 2.9241e-01, PNorm = 143.1965, GNorm = 0.3448
Meta loss on this task batch = 2.5851e-01, Meta loss averaged over last 500 steps = 2.9234e-01, PNorm = 143.2011, GNorm = 0.3736
Meta loss on this task batch = 3.2802e-01, Meta loss averaged over last 500 steps = 2.9229e-01, PNorm = 143.2064, GNorm = 0.3547
Meta loss on this task batch = 2.6841e-01, Meta loss averaged over last 500 steps = 2.9232e-01, PNorm = 143.2112, GNorm = 0.3409
Meta loss on this task batch = 2.8755e-01, Meta loss averaged over last 500 steps = 2.9227e-01, PNorm = 143.2159, GNorm = 0.2941
Meta loss on this task batch = 3.0902e-01, Meta loss averaged over last 500 steps = 2.9235e-01, PNorm = 143.2200, GNorm = 0.4422
Meta loss on this task batch = 2.8804e-01, Meta loss averaged over last 500 steps = 2.9233e-01, PNorm = 143.2238, GNorm = 0.2663
Meta loss on this task batch = 2.7886e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 143.2278, GNorm = 0.3226
Meta loss on this task batch = 2.7144e-01, Meta loss averaged over last 500 steps = 2.9223e-01, PNorm = 143.2307, GNorm = 0.3331
Meta loss on this task batch = 3.4851e-01, Meta loss averaged over last 500 steps = 2.9239e-01, PNorm = 143.2326, GNorm = 0.3427
Meta loss on this task batch = 2.7788e-01, Meta loss averaged over last 500 steps = 2.9240e-01, PNorm = 143.2352, GNorm = 0.2598
Meta loss on this task batch = 2.5814e-01, Meta loss averaged over last 500 steps = 2.9244e-01, PNorm = 143.2389, GNorm = 0.3228
Meta loss on this task batch = 2.8617e-01, Meta loss averaged over last 500 steps = 2.9246e-01, PNorm = 143.2420, GNorm = 0.2894
Meta loss on this task batch = 2.9109e-01, Meta loss averaged over last 500 steps = 2.9241e-01, PNorm = 143.2447, GNorm = 0.2560
Meta loss on this task batch = 2.7275e-01, Meta loss averaged over last 500 steps = 2.9236e-01, PNorm = 143.2481, GNorm = 0.2714
Meta loss on this task batch = 3.0894e-01, Meta loss averaged over last 500 steps = 2.9239e-01, PNorm = 143.2515, GNorm = 0.3465
Meta loss on this task batch = 3.9960e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 143.2542, GNorm = 0.4151
Took 112.3642680644989 seconds to complete one epoch of meta training
Took 120.57631087303162 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465930
Epoch 596
Meta loss on this task batch = 3.4255e-01, Meta loss averaged over last 500 steps = 2.9269e-01, PNorm = 143.2568, GNorm = 0.3173
Meta loss on this task batch = 2.9246e-01, Meta loss averaged over last 500 steps = 2.9273e-01, PNorm = 143.2600, GNorm = 0.2676
Meta loss on this task batch = 3.1198e-01, Meta loss averaged over last 500 steps = 2.9277e-01, PNorm = 143.2636, GNorm = 0.3241
Meta loss on this task batch = 2.6163e-01, Meta loss averaged over last 500 steps = 2.9274e-01, PNorm = 143.2676, GNorm = 0.2689
Meta loss on this task batch = 3.1595e-01, Meta loss averaged over last 500 steps = 2.9276e-01, PNorm = 143.2721, GNorm = 0.2682
Meta loss on this task batch = 2.9585e-01, Meta loss averaged over last 500 steps = 2.9277e-01, PNorm = 143.2773, GNorm = 0.2616
Meta loss on this task batch = 3.0524e-01, Meta loss averaged over last 500 steps = 2.9268e-01, PNorm = 143.2835, GNorm = 0.3358
Meta loss on this task batch = 2.6891e-01, Meta loss averaged over last 500 steps = 2.9260e-01, PNorm = 143.2904, GNorm = 0.2529
Meta loss on this task batch = 2.7881e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 143.2973, GNorm = 0.3522
Meta loss on this task batch = 2.1206e-01, Meta loss averaged over last 500 steps = 2.9250e-01, PNorm = 143.3049, GNorm = 0.2393
Meta loss on this task batch = 2.1716e-01, Meta loss averaged over last 500 steps = 2.9244e-01, PNorm = 143.3129, GNorm = 0.2279
Meta loss on this task batch = 3.5250e-01, Meta loss averaged over last 500 steps = 2.9264e-01, PNorm = 143.3205, GNorm = 0.5134
Meta loss on this task batch = 2.9935e-01, Meta loss averaged over last 500 steps = 2.9263e-01, PNorm = 143.3272, GNorm = 0.3033
Meta loss on this task batch = 2.6651e-01, Meta loss averaged over last 500 steps = 2.9258e-01, PNorm = 143.3335, GNorm = 0.2924
Meta loss on this task batch = 2.5582e-01, Meta loss averaged over last 500 steps = 2.9249e-01, PNorm = 143.3400, GNorm = 0.2765
Meta loss on this task batch = 3.2479e-01, Meta loss averaged over last 500 steps = 2.9249e-01, PNorm = 143.3457, GNorm = 0.3817
Meta loss on this task batch = 2.6378e-01, Meta loss averaged over last 500 steps = 2.9250e-01, PNorm = 143.3514, GNorm = 0.3553
Meta loss on this task batch = 2.1652e-01, Meta loss averaged over last 500 steps = 2.9234e-01, PNorm = 143.3579, GNorm = 0.2895
Meta loss on this task batch = 3.8488e-01, Meta loss averaged over last 500 steps = 2.9243e-01, PNorm = 143.3642, GNorm = 0.4135
Took 114.61191964149475 seconds to complete one epoch of meta training
Took 122.72038650512695 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.442019
Epoch 597
Meta loss on this task batch = 2.9267e-01, Meta loss averaged over last 500 steps = 2.9255e-01, PNorm = 143.3698, GNorm = 0.3169
Meta loss on this task batch = 2.7916e-01, Meta loss averaged over last 500 steps = 2.9253e-01, PNorm = 143.3748, GNorm = 0.3109
Meta loss on this task batch = 2.5867e-01, Meta loss averaged over last 500 steps = 2.9249e-01, PNorm = 143.3790, GNorm = 0.2948
Meta loss on this task batch = 3.4334e-01, Meta loss averaged over last 500 steps = 2.9249e-01, PNorm = 143.3822, GNorm = 0.3139
Meta loss on this task batch = 3.0297e-01, Meta loss averaged over last 500 steps = 2.9247e-01, PNorm = 143.3850, GNorm = 0.3015
Meta loss on this task batch = 2.5442e-01, Meta loss averaged over last 500 steps = 2.9237e-01, PNorm = 143.3876, GNorm = 0.2669
Meta loss on this task batch = 2.5878e-01, Meta loss averaged over last 500 steps = 2.9228e-01, PNorm = 143.3907, GNorm = 0.2888
Meta loss on this task batch = 3.0589e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 143.3941, GNorm = 0.3587
Meta loss on this task batch = 3.1943e-01, Meta loss averaged over last 500 steps = 2.9226e-01, PNorm = 143.3977, GNorm = 0.2726
Meta loss on this task batch = 3.5969e-01, Meta loss averaged over last 500 steps = 2.9255e-01, PNorm = 143.4002, GNorm = 0.4519
Meta loss on this task batch = 2.8548e-01, Meta loss averaged over last 500 steps = 2.9263e-01, PNorm = 143.4025, GNorm = 0.2919
Meta loss on this task batch = 3.0672e-01, Meta loss averaged over last 500 steps = 2.9270e-01, PNorm = 143.4053, GNorm = 0.3620
Meta loss on this task batch = 3.1486e-01, Meta loss averaged over last 500 steps = 2.9271e-01, PNorm = 143.4080, GNorm = 0.2815
Meta loss on this task batch = 2.5982e-01, Meta loss averaged over last 500 steps = 2.9269e-01, PNorm = 143.4110, GNorm = 0.2527
Meta loss on this task batch = 2.7485e-01, Meta loss averaged over last 500 steps = 2.9272e-01, PNorm = 143.4140, GNorm = 0.2376
Meta loss on this task batch = 3.0535e-01, Meta loss averaged over last 500 steps = 2.9271e-01, PNorm = 143.4174, GNorm = 0.4042
Meta loss on this task batch = 2.8695e-01, Meta loss averaged over last 500 steps = 2.9270e-01, PNorm = 143.4212, GNorm = 0.2782
Meta loss on this task batch = 2.4777e-01, Meta loss averaged over last 500 steps = 2.9260e-01, PNorm = 143.4255, GNorm = 0.2858
Meta loss on this task batch = 3.1893e-01, Meta loss averaged over last 500 steps = 2.9265e-01, PNorm = 143.4308, GNorm = 0.4324
Took 113.19607138633728 seconds to complete one epoch of meta training
Took 121.0509819984436 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.435093
Epoch 598
Meta loss on this task batch = 2.1427e-01, Meta loss averaged over last 500 steps = 2.9255e-01, PNorm = 143.4369, GNorm = 0.2628
Meta loss on this task batch = 3.0284e-01, Meta loss averaged over last 500 steps = 2.9241e-01, PNorm = 143.4422, GNorm = 0.4504
Meta loss on this task batch = 2.2549e-01, Meta loss averaged over last 500 steps = 2.9228e-01, PNorm = 143.4480, GNorm = 0.2290
Meta loss on this task batch = 3.0296e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 143.4543, GNorm = 0.2639
Meta loss on this task batch = 3.1944e-01, Meta loss averaged over last 500 steps = 2.9234e-01, PNorm = 143.4604, GNorm = 0.2989
Meta loss on this task batch = 2.8647e-01, Meta loss averaged over last 500 steps = 2.9229e-01, PNorm = 143.4668, GNorm = 0.4394
Meta loss on this task batch = 3.1154e-01, Meta loss averaged over last 500 steps = 2.9240e-01, PNorm = 143.4716, GNorm = 0.2710
Meta loss on this task batch = 3.6946e-01, Meta loss averaged over last 500 steps = 2.9253e-01, PNorm = 143.4740, GNorm = 0.3843
Meta loss on this task batch = 2.6594e-01, Meta loss averaged over last 500 steps = 2.9239e-01, PNorm = 143.4770, GNorm = 0.2422
Meta loss on this task batch = 3.1239e-01, Meta loss averaged over last 500 steps = 2.9249e-01, PNorm = 143.4811, GNorm = 0.3211
Meta loss on this task batch = 3.0629e-01, Meta loss averaged over last 500 steps = 2.9248e-01, PNorm = 143.4857, GNorm = 0.3610
Meta loss on this task batch = 3.7125e-01, Meta loss averaged over last 500 steps = 2.9273e-01, PNorm = 143.4899, GNorm = 0.4415
Meta loss on this task batch = 3.1328e-01, Meta loss averaged over last 500 steps = 2.9274e-01, PNorm = 143.4941, GNorm = 0.3126
Meta loss on this task batch = 3.2707e-01, Meta loss averaged over last 500 steps = 2.9272e-01, PNorm = 143.4982, GNorm = 0.2851
Meta loss on this task batch = 2.8459e-01, Meta loss averaged over last 500 steps = 2.9259e-01, PNorm = 143.5035, GNorm = 0.2762
Meta loss on this task batch = 3.1651e-01, Meta loss averaged over last 500 steps = 2.9272e-01, PNorm = 143.5091, GNorm = 0.3960
Meta loss on this task batch = 2.9413e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 143.5154, GNorm = 0.3587
Meta loss on this task batch = 2.7193e-01, Meta loss averaged over last 500 steps = 2.9276e-01, PNorm = 143.5215, GNorm = 0.2616
Meta loss on this task batch = 1.9144e-01, Meta loss averaged over last 500 steps = 2.9253e-01, PNorm = 143.5281, GNorm = 0.3296
Took 111.86693096160889 seconds to complete one epoch of meta training
Took 119.46089625358582 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461033
Epoch 599
Meta loss on this task batch = 2.9713e-01, Meta loss averaged over last 500 steps = 2.9252e-01, PNorm = 143.5337, GNorm = 0.3524
Meta loss on this task batch = 2.5256e-01, Meta loss averaged over last 500 steps = 2.9244e-01, PNorm = 143.5400, GNorm = 0.3924
Meta loss on this task batch = 2.5125e-01, Meta loss averaged over last 500 steps = 2.9237e-01, PNorm = 143.5463, GNorm = 0.2777
Meta loss on this task batch = 3.0791e-01, Meta loss averaged over last 500 steps = 2.9237e-01, PNorm = 143.5521, GNorm = 0.4415
Meta loss on this task batch = 2.6283e-01, Meta loss averaged over last 500 steps = 2.9233e-01, PNorm = 143.5572, GNorm = 0.2696
Meta loss on this task batch = 2.5284e-01, Meta loss averaged over last 500 steps = 2.9217e-01, PNorm = 143.5626, GNorm = 0.3655
Meta loss on this task batch = 3.0698e-01, Meta loss averaged over last 500 steps = 2.9222e-01, PNorm = 143.5680, GNorm = 0.2539
Meta loss on this task batch = 2.0101e-01, Meta loss averaged over last 500 steps = 2.9205e-01, PNorm = 143.5737, GNorm = 0.3083
Meta loss on this task batch = 2.6363e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 143.5789, GNorm = 0.3875
Meta loss on this task batch = 2.9488e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 143.5835, GNorm = 0.2322
Meta loss on this task batch = 2.6784e-01, Meta loss averaged over last 500 steps = 2.9202e-01, PNorm = 143.5877, GNorm = 0.3079
Meta loss on this task batch = 3.1859e-01, Meta loss averaged over last 500 steps = 2.9211e-01, PNorm = 143.5916, GNorm = 0.2841
Meta loss on this task batch = 3.2483e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 143.5944, GNorm = 0.4223
Meta loss on this task batch = 3.1763e-01, Meta loss averaged over last 500 steps = 2.9198e-01, PNorm = 143.5969, GNorm = 0.2799
Meta loss on this task batch = 3.2642e-01, Meta loss averaged over last 500 steps = 2.9212e-01, PNorm = 143.5995, GNorm = 0.3751
Meta loss on this task batch = 2.8691e-01, Meta loss averaged over last 500 steps = 2.9211e-01, PNorm = 143.6022, GNorm = 0.3415
Meta loss on this task batch = 3.0018e-01, Meta loss averaged over last 500 steps = 2.9215e-01, PNorm = 143.6047, GNorm = 0.3309
Meta loss on this task batch = 2.8886e-01, Meta loss averaged over last 500 steps = 2.9208e-01, PNorm = 143.6078, GNorm = 0.2865
Meta loss on this task batch = 2.5873e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 143.6113, GNorm = 0.3571
Took 109.97501730918884 seconds to complete one epoch of meta training
Took 117.81036901473999 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461510
Epoch 600
Meta loss on this task batch = 2.6740e-01, Meta loss averaged over last 500 steps = 2.9200e-01, PNorm = 143.6158, GNorm = 0.2578
Meta loss on this task batch = 3.1947e-01, Meta loss averaged over last 500 steps = 2.9198e-01, PNorm = 143.6197, GNorm = 0.2612
Meta loss on this task batch = 2.9276e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 143.6247, GNorm = 0.3693
Meta loss on this task batch = 3.1890e-01, Meta loss averaged over last 500 steps = 2.9222e-01, PNorm = 143.6301, GNorm = 0.3025
Meta loss on this task batch = 2.4487e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 143.6352, GNorm = 0.2681
Meta loss on this task batch = 3.1918e-01, Meta loss averaged over last 500 steps = 2.9225e-01, PNorm = 143.6400, GNorm = 0.2760
Meta loss on this task batch = 2.9407e-01, Meta loss averaged over last 500 steps = 2.9226e-01, PNorm = 143.6452, GNorm = 0.3323
Meta loss on this task batch = 3.2503e-01, Meta loss averaged over last 500 steps = 2.9235e-01, PNorm = 143.6507, GNorm = 0.2646
Meta loss on this task batch = 2.7025e-01, Meta loss averaged over last 500 steps = 2.9239e-01, PNorm = 143.6567, GNorm = 0.4428
Meta loss on this task batch = 2.9553e-01, Meta loss averaged over last 500 steps = 2.9240e-01, PNorm = 143.6629, GNorm = 0.3289
Meta loss on this task batch = 3.2207e-01, Meta loss averaged over last 500 steps = 2.9242e-01, PNorm = 143.6686, GNorm = 0.2653
Meta loss on this task batch = 2.6202e-01, Meta loss averaged over last 500 steps = 2.9237e-01, PNorm = 143.6742, GNorm = 0.3099
Meta loss on this task batch = 2.6350e-01, Meta loss averaged over last 500 steps = 2.9231e-01, PNorm = 143.6793, GNorm = 0.3223
Meta loss on this task batch = 2.6193e-01, Meta loss averaged over last 500 steps = 2.9232e-01, PNorm = 143.6841, GNorm = 0.2687
Meta loss on this task batch = 3.2878e-01, Meta loss averaged over last 500 steps = 2.9231e-01, PNorm = 143.6881, GNorm = 0.3521
Meta loss on this task batch = 2.6838e-01, Meta loss averaged over last 500 steps = 2.9222e-01, PNorm = 143.6919, GNorm = 0.3098
Meta loss on this task batch = 2.8139e-01, Meta loss averaged over last 500 steps = 2.9207e-01, PNorm = 143.6965, GNorm = 0.2675
Meta loss on this task batch = 3.6235e-01, Meta loss averaged over last 500 steps = 2.9218e-01, PNorm = 143.7002, GNorm = 0.3131
Meta loss on this task batch = 2.3725e-01, Meta loss averaged over last 500 steps = 2.9207e-01, PNorm = 143.7049, GNorm = 0.3163
Took 110.95199203491211 seconds to complete one epoch of meta training
Took 118.3889389038086 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451357
Epoch 601
Meta loss on this task batch = 3.0870e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 143.7104, GNorm = 0.4422
Meta loss on this task batch = 2.0411e-01, Meta loss averaged over last 500 steps = 2.9185e-01, PNorm = 143.7161, GNorm = 0.3061
Meta loss on this task batch = 2.8126e-01, Meta loss averaged over last 500 steps = 2.9184e-01, PNorm = 143.7217, GNorm = 0.3649
Meta loss on this task batch = 2.7833e-01, Meta loss averaged over last 500 steps = 2.9177e-01, PNorm = 143.7278, GNorm = 0.2820
Meta loss on this task batch = 3.3602e-01, Meta loss averaged over last 500 steps = 2.9189e-01, PNorm = 143.7329, GNorm = 0.4050
Meta loss on this task batch = 3.0443e-01, Meta loss averaged over last 500 steps = 2.9187e-01, PNorm = 143.7368, GNorm = 0.3985
Meta loss on this task batch = 2.9396e-01, Meta loss averaged over last 500 steps = 2.9182e-01, PNorm = 143.7406, GNorm = 0.2788
Meta loss on this task batch = 2.6959e-01, Meta loss averaged over last 500 steps = 2.9180e-01, PNorm = 143.7445, GNorm = 0.2810
Meta loss on this task batch = 3.2404e-01, Meta loss averaged over last 500 steps = 2.9183e-01, PNorm = 143.7486, GNorm = 0.2782
Meta loss on this task batch = 2.5893e-01, Meta loss averaged over last 500 steps = 2.9183e-01, PNorm = 143.7524, GNorm = 0.3504
Meta loss on this task batch = 3.2984e-01, Meta loss averaged over last 500 steps = 2.9199e-01, PNorm = 143.7559, GNorm = 0.3134
Meta loss on this task batch = 2.5848e-01, Meta loss averaged over last 500 steps = 2.9196e-01, PNorm = 143.7600, GNorm = 0.2355
Meta loss on this task batch = 3.1736e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 143.7644, GNorm = 0.2740
Meta loss on this task batch = 2.8518e-01, Meta loss averaged over last 500 steps = 2.9198e-01, PNorm = 143.7691, GNorm = 0.3059
Meta loss on this task batch = 3.0621e-01, Meta loss averaged over last 500 steps = 2.9200e-01, PNorm = 143.7740, GNorm = 0.3661
Meta loss on this task batch = 2.5764e-01, Meta loss averaged over last 500 steps = 2.9195e-01, PNorm = 143.7794, GNorm = 0.2274
Meta loss on this task batch = 2.8105e-01, Meta loss averaged over last 500 steps = 2.9198e-01, PNorm = 143.7855, GNorm = 0.2746
Meta loss on this task batch = 3.8294e-01, Meta loss averaged over last 500 steps = 2.9208e-01, PNorm = 143.7909, GNorm = 0.3692
Meta loss on this task batch = 3.2596e-01, Meta loss averaged over last 500 steps = 2.9213e-01, PNorm = 143.7962, GNorm = 0.4173
Took 111.55622696876526 seconds to complete one epoch of meta training
Took 118.4631724357605 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.431176
Epoch 602
Meta loss on this task batch = 3.2799e-01, Meta loss averaged over last 500 steps = 2.9211e-01, PNorm = 143.8016, GNorm = 0.2540
Meta loss on this task batch = 2.5673e-01, Meta loss averaged over last 500 steps = 2.9198e-01, PNorm = 143.8074, GNorm = 0.2348
Meta loss on this task batch = 2.3759e-01, Meta loss averaged over last 500 steps = 2.9191e-01, PNorm = 143.8140, GNorm = 0.2369
Meta loss on this task batch = 2.8641e-01, Meta loss averaged over last 500 steps = 2.9202e-01, PNorm = 143.8203, GNorm = 0.2622
Meta loss on this task batch = 2.7037e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 143.8271, GNorm = 0.3419
Meta loss on this task batch = 2.6945e-01, Meta loss averaged over last 500 steps = 2.9200e-01, PNorm = 143.8340, GNorm = 0.2387
Meta loss on this task batch = 3.3122e-01, Meta loss averaged over last 500 steps = 2.9207e-01, PNorm = 143.8400, GNorm = 0.6088
Meta loss on this task batch = 3.6062e-01, Meta loss averaged over last 500 steps = 2.9210e-01, PNorm = 143.8452, GNorm = 0.3224
Meta loss on this task batch = 3.7574e-01, Meta loss averaged over last 500 steps = 2.9243e-01, PNorm = 143.8497, GNorm = 0.3043
Meta loss on this task batch = 2.4882e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 143.8542, GNorm = 0.2678
Meta loss on this task batch = 3.0767e-01, Meta loss averaged over last 500 steps = 2.9223e-01, PNorm = 143.8585, GNorm = 0.2998
Meta loss on this task batch = 2.7896e-01, Meta loss averaged over last 500 steps = 2.9222e-01, PNorm = 143.8634, GNorm = 0.3274
Meta loss on this task batch = 2.9311e-01, Meta loss averaged over last 500 steps = 2.9214e-01, PNorm = 143.8686, GNorm = 0.2947
Meta loss on this task batch = 3.6573e-01, Meta loss averaged over last 500 steps = 2.9227e-01, PNorm = 143.8733, GNorm = 0.4689
Meta loss on this task batch = 2.4961e-01, Meta loss averaged over last 500 steps = 2.9224e-01, PNorm = 143.8787, GNorm = 0.2025
Meta loss on this task batch = 3.1491e-01, Meta loss averaged over last 500 steps = 2.9229e-01, PNorm = 143.8840, GNorm = 0.3022
Meta loss on this task batch = 2.7951e-01, Meta loss averaged over last 500 steps = 2.9226e-01, PNorm = 143.8900, GNorm = 0.2847
Meta loss on this task batch = 2.4948e-01, Meta loss averaged over last 500 steps = 2.9221e-01, PNorm = 143.8961, GNorm = 0.2445
Meta loss on this task batch = 3.0375e-01, Meta loss averaged over last 500 steps = 2.9227e-01, PNorm = 143.9020, GNorm = 0.2764
Took 113.82767033576965 seconds to complete one epoch of meta training
Took 121.75316333770752 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457098
Epoch 603
Meta loss on this task batch = 2.4207e-01, Meta loss averaged over last 500 steps = 2.9220e-01, PNorm = 143.9080, GNorm = 0.2386
Meta loss on this task batch = 2.6137e-01, Meta loss averaged over last 500 steps = 2.9212e-01, PNorm = 143.9142, GNorm = 0.2623
Meta loss on this task batch = 3.4158e-01, Meta loss averaged over last 500 steps = 2.9212e-01, PNorm = 143.9199, GNorm = 0.2978
Meta loss on this task batch = 3.2016e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 143.9253, GNorm = 0.2934
Meta loss on this task batch = 2.9184e-01, Meta loss averaged over last 500 steps = 2.9232e-01, PNorm = 143.9303, GNorm = 0.2411
Meta loss on this task batch = 2.3127e-01, Meta loss averaged over last 500 steps = 2.9218e-01, PNorm = 143.9358, GNorm = 0.2268
Meta loss on this task batch = 2.7778e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 143.9421, GNorm = 0.3609
Meta loss on this task batch = 2.8755e-01, Meta loss averaged over last 500 steps = 2.9214e-01, PNorm = 143.9487, GNorm = 0.3193
Meta loss on this task batch = 2.8817e-01, Meta loss averaged over last 500 steps = 2.9208e-01, PNorm = 143.9544, GNorm = 0.2807
Meta loss on this task batch = 3.3583e-01, Meta loss averaged over last 500 steps = 2.9213e-01, PNorm = 143.9587, GNorm = 0.4478
Meta loss on this task batch = 2.9159e-01, Meta loss averaged over last 500 steps = 2.9219e-01, PNorm = 143.9633, GNorm = 0.2934
Meta loss on this task batch = 3.1117e-01, Meta loss averaged over last 500 steps = 2.9224e-01, PNorm = 143.9673, GNorm = 0.3513
Meta loss on this task batch = 3.0153e-01, Meta loss averaged over last 500 steps = 2.9229e-01, PNorm = 143.9714, GNorm = 0.3041
Meta loss on this task batch = 3.2809e-01, Meta loss averaged over last 500 steps = 2.9246e-01, PNorm = 143.9754, GNorm = 0.3026
Meta loss on this task batch = 2.9548e-01, Meta loss averaged over last 500 steps = 2.9240e-01, PNorm = 143.9792, GNorm = 0.2643
Meta loss on this task batch = 2.7543e-01, Meta loss averaged over last 500 steps = 2.9245e-01, PNorm = 143.9835, GNorm = 0.2696
Meta loss on this task batch = 3.0821e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 143.9886, GNorm = 0.2756
Meta loss on this task batch = 2.4609e-01, Meta loss averaged over last 500 steps = 2.9243e-01, PNorm = 143.9940, GNorm = 0.2353
Meta loss on this task batch = 2.9599e-01, Meta loss averaged over last 500 steps = 2.9244e-01, PNorm = 143.9991, GNorm = 0.3692
Took 113.20382356643677 seconds to complete one epoch of meta training
Took 120.71018981933594 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463346
Epoch 604
Meta loss on this task batch = 3.0016e-01, Meta loss averaged over last 500 steps = 2.9242e-01, PNorm = 144.0040, GNorm = 0.2804
Meta loss on this task batch = 3.0704e-01, Meta loss averaged over last 500 steps = 2.9240e-01, PNorm = 144.0084, GNorm = 0.4034
Meta loss on this task batch = 2.1113e-01, Meta loss averaged over last 500 steps = 2.9221e-01, PNorm = 144.0141, GNorm = 0.2990
Meta loss on this task batch = 2.5649e-01, Meta loss averaged over last 500 steps = 2.9231e-01, PNorm = 144.0199, GNorm = 0.2748
Meta loss on this task batch = 3.3898e-01, Meta loss averaged over last 500 steps = 2.9232e-01, PNorm = 144.0259, GNorm = 0.3881
Meta loss on this task batch = 3.1870e-01, Meta loss averaged over last 500 steps = 2.9227e-01, PNorm = 144.0312, GNorm = 0.2594
Meta loss on this task batch = 2.8758e-01, Meta loss averaged over last 500 steps = 2.9223e-01, PNorm = 144.0362, GNorm = 0.3321
Meta loss on this task batch = 2.5659e-01, Meta loss averaged over last 500 steps = 2.9224e-01, PNorm = 144.0417, GNorm = 0.3070
Meta loss on this task batch = 3.1766e-01, Meta loss averaged over last 500 steps = 2.9234e-01, PNorm = 144.0466, GNorm = 0.3053
Meta loss on this task batch = 2.7496e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 144.0509, GNorm = 0.2521
Meta loss on this task batch = 3.2447e-01, Meta loss averaged over last 500 steps = 2.9228e-01, PNorm = 144.0552, GNorm = 0.5032
Meta loss on this task batch = 3.1909e-01, Meta loss averaged over last 500 steps = 2.9235e-01, PNorm = 144.0590, GNorm = 0.2686
Meta loss on this task batch = 3.3228e-01, Meta loss averaged over last 500 steps = 2.9244e-01, PNorm = 144.0625, GNorm = 0.4296
Meta loss on this task batch = 2.8512e-01, Meta loss averaged over last 500 steps = 2.9245e-01, PNorm = 144.0662, GNorm = 0.2874
Meta loss on this task batch = 2.7681e-01, Meta loss averaged over last 500 steps = 2.9249e-01, PNorm = 144.0700, GNorm = 0.3698
Meta loss on this task batch = 2.9236e-01, Meta loss averaged over last 500 steps = 2.9241e-01, PNorm = 144.0737, GNorm = 0.2885
Meta loss on this task batch = 2.7800e-01, Meta loss averaged over last 500 steps = 2.9232e-01, PNorm = 144.0778, GNorm = 0.2535
Meta loss on this task batch = 2.6797e-01, Meta loss averaged over last 500 steps = 2.9221e-01, PNorm = 144.0824, GNorm = 0.2974
Meta loss on this task batch = 2.7134e-01, Meta loss averaged over last 500 steps = 2.9231e-01, PNorm = 144.0868, GNorm = 0.3470
Took 110.87310409545898 seconds to complete one epoch of meta training
Took 118.99314069747925 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468668
Epoch 605
Meta loss on this task batch = 2.7401e-01, Meta loss averaged over last 500 steps = 2.9228e-01, PNorm = 144.0914, GNorm = 0.2765
Meta loss on this task batch = 2.8586e-01, Meta loss averaged over last 500 steps = 2.9232e-01, PNorm = 144.0963, GNorm = 0.2681
Meta loss on this task batch = 2.4041e-01, Meta loss averaged over last 500 steps = 2.9227e-01, PNorm = 144.1012, GNorm = 0.2356
Meta loss on this task batch = 2.6269e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 144.1059, GNorm = 0.2887
Meta loss on this task batch = 2.7559e-01, Meta loss averaged over last 500 steps = 2.9210e-01, PNorm = 144.1104, GNorm = 0.3430
Meta loss on this task batch = 2.7733e-01, Meta loss averaged over last 500 steps = 2.9206e-01, PNorm = 144.1144, GNorm = 0.3274
Meta loss on this task batch = 2.2929e-01, Meta loss averaged over last 500 steps = 2.9200e-01, PNorm = 144.1180, GNorm = 0.3787
Meta loss on this task batch = 3.1331e-01, Meta loss averaged over last 500 steps = 2.9196e-01, PNorm = 144.1216, GNorm = 0.4034
Meta loss on this task batch = 2.6906e-01, Meta loss averaged over last 500 steps = 2.9194e-01, PNorm = 144.1254, GNorm = 0.4295
Meta loss on this task batch = 3.1482e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 144.1294, GNorm = 0.3337
Meta loss on this task batch = 3.0171e-01, Meta loss averaged over last 500 steps = 2.9202e-01, PNorm = 144.1331, GNorm = 0.2876
Meta loss on this task batch = 2.6707e-01, Meta loss averaged over last 500 steps = 2.9197e-01, PNorm = 144.1371, GNorm = 0.3150
Meta loss on this task batch = 2.6650e-01, Meta loss averaged over last 500 steps = 2.9194e-01, PNorm = 144.1412, GNorm = 0.3292
Meta loss on this task batch = 2.4451e-01, Meta loss averaged over last 500 steps = 2.9189e-01, PNorm = 144.1454, GNorm = 0.3310
Meta loss on this task batch = 3.4601e-01, Meta loss averaged over last 500 steps = 2.9200e-01, PNorm = 144.1489, GNorm = 0.3384
Meta loss on this task batch = 2.8897e-01, Meta loss averaged over last 500 steps = 2.9196e-01, PNorm = 144.1531, GNorm = 0.3138
Meta loss on this task batch = 3.1434e-01, Meta loss averaged over last 500 steps = 2.9202e-01, PNorm = 144.1575, GNorm = 0.2564
Meta loss on this task batch = 3.5533e-01, Meta loss averaged over last 500 steps = 2.9217e-01, PNorm = 144.1616, GNorm = 0.3067
Meta loss on this task batch = 2.5494e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 144.1664, GNorm = 0.2957
Took 113.51510453224182 seconds to complete one epoch of meta training
Took 120.90041708946228 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449086
Epoch 606
Meta loss on this task batch = 3.2555e-01, Meta loss averaged over last 500 steps = 2.9215e-01, PNorm = 144.1714, GNorm = 0.2724
Meta loss on this task batch = 2.8966e-01, Meta loss averaged over last 500 steps = 2.9217e-01, PNorm = 144.1771, GNorm = 0.3896
Meta loss on this task batch = 2.5054e-01, Meta loss averaged over last 500 steps = 2.9213e-01, PNorm = 144.1829, GNorm = 0.2353
Meta loss on this task batch = 2.8053e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 144.1885, GNorm = 0.3324
Meta loss on this task batch = 3.0896e-01, Meta loss averaged over last 500 steps = 2.9224e-01, PNorm = 144.1943, GNorm = 0.4052
Meta loss on this task batch = 2.9071e-01, Meta loss averaged over last 500 steps = 2.9214e-01, PNorm = 144.2003, GNorm = 0.2509
Meta loss on this task batch = 2.4099e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 144.2066, GNorm = 0.2667
Meta loss on this task batch = 3.1315e-01, Meta loss averaged over last 500 steps = 2.9199e-01, PNorm = 144.2118, GNorm = 0.4045
Meta loss on this task batch = 2.2806e-01, Meta loss averaged over last 500 steps = 2.9185e-01, PNorm = 144.2182, GNorm = 0.2868
Meta loss on this task batch = 2.7763e-01, Meta loss averaged over last 500 steps = 2.9191e-01, PNorm = 144.2239, GNorm = 0.2612
Meta loss on this task batch = 2.5005e-01, Meta loss averaged over last 500 steps = 2.9192e-01, PNorm = 144.2303, GNorm = 0.2549
Meta loss on this task batch = 2.7018e-01, Meta loss averaged over last 500 steps = 2.9189e-01, PNorm = 144.2358, GNorm = 0.2744
Meta loss on this task batch = 3.0343e-01, Meta loss averaged over last 500 steps = 2.9185e-01, PNorm = 144.2410, GNorm = 0.2561
Meta loss on this task batch = 3.0657e-01, Meta loss averaged over last 500 steps = 2.9192e-01, PNorm = 144.2456, GNorm = 0.2721
Meta loss on this task batch = 3.3135e-01, Meta loss averaged over last 500 steps = 2.9198e-01, PNorm = 144.2505, GNorm = 0.3042
Meta loss on this task batch = 2.5736e-01, Meta loss averaged over last 500 steps = 2.9198e-01, PNorm = 144.2551, GNorm = 0.2142
Meta loss on this task batch = 3.0613e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 144.2594, GNorm = 0.3323
Meta loss on this task batch = 2.7380e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 144.2643, GNorm = 0.2716
Meta loss on this task batch = 3.1680e-01, Meta loss averaged over last 500 steps = 2.9219e-01, PNorm = 144.2683, GNorm = 0.3266
Took 113.19356226921082 seconds to complete one epoch of meta training
Took 121.10262393951416 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473465
Epoch 607
Meta loss on this task batch = 3.0475e-01, Meta loss averaged over last 500 steps = 2.9207e-01, PNorm = 144.2719, GNorm = 0.2913
Meta loss on this task batch = 3.0044e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 144.2754, GNorm = 0.2763
Meta loss on this task batch = 2.6444e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 144.2791, GNorm = 0.3786
Meta loss on this task batch = 2.5127e-01, Meta loss averaged over last 500 steps = 2.9200e-01, PNorm = 144.2835, GNorm = 0.3046
Meta loss on this task batch = 3.3103e-01, Meta loss averaged over last 500 steps = 2.9214e-01, PNorm = 144.2880, GNorm = 0.2921
Meta loss on this task batch = 2.5820e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 144.2927, GNorm = 0.3713
Meta loss on this task batch = 3.0244e-01, Meta loss averaged over last 500 steps = 2.9213e-01, PNorm = 144.2975, GNorm = 0.3540
Meta loss on this task batch = 3.0525e-01, Meta loss averaged over last 500 steps = 2.9217e-01, PNorm = 144.3020, GNorm = 0.2636
Meta loss on this task batch = 3.1148e-01, Meta loss averaged over last 500 steps = 2.9222e-01, PNorm = 144.3065, GNorm = 0.3071
Meta loss on this task batch = 3.1033e-01, Meta loss averaged over last 500 steps = 2.9219e-01, PNorm = 144.3106, GNorm = 0.4345
Meta loss on this task batch = 3.1750e-01, Meta loss averaged over last 500 steps = 2.9222e-01, PNorm = 144.3145, GNorm = 0.5159
Meta loss on this task batch = 2.7794e-01, Meta loss averaged over last 500 steps = 2.9221e-01, PNorm = 144.3182, GNorm = 0.2921
Meta loss on this task batch = 2.5776e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 144.3223, GNorm = 0.2800
Meta loss on this task batch = 2.7628e-01, Meta loss averaged over last 500 steps = 2.9197e-01, PNorm = 144.3264, GNorm = 0.2755
Meta loss on this task batch = 2.7334e-01, Meta loss averaged over last 500 steps = 2.9184e-01, PNorm = 144.3308, GNorm = 0.2846
Meta loss on this task batch = 2.8934e-01, Meta loss averaged over last 500 steps = 2.9172e-01, PNorm = 144.3355, GNorm = 0.3849
Meta loss on this task batch = 2.9280e-01, Meta loss averaged over last 500 steps = 2.9177e-01, PNorm = 144.3404, GNorm = 0.2646
Meta loss on this task batch = 3.1589e-01, Meta loss averaged over last 500 steps = 2.9189e-01, PNorm = 144.3450, GNorm = 0.2714
Meta loss on this task batch = 2.1539e-01, Meta loss averaged over last 500 steps = 2.9178e-01, PNorm = 144.3501, GNorm = 0.3668
Took 113.14519333839417 seconds to complete one epoch of meta training
Took 120.96450400352478 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458222
Epoch 608
Meta loss on this task batch = 3.2446e-01, Meta loss averaged over last 500 steps = 2.9185e-01, PNorm = 144.3547, GNorm = 0.4178
Meta loss on this task batch = 2.3706e-01, Meta loss averaged over last 500 steps = 2.9173e-01, PNorm = 144.3595, GNorm = 0.3014
Meta loss on this task batch = 2.4260e-01, Meta loss averaged over last 500 steps = 2.9169e-01, PNorm = 144.3649, GNorm = 0.3089
Meta loss on this task batch = 2.7600e-01, Meta loss averaged over last 500 steps = 2.9169e-01, PNorm = 144.3704, GNorm = 0.3588
Meta loss on this task batch = 3.1863e-01, Meta loss averaged over last 500 steps = 2.9168e-01, PNorm = 144.3750, GNorm = 0.2834
Meta loss on this task batch = 3.0916e-01, Meta loss averaged over last 500 steps = 2.9173e-01, PNorm = 144.3794, GNorm = 0.4485
Meta loss on this task batch = 2.5603e-01, Meta loss averaged over last 500 steps = 2.9168e-01, PNorm = 144.3836, GNorm = 0.2672
Meta loss on this task batch = 3.0865e-01, Meta loss averaged over last 500 steps = 2.9177e-01, PNorm = 144.3875, GNorm = 0.2985
Meta loss on this task batch = 3.0792e-01, Meta loss averaged over last 500 steps = 2.9188e-01, PNorm = 144.3906, GNorm = 0.4558
Meta loss on this task batch = 3.6090e-01, Meta loss averaged over last 500 steps = 2.9208e-01, PNorm = 144.3935, GNorm = 0.3731
Meta loss on this task batch = 2.9289e-01, Meta loss averaged over last 500 steps = 2.9205e-01, PNorm = 144.3956, GNorm = 0.3262
Meta loss on this task batch = 3.2990e-01, Meta loss averaged over last 500 steps = 2.9206e-01, PNorm = 144.3977, GNorm = 0.4325
Meta loss on this task batch = 2.6774e-01, Meta loss averaged over last 500 steps = 2.9199e-01, PNorm = 144.4007, GNorm = 0.2515
Meta loss on this task batch = 2.9373e-01, Meta loss averaged over last 500 steps = 2.9199e-01, PNorm = 144.4040, GNorm = 0.2741
Meta loss on this task batch = 2.7098e-01, Meta loss averaged over last 500 steps = 2.9195e-01, PNorm = 144.4072, GNorm = 0.4027
Meta loss on this task batch = 2.9530e-01, Meta loss averaged over last 500 steps = 2.9211e-01, PNorm = 144.4107, GNorm = 0.2643
Meta loss on this task batch = 2.7757e-01, Meta loss averaged over last 500 steps = 2.9200e-01, PNorm = 144.4148, GNorm = 0.2950
Meta loss on this task batch = 3.3005e-01, Meta loss averaged over last 500 steps = 2.9218e-01, PNorm = 144.4190, GNorm = 0.3000
Meta loss on this task batch = 2.8846e-01, Meta loss averaged over last 500 steps = 2.9202e-01, PNorm = 144.4233, GNorm = 0.3723
Took 112.61518955230713 seconds to complete one epoch of meta training
Took 120.47472262382507 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482939
Epoch 609
Meta loss on this task batch = 2.7901e-01, Meta loss averaged over last 500 steps = 2.9205e-01, PNorm = 144.4280, GNorm = 0.3136
Meta loss on this task batch = 2.6175e-01, Meta loss averaged over last 500 steps = 2.9192e-01, PNorm = 144.4334, GNorm = 0.3655
Meta loss on this task batch = 2.4754e-01, Meta loss averaged over last 500 steps = 2.9186e-01, PNorm = 144.4398, GNorm = 0.2699
Meta loss on this task batch = 2.9892e-01, Meta loss averaged over last 500 steps = 2.9190e-01, PNorm = 144.4459, GNorm = 0.3744
Meta loss on this task batch = 3.3576e-01, Meta loss averaged over last 500 steps = 2.9191e-01, PNorm = 144.4516, GNorm = 0.4187
Meta loss on this task batch = 3.3496e-01, Meta loss averaged over last 500 steps = 2.9202e-01, PNorm = 144.4573, GNorm = 0.3717
Meta loss on this task batch = 2.6751e-01, Meta loss averaged over last 500 steps = 2.9207e-01, PNorm = 144.4631, GNorm = 0.3688
Meta loss on this task batch = 2.7429e-01, Meta loss averaged over last 500 steps = 2.9213e-01, PNorm = 144.4690, GNorm = 0.2604
Meta loss on this task batch = 2.6110e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 144.4744, GNorm = 0.2908
Meta loss on this task batch = 3.3237e-01, Meta loss averaged over last 500 steps = 2.9206e-01, PNorm = 144.4789, GNorm = 0.4025
Meta loss on this task batch = 3.4140e-01, Meta loss averaged over last 500 steps = 2.9218e-01, PNorm = 144.4833, GNorm = 0.4486
Meta loss on this task batch = 3.0536e-01, Meta loss averaged over last 500 steps = 2.9229e-01, PNorm = 144.4880, GNorm = 0.3105
Meta loss on this task batch = 3.3139e-01, Meta loss averaged over last 500 steps = 2.9232e-01, PNorm = 144.4921, GNorm = 0.3128
Meta loss on this task batch = 2.7577e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 144.4955, GNorm = 0.3461
Meta loss on this task batch = 2.3565e-01, Meta loss averaged over last 500 steps = 2.9214e-01, PNorm = 144.4993, GNorm = 0.2953
Meta loss on this task batch = 2.6787e-01, Meta loss averaged over last 500 steps = 2.9210e-01, PNorm = 144.5027, GNorm = 0.2777
Meta loss on this task batch = 2.6150e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 144.5065, GNorm = 0.2223
Meta loss on this task batch = 3.2351e-01, Meta loss averaged over last 500 steps = 2.9208e-01, PNorm = 144.5103, GNorm = 0.3454
Meta loss on this task batch = 2.8697e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 144.5150, GNorm = 0.2635
Took 111.04632806777954 seconds to complete one epoch of meta training
Took 118.45065569877625 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455404
Epoch 610
Meta loss on this task batch = 2.7206e-01, Meta loss averaged over last 500 steps = 2.9205e-01, PNorm = 144.5202, GNorm = 0.4052
Meta loss on this task batch = 3.0388e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 144.5255, GNorm = 0.3497
Meta loss on this task batch = 2.8054e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 144.5306, GNorm = 0.2239
Meta loss on this task batch = 3.0544e-01, Meta loss averaged over last 500 steps = 2.9193e-01, PNorm = 144.5367, GNorm = 0.2956
Meta loss on this task batch = 2.8242e-01, Meta loss averaged over last 500 steps = 2.9192e-01, PNorm = 144.5421, GNorm = 0.2758
Meta loss on this task batch = 3.8113e-01, Meta loss averaged over last 500 steps = 2.9206e-01, PNorm = 144.5473, GNorm = 0.3196
Meta loss on this task batch = 3.2220e-01, Meta loss averaged over last 500 steps = 2.9199e-01, PNorm = 144.5520, GNorm = 0.2982
Meta loss on this task batch = 2.8581e-01, Meta loss averaged over last 500 steps = 2.9201e-01, PNorm = 144.5570, GNorm = 0.2818
Meta loss on this task batch = 3.0182e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 144.5619, GNorm = 0.2800
Meta loss on this task batch = 2.9180e-01, Meta loss averaged over last 500 steps = 2.9199e-01, PNorm = 144.5661, GNorm = 0.2988
Meta loss on this task batch = 2.6668e-01, Meta loss averaged over last 500 steps = 2.9191e-01, PNorm = 144.5703, GNorm = 0.2282
Meta loss on this task batch = 2.4543e-01, Meta loss averaged over last 500 steps = 2.9188e-01, PNorm = 144.5742, GNorm = 0.2224
Meta loss on this task batch = 2.7552e-01, Meta loss averaged over last 500 steps = 2.9178e-01, PNorm = 144.5782, GNorm = 0.2359
Meta loss on this task batch = 2.6588e-01, Meta loss averaged over last 500 steps = 2.9166e-01, PNorm = 144.5824, GNorm = 0.3103
Meta loss on this task batch = 2.6488e-01, Meta loss averaged over last 500 steps = 2.9156e-01, PNorm = 144.5868, GNorm = 0.2703
Meta loss on this task batch = 3.0891e-01, Meta loss averaged over last 500 steps = 2.9161e-01, PNorm = 144.5911, GNorm = 0.2829
Meta loss on this task batch = 3.0653e-01, Meta loss averaged over last 500 steps = 2.9171e-01, PNorm = 144.5952, GNorm = 0.3140
Meta loss on this task batch = 2.9038e-01, Meta loss averaged over last 500 steps = 2.9167e-01, PNorm = 144.5986, GNorm = 0.3586
Meta loss on this task batch = 2.7457e-01, Meta loss averaged over last 500 steps = 2.9158e-01, PNorm = 144.6019, GNorm = 0.4704
Took 111.42268776893616 seconds to complete one epoch of meta training
Took 119.34747767448425 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479528
Epoch 611
Meta loss on this task batch = 2.9127e-01, Meta loss averaged over last 500 steps = 2.9162e-01, PNorm = 144.6058, GNorm = 0.2629
Meta loss on this task batch = 2.8343e-01, Meta loss averaged over last 500 steps = 2.9156e-01, PNorm = 144.6100, GNorm = 0.2409
Meta loss on this task batch = 2.8622e-01, Meta loss averaged over last 500 steps = 2.9170e-01, PNorm = 144.6141, GNorm = 0.3234
Meta loss on this task batch = 2.3083e-01, Meta loss averaged over last 500 steps = 2.9149e-01, PNorm = 144.6187, GNorm = 0.2679
Meta loss on this task batch = 3.2276e-01, Meta loss averaged over last 500 steps = 2.9155e-01, PNorm = 144.6229, GNorm = 0.3479
Meta loss on this task batch = 2.6288e-01, Meta loss averaged over last 500 steps = 2.9151e-01, PNorm = 144.6274, GNorm = 0.3844
Meta loss on this task batch = 3.6170e-01, Meta loss averaged over last 500 steps = 2.9168e-01, PNorm = 144.6310, GNorm = 0.3912
Meta loss on this task batch = 3.2590e-01, Meta loss averaged over last 500 steps = 2.9179e-01, PNorm = 144.6340, GNorm = 0.2664
Meta loss on this task batch = 2.9718e-01, Meta loss averaged over last 500 steps = 2.9185e-01, PNorm = 144.6379, GNorm = 0.2663
Meta loss on this task batch = 3.3279e-01, Meta loss averaged over last 500 steps = 2.9193e-01, PNorm = 144.6411, GNorm = 0.3527
Meta loss on this task batch = 3.1675e-01, Meta loss averaged over last 500 steps = 2.9194e-01, PNorm = 144.6442, GNorm = 0.4616
Meta loss on this task batch = 2.3950e-01, Meta loss averaged over last 500 steps = 2.9191e-01, PNorm = 144.6480, GNorm = 0.2428
Meta loss on this task batch = 2.7409e-01, Meta loss averaged over last 500 steps = 2.9190e-01, PNorm = 144.6525, GNorm = 0.4791
Meta loss on this task batch = 2.2434e-01, Meta loss averaged over last 500 steps = 2.9165e-01, PNorm = 144.6579, GNorm = 0.3034
Meta loss on this task batch = 3.1845e-01, Meta loss averaged over last 500 steps = 2.9168e-01, PNorm = 144.6637, GNorm = 0.3556
Meta loss on this task batch = 3.5136e-01, Meta loss averaged over last 500 steps = 2.9191e-01, PNorm = 144.6690, GNorm = 0.4174
Meta loss on this task batch = 2.7523e-01, Meta loss averaged over last 500 steps = 2.9173e-01, PNorm = 144.6746, GNorm = 0.2704
Meta loss on this task batch = 2.8962e-01, Meta loss averaged over last 500 steps = 2.9167e-01, PNorm = 144.6793, GNorm = 0.3181
Meta loss on this task batch = 2.5006e-01, Meta loss averaged over last 500 steps = 2.9165e-01, PNorm = 144.6845, GNorm = 0.3865
Took 114.23145508766174 seconds to complete one epoch of meta training
Took 121.7407295703888 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474566
Epoch 612
Meta loss on this task batch = 3.3334e-01, Meta loss averaged over last 500 steps = 2.9183e-01, PNorm = 144.6895, GNorm = 0.4487
Meta loss on this task batch = 3.4734e-01, Meta loss averaged over last 500 steps = 2.9190e-01, PNorm = 144.6946, GNorm = 0.5025
Meta loss on this task batch = 2.7938e-01, Meta loss averaged over last 500 steps = 2.9176e-01, PNorm = 144.6995, GNorm = 0.2447
Meta loss on this task batch = 2.8449e-01, Meta loss averaged over last 500 steps = 2.9171e-01, PNorm = 144.7040, GNorm = 0.3052
Meta loss on this task batch = 2.9347e-01, Meta loss averaged over last 500 steps = 2.9169e-01, PNorm = 144.7095, GNorm = 0.3156
Meta loss on this task batch = 2.6309e-01, Meta loss averaged over last 500 steps = 2.9164e-01, PNorm = 144.7156, GNorm = 0.2679
Meta loss on this task batch = 3.0407e-01, Meta loss averaged over last 500 steps = 2.9166e-01, PNorm = 144.7217, GNorm = 0.2940
Meta loss on this task batch = 2.7150e-01, Meta loss averaged over last 500 steps = 2.9167e-01, PNorm = 144.7280, GNorm = 0.2755
Meta loss on this task batch = 2.6197e-01, Meta loss averaged over last 500 steps = 2.9159e-01, PNorm = 144.7345, GNorm = 0.2850
Meta loss on this task batch = 2.9844e-01, Meta loss averaged over last 500 steps = 2.9165e-01, PNorm = 144.7409, GNorm = 0.3900
Meta loss on this task batch = 2.6340e-01, Meta loss averaged over last 500 steps = 2.9154e-01, PNorm = 144.7471, GNorm = 0.3670
Meta loss on this task batch = 2.3887e-01, Meta loss averaged over last 500 steps = 2.9154e-01, PNorm = 144.7538, GNorm = 0.3365
Meta loss on this task batch = 3.2349e-01, Meta loss averaged over last 500 steps = 2.9164e-01, PNorm = 144.7602, GNorm = 0.4615
Meta loss on this task batch = 3.1952e-01, Meta loss averaged over last 500 steps = 2.9162e-01, PNorm = 144.7662, GNorm = 0.3080
Meta loss on this task batch = 3.0772e-01, Meta loss averaged over last 500 steps = 2.9161e-01, PNorm = 144.7725, GNorm = 0.2809
Meta loss on this task batch = 2.6903e-01, Meta loss averaged over last 500 steps = 2.9148e-01, PNorm = 144.7785, GNorm = 0.3216
Meta loss on this task batch = 2.7457e-01, Meta loss averaged over last 500 steps = 2.9155e-01, PNorm = 144.7844, GNorm = 0.3053
Meta loss on this task batch = 3.1201e-01, Meta loss averaged over last 500 steps = 2.9151e-01, PNorm = 144.7896, GNorm = 0.3388
Meta loss on this task batch = 2.9721e-01, Meta loss averaged over last 500 steps = 2.9154e-01, PNorm = 144.7937, GNorm = 0.4634
Took 114.32057762145996 seconds to complete one epoch of meta training
Took 122.38631439208984 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485210
Epoch 613
Meta loss on this task batch = 2.6334e-01, Meta loss averaged over last 500 steps = 2.9145e-01, PNorm = 144.7976, GNorm = 0.2981
Meta loss on this task batch = 3.0271e-01, Meta loss averaged over last 500 steps = 2.9135e-01, PNorm = 144.7994, GNorm = 0.3655
Meta loss on this task batch = 2.4286e-01, Meta loss averaged over last 500 steps = 2.9122e-01, PNorm = 144.8015, GNorm = 0.3151
Meta loss on this task batch = 3.1656e-01, Meta loss averaged over last 500 steps = 2.9129e-01, PNorm = 144.8040, GNorm = 0.2730
Meta loss on this task batch = 2.5481e-01, Meta loss averaged over last 500 steps = 2.9124e-01, PNorm = 144.8061, GNorm = 0.2630
Meta loss on this task batch = 3.5032e-01, Meta loss averaged over last 500 steps = 2.9132e-01, PNorm = 144.8081, GNorm = 0.3747
Meta loss on this task batch = 2.6037e-01, Meta loss averaged over last 500 steps = 2.9119e-01, PNorm = 144.8116, GNorm = 0.4417
Meta loss on this task batch = 2.8328e-01, Meta loss averaged over last 500 steps = 2.9108e-01, PNorm = 144.8153, GNorm = 0.3240
Meta loss on this task batch = 3.1482e-01, Meta loss averaged over last 500 steps = 2.9111e-01, PNorm = 144.8195, GNorm = 0.2539
Meta loss on this task batch = 2.7766e-01, Meta loss averaged over last 500 steps = 2.9112e-01, PNorm = 144.8243, GNorm = 0.2576
Meta loss on this task batch = 2.9206e-01, Meta loss averaged over last 500 steps = 2.9118e-01, PNorm = 144.8296, GNorm = 0.3316
Meta loss on this task batch = 2.8213e-01, Meta loss averaged over last 500 steps = 2.9112e-01, PNorm = 144.8348, GNorm = 0.4365
Meta loss on this task batch = 2.5252e-01, Meta loss averaged over last 500 steps = 2.9099e-01, PNorm = 144.8399, GNorm = 0.2393
Meta loss on this task batch = 2.3098e-01, Meta loss averaged over last 500 steps = 2.9082e-01, PNorm = 144.8454, GNorm = 0.2482
Meta loss on this task batch = 2.8482e-01, Meta loss averaged over last 500 steps = 2.9082e-01, PNorm = 144.8505, GNorm = 0.2956
Meta loss on this task batch = 2.5902e-01, Meta loss averaged over last 500 steps = 2.9071e-01, PNorm = 144.8548, GNorm = 0.2797
Meta loss on this task batch = 2.6298e-01, Meta loss averaged over last 500 steps = 2.9058e-01, PNorm = 144.8580, GNorm = 0.2945
Meta loss on this task batch = 3.6074e-01, Meta loss averaged over last 500 steps = 2.9075e-01, PNorm = 144.8608, GNorm = 0.5013
Meta loss on this task batch = 3.0492e-01, Meta loss averaged over last 500 steps = 2.9068e-01, PNorm = 144.8640, GNorm = 0.3414
Took 111.52316904067993 seconds to complete one epoch of meta training
Took 118.24553442001343 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459371
Epoch 614
Meta loss on this task batch = 2.4121e-01, Meta loss averaged over last 500 steps = 2.9054e-01, PNorm = 144.8679, GNorm = 0.3886
Meta loss on this task batch = 2.9034e-01, Meta loss averaged over last 500 steps = 2.9057e-01, PNorm = 144.8719, GNorm = 0.3502
Meta loss on this task batch = 3.1662e-01, Meta loss averaged over last 500 steps = 2.9063e-01, PNorm = 144.8761, GNorm = 0.2777
Meta loss on this task batch = 3.4714e-01, Meta loss averaged over last 500 steps = 2.9087e-01, PNorm = 144.8800, GNorm = 0.3504
Meta loss on this task batch = 2.5551e-01, Meta loss averaged over last 500 steps = 2.9085e-01, PNorm = 144.8839, GNorm = 0.3000
Meta loss on this task batch = 2.9818e-01, Meta loss averaged over last 500 steps = 2.9090e-01, PNorm = 144.8871, GNorm = 0.4096
Meta loss on this task batch = 2.9354e-01, Meta loss averaged over last 500 steps = 2.9095e-01, PNorm = 144.8903, GNorm = 0.3294
Meta loss on this task batch = 2.4679e-01, Meta loss averaged over last 500 steps = 2.9083e-01, PNorm = 144.8945, GNorm = 0.2464
Meta loss on this task batch = 2.9916e-01, Meta loss averaged over last 500 steps = 2.9085e-01, PNorm = 144.8989, GNorm = 0.2762
Meta loss on this task batch = 2.6960e-01, Meta loss averaged over last 500 steps = 2.9075e-01, PNorm = 144.9029, GNorm = 0.2455
Meta loss on this task batch = 2.3485e-01, Meta loss averaged over last 500 steps = 2.9059e-01, PNorm = 144.9066, GNorm = 0.2669
Meta loss on this task batch = 3.7156e-01, Meta loss averaged over last 500 steps = 2.9079e-01, PNorm = 144.9100, GNorm = 0.3556
Meta loss on this task batch = 2.7200e-01, Meta loss averaged over last 500 steps = 2.9059e-01, PNorm = 144.9137, GNorm = 0.2602
Meta loss on this task batch = 2.8513e-01, Meta loss averaged over last 500 steps = 2.9062e-01, PNorm = 144.9171, GNorm = 0.3041
Meta loss on this task batch = 2.3677e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 144.9214, GNorm = 0.2469
Meta loss on this task batch = 3.0096e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 144.9259, GNorm = 0.2906
Meta loss on this task batch = 3.2262e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 144.9310, GNorm = 0.3143
Meta loss on this task batch = 3.0250e-01, Meta loss averaged over last 500 steps = 2.9055e-01, PNorm = 144.9358, GNorm = 0.2781
Meta loss on this task batch = 2.8663e-01, Meta loss averaged over last 500 steps = 2.9045e-01, PNorm = 144.9409, GNorm = 0.3955
Took 114.09976029396057 seconds to complete one epoch of meta training
Took 121.71071076393127 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.433933
Epoch 615
Meta loss on this task batch = 3.0378e-01, Meta loss averaged over last 500 steps = 2.9058e-01, PNorm = 144.9461, GNorm = 0.3168
Meta loss on this task batch = 2.7172e-01, Meta loss averaged over last 500 steps = 2.9053e-01, PNorm = 144.9510, GNorm = 0.2646
Meta loss on this task batch = 3.2473e-01, Meta loss averaged over last 500 steps = 2.9068e-01, PNorm = 144.9557, GNorm = 0.4392
Meta loss on this task batch = 3.1012e-01, Meta loss averaged over last 500 steps = 2.9063e-01, PNorm = 144.9603, GNorm = 0.2502
Meta loss on this task batch = 3.0042e-01, Meta loss averaged over last 500 steps = 2.9055e-01, PNorm = 144.9640, GNorm = 0.3325
Meta loss on this task batch = 2.7535e-01, Meta loss averaged over last 500 steps = 2.9063e-01, PNorm = 144.9681, GNorm = 0.2429
Meta loss on this task batch = 2.5574e-01, Meta loss averaged over last 500 steps = 2.9052e-01, PNorm = 144.9725, GNorm = 0.3658
Meta loss on this task batch = 2.9180e-01, Meta loss averaged over last 500 steps = 2.9061e-01, PNorm = 144.9767, GNorm = 0.3159
Meta loss on this task batch = 2.8953e-01, Meta loss averaged over last 500 steps = 2.9056e-01, PNorm = 144.9800, GNorm = 0.5253
Meta loss on this task batch = 2.9132e-01, Meta loss averaged over last 500 steps = 2.9065e-01, PNorm = 144.9842, GNorm = 0.2790
Meta loss on this task batch = 2.7368e-01, Meta loss averaged over last 500 steps = 2.9066e-01, PNorm = 144.9886, GNorm = 0.3398
Meta loss on this task batch = 3.5001e-01, Meta loss averaged over last 500 steps = 2.9076e-01, PNorm = 144.9931, GNorm = 0.3436
Meta loss on this task batch = 3.4080e-01, Meta loss averaged over last 500 steps = 2.9081e-01, PNorm = 144.9978, GNorm = 0.3704
Meta loss on this task batch = 2.2903e-01, Meta loss averaged over last 500 steps = 2.9077e-01, PNorm = 145.0031, GNorm = 0.2789
Meta loss on this task batch = 2.9294e-01, Meta loss averaged over last 500 steps = 2.9079e-01, PNorm = 145.0086, GNorm = 0.3601
Meta loss on this task batch = 3.1327e-01, Meta loss averaged over last 500 steps = 2.9084e-01, PNorm = 145.0140, GNorm = 0.2998
Meta loss on this task batch = 2.8296e-01, Meta loss averaged over last 500 steps = 2.9077e-01, PNorm = 145.0193, GNorm = 0.2656
Meta loss on this task batch = 2.6755e-01, Meta loss averaged over last 500 steps = 2.9074e-01, PNorm = 145.0251, GNorm = 0.3390
Meta loss on this task batch = 3.3478e-01, Meta loss averaged over last 500 steps = 2.9064e-01, PNorm = 145.0308, GNorm = 0.3480
Took 137.8900773525238 seconds to complete one epoch of meta training
Took 145.27966904640198 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452496
Epoch 616
Meta loss on this task batch = 3.0539e-01, Meta loss averaged over last 500 steps = 2.9064e-01, PNorm = 145.0365, GNorm = 0.2477
Meta loss on this task batch = 3.0719e-01, Meta loss averaged over last 500 steps = 2.9081e-01, PNorm = 145.0416, GNorm = 0.3976
Meta loss on this task batch = 2.3380e-01, Meta loss averaged over last 500 steps = 2.9064e-01, PNorm = 145.0466, GNorm = 0.2879
Meta loss on this task batch = 2.8066e-01, Meta loss averaged over last 500 steps = 2.9067e-01, PNorm = 145.0513, GNorm = 0.2693
Meta loss on this task batch = 3.4202e-01, Meta loss averaged over last 500 steps = 2.9089e-01, PNorm = 145.0561, GNorm = 0.2761
Meta loss on this task batch = 2.6834e-01, Meta loss averaged over last 500 steps = 2.9072e-01, PNorm = 145.0610, GNorm = 0.3224
Meta loss on this task batch = 2.4491e-01, Meta loss averaged over last 500 steps = 2.9064e-01, PNorm = 145.0653, GNorm = 0.2870
Meta loss on this task batch = 3.5961e-01, Meta loss averaged over last 500 steps = 2.9073e-01, PNorm = 145.0691, GNorm = 0.3778
Meta loss on this task batch = 2.8464e-01, Meta loss averaged over last 500 steps = 2.9073e-01, PNorm = 145.0729, GNorm = 0.2842
Meta loss on this task batch = 3.2388e-01, Meta loss averaged over last 500 steps = 2.9086e-01, PNorm = 145.0762, GNorm = 0.2972
Meta loss on this task batch = 2.9764e-01, Meta loss averaged over last 500 steps = 2.9085e-01, PNorm = 145.0794, GNorm = 0.2838
Meta loss on this task batch = 2.8720e-01, Meta loss averaged over last 500 steps = 2.9093e-01, PNorm = 145.0829, GNorm = 0.3016
Meta loss on this task batch = 3.1898e-01, Meta loss averaged over last 500 steps = 2.9091e-01, PNorm = 145.0874, GNorm = 0.3544
Meta loss on this task batch = 3.0323e-01, Meta loss averaged over last 500 steps = 2.9100e-01, PNorm = 145.0929, GNorm = 0.3083
Meta loss on this task batch = 2.5924e-01, Meta loss averaged over last 500 steps = 2.9096e-01, PNorm = 145.0984, GNorm = 0.2656
Meta loss on this task batch = 3.0734e-01, Meta loss averaged over last 500 steps = 2.9093e-01, PNorm = 145.1039, GNorm = 0.4057
Meta loss on this task batch = 3.0000e-01, Meta loss averaged over last 500 steps = 2.9093e-01, PNorm = 145.1088, GNorm = 0.3530
Meta loss on this task batch = 2.5652e-01, Meta loss averaged over last 500 steps = 2.9093e-01, PNorm = 145.1141, GNorm = 0.2901
Meta loss on this task batch = 3.0154e-01, Meta loss averaged over last 500 steps = 2.9107e-01, PNorm = 145.1197, GNorm = 0.6520
Took 111.71651577949524 seconds to complete one epoch of meta training
Took 119.48966193199158 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451050
Epoch 617
Meta loss on this task batch = 2.7434e-01, Meta loss averaged over last 500 steps = 2.9105e-01, PNorm = 145.1255, GNorm = 0.2860
Meta loss on this task batch = 2.5314e-01, Meta loss averaged over last 500 steps = 2.9101e-01, PNorm = 145.1319, GNorm = 0.2376
Meta loss on this task batch = 2.6781e-01, Meta loss averaged over last 500 steps = 2.9079e-01, PNorm = 145.1378, GNorm = 0.2995
Meta loss on this task batch = 2.5590e-01, Meta loss averaged over last 500 steps = 2.9073e-01, PNorm = 145.1437, GNorm = 0.3570
Meta loss on this task batch = 3.1316e-01, Meta loss averaged over last 500 steps = 2.9075e-01, PNorm = 145.1486, GNorm = 0.3389
Meta loss on this task batch = 3.0902e-01, Meta loss averaged over last 500 steps = 2.9084e-01, PNorm = 145.1538, GNorm = 0.2834
Meta loss on this task batch = 2.8117e-01, Meta loss averaged over last 500 steps = 2.9082e-01, PNorm = 145.1592, GNorm = 0.3136
Meta loss on this task batch = 3.2955e-01, Meta loss averaged over last 500 steps = 2.9092e-01, PNorm = 145.1636, GNorm = 0.3924
Meta loss on this task batch = 2.6466e-01, Meta loss averaged over last 500 steps = 2.9086e-01, PNorm = 145.1682, GNorm = 0.3672
Meta loss on this task batch = 3.0424e-01, Meta loss averaged over last 500 steps = 2.9094e-01, PNorm = 145.1729, GNorm = 0.3972
Meta loss on this task batch = 3.2295e-01, Meta loss averaged over last 500 steps = 2.9106e-01, PNorm = 145.1774, GNorm = 0.3387
Meta loss on this task batch = 3.3450e-01, Meta loss averaged over last 500 steps = 2.9120e-01, PNorm = 145.1818, GNorm = 0.4077
Meta loss on this task batch = 3.2941e-01, Meta loss averaged over last 500 steps = 2.9126e-01, PNorm = 145.1862, GNorm = 0.3744
Meta loss on this task batch = 2.9032e-01, Meta loss averaged over last 500 steps = 2.9127e-01, PNorm = 145.1912, GNorm = 0.2873
Meta loss on this task batch = 3.1656e-01, Meta loss averaged over last 500 steps = 2.9122e-01, PNorm = 145.1964, GNorm = 0.2782
Meta loss on this task batch = 2.9465e-01, Meta loss averaged over last 500 steps = 2.9128e-01, PNorm = 145.2021, GNorm = 0.2884
Meta loss on this task batch = 2.2262e-01, Meta loss averaged over last 500 steps = 2.9116e-01, PNorm = 145.2081, GNorm = 0.2330
Meta loss on this task batch = 2.7152e-01, Meta loss averaged over last 500 steps = 2.9118e-01, PNorm = 145.2146, GNorm = 0.2663
Meta loss on this task batch = 2.8477e-01, Meta loss averaged over last 500 steps = 2.9128e-01, PNorm = 145.2208, GNorm = 0.2952
Took 109.85522103309631 seconds to complete one epoch of meta training
Took 117.66853737831116 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452550
Epoch 618
Meta loss on this task batch = 3.4335e-01, Meta loss averaged over last 500 steps = 2.9135e-01, PNorm = 145.2262, GNorm = 0.2979
Meta loss on this task batch = 2.0563e-01, Meta loss averaged over last 500 steps = 2.9112e-01, PNorm = 145.2317, GNorm = 0.2055
Meta loss on this task batch = 2.7324e-01, Meta loss averaged over last 500 steps = 2.9100e-01, PNorm = 145.2366, GNorm = 0.2848
Meta loss on this task batch = 2.8566e-01, Meta loss averaged over last 500 steps = 2.9086e-01, PNorm = 145.2407, GNorm = 0.2451
Meta loss on this task batch = 2.9862e-01, Meta loss averaged over last 500 steps = 2.9085e-01, PNorm = 145.2444, GNorm = 0.2748
Meta loss on this task batch = 3.2331e-01, Meta loss averaged over last 500 steps = 2.9075e-01, PNorm = 145.2477, GNorm = 0.3488
Meta loss on this task batch = 2.9495e-01, Meta loss averaged over last 500 steps = 2.9078e-01, PNorm = 145.2506, GNorm = 0.4770
Meta loss on this task batch = 2.8216e-01, Meta loss averaged over last 500 steps = 2.9060e-01, PNorm = 145.2543, GNorm = 0.2326
Meta loss on this task batch = 2.4998e-01, Meta loss averaged over last 500 steps = 2.9057e-01, PNorm = 145.2594, GNorm = 0.2570
Meta loss on this task batch = 2.9669e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 145.2644, GNorm = 0.3066
Meta loss on this task batch = 3.2501e-01, Meta loss averaged over last 500 steps = 2.9060e-01, PNorm = 145.2688, GNorm = 0.4305
Meta loss on this task batch = 2.6089e-01, Meta loss averaged over last 500 steps = 2.9061e-01, PNorm = 145.2739, GNorm = 0.2537
Meta loss on this task batch = 2.8513e-01, Meta loss averaged over last 500 steps = 2.9061e-01, PNorm = 145.2788, GNorm = 0.3579
Meta loss on this task batch = 3.1178e-01, Meta loss averaged over last 500 steps = 2.9065e-01, PNorm = 145.2826, GNorm = 0.3164
Meta loss on this task batch = 2.7227e-01, Meta loss averaged over last 500 steps = 2.9061e-01, PNorm = 145.2863, GNorm = 0.2671
Meta loss on this task batch = 2.8835e-01, Meta loss averaged over last 500 steps = 2.9054e-01, PNorm = 145.2904, GNorm = 0.2698
Meta loss on this task batch = 2.0925e-01, Meta loss averaged over last 500 steps = 2.9038e-01, PNorm = 145.2953, GNorm = 0.2519
Meta loss on this task batch = 3.4938e-01, Meta loss averaged over last 500 steps = 2.9049e-01, PNorm = 145.3004, GNorm = 0.3038
Meta loss on this task batch = 2.1077e-01, Meta loss averaged over last 500 steps = 2.9034e-01, PNorm = 145.3063, GNorm = 0.4239
Took 112.81077027320862 seconds to complete one epoch of meta training
Took 119.74782752990723 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.435731
Epoch 619
Meta loss on this task batch = 2.9837e-01, Meta loss averaged over last 500 steps = 2.9051e-01, PNorm = 145.3115, GNorm = 0.2948
Meta loss on this task batch = 2.9159e-01, Meta loss averaged over last 500 steps = 2.9049e-01, PNorm = 145.3170, GNorm = 0.2682
Meta loss on this task batch = 2.6213e-01, Meta loss averaged over last 500 steps = 2.9048e-01, PNorm = 145.3228, GNorm = 0.2725
Meta loss on this task batch = 2.8370e-01, Meta loss averaged over last 500 steps = 2.9041e-01, PNorm = 145.3271, GNorm = 0.5392
Meta loss on this task batch = 2.9113e-01, Meta loss averaged over last 500 steps = 2.9040e-01, PNorm = 145.3320, GNorm = 0.2944
Meta loss on this task batch = 3.2532e-01, Meta loss averaged over last 500 steps = 2.9049e-01, PNorm = 145.3365, GNorm = 0.3207
Meta loss on this task batch = 2.7924e-01, Meta loss averaged over last 500 steps = 2.9055e-01, PNorm = 145.3417, GNorm = 0.2552
Meta loss on this task batch = 3.0455e-01, Meta loss averaged over last 500 steps = 2.9055e-01, PNorm = 145.3463, GNorm = 0.4221
Meta loss on this task batch = 3.0515e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 145.3511, GNorm = 0.2974
Meta loss on this task batch = 2.5001e-01, Meta loss averaged over last 500 steps = 2.9049e-01, PNorm = 145.3558, GNorm = 0.2606
Meta loss on this task batch = 3.0211e-01, Meta loss averaged over last 500 steps = 2.9045e-01, PNorm = 145.3610, GNorm = 0.2624
Meta loss on this task batch = 3.0693e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 145.3655, GNorm = 0.2690
Meta loss on this task batch = 2.4910e-01, Meta loss averaged over last 500 steps = 2.9047e-01, PNorm = 145.3705, GNorm = 0.2679
Meta loss on this task batch = 2.9010e-01, Meta loss averaged over last 500 steps = 2.9045e-01, PNorm = 145.3754, GNorm = 0.2921
Meta loss on this task batch = 3.2696e-01, Meta loss averaged over last 500 steps = 2.9043e-01, PNorm = 145.3796, GNorm = 0.2659
Meta loss on this task batch = 2.4413e-01, Meta loss averaged over last 500 steps = 2.9035e-01, PNorm = 145.3847, GNorm = 0.3368
Meta loss on this task batch = 3.0898e-01, Meta loss averaged over last 500 steps = 2.9023e-01, PNorm = 145.3892, GNorm = 0.3544
Meta loss on this task batch = 3.2229e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 145.3931, GNorm = 0.3614
Meta loss on this task batch = 2.5320e-01, Meta loss averaged over last 500 steps = 2.9022e-01, PNorm = 145.3978, GNorm = 0.3845
Took 111.99801397323608 seconds to complete one epoch of meta training
Took 120.02012395858765 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.438002
Epoch 620
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 2.9020e-01, PNorm = 145.4018, GNorm = 0.3665
Meta loss on this task batch = 2.9355e-01, Meta loss averaged over last 500 steps = 2.9021e-01, PNorm = 145.4058, GNorm = 0.4032
Meta loss on this task batch = 2.5746e-01, Meta loss averaged over last 500 steps = 2.9022e-01, PNorm = 145.4100, GNorm = 0.2507
Meta loss on this task batch = 3.0572e-01, Meta loss averaged over last 500 steps = 2.9021e-01, PNorm = 145.4138, GNorm = 0.2721
Meta loss on this task batch = 3.2819e-01, Meta loss averaged over last 500 steps = 2.9023e-01, PNorm = 145.4165, GNorm = 0.3216
Meta loss on this task batch = 2.9693e-01, Meta loss averaged over last 500 steps = 2.9025e-01, PNorm = 145.4195, GNorm = 0.3258
Meta loss on this task batch = 2.2127e-01, Meta loss averaged over last 500 steps = 2.9016e-01, PNorm = 145.4233, GNorm = 0.2131
Meta loss on this task batch = 2.6912e-01, Meta loss averaged over last 500 steps = 2.9012e-01, PNorm = 145.4267, GNorm = 0.2683
Meta loss on this task batch = 3.1983e-01, Meta loss averaged over last 500 steps = 2.9023e-01, PNorm = 145.4303, GNorm = 0.2915
Meta loss on this task batch = 2.9399e-01, Meta loss averaged over last 500 steps = 2.9021e-01, PNorm = 145.4342, GNorm = 0.2888
Meta loss on this task batch = 3.0480e-01, Meta loss averaged over last 500 steps = 2.9017e-01, PNorm = 145.4384, GNorm = 0.3038
Meta loss on this task batch = 2.3872e-01, Meta loss averaged over last 500 steps = 2.9006e-01, PNorm = 145.4421, GNorm = 0.2511
Meta loss on this task batch = 2.7453e-01, Meta loss averaged over last 500 steps = 2.9006e-01, PNorm = 145.4464, GNorm = 0.2651
Meta loss on this task batch = 2.5126e-01, Meta loss averaged over last 500 steps = 2.9007e-01, PNorm = 145.4515, GNorm = 0.2510
Meta loss on this task batch = 3.2017e-01, Meta loss averaged over last 500 steps = 2.9002e-01, PNorm = 145.4559, GNorm = 0.6212
Meta loss on this task batch = 2.9315e-01, Meta loss averaged over last 500 steps = 2.9006e-01, PNorm = 145.4600, GNorm = 0.3100
Meta loss on this task batch = 2.9554e-01, Meta loss averaged over last 500 steps = 2.9007e-01, PNorm = 145.4644, GNorm = 0.2481
Meta loss on this task batch = 2.9043e-01, Meta loss averaged over last 500 steps = 2.8999e-01, PNorm = 145.4693, GNorm = 0.2935
Meta loss on this task batch = 3.0187e-01, Meta loss averaged over last 500 steps = 2.8994e-01, PNorm = 145.4737, GNorm = 0.3117
Took 115.8607406616211 seconds to complete one epoch of meta training
Took 123.72836136817932 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455678
Epoch 621
Meta loss on this task batch = 2.6781e-01, Meta loss averaged over last 500 steps = 2.9001e-01, PNorm = 145.4788, GNorm = 0.3481
Meta loss on this task batch = 2.9350e-01, Meta loss averaged over last 500 steps = 2.9002e-01, PNorm = 145.4837, GNorm = 0.3215
Meta loss on this task batch = 2.5012e-01, Meta loss averaged over last 500 steps = 2.8990e-01, PNorm = 145.4895, GNorm = 0.2639
Meta loss on this task batch = 3.0331e-01, Meta loss averaged over last 500 steps = 2.8988e-01, PNorm = 145.4954, GNorm = 0.4825
Meta loss on this task batch = 2.7885e-01, Meta loss averaged over last 500 steps = 2.8982e-01, PNorm = 145.5017, GNorm = 0.3782
Meta loss on this task batch = 2.6510e-01, Meta loss averaged over last 500 steps = 2.8968e-01, PNorm = 145.5082, GNorm = 0.3183
Meta loss on this task batch = 3.3721e-01, Meta loss averaged over last 500 steps = 2.8983e-01, PNorm = 145.5144, GNorm = 0.3093
Meta loss on this task batch = 2.6458e-01, Meta loss averaged over last 500 steps = 2.8984e-01, PNorm = 145.5208, GNorm = 0.2891
Meta loss on this task batch = 3.6720e-01, Meta loss averaged over last 500 steps = 2.9001e-01, PNorm = 145.5269, GNorm = 0.3176
Meta loss on this task batch = 3.6559e-01, Meta loss averaged over last 500 steps = 2.9023e-01, PNorm = 145.5326, GNorm = 0.3102
Meta loss on this task batch = 2.8212e-01, Meta loss averaged over last 500 steps = 2.9013e-01, PNorm = 145.5373, GNorm = 0.2975
Meta loss on this task batch = 2.5048e-01, Meta loss averaged over last 500 steps = 2.9010e-01, PNorm = 145.5426, GNorm = 0.3515
Meta loss on this task batch = 3.0346e-01, Meta loss averaged over last 500 steps = 2.9013e-01, PNorm = 145.5481, GNorm = 0.2722
Meta loss on this task batch = 2.8908e-01, Meta loss averaged over last 500 steps = 2.9009e-01, PNorm = 145.5542, GNorm = 0.2757
Meta loss on this task batch = 2.6705e-01, Meta loss averaged over last 500 steps = 2.9005e-01, PNorm = 145.5605, GNorm = 0.2709
Meta loss on this task batch = 2.7376e-01, Meta loss averaged over last 500 steps = 2.9004e-01, PNorm = 145.5663, GNorm = 0.3185
Meta loss on this task batch = 2.6018e-01, Meta loss averaged over last 500 steps = 2.9002e-01, PNorm = 145.5723, GNorm = 0.3463
Meta loss on this task batch = 2.4625e-01, Meta loss averaged over last 500 steps = 2.8981e-01, PNorm = 145.5780, GNorm = 0.3387
Meta loss on this task batch = 2.0621e-01, Meta loss averaged over last 500 steps = 2.8967e-01, PNorm = 145.5844, GNorm = 0.2314
Took 112.75170016288757 seconds to complete one epoch of meta training
Took 120.28640913963318 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460188
Epoch 622
Meta loss on this task batch = 3.2456e-01, Meta loss averaged over last 500 steps = 2.8980e-01, PNorm = 145.5903, GNorm = 0.3062
Meta loss on this task batch = 2.8844e-01, Meta loss averaged over last 500 steps = 2.8981e-01, PNorm = 145.5964, GNorm = 0.3569
Meta loss on this task batch = 2.7507e-01, Meta loss averaged over last 500 steps = 2.8977e-01, PNorm = 145.6013, GNorm = 0.2681
Meta loss on this task batch = 2.7032e-01, Meta loss averaged over last 500 steps = 2.8977e-01, PNorm = 145.6055, GNorm = 0.3226
Meta loss on this task batch = 2.3759e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 145.6099, GNorm = 0.2474
Meta loss on this task batch = 3.0755e-01, Meta loss averaged over last 500 steps = 2.8944e-01, PNorm = 145.6144, GNorm = 0.3518
Meta loss on this task batch = 2.4320e-01, Meta loss averaged over last 500 steps = 2.8924e-01, PNorm = 145.6194, GNorm = 0.2817
Meta loss on this task batch = 2.9371e-01, Meta loss averaged over last 500 steps = 2.8925e-01, PNorm = 145.6245, GNorm = 0.3050
Meta loss on this task batch = 2.6644e-01, Meta loss averaged over last 500 steps = 2.8915e-01, PNorm = 145.6291, GNorm = 0.2867
Meta loss on this task batch = 2.1750e-01, Meta loss averaged over last 500 steps = 2.8907e-01, PNorm = 145.6340, GNorm = 0.2836
Meta loss on this task batch = 3.1025e-01, Meta loss averaged over last 500 steps = 2.8905e-01, PNorm = 145.6385, GNorm = 0.3146
Meta loss on this task batch = 2.8426e-01, Meta loss averaged over last 500 steps = 2.8903e-01, PNorm = 145.6434, GNorm = 0.3034
Meta loss on this task batch = 2.7638e-01, Meta loss averaged over last 500 steps = 2.8897e-01, PNorm = 145.6480, GNorm = 0.2572
Meta loss on this task batch = 3.1866e-01, Meta loss averaged over last 500 steps = 2.8907e-01, PNorm = 145.6517, GNorm = 0.2578
Meta loss on this task batch = 2.8623e-01, Meta loss averaged over last 500 steps = 2.8909e-01, PNorm = 145.6559, GNorm = 0.2998
Meta loss on this task batch = 2.9004e-01, Meta loss averaged over last 500 steps = 2.8924e-01, PNorm = 145.6596, GNorm = 0.2940
Meta loss on this task batch = 2.2776e-01, Meta loss averaged over last 500 steps = 2.8926e-01, PNorm = 145.6636, GNorm = 0.2524
Meta loss on this task batch = 2.8232e-01, Meta loss averaged over last 500 steps = 2.8912e-01, PNorm = 145.6675, GNorm = 0.3314
Meta loss on this task batch = 3.3060e-01, Meta loss averaged over last 500 steps = 2.8919e-01, PNorm = 145.6725, GNorm = 0.4807
Took 112.9753725528717 seconds to complete one epoch of meta training
Took 120.47111415863037 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470741
Epoch 623
Meta loss on this task batch = 2.6769e-01, Meta loss averaged over last 500 steps = 2.8919e-01, PNorm = 145.6787, GNorm = 0.2843
Meta loss on this task batch = 3.2365e-01, Meta loss averaged over last 500 steps = 2.8933e-01, PNorm = 145.6846, GNorm = 0.4087
Meta loss on this task batch = 2.9536e-01, Meta loss averaged over last 500 steps = 2.8927e-01, PNorm = 145.6903, GNorm = 0.2518
Meta loss on this task batch = 2.7358e-01, Meta loss averaged over last 500 steps = 2.8929e-01, PNorm = 145.6964, GNorm = 0.2435
Meta loss on this task batch = 2.4373e-01, Meta loss averaged over last 500 steps = 2.8934e-01, PNorm = 145.7026, GNorm = 0.2844
Meta loss on this task batch = 2.5773e-01, Meta loss averaged over last 500 steps = 2.8909e-01, PNorm = 145.7083, GNorm = 0.2687
Meta loss on this task batch = 3.6791e-01, Meta loss averaged over last 500 steps = 2.8924e-01, PNorm = 145.7134, GNorm = 0.4480
Meta loss on this task batch = 3.3372e-01, Meta loss averaged over last 500 steps = 2.8935e-01, PNorm = 145.7179, GNorm = 0.4087
Meta loss on this task batch = 2.8093e-01, Meta loss averaged over last 500 steps = 2.8939e-01, PNorm = 145.7223, GNorm = 0.2931
Meta loss on this task batch = 3.2949e-01, Meta loss averaged over last 500 steps = 2.8936e-01, PNorm = 145.7266, GNorm = 0.3461
Meta loss on this task batch = 2.6630e-01, Meta loss averaged over last 500 steps = 2.8929e-01, PNorm = 145.7308, GNorm = 0.2393
Meta loss on this task batch = 2.5632e-01, Meta loss averaged over last 500 steps = 2.8929e-01, PNorm = 145.7353, GNorm = 0.2455
Meta loss on this task batch = 3.0364e-01, Meta loss averaged over last 500 steps = 2.8938e-01, PNorm = 145.7395, GNorm = 0.2573
Meta loss on this task batch = 2.9225e-01, Meta loss averaged over last 500 steps = 2.8936e-01, PNorm = 145.7436, GNorm = 0.2852
Meta loss on this task batch = 2.8635e-01, Meta loss averaged over last 500 steps = 2.8929e-01, PNorm = 145.7480, GNorm = 0.2538
Meta loss on this task batch = 2.8349e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 145.7531, GNorm = 0.3461
Meta loss on this task batch = 2.6884e-01, Meta loss averaged over last 500 steps = 2.8910e-01, PNorm = 145.7573, GNorm = 0.2569
Meta loss on this task batch = 3.2839e-01, Meta loss averaged over last 500 steps = 2.8915e-01, PNorm = 145.7610, GNorm = 0.3840
Meta loss on this task batch = 2.6474e-01, Meta loss averaged over last 500 steps = 2.8905e-01, PNorm = 145.7646, GNorm = 0.2589
Took 111.3910300731659 seconds to complete one epoch of meta training
Took 119.20377135276794 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469372
Epoch 624
Meta loss on this task batch = 2.6901e-01, Meta loss averaged over last 500 steps = 2.8906e-01, PNorm = 145.7681, GNorm = 0.2627
Meta loss on this task batch = 3.2098e-01, Meta loss averaged over last 500 steps = 2.8916e-01, PNorm = 145.7717, GNorm = 0.3458
Meta loss on this task batch = 3.2426e-01, Meta loss averaged over last 500 steps = 2.8920e-01, PNorm = 145.7752, GNorm = 0.2997
Meta loss on this task batch = 3.6202e-01, Meta loss averaged over last 500 steps = 2.8935e-01, PNorm = 145.7777, GNorm = 0.2833
Meta loss on this task batch = 2.8384e-01, Meta loss averaged over last 500 steps = 2.8942e-01, PNorm = 145.7810, GNorm = 0.4312
Meta loss on this task batch = 2.8931e-01, Meta loss averaged over last 500 steps = 2.8936e-01, PNorm = 145.7844, GNorm = 0.3549
Meta loss on this task batch = 3.3665e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 145.7873, GNorm = 0.2605
Meta loss on this task batch = 2.3776e-01, Meta loss averaged over last 500 steps = 2.8947e-01, PNorm = 145.7903, GNorm = 0.2171
Meta loss on this task batch = 2.9011e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 145.7931, GNorm = 0.4008
Meta loss on this task batch = 3.0085e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 145.7958, GNorm = 0.2868
Meta loss on this task batch = 3.0027e-01, Meta loss averaged over last 500 steps = 2.8956e-01, PNorm = 145.7990, GNorm = 0.3799
Meta loss on this task batch = 2.9436e-01, Meta loss averaged over last 500 steps = 2.8958e-01, PNorm = 145.8015, GNorm = 0.3206
Meta loss on this task batch = 3.0613e-01, Meta loss averaged over last 500 steps = 2.8956e-01, PNorm = 145.8043, GNorm = 0.2774
Meta loss on this task batch = 2.2576e-01, Meta loss averaged over last 500 steps = 2.8928e-01, PNorm = 145.8076, GNorm = 0.2736
Meta loss on this task batch = 3.1115e-01, Meta loss averaged over last 500 steps = 2.8937e-01, PNorm = 145.8120, GNorm = 0.2547
Meta loss on this task batch = 2.5104e-01, Meta loss averaged over last 500 steps = 2.8924e-01, PNorm = 145.8175, GNorm = 0.2231
Meta loss on this task batch = 2.8816e-01, Meta loss averaged over last 500 steps = 2.8921e-01, PNorm = 145.8228, GNorm = 0.2603
Meta loss on this task batch = 2.6830e-01, Meta loss averaged over last 500 steps = 2.8900e-01, PNorm = 145.8284, GNorm = 0.2823
Meta loss on this task batch = 3.2377e-01, Meta loss averaged over last 500 steps = 2.8902e-01, PNorm = 145.8340, GNorm = 0.5823
Took 112.74860525131226 seconds to complete one epoch of meta training
Took 120.47218608856201 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462358
Epoch 625
Meta loss on this task batch = 2.9218e-01, Meta loss averaged over last 500 steps = 2.8895e-01, PNorm = 145.8400, GNorm = 0.2987
Meta loss on this task batch = 3.1384e-01, Meta loss averaged over last 500 steps = 2.8901e-01, PNorm = 145.8453, GNorm = 0.3320
Meta loss on this task batch = 2.6713e-01, Meta loss averaged over last 500 steps = 2.8891e-01, PNorm = 145.8505, GNorm = 0.2633
Meta loss on this task batch = 2.8270e-01, Meta loss averaged over last 500 steps = 2.8889e-01, PNorm = 145.8559, GNorm = 0.4343
Meta loss on this task batch = 2.7439e-01, Meta loss averaged over last 500 steps = 2.8890e-01, PNorm = 145.8613, GNorm = 0.3084
Meta loss on this task batch = 2.8294e-01, Meta loss averaged over last 500 steps = 2.8908e-01, PNorm = 145.8665, GNorm = 0.3804
Meta loss on this task batch = 2.4060e-01, Meta loss averaged over last 500 steps = 2.8897e-01, PNorm = 145.8717, GNorm = 0.2986
Meta loss on this task batch = 2.3396e-01, Meta loss averaged over last 500 steps = 2.8893e-01, PNorm = 145.8775, GNorm = 0.2482
Meta loss on this task batch = 3.6941e-01, Meta loss averaged over last 500 steps = 2.8916e-01, PNorm = 145.8820, GNorm = 0.5273
Meta loss on this task batch = 2.3046e-01, Meta loss averaged over last 500 steps = 2.8901e-01, PNorm = 145.8867, GNorm = 0.4037
Meta loss on this task batch = 2.5882e-01, Meta loss averaged over last 500 steps = 2.8900e-01, PNorm = 145.8906, GNorm = 0.3423
Meta loss on this task batch = 3.1415e-01, Meta loss averaged over last 500 steps = 2.8912e-01, PNorm = 145.8939, GNorm = 0.3465
Meta loss on this task batch = 3.1964e-01, Meta loss averaged over last 500 steps = 2.8915e-01, PNorm = 145.8967, GNorm = 0.3205
Meta loss on this task batch = 3.0741e-01, Meta loss averaged over last 500 steps = 2.8936e-01, PNorm = 145.8995, GNorm = 0.3130
Meta loss on this task batch = 3.3040e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 145.9019, GNorm = 0.3398
Meta loss on this task batch = 2.7229e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 145.9050, GNorm = 0.3382
Meta loss on this task batch = 3.3226e-01, Meta loss averaged over last 500 steps = 2.8958e-01, PNorm = 145.9075, GNorm = 0.3173
Meta loss on this task batch = 2.7805e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 145.9100, GNorm = 0.3244
Meta loss on this task batch = 2.5208e-01, Meta loss averaged over last 500 steps = 2.8935e-01, PNorm = 145.9129, GNorm = 0.3338
Took 114.07786989212036 seconds to complete one epoch of meta training
Took 121.9744942188263 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450419
Epoch 626
Meta loss on this task batch = 2.8813e-01, Meta loss averaged over last 500 steps = 2.8929e-01, PNorm = 145.9160, GNorm = 0.3723
Meta loss on this task batch = 2.8335e-01, Meta loss averaged over last 500 steps = 2.8921e-01, PNorm = 145.9195, GNorm = 0.3192
Meta loss on this task batch = 2.5425e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 145.9238, GNorm = 0.2332
Meta loss on this task batch = 3.3720e-01, Meta loss averaged over last 500 steps = 2.8922e-01, PNorm = 145.9291, GNorm = 0.3535
Meta loss on this task batch = 2.0720e-01, Meta loss averaged over last 500 steps = 2.8905e-01, PNorm = 145.9349, GNorm = 0.2726
Meta loss on this task batch = 2.3541e-01, Meta loss averaged over last 500 steps = 2.8901e-01, PNorm = 145.9412, GNorm = 0.2321
Meta loss on this task batch = 3.4597e-01, Meta loss averaged over last 500 steps = 2.8916e-01, PNorm = 145.9472, GNorm = 0.3628
Meta loss on this task batch = 3.0978e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 145.9525, GNorm = 0.3865
Meta loss on this task batch = 2.7068e-01, Meta loss averaged over last 500 steps = 2.8910e-01, PNorm = 145.9579, GNorm = 0.2825
Meta loss on this task batch = 2.5870e-01, Meta loss averaged over last 500 steps = 2.8898e-01, PNorm = 145.9635, GNorm = 0.2064
Meta loss on this task batch = 2.8880e-01, Meta loss averaged over last 500 steps = 2.8907e-01, PNorm = 145.9694, GNorm = 0.3054
Meta loss on this task batch = 2.8149e-01, Meta loss averaged over last 500 steps = 2.8899e-01, PNorm = 145.9742, GNorm = 0.2529
Meta loss on this task batch = 2.9050e-01, Meta loss averaged over last 500 steps = 2.8899e-01, PNorm = 145.9783, GNorm = 0.2499
Meta loss on this task batch = 2.8097e-01, Meta loss averaged over last 500 steps = 2.8890e-01, PNorm = 145.9822, GNorm = 0.4073
Meta loss on this task batch = 3.3523e-01, Meta loss averaged over last 500 steps = 2.8903e-01, PNorm = 145.9856, GNorm = 0.3082
Meta loss on this task batch = 2.9769e-01, Meta loss averaged over last 500 steps = 2.8903e-01, PNorm = 145.9894, GNorm = 0.2764
Meta loss on this task batch = 3.1816e-01, Meta loss averaged over last 500 steps = 2.8902e-01, PNorm = 145.9929, GNorm = 0.2357
Meta loss on this task batch = 2.8136e-01, Meta loss averaged over last 500 steps = 2.8906e-01, PNorm = 145.9970, GNorm = 0.2664
Meta loss on this task batch = 2.9319e-01, Meta loss averaged over last 500 steps = 2.8912e-01, PNorm = 146.0004, GNorm = 0.4903
Took 110.90976548194885 seconds to complete one epoch of meta training
Took 118.84943866729736 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473856
Epoch 627
Meta loss on this task batch = 2.6550e-01, Meta loss averaged over last 500 steps = 2.8913e-01, PNorm = 146.0051, GNorm = 0.2298
Meta loss on this task batch = 2.6347e-01, Meta loss averaged over last 500 steps = 2.8900e-01, PNorm = 146.0100, GNorm = 0.2523
Meta loss on this task batch = 3.1461e-01, Meta loss averaged over last 500 steps = 2.8909e-01, PNorm = 146.0140, GNorm = 0.3152
Meta loss on this task batch = 2.2706e-01, Meta loss averaged over last 500 steps = 2.8898e-01, PNorm = 146.0179, GNorm = 0.2705
Meta loss on this task batch = 2.7203e-01, Meta loss averaged over last 500 steps = 2.8880e-01, PNorm = 146.0214, GNorm = 0.2968
Meta loss on this task batch = 3.5665e-01, Meta loss averaged over last 500 steps = 2.8904e-01, PNorm = 146.0244, GNorm = 0.2990
Meta loss on this task batch = 3.0828e-01, Meta loss averaged over last 500 steps = 2.8904e-01, PNorm = 146.0269, GNorm = 0.4083
Meta loss on this task batch = 2.8199e-01, Meta loss averaged over last 500 steps = 2.8919e-01, PNorm = 146.0293, GNorm = 0.2989
Meta loss on this task batch = 3.2559e-01, Meta loss averaged over last 500 steps = 2.8928e-01, PNorm = 146.0314, GNorm = 0.3870
Meta loss on this task batch = 2.5457e-01, Meta loss averaged over last 500 steps = 2.8924e-01, PNorm = 146.0345, GNorm = 0.2588
Meta loss on this task batch = 3.1256e-01, Meta loss averaged over last 500 steps = 2.8919e-01, PNorm = 146.0384, GNorm = 0.5711
Meta loss on this task batch = 2.7130e-01, Meta loss averaged over last 500 steps = 2.8912e-01, PNorm = 146.0431, GNorm = 0.4523
Meta loss on this task batch = 3.0730e-01, Meta loss averaged over last 500 steps = 2.8915e-01, PNorm = 146.0472, GNorm = 0.2858
Meta loss on this task batch = 2.4951e-01, Meta loss averaged over last 500 steps = 2.8911e-01, PNorm = 146.0516, GNorm = 0.2938
Meta loss on this task batch = 2.6231e-01, Meta loss averaged over last 500 steps = 2.8899e-01, PNorm = 146.0563, GNorm = 0.2366
Meta loss on this task batch = 3.1056e-01, Meta loss averaged over last 500 steps = 2.8909e-01, PNorm = 146.0602, GNorm = 0.2978
Meta loss on this task batch = 3.4157e-01, Meta loss averaged over last 500 steps = 2.8911e-01, PNorm = 146.0641, GNorm = 0.3012
Meta loss on this task batch = 3.0850e-01, Meta loss averaged over last 500 steps = 2.8921e-01, PNorm = 146.0682, GNorm = 0.3517
Meta loss on this task batch = 2.8287e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 146.0725, GNorm = 0.3153
Took 110.98874354362488 seconds to complete one epoch of meta training
Took 117.91425275802612 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471607
Epoch 628
Meta loss on this task batch = 2.8405e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 146.0767, GNorm = 0.2755
Meta loss on this task batch = 2.2494e-01, Meta loss averaged over last 500 steps = 2.8898e-01, PNorm = 146.0815, GNorm = 0.3561
Meta loss on this task batch = 2.5957e-01, Meta loss averaged over last 500 steps = 2.8898e-01, PNorm = 146.0868, GNorm = 0.2630
Meta loss on this task batch = 2.7719e-01, Meta loss averaged over last 500 steps = 2.8898e-01, PNorm = 146.0920, GNorm = 0.2808
Meta loss on this task batch = 2.7259e-01, Meta loss averaged over last 500 steps = 2.8875e-01, PNorm = 146.0965, GNorm = 0.2730
Meta loss on this task batch = 3.2475e-01, Meta loss averaged over last 500 steps = 2.8875e-01, PNorm = 146.1008, GNorm = 0.4454
Meta loss on this task batch = 2.8617e-01, Meta loss averaged over last 500 steps = 2.8867e-01, PNorm = 146.1062, GNorm = 0.4988
Meta loss on this task batch = 3.5879e-01, Meta loss averaged over last 500 steps = 2.8887e-01, PNorm = 146.1112, GNorm = 0.3798
Meta loss on this task batch = 3.2054e-01, Meta loss averaged over last 500 steps = 2.8904e-01, PNorm = 146.1158, GNorm = 0.3428
Meta loss on this task batch = 3.4791e-01, Meta loss averaged over last 500 steps = 2.8916e-01, PNorm = 146.1199, GNorm = 0.3171
Meta loss on this task batch = 3.1868e-01, Meta loss averaged over last 500 steps = 2.8926e-01, PNorm = 146.1238, GNorm = 0.3336
Meta loss on this task batch = 2.8068e-01, Meta loss averaged over last 500 steps = 2.8928e-01, PNorm = 146.1281, GNorm = 0.3085
Meta loss on this task batch = 3.1330e-01, Meta loss averaged over last 500 steps = 2.8924e-01, PNorm = 146.1318, GNorm = 0.3198
Meta loss on this task batch = 2.3022e-01, Meta loss averaged over last 500 steps = 2.8898e-01, PNorm = 146.1359, GNorm = 0.2693
Meta loss on this task batch = 2.7874e-01, Meta loss averaged over last 500 steps = 2.8879e-01, PNorm = 146.1404, GNorm = 0.2606
Meta loss on this task batch = 2.8975e-01, Meta loss averaged over last 500 steps = 2.8887e-01, PNorm = 146.1446, GNorm = 0.3312
Meta loss on this task batch = 2.3663e-01, Meta loss averaged over last 500 steps = 2.8873e-01, PNorm = 146.1497, GNorm = 0.2527
Meta loss on this task batch = 2.6924e-01, Meta loss averaged over last 500 steps = 2.8871e-01, PNorm = 146.1553, GNorm = 0.3427
Meta loss on this task batch = 3.4113e-01, Meta loss averaged over last 500 steps = 2.8881e-01, PNorm = 146.1608, GNorm = 0.3947
Took 111.30323147773743 seconds to complete one epoch of meta training
Took 119.24698114395142 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468517
Epoch 629
Meta loss on this task batch = 2.5311e-01, Meta loss averaged over last 500 steps = 2.8858e-01, PNorm = 146.1670, GNorm = 0.2902
Meta loss on this task batch = 3.2065e-01, Meta loss averaged over last 500 steps = 2.8872e-01, PNorm = 146.1727, GNorm = 0.3018
Meta loss on this task batch = 2.7828e-01, Meta loss averaged over last 500 steps = 2.8865e-01, PNorm = 146.1782, GNorm = 0.2895
Meta loss on this task batch = 2.3627e-01, Meta loss averaged over last 500 steps = 2.8856e-01, PNorm = 146.1838, GNorm = 0.3151
Meta loss on this task batch = 3.0092e-01, Meta loss averaged over last 500 steps = 2.8867e-01, PNorm = 146.1888, GNorm = 0.3209
Meta loss on this task batch = 3.4729e-01, Meta loss averaged over last 500 steps = 2.8875e-01, PNorm = 146.1932, GNorm = 0.3116
Meta loss on this task batch = 2.9580e-01, Meta loss averaged over last 500 steps = 2.8886e-01, PNorm = 146.1970, GNorm = 0.3407
Meta loss on this task batch = 2.6225e-01, Meta loss averaged over last 500 steps = 2.8886e-01, PNorm = 146.2005, GNorm = 0.2346
Meta loss on this task batch = 2.5986e-01, Meta loss averaged over last 500 steps = 2.8870e-01, PNorm = 146.2048, GNorm = 0.4162
Meta loss on this task batch = 3.2718e-01, Meta loss averaged over last 500 steps = 2.8871e-01, PNorm = 146.2087, GNorm = 0.3038
Meta loss on this task batch = 2.9636e-01, Meta loss averaged over last 500 steps = 2.8872e-01, PNorm = 146.2131, GNorm = 0.2979
Meta loss on this task batch = 3.1308e-01, Meta loss averaged over last 500 steps = 2.8889e-01, PNorm = 146.2174, GNorm = 0.3075
Meta loss on this task batch = 2.7734e-01, Meta loss averaged over last 500 steps = 2.8888e-01, PNorm = 146.2223, GNorm = 0.2410
Meta loss on this task batch = 2.6951e-01, Meta loss averaged over last 500 steps = 2.8885e-01, PNorm = 146.2268, GNorm = 0.3503
Meta loss on this task batch = 3.4278e-01, Meta loss averaged over last 500 steps = 2.8896e-01, PNorm = 146.2310, GNorm = 0.3283
Meta loss on this task batch = 2.6972e-01, Meta loss averaged over last 500 steps = 2.8883e-01, PNorm = 146.2354, GNorm = 0.2313
Meta loss on this task batch = 2.8395e-01, Meta loss averaged over last 500 steps = 2.8881e-01, PNorm = 146.2398, GNorm = 0.2859
Meta loss on this task batch = 2.8954e-01, Meta loss averaged over last 500 steps = 2.8877e-01, PNorm = 146.2441, GNorm = 0.3024
Meta loss on this task batch = 2.5144e-01, Meta loss averaged over last 500 steps = 2.8867e-01, PNorm = 146.2491, GNorm = 0.2979
Took 112.068519115448 seconds to complete one epoch of meta training
Took 119.95286345481873 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453879
Epoch 630
Meta loss on this task batch = 2.6012e-01, Meta loss averaged over last 500 steps = 2.8853e-01, PNorm = 146.2545, GNorm = 0.2775
Meta loss on this task batch = 2.8195e-01, Meta loss averaged over last 500 steps = 2.8850e-01, PNorm = 146.2594, GNorm = 0.2628
Meta loss on this task batch = 2.8401e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 146.2642, GNorm = 0.2704
Meta loss on this task batch = 2.7802e-01, Meta loss averaged over last 500 steps = 2.8846e-01, PNorm = 146.2684, GNorm = 0.3092
Meta loss on this task batch = 3.3728e-01, Meta loss averaged over last 500 steps = 2.8864e-01, PNorm = 146.2716, GNorm = 0.3673
Meta loss on this task batch = 2.8766e-01, Meta loss averaged over last 500 steps = 2.8863e-01, PNorm = 146.2756, GNorm = 0.3978
Meta loss on this task batch = 2.5813e-01, Meta loss averaged over last 500 steps = 2.8854e-01, PNorm = 146.2810, GNorm = 0.2913
Meta loss on this task batch = 2.7746e-01, Meta loss averaged over last 500 steps = 2.8848e-01, PNorm = 146.2860, GNorm = 0.2633
Meta loss on this task batch = 3.3353e-01, Meta loss averaged over last 500 steps = 2.8873e-01, PNorm = 146.2912, GNorm = 0.7108
Meta loss on this task batch = 2.7911e-01, Meta loss averaged over last 500 steps = 2.8877e-01, PNorm = 146.2971, GNorm = 0.2359
Meta loss on this task batch = 3.0853e-01, Meta loss averaged over last 500 steps = 2.8871e-01, PNorm = 146.3027, GNorm = 0.4102
Meta loss on this task batch = 3.2648e-01, Meta loss averaged over last 500 steps = 2.8873e-01, PNorm = 146.3078, GNorm = 0.2965
Meta loss on this task batch = 2.2028e-01, Meta loss averaged over last 500 steps = 2.8859e-01, PNorm = 146.3131, GNorm = 0.2821
Meta loss on this task batch = 2.9065e-01, Meta loss averaged over last 500 steps = 2.8866e-01, PNorm = 146.3181, GNorm = 0.3176
Meta loss on this task batch = 2.4546e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 146.3236, GNorm = 0.3333
Meta loss on this task batch = 2.7651e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 146.3284, GNorm = 0.2786
Meta loss on this task batch = 3.5752e-01, Meta loss averaged over last 500 steps = 2.8859e-01, PNorm = 146.3329, GNorm = 0.3267
Meta loss on this task batch = 2.5982e-01, Meta loss averaged over last 500 steps = 2.8847e-01, PNorm = 146.3375, GNorm = 0.2737
Meta loss on this task batch = 2.9947e-01, Meta loss averaged over last 500 steps = 2.8840e-01, PNorm = 146.3429, GNorm = 0.3878
Took 113.48794794082642 seconds to complete one epoch of meta training
Took 121.35230207443237 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449678
Epoch 631
Meta loss on this task batch = 2.5751e-01, Meta loss averaged over last 500 steps = 2.8835e-01, PNorm = 146.3483, GNorm = 0.2959
Meta loss on this task batch = 3.1081e-01, Meta loss averaged over last 500 steps = 2.8841e-01, PNorm = 146.3542, GNorm = 0.2862
Meta loss on this task batch = 2.6876e-01, Meta loss averaged over last 500 steps = 2.8837e-01, PNorm = 146.3602, GNorm = 0.2438
Meta loss on this task batch = 2.6119e-01, Meta loss averaged over last 500 steps = 2.8833e-01, PNorm = 146.3657, GNorm = 0.2948
Meta loss on this task batch = 3.2576e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 146.3714, GNorm = 0.2874
Meta loss on this task batch = 2.8012e-01, Meta loss averaged over last 500 steps = 2.8847e-01, PNorm = 146.3774, GNorm = 0.3090
Meta loss on this task batch = 2.6753e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 146.3831, GNorm = 0.2591
Meta loss on this task batch = 2.8089e-01, Meta loss averaged over last 500 steps = 2.8844e-01, PNorm = 146.3882, GNorm = 0.3904
Meta loss on this task batch = 3.1041e-01, Meta loss averaged over last 500 steps = 2.8858e-01, PNorm = 146.3937, GNorm = 0.2630
Meta loss on this task batch = 2.9449e-01, Meta loss averaged over last 500 steps = 2.8865e-01, PNorm = 146.3999, GNorm = 0.2782
Meta loss on this task batch = 3.2031e-01, Meta loss averaged over last 500 steps = 2.8874e-01, PNorm = 146.4056, GNorm = 0.3187
Meta loss on this task batch = 3.1921e-01, Meta loss averaged over last 500 steps = 2.8882e-01, PNorm = 146.4113, GNorm = 0.2641
Meta loss on this task batch = 2.4651e-01, Meta loss averaged over last 500 steps = 2.8886e-01, PNorm = 146.4171, GNorm = 0.3860
Meta loss on this task batch = 2.7911e-01, Meta loss averaged over last 500 steps = 2.8879e-01, PNorm = 146.4221, GNorm = 0.3616
Meta loss on this task batch = 2.6435e-01, Meta loss averaged over last 500 steps = 2.8878e-01, PNorm = 146.4279, GNorm = 0.3721
Meta loss on this task batch = 2.6460e-01, Meta loss averaged over last 500 steps = 2.8868e-01, PNorm = 146.4333, GNorm = 0.2727
Meta loss on this task batch = 2.9856e-01, Meta loss averaged over last 500 steps = 2.8867e-01, PNorm = 146.4385, GNorm = 0.2692
Meta loss on this task batch = 2.9666e-01, Meta loss averaged over last 500 steps = 2.8873e-01, PNorm = 146.4432, GNorm = 0.2605
Meta loss on this task batch = 2.9632e-01, Meta loss averaged over last 500 steps = 2.8879e-01, PNorm = 146.4478, GNorm = 0.3544
Took 113.89743065834045 seconds to complete one epoch of meta training
Took 121.7038083076477 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.436580
Epoch 632
Meta loss on this task batch = 1.9845e-01, Meta loss averaged over last 500 steps = 2.8870e-01, PNorm = 146.4526, GNorm = 0.2463
Meta loss on this task batch = 2.8556e-01, Meta loss averaged over last 500 steps = 2.8858e-01, PNorm = 146.4574, GNorm = 0.3024
Meta loss on this task batch = 3.0803e-01, Meta loss averaged over last 500 steps = 2.8861e-01, PNorm = 146.4617, GNorm = 0.4663
Meta loss on this task batch = 3.2789e-01, Meta loss averaged over last 500 steps = 2.8864e-01, PNorm = 146.4661, GNorm = 0.3385
Meta loss on this task batch = 2.9600e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 146.4704, GNorm = 0.2680
Meta loss on this task batch = 2.7959e-01, Meta loss averaged over last 500 steps = 2.8857e-01, PNorm = 146.4746, GNorm = 0.3654
Meta loss on this task batch = 2.0795e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 146.4799, GNorm = 0.2593
Meta loss on this task batch = 3.4875e-01, Meta loss averaged over last 500 steps = 2.8846e-01, PNorm = 146.4844, GNorm = 0.3800
Meta loss on this task batch = 2.6438e-01, Meta loss averaged over last 500 steps = 2.8848e-01, PNorm = 146.4894, GNorm = 0.2757
Meta loss on this task batch = 2.7680e-01, Meta loss averaged over last 500 steps = 2.8848e-01, PNorm = 146.4943, GNorm = 0.3011
Meta loss on this task batch = 2.8582e-01, Meta loss averaged over last 500 steps = 2.8843e-01, PNorm = 146.4995, GNorm = 0.2721
Meta loss on this task batch = 2.7651e-01, Meta loss averaged over last 500 steps = 2.8840e-01, PNorm = 146.5052, GNorm = 0.2864
Meta loss on this task batch = 2.9622e-01, Meta loss averaged over last 500 steps = 2.8851e-01, PNorm = 146.5116, GNorm = 0.4349
Meta loss on this task batch = 2.5742e-01, Meta loss averaged over last 500 steps = 2.8840e-01, PNorm = 146.5182, GNorm = 0.2809
Meta loss on this task batch = 3.0308e-01, Meta loss averaged over last 500 steps = 2.8855e-01, PNorm = 146.5242, GNorm = 0.3352
Meta loss on this task batch = 2.6376e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 146.5302, GNorm = 0.2863
Meta loss on this task batch = 3.0873e-01, Meta loss averaged over last 500 steps = 2.8864e-01, PNorm = 146.5360, GNorm = 0.2934
Meta loss on this task batch = 3.1550e-01, Meta loss averaged over last 500 steps = 2.8873e-01, PNorm = 146.5421, GNorm = 0.3069
Meta loss on this task batch = 3.3188e-01, Meta loss averaged over last 500 steps = 2.8879e-01, PNorm = 146.5483, GNorm = 0.4234
Took 112.45837426185608 seconds to complete one epoch of meta training
Took 120.53531622886658 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472884
Epoch 633
Meta loss on this task batch = 2.7037e-01, Meta loss averaged over last 500 steps = 2.8872e-01, PNorm = 146.5545, GNorm = 0.2584
Meta loss on this task batch = 2.4763e-01, Meta loss averaged over last 500 steps = 2.8855e-01, PNorm = 146.5616, GNorm = 0.4864
Meta loss on this task batch = 2.8698e-01, Meta loss averaged over last 500 steps = 2.8861e-01, PNorm = 146.5686, GNorm = 0.3061
Meta loss on this task batch = 2.7168e-01, Meta loss averaged over last 500 steps = 2.8854e-01, PNorm = 146.5762, GNorm = 0.3277
Meta loss on this task batch = 3.7358e-01, Meta loss averaged over last 500 steps = 2.8874e-01, PNorm = 146.5829, GNorm = 0.4110
Meta loss on this task batch = 2.8017e-01, Meta loss averaged over last 500 steps = 2.8866e-01, PNorm = 146.5897, GNorm = 0.2335
Meta loss on this task batch = 3.2891e-01, Meta loss averaged over last 500 steps = 2.8871e-01, PNorm = 146.5969, GNorm = 0.3329
Meta loss on this task batch = 2.5573e-01, Meta loss averaged over last 500 steps = 2.8862e-01, PNorm = 146.6038, GNorm = 0.3190
Meta loss on this task batch = 3.0108e-01, Meta loss averaged over last 500 steps = 2.8870e-01, PNorm = 146.6103, GNorm = 0.3354
Meta loss on this task batch = 3.1284e-01, Meta loss averaged over last 500 steps = 2.8882e-01, PNorm = 146.6166, GNorm = 0.2913
Meta loss on this task batch = 2.6382e-01, Meta loss averaged over last 500 steps = 2.8869e-01, PNorm = 146.6227, GNorm = 0.2513
Meta loss on this task batch = 2.6198e-01, Meta loss averaged over last 500 steps = 2.8869e-01, PNorm = 146.6286, GNorm = 0.3576
Meta loss on this task batch = 3.5644e-01, Meta loss averaged over last 500 steps = 2.8880e-01, PNorm = 146.6348, GNorm = 0.3518
Meta loss on this task batch = 2.7362e-01, Meta loss averaged over last 500 steps = 2.8874e-01, PNorm = 146.6411, GNorm = 0.3963
Meta loss on this task batch = 3.0978e-01, Meta loss averaged over last 500 steps = 2.8873e-01, PNorm = 146.6468, GNorm = 0.2906
Meta loss on this task batch = 3.1240e-01, Meta loss averaged over last 500 steps = 2.8874e-01, PNorm = 146.6517, GNorm = 0.3805
Meta loss on this task batch = 3.3568e-01, Meta loss averaged over last 500 steps = 2.8877e-01, PNorm = 146.6562, GNorm = 0.2791
Meta loss on this task batch = 2.6290e-01, Meta loss averaged over last 500 steps = 2.8874e-01, PNorm = 146.6603, GNorm = 0.4251
Meta loss on this task batch = 2.2881e-01, Meta loss averaged over last 500 steps = 2.8869e-01, PNorm = 146.6647, GNorm = 0.2657
Took 111.83805298805237 seconds to complete one epoch of meta training
Took 118.85070967674255 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467985
Epoch 634
Meta loss on this task batch = 3.2476e-01, Meta loss averaged over last 500 steps = 2.8878e-01, PNorm = 146.6687, GNorm = 0.3725
Meta loss on this task batch = 2.8284e-01, Meta loss averaged over last 500 steps = 2.8880e-01, PNorm = 146.6722, GNorm = 0.3808
Meta loss on this task batch = 2.8509e-01, Meta loss averaged over last 500 steps = 2.8879e-01, PNorm = 146.6753, GNorm = 0.2615
Meta loss on this task batch = 2.4965e-01, Meta loss averaged over last 500 steps = 2.8871e-01, PNorm = 146.6784, GNorm = 0.2410
Meta loss on this task batch = 2.9992e-01, Meta loss averaged over last 500 steps = 2.8868e-01, PNorm = 146.6817, GNorm = 0.2523
Meta loss on this task batch = 2.5271e-01, Meta loss averaged over last 500 steps = 2.8875e-01, PNorm = 146.6857, GNorm = 0.2534
Meta loss on this task batch = 2.4562e-01, Meta loss averaged over last 500 steps = 2.8859e-01, PNorm = 146.6901, GNorm = 0.3412
Meta loss on this task batch = 2.9485e-01, Meta loss averaged over last 500 steps = 2.8871e-01, PNorm = 146.6944, GNorm = 0.2960
Meta loss on this task batch = 2.4473e-01, Meta loss averaged over last 500 steps = 2.8871e-01, PNorm = 146.6991, GNorm = 0.3920
Meta loss on this task batch = 2.4572e-01, Meta loss averaged over last 500 steps = 2.8865e-01, PNorm = 146.7049, GNorm = 0.2556
Meta loss on this task batch = 3.2655e-01, Meta loss averaged over last 500 steps = 2.8867e-01, PNorm = 146.7099, GNorm = 0.3758
Meta loss on this task batch = 2.9411e-01, Meta loss averaged over last 500 steps = 2.8864e-01, PNorm = 146.7153, GNorm = 0.2793
Meta loss on this task batch = 3.5677e-01, Meta loss averaged over last 500 steps = 2.8884e-01, PNorm = 146.7212, GNorm = 0.3715
Meta loss on this task batch = 2.4728e-01, Meta loss averaged over last 500 steps = 2.8872e-01, PNorm = 146.7274, GNorm = 0.3146
Meta loss on this task batch = 3.5264e-01, Meta loss averaged over last 500 steps = 2.8881e-01, PNorm = 146.7334, GNorm = 0.3766
Meta loss on this task batch = 2.8966e-01, Meta loss averaged over last 500 steps = 2.8866e-01, PNorm = 146.7385, GNorm = 0.2726
Meta loss on this task batch = 2.2168e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 146.7439, GNorm = 0.2910
Meta loss on this task batch = 3.0397e-01, Meta loss averaged over last 500 steps = 2.8847e-01, PNorm = 146.7490, GNorm = 0.2940
Meta loss on this task batch = 3.1692e-01, Meta loss averaged over last 500 steps = 2.8857e-01, PNorm = 146.7540, GNorm = 0.3649
Took 112.87992191314697 seconds to complete one epoch of meta training
Took 120.55902791023254 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470150
Epoch 635
Meta loss on this task batch = 3.0967e-01, Meta loss averaged over last 500 steps = 2.8860e-01, PNorm = 146.7587, GNorm = 0.3877
Meta loss on this task batch = 2.6898e-01, Meta loss averaged over last 500 steps = 2.8860e-01, PNorm = 146.7632, GNorm = 0.2787
Meta loss on this task batch = 2.4918e-01, Meta loss averaged over last 500 steps = 2.8850e-01, PNorm = 146.7681, GNorm = 0.2748
Meta loss on this task batch = 2.3239e-01, Meta loss averaged over last 500 steps = 2.8841e-01, PNorm = 146.7728, GNorm = 0.3112
Meta loss on this task batch = 3.3275e-01, Meta loss averaged over last 500 steps = 2.8842e-01, PNorm = 146.7767, GNorm = 0.3084
Meta loss on this task batch = 2.7231e-01, Meta loss averaged over last 500 steps = 2.8839e-01, PNorm = 146.7803, GNorm = 0.4017
Meta loss on this task batch = 2.9566e-01, Meta loss averaged over last 500 steps = 2.8842e-01, PNorm = 146.7841, GNorm = 0.2337
Meta loss on this task batch = 2.6959e-01, Meta loss averaged over last 500 steps = 2.8843e-01, PNorm = 146.7887, GNorm = 0.2603
Meta loss on this task batch = 2.9056e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 146.7924, GNorm = 0.2685
Meta loss on this task batch = 2.8198e-01, Meta loss averaged over last 500 steps = 2.8849e-01, PNorm = 146.7962, GNorm = 0.2855
Meta loss on this task batch = 2.6324e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 146.8001, GNorm = 0.2895
Meta loss on this task batch = 3.4211e-01, Meta loss averaged over last 500 steps = 2.8836e-01, PNorm = 146.8038, GNorm = 0.4027
Meta loss on this task batch = 3.2000e-01, Meta loss averaged over last 500 steps = 2.8846e-01, PNorm = 146.8073, GNorm = 0.4020
Meta loss on this task batch = 2.9827e-01, Meta loss averaged over last 500 steps = 2.8851e-01, PNorm = 146.8107, GNorm = 0.2699
Meta loss on this task batch = 2.8624e-01, Meta loss averaged over last 500 steps = 2.8856e-01, PNorm = 146.8151, GNorm = 0.2743
Meta loss on this task batch = 2.9189e-01, Meta loss averaged over last 500 steps = 2.8848e-01, PNorm = 146.8199, GNorm = 0.2995
Meta loss on this task batch = 2.6156e-01, Meta loss averaged over last 500 steps = 2.8832e-01, PNorm = 146.8249, GNorm = 0.2388
Meta loss on this task batch = 2.6462e-01, Meta loss averaged over last 500 steps = 2.8824e-01, PNorm = 146.8303, GNorm = 0.2620
Meta loss on this task batch = 3.0282e-01, Meta loss averaged over last 500 steps = 2.8818e-01, PNorm = 146.8348, GNorm = 0.3735
Took 112.67278838157654 seconds to complete one epoch of meta training
Took 120.72064733505249 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481338
Epoch 636
Meta loss on this task batch = 2.9362e-01, Meta loss averaged over last 500 steps = 2.8822e-01, PNorm = 146.8393, GNorm = 0.2695
Meta loss on this task batch = 2.3839e-01, Meta loss averaged over last 500 steps = 2.8822e-01, PNorm = 146.8435, GNorm = 0.4358
Meta loss on this task batch = 3.2493e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 146.8475, GNorm = 0.2694
Meta loss on this task batch = 3.3943e-01, Meta loss averaged over last 500 steps = 2.8849e-01, PNorm = 146.8504, GNorm = 0.2871
Meta loss on this task batch = 2.5991e-01, Meta loss averaged over last 500 steps = 2.8836e-01, PNorm = 146.8525, GNorm = 0.2633
Meta loss on this task batch = 2.9210e-01, Meta loss averaged over last 500 steps = 2.8837e-01, PNorm = 146.8549, GNorm = 0.2577
Meta loss on this task batch = 2.5443e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 146.8570, GNorm = 0.3356
Meta loss on this task batch = 2.7098e-01, Meta loss averaged over last 500 steps = 2.8827e-01, PNorm = 146.8595, GNorm = 0.2563
Meta loss on this task batch = 3.6128e-01, Meta loss averaged over last 500 steps = 2.8843e-01, PNorm = 146.8615, GNorm = 0.3259
Meta loss on this task batch = 2.8273e-01, Meta loss averaged over last 500 steps = 2.8839e-01, PNorm = 146.8634, GNorm = 0.3737
Meta loss on this task batch = 2.6138e-01, Meta loss averaged over last 500 steps = 2.8835e-01, PNorm = 146.8660, GNorm = 0.3211
Meta loss on this task batch = 2.8849e-01, Meta loss averaged over last 500 steps = 2.8816e-01, PNorm = 146.8685, GNorm = 0.4250
Meta loss on this task batch = 3.5654e-01, Meta loss averaged over last 500 steps = 2.8823e-01, PNorm = 146.8702, GNorm = 0.3104
Meta loss on this task batch = 2.5008e-01, Meta loss averaged over last 500 steps = 2.8816e-01, PNorm = 146.8719, GNorm = 0.2928
Meta loss on this task batch = 2.8938e-01, Meta loss averaged over last 500 steps = 2.8813e-01, PNorm = 146.8748, GNorm = 0.3406
Meta loss on this task batch = 3.0719e-01, Meta loss averaged over last 500 steps = 2.8817e-01, PNorm = 146.8780, GNorm = 0.2986
Meta loss on this task batch = 2.8480e-01, Meta loss averaged over last 500 steps = 2.8820e-01, PNorm = 146.8815, GNorm = 0.2650
Meta loss on this task batch = 3.2169e-01, Meta loss averaged over last 500 steps = 2.8835e-01, PNorm = 146.8859, GNorm = 0.3203
Meta loss on this task batch = 3.0828e-01, Meta loss averaged over last 500 steps = 2.8842e-01, PNorm = 146.8904, GNorm = 0.5148
Took 111.35387134552002 seconds to complete one epoch of meta training
Took 119.37179279327393 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490702
Epoch 637
Meta loss on this task batch = 3.1457e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 146.8953, GNorm = 0.3666
Meta loss on this task batch = 2.7401e-01, Meta loss averaged over last 500 steps = 2.8854e-01, PNorm = 146.9004, GNorm = 0.3386
Meta loss on this task batch = 2.8518e-01, Meta loss averaged over last 500 steps = 2.8849e-01, PNorm = 146.9062, GNorm = 0.3370
Meta loss on this task batch = 2.5804e-01, Meta loss averaged over last 500 steps = 2.8839e-01, PNorm = 146.9124, GNorm = 0.2850
Meta loss on this task batch = 2.9787e-01, Meta loss averaged over last 500 steps = 2.8841e-01, PNorm = 146.9180, GNorm = 0.4140
Meta loss on this task batch = 2.6870e-01, Meta loss averaged over last 500 steps = 2.8839e-01, PNorm = 146.9234, GNorm = 0.2677
Meta loss on this task batch = 2.6781e-01, Meta loss averaged over last 500 steps = 2.8835e-01, PNorm = 146.9288, GNorm = 0.2475
Meta loss on this task batch = 3.3824e-01, Meta loss averaged over last 500 steps = 2.8846e-01, PNorm = 146.9337, GNorm = 0.3208
Meta loss on this task batch = 2.8319e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 146.9392, GNorm = 0.3881
Meta loss on this task batch = 2.8709e-01, Meta loss averaged over last 500 steps = 2.8856e-01, PNorm = 146.9453, GNorm = 0.3199
Meta loss on this task batch = 2.6391e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 146.9514, GNorm = 0.2691
Meta loss on this task batch = 3.1059e-01, Meta loss averaged over last 500 steps = 2.8854e-01, PNorm = 146.9575, GNorm = 0.2501
Meta loss on this task batch = 2.7069e-01, Meta loss averaged over last 500 steps = 2.8836e-01, PNorm = 146.9640, GNorm = 0.3827
Meta loss on this task batch = 3.1642e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 146.9683, GNorm = 0.5083
Meta loss on this task batch = 2.7676e-01, Meta loss averaged over last 500 steps = 2.8830e-01, PNorm = 146.9728, GNorm = 0.3226
Meta loss on this task batch = 2.8836e-01, Meta loss averaged over last 500 steps = 2.8821e-01, PNorm = 146.9764, GNorm = 0.2929
Meta loss on this task batch = 3.1682e-01, Meta loss averaged over last 500 steps = 2.8821e-01, PNorm = 146.9809, GNorm = 0.3496
Meta loss on this task batch = 2.7572e-01, Meta loss averaged over last 500 steps = 2.8828e-01, PNorm = 146.9850, GNorm = 0.2906
Meta loss on this task batch = 3.2331e-01, Meta loss averaged over last 500 steps = 2.8838e-01, PNorm = 146.9888, GNorm = 0.4187
Took 109.43231177330017 seconds to complete one epoch of meta training
Took 117.29375863075256 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491403
Epoch 638
Meta loss on this task batch = 3.4833e-01, Meta loss averaged over last 500 steps = 2.8863e-01, PNorm = 146.9929, GNorm = 0.5333
Meta loss on this task batch = 2.1874e-01, Meta loss averaged over last 500 steps = 2.8843e-01, PNorm = 146.9975, GNorm = 0.2451
Meta loss on this task batch = 3.0414e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 147.0024, GNorm = 0.3502
Meta loss on this task batch = 2.7051e-01, Meta loss averaged over last 500 steps = 2.8833e-01, PNorm = 147.0075, GNorm = 0.2701
Meta loss on this task batch = 3.3639e-01, Meta loss averaged over last 500 steps = 2.8842e-01, PNorm = 147.0122, GNorm = 0.2698
Meta loss on this task batch = 2.4291e-01, Meta loss averaged over last 500 steps = 2.8841e-01, PNorm = 147.0164, GNorm = 0.3257
Meta loss on this task batch = 2.7409e-01, Meta loss averaged over last 500 steps = 2.8829e-01, PNorm = 147.0206, GNorm = 0.2495
Meta loss on this task batch = 2.8974e-01, Meta loss averaged over last 500 steps = 2.8817e-01, PNorm = 147.0247, GNorm = 0.2848
Meta loss on this task batch = 3.2443e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 147.0287, GNorm = 0.2890
Meta loss on this task batch = 2.7116e-01, Meta loss averaged over last 500 steps = 2.8823e-01, PNorm = 147.0328, GNorm = 0.2538
Meta loss on this task batch = 3.0225e-01, Meta loss averaged over last 500 steps = 2.8825e-01, PNorm = 147.0375, GNorm = 0.2870
Meta loss on this task batch = 2.6130e-01, Meta loss averaged over last 500 steps = 2.8825e-01, PNorm = 147.0434, GNorm = 0.2564
Meta loss on this task batch = 2.9383e-01, Meta loss averaged over last 500 steps = 2.8823e-01, PNorm = 147.0495, GNorm = 0.4392
Meta loss on this task batch = 2.4780e-01, Meta loss averaged over last 500 steps = 2.8818e-01, PNorm = 147.0557, GNorm = 0.2311
Meta loss on this task batch = 2.5941e-01, Meta loss averaged over last 500 steps = 2.8818e-01, PNorm = 147.0624, GNorm = 0.2657
Meta loss on this task batch = 3.0346e-01, Meta loss averaged over last 500 steps = 2.8819e-01, PNorm = 147.0686, GNorm = 0.5565
Meta loss on this task batch = 3.2832e-01, Meta loss averaged over last 500 steps = 2.8832e-01, PNorm = 147.0744, GNorm = 0.6215
Meta loss on this task batch = 2.8564e-01, Meta loss averaged over last 500 steps = 2.8841e-01, PNorm = 147.0807, GNorm = 0.3585
Meta loss on this task batch = 3.4308e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 147.0862, GNorm = 0.3340
Took 110.65753388404846 seconds to complete one epoch of meta training
Took 118.38289713859558 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459669
Epoch 639
Meta loss on this task batch = 2.7401e-01, Meta loss averaged over last 500 steps = 2.8836e-01, PNorm = 147.0916, GNorm = 0.2418
Meta loss on this task batch = 3.0825e-01, Meta loss averaged over last 500 steps = 2.8836e-01, PNorm = 147.0971, GNorm = 0.3502
Meta loss on this task batch = 3.0688e-01, Meta loss averaged over last 500 steps = 2.8843e-01, PNorm = 147.1024, GNorm = 0.2934
Meta loss on this task batch = 3.0354e-01, Meta loss averaged over last 500 steps = 2.8849e-01, PNorm = 147.1069, GNorm = 0.3164
Meta loss on this task batch = 2.8809e-01, Meta loss averaged over last 500 steps = 2.8844e-01, PNorm = 147.1118, GNorm = 0.3603
Meta loss on this task batch = 3.3096e-01, Meta loss averaged over last 500 steps = 2.8851e-01, PNorm = 147.1163, GNorm = 0.3603
Meta loss on this task batch = 2.5798e-01, Meta loss averaged over last 500 steps = 2.8850e-01, PNorm = 147.1201, GNorm = 0.3645
Meta loss on this task batch = 2.1975e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 147.1243, GNorm = 0.4651
Meta loss on this task batch = 2.5229e-01, Meta loss averaged over last 500 steps = 2.8835e-01, PNorm = 147.1286, GNorm = 0.3678
Meta loss on this task batch = 2.6914e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 147.1334, GNorm = 0.2947
Meta loss on this task batch = 2.9549e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 147.1384, GNorm = 0.3361
Meta loss on this task batch = 2.9024e-01, Meta loss averaged over last 500 steps = 2.8822e-01, PNorm = 147.1429, GNorm = 0.3582
Meta loss on this task batch = 2.9086e-01, Meta loss averaged over last 500 steps = 2.8828e-01, PNorm = 147.1477, GNorm = 0.2585
Meta loss on this task batch = 2.6791e-01, Meta loss averaged over last 500 steps = 2.8825e-01, PNorm = 147.1528, GNorm = 0.2535
Meta loss on this task batch = 3.1949e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 147.1585, GNorm = 0.2997
Meta loss on this task batch = 4.0191e-01, Meta loss averaged over last 500 steps = 2.8851e-01, PNorm = 147.1630, GNorm = 0.4318
Meta loss on this task batch = 2.9278e-01, Meta loss averaged over last 500 steps = 2.8851e-01, PNorm = 147.1679, GNorm = 0.4090
Meta loss on this task batch = 2.9046e-01, Meta loss averaged over last 500 steps = 2.8853e-01, PNorm = 147.1729, GNorm = 0.2890
Meta loss on this task batch = 2.5211e-01, Meta loss averaged over last 500 steps = 2.8853e-01, PNorm = 147.1781, GNorm = 0.3755
Took 111.31357407569885 seconds to complete one epoch of meta training
Took 118.32708787918091 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451820
Epoch 640
Meta loss on this task batch = 2.4360e-01, Meta loss averaged over last 500 steps = 2.8855e-01, PNorm = 147.1837, GNorm = 0.2342
Meta loss on this task batch = 2.8944e-01, Meta loss averaged over last 500 steps = 2.8856e-01, PNorm = 147.1899, GNorm = 0.2663
Meta loss on this task batch = 3.0673e-01, Meta loss averaged over last 500 steps = 2.8866e-01, PNorm = 147.1949, GNorm = 0.3422
Meta loss on this task batch = 2.3993e-01, Meta loss averaged over last 500 steps = 2.8861e-01, PNorm = 147.2000, GNorm = 0.2315
Meta loss on this task batch = 3.2943e-01, Meta loss averaged over last 500 steps = 2.8855e-01, PNorm = 147.2052, GNorm = 0.3034
Meta loss on this task batch = 2.7782e-01, Meta loss averaged over last 500 steps = 2.8849e-01, PNorm = 147.2105, GNorm = 0.2599
Meta loss on this task batch = 2.4353e-01, Meta loss averaged over last 500 steps = 2.8850e-01, PNorm = 147.2167, GNorm = 0.2383
Meta loss on this task batch = 2.7361e-01, Meta loss averaged over last 500 steps = 2.8846e-01, PNorm = 147.2231, GNorm = 0.2439
Meta loss on this task batch = 3.0812e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 147.2301, GNorm = 0.5015
Meta loss on this task batch = 3.1337e-01, Meta loss averaged over last 500 steps = 2.8838e-01, PNorm = 147.2368, GNorm = 0.2636
Meta loss on this task batch = 2.8270e-01, Meta loss averaged over last 500 steps = 2.8843e-01, PNorm = 147.2429, GNorm = 0.2901
Meta loss on this task batch = 2.7828e-01, Meta loss averaged over last 500 steps = 2.8839e-01, PNorm = 147.2487, GNorm = 0.3156
Meta loss on this task batch = 2.9433e-01, Meta loss averaged over last 500 steps = 2.8840e-01, PNorm = 147.2542, GNorm = 0.3623
Meta loss on this task batch = 3.3442e-01, Meta loss averaged over last 500 steps = 2.8857e-01, PNorm = 147.2593, GNorm = 0.3513
Meta loss on this task batch = 2.5518e-01, Meta loss averaged over last 500 steps = 2.8848e-01, PNorm = 147.2644, GNorm = 0.2391
Meta loss on this task batch = 3.3409e-01, Meta loss averaged over last 500 steps = 2.8861e-01, PNorm = 147.2689, GNorm = 0.3219
Meta loss on this task batch = 2.5716e-01, Meta loss averaged over last 500 steps = 2.8866e-01, PNorm = 147.2740, GNorm = 0.2928
Meta loss on this task batch = 3.3562e-01, Meta loss averaged over last 500 steps = 2.8858e-01, PNorm = 147.2778, GNorm = 0.3584
Meta loss on this task batch = 3.0040e-01, Meta loss averaged over last 500 steps = 2.8864e-01, PNorm = 147.2821, GNorm = 0.4033
Took 111.7953109741211 seconds to complete one epoch of meta training
Took 119.79889416694641 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469043
Epoch 641
Meta loss on this task batch = 2.7061e-01, Meta loss averaged over last 500 steps = 2.8861e-01, PNorm = 147.2871, GNorm = 0.2353
Meta loss on this task batch = 2.7062e-01, Meta loss averaged over last 500 steps = 2.8868e-01, PNorm = 147.2923, GNorm = 0.3460
Meta loss on this task batch = 3.1004e-01, Meta loss averaged over last 500 steps = 2.8870e-01, PNorm = 147.2971, GNorm = 0.2699
Meta loss on this task batch = 2.3506e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 147.3023, GNorm = 0.2185
Meta loss on this task batch = 2.7302e-01, Meta loss averaged over last 500 steps = 2.8846e-01, PNorm = 147.3078, GNorm = 0.3152
Meta loss on this task batch = 2.8390e-01, Meta loss averaged over last 500 steps = 2.8846e-01, PNorm = 147.3134, GNorm = 0.2867
Meta loss on this task batch = 2.5714e-01, Meta loss averaged over last 500 steps = 2.8837e-01, PNorm = 147.3198, GNorm = 0.2812
Meta loss on this task batch = 3.3130e-01, Meta loss averaged over last 500 steps = 2.8848e-01, PNorm = 147.3255, GNorm = 0.2414
Meta loss on this task batch = 2.8389e-01, Meta loss averaged over last 500 steps = 2.8840e-01, PNorm = 147.3315, GNorm = 0.2798
Meta loss on this task batch = 2.9583e-01, Meta loss averaged over last 500 steps = 2.8837e-01, PNorm = 147.3363, GNorm = 0.2697
Meta loss on this task batch = 2.7487e-01, Meta loss averaged over last 500 steps = 2.8832e-01, PNorm = 147.3415, GNorm = 0.2520
Meta loss on this task batch = 3.1295e-01, Meta loss averaged over last 500 steps = 2.8840e-01, PNorm = 147.3466, GNorm = 0.3035
Meta loss on this task batch = 3.3006e-01, Meta loss averaged over last 500 steps = 2.8855e-01, PNorm = 147.3509, GNorm = 0.3646
Meta loss on this task batch = 3.1276e-01, Meta loss averaged over last 500 steps = 2.8859e-01, PNorm = 147.3545, GNorm = 0.2811
Meta loss on this task batch = 2.7586e-01, Meta loss averaged over last 500 steps = 2.8856e-01, PNorm = 147.3577, GNorm = 0.2983
Meta loss on this task batch = 2.0925e-01, Meta loss averaged over last 500 steps = 2.8840e-01, PNorm = 147.3613, GNorm = 0.2179
Meta loss on this task batch = 2.9894e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 147.3652, GNorm = 0.2873
Meta loss on this task batch = 3.3524e-01, Meta loss averaged over last 500 steps = 2.8842e-01, PNorm = 147.3688, GNorm = 0.4520
Meta loss on this task batch = 3.0006e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 147.3725, GNorm = 0.3716
Took 111.21942138671875 seconds to complete one epoch of meta training
Took 119.23871755599976 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455315
Epoch 642
Meta loss on this task batch = 3.1660e-01, Meta loss averaged over last 500 steps = 2.8851e-01, PNorm = 147.3761, GNorm = 0.3166
Meta loss on this task batch = 3.0110e-01, Meta loss averaged over last 500 steps = 2.8853e-01, PNorm = 147.3806, GNorm = 0.2935
Meta loss on this task batch = 2.6667e-01, Meta loss averaged over last 500 steps = 2.8844e-01, PNorm = 147.3863, GNorm = 0.3657
Meta loss on this task batch = 3.0727e-01, Meta loss averaged over last 500 steps = 2.8848e-01, PNorm = 147.3918, GNorm = 0.3523
Meta loss on this task batch = 2.7698e-01, Meta loss averaged over last 500 steps = 2.8850e-01, PNorm = 147.3974, GNorm = 0.3257
Meta loss on this task batch = 3.0638e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 147.4028, GNorm = 0.2881
Meta loss on this task batch = 3.0433e-01, Meta loss averaged over last 500 steps = 2.8844e-01, PNorm = 147.4084, GNorm = 0.2708
Meta loss on this task batch = 2.2513e-01, Meta loss averaged over last 500 steps = 2.8828e-01, PNorm = 147.4147, GNorm = 0.2630
Meta loss on this task batch = 2.6431e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 147.4212, GNorm = 0.2419
Meta loss on this task batch = 3.8170e-01, Meta loss averaged over last 500 steps = 2.8854e-01, PNorm = 147.4276, GNorm = 0.3466
Meta loss on this task batch = 2.7845e-01, Meta loss averaged over last 500 steps = 2.8842e-01, PNorm = 147.4336, GNorm = 0.3026
Meta loss on this task batch = 2.7096e-01, Meta loss averaged over last 500 steps = 2.8842e-01, PNorm = 147.4402, GNorm = 0.3004
Meta loss on this task batch = 2.4673e-01, Meta loss averaged over last 500 steps = 2.8842e-01, PNorm = 147.4465, GNorm = 0.2242
Meta loss on this task batch = 2.8118e-01, Meta loss averaged over last 500 steps = 2.8827e-01, PNorm = 147.4524, GNorm = 0.2521
Meta loss on this task batch = 3.2941e-01, Meta loss averaged over last 500 steps = 2.8836e-01, PNorm = 147.4572, GNorm = 0.3236
Meta loss on this task batch = 3.0232e-01, Meta loss averaged over last 500 steps = 2.8831e-01, PNorm = 147.4613, GNorm = 0.2649
Meta loss on this task batch = 2.7381e-01, Meta loss averaged over last 500 steps = 2.8827e-01, PNorm = 147.4656, GNorm = 0.2496
Meta loss on this task batch = 2.5492e-01, Meta loss averaged over last 500 steps = 2.8820e-01, PNorm = 147.4696, GNorm = 0.2414
Meta loss on this task batch = 2.7428e-01, Meta loss averaged over last 500 steps = 2.8811e-01, PNorm = 147.4743, GNorm = 0.3168
Took 112.93216156959534 seconds to complete one epoch of meta training
Took 120.78617262840271 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479449
Epoch 643
Meta loss on this task batch = 3.0445e-01, Meta loss averaged over last 500 steps = 2.8811e-01, PNorm = 147.4789, GNorm = 0.2979
Meta loss on this task batch = 2.4679e-01, Meta loss averaged over last 500 steps = 2.8809e-01, PNorm = 147.4836, GNorm = 0.2951
Meta loss on this task batch = 2.7150e-01, Meta loss averaged over last 500 steps = 2.8802e-01, PNorm = 147.4882, GNorm = 0.2228
Meta loss on this task batch = 2.4238e-01, Meta loss averaged over last 500 steps = 2.8790e-01, PNorm = 147.4927, GNorm = 0.2409
Meta loss on this task batch = 2.6709e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 147.4972, GNorm = 0.2901
Meta loss on this task batch = 3.1863e-01, Meta loss averaged over last 500 steps = 2.8796e-01, PNorm = 147.5011, GNorm = 0.3216
Meta loss on this task batch = 2.3710e-01, Meta loss averaged over last 500 steps = 2.8788e-01, PNorm = 147.5046, GNorm = 0.2703
Meta loss on this task batch = 3.1031e-01, Meta loss averaged over last 500 steps = 2.8800e-01, PNorm = 147.5071, GNorm = 0.3115
Meta loss on this task batch = 3.1274e-01, Meta loss averaged over last 500 steps = 2.8809e-01, PNorm = 147.5094, GNorm = 0.3205
Meta loss on this task batch = 2.7469e-01, Meta loss averaged over last 500 steps = 2.8813e-01, PNorm = 147.5121, GNorm = 0.2845
Meta loss on this task batch = 2.8782e-01, Meta loss averaged over last 500 steps = 2.8807e-01, PNorm = 147.5147, GNorm = 0.4025
Meta loss on this task batch = 3.1443e-01, Meta loss averaged over last 500 steps = 2.8809e-01, PNorm = 147.5178, GNorm = 0.3206
Meta loss on this task batch = 2.8556e-01, Meta loss averaged over last 500 steps = 2.8809e-01, PNorm = 147.5209, GNorm = 0.3012
Meta loss on this task batch = 2.5222e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 147.5247, GNorm = 0.2817
Meta loss on this task batch = 2.8203e-01, Meta loss averaged over last 500 steps = 2.8797e-01, PNorm = 147.5284, GNorm = 0.2692
Meta loss on this task batch = 3.2515e-01, Meta loss averaged over last 500 steps = 2.8802e-01, PNorm = 147.5325, GNorm = 0.4648
Meta loss on this task batch = 2.3874e-01, Meta loss averaged over last 500 steps = 2.8785e-01, PNorm = 147.5367, GNorm = 0.2777
Meta loss on this task batch = 3.0905e-01, Meta loss averaged over last 500 steps = 2.8780e-01, PNorm = 147.5412, GNorm = 0.2657
Meta loss on this task batch = 2.8198e-01, Meta loss averaged over last 500 steps = 2.8770e-01, PNorm = 147.5467, GNorm = 0.5952
Took 113.2732765674591 seconds to complete one epoch of meta training
Took 120.90372037887573 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465100
Epoch 644
Meta loss on this task batch = 2.9210e-01, Meta loss averaged over last 500 steps = 2.8771e-01, PNorm = 147.5519, GNorm = 0.2481
Meta loss on this task batch = 2.7875e-01, Meta loss averaged over last 500 steps = 2.8763e-01, PNorm = 147.5575, GNorm = 0.3107
Meta loss on this task batch = 2.7095e-01, Meta loss averaged over last 500 steps = 2.8758e-01, PNorm = 147.5633, GNorm = 0.3643
Meta loss on this task batch = 3.2010e-01, Meta loss averaged over last 500 steps = 2.8778e-01, PNorm = 147.5685, GNorm = 0.3408
Meta loss on this task batch = 3.0105e-01, Meta loss averaged over last 500 steps = 2.8784e-01, PNorm = 147.5730, GNorm = 0.2939
Meta loss on this task batch = 2.8219e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 147.5772, GNorm = 0.3934
Meta loss on this task batch = 2.9367e-01, Meta loss averaged over last 500 steps = 2.8773e-01, PNorm = 147.5811, GNorm = 0.2620
Meta loss on this task batch = 2.9716e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 147.5850, GNorm = 0.2716
Meta loss on this task batch = 3.2409e-01, Meta loss averaged over last 500 steps = 2.8802e-01, PNorm = 147.5889, GNorm = 0.3298
Meta loss on this task batch = 2.9238e-01, Meta loss averaged over last 500 steps = 2.8803e-01, PNorm = 147.5942, GNorm = 0.3849
Meta loss on this task batch = 2.7733e-01, Meta loss averaged over last 500 steps = 2.8799e-01, PNorm = 147.5995, GNorm = 0.2416
Meta loss on this task batch = 2.8815e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 147.6049, GNorm = 0.3319
Meta loss on this task batch = 2.8876e-01, Meta loss averaged over last 500 steps = 2.8791e-01, PNorm = 147.6104, GNorm = 0.3260
Meta loss on this task batch = 2.8398e-01, Meta loss averaged over last 500 steps = 2.8791e-01, PNorm = 147.6151, GNorm = 0.4170
Meta loss on this task batch = 2.6642e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 147.6204, GNorm = 0.2541
Meta loss on this task batch = 3.2381e-01, Meta loss averaged over last 500 steps = 2.8800e-01, PNorm = 147.6255, GNorm = 0.2544
Meta loss on this task batch = 2.4092e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 147.6306, GNorm = 0.2896
Meta loss on this task batch = 2.9063e-01, Meta loss averaged over last 500 steps = 2.8789e-01, PNorm = 147.6358, GNorm = 0.2622
Meta loss on this task batch = 2.5456e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 147.6414, GNorm = 0.2587
Took 144.16913771629333 seconds to complete one epoch of meta training
Took 152.52184200286865 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473287
Epoch 645
Meta loss on this task batch = 3.1289e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 147.6466, GNorm = 0.4431
Meta loss on this task batch = 2.7069e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 147.6517, GNorm = 0.3590
Meta loss on this task batch = 2.5962e-01, Meta loss averaged over last 500 steps = 2.8777e-01, PNorm = 147.6569, GNorm = 0.3092
Meta loss on this task batch = 3.1141e-01, Meta loss averaged over last 500 steps = 2.8797e-01, PNorm = 147.6616, GNorm = 0.2776
Meta loss on this task batch = 2.7589e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 147.6658, GNorm = 0.2857
Meta loss on this task batch = 2.5599e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 147.6700, GNorm = 0.2666
Meta loss on this task batch = 3.1735e-01, Meta loss averaged over last 500 steps = 2.8795e-01, PNorm = 147.6735, GNorm = 0.2850
Meta loss on this task batch = 3.2010e-01, Meta loss averaged over last 500 steps = 2.8801e-01, PNorm = 147.6766, GNorm = 0.2967
Meta loss on this task batch = 2.3983e-01, Meta loss averaged over last 500 steps = 2.8797e-01, PNorm = 147.6801, GNorm = 0.2998
Meta loss on this task batch = 2.4789e-01, Meta loss averaged over last 500 steps = 2.8789e-01, PNorm = 147.6846, GNorm = 0.2800
Meta loss on this task batch = 2.1515e-01, Meta loss averaged over last 500 steps = 2.8774e-01, PNorm = 147.6898, GNorm = 0.2950
Meta loss on this task batch = 2.5720e-01, Meta loss averaged over last 500 steps = 2.8761e-01, PNorm = 147.6944, GNorm = 0.2576
Meta loss on this task batch = 2.8251e-01, Meta loss averaged over last 500 steps = 2.8761e-01, PNorm = 147.6987, GNorm = 0.2714
Meta loss on this task batch = 2.8744e-01, Meta loss averaged over last 500 steps = 2.8758e-01, PNorm = 147.7033, GNorm = 0.3925
Meta loss on this task batch = 3.0461e-01, Meta loss averaged over last 500 steps = 2.8758e-01, PNorm = 147.7080, GNorm = 0.4267
Meta loss on this task batch = 3.5675e-01, Meta loss averaged over last 500 steps = 2.8779e-01, PNorm = 147.7121, GNorm = 0.3310
Meta loss on this task batch = 3.6790e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 147.7158, GNorm = 0.3207
Meta loss on this task batch = 2.8310e-01, Meta loss averaged over last 500 steps = 2.8787e-01, PNorm = 147.7202, GNorm = 0.2522
Meta loss on this task batch = 3.6224e-01, Meta loss averaged over last 500 steps = 2.8810e-01, PNorm = 147.7237, GNorm = 0.3648
Took 112.96775007247925 seconds to complete one epoch of meta training
Took 120.7684543132782 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494091
Epoch 646
Meta loss on this task batch = 2.6132e-01, Meta loss averaged over last 500 steps = 2.8804e-01, PNorm = 147.7275, GNorm = 0.2631
Meta loss on this task batch = 2.9604e-01, Meta loss averaged over last 500 steps = 2.8798e-01, PNorm = 147.7312, GNorm = 0.4121
Meta loss on this task batch = 3.1087e-01, Meta loss averaged over last 500 steps = 2.8812e-01, PNorm = 147.7355, GNorm = 0.3699
Meta loss on this task batch = 3.0391e-01, Meta loss averaged over last 500 steps = 2.8810e-01, PNorm = 147.7396, GNorm = 0.2603
Meta loss on this task batch = 3.1389e-01, Meta loss averaged over last 500 steps = 2.8809e-01, PNorm = 147.7437, GNorm = 0.3551
Meta loss on this task batch = 2.6233e-01, Meta loss averaged over last 500 steps = 2.8811e-01, PNorm = 147.7488, GNorm = 0.3181
Meta loss on this task batch = 3.1797e-01, Meta loss averaged over last 500 steps = 2.8818e-01, PNorm = 147.7538, GNorm = 0.2664
Meta loss on this task batch = 2.7708e-01, Meta loss averaged over last 500 steps = 2.8815e-01, PNorm = 147.7582, GNorm = 0.2686
Meta loss on this task batch = 2.8087e-01, Meta loss averaged over last 500 steps = 2.8820e-01, PNorm = 147.7627, GNorm = 0.3138
Meta loss on this task batch = 2.9446e-01, Meta loss averaged over last 500 steps = 2.8818e-01, PNorm = 147.7670, GNorm = 0.2640
Meta loss on this task batch = 3.0777e-01, Meta loss averaged over last 500 steps = 2.8813e-01, PNorm = 147.7714, GNorm = 0.3091
Meta loss on this task batch = 2.8963e-01, Meta loss averaged over last 500 steps = 2.8812e-01, PNorm = 147.7759, GNorm = 0.4303
Meta loss on this task batch = 2.3727e-01, Meta loss averaged over last 500 steps = 2.8815e-01, PNorm = 147.7805, GNorm = 0.3836
Meta loss on this task batch = 2.5295e-01, Meta loss averaged over last 500 steps = 2.8812e-01, PNorm = 147.7853, GNorm = 0.3156
Meta loss on this task batch = 2.5411e-01, Meta loss averaged over last 500 steps = 2.8799e-01, PNorm = 147.7897, GNorm = 0.2668
Meta loss on this task batch = 2.5197e-01, Meta loss averaged over last 500 steps = 2.8790e-01, PNorm = 147.7946, GNorm = 0.4503
Meta loss on this task batch = 3.5278e-01, Meta loss averaged over last 500 steps = 2.8800e-01, PNorm = 147.7990, GNorm = 0.4762
Meta loss on this task batch = 2.9936e-01, Meta loss averaged over last 500 steps = 2.8812e-01, PNorm = 147.8033, GNorm = 0.3804
Meta loss on this task batch = 2.8963e-01, Meta loss averaged over last 500 steps = 2.8815e-01, PNorm = 147.8069, GNorm = 0.4029
Took 112.74126505851746 seconds to complete one epoch of meta training
Took 120.9955005645752 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461308
Epoch 647
Meta loss on this task batch = 2.9523e-01, Meta loss averaged over last 500 steps = 2.8824e-01, PNorm = 147.8107, GNorm = 0.5565
Meta loss on this task batch = 3.3064e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 147.8124, GNorm = 0.6196
Meta loss on this task batch = 3.0335e-01, Meta loss averaged over last 500 steps = 2.8828e-01, PNorm = 147.8142, GNorm = 0.3212
Meta loss on this task batch = 3.0140e-01, Meta loss averaged over last 500 steps = 2.8829e-01, PNorm = 147.8168, GNorm = 0.3246
Meta loss on this task batch = 2.9076e-01, Meta loss averaged over last 500 steps = 2.8829e-01, PNorm = 147.8197, GNorm = 0.3031
Meta loss on this task batch = 2.6247e-01, Meta loss averaged over last 500 steps = 2.8821e-01, PNorm = 147.8234, GNorm = 0.2940
Meta loss on this task batch = 3.1778e-01, Meta loss averaged over last 500 steps = 2.8831e-01, PNorm = 147.8272, GNorm = 0.2739
Meta loss on this task batch = 2.7092e-01, Meta loss averaged over last 500 steps = 2.8827e-01, PNorm = 147.8315, GNorm = 0.3232
Meta loss on this task batch = 2.4487e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 147.8369, GNorm = 0.2670
Meta loss on this task batch = 2.9525e-01, Meta loss averaged over last 500 steps = 2.8824e-01, PNorm = 147.8426, GNorm = 0.2673
Meta loss on this task batch = 3.0659e-01, Meta loss averaged over last 500 steps = 2.8830e-01, PNorm = 147.8480, GNorm = 0.3941
Meta loss on this task batch = 2.4691e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 147.8543, GNorm = 0.2638
Meta loss on this task batch = 2.3740e-01, Meta loss averaged over last 500 steps = 2.8806e-01, PNorm = 147.8614, GNorm = 0.3152
Meta loss on this task batch = 2.4954e-01, Meta loss averaged over last 500 steps = 2.8803e-01, PNorm = 147.8686, GNorm = 0.2840
Meta loss on this task batch = 2.5261e-01, Meta loss averaged over last 500 steps = 2.8780e-01, PNorm = 147.8761, GNorm = 0.2728
Meta loss on this task batch = 3.2550e-01, Meta loss averaged over last 500 steps = 2.8772e-01, PNorm = 147.8837, GNorm = 0.3293
Meta loss on this task batch = 2.8615e-01, Meta loss averaged over last 500 steps = 2.8773e-01, PNorm = 147.8910, GNorm = 0.2964
Meta loss on this task batch = 2.9766e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 147.8974, GNorm = 0.3285
Meta loss on this task batch = 2.4725e-01, Meta loss averaged over last 500 steps = 2.8771e-01, PNorm = 147.9036, GNorm = 0.3269
Took 113.27784037590027 seconds to complete one epoch of meta training
Took 121.09719395637512 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457674
Epoch 648
Meta loss on this task batch = 3.2546e-01, Meta loss averaged over last 500 steps = 2.8779e-01, PNorm = 147.9094, GNorm = 0.2871
Meta loss on this task batch = 2.8645e-01, Meta loss averaged over last 500 steps = 2.8782e-01, PNorm = 147.9149, GNorm = 0.2898
Meta loss on this task batch = 2.3575e-01, Meta loss averaged over last 500 steps = 2.8775e-01, PNorm = 147.9207, GNorm = 0.3001
Meta loss on this task batch = 2.8815e-01, Meta loss averaged over last 500 steps = 2.8780e-01, PNorm = 147.9253, GNorm = 0.4388
Meta loss on this task batch = 3.8556e-01, Meta loss averaged over last 500 steps = 2.8808e-01, PNorm = 147.9290, GNorm = 0.3813
Meta loss on this task batch = 2.5817e-01, Meta loss averaged over last 500 steps = 2.8819e-01, PNorm = 147.9327, GNorm = 0.2978
Meta loss on this task batch = 2.6667e-01, Meta loss averaged over last 500 steps = 2.8807e-01, PNorm = 147.9360, GNorm = 0.2663
Meta loss on this task batch = 2.3743e-01, Meta loss averaged over last 500 steps = 2.8797e-01, PNorm = 147.9396, GNorm = 0.2501
Meta loss on this task batch = 3.5494e-01, Meta loss averaged over last 500 steps = 2.8813e-01, PNorm = 147.9422, GNorm = 0.3354
Meta loss on this task batch = 2.6505e-01, Meta loss averaged over last 500 steps = 2.8812e-01, PNorm = 147.9443, GNorm = 0.3581
Meta loss on this task batch = 2.4305e-01, Meta loss averaged over last 500 steps = 2.8813e-01, PNorm = 147.9472, GNorm = 0.2298
Meta loss on this task batch = 2.9530e-01, Meta loss averaged over last 500 steps = 2.8810e-01, PNorm = 147.9505, GNorm = 0.3158
Meta loss on this task batch = 3.2059e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 147.9543, GNorm = 0.4781
Meta loss on this task batch = 2.7914e-01, Meta loss averaged over last 500 steps = 2.8823e-01, PNorm = 147.9585, GNorm = 0.2828
Meta loss on this task batch = 2.7421e-01, Meta loss averaged over last 500 steps = 2.8825e-01, PNorm = 147.9633, GNorm = 0.3027
Meta loss on this task batch = 3.0953e-01, Meta loss averaged over last 500 steps = 2.8843e-01, PNorm = 147.9686, GNorm = 0.2968
Meta loss on this task batch = 2.7556e-01, Meta loss averaged over last 500 steps = 2.8836e-01, PNorm = 147.9739, GNorm = 0.2887
Meta loss on this task batch = 2.9312e-01, Meta loss averaged over last 500 steps = 2.8838e-01, PNorm = 147.9801, GNorm = 0.5270
Meta loss on this task batch = 2.0871e-01, Meta loss averaged over last 500 steps = 2.8824e-01, PNorm = 147.9874, GNorm = 0.2561
Took 114.01733183860779 seconds to complete one epoch of meta training
Took 120.75194644927979 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482657
Epoch 649
Meta loss on this task batch = 2.6510e-01, Meta loss averaged over last 500 steps = 2.8814e-01, PNorm = 147.9944, GNorm = 0.2968
Meta loss on this task batch = 2.9651e-01, Meta loss averaged over last 500 steps = 2.8816e-01, PNorm = 148.0012, GNorm = 0.2577
Meta loss on this task batch = 2.5385e-01, Meta loss averaged over last 500 steps = 2.8808e-01, PNorm = 148.0074, GNorm = 0.3159
Meta loss on this task batch = 2.6725e-01, Meta loss averaged over last 500 steps = 2.8816e-01, PNorm = 148.0137, GNorm = 0.3816
Meta loss on this task batch = 2.3643e-01, Meta loss averaged over last 500 steps = 2.8807e-01, PNorm = 148.0192, GNorm = 0.2466
Meta loss on this task batch = 2.8648e-01, Meta loss averaged over last 500 steps = 2.8798e-01, PNorm = 148.0246, GNorm = 0.2867
Meta loss on this task batch = 2.9101e-01, Meta loss averaged over last 500 steps = 2.8803e-01, PNorm = 148.0301, GNorm = 0.2832
Meta loss on this task batch = 2.7693e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 148.0356, GNorm = 0.4642
Meta loss on this task batch = 3.1695e-01, Meta loss averaged over last 500 steps = 2.8798e-01, PNorm = 148.0406, GNorm = 0.3143
Meta loss on this task batch = 2.7947e-01, Meta loss averaged over last 500 steps = 2.8799e-01, PNorm = 148.0456, GNorm = 0.2626
Meta loss on this task batch = 2.9933e-01, Meta loss averaged over last 500 steps = 2.8810e-01, PNorm = 148.0513, GNorm = 0.2546
Meta loss on this task batch = 3.2296e-01, Meta loss averaged over last 500 steps = 2.8823e-01, PNorm = 148.0557, GNorm = 0.4089
Meta loss on this task batch = 3.0665e-01, Meta loss averaged over last 500 steps = 2.8811e-01, PNorm = 148.0607, GNorm = 0.5630
Meta loss on this task batch = 3.0918e-01, Meta loss averaged over last 500 steps = 2.8806e-01, PNorm = 148.0653, GNorm = 0.2877
Meta loss on this task batch = 2.7673e-01, Meta loss averaged over last 500 steps = 2.8805e-01, PNorm = 148.0698, GNorm = 0.3970
Meta loss on this task batch = 2.8180e-01, Meta loss averaged over last 500 steps = 2.8796e-01, PNorm = 148.0743, GNorm = 0.2642
Meta loss on this task batch = 2.8827e-01, Meta loss averaged over last 500 steps = 2.8800e-01, PNorm = 148.0795, GNorm = 0.3021
Meta loss on this task batch = 2.6635e-01, Meta loss averaged over last 500 steps = 2.8802e-01, PNorm = 148.0846, GNorm = 0.2579
Meta loss on this task batch = 2.8817e-01, Meta loss averaged over last 500 steps = 2.8799e-01, PNorm = 148.0899, GNorm = 0.3121
Took 112.39606022834778 seconds to complete one epoch of meta training
Took 120.30541467666626 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462669
Epoch 650
Meta loss on this task batch = 3.0027e-01, Meta loss averaged over last 500 steps = 2.8801e-01, PNorm = 148.0955, GNorm = 0.3069
Meta loss on this task batch = 2.9378e-01, Meta loss averaged over last 500 steps = 2.8802e-01, PNorm = 148.1017, GNorm = 0.2758
Meta loss on this task batch = 2.6491e-01, Meta loss averaged over last 500 steps = 2.8798e-01, PNorm = 148.1074, GNorm = 0.2571
Meta loss on this task batch = 3.2104e-01, Meta loss averaged over last 500 steps = 2.8809e-01, PNorm = 148.1123, GNorm = 0.3116
Meta loss on this task batch = 3.2768e-01, Meta loss averaged over last 500 steps = 2.8809e-01, PNorm = 148.1166, GNorm = 0.2602
Meta loss on this task batch = 3.7181e-01, Meta loss averaged over last 500 steps = 2.8830e-01, PNorm = 148.1210, GNorm = 0.3109
Meta loss on this task batch = 2.4534e-01, Meta loss averaged over last 500 steps = 2.8825e-01, PNorm = 148.1256, GNorm = 0.2359
Meta loss on this task batch = 3.0566e-01, Meta loss averaged over last 500 steps = 2.8822e-01, PNorm = 148.1300, GNorm = 0.3114
Meta loss on this task batch = 2.7800e-01, Meta loss averaged over last 500 steps = 2.8813e-01, PNorm = 148.1358, GNorm = 0.5626
Meta loss on this task batch = 2.8751e-01, Meta loss averaged over last 500 steps = 2.8798e-01, PNorm = 148.1417, GNorm = 0.2667
Meta loss on this task batch = 2.9102e-01, Meta loss averaged over last 500 steps = 2.8800e-01, PNorm = 148.1472, GNorm = 0.3166
Meta loss on this task batch = 2.6332e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 148.1533, GNorm = 0.2737
Meta loss on this task batch = 2.2320e-01, Meta loss averaged over last 500 steps = 2.8772e-01, PNorm = 148.1598, GNorm = 0.2514
Meta loss on this task batch = 2.7162e-01, Meta loss averaged over last 500 steps = 2.8779e-01, PNorm = 148.1663, GNorm = 0.2599
Meta loss on this task batch = 2.8904e-01, Meta loss averaged over last 500 steps = 2.8778e-01, PNorm = 148.1730, GNorm = 0.3920
Meta loss on this task batch = 3.0059e-01, Meta loss averaged over last 500 steps = 2.8778e-01, PNorm = 148.1794, GNorm = 0.3730
Meta loss on this task batch = 2.1768e-01, Meta loss averaged over last 500 steps = 2.8762e-01, PNorm = 148.1861, GNorm = 0.2580
Meta loss on this task batch = 2.3945e-01, Meta loss averaged over last 500 steps = 2.8751e-01, PNorm = 148.1922, GNorm = 0.2530
Meta loss on this task batch = 2.7856e-01, Meta loss averaged over last 500 steps = 2.8745e-01, PNorm = 148.1964, GNorm = 0.5912
Took 113.31770706176758 seconds to complete one epoch of meta training
Took 120.89692711830139 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471122
Epoch 651
Meta loss on this task batch = 3.2979e-01, Meta loss averaged over last 500 steps = 2.8766e-01, PNorm = 148.1993, GNorm = 0.3096
Meta loss on this task batch = 3.4416e-01, Meta loss averaged over last 500 steps = 2.8773e-01, PNorm = 148.2017, GNorm = 0.4430
Meta loss on this task batch = 2.8254e-01, Meta loss averaged over last 500 steps = 2.8779e-01, PNorm = 148.2048, GNorm = 0.4089
Meta loss on this task batch = 2.5810e-01, Meta loss averaged over last 500 steps = 2.8773e-01, PNorm = 148.2077, GNorm = 0.2921
Meta loss on this task batch = 3.4674e-01, Meta loss averaged over last 500 steps = 2.8789e-01, PNorm = 148.2094, GNorm = 0.3163
Meta loss on this task batch = 2.8495e-01, Meta loss averaged over last 500 steps = 2.8781e-01, PNorm = 148.2111, GNorm = 0.3299
Meta loss on this task batch = 2.6683e-01, Meta loss averaged over last 500 steps = 2.8776e-01, PNorm = 148.2128, GNorm = 0.4268
Meta loss on this task batch = 3.0629e-01, Meta loss averaged over last 500 steps = 2.8774e-01, PNorm = 148.2150, GNorm = 0.3738
Meta loss on this task batch = 2.8331e-01, Meta loss averaged over last 500 steps = 2.8778e-01, PNorm = 148.2176, GNorm = 0.3166
Meta loss on this task batch = 3.0683e-01, Meta loss averaged over last 500 steps = 2.8782e-01, PNorm = 148.2204, GNorm = 0.3626
Meta loss on this task batch = 2.6506e-01, Meta loss averaged over last 500 steps = 2.8780e-01, PNorm = 148.2243, GNorm = 0.3796
Meta loss on this task batch = 2.9403e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 148.2281, GNorm = 0.2379
Meta loss on this task batch = 2.9187e-01, Meta loss averaged over last 500 steps = 2.8793e-01, PNorm = 148.2320, GNorm = 0.2584
Meta loss on this task batch = 2.6870e-01, Meta loss averaged over last 500 steps = 2.8800e-01, PNorm = 148.2364, GNorm = 0.2666
Meta loss on this task batch = 3.1250e-01, Meta loss averaged over last 500 steps = 2.8789e-01, PNorm = 148.2410, GNorm = 0.2672
Meta loss on this task batch = 2.4868e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 148.2460, GNorm = 0.2168
Meta loss on this task batch = 2.9978e-01, Meta loss averaged over last 500 steps = 2.8800e-01, PNorm = 148.2516, GNorm = 0.2980
Meta loss on this task batch = 2.9621e-01, Meta loss averaged over last 500 steps = 2.8797e-01, PNorm = 148.2571, GNorm = 0.2527
Meta loss on this task batch = 2.5605e-01, Meta loss averaged over last 500 steps = 2.8784e-01, PNorm = 148.2632, GNorm = 0.3013
Took 111.93278932571411 seconds to complete one epoch of meta training
Took 119.96676182746887 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444327
Epoch 652
Meta loss on this task batch = 3.0484e-01, Meta loss averaged over last 500 steps = 2.8784e-01, PNorm = 148.2696, GNorm = 0.2862
Meta loss on this task batch = 3.4165e-01, Meta loss averaged over last 500 steps = 2.8786e-01, PNorm = 148.2759, GNorm = 0.3325
Meta loss on this task batch = 3.1509e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 148.2821, GNorm = 0.2651
Meta loss on this task batch = 2.6636e-01, Meta loss averaged over last 500 steps = 2.8781e-01, PNorm = 148.2882, GNorm = 0.2410
Meta loss on this task batch = 2.7051e-01, Meta loss averaged over last 500 steps = 2.8780e-01, PNorm = 148.2948, GNorm = 0.2564
Meta loss on this task batch = 2.6461e-01, Meta loss averaged over last 500 steps = 2.8782e-01, PNorm = 148.3013, GNorm = 0.2798
Meta loss on this task batch = 3.1501e-01, Meta loss averaged over last 500 steps = 2.8788e-01, PNorm = 148.3080, GNorm = 0.2766
Meta loss on this task batch = 2.3679e-01, Meta loss averaged over last 500 steps = 2.8778e-01, PNorm = 148.3154, GNorm = 0.3200
Meta loss on this task batch = 3.2536e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 148.3224, GNorm = 0.3051
Meta loss on this task batch = 3.4688e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 148.3288, GNorm = 0.4375
Meta loss on this task batch = 2.7217e-01, Meta loss averaged over last 500 steps = 2.8807e-01, PNorm = 148.3355, GNorm = 0.2648
Meta loss on this task batch = 2.7618e-01, Meta loss averaged over last 500 steps = 2.8816e-01, PNorm = 148.3416, GNorm = 0.3011
Meta loss on this task batch = 3.1084e-01, Meta loss averaged over last 500 steps = 2.8808e-01, PNorm = 148.3464, GNorm = 0.2753
Meta loss on this task batch = 2.9018e-01, Meta loss averaged over last 500 steps = 2.8805e-01, PNorm = 148.3515, GNorm = 0.2583
Meta loss on this task batch = 3.0558e-01, Meta loss averaged over last 500 steps = 2.8812e-01, PNorm = 148.3571, GNorm = 0.3313
Meta loss on this task batch = 3.1554e-01, Meta loss averaged over last 500 steps = 2.8823e-01, PNorm = 148.3627, GNorm = 0.3196
Meta loss on this task batch = 2.2875e-01, Meta loss averaged over last 500 steps = 2.8811e-01, PNorm = 148.3681, GNorm = 0.2570
Meta loss on this task batch = 2.7226e-01, Meta loss averaged over last 500 steps = 2.8809e-01, PNorm = 148.3728, GNorm = 0.2540
Meta loss on this task batch = 2.2039e-01, Meta loss averaged over last 500 steps = 2.8795e-01, PNorm = 148.3777, GNorm = 0.3051
Took 112.71069574356079 seconds to complete one epoch of meta training
Took 120.76164102554321 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464352
Epoch 653
Meta loss on this task batch = 2.3909e-01, Meta loss averaged over last 500 steps = 2.8787e-01, PNorm = 148.3824, GNorm = 0.2782
Meta loss on this task batch = 3.4264e-01, Meta loss averaged over last 500 steps = 2.8788e-01, PNorm = 148.3863, GNorm = 0.6045
Meta loss on this task batch = 2.7006e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 148.3901, GNorm = 0.2499
Meta loss on this task batch = 2.9757e-01, Meta loss averaged over last 500 steps = 2.8779e-01, PNorm = 148.3936, GNorm = 0.3070
Meta loss on this task batch = 2.5435e-01, Meta loss averaged over last 500 steps = 2.8773e-01, PNorm = 148.3979, GNorm = 0.2774
Meta loss on this task batch = 2.8573e-01, Meta loss averaged over last 500 steps = 2.8772e-01, PNorm = 148.4018, GNorm = 0.2558
Meta loss on this task batch = 3.3571e-01, Meta loss averaged over last 500 steps = 2.8786e-01, PNorm = 148.4051, GNorm = 0.4018
Meta loss on this task batch = 3.2244e-01, Meta loss averaged over last 500 steps = 2.8797e-01, PNorm = 148.4083, GNorm = 0.3163
Meta loss on this task batch = 2.8200e-01, Meta loss averaged over last 500 steps = 2.8791e-01, PNorm = 148.4110, GNorm = 0.3065
Meta loss on this task batch = 2.1285e-01, Meta loss averaged over last 500 steps = 2.8788e-01, PNorm = 148.4145, GNorm = 0.2690
Meta loss on this task batch = 2.6002e-01, Meta loss averaged over last 500 steps = 2.8786e-01, PNorm = 148.4186, GNorm = 0.2650
Meta loss on this task batch = 2.8238e-01, Meta loss averaged over last 500 steps = 2.8771e-01, PNorm = 148.4218, GNorm = 0.3651
Meta loss on this task batch = 3.3611e-01, Meta loss averaged over last 500 steps = 2.8776e-01, PNorm = 148.4243, GNorm = 0.6251
Meta loss on this task batch = 2.4571e-01, Meta loss averaged over last 500 steps = 2.8769e-01, PNorm = 148.4275, GNorm = 0.2259
Meta loss on this task batch = 3.4007e-01, Meta loss averaged over last 500 steps = 2.8772e-01, PNorm = 148.4311, GNorm = 0.2745
Meta loss on this task batch = 3.1937e-01, Meta loss averaged over last 500 steps = 2.8785e-01, PNorm = 148.4351, GNorm = 0.2847
Meta loss on this task batch = 2.6526e-01, Meta loss averaged over last 500 steps = 2.8776e-01, PNorm = 148.4397, GNorm = 0.2603
Meta loss on this task batch = 3.0239e-01, Meta loss averaged over last 500 steps = 2.8782e-01, PNorm = 148.4434, GNorm = 0.4083
Meta loss on this task batch = 2.6542e-01, Meta loss averaged over last 500 steps = 2.8773e-01, PNorm = 148.4477, GNorm = 0.3508
Took 111.70194745063782 seconds to complete one epoch of meta training
Took 119.47773933410645 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444789
Epoch 654
Meta loss on this task batch = 2.8647e-01, Meta loss averaged over last 500 steps = 2.8781e-01, PNorm = 148.4530, GNorm = 0.2861
Meta loss on this task batch = 2.7713e-01, Meta loss averaged over last 500 steps = 2.8784e-01, PNorm = 148.4583, GNorm = 0.2384
Meta loss on this task batch = 2.5377e-01, Meta loss averaged over last 500 steps = 2.8772e-01, PNorm = 148.4636, GNorm = 0.3029
Meta loss on this task batch = 2.8222e-01, Meta loss averaged over last 500 steps = 2.8761e-01, PNorm = 148.4692, GNorm = 0.3279
Meta loss on this task batch = 2.7953e-01, Meta loss averaged over last 500 steps = 2.8755e-01, PNorm = 148.4745, GNorm = 0.2857
Meta loss on this task batch = 3.3321e-01, Meta loss averaged over last 500 steps = 2.8765e-01, PNorm = 148.4802, GNorm = 0.3801
Meta loss on this task batch = 3.2291e-01, Meta loss averaged over last 500 steps = 2.8773e-01, PNorm = 148.4854, GNorm = 0.2725
Meta loss on this task batch = 2.8787e-01, Meta loss averaged over last 500 steps = 2.8785e-01, PNorm = 148.4910, GNorm = 0.3583
Meta loss on this task batch = 3.4785e-01, Meta loss averaged over last 500 steps = 2.8803e-01, PNorm = 148.4952, GNorm = 0.3247
Meta loss on this task batch = 2.6634e-01, Meta loss averaged over last 500 steps = 2.8801e-01, PNorm = 148.4993, GNorm = 0.4485
Meta loss on this task batch = 2.8940e-01, Meta loss averaged over last 500 steps = 2.8804e-01, PNorm = 148.5033, GNorm = 0.3522
Meta loss on this task batch = 3.1430e-01, Meta loss averaged over last 500 steps = 2.8802e-01, PNorm = 148.5068, GNorm = 0.2743
Meta loss on this task batch = 2.6721e-01, Meta loss averaged over last 500 steps = 2.8798e-01, PNorm = 148.5110, GNorm = 0.2982
Meta loss on this task batch = 2.8473e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 148.5151, GNorm = 0.2323
Meta loss on this task batch = 2.7795e-01, Meta loss averaged over last 500 steps = 2.8775e-01, PNorm = 148.5192, GNorm = 0.2769
Meta loss on this task batch = 2.9772e-01, Meta loss averaged over last 500 steps = 2.8765e-01, PNorm = 148.5233, GNorm = 0.3173
Meta loss on this task batch = 2.8946e-01, Meta loss averaged over last 500 steps = 2.8759e-01, PNorm = 148.5278, GNorm = 0.3067
Meta loss on this task batch = 3.3208e-01, Meta loss averaged over last 500 steps = 2.8769e-01, PNorm = 148.5323, GNorm = 0.3763
Meta loss on this task batch = 2.7069e-01, Meta loss averaged over last 500 steps = 2.8761e-01, PNorm = 148.5374, GNorm = 0.3437
Took 110.58502411842346 seconds to complete one epoch of meta training
Took 118.74787759780884 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464411
Epoch 655
Meta loss on this task batch = 2.8546e-01, Meta loss averaged over last 500 steps = 2.8772e-01, PNorm = 148.5419, GNorm = 0.2773
Meta loss on this task batch = 3.3477e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 148.5457, GNorm = 0.2763
Meta loss on this task batch = 2.7967e-01, Meta loss averaged over last 500 steps = 2.8781e-01, PNorm = 148.5505, GNorm = 0.3607
Meta loss on this task batch = 2.6514e-01, Meta loss averaged over last 500 steps = 2.8787e-01, PNorm = 148.5554, GNorm = 0.2989
Meta loss on this task batch = 3.4804e-01, Meta loss averaged over last 500 steps = 2.8802e-01, PNorm = 148.5601, GNorm = 0.3670
Meta loss on this task batch = 2.8513e-01, Meta loss averaged over last 500 steps = 2.8791e-01, PNorm = 148.5649, GNorm = 0.2750
Meta loss on this task batch = 2.9903e-01, Meta loss averaged over last 500 steps = 2.8800e-01, PNorm = 148.5698, GNorm = 0.2316
Meta loss on this task batch = 2.7621e-01, Meta loss averaged over last 500 steps = 2.8791e-01, PNorm = 148.5751, GNorm = 0.2633
Meta loss on this task batch = 2.8320e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 148.5807, GNorm = 0.2848
Meta loss on this task batch = 3.0826e-01, Meta loss averaged over last 500 steps = 2.8807e-01, PNorm = 148.5860, GNorm = 0.5160
Meta loss on this task batch = 2.5108e-01, Meta loss averaged over last 500 steps = 2.8797e-01, PNorm = 148.5909, GNorm = 0.2615
Meta loss on this task batch = 2.2677e-01, Meta loss averaged over last 500 steps = 2.8773e-01, PNorm = 148.5963, GNorm = 0.2856
Meta loss on this task batch = 2.7814e-01, Meta loss averaged over last 500 steps = 2.8769e-01, PNorm = 148.6013, GNorm = 0.3443
Meta loss on this task batch = 2.6647e-01, Meta loss averaged over last 500 steps = 2.8770e-01, PNorm = 148.6061, GNorm = 0.3180
Meta loss on this task batch = 2.8296e-01, Meta loss averaged over last 500 steps = 2.8775e-01, PNorm = 148.6108, GNorm = 0.2366
Meta loss on this task batch = 2.3079e-01, Meta loss averaged over last 500 steps = 2.8755e-01, PNorm = 148.6158, GNorm = 0.2900
Meta loss on this task batch = 2.9928e-01, Meta loss averaged over last 500 steps = 2.8756e-01, PNorm = 148.6209, GNorm = 0.2560
Meta loss on this task batch = 2.5047e-01, Meta loss averaged over last 500 steps = 2.8743e-01, PNorm = 148.6257, GNorm = 0.2275
Meta loss on this task batch = 2.5697e-01, Meta loss averaged over last 500 steps = 2.8739e-01, PNorm = 148.6306, GNorm = 0.2888
Took 112.96810412406921 seconds to complete one epoch of meta training
Took 120.55018901824951 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450598
Epoch 656
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 2.8742e-01, PNorm = 148.6352, GNorm = 0.2352
Meta loss on this task batch = 2.3323e-01, Meta loss averaged over last 500 steps = 2.8720e-01, PNorm = 148.6403, GNorm = 0.2552
Meta loss on this task batch = 3.1331e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 148.6450, GNorm = 0.2920
Meta loss on this task batch = 2.6583e-01, Meta loss averaged over last 500 steps = 2.8725e-01, PNorm = 148.6490, GNorm = 0.4947
Meta loss on this task batch = 2.7749e-01, Meta loss averaged over last 500 steps = 2.8723e-01, PNorm = 148.6535, GNorm = 0.2830
Meta loss on this task batch = 2.9409e-01, Meta loss averaged over last 500 steps = 2.8731e-01, PNorm = 148.6572, GNorm = 0.3184
Meta loss on this task batch = 2.3757e-01, Meta loss averaged over last 500 steps = 2.8727e-01, PNorm = 148.6613, GNorm = 0.4340
Meta loss on this task batch = 2.6308e-01, Meta loss averaged over last 500 steps = 2.8723e-01, PNorm = 148.6655, GNorm = 0.4834
Meta loss on this task batch = 3.0025e-01, Meta loss averaged over last 500 steps = 2.8726e-01, PNorm = 148.6693, GNorm = 0.3386
Meta loss on this task batch = 2.9045e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 148.6730, GNorm = 0.2770
Meta loss on this task batch = 2.9708e-01, Meta loss averaged over last 500 steps = 2.8721e-01, PNorm = 148.6767, GNorm = 0.2623
Meta loss on this task batch = 2.8870e-01, Meta loss averaged over last 500 steps = 2.8721e-01, PNorm = 148.6799, GNorm = 0.2858
Meta loss on this task batch = 2.3010e-01, Meta loss averaged over last 500 steps = 2.8715e-01, PNorm = 148.6838, GNorm = 0.3090
Meta loss on this task batch = 2.5660e-01, Meta loss averaged over last 500 steps = 2.8711e-01, PNorm = 148.6884, GNorm = 0.2697
Meta loss on this task batch = 3.3685e-01, Meta loss averaged over last 500 steps = 2.8712e-01, PNorm = 148.6935, GNorm = 0.3419
Meta loss on this task batch = 2.8199e-01, Meta loss averaged over last 500 steps = 2.8712e-01, PNorm = 148.6990, GNorm = 0.2997
Meta loss on this task batch = 3.0536e-01, Meta loss averaged over last 500 steps = 2.8712e-01, PNorm = 148.7047, GNorm = 0.4442
Meta loss on this task batch = 2.9147e-01, Meta loss averaged over last 500 steps = 2.8705e-01, PNorm = 148.7108, GNorm = 0.3437
Meta loss on this task batch = 3.1499e-01, Meta loss averaged over last 500 steps = 2.8724e-01, PNorm = 148.7164, GNorm = 0.2915
Took 139.35059475898743 seconds to complete one epoch of meta training
Took 147.3620524406433 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464966
Epoch 657
Meta loss on this task batch = 2.6364e-01, Meta loss averaged over last 500 steps = 2.8718e-01, PNorm = 148.7223, GNorm = 0.2671
Meta loss on this task batch = 3.0726e-01, Meta loss averaged over last 500 steps = 2.8730e-01, PNorm = 148.7280, GNorm = 0.2932
Meta loss on this task batch = 2.7709e-01, Meta loss averaged over last 500 steps = 2.8731e-01, PNorm = 148.7330, GNorm = 0.4603
Meta loss on this task batch = 2.6994e-01, Meta loss averaged over last 500 steps = 2.8713e-01, PNorm = 148.7380, GNorm = 0.2460
Meta loss on this task batch = 2.7910e-01, Meta loss averaged over last 500 steps = 2.8717e-01, PNorm = 148.7430, GNorm = 0.2533
Meta loss on this task batch = 2.8182e-01, Meta loss averaged over last 500 steps = 2.8713e-01, PNorm = 148.7482, GNorm = 0.4150
Meta loss on this task batch = 2.4561e-01, Meta loss averaged over last 500 steps = 2.8711e-01, PNorm = 148.7531, GNorm = 0.3707
Meta loss on this task batch = 3.3134e-01, Meta loss averaged over last 500 steps = 2.8715e-01, PNorm = 148.7570, GNorm = 0.2978
Meta loss on this task batch = 2.7780e-01, Meta loss averaged over last 500 steps = 2.8717e-01, PNorm = 148.7612, GNorm = 0.2724
Meta loss on this task batch = 2.4047e-01, Meta loss averaged over last 500 steps = 2.8713e-01, PNorm = 148.7659, GNorm = 0.2626
Meta loss on this task batch = 2.7748e-01, Meta loss averaged over last 500 steps = 2.8703e-01, PNorm = 148.7705, GNorm = 0.2512
Meta loss on this task batch = 3.0965e-01, Meta loss averaged over last 500 steps = 2.8709e-01, PNorm = 148.7749, GNorm = 0.2852
Meta loss on this task batch = 2.7978e-01, Meta loss averaged over last 500 steps = 2.8712e-01, PNorm = 148.7792, GNorm = 0.3773
Meta loss on this task batch = 2.6409e-01, Meta loss averaged over last 500 steps = 2.8708e-01, PNorm = 148.7832, GNorm = 0.2650
Meta loss on this task batch = 2.8799e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 148.7869, GNorm = 0.3283
Meta loss on this task batch = 3.5263e-01, Meta loss averaged over last 500 steps = 2.8715e-01, PNorm = 148.7902, GNorm = 0.3444
Meta loss on this task batch = 3.2882e-01, Meta loss averaged over last 500 steps = 2.8717e-01, PNorm = 148.7929, GNorm = 0.4375
Meta loss on this task batch = 2.8095e-01, Meta loss averaged over last 500 steps = 2.8709e-01, PNorm = 148.7954, GNorm = 0.2756
Meta loss on this task batch = 2.8051e-01, Meta loss averaged over last 500 steps = 2.8716e-01, PNorm = 148.7987, GNorm = 0.2820
Took 110.8587965965271 seconds to complete one epoch of meta training
Took 118.79862952232361 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461530
Epoch 658
Meta loss on this task batch = 2.6153e-01, Meta loss averaged over last 500 steps = 2.8713e-01, PNorm = 148.8026, GNorm = 0.2408
Meta loss on this task batch = 2.5759e-01, Meta loss averaged over last 500 steps = 2.8711e-01, PNorm = 148.8073, GNorm = 0.2736
Meta loss on this task batch = 2.7713e-01, Meta loss averaged over last 500 steps = 2.8714e-01, PNorm = 148.8118, GNorm = 0.3345
Meta loss on this task batch = 2.3784e-01, Meta loss averaged over last 500 steps = 2.8702e-01, PNorm = 148.8165, GNorm = 0.3413
Meta loss on this task batch = 3.2458e-01, Meta loss averaged over last 500 steps = 2.8707e-01, PNorm = 148.8210, GNorm = 0.3274
Meta loss on this task batch = 2.8317e-01, Meta loss averaged over last 500 steps = 2.8705e-01, PNorm = 148.8257, GNorm = 0.2889
Meta loss on this task batch = 3.1058e-01, Meta loss averaged over last 500 steps = 2.8727e-01, PNorm = 148.8307, GNorm = 0.3268
Meta loss on this task batch = 2.3975e-01, Meta loss averaged over last 500 steps = 2.8718e-01, PNorm = 148.8367, GNorm = 0.2772
Meta loss on this task batch = 2.9584e-01, Meta loss averaged over last 500 steps = 2.8715e-01, PNorm = 148.8421, GNorm = 0.3454
Meta loss on this task batch = 3.1285e-01, Meta loss averaged over last 500 steps = 2.8712e-01, PNorm = 148.8474, GNorm = 0.2792
Meta loss on this task batch = 2.5232e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 148.8527, GNorm = 0.2707
Meta loss on this task batch = 2.9045e-01, Meta loss averaged over last 500 steps = 2.8706e-01, PNorm = 148.8579, GNorm = 0.2769
Meta loss on this task batch = 3.2151e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 148.8620, GNorm = 0.2933
Meta loss on this task batch = 2.8968e-01, Meta loss averaged over last 500 steps = 2.8717e-01, PNorm = 148.8666, GNorm = 0.2344
Meta loss on this task batch = 2.7086e-01, Meta loss averaged over last 500 steps = 2.8718e-01, PNorm = 148.8715, GNorm = 0.3155
Meta loss on this task batch = 2.9982e-01, Meta loss averaged over last 500 steps = 2.8723e-01, PNorm = 148.8765, GNorm = 0.2846
Meta loss on this task batch = 3.0166e-01, Meta loss averaged over last 500 steps = 2.8726e-01, PNorm = 148.8815, GNorm = 0.3797
Meta loss on this task batch = 2.9491e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 148.8865, GNorm = 0.3181
Meta loss on this task batch = 3.7703e-01, Meta loss averaged over last 500 steps = 2.8746e-01, PNorm = 148.8911, GNorm = 0.4520
Took 110.3616235256195 seconds to complete one epoch of meta training
Took 117.99250912666321 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457893
Epoch 659
Meta loss on this task batch = 2.9947e-01, Meta loss averaged over last 500 steps = 2.8754e-01, PNorm = 148.8950, GNorm = 0.3526
Meta loss on this task batch = 2.6878e-01, Meta loss averaged over last 500 steps = 2.8747e-01, PNorm = 148.8984, GNorm = 0.2910
Meta loss on this task batch = 2.6240e-01, Meta loss averaged over last 500 steps = 2.8747e-01, PNorm = 148.9021, GNorm = 0.2376
Meta loss on this task batch = 2.3752e-01, Meta loss averaged over last 500 steps = 2.8733e-01, PNorm = 148.9063, GNorm = 0.2309
Meta loss on this task batch = 2.9871e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 148.9111, GNorm = 0.3113
Meta loss on this task batch = 2.9947e-01, Meta loss averaged over last 500 steps = 2.8723e-01, PNorm = 148.9158, GNorm = 0.2713
Meta loss on this task batch = 2.9279e-01, Meta loss averaged over last 500 steps = 2.8727e-01, PNorm = 148.9203, GNorm = 0.2947
Meta loss on this task batch = 2.8014e-01, Meta loss averaged over last 500 steps = 2.8734e-01, PNorm = 148.9249, GNorm = 0.3365
Meta loss on this task batch = 3.2464e-01, Meta loss averaged over last 500 steps = 2.8741e-01, PNorm = 148.9283, GNorm = 0.4016
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 2.8743e-01, PNorm = 148.9319, GNorm = 0.3052
Meta loss on this task batch = 3.2678e-01, Meta loss averaged over last 500 steps = 2.8734e-01, PNorm = 148.9346, GNorm = 0.5505
Meta loss on this task batch = 3.1028e-01, Meta loss averaged over last 500 steps = 2.8740e-01, PNorm = 148.9375, GNorm = 0.3105
Meta loss on this task batch = 2.9820e-01, Meta loss averaged over last 500 steps = 2.8734e-01, PNorm = 148.9405, GNorm = 0.2745
Meta loss on this task batch = 2.7532e-01, Meta loss averaged over last 500 steps = 2.8738e-01, PNorm = 148.9439, GNorm = 0.3593
Meta loss on this task batch = 2.6439e-01, Meta loss averaged over last 500 steps = 2.8730e-01, PNorm = 148.9477, GNorm = 0.3368
Meta loss on this task batch = 3.1761e-01, Meta loss averaged over last 500 steps = 2.8731e-01, PNorm = 148.9516, GNorm = 0.2517
Meta loss on this task batch = 2.9883e-01, Meta loss averaged over last 500 steps = 2.8738e-01, PNorm = 148.9549, GNorm = 0.2925
Meta loss on this task batch = 1.9889e-01, Meta loss averaged over last 500 steps = 2.8726e-01, PNorm = 148.9590, GNorm = 0.3192
Meta loss on this task batch = 2.4405e-01, Meta loss averaged over last 500 steps = 2.8703e-01, PNorm = 148.9636, GNorm = 0.4854
Took 108.55140519142151 seconds to complete one epoch of meta training
Took 115.41329503059387 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474622
Epoch 660
Meta loss on this task batch = 2.8806e-01, Meta loss averaged over last 500 steps = 2.8706e-01, PNorm = 148.9681, GNorm = 0.2868
Meta loss on this task batch = 3.3509e-01, Meta loss averaged over last 500 steps = 2.8711e-01, PNorm = 148.9722, GNorm = 0.3209
Meta loss on this task batch = 2.2579e-01, Meta loss averaged over last 500 steps = 2.8694e-01, PNorm = 148.9764, GNorm = 0.3612
Meta loss on this task batch = 2.3324e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 148.9809, GNorm = 0.2693
Meta loss on this task batch = 3.4162e-01, Meta loss averaged over last 500 steps = 2.8689e-01, PNorm = 148.9842, GNorm = 0.2790
Meta loss on this task batch = 3.4506e-01, Meta loss averaged over last 500 steps = 2.8712e-01, PNorm = 148.9873, GNorm = 0.3194
Meta loss on this task batch = 2.7463e-01, Meta loss averaged over last 500 steps = 2.8702e-01, PNorm = 148.9911, GNorm = 0.3265
Meta loss on this task batch = 2.9096e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 148.9947, GNorm = 0.2995
Meta loss on this task batch = 2.6276e-01, Meta loss averaged over last 500 steps = 2.8700e-01, PNorm = 148.9989, GNorm = 0.2285
Meta loss on this task batch = 2.7180e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 149.0035, GNorm = 0.2916
Meta loss on this task batch = 3.1348e-01, Meta loss averaged over last 500 steps = 2.8707e-01, PNorm = 149.0078, GNorm = 0.3627
Meta loss on this task batch = 2.9105e-01, Meta loss averaged over last 500 steps = 2.8714e-01, PNorm = 149.0121, GNorm = 0.4940
Meta loss on this task batch = 2.9174e-01, Meta loss averaged over last 500 steps = 2.8724e-01, PNorm = 149.0155, GNorm = 0.3180
Meta loss on this task batch = 2.4494e-01, Meta loss averaged over last 500 steps = 2.8714e-01, PNorm = 149.0194, GNorm = 0.2395
Meta loss on this task batch = 2.4819e-01, Meta loss averaged over last 500 steps = 2.8714e-01, PNorm = 149.0239, GNorm = 0.3579
Meta loss on this task batch = 2.7265e-01, Meta loss averaged over last 500 steps = 2.8720e-01, PNorm = 149.0282, GNorm = 0.2960
Meta loss on this task batch = 2.7988e-01, Meta loss averaged over last 500 steps = 2.8710e-01, PNorm = 149.0324, GNorm = 0.3066
Meta loss on this task batch = 3.3500e-01, Meta loss averaged over last 500 steps = 2.8719e-01, PNorm = 149.0362, GNorm = 0.3202
Meta loss on this task batch = 2.7193e-01, Meta loss averaged over last 500 steps = 2.8702e-01, PNorm = 149.0403, GNorm = 0.3213
Took 112.92921566963196 seconds to complete one epoch of meta training
Took 120.53975868225098 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474601
Epoch 661
Meta loss on this task batch = 3.2555e-01, Meta loss averaged over last 500 steps = 2.8717e-01, PNorm = 149.0440, GNorm = 0.3331
Meta loss on this task batch = 2.8719e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 149.0481, GNorm = 0.2711
Meta loss on this task batch = 2.4102e-01, Meta loss averaged over last 500 steps = 2.8694e-01, PNorm = 149.0527, GNorm = 0.3099
Meta loss on this task batch = 2.7309e-01, Meta loss averaged over last 500 steps = 2.8705e-01, PNorm = 149.0574, GNorm = 0.2599
Meta loss on this task batch = 3.2583e-01, Meta loss averaged over last 500 steps = 2.8709e-01, PNorm = 149.0619, GNorm = 0.2674
Meta loss on this task batch = 2.8026e-01, Meta loss averaged over last 500 steps = 2.8702e-01, PNorm = 149.0661, GNorm = 0.3358
Meta loss on this task batch = 2.2331e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 149.0704, GNorm = 0.2257
Meta loss on this task batch = 2.5333e-01, Meta loss averaged over last 500 steps = 2.8681e-01, PNorm = 149.0751, GNorm = 0.3508
Meta loss on this task batch = 2.5636e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 149.0805, GNorm = 0.4580
Meta loss on this task batch = 2.9415e-01, Meta loss averaged over last 500 steps = 2.8695e-01, PNorm = 149.0859, GNorm = 0.4988
Meta loss on this task batch = 2.8028e-01, Meta loss averaged over last 500 steps = 2.8685e-01, PNorm = 149.0917, GNorm = 0.2479
Meta loss on this task batch = 2.7289e-01, Meta loss averaged over last 500 steps = 2.8685e-01, PNorm = 149.0978, GNorm = 0.2788
Meta loss on this task batch = 2.6252e-01, Meta loss averaged over last 500 steps = 2.8678e-01, PNorm = 149.1041, GNorm = 0.3048
Meta loss on this task batch = 2.6813e-01, Meta loss averaged over last 500 steps = 2.8678e-01, PNorm = 149.1101, GNorm = 0.2804
Meta loss on this task batch = 2.7826e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 149.1161, GNorm = 0.2740
Meta loss on this task batch = 3.0279e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 149.1222, GNorm = 0.3353
Meta loss on this task batch = 2.8008e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 149.1282, GNorm = 0.2776
Meta loss on this task batch = 2.9096e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 149.1340, GNorm = 0.2838
Meta loss on this task batch = 3.3902e-01, Meta loss averaged over last 500 steps = 2.8677e-01, PNorm = 149.1393, GNorm = 0.3428
Took 226.0372302532196 seconds to complete one epoch of meta training
Took 233.67449951171875 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461950
Epoch 662
Meta loss on this task batch = 2.7524e-01, Meta loss averaged over last 500 steps = 2.8672e-01, PNorm = 149.1441, GNorm = 0.3439
Meta loss on this task batch = 2.7595e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 149.1489, GNorm = 0.2848
Meta loss on this task batch = 3.0807e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 149.1537, GNorm = 0.3701
Meta loss on this task batch = 2.9705e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 149.1590, GNorm = 0.3850
Meta loss on this task batch = 2.7602e-01, Meta loss averaged over last 500 steps = 2.8682e-01, PNorm = 149.1643, GNorm = 0.2796
Meta loss on this task batch = 2.8244e-01, Meta loss averaged over last 500 steps = 2.8678e-01, PNorm = 149.1696, GNorm = 0.3718
Meta loss on this task batch = 2.7594e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 149.1745, GNorm = 0.2700
Meta loss on this task batch = 3.2031e-01, Meta loss averaged over last 500 steps = 2.8691e-01, PNorm = 149.1788, GNorm = 0.2697
Meta loss on this task batch = 2.9989e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 149.1829, GNorm = 0.3288
Meta loss on this task batch = 2.9267e-01, Meta loss averaged over last 500 steps = 2.8677e-01, PNorm = 149.1871, GNorm = 0.2804
Meta loss on this task batch = 2.7757e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 149.1913, GNorm = 0.2547
Meta loss on this task batch = 3.1865e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 149.1958, GNorm = 0.3316
Meta loss on this task batch = 2.3918e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 149.2007, GNorm = 0.2211
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 149.2057, GNorm = 0.4502
Meta loss on this task batch = 3.0593e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 149.2100, GNorm = 0.2891
Meta loss on this task batch = 2.5740e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 149.2150, GNorm = 0.2929
Meta loss on this task batch = 2.4589e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 149.2203, GNorm = 0.2542
Meta loss on this task batch = 3.5328e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 149.2257, GNorm = 0.4147
Meta loss on this task batch = 2.7083e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 149.2311, GNorm = 0.2664
Took 137.58763766288757 seconds to complete one epoch of meta training
Took 145.2419457435608 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454423
Epoch 663
Meta loss on this task batch = 3.4816e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 149.2359, GNorm = 0.3803
Meta loss on this task batch = 3.3084e-01, Meta loss averaged over last 500 steps = 2.8685e-01, PNorm = 149.2411, GNorm = 0.3094
Meta loss on this task batch = 2.9722e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 149.2460, GNorm = 0.2751
Meta loss on this task batch = 2.4104e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 149.2511, GNorm = 0.3192
Meta loss on this task batch = 2.9941e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 149.2565, GNorm = 0.3316
Meta loss on this task batch = 3.0893e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 149.2623, GNorm = 0.4250
Meta loss on this task batch = 2.8464e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 149.2674, GNorm = 0.3184
Meta loss on this task batch = 3.0199e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 149.2725, GNorm = 0.2864
Meta loss on this task batch = 2.2458e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 149.2775, GNorm = 0.2679
Meta loss on this task batch = 2.7022e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 149.2826, GNorm = 0.3378
Meta loss on this task batch = 2.9211e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 149.2879, GNorm = 0.3670
Meta loss on this task batch = 2.5942e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 149.2940, GNorm = 0.2752
Meta loss on this task batch = 2.7346e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 149.3003, GNorm = 0.2885
Meta loss on this task batch = 2.9281e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 149.3054, GNorm = 0.3921
Meta loss on this task batch = 3.7145e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 149.3101, GNorm = 0.3196
Meta loss on this task batch = 2.5617e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 149.3146, GNorm = 0.3035
Meta loss on this task batch = 2.7834e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 149.3192, GNorm = 0.2629
Meta loss on this task batch = 3.3736e-01, Meta loss averaged over last 500 steps = 2.8668e-01, PNorm = 149.3233, GNorm = 0.3093
Meta loss on this task batch = 3.0482e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 149.3273, GNorm = 0.3544
Took 149.30311679840088 seconds to complete one epoch of meta training
Took 156.91541051864624 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462613
Epoch 664
Meta loss on this task batch = 3.0742e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 149.3309, GNorm = 0.3170
Meta loss on this task batch = 2.3269e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 149.3345, GNorm = 0.2364
Meta loss on this task batch = 2.7953e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 149.3378, GNorm = 0.4615
Meta loss on this task batch = 2.5024e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 149.3412, GNorm = 0.3164
Meta loss on this task batch = 3.0246e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 149.3442, GNorm = 0.2994
Meta loss on this task batch = 3.0308e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 149.3470, GNorm = 0.3558
Meta loss on this task batch = 2.8206e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 149.3498, GNorm = 0.3991
Meta loss on this task batch = 3.0386e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 149.3536, GNorm = 0.2276
Meta loss on this task batch = 2.4303e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 149.3578, GNorm = 0.2099
Meta loss on this task batch = 2.8934e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 149.3620, GNorm = 0.3344
Meta loss on this task batch = 3.1410e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 149.3661, GNorm = 0.3206
Meta loss on this task batch = 2.7659e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 149.3693, GNorm = 0.2441
Meta loss on this task batch = 2.8037e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 149.3716, GNorm = 0.2837
Meta loss on this task batch = 3.3946e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 149.3732, GNorm = 0.3531
Meta loss on this task batch = 3.1139e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 149.3749, GNorm = 0.3156
Meta loss on this task batch = 3.1814e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 149.3757, GNorm = 0.3386
Meta loss on this task batch = 2.4499e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 149.3775, GNorm = 0.2708
Meta loss on this task batch = 3.2153e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 149.3786, GNorm = 0.3197
Meta loss on this task batch = 2.7907e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 149.3799, GNorm = 0.3007
Took 111.73845505714417 seconds to complete one epoch of meta training
Took 119.26187825202942 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457459
Epoch 665
Meta loss on this task batch = 2.8831e-01, Meta loss averaged over last 500 steps = 2.8672e-01, PNorm = 149.3809, GNorm = 0.2988
Meta loss on this task batch = 3.3837e-01, Meta loss averaged over last 500 steps = 2.8688e-01, PNorm = 149.3820, GNorm = 0.3706
Meta loss on this task batch = 3.0739e-01, Meta loss averaged over last 500 steps = 2.8689e-01, PNorm = 149.3836, GNorm = 0.2930
Meta loss on this task batch = 2.5110e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 149.3861, GNorm = 0.2478
Meta loss on this task batch = 2.9167e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 149.3886, GNorm = 0.3193
Meta loss on this task batch = 2.9883e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 149.3918, GNorm = 0.2837
Meta loss on this task batch = 2.4000e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 149.3950, GNorm = 0.2481
Meta loss on this task batch = 3.4552e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 149.3986, GNorm = 0.2841
Meta loss on this task batch = 2.8515e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 149.4020, GNorm = 0.2501
Meta loss on this task batch = 3.1005e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 149.4060, GNorm = 0.2841
Meta loss on this task batch = 3.0765e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 149.4099, GNorm = 0.3660
Meta loss on this task batch = 2.9352e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 149.4145, GNorm = 0.2329
Meta loss on this task batch = 2.7070e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 149.4194, GNorm = 0.3183
Meta loss on this task batch = 2.8139e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 149.4250, GNorm = 0.3004
Meta loss on this task batch = 2.7936e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 149.4305, GNorm = 0.2562
Meta loss on this task batch = 3.3217e-01, Meta loss averaged over last 500 steps = 2.8693e-01, PNorm = 149.4350, GNorm = 0.3491
Meta loss on this task batch = 3.0411e-01, Meta loss averaged over last 500 steps = 2.8694e-01, PNorm = 149.4393, GNorm = 0.2953
Meta loss on this task batch = 2.5576e-01, Meta loss averaged over last 500 steps = 2.8687e-01, PNorm = 149.4441, GNorm = 0.3774
Meta loss on this task batch = 2.5386e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 149.4490, GNorm = 0.4127
Took 112.89895915985107 seconds to complete one epoch of meta training
Took 120.64437317848206 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478573
Epoch 666
Meta loss on this task batch = 2.7235e-01, Meta loss averaged over last 500 steps = 2.8681e-01, PNorm = 149.4543, GNorm = 0.2725
Meta loss on this task batch = 2.5034e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 149.4598, GNorm = 0.2735
Meta loss on this task batch = 2.8776e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 149.4655, GNorm = 0.2729
Meta loss on this task batch = 2.9291e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 149.4707, GNorm = 0.3205
Meta loss on this task batch = 2.9279e-01, Meta loss averaged over last 500 steps = 2.8645e-01, PNorm = 149.4754, GNorm = 0.2888
Meta loss on this task batch = 2.9786e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 149.4804, GNorm = 0.2525
Meta loss on this task batch = 3.2513e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 149.4845, GNorm = 0.3394
Meta loss on this task batch = 2.6767e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 149.4893, GNorm = 0.2542
Meta loss on this task batch = 2.8503e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 149.4937, GNorm = 0.3341
Meta loss on this task batch = 3.2817e-01, Meta loss averaged over last 500 steps = 2.8679e-01, PNorm = 149.4982, GNorm = 0.3184
Meta loss on this task batch = 3.2382e-01, Meta loss averaged over last 500 steps = 2.8678e-01, PNorm = 149.5020, GNorm = 0.3058
Meta loss on this task batch = 3.0255e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 149.5056, GNorm = 0.3095
Meta loss on this task batch = 2.4801e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 149.5090, GNorm = 0.2350
Meta loss on this task batch = 3.0115e-01, Meta loss averaged over last 500 steps = 2.8689e-01, PNorm = 149.5119, GNorm = 0.2599
Meta loss on this task batch = 2.3510e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 149.5151, GNorm = 0.2733
Meta loss on this task batch = 2.5155e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 149.5183, GNorm = 0.2769
Meta loss on this task batch = 3.2386e-01, Meta loss averaged over last 500 steps = 2.8671e-01, PNorm = 149.5223, GNorm = 0.4910
Meta loss on this task batch = 2.8412e-01, Meta loss averaged over last 500 steps = 2.8672e-01, PNorm = 149.5259, GNorm = 0.2779
Meta loss on this task batch = 2.8641e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 149.5297, GNorm = 0.3863
Took 112.06395244598389 seconds to complete one epoch of meta training
Took 119.79330730438232 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475066
Epoch 667
Meta loss on this task batch = 2.9200e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 149.5343, GNorm = 0.2670
Meta loss on this task batch = 3.0997e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 149.5395, GNorm = 0.3662
Meta loss on this task batch = 3.4730e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 149.5449, GNorm = 0.3404
Meta loss on this task batch = 2.5748e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 149.5513, GNorm = 0.2421
Meta loss on this task batch = 2.7431e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 149.5580, GNorm = 0.3251
Meta loss on this task batch = 2.6753e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 149.5646, GNorm = 0.2974
Meta loss on this task batch = 3.1878e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 149.5710, GNorm = 0.3185
Meta loss on this task batch = 2.9915e-01, Meta loss averaged over last 500 steps = 2.8672e-01, PNorm = 149.5774, GNorm = 0.4202
Meta loss on this task batch = 2.8325e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 149.5841, GNorm = 0.3215
Meta loss on this task batch = 2.9937e-01, Meta loss averaged over last 500 steps = 2.8679e-01, PNorm = 149.5902, GNorm = 0.2491
Meta loss on this task batch = 2.4502e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 149.5967, GNorm = 0.2932
Meta loss on this task batch = 3.2168e-01, Meta loss averaged over last 500 steps = 2.8681e-01, PNorm = 149.6019, GNorm = 0.3044
Meta loss on this task batch = 2.9202e-01, Meta loss averaged over last 500 steps = 2.8688e-01, PNorm = 149.6068, GNorm = 0.3064
Meta loss on this task batch = 2.7921e-01, Meta loss averaged over last 500 steps = 2.8678e-01, PNorm = 149.6118, GNorm = 0.2885
Meta loss on this task batch = 3.0000e-01, Meta loss averaged over last 500 steps = 2.8681e-01, PNorm = 149.6160, GNorm = 0.3761
Meta loss on this task batch = 2.8507e-01, Meta loss averaged over last 500 steps = 2.8679e-01, PNorm = 149.6202, GNorm = 0.3776
Meta loss on this task batch = 2.9445e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 149.6245, GNorm = 0.4039
Meta loss on this task batch = 2.4409e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 149.6296, GNorm = 0.4330
Meta loss on this task batch = 2.9270e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 149.6354, GNorm = 0.4485
Took 112.83483958244324 seconds to complete one epoch of meta training
Took 120.64622092247009 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452584
Epoch 668
Meta loss on this task batch = 3.0104e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 149.6418, GNorm = 0.2616
Meta loss on this task batch = 2.7122e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 149.6479, GNorm = 0.2770
Meta loss on this task batch = 2.5125e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 149.6544, GNorm = 0.3505
Meta loss on this task batch = 2.6934e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 149.6601, GNorm = 0.3107
Meta loss on this task batch = 2.7177e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 149.6656, GNorm = 0.3167
Meta loss on this task batch = 2.9201e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 149.6708, GNorm = 0.3374
Meta loss on this task batch = 2.6899e-01, Meta loss averaged over last 500 steps = 2.8637e-01, PNorm = 149.6756, GNorm = 0.2604
Meta loss on this task batch = 2.6302e-01, Meta loss averaged over last 500 steps = 2.8629e-01, PNorm = 149.6803, GNorm = 0.3111
Meta loss on this task batch = 2.8840e-01, Meta loss averaged over last 500 steps = 2.8634e-01, PNorm = 149.6846, GNorm = 0.3044
Meta loss on this task batch = 3.9301e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 149.6876, GNorm = 0.4469
Meta loss on this task batch = 2.6902e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 149.6910, GNorm = 0.2421
Meta loss on this task batch = 2.6837e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 149.6943, GNorm = 0.2719
Meta loss on this task batch = 2.5040e-01, Meta loss averaged over last 500 steps = 2.8631e-01, PNorm = 149.6982, GNorm = 0.2461
Meta loss on this task batch = 2.9492e-01, Meta loss averaged over last 500 steps = 2.8645e-01, PNorm = 149.7018, GNorm = 0.2903
Meta loss on this task batch = 2.9682e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 149.7051, GNorm = 0.3279
Meta loss on this task batch = 2.8730e-01, Meta loss averaged over last 500 steps = 2.8632e-01, PNorm = 149.7084, GNorm = 0.3876
Meta loss on this task batch = 2.3025e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 149.7128, GNorm = 0.2858
Meta loss on this task batch = 3.1329e-01, Meta loss averaged over last 500 steps = 2.8631e-01, PNorm = 149.7171, GNorm = 0.3472
Meta loss on this task batch = 2.7100e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 149.7216, GNorm = 0.3171
Took 112.43955731391907 seconds to complete one epoch of meta training
Took 120.075270652771 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469153
Epoch 669
Meta loss on this task batch = 3.2920e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 149.7261, GNorm = 0.3428
Meta loss on this task batch = 3.0538e-01, Meta loss averaged over last 500 steps = 2.8641e-01, PNorm = 149.7311, GNorm = 0.2927
Meta loss on this task batch = 2.7297e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 149.7358, GNorm = 0.2569
Meta loss on this task batch = 3.3053e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 149.7397, GNorm = 0.3921
Meta loss on this task batch = 2.8051e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 149.7434, GNorm = 0.2815
Meta loss on this task batch = 2.7177e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 149.7478, GNorm = 0.3443
Meta loss on this task batch = 2.9865e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 149.7524, GNorm = 0.2852
Meta loss on this task batch = 2.8088e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 149.7576, GNorm = 0.3072
Meta loss on this task batch = 2.6001e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 149.7628, GNorm = 0.2570
Meta loss on this task batch = 2.6158e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 149.7682, GNorm = 0.2580
Meta loss on this task batch = 2.3954e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 149.7739, GNorm = 0.2617
Meta loss on this task batch = 3.0133e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 149.7792, GNorm = 0.2826
Meta loss on this task batch = 2.9980e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 149.7845, GNorm = 0.2905
Meta loss on this task batch = 2.9599e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 149.7889, GNorm = 0.3684
Meta loss on this task batch = 2.7836e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 149.7937, GNorm = 0.4386
Meta loss on this task batch = 3.3523e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 149.7995, GNorm = 0.3635
Meta loss on this task batch = 2.8089e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 149.8056, GNorm = 0.2740
Meta loss on this task batch = 3.1273e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 149.8113, GNorm = 0.3227
Meta loss on this task batch = 2.7048e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 149.8170, GNorm = 0.4025
Took 109.94897699356079 seconds to complete one epoch of meta training
Took 117.76449966430664 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457788
Epoch 670
Meta loss on this task batch = 2.8092e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 149.8215, GNorm = 0.3552
Meta loss on this task batch = 2.7041e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 149.8266, GNorm = 0.2217
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 149.8318, GNorm = 0.3487
Meta loss on this task batch = 2.3430e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 149.8370, GNorm = 0.2700
Meta loss on this task batch = 3.0452e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 149.8417, GNorm = 0.3870
Meta loss on this task batch = 2.8108e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 149.8459, GNorm = 0.3226
Meta loss on this task batch = 3.6465e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 149.8481, GNorm = 0.3457
Meta loss on this task batch = 2.5237e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 149.8513, GNorm = 0.3309
Meta loss on this task batch = 2.9157e-01, Meta loss averaged over last 500 steps = 2.8668e-01, PNorm = 149.8538, GNorm = 0.3044
Meta loss on this task batch = 3.5347e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 149.8550, GNorm = 0.3888
Meta loss on this task batch = 2.7724e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 149.8572, GNorm = 0.2297
Meta loss on this task batch = 2.8596e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 149.8593, GNorm = 0.3774
Meta loss on this task batch = 3.4158e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 149.8613, GNorm = 0.3060
Meta loss on this task batch = 2.1397e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 149.8642, GNorm = 0.2304
Meta loss on this task batch = 2.3208e-01, Meta loss averaged over last 500 steps = 2.8645e-01, PNorm = 149.8680, GNorm = 0.2412
Meta loss on this task batch = 3.4621e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 149.8713, GNorm = 0.3146
Meta loss on this task batch = 2.7263e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 149.8752, GNorm = 0.2714
Meta loss on this task batch = 3.6007e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 149.8790, GNorm = 0.2885
Meta loss on this task batch = 3.1046e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 149.8831, GNorm = 0.2824
Took 112.1217896938324 seconds to complete one epoch of meta training
Took 120.18985223770142 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.414934
Epoch 671
Meta loss on this task batch = 2.6251e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 149.8875, GNorm = 0.3302
Meta loss on this task batch = 2.7473e-01, Meta loss averaged over last 500 steps = 2.8671e-01, PNorm = 149.8915, GNorm = 0.3965
Meta loss on this task batch = 3.1242e-01, Meta loss averaged over last 500 steps = 2.8668e-01, PNorm = 149.8949, GNorm = 0.3378
Meta loss on this task batch = 3.3749e-01, Meta loss averaged over last 500 steps = 2.8688e-01, PNorm = 149.8982, GNorm = 0.5079
Meta loss on this task batch = 2.9261e-01, Meta loss averaged over last 500 steps = 2.8688e-01, PNorm = 149.9020, GNorm = 0.3862
Meta loss on this task batch = 2.5723e-01, Meta loss averaged over last 500 steps = 2.8689e-01, PNorm = 149.9075, GNorm = 0.3690
Meta loss on this task batch = 2.5963e-01, Meta loss averaged over last 500 steps = 2.8678e-01, PNorm = 149.9130, GNorm = 0.2488
Meta loss on this task batch = 2.6142e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 149.9189, GNorm = 0.2889
Meta loss on this task batch = 2.5846e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 149.9252, GNorm = 0.2950
Meta loss on this task batch = 2.5158e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 149.9316, GNorm = 0.2714
Meta loss on this task batch = 2.4503e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 149.9381, GNorm = 0.2614
Meta loss on this task batch = 2.6533e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 149.9442, GNorm = 0.3533
Meta loss on this task batch = 2.9183e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 149.9502, GNorm = 0.2951
Meta loss on this task batch = 3.0093e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 149.9562, GNorm = 0.2824
Meta loss on this task batch = 3.5129e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 149.9623, GNorm = 0.2825
Meta loss on this task batch = 2.9399e-01, Meta loss averaged over last 500 steps = 2.8682e-01, PNorm = 149.9681, GNorm = 0.2863
Meta loss on this task batch = 2.7969e-01, Meta loss averaged over last 500 steps = 2.8695e-01, PNorm = 149.9745, GNorm = 0.3021
Meta loss on this task batch = 3.3825e-01, Meta loss averaged over last 500 steps = 2.8711e-01, PNorm = 149.9797, GNorm = 0.4102
Meta loss on this task batch = 3.0460e-01, Meta loss averaged over last 500 steps = 2.8716e-01, PNorm = 149.9847, GNorm = 0.3324
Took 117.24691390991211 seconds to complete one epoch of meta training
Took 124.86149001121521 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456716
Epoch 672
Meta loss on this task batch = 2.4788e-01, Meta loss averaged over last 500 steps = 2.8708e-01, PNorm = 149.9894, GNorm = 0.3184
Meta loss on this task batch = 2.6867e-01, Meta loss averaged over last 500 steps = 2.8701e-01, PNorm = 149.9945, GNorm = 0.3485
Meta loss on this task batch = 2.7331e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 149.9989, GNorm = 0.4347
Meta loss on this task batch = 2.7341e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 150.0031, GNorm = 0.2482
Meta loss on this task batch = 2.7283e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 150.0077, GNorm = 0.2346
Meta loss on this task batch = 3.0600e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 150.0125, GNorm = 0.5725
Meta loss on this task batch = 2.7257e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 150.0173, GNorm = 0.3864
Meta loss on this task batch = 2.5831e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 150.0217, GNorm = 0.2488
Meta loss on this task batch = 2.5850e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 150.0269, GNorm = 0.2573
Meta loss on this task batch = 2.7331e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 150.0323, GNorm = 0.3190
Meta loss on this task batch = 2.8881e-01, Meta loss averaged over last 500 steps = 2.8625e-01, PNorm = 150.0372, GNorm = 0.4423
Meta loss on this task batch = 2.9613e-01, Meta loss averaged over last 500 steps = 2.8632e-01, PNorm = 150.0420, GNorm = 0.3187
Meta loss on this task batch = 3.7070e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 150.0465, GNorm = 0.3992
Meta loss on this task batch = 2.9739e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 150.0510, GNorm = 0.2671
Meta loss on this task batch = 2.2817e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 150.0559, GNorm = 0.3505
Meta loss on this task batch = 3.1716e-01, Meta loss averaged over last 500 steps = 2.8640e-01, PNorm = 150.0601, GNorm = 0.3771
Meta loss on this task batch = 3.0691e-01, Meta loss averaged over last 500 steps = 2.8640e-01, PNorm = 150.0647, GNorm = 0.3510
Meta loss on this task batch = 2.8073e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 150.0690, GNorm = 0.2703
Meta loss on this task batch = 2.8780e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 150.0728, GNorm = 0.3487
Took 114.18142199516296 seconds to complete one epoch of meta training
Took 122.59515237808228 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459433
Epoch 673
Meta loss on this task batch = 3.0765e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 150.0761, GNorm = 0.6109
Meta loss on this task batch = 2.2909e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 150.0799, GNorm = 0.3666
Meta loss on this task batch = 2.5181e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 150.0842, GNorm = 0.2834
Meta loss on this task batch = 3.2860e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 150.0880, GNorm = 0.3217
Meta loss on this task batch = 2.8699e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 150.0922, GNorm = 0.3980
Meta loss on this task batch = 2.8591e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 150.0966, GNorm = 0.3143
Meta loss on this task batch = 3.0086e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 150.1013, GNorm = 0.3058
Meta loss on this task batch = 2.4276e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 150.1059, GNorm = 0.2511
Meta loss on this task batch = 3.9718e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 150.1103, GNorm = 0.4043
Meta loss on this task batch = 3.1057e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 150.1147, GNorm = 0.3161
Meta loss on this task batch = 2.6415e-01, Meta loss averaged over last 500 steps = 2.8645e-01, PNorm = 150.1196, GNorm = 0.2794
Meta loss on this task batch = 2.8259e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 150.1244, GNorm = 0.3597
Meta loss on this task batch = 3.0683e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 150.1296, GNorm = 0.3089
Meta loss on this task batch = 2.8361e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 150.1341, GNorm = 0.4188
Meta loss on this task batch = 3.0020e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 150.1383, GNorm = 0.4768
Meta loss on this task batch = 2.7386e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 150.1429, GNorm = 0.3125
Meta loss on this task batch = 2.4575e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 150.1480, GNorm = 0.2064
Meta loss on this task batch = 2.6292e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 150.1537, GNorm = 0.3406
Meta loss on this task batch = 3.3737e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 150.1593, GNorm = 0.4562
Took 113.92727398872375 seconds to complete one epoch of meta training
Took 122.25495481491089 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.435617
Epoch 674
Meta loss on this task batch = 2.7092e-01, Meta loss averaged over last 500 steps = 2.8672e-01, PNorm = 150.1654, GNorm = 0.4923
Meta loss on this task batch = 3.0948e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 150.1710, GNorm = 0.3404
Meta loss on this task batch = 3.0402e-01, Meta loss averaged over last 500 steps = 2.8679e-01, PNorm = 150.1769, GNorm = 0.2613
Meta loss on this task batch = 3.1944e-01, Meta loss averaged over last 500 steps = 2.8685e-01, PNorm = 150.1837, GNorm = 0.2690
Meta loss on this task batch = 2.6524e-01, Meta loss averaged over last 500 steps = 2.8679e-01, PNorm = 150.1898, GNorm = 0.2539
Meta loss on this task batch = 2.8150e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 150.1960, GNorm = 0.3285
Meta loss on this task batch = 2.7975e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 150.2019, GNorm = 0.2828
Meta loss on this task batch = 3.1345e-01, Meta loss averaged over last 500 steps = 2.8682e-01, PNorm = 150.2069, GNorm = 0.4019
Meta loss on this task batch = 3.0755e-01, Meta loss averaged over last 500 steps = 2.8696e-01, PNorm = 150.2105, GNorm = 0.3257
Meta loss on this task batch = 2.9726e-01, Meta loss averaged over last 500 steps = 2.8698e-01, PNorm = 150.2138, GNorm = 0.3375
Meta loss on this task batch = 2.7046e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 150.2172, GNorm = 0.3413
Meta loss on this task batch = 2.5040e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 150.2209, GNorm = 0.3136
Meta loss on this task batch = 2.9161e-01, Meta loss averaged over last 500 steps = 2.8678e-01, PNorm = 150.2249, GNorm = 0.2790
Meta loss on this task batch = 2.6394e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 150.2293, GNorm = 0.3883
Meta loss on this task batch = 3.3283e-01, Meta loss averaged over last 500 steps = 2.8679e-01, PNorm = 150.2337, GNorm = 0.3458
Meta loss on this task batch = 2.4289e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 150.2386, GNorm = 0.2763
Meta loss on this task batch = 2.5731e-01, Meta loss averaged over last 500 steps = 2.8678e-01, PNorm = 150.2428, GNorm = 0.2261
Meta loss on this task batch = 3.0051e-01, Meta loss averaged over last 500 steps = 2.8679e-01, PNorm = 150.2476, GNorm = 0.2928
Meta loss on this task batch = 2.5003e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 150.2525, GNorm = 0.3444
Took 112.9819986820221 seconds to complete one epoch of meta training
Took 121.18343710899353 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446243
Epoch 675
Meta loss on this task batch = 3.2010e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 150.2577, GNorm = 0.4319
Meta loss on this task batch = 2.3726e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 150.2632, GNorm = 0.2724
Meta loss on this task batch = 2.2368e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 150.2690, GNorm = 0.2876
Meta loss on this task batch = 2.6190e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 150.2748, GNorm = 0.3446
Meta loss on this task batch = 2.8206e-01, Meta loss averaged over last 500 steps = 2.8643e-01, PNorm = 150.2804, GNorm = 0.2903
Meta loss on this task batch = 3.4096e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 150.2856, GNorm = 0.3832
Meta loss on this task batch = 2.4886e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 150.2909, GNorm = 0.2477
Meta loss on this task batch = 3.1382e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 150.2956, GNorm = 0.3497
Meta loss on this task batch = 3.1648e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 150.2996, GNorm = 0.3216
Meta loss on this task batch = 3.0641e-01, Meta loss averaged over last 500 steps = 2.8690e-01, PNorm = 150.3037, GNorm = 0.2757
Meta loss on this task batch = 2.8474e-01, Meta loss averaged over last 500 steps = 2.8700e-01, PNorm = 150.3080, GNorm = 0.3065
Meta loss on this task batch = 2.9268e-01, Meta loss averaged over last 500 steps = 2.8701e-01, PNorm = 150.3127, GNorm = 0.3211
Meta loss on this task batch = 3.1629e-01, Meta loss averaged over last 500 steps = 2.8706e-01, PNorm = 150.3172, GNorm = 0.3303
Meta loss on this task batch = 3.5356e-01, Meta loss averaged over last 500 steps = 2.8722e-01, PNorm = 150.3215, GNorm = 0.4136
Meta loss on this task batch = 2.5282e-01, Meta loss averaged over last 500 steps = 2.8709e-01, PNorm = 150.3258, GNorm = 0.3533
Meta loss on this task batch = 2.1585e-01, Meta loss averaged over last 500 steps = 2.8696e-01, PNorm = 150.3300, GNorm = 0.2862
Meta loss on this task batch = 2.4050e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 150.3338, GNorm = 0.2354
Meta loss on this task batch = 3.3164e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 150.3387, GNorm = 0.2733
Meta loss on this task batch = 3.1369e-01, Meta loss averaged over last 500 steps = 2.8688e-01, PNorm = 150.3439, GNorm = 0.4327
Took 146.02531266212463 seconds to complete one epoch of meta training
Took 153.8947455883026 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472405
Epoch 676
Meta loss on this task batch = 2.7622e-01, Meta loss averaged over last 500 steps = 2.8681e-01, PNorm = 150.3493, GNorm = 0.2898
Meta loss on this task batch = 3.0391e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 150.3547, GNorm = 0.3057
Meta loss on this task batch = 2.9307e-01, Meta loss averaged over last 500 steps = 2.8689e-01, PNorm = 150.3601, GNorm = 0.4363
Meta loss on this task batch = 2.9686e-01, Meta loss averaged over last 500 steps = 2.8690e-01, PNorm = 150.3645, GNorm = 0.2644
Meta loss on this task batch = 2.9321e-01, Meta loss averaged over last 500 steps = 2.8696e-01, PNorm = 150.3682, GNorm = 0.3364
Meta loss on this task batch = 2.7666e-01, Meta loss averaged over last 500 steps = 2.8693e-01, PNorm = 150.3720, GNorm = 0.2716
Meta loss on this task batch = 2.3123e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 150.3761, GNorm = 0.2415
Meta loss on this task batch = 3.2213e-01, Meta loss averaged over last 500 steps = 2.8685e-01, PNorm = 150.3795, GNorm = 0.5286
Meta loss on this task batch = 2.9696e-01, Meta loss averaged over last 500 steps = 2.8692e-01, PNorm = 150.3830, GNorm = 0.4293
Meta loss on this task batch = 3.1023e-01, Meta loss averaged over last 500 steps = 2.8690e-01, PNorm = 150.3859, GNorm = 0.3563
Meta loss on this task batch = 2.3156e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 150.3892, GNorm = 0.3061
Meta loss on this task batch = 2.7903e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 150.3922, GNorm = 0.3179
Meta loss on this task batch = 3.0938e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 150.3953, GNorm = 0.3686
Meta loss on this task batch = 3.0443e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 150.3983, GNorm = 0.2828
Meta loss on this task batch = 3.2241e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 150.4011, GNorm = 0.2824
Meta loss on this task batch = 2.7831e-01, Meta loss averaged over last 500 steps = 2.8671e-01, PNorm = 150.4045, GNorm = 0.2894
Meta loss on this task batch = 2.6404e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 150.4082, GNorm = 0.3540
Meta loss on this task batch = 3.1085e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 150.4125, GNorm = 0.3561
Meta loss on this task batch = 2.5085e-01, Meta loss averaged over last 500 steps = 2.8681e-01, PNorm = 150.4168, GNorm = 0.3724
Took 110.83342695236206 seconds to complete one epoch of meta training
Took 118.50793552398682 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459705
Epoch 677
Meta loss on this task batch = 2.7635e-01, Meta loss averaged over last 500 steps = 2.8682e-01, PNorm = 150.4215, GNorm = 0.2505
Meta loss on this task batch = 2.9037e-01, Meta loss averaged over last 500 steps = 2.8682e-01, PNorm = 150.4267, GNorm = 0.2732
Meta loss on this task batch = 2.6668e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 150.4320, GNorm = 0.2663
Meta loss on this task batch = 2.4712e-01, Meta loss averaged over last 500 steps = 2.8681e-01, PNorm = 150.4379, GNorm = 0.2755
Meta loss on this task batch = 3.0696e-01, Meta loss averaged over last 500 steps = 2.8695e-01, PNorm = 150.4440, GNorm = 0.3691
Meta loss on this task batch = 3.2513e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 150.4491, GNorm = 0.2782
Meta loss on this task batch = 2.9688e-01, Meta loss averaged over last 500 steps = 2.8698e-01, PNorm = 150.4536, GNorm = 0.3694
Meta loss on this task batch = 3.0318e-01, Meta loss averaged over last 500 steps = 2.8689e-01, PNorm = 150.4581, GNorm = 0.3225
Meta loss on this task batch = 3.0940e-01, Meta loss averaged over last 500 steps = 2.8695e-01, PNorm = 150.4622, GNorm = 0.2836
Meta loss on this task batch = 3.3034e-01, Meta loss averaged over last 500 steps = 2.8709e-01, PNorm = 150.4652, GNorm = 0.3660
Meta loss on this task batch = 3.0384e-01, Meta loss averaged over last 500 steps = 2.8701e-01, PNorm = 150.4687, GNorm = 0.2995
Meta loss on this task batch = 2.5545e-01, Meta loss averaged over last 500 steps = 2.8695e-01, PNorm = 150.4730, GNorm = 0.2780
Meta loss on this task batch = 2.8813e-01, Meta loss averaged over last 500 steps = 2.8699e-01, PNorm = 150.4779, GNorm = 0.3177
Meta loss on this task batch = 2.4120e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 150.4840, GNorm = 0.3127
Meta loss on this task batch = 3.1157e-01, Meta loss averaged over last 500 steps = 2.8692e-01, PNorm = 150.4894, GNorm = 0.3200
Meta loss on this task batch = 2.6940e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 150.4942, GNorm = 0.2334
Meta loss on this task batch = 2.5935e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 150.4996, GNorm = 0.3005
Meta loss on this task batch = 2.9195e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 150.5048, GNorm = 0.2880
Meta loss on this task batch = 3.0782e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 150.5110, GNorm = 0.3324
Took 115.12324142456055 seconds to complete one epoch of meta training
Took 123.02708768844604 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.437850
Epoch 678
Meta loss on this task batch = 2.9471e-01, Meta loss averaged over last 500 steps = 2.8691e-01, PNorm = 150.5168, GNorm = 0.3656
Meta loss on this task batch = 2.7156e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 150.5224, GNorm = 0.3692
Meta loss on this task batch = 3.1999e-01, Meta loss averaged over last 500 steps = 2.8697e-01, PNorm = 150.5281, GNorm = 0.2644
Meta loss on this task batch = 1.8903e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 150.5343, GNorm = 0.2638
Meta loss on this task batch = 2.2704e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 150.5408, GNorm = 0.4024
Meta loss on this task batch = 2.6598e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 150.5467, GNorm = 0.3200
Meta loss on this task batch = 2.7917e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 150.5529, GNorm = 0.2584
Meta loss on this task batch = 2.9228e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 150.5590, GNorm = 0.3690
Meta loss on this task batch = 2.7920e-01, Meta loss averaged over last 500 steps = 2.8641e-01, PNorm = 150.5653, GNorm = 0.2743
Meta loss on this task batch = 3.3116e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 150.5692, GNorm = 0.2940
Meta loss on this task batch = 2.8131e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 150.5726, GNorm = 0.5647
Meta loss on this task batch = 2.5171e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 150.5766, GNorm = 0.2499
Meta loss on this task batch = 3.0799e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 150.5802, GNorm = 0.2917
Meta loss on this task batch = 2.9569e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 150.5830, GNorm = 0.2699
Meta loss on this task batch = 3.0775e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 150.5858, GNorm = 0.3110
Meta loss on this task batch = 3.2031e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 150.5885, GNorm = 0.2561
Meta loss on this task batch = 2.7680e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 150.5914, GNorm = 0.3318
Meta loss on this task batch = 2.8193e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 150.5943, GNorm = 0.3016
Meta loss on this task batch = 3.3461e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 150.5968, GNorm = 0.3449
Took 115.57852220535278 seconds to complete one epoch of meta training
Took 123.71492290496826 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451413
Epoch 679
Meta loss on this task batch = 2.4652e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 150.5995, GNorm = 0.3241
Meta loss on this task batch = 2.6597e-01, Meta loss averaged over last 500 steps = 2.8645e-01, PNorm = 150.6027, GNorm = 0.2820
Meta loss on this task batch = 2.9518e-01, Meta loss averaged over last 500 steps = 2.8641e-01, PNorm = 150.6055, GNorm = 0.2988
Meta loss on this task batch = 3.0235e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 150.6084, GNorm = 0.3473
Meta loss on this task batch = 2.4190e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 150.6120, GNorm = 0.2690
Meta loss on this task batch = 2.8076e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 150.6160, GNorm = 0.2648
Meta loss on this task batch = 2.8079e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 150.6203, GNorm = 0.2915
Meta loss on this task batch = 2.7880e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 150.6245, GNorm = 0.2899
Meta loss on this task batch = 2.7015e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 150.6289, GNorm = 0.3045
Meta loss on this task batch = 2.5857e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 150.6342, GNorm = 0.3953
Meta loss on this task batch = 2.8788e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 150.6402, GNorm = 0.2697
Meta loss on this task batch = 2.8206e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 150.6457, GNorm = 0.3018
Meta loss on this task batch = 2.7942e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 150.6513, GNorm = 0.3305
Meta loss on this task batch = 3.0616e-01, Meta loss averaged over last 500 steps = 2.8641e-01, PNorm = 150.6566, GNorm = 0.3427
Meta loss on this task batch = 2.7217e-01, Meta loss averaged over last 500 steps = 2.8639e-01, PNorm = 150.6616, GNorm = 0.4214
Meta loss on this task batch = 3.1379e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 150.6660, GNorm = 0.2734
Meta loss on this task batch = 2.4069e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 150.6715, GNorm = 0.2592
Meta loss on this task batch = 3.1103e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 150.6764, GNorm = 0.4423
Meta loss on this task batch = 3.8601e-01, Meta loss averaged over last 500 steps = 2.8671e-01, PNorm = 150.6802, GNorm = 0.5875
Took 111.87244415283203 seconds to complete one epoch of meta training
Took 119.65899395942688 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465105
Epoch 680
Meta loss on this task batch = 2.8312e-01, Meta loss averaged over last 500 steps = 2.8678e-01, PNorm = 150.6839, GNorm = 0.4268
Meta loss on this task batch = 3.2545e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 150.6875, GNorm = 0.3392
Meta loss on this task batch = 2.2341e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 150.6913, GNorm = 0.2436
Meta loss on this task batch = 3.0897e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 150.6949, GNorm = 0.2922
Meta loss on this task batch = 3.3848e-01, Meta loss averaged over last 500 steps = 2.8672e-01, PNorm = 150.6982, GNorm = 0.2919
Meta loss on this task batch = 2.8600e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 150.7012, GNorm = 0.2571
Meta loss on this task batch = 2.2423e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 150.7051, GNorm = 0.2686
Meta loss on this task batch = 2.7542e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 150.7098, GNorm = 0.3878
Meta loss on this task batch = 3.0433e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 150.7155, GNorm = 0.3878
Meta loss on this task batch = 3.4295e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 150.7208, GNorm = 0.3095
Meta loss on this task batch = 3.0804e-01, Meta loss averaged over last 500 steps = 2.8692e-01, PNorm = 150.7254, GNorm = 0.3186
Meta loss on this task batch = 3.0099e-01, Meta loss averaged over last 500 steps = 2.8685e-01, PNorm = 150.7294, GNorm = 0.3092
Meta loss on this task batch = 2.5031e-01, Meta loss averaged over last 500 steps = 2.8671e-01, PNorm = 150.7333, GNorm = 0.4623
Meta loss on this task batch = 3.0251e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 150.7368, GNorm = 0.2898
Meta loss on this task batch = 2.8820e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 150.7402, GNorm = 0.3997
Meta loss on this task batch = 2.6773e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 150.7441, GNorm = 0.3796
Meta loss on this task batch = 3.2513e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 150.7478, GNorm = 0.3593
Meta loss on this task batch = 2.7897e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 150.7512, GNorm = 0.2575
Meta loss on this task batch = 2.8001e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 150.7538, GNorm = 0.2954
Took 111.69624662399292 seconds to complete one epoch of meta training
Took 119.93541598320007 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471808
Epoch 681
Meta loss on this task batch = 2.8624e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 150.7566, GNorm = 0.3355
Meta loss on this task batch = 2.9260e-01, Meta loss averaged over last 500 steps = 2.8668e-01, PNorm = 150.7599, GNorm = 0.2643
Meta loss on this task batch = 2.4251e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 150.7640, GNorm = 0.2401
Meta loss on this task batch = 3.0596e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 150.7682, GNorm = 0.2654
Meta loss on this task batch = 2.9103e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 150.7718, GNorm = 0.3074
Meta loss on this task batch = 2.6250e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 150.7754, GNorm = 0.2956
Meta loss on this task batch = 3.0109e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 150.7789, GNorm = 0.3656
Meta loss on this task batch = 2.5070e-01, Meta loss averaged over last 500 steps = 2.8637e-01, PNorm = 150.7822, GNorm = 0.2368
Meta loss on this task batch = 2.3870e-01, Meta loss averaged over last 500 steps = 2.8628e-01, PNorm = 150.7860, GNorm = 0.2566
Meta loss on this task batch = 2.8108e-01, Meta loss averaged over last 500 steps = 2.8632e-01, PNorm = 150.7897, GNorm = 0.2988
Meta loss on this task batch = 3.3561e-01, Meta loss averaged over last 500 steps = 2.8629e-01, PNorm = 150.7928, GNorm = 0.3267
Meta loss on this task batch = 3.1808e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 150.7957, GNorm = 0.2854
Meta loss on this task batch = 3.0688e-01, Meta loss averaged over last 500 steps = 2.8637e-01, PNorm = 150.7994, GNorm = 0.2646
Meta loss on this task batch = 2.6898e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 150.8036, GNorm = 0.2624
Meta loss on this task batch = 2.7674e-01, Meta loss averaged over last 500 steps = 2.8634e-01, PNorm = 150.8086, GNorm = 0.2611
Meta loss on this task batch = 3.1997e-01, Meta loss averaged over last 500 steps = 2.8637e-01, PNorm = 150.8137, GNorm = 0.4428
Meta loss on this task batch = 2.6380e-01, Meta loss averaged over last 500 steps = 2.8639e-01, PNorm = 150.8187, GNorm = 0.2976
Meta loss on this task batch = 2.7922e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 150.8232, GNorm = 0.2822
Meta loss on this task batch = 2.5687e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 150.8285, GNorm = 0.5176
Took 112.77808880805969 seconds to complete one epoch of meta training
Took 121.85924458503723 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458629
Epoch 682
Meta loss on this task batch = 2.8504e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 150.8332, GNorm = 0.3298
Meta loss on this task batch = 2.7242e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 150.8371, GNorm = 0.2793
Meta loss on this task batch = 2.3906e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 150.8412, GNorm = 0.3168
Meta loss on this task batch = 2.3014e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 150.8460, GNorm = 0.3369
Meta loss on this task batch = 1.9280e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 150.8506, GNorm = 0.2433
Meta loss on this task batch = 2.5385e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 150.8561, GNorm = 0.3893
Meta loss on this task batch = 3.0019e-01, Meta loss averaged over last 500 steps = 2.8627e-01, PNorm = 150.8615, GNorm = 0.2774
Meta loss on this task batch = 2.8882e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 150.8669, GNorm = 0.2915
Meta loss on this task batch = 3.3980e-01, Meta loss averaged over last 500 steps = 2.8643e-01, PNorm = 150.8714, GNorm = 0.3109
Meta loss on this task batch = 3.5543e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 150.8751, GNorm = 0.3381
Meta loss on this task batch = 3.3621e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 150.8786, GNorm = 0.3461
Meta loss on this task batch = 2.8654e-01, Meta loss averaged over last 500 steps = 2.8671e-01, PNorm = 150.8828, GNorm = 0.2967
Meta loss on this task batch = 2.7528e-01, Meta loss averaged over last 500 steps = 2.8679e-01, PNorm = 150.8869, GNorm = 0.2795
Meta loss on this task batch = 2.8832e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 150.8913, GNorm = 0.2840
Meta loss on this task batch = 3.5168e-01, Meta loss averaged over last 500 steps = 2.8694e-01, PNorm = 150.8954, GNorm = 0.3452
Meta loss on this task batch = 2.6345e-01, Meta loss averaged over last 500 steps = 2.8689e-01, PNorm = 150.8996, GNorm = 0.2899
Meta loss on this task batch = 3.1501e-01, Meta loss averaged over last 500 steps = 2.8692e-01, PNorm = 150.9040, GNorm = 0.2944
Meta loss on this task batch = 2.8111e-01, Meta loss averaged over last 500 steps = 2.8691e-01, PNorm = 150.9093, GNorm = 0.2912
Meta loss on this task batch = 2.3887e-01, Meta loss averaged over last 500 steps = 2.8692e-01, PNorm = 150.9143, GNorm = 0.3492
Took 114.60963487625122 seconds to complete one epoch of meta training
Took 122.41559553146362 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473170
Epoch 683
Meta loss on this task batch = 2.8104e-01, Meta loss averaged over last 500 steps = 2.8697e-01, PNorm = 150.9194, GNorm = 0.2652
Meta loss on this task batch = 2.8819e-01, Meta loss averaged over last 500 steps = 2.8688e-01, PNorm = 150.9255, GNorm = 0.3055
Meta loss on this task batch = 2.8093e-01, Meta loss averaged over last 500 steps = 2.8687e-01, PNorm = 150.9319, GNorm = 0.2609
Meta loss on this task batch = 2.5168e-01, Meta loss averaged over last 500 steps = 2.8677e-01, PNorm = 150.9381, GNorm = 0.2856
Meta loss on this task batch = 3.0865e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 150.9435, GNorm = 0.2985
Meta loss on this task batch = 3.0466e-01, Meta loss averaged over last 500 steps = 2.8678e-01, PNorm = 150.9486, GNorm = 0.5681
Meta loss on this task batch = 2.8376e-01, Meta loss averaged over last 500 steps = 2.8682e-01, PNorm = 150.9535, GNorm = 0.3129
Meta loss on this task batch = 2.6475e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 150.9581, GNorm = 0.2850
Meta loss on this task batch = 2.9341e-01, Meta loss averaged over last 500 steps = 2.8677e-01, PNorm = 150.9624, GNorm = 0.3003
Meta loss on this task batch = 3.0850e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 150.9660, GNorm = 0.3483
Meta loss on this task batch = 3.0999e-01, Meta loss averaged over last 500 steps = 2.8691e-01, PNorm = 150.9690, GNorm = 0.2916
Meta loss on this task batch = 2.6565e-01, Meta loss averaged over last 500 steps = 2.8687e-01, PNorm = 150.9722, GNorm = 0.2619
Meta loss on this task batch = 3.2600e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 150.9755, GNorm = 0.3745
Meta loss on this task batch = 2.6961e-01, Meta loss averaged over last 500 steps = 2.8691e-01, PNorm = 150.9788, GNorm = 0.2915
Meta loss on this task batch = 3.4336e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 150.9820, GNorm = 0.4041
Meta loss on this task batch = 2.9390e-01, Meta loss averaged over last 500 steps = 2.8715e-01, PNorm = 150.9856, GNorm = 0.2729
Meta loss on this task batch = 2.8517e-01, Meta loss averaged over last 500 steps = 2.8716e-01, PNorm = 150.9903, GNorm = 0.4850
Meta loss on this task batch = 2.9561e-01, Meta loss averaged over last 500 steps = 2.8714e-01, PNorm = 150.9959, GNorm = 0.3220
Meta loss on this task batch = 2.7819e-01, Meta loss averaged over last 500 steps = 2.8713e-01, PNorm = 151.0019, GNorm = 0.3488
Took 114.00692009925842 seconds to complete one epoch of meta training
Took 120.92700529098511 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452333
Epoch 684
Meta loss on this task batch = 2.6041e-01, Meta loss averaged over last 500 steps = 2.8713e-01, PNorm = 151.0077, GNorm = 0.2882
Meta loss on this task batch = 2.7241e-01, Meta loss averaged over last 500 steps = 2.8710e-01, PNorm = 151.0136, GNorm = 0.4599
Meta loss on this task batch = 2.7821e-01, Meta loss averaged over last 500 steps = 2.8695e-01, PNorm = 151.0194, GNorm = 0.3045
Meta loss on this task batch = 3.1307e-01, Meta loss averaged over last 500 steps = 2.8691e-01, PNorm = 151.0249, GNorm = 0.3697
Meta loss on this task batch = 2.8792e-01, Meta loss averaged over last 500 steps = 2.8693e-01, PNorm = 151.0304, GNorm = 0.3148
Meta loss on this task batch = 2.8794e-01, Meta loss averaged over last 500 steps = 2.8694e-01, PNorm = 151.0362, GNorm = 0.2427
Meta loss on this task batch = 2.8063e-01, Meta loss averaged over last 500 steps = 2.8698e-01, PNorm = 151.0411, GNorm = 0.2754
Meta loss on this task batch = 3.1185e-01, Meta loss averaged over last 500 steps = 2.8709e-01, PNorm = 151.0463, GNorm = 0.3781
Meta loss on this task batch = 3.3300e-01, Meta loss averaged over last 500 steps = 2.8720e-01, PNorm = 151.0515, GNorm = 0.3505
Meta loss on this task batch = 2.7990e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 151.0569, GNorm = 0.3527
Meta loss on this task batch = 2.7965e-01, Meta loss averaged over last 500 steps = 2.8720e-01, PNorm = 151.0628, GNorm = 0.3167
Meta loss on this task batch = 2.6243e-01, Meta loss averaged over last 500 steps = 2.8715e-01, PNorm = 151.0685, GNorm = 0.3875
Meta loss on this task batch = 2.7118e-01, Meta loss averaged over last 500 steps = 2.8708e-01, PNorm = 151.0740, GNorm = 0.2443
Meta loss on this task batch = 2.7730e-01, Meta loss averaged over last 500 steps = 2.8715e-01, PNorm = 151.0800, GNorm = 0.4312
Meta loss on this task batch = 2.5219e-01, Meta loss averaged over last 500 steps = 2.8706e-01, PNorm = 151.0853, GNorm = 0.2917
Meta loss on this task batch = 3.2462e-01, Meta loss averaged over last 500 steps = 2.8709e-01, PNorm = 151.0908, GNorm = 0.2813
Meta loss on this task batch = 2.7983e-01, Meta loss averaged over last 500 steps = 2.8714e-01, PNorm = 151.0957, GNorm = 0.3965
Meta loss on this task batch = 2.9250e-01, Meta loss averaged over last 500 steps = 2.8715e-01, PNorm = 151.1006, GNorm = 0.3740
Meta loss on this task batch = 2.9551e-01, Meta loss averaged over last 500 steps = 2.8709e-01, PNorm = 151.1062, GNorm = 0.2896
Took 113.29590940475464 seconds to complete one epoch of meta training
Took 120.81266593933105 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468146
Epoch 685
Meta loss on this task batch = 2.7165e-01, Meta loss averaged over last 500 steps = 2.8706e-01, PNorm = 151.1112, GNorm = 0.2818
Meta loss on this task batch = 2.3576e-01, Meta loss averaged over last 500 steps = 2.8699e-01, PNorm = 151.1169, GNorm = 0.3407
Meta loss on this task batch = 2.9012e-01, Meta loss averaged over last 500 steps = 2.8697e-01, PNorm = 151.1217, GNorm = 0.2588
Meta loss on this task batch = 2.6811e-01, Meta loss averaged over last 500 steps = 2.8690e-01, PNorm = 151.1263, GNorm = 0.3128
Meta loss on this task batch = 3.0231e-01, Meta loss averaged over last 500 steps = 2.8692e-01, PNorm = 151.1309, GNorm = 0.3100
Meta loss on this task batch = 2.4863e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 151.1356, GNorm = 0.4002
Meta loss on this task batch = 2.8105e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 151.1395, GNorm = 0.3046
Meta loss on this task batch = 3.3076e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 151.1426, GNorm = 0.5717
Meta loss on this task batch = 2.9549e-01, Meta loss averaged over last 500 steps = 2.8681e-01, PNorm = 151.1452, GNorm = 0.2764
Meta loss on this task batch = 3.0091e-01, Meta loss averaged over last 500 steps = 2.8694e-01, PNorm = 151.1477, GNorm = 0.3071
Meta loss on this task batch = 2.4351e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 151.1515, GNorm = 0.2983
Meta loss on this task batch = 3.0131e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 151.1560, GNorm = 0.2811
Meta loss on this task batch = 3.2287e-01, Meta loss averaged over last 500 steps = 2.8689e-01, PNorm = 151.1600, GNorm = 0.3385
Meta loss on this task batch = 2.7866e-01, Meta loss averaged over last 500 steps = 2.8689e-01, PNorm = 151.1637, GNorm = 0.2984
Meta loss on this task batch = 2.8378e-01, Meta loss averaged over last 500 steps = 2.8681e-01, PNorm = 151.1677, GNorm = 0.4655
Meta loss on this task batch = 2.9534e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 151.1714, GNorm = 0.2927
Meta loss on this task batch = 3.0902e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 151.1754, GNorm = 0.2530
Meta loss on this task batch = 3.1570e-01, Meta loss averaged over last 500 steps = 2.8681e-01, PNorm = 151.1798, GNorm = 0.2919
Meta loss on this task batch = 3.0873e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 151.1839, GNorm = 0.3600
Took 113.3171796798706 seconds to complete one epoch of meta training
Took 120.23681163787842 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456740
Epoch 686
Meta loss on this task batch = 2.8676e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 151.1881, GNorm = 0.3251
Meta loss on this task batch = 2.4594e-01, Meta loss averaged over last 500 steps = 2.8682e-01, PNorm = 151.1922, GNorm = 0.2880
Meta loss on this task batch = 3.3394e-01, Meta loss averaged over last 500 steps = 2.8685e-01, PNorm = 151.1957, GNorm = 0.5157
Meta loss on this task batch = 2.5823e-01, Meta loss averaged over last 500 steps = 2.8677e-01, PNorm = 151.1995, GNorm = 0.2975
Meta loss on this task batch = 2.8654e-01, Meta loss averaged over last 500 steps = 2.8695e-01, PNorm = 151.2038, GNorm = 0.3628
Meta loss on this task batch = 2.3385e-01, Meta loss averaged over last 500 steps = 2.8692e-01, PNorm = 151.2082, GNorm = 0.2126
Meta loss on this task batch = 3.0435e-01, Meta loss averaged over last 500 steps = 2.8696e-01, PNorm = 151.2121, GNorm = 0.3420
Meta loss on this task batch = 2.9170e-01, Meta loss averaged over last 500 steps = 2.8687e-01, PNorm = 151.2165, GNorm = 0.2941
Meta loss on this task batch = 2.9398e-01, Meta loss averaged over last 500 steps = 2.8701e-01, PNorm = 151.2210, GNorm = 0.2833
Meta loss on this task batch = 2.7479e-01, Meta loss averaged over last 500 steps = 2.8709e-01, PNorm = 151.2253, GNorm = 0.2711
Meta loss on this task batch = 2.6043e-01, Meta loss averaged over last 500 steps = 2.8693e-01, PNorm = 151.2308, GNorm = 0.3504
Meta loss on this task batch = 2.6889e-01, Meta loss averaged over last 500 steps = 2.8678e-01, PNorm = 151.2359, GNorm = 0.2753
Meta loss on this task batch = 2.6395e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 151.2404, GNorm = 0.3232
Meta loss on this task batch = 2.5846e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 151.2457, GNorm = 0.4059
Meta loss on this task batch = 3.0358e-01, Meta loss averaged over last 500 steps = 2.8677e-01, PNorm = 151.2506, GNorm = 0.3426
Meta loss on this task batch = 3.2484e-01, Meta loss averaged over last 500 steps = 2.8688e-01, PNorm = 151.2542, GNorm = 0.3669
Meta loss on this task batch = 3.0286e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 151.2577, GNorm = 0.2471
Meta loss on this task batch = 2.9882e-01, Meta loss averaged over last 500 steps = 2.8687e-01, PNorm = 151.2615, GNorm = 0.3234
Meta loss on this task batch = 3.4736e-01, Meta loss averaged over last 500 steps = 2.8698e-01, PNorm = 151.2640, GNorm = 0.3664
Took 113.76898264884949 seconds to complete one epoch of meta training
Took 121.50256180763245 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472961
Epoch 687
Meta loss on this task batch = 3.3684e-01, Meta loss averaged over last 500 steps = 2.8717e-01, PNorm = 151.2669, GNorm = 0.2974
Meta loss on this task batch = 2.5108e-01, Meta loss averaged over last 500 steps = 2.8717e-01, PNorm = 151.2706, GNorm = 0.2995
Meta loss on this task batch = 2.7825e-01, Meta loss averaged over last 500 steps = 2.8718e-01, PNorm = 151.2742, GNorm = 0.2508
Meta loss on this task batch = 2.2513e-01, Meta loss averaged over last 500 steps = 2.8707e-01, PNorm = 151.2785, GNorm = 0.2307
Meta loss on this task batch = 3.1629e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 151.2821, GNorm = 0.2993
Meta loss on this task batch = 2.8067e-01, Meta loss averaged over last 500 steps = 2.8705e-01, PNorm = 151.2860, GNorm = 0.2905
Meta loss on this task batch = 2.7856e-01, Meta loss averaged over last 500 steps = 2.8696e-01, PNorm = 151.2902, GNorm = 0.3004
Meta loss on this task batch = 2.7012e-01, Meta loss averaged over last 500 steps = 2.8693e-01, PNorm = 151.2950, GNorm = 0.3402
Meta loss on this task batch = 2.8152e-01, Meta loss averaged over last 500 steps = 2.8701e-01, PNorm = 151.2994, GNorm = 0.2521
Meta loss on this task batch = 3.1147e-01, Meta loss averaged over last 500 steps = 2.8708e-01, PNorm = 151.3038, GNorm = 0.3488
Meta loss on this task batch = 2.7713e-01, Meta loss averaged over last 500 steps = 2.8699e-01, PNorm = 151.3083, GNorm = 0.2859
Meta loss on this task batch = 2.7287e-01, Meta loss averaged over last 500 steps = 2.8697e-01, PNorm = 151.3127, GNorm = 0.2986
Meta loss on this task batch = 2.3929e-01, Meta loss averaged over last 500 steps = 2.8700e-01, PNorm = 151.3171, GNorm = 0.3566
Meta loss on this task batch = 2.8735e-01, Meta loss averaged over last 500 steps = 2.8707e-01, PNorm = 151.3211, GNorm = 0.3121
Meta loss on this task batch = 2.7705e-01, Meta loss averaged over last 500 steps = 2.8711e-01, PNorm = 151.3254, GNorm = 0.2941
Meta loss on this task batch = 2.9165e-01, Meta loss averaged over last 500 steps = 2.8711e-01, PNorm = 151.3294, GNorm = 0.2899
Meta loss on this task batch = 3.1299e-01, Meta loss averaged over last 500 steps = 2.8717e-01, PNorm = 151.3338, GNorm = 0.3277
Meta loss on this task batch = 2.9820e-01, Meta loss averaged over last 500 steps = 2.8722e-01, PNorm = 151.3380, GNorm = 0.2843
Meta loss on this task batch = 3.1269e-01, Meta loss averaged over last 500 steps = 2.8732e-01, PNorm = 151.3409, GNorm = 0.3458
Took 113.07119154930115 seconds to complete one epoch of meta training
Took 120.9552092552185 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458825
Epoch 688
Meta loss on this task batch = 2.7905e-01, Meta loss averaged over last 500 steps = 2.8735e-01, PNorm = 151.3446, GNorm = 0.5629
Meta loss on this task batch = 2.4298e-01, Meta loss averaged over last 500 steps = 2.8727e-01, PNorm = 151.3482, GNorm = 0.2510
Meta loss on this task batch = 3.1025e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 151.3516, GNorm = 0.2741
Meta loss on this task batch = 2.7015e-01, Meta loss averaged over last 500 steps = 2.8727e-01, PNorm = 151.3543, GNorm = 0.2959
Meta loss on this task batch = 2.4923e-01, Meta loss averaged over last 500 steps = 2.8719e-01, PNorm = 151.3564, GNorm = 0.2493
Meta loss on this task batch = 3.3475e-01, Meta loss averaged over last 500 steps = 2.8718e-01, PNorm = 151.3581, GNorm = 0.5269
Meta loss on this task batch = 3.2250e-01, Meta loss averaged over last 500 steps = 2.8727e-01, PNorm = 151.3597, GNorm = 0.3656
Meta loss on this task batch = 3.1222e-01, Meta loss averaged over last 500 steps = 2.8735e-01, PNorm = 151.3617, GNorm = 0.3562
Meta loss on this task batch = 2.5509e-01, Meta loss averaged over last 500 steps = 2.8724e-01, PNorm = 151.3640, GNorm = 0.3019
Meta loss on this task batch = 2.6172e-01, Meta loss averaged over last 500 steps = 2.8717e-01, PNorm = 151.3665, GNorm = 0.3640
Meta loss on this task batch = 3.1350e-01, Meta loss averaged over last 500 steps = 2.8724e-01, PNorm = 151.3688, GNorm = 0.2855
Meta loss on this task batch = 2.7666e-01, Meta loss averaged over last 500 steps = 2.8723e-01, PNorm = 151.3715, GNorm = 0.3147
Meta loss on this task batch = 3.0396e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 151.3750, GNorm = 0.3622
Meta loss on this task batch = 3.3102e-01, Meta loss averaged over last 500 steps = 2.8731e-01, PNorm = 151.3786, GNorm = 0.3524
Meta loss on this task batch = 2.8987e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 151.3830, GNorm = 0.3219
Meta loss on this task batch = 2.9744e-01, Meta loss averaged over last 500 steps = 2.8730e-01, PNorm = 151.3886, GNorm = 0.5336
Meta loss on this task batch = 3.4864e-01, Meta loss averaged over last 500 steps = 2.8744e-01, PNorm = 151.3948, GNorm = 0.3873
Meta loss on this task batch = 2.3867e-01, Meta loss averaged over last 500 steps = 2.8728e-01, PNorm = 151.4018, GNorm = 0.2543
Meta loss on this task batch = 2.9832e-01, Meta loss averaged over last 500 steps = 2.8740e-01, PNorm = 151.4088, GNorm = 0.3026
Took 110.72131133079529 seconds to complete one epoch of meta training
Took 118.7543568611145 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444393
Epoch 689
Meta loss on this task batch = 2.8937e-01, Meta loss averaged over last 500 steps = 2.8746e-01, PNorm = 151.4158, GNorm = 0.3059
Meta loss on this task batch = 2.5262e-01, Meta loss averaged over last 500 steps = 2.8735e-01, PNorm = 151.4231, GNorm = 0.2778
Meta loss on this task batch = 2.1862e-01, Meta loss averaged over last 500 steps = 2.8728e-01, PNorm = 151.4309, GNorm = 0.2751
Meta loss on this task batch = 2.4778e-01, Meta loss averaged over last 500 steps = 2.8728e-01, PNorm = 151.4391, GNorm = 0.2446
Meta loss on this task batch = 3.2160e-01, Meta loss averaged over last 500 steps = 2.8722e-01, PNorm = 151.4469, GNorm = 0.2968
Meta loss on this task batch = 3.4023e-01, Meta loss averaged over last 500 steps = 2.8735e-01, PNorm = 151.4541, GNorm = 0.4317
Meta loss on this task batch = 2.5955e-01, Meta loss averaged over last 500 steps = 2.8718e-01, PNorm = 151.4609, GNorm = 0.2339
Meta loss on this task batch = 2.3279e-01, Meta loss averaged over last 500 steps = 2.8698e-01, PNorm = 151.4683, GNorm = 0.2543
Meta loss on this task batch = 3.3486e-01, Meta loss averaged over last 500 steps = 2.8706e-01, PNorm = 151.4751, GNorm = 0.2957
Meta loss on this task batch = 3.1967e-01, Meta loss averaged over last 500 steps = 2.8721e-01, PNorm = 151.4814, GNorm = 0.4317
Meta loss on this task batch = 3.0014e-01, Meta loss averaged over last 500 steps = 2.8722e-01, PNorm = 151.4872, GNorm = 0.3706
Meta loss on this task batch = 2.9959e-01, Meta loss averaged over last 500 steps = 2.8720e-01, PNorm = 151.4914, GNorm = 0.3236
Meta loss on this task batch = 2.1569e-01, Meta loss averaged over last 500 steps = 2.8706e-01, PNorm = 151.4954, GNorm = 0.3019
Meta loss on this task batch = 3.1630e-01, Meta loss averaged over last 500 steps = 2.8709e-01, PNorm = 151.4991, GNorm = 0.3131
Meta loss on this task batch = 3.2331e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 151.5026, GNorm = 0.2853
Meta loss on this task batch = 3.0754e-01, Meta loss averaged over last 500 steps = 2.8736e-01, PNorm = 151.5067, GNorm = 0.2684
Meta loss on this task batch = 2.3916e-01, Meta loss averaged over last 500 steps = 2.8725e-01, PNorm = 151.5105, GNorm = 0.2884
Meta loss on this task batch = 3.1302e-01, Meta loss averaged over last 500 steps = 2.8736e-01, PNorm = 151.5140, GNorm = 0.3279
Meta loss on this task batch = 3.0601e-01, Meta loss averaged over last 500 steps = 2.8743e-01, PNorm = 151.5181, GNorm = 0.3066
Took 112.03152298927307 seconds to complete one epoch of meta training
Took 119.74509382247925 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459338
Epoch 690
Meta loss on this task batch = 2.8326e-01, Meta loss averaged over last 500 steps = 2.8741e-01, PNorm = 151.5228, GNorm = 0.2356
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.8724e-01, PNorm = 151.5281, GNorm = 0.2908
Meta loss on this task batch = 2.9095e-01, Meta loss averaged over last 500 steps = 2.8731e-01, PNorm = 151.5332, GNorm = 0.2749
Meta loss on this task batch = 2.6642e-01, Meta loss averaged over last 500 steps = 2.8728e-01, PNorm = 151.5392, GNorm = 0.3201
Meta loss on this task batch = 3.4028e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 151.5446, GNorm = 0.3135
Meta loss on this task batch = 2.8568e-01, Meta loss averaged over last 500 steps = 2.8725e-01, PNorm = 151.5499, GNorm = 0.3978
Meta loss on this task batch = 3.2705e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 151.5549, GNorm = 0.3611
Meta loss on this task batch = 2.7782e-01, Meta loss averaged over last 500 steps = 2.8738e-01, PNorm = 151.5603, GNorm = 0.2723
Meta loss on this task batch = 2.8710e-01, Meta loss averaged over last 500 steps = 2.8739e-01, PNorm = 151.5649, GNorm = 0.3899
Meta loss on this task batch = 2.8215e-01, Meta loss averaged over last 500 steps = 2.8746e-01, PNorm = 151.5698, GNorm = 0.2733
Meta loss on this task batch = 3.0573e-01, Meta loss averaged over last 500 steps = 2.8746e-01, PNorm = 151.5751, GNorm = 0.2653
Meta loss on this task batch = 2.5752e-01, Meta loss averaged over last 500 steps = 2.8737e-01, PNorm = 151.5807, GNorm = 0.2710
Meta loss on this task batch = 2.9467e-01, Meta loss averaged over last 500 steps = 2.8740e-01, PNorm = 151.5858, GNorm = 0.3127
Meta loss on this task batch = 2.2789e-01, Meta loss averaged over last 500 steps = 2.8725e-01, PNorm = 151.5907, GNorm = 0.2327
Meta loss on this task batch = 3.1032e-01, Meta loss averaged over last 500 steps = 2.8738e-01, PNorm = 151.5955, GNorm = 0.2737
Meta loss on this task batch = 2.7172e-01, Meta loss averaged over last 500 steps = 2.8735e-01, PNorm = 151.6007, GNorm = 0.2953
Meta loss on this task batch = 2.8988e-01, Meta loss averaged over last 500 steps = 2.8730e-01, PNorm = 151.6057, GNorm = 0.2789
Meta loss on this task batch = 2.7694e-01, Meta loss averaged over last 500 steps = 2.8730e-01, PNorm = 151.6108, GNorm = 0.2860
Meta loss on this task batch = 2.6328e-01, Meta loss averaged over last 500 steps = 2.8726e-01, PNorm = 151.6156, GNorm = 0.4805
Took 112.01543641090393 seconds to complete one epoch of meta training
Took 120.08633327484131 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462771
Epoch 691
Meta loss on this task batch = 3.4007e-01, Meta loss averaged over last 500 steps = 2.8727e-01, PNorm = 151.6192, GNorm = 0.2786
Meta loss on this task batch = 2.7311e-01, Meta loss averaged over last 500 steps = 2.8719e-01, PNorm = 151.6233, GNorm = 0.2452
Meta loss on this task batch = 3.2107e-01, Meta loss averaged over last 500 steps = 2.8719e-01, PNorm = 151.6271, GNorm = 0.3103
Meta loss on this task batch = 2.7228e-01, Meta loss averaged over last 500 steps = 2.8725e-01, PNorm = 151.6313, GNorm = 0.2784
Meta loss on this task batch = 2.5869e-01, Meta loss averaged over last 500 steps = 2.8712e-01, PNorm = 151.6363, GNorm = 0.2682
Meta loss on this task batch = 2.5965e-01, Meta loss averaged over last 500 steps = 2.8708e-01, PNorm = 151.6415, GNorm = 0.2994
Meta loss on this task batch = 2.9582e-01, Meta loss averaged over last 500 steps = 2.8710e-01, PNorm = 151.6463, GNorm = 0.2766
Meta loss on this task batch = 2.6630e-01, Meta loss averaged over last 500 steps = 2.8696e-01, PNorm = 151.6502, GNorm = 0.2774
Meta loss on this task batch = 2.7136e-01, Meta loss averaged over last 500 steps = 2.8688e-01, PNorm = 151.6540, GNorm = 0.2976
Meta loss on this task batch = 3.1123e-01, Meta loss averaged over last 500 steps = 2.8700e-01, PNorm = 151.6570, GNorm = 0.3111
Meta loss on this task batch = 3.2472e-01, Meta loss averaged over last 500 steps = 2.8707e-01, PNorm = 151.6597, GNorm = 0.3000
Meta loss on this task batch = 2.1897e-01, Meta loss averaged over last 500 steps = 2.8691e-01, PNorm = 151.6626, GNorm = 0.2687
Meta loss on this task batch = 3.4045e-01, Meta loss averaged over last 500 steps = 2.8711e-01, PNorm = 151.6653, GNorm = 0.3848
Meta loss on this task batch = 3.2682e-01, Meta loss averaged over last 500 steps = 2.8707e-01, PNorm = 151.6675, GNorm = 0.3454
Meta loss on this task batch = 2.7630e-01, Meta loss averaged over last 500 steps = 2.8706e-01, PNorm = 151.6701, GNorm = 0.2916
Meta loss on this task batch = 2.1808e-01, Meta loss averaged over last 500 steps = 2.8687e-01, PNorm = 151.6727, GNorm = 0.3046
Meta loss on this task batch = 2.4634e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 151.6751, GNorm = 0.2513
Meta loss on this task batch = 2.8831e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 151.6774, GNorm = 0.3188
Meta loss on this task batch = 2.8183e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 151.6796, GNorm = 0.3022
Took 112.34330534934998 seconds to complete one epoch of meta training
Took 120.05561542510986 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461155
Epoch 692
Meta loss on this task batch = 3.1825e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 151.6823, GNorm = 0.2930
Meta loss on this task batch = 2.7203e-01, Meta loss averaged over last 500 steps = 2.8682e-01, PNorm = 151.6849, GNorm = 0.3703
Meta loss on this task batch = 2.5439e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 151.6881, GNorm = 0.2579
Meta loss on this task batch = 2.4384e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 151.6913, GNorm = 0.2641
Meta loss on this task batch = 2.6857e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 151.6944, GNorm = 0.4292
Meta loss on this task batch = 2.5802e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 151.6984, GNorm = 0.3480
Meta loss on this task batch = 2.6250e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 151.7024, GNorm = 0.2384
Meta loss on this task batch = 2.7127e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 151.7068, GNorm = 0.2517
Meta loss on this task batch = 2.8119e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 151.7113, GNorm = 0.2633
Meta loss on this task batch = 2.8228e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 151.7157, GNorm = 0.3549
Meta loss on this task batch = 2.9347e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 151.7201, GNorm = 0.3242
Meta loss on this task batch = 2.8219e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 151.7248, GNorm = 0.2804
Meta loss on this task batch = 3.1676e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 151.7292, GNorm = 0.3064
Meta loss on this task batch = 2.2909e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 151.7339, GNorm = 0.2959
Meta loss on this task batch = 3.1877e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 151.7383, GNorm = 0.2961
Meta loss on this task batch = 3.1207e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 151.7416, GNorm = 0.2514
Meta loss on this task batch = 3.4890e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 151.7446, GNorm = 0.6081
Meta loss on this task batch = 2.5181e-01, Meta loss averaged over last 500 steps = 2.8643e-01, PNorm = 151.7479, GNorm = 0.2512
Meta loss on this task batch = 2.9158e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 151.7512, GNorm = 0.5573
Took 112.59055590629578 seconds to complete one epoch of meta training
Took 120.29539775848389 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463101
Epoch 693
Meta loss on this task batch = 2.9324e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 151.7545, GNorm = 0.3233
Meta loss on this task batch = 3.1202e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 151.7577, GNorm = 0.3801
Meta loss on this task batch = 3.0521e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 151.7609, GNorm = 0.2701
Meta loss on this task batch = 2.0765e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 151.7647, GNorm = 0.1919
Meta loss on this task batch = 2.7695e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 151.7684, GNorm = 0.3459
Meta loss on this task batch = 3.0742e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 151.7727, GNorm = 0.3512
Meta loss on this task batch = 2.7802e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 151.7771, GNorm = 0.2671
Meta loss on this task batch = 2.8397e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 151.7820, GNorm = 0.2638
Meta loss on this task batch = 3.1342e-01, Meta loss averaged over last 500 steps = 2.8641e-01, PNorm = 151.7861, GNorm = 0.3585
Meta loss on this task batch = 2.5275e-01, Meta loss averaged over last 500 steps = 2.8640e-01, PNorm = 151.7902, GNorm = 0.3579
Meta loss on this task batch = 2.6552e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 151.7935, GNorm = 0.3172
Meta loss on this task batch = 2.8029e-01, Meta loss averaged over last 500 steps = 2.8640e-01, PNorm = 151.7971, GNorm = 0.2737
Meta loss on this task batch = 2.9722e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 151.8008, GNorm = 0.3437
Meta loss on this task batch = 2.6456e-01, Meta loss averaged over last 500 steps = 2.8629e-01, PNorm = 151.8051, GNorm = 0.2648
Meta loss on this task batch = 2.3176e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 151.8096, GNorm = 0.2822
Meta loss on this task batch = 3.1967e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 151.8138, GNorm = 0.3116
Meta loss on this task batch = 3.0636e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 151.8177, GNorm = 0.3568
Meta loss on this task batch = 2.2656e-01, Meta loss averaged over last 500 steps = 2.8616e-01, PNorm = 151.8219, GNorm = 0.2530
Meta loss on this task batch = 2.5286e-01, Meta loss averaged over last 500 steps = 2.8608e-01, PNorm = 151.8262, GNorm = 0.2991
Took 110.77457880973816 seconds to complete one epoch of meta training
Took 118.68140435218811 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456531
Epoch 694
Meta loss on this task batch = 2.6535e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 151.8305, GNorm = 0.2811
Meta loss on this task batch = 3.4263e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 151.8352, GNorm = 0.3345
Meta loss on this task batch = 2.3669e-01, Meta loss averaged over last 500 steps = 2.8604e-01, PNorm = 151.8398, GNorm = 0.2523
Meta loss on this task batch = 2.7578e-01, Meta loss averaged over last 500 steps = 2.8601e-01, PNorm = 151.8443, GNorm = 0.3847
Meta loss on this task batch = 3.3642e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 151.8484, GNorm = 0.3740
Meta loss on this task batch = 2.5967e-01, Meta loss averaged over last 500 steps = 2.8613e-01, PNorm = 151.8520, GNorm = 0.2824
Meta loss on this task batch = 3.5065e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 151.8562, GNorm = 0.3436
Meta loss on this task batch = 2.4434e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 151.8604, GNorm = 0.2476
Meta loss on this task batch = 2.3616e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 151.8650, GNorm = 0.2831
Meta loss on this task batch = 2.7504e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 151.8697, GNorm = 0.2893
Meta loss on this task batch = 2.4581e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 151.8746, GNorm = 0.2882
Meta loss on this task batch = 3.4037e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 151.8796, GNorm = 0.3090
Meta loss on this task batch = 2.3951e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 151.8848, GNorm = 0.3120
Meta loss on this task batch = 3.4602e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 151.8884, GNorm = 0.3594
Meta loss on this task batch = 2.5261e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 151.8915, GNorm = 0.3051
Meta loss on this task batch = 3.0488e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 151.8942, GNorm = 0.4586
Meta loss on this task batch = 2.7369e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 151.8970, GNorm = 0.4610
Meta loss on this task batch = 2.8884e-01, Meta loss averaged over last 500 steps = 2.8611e-01, PNorm = 151.8997, GNorm = 0.2847
Meta loss on this task batch = 2.4674e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 151.9030, GNorm = 0.2720
Took 112.13833904266357 seconds to complete one epoch of meta training
Took 119.78387975692749 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460841
Epoch 695
Meta loss on this task batch = 3.1031e-01, Meta loss averaged over last 500 steps = 2.8613e-01, PNorm = 151.9068, GNorm = 0.2601
Meta loss on this task batch = 2.4932e-01, Meta loss averaged over last 500 steps = 2.8604e-01, PNorm = 151.9106, GNorm = 0.2448
Meta loss on this task batch = 2.8304e-01, Meta loss averaged over last 500 steps = 2.8603e-01, PNorm = 151.9145, GNorm = 0.3245
Meta loss on this task batch = 3.0235e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 151.9182, GNorm = 0.3901
Meta loss on this task batch = 3.3493e-01, Meta loss averaged over last 500 steps = 2.8621e-01, PNorm = 151.9219, GNorm = 0.2958
Meta loss on this task batch = 3.0196e-01, Meta loss averaged over last 500 steps = 2.8628e-01, PNorm = 151.9256, GNorm = 0.3079
Meta loss on this task batch = 2.3945e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 151.9301, GNorm = 0.2974
Meta loss on this task batch = 2.6461e-01, Meta loss averaged over last 500 steps = 2.8602e-01, PNorm = 151.9350, GNorm = 0.2645
Meta loss on this task batch = 3.0752e-01, Meta loss averaged over last 500 steps = 2.8608e-01, PNorm = 151.9395, GNorm = 0.2609
Meta loss on this task batch = 2.8647e-01, Meta loss averaged over last 500 steps = 2.8600e-01, PNorm = 151.9438, GNorm = 0.3057
Meta loss on this task batch = 2.5363e-01, Meta loss averaged over last 500 steps = 2.8594e-01, PNorm = 151.9488, GNorm = 0.3234
Meta loss on this task batch = 3.3880e-01, Meta loss averaged over last 500 steps = 2.8608e-01, PNorm = 151.9537, GNorm = 0.3441
Meta loss on this task batch = 2.7450e-01, Meta loss averaged over last 500 steps = 2.8603e-01, PNorm = 151.9584, GNorm = 0.2414
Meta loss on this task batch = 3.1299e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 151.9633, GNorm = 0.2610
Meta loss on this task batch = 3.0447e-01, Meta loss averaged over last 500 steps = 2.8618e-01, PNorm = 151.9677, GNorm = 0.4070
Meta loss on this task batch = 2.5056e-01, Meta loss averaged over last 500 steps = 2.8616e-01, PNorm = 151.9725, GNorm = 0.2349
Meta loss on this task batch = 3.3428e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 151.9769, GNorm = 0.2737
Meta loss on this task batch = 2.3523e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 151.9807, GNorm = 0.4188
Meta loss on this task batch = 2.8247e-01, Meta loss averaged over last 500 steps = 2.8618e-01, PNorm = 151.9834, GNorm = 0.2907
Took 111.38469910621643 seconds to complete one epoch of meta training
Took 118.58103632926941 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463212
Epoch 696
Meta loss on this task batch = 2.9560e-01, Meta loss averaged over last 500 steps = 2.8618e-01, PNorm = 151.9857, GNorm = 0.2899
Meta loss on this task batch = 3.3435e-01, Meta loss averaged over last 500 steps = 2.8629e-01, PNorm = 151.9872, GNorm = 0.4056
Meta loss on this task batch = 3.0965e-01, Meta loss averaged over last 500 steps = 2.8624e-01, PNorm = 151.9895, GNorm = 0.3107
Meta loss on this task batch = 2.7896e-01, Meta loss averaged over last 500 steps = 2.8624e-01, PNorm = 151.9920, GNorm = 0.3361
Meta loss on this task batch = 2.8548e-01, Meta loss averaged over last 500 steps = 2.8618e-01, PNorm = 151.9951, GNorm = 0.3843
Meta loss on this task batch = 3.0462e-01, Meta loss averaged over last 500 steps = 2.8625e-01, PNorm = 151.9983, GNorm = 0.3814
Meta loss on this task batch = 2.3401e-01, Meta loss averaged over last 500 steps = 2.8616e-01, PNorm = 152.0020, GNorm = 0.3554
Meta loss on this task batch = 2.3210e-01, Meta loss averaged over last 500 steps = 2.8608e-01, PNorm = 152.0059, GNorm = 0.2762
Meta loss on this task batch = 3.2557e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 152.0098, GNorm = 0.2660
Meta loss on this task batch = 2.6296e-01, Meta loss averaged over last 500 steps = 2.8621e-01, PNorm = 152.0140, GNorm = 0.3115
Meta loss on this task batch = 3.1050e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 152.0183, GNorm = 0.4296
Meta loss on this task batch = 2.7697e-01, Meta loss averaged over last 500 steps = 2.8621e-01, PNorm = 152.0224, GNorm = 0.2737
Meta loss on this task batch = 2.6751e-01, Meta loss averaged over last 500 steps = 2.8602e-01, PNorm = 152.0268, GNorm = 0.3418
Meta loss on this task batch = 3.0937e-01, Meta loss averaged over last 500 steps = 2.8613e-01, PNorm = 152.0314, GNorm = 0.2512
Meta loss on this task batch = 2.0813e-01, Meta loss averaged over last 500 steps = 2.8596e-01, PNorm = 152.0369, GNorm = 0.2807
Meta loss on this task batch = 2.9382e-01, Meta loss averaged over last 500 steps = 2.8584e-01, PNorm = 152.0423, GNorm = 0.3042
Meta loss on this task batch = 2.9678e-01, Meta loss averaged over last 500 steps = 2.8588e-01, PNorm = 152.0479, GNorm = 0.2613
Meta loss on this task batch = 2.3831e-01, Meta loss averaged over last 500 steps = 2.8579e-01, PNorm = 152.0535, GNorm = 0.3052
Meta loss on this task batch = 2.4111e-01, Meta loss averaged over last 500 steps = 2.8559e-01, PNorm = 152.0589, GNorm = 0.3998
Took 112.35625433921814 seconds to complete one epoch of meta training
Took 120.39713335037231 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483628
Epoch 697
Meta loss on this task batch = 3.0067e-01, Meta loss averaged over last 500 steps = 2.8576e-01, PNorm = 152.0643, GNorm = 0.2900
Meta loss on this task batch = 2.8990e-01, Meta loss averaged over last 500 steps = 2.8588e-01, PNorm = 152.0696, GNorm = 0.3360
Meta loss on this task batch = 2.7257e-01, Meta loss averaged over last 500 steps = 2.8573e-01, PNorm = 152.0753, GNorm = 0.2717
Meta loss on this task batch = 2.2331e-01, Meta loss averaged over last 500 steps = 2.8563e-01, PNorm = 152.0810, GNorm = 0.2614
Meta loss on this task batch = 2.8505e-01, Meta loss averaged over last 500 steps = 2.8548e-01, PNorm = 152.0867, GNorm = 0.4253
Meta loss on this task batch = 2.8555e-01, Meta loss averaged over last 500 steps = 2.8543e-01, PNorm = 152.0919, GNorm = 0.2696
Meta loss on this task batch = 2.7861e-01, Meta loss averaged over last 500 steps = 2.8546e-01, PNorm = 152.0966, GNorm = 0.3325
Meta loss on this task batch = 2.6233e-01, Meta loss averaged over last 500 steps = 2.8544e-01, PNorm = 152.1013, GNorm = 0.2501
Meta loss on this task batch = 1.9732e-01, Meta loss averaged over last 500 steps = 2.8521e-01, PNorm = 152.1061, GNorm = 0.2423
Meta loss on this task batch = 3.1720e-01, Meta loss averaged over last 500 steps = 2.8517e-01, PNorm = 152.1106, GNorm = 0.3651
Meta loss on this task batch = 2.6495e-01, Meta loss averaged over last 500 steps = 2.8511e-01, PNorm = 152.1156, GNorm = 0.2886
Meta loss on this task batch = 3.4471e-01, Meta loss averaged over last 500 steps = 2.8529e-01, PNorm = 152.1201, GNorm = 0.2932
Meta loss on this task batch = 3.1793e-01, Meta loss averaged over last 500 steps = 2.8540e-01, PNorm = 152.1241, GNorm = 0.2768
Meta loss on this task batch = 2.4985e-01, Meta loss averaged over last 500 steps = 2.8538e-01, PNorm = 152.1283, GNorm = 0.2931
Meta loss on this task batch = 2.9165e-01, Meta loss averaged over last 500 steps = 2.8545e-01, PNorm = 152.1326, GNorm = 0.2828
Meta loss on this task batch = 2.4722e-01, Meta loss averaged over last 500 steps = 2.8544e-01, PNorm = 152.1368, GNorm = 0.2588
Meta loss on this task batch = 3.3059e-01, Meta loss averaged over last 500 steps = 2.8561e-01, PNorm = 152.1405, GNorm = 0.3008
Meta loss on this task batch = 2.9104e-01, Meta loss averaged over last 500 steps = 2.8566e-01, PNorm = 152.1446, GNorm = 0.3252
Meta loss on this task batch = 3.1197e-01, Meta loss averaged over last 500 steps = 2.8570e-01, PNorm = 152.1488, GNorm = 0.3881
Took 111.13752126693726 seconds to complete one epoch of meta training
Took 118.7599024772644 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467953
Epoch 698
Meta loss on this task batch = 2.6006e-01, Meta loss averaged over last 500 steps = 2.8562e-01, PNorm = 152.1529, GNorm = 0.2660
Meta loss on this task batch = 2.7645e-01, Meta loss averaged over last 500 steps = 2.8547e-01, PNorm = 152.1568, GNorm = 0.2669
Meta loss on this task batch = 2.9387e-01, Meta loss averaged over last 500 steps = 2.8547e-01, PNorm = 152.1605, GNorm = 0.2961
Meta loss on this task batch = 3.2868e-01, Meta loss averaged over last 500 steps = 2.8557e-01, PNorm = 152.1641, GNorm = 0.3001
Meta loss on this task batch = 3.3953e-01, Meta loss averaged over last 500 steps = 2.8557e-01, PNorm = 152.1671, GNorm = 0.3288
Meta loss on this task batch = 2.9561e-01, Meta loss averaged over last 500 steps = 2.8555e-01, PNorm = 152.1703, GNorm = 0.4459
Meta loss on this task batch = 2.3642e-01, Meta loss averaged over last 500 steps = 2.8553e-01, PNorm = 152.1739, GNorm = 0.3351
Meta loss on this task batch = 3.2042e-01, Meta loss averaged over last 500 steps = 2.8563e-01, PNorm = 152.1769, GNorm = 0.3518
Meta loss on this task batch = 3.2209e-01, Meta loss averaged over last 500 steps = 2.8573e-01, PNorm = 152.1799, GNorm = 0.3957
Meta loss on this task batch = 3.2373e-01, Meta loss averaged over last 500 steps = 2.8583e-01, PNorm = 152.1829, GNorm = 0.2680
Meta loss on this task batch = 2.7217e-01, Meta loss averaged over last 500 steps = 2.8583e-01, PNorm = 152.1860, GNorm = 0.2613
Meta loss on this task batch = 3.1383e-01, Meta loss averaged over last 500 steps = 2.8584e-01, PNorm = 152.1893, GNorm = 0.2873
Meta loss on this task batch = 2.7397e-01, Meta loss averaged over last 500 steps = 2.8585e-01, PNorm = 152.1926, GNorm = 0.3286
Meta loss on this task batch = 2.9496e-01, Meta loss averaged over last 500 steps = 2.8592e-01, PNorm = 152.1966, GNorm = 0.3028
Meta loss on this task batch = 2.7262e-01, Meta loss averaged over last 500 steps = 2.8595e-01, PNorm = 152.2011, GNorm = 0.4753
Meta loss on this task batch = 1.8736e-01, Meta loss averaged over last 500 steps = 2.8578e-01, PNorm = 152.2061, GNorm = 0.2214
Meta loss on this task batch = 2.8628e-01, Meta loss averaged over last 500 steps = 2.8577e-01, PNorm = 152.2109, GNorm = 0.3615
Meta loss on this task batch = 2.5146e-01, Meta loss averaged over last 500 steps = 2.8568e-01, PNorm = 152.2154, GNorm = 0.2661
Meta loss on this task batch = 2.6384e-01, Meta loss averaged over last 500 steps = 2.8547e-01, PNorm = 152.2197, GNorm = 0.3923
Took 112.62457799911499 seconds to complete one epoch of meta training
Took 120.77515721321106 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467828
Epoch 699
Meta loss on this task batch = 2.7393e-01, Meta loss averaged over last 500 steps = 2.8542e-01, PNorm = 152.2244, GNorm = 0.2888
Meta loss on this task batch = 2.2214e-01, Meta loss averaged over last 500 steps = 2.8541e-01, PNorm = 152.2298, GNorm = 0.2450
Meta loss on this task batch = 2.5431e-01, Meta loss averaged over last 500 steps = 2.8528e-01, PNorm = 152.2353, GNorm = 0.4149
Meta loss on this task batch = 3.2600e-01, Meta loss averaged over last 500 steps = 2.8532e-01, PNorm = 152.2404, GNorm = 0.3413
Meta loss on this task batch = 2.5621e-01, Meta loss averaged over last 500 steps = 2.8527e-01, PNorm = 152.2452, GNorm = 0.3184
Meta loss on this task batch = 2.6930e-01, Meta loss averaged over last 500 steps = 2.8524e-01, PNorm = 152.2495, GNorm = 0.4933
Meta loss on this task batch = 3.1376e-01, Meta loss averaged over last 500 steps = 2.8525e-01, PNorm = 152.2531, GNorm = 0.3729
Meta loss on this task batch = 2.9543e-01, Meta loss averaged over last 500 steps = 2.8538e-01, PNorm = 152.2565, GNorm = 0.2852
Meta loss on this task batch = 3.0571e-01, Meta loss averaged over last 500 steps = 2.8549e-01, PNorm = 152.2603, GNorm = 0.3219
Meta loss on this task batch = 2.9114e-01, Meta loss averaged over last 500 steps = 2.8541e-01, PNorm = 152.2638, GNorm = 0.2917
Meta loss on this task batch = 2.7277e-01, Meta loss averaged over last 500 steps = 2.8539e-01, PNorm = 152.2673, GNorm = 0.2885
Meta loss on this task batch = 2.6927e-01, Meta loss averaged over last 500 steps = 2.8535e-01, PNorm = 152.2710, GNorm = 0.2994
Meta loss on this task batch = 2.3318e-01, Meta loss averaged over last 500 steps = 2.8522e-01, PNorm = 152.2747, GNorm = 0.2513
Meta loss on this task batch = 3.6162e-01, Meta loss averaged over last 500 steps = 2.8545e-01, PNorm = 152.2780, GNorm = 0.3929
Meta loss on this task batch = 2.6924e-01, Meta loss averaged over last 500 steps = 2.8520e-01, PNorm = 152.2807, GNorm = 0.2608
Meta loss on this task batch = 2.7451e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 152.2844, GNorm = 0.3224
Meta loss on this task batch = 2.9478e-01, Meta loss averaged over last 500 steps = 2.8519e-01, PNorm = 152.2883, GNorm = 0.3485
Meta loss on this task batch = 2.7903e-01, Meta loss averaged over last 500 steps = 2.8518e-01, PNorm = 152.2923, GNorm = 0.4509
Meta loss on this task batch = 3.1713e-01, Meta loss averaged over last 500 steps = 2.8520e-01, PNorm = 152.2969, GNorm = 0.3738
Took 113.5691466331482 seconds to complete one epoch of meta training
Took 121.05262160301208 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450999
Epoch 700
Meta loss on this task batch = 2.5938e-01, Meta loss averaged over last 500 steps = 2.8515e-01, PNorm = 152.3032, GNorm = 0.3543
Meta loss on this task batch = 2.4450e-01, Meta loss averaged over last 500 steps = 2.8504e-01, PNorm = 152.3100, GNorm = 0.3333
Meta loss on this task batch = 3.0240e-01, Meta loss averaged over last 500 steps = 2.8510e-01, PNorm = 152.3166, GNorm = 0.2579
Meta loss on this task batch = 2.6576e-01, Meta loss averaged over last 500 steps = 2.8514e-01, PNorm = 152.3236, GNorm = 0.2759
Meta loss on this task batch = 2.5323e-01, Meta loss averaged over last 500 steps = 2.8512e-01, PNorm = 152.3308, GNorm = 0.2875
Meta loss on this task batch = 3.3515e-01, Meta loss averaged over last 500 steps = 2.8511e-01, PNorm = 152.3372, GNorm = 0.3636
Meta loss on this task batch = 2.2765e-01, Meta loss averaged over last 500 steps = 2.8503e-01, PNorm = 152.3435, GNorm = 0.3166
Meta loss on this task batch = 3.6125e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 152.3498, GNorm = 0.3789
Meta loss on this task batch = 2.9410e-01, Meta loss averaged over last 500 steps = 2.8511e-01, PNorm = 152.3561, GNorm = 0.2949
Meta loss on this task batch = 3.0921e-01, Meta loss averaged over last 500 steps = 2.8509e-01, PNorm = 152.3620, GNorm = 0.2415
Meta loss on this task batch = 2.6996e-01, Meta loss averaged over last 500 steps = 2.8510e-01, PNorm = 152.3675, GNorm = 0.2615
Meta loss on this task batch = 2.7723e-01, Meta loss averaged over last 500 steps = 2.8509e-01, PNorm = 152.3727, GNorm = 0.4450
Meta loss on this task batch = 3.0814e-01, Meta loss averaged over last 500 steps = 2.8515e-01, PNorm = 152.3778, GNorm = 0.2355
Meta loss on this task batch = 2.7028e-01, Meta loss averaged over last 500 steps = 2.8506e-01, PNorm = 152.3826, GNorm = 0.3060
Meta loss on this task batch = 3.0944e-01, Meta loss averaged over last 500 steps = 2.8507e-01, PNorm = 152.3864, GNorm = 0.3995
Meta loss on this task batch = 3.0846e-01, Meta loss averaged over last 500 steps = 2.8509e-01, PNorm = 152.3896, GNorm = 0.3034
Meta loss on this task batch = 2.3804e-01, Meta loss averaged over last 500 steps = 2.8502e-01, PNorm = 152.3923, GNorm = 0.3805
Meta loss on this task batch = 3.0963e-01, Meta loss averaged over last 500 steps = 2.8514e-01, PNorm = 152.3960, GNorm = 0.2712
Meta loss on this task batch = 3.2897e-01, Meta loss averaged over last 500 steps = 2.8522e-01, PNorm = 152.3994, GNorm = 0.3684
Took 112.63336563110352 seconds to complete one epoch of meta training
Took 120.54741835594177 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454004
Epoch 701
Meta loss on this task batch = 3.0447e-01, Meta loss averaged over last 500 steps = 2.8530e-01, PNorm = 152.4038, GNorm = 0.2317
Meta loss on this task batch = 3.4685e-01, Meta loss averaged over last 500 steps = 2.8533e-01, PNorm = 152.4077, GNorm = 0.3002
Meta loss on this task batch = 3.2051e-01, Meta loss averaged over last 500 steps = 2.8548e-01, PNorm = 152.4124, GNorm = 0.4160
Meta loss on this task batch = 2.8781e-01, Meta loss averaged over last 500 steps = 2.8554e-01, PNorm = 152.4176, GNorm = 0.3164
Meta loss on this task batch = 2.7510e-01, Meta loss averaged over last 500 steps = 2.8549e-01, PNorm = 152.4223, GNorm = 0.2723
Meta loss on this task batch = 2.5641e-01, Meta loss averaged over last 500 steps = 2.8550e-01, PNorm = 152.4277, GNorm = 0.2611
Meta loss on this task batch = 2.7889e-01, Meta loss averaged over last 500 steps = 2.8542e-01, PNorm = 152.4337, GNorm = 0.2510
Meta loss on this task batch = 2.3839e-01, Meta loss averaged over last 500 steps = 2.8542e-01, PNorm = 152.4404, GNorm = 0.2875
Meta loss on this task batch = 2.8455e-01, Meta loss averaged over last 500 steps = 2.8555e-01, PNorm = 152.4471, GNorm = 0.3195
Meta loss on this task batch = 2.9002e-01, Meta loss averaged over last 500 steps = 2.8560e-01, PNorm = 152.4535, GNorm = 0.3406
Meta loss on this task batch = 2.3438e-01, Meta loss averaged over last 500 steps = 2.8551e-01, PNorm = 152.4596, GNorm = 0.2822
Meta loss on this task batch = 2.8341e-01, Meta loss averaged over last 500 steps = 2.8539e-01, PNorm = 152.4646, GNorm = 0.3067
Meta loss on this task batch = 2.3967e-01, Meta loss averaged over last 500 steps = 2.8537e-01, PNorm = 152.4699, GNorm = 0.5324
Meta loss on this task batch = 2.8718e-01, Meta loss averaged over last 500 steps = 2.8532e-01, PNorm = 152.4747, GNorm = 0.2804
Meta loss on this task batch = 2.3392e-01, Meta loss averaged over last 500 steps = 2.8515e-01, PNorm = 152.4795, GNorm = 0.2380
Meta loss on this task batch = 2.6160e-01, Meta loss averaged over last 500 steps = 2.8507e-01, PNorm = 152.4842, GNorm = 0.2972
Meta loss on this task batch = 2.8062e-01, Meta loss averaged over last 500 steps = 2.8506e-01, PNorm = 152.4886, GNorm = 0.3366
Meta loss on this task batch = 2.6939e-01, Meta loss averaged over last 500 steps = 2.8501e-01, PNorm = 152.4931, GNorm = 0.2600
Meta loss on this task batch = 3.7746e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 152.4959, GNorm = 0.4956
Took 114.36303973197937 seconds to complete one epoch of meta training
Took 122.1608235836029 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.441143
Epoch 702
Meta loss on this task batch = 2.6842e-01, Meta loss averaged over last 500 steps = 2.8496e-01, PNorm = 152.4987, GNorm = 0.3138
Meta loss on this task batch = 3.2791e-01, Meta loss averaged over last 500 steps = 2.8511e-01, PNorm = 152.5011, GNorm = 0.4797
Meta loss on this task batch = 3.0703e-01, Meta loss averaged over last 500 steps = 2.8530e-01, PNorm = 152.5042, GNorm = 0.3060
Meta loss on this task batch = 2.8815e-01, Meta loss averaged over last 500 steps = 2.8539e-01, PNorm = 152.5079, GNorm = 0.2580
Meta loss on this task batch = 2.6981e-01, Meta loss averaged over last 500 steps = 2.8527e-01, PNorm = 152.5115, GNorm = 0.2704
Meta loss on this task batch = 2.5347e-01, Meta loss averaged over last 500 steps = 2.8515e-01, PNorm = 152.5160, GNorm = 0.2889
Meta loss on this task batch = 3.1046e-01, Meta loss averaged over last 500 steps = 2.8521e-01, PNorm = 152.5216, GNorm = 0.2973
Meta loss on this task batch = 2.8354e-01, Meta loss averaged over last 500 steps = 2.8517e-01, PNorm = 152.5274, GNorm = 0.2846
Meta loss on this task batch = 2.6426e-01, Meta loss averaged over last 500 steps = 2.8512e-01, PNorm = 152.5335, GNorm = 0.2846
Meta loss on this task batch = 2.8911e-01, Meta loss averaged over last 500 steps = 2.8510e-01, PNorm = 152.5400, GNorm = 0.2901
Meta loss on this task batch = 2.4948e-01, Meta loss averaged over last 500 steps = 2.8501e-01, PNorm = 152.5469, GNorm = 0.3345
Meta loss on this task batch = 3.3295e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 152.5534, GNorm = 0.2804
Meta loss on this task batch = 2.9560e-01, Meta loss averaged over last 500 steps = 2.8525e-01, PNorm = 152.5602, GNorm = 0.3916
Meta loss on this task batch = 3.3789e-01, Meta loss averaged over last 500 steps = 2.8529e-01, PNorm = 152.5663, GNorm = 0.3260
Meta loss on this task batch = 2.8532e-01, Meta loss averaged over last 500 steps = 2.8526e-01, PNorm = 152.5726, GNorm = 0.2594
Meta loss on this task batch = 2.0908e-01, Meta loss averaged over last 500 steps = 2.8506e-01, PNorm = 152.5793, GNorm = 0.2673
Meta loss on this task batch = 2.6779e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 152.5857, GNorm = 0.2786
Meta loss on this task batch = 3.3327e-01, Meta loss averaged over last 500 steps = 2.8524e-01, PNorm = 152.5918, GNorm = 0.3035
Meta loss on this task batch = 2.7760e-01, Meta loss averaged over last 500 steps = 2.8518e-01, PNorm = 152.5980, GNorm = 0.3818
Took 110.52346515655518 seconds to complete one epoch of meta training
Took 118.30889296531677 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460734
Epoch 703
Meta loss on this task batch = 3.1857e-01, Meta loss averaged over last 500 steps = 2.8521e-01, PNorm = 152.6040, GNorm = 0.3450
Meta loss on this task batch = 2.6565e-01, Meta loss averaged over last 500 steps = 2.8509e-01, PNorm = 152.6096, GNorm = 0.2668
Meta loss on this task batch = 3.2442e-01, Meta loss averaged over last 500 steps = 2.8519e-01, PNorm = 152.6148, GNorm = 0.4432
Meta loss on this task batch = 2.6898e-01, Meta loss averaged over last 500 steps = 2.8519e-01, PNorm = 152.6189, GNorm = 0.3934
Meta loss on this task batch = 2.5251e-01, Meta loss averaged over last 500 steps = 2.8508e-01, PNorm = 152.6230, GNorm = 0.2298
Meta loss on this task batch = 2.6640e-01, Meta loss averaged over last 500 steps = 2.8511e-01, PNorm = 152.6265, GNorm = 0.3265
Meta loss on this task batch = 2.8680e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 152.6293, GNorm = 0.2882
Meta loss on this task batch = 2.6009e-01, Meta loss averaged over last 500 steps = 2.8507e-01, PNorm = 152.6323, GNorm = 0.3376
Meta loss on this task batch = 2.5918e-01, Meta loss averaged over last 500 steps = 2.8505e-01, PNorm = 152.6355, GNorm = 0.2757
Meta loss on this task batch = 2.5391e-01, Meta loss averaged over last 500 steps = 2.8507e-01, PNorm = 152.6391, GNorm = 0.2550
Meta loss on this task batch = 2.5554e-01, Meta loss averaged over last 500 steps = 2.8497e-01, PNorm = 152.6430, GNorm = 0.2698
Meta loss on this task batch = 3.2338e-01, Meta loss averaged over last 500 steps = 2.8496e-01, PNorm = 152.6464, GNorm = 0.4084
Meta loss on this task batch = 2.9028e-01, Meta loss averaged over last 500 steps = 2.8495e-01, PNorm = 152.6497, GNorm = 0.4379
Meta loss on this task batch = 2.7511e-01, Meta loss averaged over last 500 steps = 2.8489e-01, PNorm = 152.6534, GNorm = 0.2748
Meta loss on this task batch = 2.3767e-01, Meta loss averaged over last 500 steps = 2.8475e-01, PNorm = 152.6574, GNorm = 0.3278
Meta loss on this task batch = 3.2869e-01, Meta loss averaged over last 500 steps = 2.8475e-01, PNorm = 152.6613, GNorm = 0.3885
Meta loss on this task batch = 2.5848e-01, Meta loss averaged over last 500 steps = 2.8466e-01, PNorm = 152.6647, GNorm = 0.2761
Meta loss on this task batch = 3.7303e-01, Meta loss averaged over last 500 steps = 2.8489e-01, PNorm = 152.6673, GNorm = 0.3443
Meta loss on this task batch = 2.4160e-01, Meta loss averaged over last 500 steps = 2.8480e-01, PNorm = 152.6698, GNorm = 0.6187
Took 113.71408247947693 seconds to complete one epoch of meta training
Took 121.68174457550049 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466057
Epoch 704
Meta loss on this task batch = 2.4687e-01, Meta loss averaged over last 500 steps = 2.8481e-01, PNorm = 152.6731, GNorm = 0.2453
Meta loss on this task batch = 2.9032e-01, Meta loss averaged over last 500 steps = 2.8477e-01, PNorm = 152.6760, GNorm = 0.4064
Meta loss on this task batch = 2.7279e-01, Meta loss averaged over last 500 steps = 2.8477e-01, PNorm = 152.6789, GNorm = 0.2577
Meta loss on this task batch = 2.9812e-01, Meta loss averaged over last 500 steps = 2.8485e-01, PNorm = 152.6820, GNorm = 0.2527
Meta loss on this task batch = 2.3091e-01, Meta loss averaged over last 500 steps = 2.8473e-01, PNorm = 152.6858, GNorm = 0.2831
Meta loss on this task batch = 2.8188e-01, Meta loss averaged over last 500 steps = 2.8468e-01, PNorm = 152.6892, GNorm = 0.2744
Meta loss on this task batch = 2.9243e-01, Meta loss averaged over last 500 steps = 2.8467e-01, PNorm = 152.6933, GNorm = 0.3534
Meta loss on this task batch = 2.6867e-01, Meta loss averaged over last 500 steps = 2.8467e-01, PNorm = 152.6981, GNorm = 0.3129
Meta loss on this task batch = 3.0935e-01, Meta loss averaged over last 500 steps = 2.8464e-01, PNorm = 152.7027, GNorm = 0.2655
Meta loss on this task batch = 2.4296e-01, Meta loss averaged over last 500 steps = 2.8475e-01, PNorm = 152.7070, GNorm = 0.3491
Meta loss on this task batch = 3.2527e-01, Meta loss averaged over last 500 steps = 2.8495e-01, PNorm = 152.7109, GNorm = 0.2645
Meta loss on this task batch = 2.6693e-01, Meta loss averaged over last 500 steps = 2.8495e-01, PNorm = 152.7150, GNorm = 0.2938
Meta loss on this task batch = 3.1637e-01, Meta loss averaged over last 500 steps = 2.8503e-01, PNorm = 152.7181, GNorm = 0.3077
Meta loss on this task batch = 2.7175e-01, Meta loss averaged over last 500 steps = 2.8498e-01, PNorm = 152.7219, GNorm = 0.3045
Meta loss on this task batch = 2.4961e-01, Meta loss averaged over last 500 steps = 2.8493e-01, PNorm = 152.7261, GNorm = 0.2513
Meta loss on this task batch = 2.5478e-01, Meta loss averaged over last 500 steps = 2.8477e-01, PNorm = 152.7300, GNorm = 0.3043
Meta loss on this task batch = 3.2016e-01, Meta loss averaged over last 500 steps = 2.8485e-01, PNorm = 152.7336, GNorm = 0.3368
Meta loss on this task batch = 2.2512e-01, Meta loss averaged over last 500 steps = 2.8480e-01, PNorm = 152.7375, GNorm = 0.2970
Meta loss on this task batch = 2.8445e-01, Meta loss averaged over last 500 steps = 2.8475e-01, PNorm = 152.7419, GNorm = 0.4563
Took 114.42033624649048 seconds to complete one epoch of meta training
Took 122.48932266235352 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446272
Epoch 705
Meta loss on this task batch = 3.0183e-01, Meta loss averaged over last 500 steps = 2.8476e-01, PNorm = 152.7465, GNorm = 0.3599
Meta loss on this task batch = 2.7762e-01, Meta loss averaged over last 500 steps = 2.8470e-01, PNorm = 152.7508, GNorm = 0.2998
Meta loss on this task batch = 2.4495e-01, Meta loss averaged over last 500 steps = 2.8455e-01, PNorm = 152.7553, GNorm = 0.4414
Meta loss on this task batch = 3.0507e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 152.7605, GNorm = 0.4238
Meta loss on this task batch = 3.2116e-01, Meta loss averaged over last 500 steps = 2.8469e-01, PNorm = 152.7657, GNorm = 0.2620
Meta loss on this task batch = 2.6471e-01, Meta loss averaged over last 500 steps = 2.8455e-01, PNorm = 152.7713, GNorm = 0.2819
Meta loss on this task batch = 2.5655e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 152.7767, GNorm = 0.2791
Meta loss on this task batch = 2.8886e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 152.7816, GNorm = 0.4498
Meta loss on this task batch = 3.1777e-01, Meta loss averaged over last 500 steps = 2.8466e-01, PNorm = 152.7861, GNorm = 0.3784
Meta loss on this task batch = 2.9787e-01, Meta loss averaged over last 500 steps = 2.8465e-01, PNorm = 152.7904, GNorm = 0.2806
Meta loss on this task batch = 2.5210e-01, Meta loss averaged over last 500 steps = 2.8467e-01, PNorm = 152.7941, GNorm = 0.2688
Meta loss on this task batch = 2.7179e-01, Meta loss averaged over last 500 steps = 2.8465e-01, PNorm = 152.7977, GNorm = 0.2437
Meta loss on this task batch = 3.0410e-01, Meta loss averaged over last 500 steps = 2.8470e-01, PNorm = 152.8012, GNorm = 0.3665
Meta loss on this task batch = 3.2876e-01, Meta loss averaged over last 500 steps = 2.8480e-01, PNorm = 152.8044, GNorm = 0.3856
Meta loss on this task batch = 2.5767e-01, Meta loss averaged over last 500 steps = 2.8477e-01, PNorm = 152.8081, GNorm = 0.2691
Meta loss on this task batch = 3.2385e-01, Meta loss averaged over last 500 steps = 2.8490e-01, PNorm = 152.8119, GNorm = 0.3841
Meta loss on this task batch = 3.0623e-01, Meta loss averaged over last 500 steps = 2.8494e-01, PNorm = 152.8159, GNorm = 0.2721
Meta loss on this task batch = 2.9128e-01, Meta loss averaged over last 500 steps = 2.8496e-01, PNorm = 152.8203, GNorm = 0.2501
Meta loss on this task batch = 2.5308e-01, Meta loss averaged over last 500 steps = 2.8491e-01, PNorm = 152.8252, GNorm = 0.3343
Took 112.49104928970337 seconds to complete one epoch of meta training
Took 119.16053366661072 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449720
Epoch 706
Meta loss on this task batch = 2.7467e-01, Meta loss averaged over last 500 steps = 2.8484e-01, PNorm = 152.8303, GNorm = 0.2442
Meta loss on this task batch = 2.7012e-01, Meta loss averaged over last 500 steps = 2.8484e-01, PNorm = 152.8360, GNorm = 0.3552
Meta loss on this task batch = 2.9323e-01, Meta loss averaged over last 500 steps = 2.8480e-01, PNorm = 152.8414, GNorm = 0.3138
Meta loss on this task batch = 2.6713e-01, Meta loss averaged over last 500 steps = 2.8485e-01, PNorm = 152.8465, GNorm = 0.2934
Meta loss on this task batch = 2.4020e-01, Meta loss averaged over last 500 steps = 2.8471e-01, PNorm = 152.8521, GNorm = 0.3775
Meta loss on this task batch = 3.0340e-01, Meta loss averaged over last 500 steps = 2.8454e-01, PNorm = 152.8575, GNorm = 0.3039
Meta loss on this task batch = 3.1453e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 152.8621, GNorm = 0.3853
Meta loss on this task batch = 2.7773e-01, Meta loss averaged over last 500 steps = 2.8451e-01, PNorm = 152.8670, GNorm = 0.2816
Meta loss on this task batch = 3.0561e-01, Meta loss averaged over last 500 steps = 2.8468e-01, PNorm = 152.8721, GNorm = 0.3547
Meta loss on this task batch = 3.1636e-01, Meta loss averaged over last 500 steps = 2.8469e-01, PNorm = 152.8770, GNorm = 0.3001
Meta loss on this task batch = 2.7862e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 152.8819, GNorm = 0.2564
Meta loss on this task batch = 2.8367e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 152.8863, GNorm = 0.3323
Meta loss on this task batch = 3.4630e-01, Meta loss averaged over last 500 steps = 2.8481e-01, PNorm = 152.8905, GNorm = 0.3233
Meta loss on this task batch = 2.8545e-01, Meta loss averaged over last 500 steps = 2.8483e-01, PNorm = 152.8953, GNorm = 0.2533
Meta loss on this task batch = 2.6762e-01, Meta loss averaged over last 500 steps = 2.8476e-01, PNorm = 152.8996, GNorm = 0.3528
Meta loss on this task batch = 2.6292e-01, Meta loss averaged over last 500 steps = 2.8460e-01, PNorm = 152.9035, GNorm = 0.2946
Meta loss on this task batch = 2.9600e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 152.9076, GNorm = 0.4060
Meta loss on this task batch = 2.5640e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 152.9120, GNorm = 0.2746
Meta loss on this task batch = 2.5368e-01, Meta loss averaged over last 500 steps = 2.8449e-01, PNorm = 152.9165, GNorm = 0.2636
Took 111.4607663154602 seconds to complete one epoch of meta training
Took 119.59756231307983 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.433400
Epoch 707
Meta loss on this task batch = 3.2285e-01, Meta loss averaged over last 500 steps = 2.8453e-01, PNorm = 152.9210, GNorm = 0.3655
Meta loss on this task batch = 2.7560e-01, Meta loss averaged over last 500 steps = 2.8451e-01, PNorm = 152.9256, GNorm = 0.2643
Meta loss on this task batch = 2.9453e-01, Meta loss averaged over last 500 steps = 2.8456e-01, PNorm = 152.9297, GNorm = 0.3116
Meta loss on this task batch = 3.1281e-01, Meta loss averaged over last 500 steps = 2.8453e-01, PNorm = 152.9328, GNorm = 0.3042
Meta loss on this task batch = 2.6145e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 152.9357, GNorm = 0.2930
Meta loss on this task batch = 3.0387e-01, Meta loss averaged over last 500 steps = 2.8455e-01, PNorm = 152.9381, GNorm = 0.2585
Meta loss on this task batch = 2.7916e-01, Meta loss averaged over last 500 steps = 2.8453e-01, PNorm = 152.9402, GNorm = 0.3103
Meta loss on this task batch = 2.3607e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 152.9432, GNorm = 0.2128
Meta loss on this task batch = 2.6150e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 152.9467, GNorm = 0.3571
Meta loss on this task batch = 2.8156e-01, Meta loss averaged over last 500 steps = 2.8441e-01, PNorm = 152.9509, GNorm = 0.3069
Meta loss on this task batch = 2.6844e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 152.9563, GNorm = 0.4778
Meta loss on this task batch = 4.1110e-01, Meta loss averaged over last 500 steps = 2.8466e-01, PNorm = 152.9609, GNorm = 0.3764
Meta loss on this task batch = 2.6161e-01, Meta loss averaged over last 500 steps = 2.8458e-01, PNorm = 152.9659, GNorm = 0.2813
Meta loss on this task batch = 2.7601e-01, Meta loss averaged over last 500 steps = 2.8463e-01, PNorm = 152.9708, GNorm = 0.2899
Meta loss on this task batch = 2.6391e-01, Meta loss averaged over last 500 steps = 2.8468e-01, PNorm = 152.9756, GNorm = 0.3157
Meta loss on this task batch = 3.2600e-01, Meta loss averaged over last 500 steps = 2.8477e-01, PNorm = 152.9793, GNorm = 0.4146
Meta loss on this task batch = 2.8983e-01, Meta loss averaged over last 500 steps = 2.8468e-01, PNorm = 152.9827, GNorm = 0.3623
Meta loss on this task batch = 2.7678e-01, Meta loss averaged over last 500 steps = 2.8460e-01, PNorm = 152.9860, GNorm = 0.3520
Meta loss on this task batch = 2.5293e-01, Meta loss averaged over last 500 steps = 2.8449e-01, PNorm = 152.9890, GNorm = 0.3232
Took 112.53938794136047 seconds to complete one epoch of meta training
Took 120.60234427452087 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466298
Epoch 708
Meta loss on this task batch = 2.5700e-01, Meta loss averaged over last 500 steps = 2.8447e-01, PNorm = 152.9918, GNorm = 0.2574
Meta loss on this task batch = 2.7784e-01, Meta loss averaged over last 500 steps = 2.8447e-01, PNorm = 152.9952, GNorm = 0.2353
Meta loss on this task batch = 3.2538e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 152.9986, GNorm = 0.3610
Meta loss on this task batch = 2.4775e-01, Meta loss averaged over last 500 steps = 2.8445e-01, PNorm = 153.0012, GNorm = 0.3261
Meta loss on this task batch = 2.6378e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 153.0040, GNorm = 0.2529
Meta loss on this task batch = 2.4333e-01, Meta loss averaged over last 500 steps = 2.8439e-01, PNorm = 153.0066, GNorm = 0.2677
Meta loss on this task batch = 2.6127e-01, Meta loss averaged over last 500 steps = 2.8434e-01, PNorm = 153.0090, GNorm = 0.2979
Meta loss on this task batch = 3.0144e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 153.0114, GNorm = 0.2837
Meta loss on this task batch = 2.7724e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 153.0135, GNorm = 0.3022
Meta loss on this task batch = 3.5890e-01, Meta loss averaged over last 500 steps = 2.8473e-01, PNorm = 153.0149, GNorm = 0.3140
Meta loss on this task batch = 2.9887e-01, Meta loss averaged over last 500 steps = 2.8495e-01, PNorm = 153.0161, GNorm = 0.3667
Meta loss on this task batch = 2.9836e-01, Meta loss averaged over last 500 steps = 2.8504e-01, PNorm = 153.0177, GNorm = 0.2966
Meta loss on this task batch = 2.5567e-01, Meta loss averaged over last 500 steps = 2.8495e-01, PNorm = 153.0202, GNorm = 0.3071
Meta loss on this task batch = 3.2619e-01, Meta loss averaged over last 500 steps = 2.8502e-01, PNorm = 153.0231, GNorm = 0.4048
Meta loss on this task batch = 3.0245e-01, Meta loss averaged over last 500 steps = 2.8495e-01, PNorm = 153.0260, GNorm = 0.4031
Meta loss on this task batch = 2.8964e-01, Meta loss averaged over last 500 steps = 2.8481e-01, PNorm = 153.0289, GNorm = 0.3116
Meta loss on this task batch = 2.4606e-01, Meta loss averaged over last 500 steps = 2.8463e-01, PNorm = 153.0326, GNorm = 0.2715
Meta loss on this task batch = 2.9153e-01, Meta loss averaged over last 500 steps = 2.8464e-01, PNorm = 153.0365, GNorm = 0.2658
Meta loss on this task batch = 2.7673e-01, Meta loss averaged over last 500 steps = 2.8465e-01, PNorm = 153.0411, GNorm = 0.4003
Took 114.5292820930481 seconds to complete one epoch of meta training
Took 122.24905967712402 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457670
Epoch 709
Meta loss on this task batch = 2.3440e-01, Meta loss averaged over last 500 steps = 2.8454e-01, PNorm = 153.0472, GNorm = 0.2791
Meta loss on this task batch = 3.2029e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 153.0528, GNorm = 0.4237
Meta loss on this task batch = 3.0493e-01, Meta loss averaged over last 500 steps = 2.8456e-01, PNorm = 153.0574, GNorm = 0.3330
Meta loss on this task batch = 1.9691e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 153.0629, GNorm = 0.2530
Meta loss on this task batch = 2.6149e-01, Meta loss averaged over last 500 steps = 2.8428e-01, PNorm = 153.0685, GNorm = 0.3433
Meta loss on this task batch = 2.5201e-01, Meta loss averaged over last 500 steps = 2.8431e-01, PNorm = 153.0744, GNorm = 0.2495
Meta loss on this task batch = 3.4352e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 153.0804, GNorm = 0.2837
Meta loss on this task batch = 3.1068e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 153.0859, GNorm = 0.2673
Meta loss on this task batch = 2.8070e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 153.0911, GNorm = 0.4463
Meta loss on this task batch = 2.9352e-01, Meta loss averaged over last 500 steps = 2.8456e-01, PNorm = 153.0958, GNorm = 0.2940
Meta loss on this task batch = 2.8090e-01, Meta loss averaged over last 500 steps = 2.8451e-01, PNorm = 153.0996, GNorm = 0.3592
Meta loss on this task batch = 3.0265e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 153.1038, GNorm = 0.2501
Meta loss on this task batch = 2.8134e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 153.1086, GNorm = 0.2549
Meta loss on this task batch = 2.8993e-01, Meta loss averaged over last 500 steps = 2.8455e-01, PNorm = 153.1129, GNorm = 0.3772
Meta loss on this task batch = 2.7951e-01, Meta loss averaged over last 500 steps = 2.8452e-01, PNorm = 153.1175, GNorm = 0.3449
Meta loss on this task batch = 3.2619e-01, Meta loss averaged over last 500 steps = 2.8456e-01, PNorm = 153.1222, GNorm = 0.3495
Meta loss on this task batch = 3.4063e-01, Meta loss averaged over last 500 steps = 2.8462e-01, PNorm = 153.1272, GNorm = 0.4103
Meta loss on this task batch = 2.3965e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 153.1326, GNorm = 0.2798
Meta loss on this task batch = 2.4358e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 153.1385, GNorm = 0.2874
Took 113.7931182384491 seconds to complete one epoch of meta training
Took 121.57664370536804 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470541
Epoch 710
Meta loss on this task batch = 2.7778e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 153.1439, GNorm = 0.3090
Meta loss on this task batch = 2.6890e-01, Meta loss averaged over last 500 steps = 2.8427e-01, PNorm = 153.1483, GNorm = 0.3191
Meta loss on this task batch = 3.1775e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 153.1525, GNorm = 0.2686
Meta loss on this task batch = 3.0536e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 153.1568, GNorm = 0.2420
Meta loss on this task batch = 3.2680e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 153.1603, GNorm = 0.4534
Meta loss on this task batch = 3.0731e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 153.1638, GNorm = 0.3614
Meta loss on this task batch = 2.7174e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 153.1679, GNorm = 0.2919
Meta loss on this task batch = 2.6540e-01, Meta loss averaged over last 500 steps = 2.8449e-01, PNorm = 153.1718, GNorm = 0.3119
Meta loss on this task batch = 2.9401e-01, Meta loss averaged over last 500 steps = 2.8452e-01, PNorm = 153.1762, GNorm = 0.2760
Meta loss on this task batch = 2.9411e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 153.1806, GNorm = 0.3020
Meta loss on this task batch = 2.8420e-01, Meta loss averaged over last 500 steps = 2.8447e-01, PNorm = 153.1850, GNorm = 0.2858
Meta loss on this task batch = 2.7679e-01, Meta loss averaged over last 500 steps = 2.8445e-01, PNorm = 153.1894, GNorm = 0.3567
Meta loss on this task batch = 3.0498e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 153.1938, GNorm = 0.3055
Meta loss on this task batch = 3.2708e-01, Meta loss averaged over last 500 steps = 2.8453e-01, PNorm = 153.1985, GNorm = 0.3463
Meta loss on this task batch = 2.9201e-01, Meta loss averaged over last 500 steps = 2.8445e-01, PNorm = 153.2029, GNorm = 0.2387
Meta loss on this task batch = 2.7524e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 153.2082, GNorm = 0.2385
Meta loss on this task batch = 2.8372e-01, Meta loss averaged over last 500 steps = 2.8445e-01, PNorm = 153.2139, GNorm = 0.2640
Meta loss on this task batch = 2.3682e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 153.2203, GNorm = 0.2861
Meta loss on this task batch = 2.6273e-01, Meta loss averaged over last 500 steps = 2.8438e-01, PNorm = 153.2263, GNorm = 0.4743
Took 111.42373752593994 seconds to complete one epoch of meta training
Took 118.95402717590332 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.438925
Epoch 711
Meta loss on this task batch = 3.4875e-01, Meta loss averaged over last 500 steps = 2.8452e-01, PNorm = 153.2318, GNorm = 0.2983
Meta loss on this task batch = 2.2621e-01, Meta loss averaged over last 500 steps = 2.8447e-01, PNorm = 153.2370, GNorm = 0.4044
Meta loss on this task batch = 3.1640e-01, Meta loss averaged over last 500 steps = 2.8445e-01, PNorm = 153.2417, GNorm = 0.2703
Meta loss on this task batch = 3.3660e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 153.2461, GNorm = 0.3761
Meta loss on this task batch = 3.2160e-01, Meta loss averaged over last 500 steps = 2.8462e-01, PNorm = 153.2500, GNorm = 0.4351
Meta loss on this task batch = 3.2546e-01, Meta loss averaged over last 500 steps = 2.8468e-01, PNorm = 153.2535, GNorm = 0.2694
Meta loss on this task batch = 2.6064e-01, Meta loss averaged over last 500 steps = 2.8466e-01, PNorm = 153.2573, GNorm = 0.3396
Meta loss on this task batch = 2.6430e-01, Meta loss averaged over last 500 steps = 2.8472e-01, PNorm = 153.2612, GNorm = 0.2970
Meta loss on this task batch = 2.0615e-01, Meta loss averaged over last 500 steps = 2.8455e-01, PNorm = 153.2653, GNorm = 0.2962
Meta loss on this task batch = 2.5858e-01, Meta loss averaged over last 500 steps = 2.8453e-01, PNorm = 153.2694, GNorm = 0.2742
Meta loss on this task batch = 2.6679e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 153.2735, GNorm = 0.4256
Meta loss on this task batch = 2.7613e-01, Meta loss averaged over last 500 steps = 2.8452e-01, PNorm = 153.2766, GNorm = 0.3378
Meta loss on this task batch = 3.2623e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 153.2798, GNorm = 0.3136
Meta loss on this task batch = 2.3426e-01, Meta loss averaged over last 500 steps = 2.8441e-01, PNorm = 153.2834, GNorm = 0.4674
Meta loss on this task batch = 2.8479e-01, Meta loss averaged over last 500 steps = 2.8439e-01, PNorm = 153.2876, GNorm = 0.3880
Meta loss on this task batch = 2.8416e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 153.2924, GNorm = 0.4896
Meta loss on this task batch = 2.9443e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 153.2971, GNorm = 0.2948
Meta loss on this task batch = 2.5647e-01, Meta loss averaged over last 500 steps = 2.8437e-01, PNorm = 153.3020, GNorm = 0.2875
Meta loss on this task batch = 2.6406e-01, Meta loss averaged over last 500 steps = 2.8425e-01, PNorm = 153.3073, GNorm = 0.3191
Took 112.8848295211792 seconds to complete one epoch of meta training
Took 120.40148496627808 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.432923
Epoch 712
Meta loss on this task batch = 3.1513e-01, Meta loss averaged over last 500 steps = 2.8433e-01, PNorm = 153.3121, GNorm = 0.3730
Meta loss on this task batch = 3.0396e-01, Meta loss averaged over last 500 steps = 2.8437e-01, PNorm = 153.3167, GNorm = 0.3977
Meta loss on this task batch = 2.7365e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 153.3212, GNorm = 0.2549
Meta loss on this task batch = 2.4711e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 153.3258, GNorm = 0.2467
Meta loss on this task batch = 3.0136e-01, Meta loss averaged over last 500 steps = 2.8417e-01, PNorm = 153.3302, GNorm = 0.3886
Meta loss on this task batch = 3.2265e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 153.3342, GNorm = 0.3525
Meta loss on this task batch = 2.5181e-01, Meta loss averaged over last 500 steps = 2.8413e-01, PNorm = 153.3380, GNorm = 0.2797
Meta loss on this task batch = 2.6788e-01, Meta loss averaged over last 500 steps = 2.8417e-01, PNorm = 153.3422, GNorm = 0.2983
Meta loss on this task batch = 2.9790e-01, Meta loss averaged over last 500 steps = 2.8410e-01, PNorm = 153.3459, GNorm = 0.2855
Meta loss on this task batch = 3.1485e-01, Meta loss averaged over last 500 steps = 2.8421e-01, PNorm = 153.3494, GNorm = 0.3678
Meta loss on this task batch = 2.5240e-01, Meta loss averaged over last 500 steps = 2.8415e-01, PNorm = 153.3536, GNorm = 0.2955
Meta loss on this task batch = 2.8853e-01, Meta loss averaged over last 500 steps = 2.8425e-01, PNorm = 153.3583, GNorm = 0.2798
Meta loss on this task batch = 2.6848e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 153.3632, GNorm = 0.2339
Meta loss on this task batch = 2.9594e-01, Meta loss averaged over last 500 steps = 2.8419e-01, PNorm = 153.3679, GNorm = 0.2876
Meta loss on this task batch = 2.2112e-01, Meta loss averaged over last 500 steps = 2.8405e-01, PNorm = 153.3727, GNorm = 0.2494
Meta loss on this task batch = 2.5415e-01, Meta loss averaged over last 500 steps = 2.8400e-01, PNorm = 153.3779, GNorm = 0.3282
Meta loss on this task batch = 3.5443e-01, Meta loss averaged over last 500 steps = 2.8419e-01, PNorm = 153.3825, GNorm = 0.2946
Meta loss on this task batch = 2.5904e-01, Meta loss averaged over last 500 steps = 2.8417e-01, PNorm = 153.3865, GNorm = 0.3325
Meta loss on this task batch = 2.4339e-01, Meta loss averaged over last 500 steps = 2.8413e-01, PNorm = 153.3909, GNorm = 0.2918
Took 111.96197843551636 seconds to complete one epoch of meta training
Took 118.99195981025696 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456915
Epoch 713
Meta loss on this task batch = 2.6544e-01, Meta loss averaged over last 500 steps = 2.8415e-01, PNorm = 153.3959, GNorm = 0.2729
Meta loss on this task batch = 2.8135e-01, Meta loss averaged over last 500 steps = 2.8410e-01, PNorm = 153.4005, GNorm = 0.2908
Meta loss on this task batch = 2.5771e-01, Meta loss averaged over last 500 steps = 2.8397e-01, PNorm = 153.4050, GNorm = 0.3224
Meta loss on this task batch = 3.0883e-01, Meta loss averaged over last 500 steps = 2.8398e-01, PNorm = 153.4096, GNorm = 0.2842
Meta loss on this task batch = 3.0270e-01, Meta loss averaged over last 500 steps = 2.8399e-01, PNorm = 153.4129, GNorm = 0.4340
Meta loss on this task batch = 2.2978e-01, Meta loss averaged over last 500 steps = 2.8375e-01, PNorm = 153.4161, GNorm = 0.2578
Meta loss on this task batch = 2.7169e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 153.4197, GNorm = 0.2818
Meta loss on this task batch = 2.7069e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 153.4234, GNorm = 0.3147
Meta loss on this task batch = 3.3026e-01, Meta loss averaged over last 500 steps = 2.8376e-01, PNorm = 153.4269, GNorm = 0.3094
Meta loss on this task batch = 2.7916e-01, Meta loss averaged over last 500 steps = 2.8387e-01, PNorm = 153.4303, GNorm = 0.3395
Meta loss on this task batch = 3.0571e-01, Meta loss averaged over last 500 steps = 2.8385e-01, PNorm = 153.4332, GNorm = 0.3728
Meta loss on this task batch = 2.3232e-01, Meta loss averaged over last 500 steps = 2.8375e-01, PNorm = 153.4364, GNorm = 0.2796
Meta loss on this task batch = 3.0479e-01, Meta loss averaged over last 500 steps = 2.8381e-01, PNorm = 153.4400, GNorm = 0.3181
Meta loss on this task batch = 2.6067e-01, Meta loss averaged over last 500 steps = 2.8379e-01, PNorm = 153.4442, GNorm = 0.2908
Meta loss on this task batch = 2.5504e-01, Meta loss averaged over last 500 steps = 2.8374e-01, PNorm = 153.4484, GNorm = 0.2966
Meta loss on this task batch = 2.9105e-01, Meta loss averaged over last 500 steps = 2.8369e-01, PNorm = 153.4525, GNorm = 0.3030
Meta loss on this task batch = 2.8311e-01, Meta loss averaged over last 500 steps = 2.8371e-01, PNorm = 153.4568, GNorm = 0.3850
Meta loss on this task batch = 2.8996e-01, Meta loss averaged over last 500 steps = 2.8374e-01, PNorm = 153.4614, GNorm = 0.2582
Meta loss on this task batch = 3.1447e-01, Meta loss averaged over last 500 steps = 2.8389e-01, PNorm = 153.4667, GNorm = 0.4138
Took 112.12057113647461 seconds to complete one epoch of meta training
Took 119.73292994499207 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.437389
Epoch 714
Meta loss on this task batch = 2.9549e-01, Meta loss averaged over last 500 steps = 2.8391e-01, PNorm = 153.4711, GNorm = 0.2788
Meta loss on this task batch = 2.6697e-01, Meta loss averaged over last 500 steps = 2.8389e-01, PNorm = 153.4750, GNorm = 0.3995
Meta loss on this task batch = 3.0879e-01, Meta loss averaged over last 500 steps = 2.8392e-01, PNorm = 153.4788, GNorm = 0.3431
Meta loss on this task batch = 2.6194e-01, Meta loss averaged over last 500 steps = 2.8382e-01, PNorm = 153.4831, GNorm = 0.2596
Meta loss on this task batch = 3.4353e-01, Meta loss averaged over last 500 steps = 2.8391e-01, PNorm = 153.4879, GNorm = 0.2961
Meta loss on this task batch = 2.8585e-01, Meta loss averaged over last 500 steps = 2.8386e-01, PNorm = 153.4929, GNorm = 0.3703
Meta loss on this task batch = 3.9077e-01, Meta loss averaged over last 500 steps = 2.8408e-01, PNorm = 153.4960, GNorm = 0.3846
Meta loss on this task batch = 3.0511e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 153.4993, GNorm = 0.2755
Meta loss on this task batch = 2.8641e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 153.5027, GNorm = 0.2246
Meta loss on this task batch = 2.8636e-01, Meta loss averaged over last 500 steps = 2.8419e-01, PNorm = 153.5067, GNorm = 0.2502
Meta loss on this task batch = 2.6311e-01, Meta loss averaged over last 500 steps = 2.8422e-01, PNorm = 153.5113, GNorm = 0.2571
Meta loss on this task batch = 2.3706e-01, Meta loss averaged over last 500 steps = 2.8402e-01, PNorm = 153.5164, GNorm = 0.2256
Meta loss on this task batch = 2.7461e-01, Meta loss averaged over last 500 steps = 2.8393e-01, PNorm = 153.5212, GNorm = 0.2449
Meta loss on this task batch = 2.5326e-01, Meta loss averaged over last 500 steps = 2.8381e-01, PNorm = 153.5269, GNorm = 0.3366
Meta loss on this task batch = 2.9203e-01, Meta loss averaged over last 500 steps = 2.8388e-01, PNorm = 153.5326, GNorm = 0.2900
Meta loss on this task batch = 3.3368e-01, Meta loss averaged over last 500 steps = 2.8403e-01, PNorm = 153.5382, GNorm = 0.2745
Meta loss on this task batch = 2.7088e-01, Meta loss averaged over last 500 steps = 2.8394e-01, PNorm = 153.5433, GNorm = 0.3125
Meta loss on this task batch = 2.6445e-01, Meta loss averaged over last 500 steps = 2.8392e-01, PNorm = 153.5489, GNorm = 0.2570
Meta loss on this task batch = 2.8926e-01, Meta loss averaged over last 500 steps = 2.8389e-01, PNorm = 153.5544, GNorm = 0.3175
Took 112.47731184959412 seconds to complete one epoch of meta training
Took 119.20963764190674 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.440029
Epoch 715
Meta loss on this task batch = 2.4968e-01, Meta loss averaged over last 500 steps = 2.8372e-01, PNorm = 153.5598, GNorm = 0.2379
Meta loss on this task batch = 3.1754e-01, Meta loss averaged over last 500 steps = 2.8378e-01, PNorm = 153.5663, GNorm = 0.2829
Meta loss on this task batch = 2.7505e-01, Meta loss averaged over last 500 steps = 2.8373e-01, PNorm = 153.5723, GNorm = 0.3929
Meta loss on this task batch = 2.9260e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 153.5786, GNorm = 0.2603
Meta loss on this task batch = 2.5291e-01, Meta loss averaged over last 500 steps = 2.8365e-01, PNorm = 153.5846, GNorm = 0.4159
Meta loss on this task batch = 2.8301e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 153.5901, GNorm = 0.2639
Meta loss on this task batch = 2.5335e-01, Meta loss averaged over last 500 steps = 2.8355e-01, PNorm = 153.5957, GNorm = 0.2664
Meta loss on this task batch = 2.5966e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 153.6010, GNorm = 0.2753
Meta loss on this task batch = 2.1319e-01, Meta loss averaged over last 500 steps = 2.8355e-01, PNorm = 153.6064, GNorm = 0.3686
Meta loss on this task batch = 2.7429e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 153.6112, GNorm = 0.2743
Meta loss on this task batch = 3.6024e-01, Meta loss averaged over last 500 steps = 2.8368e-01, PNorm = 153.6154, GNorm = 0.4309
Meta loss on this task batch = 2.8605e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 153.6190, GNorm = 0.3293
Meta loss on this task batch = 2.7366e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 153.6223, GNorm = 0.3503
Meta loss on this task batch = 2.9628e-01, Meta loss averaged over last 500 steps = 2.8373e-01, PNorm = 153.6256, GNorm = 0.2889
Meta loss on this task batch = 2.8912e-01, Meta loss averaged over last 500 steps = 2.8364e-01, PNorm = 153.6293, GNorm = 0.4529
Meta loss on this task batch = 3.1877e-01, Meta loss averaged over last 500 steps = 2.8364e-01, PNorm = 153.6328, GNorm = 0.3657
Meta loss on this task batch = 3.1426e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 153.6364, GNorm = 0.3042
Meta loss on this task batch = 2.8062e-01, Meta loss averaged over last 500 steps = 2.8363e-01, PNorm = 153.6408, GNorm = 0.4013
Meta loss on this task batch = 3.5207e-01, Meta loss averaged over last 500 steps = 2.8390e-01, PNorm = 153.6460, GNorm = 0.5822
Took 112.67166924476624 seconds to complete one epoch of meta training
Took 120.63842058181763 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.433740
Epoch 716
Meta loss on this task batch = 2.5042e-01, Meta loss averaged over last 500 steps = 2.8377e-01, PNorm = 153.6523, GNorm = 0.3533
Meta loss on this task batch = 2.7945e-01, Meta loss averaged over last 500 steps = 2.8368e-01, PNorm = 153.6578, GNorm = 0.3993
Meta loss on this task batch = 2.9197e-01, Meta loss averaged over last 500 steps = 2.8365e-01, PNorm = 153.6633, GNorm = 0.2955
Meta loss on this task batch = 2.4632e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 153.6694, GNorm = 0.2365
Meta loss on this task batch = 2.7468e-01, Meta loss averaged over last 500 steps = 2.8359e-01, PNorm = 153.6759, GNorm = 0.4187
Meta loss on this task batch = 2.7481e-01, Meta loss averaged over last 500 steps = 2.8352e-01, PNorm = 153.6821, GNorm = 0.3254
Meta loss on this task batch = 3.0338e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 153.6872, GNorm = 0.4393
Meta loss on this task batch = 2.6119e-01, Meta loss averaged over last 500 steps = 2.8351e-01, PNorm = 153.6929, GNorm = 0.2533
Meta loss on this task batch = 2.4319e-01, Meta loss averaged over last 500 steps = 2.8342e-01, PNorm = 153.6987, GNorm = 0.2366
Meta loss on this task batch = 3.2903e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 153.7041, GNorm = 0.2717
Meta loss on this task batch = 3.1146e-01, Meta loss averaged over last 500 steps = 2.8349e-01, PNorm = 153.7088, GNorm = 0.2988
Meta loss on this task batch = 2.7659e-01, Meta loss averaged over last 500 steps = 2.8347e-01, PNorm = 153.7135, GNorm = 0.3153
Meta loss on this task batch = 2.5107e-01, Meta loss averaged over last 500 steps = 2.8332e-01, PNorm = 153.7184, GNorm = 0.2618
Meta loss on this task batch = 2.9135e-01, Meta loss averaged over last 500 steps = 2.8334e-01, PNorm = 153.7228, GNorm = 0.3119
Meta loss on this task batch = 3.5323e-01, Meta loss averaged over last 500 steps = 2.8347e-01, PNorm = 153.7259, GNorm = 0.3263
Meta loss on this task batch = 2.9775e-01, Meta loss averaged over last 500 steps = 2.8351e-01, PNorm = 153.7283, GNorm = 0.3022
Meta loss on this task batch = 3.1290e-01, Meta loss averaged over last 500 steps = 2.8352e-01, PNorm = 153.7304, GNorm = 0.3205
Meta loss on this task batch = 2.8502e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 153.7329, GNorm = 0.4352
Meta loss on this task batch = 3.7189e-01, Meta loss averaged over last 500 steps = 2.8373e-01, PNorm = 153.7355, GNorm = 0.6611
Took 110.92660093307495 seconds to complete one epoch of meta training
Took 117.8483669757843 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.442324
Epoch 717
Meta loss on this task batch = 2.3237e-01, Meta loss averaged over last 500 steps = 2.8374e-01, PNorm = 153.7385, GNorm = 0.2969
Meta loss on this task batch = 2.5491e-01, Meta loss averaged over last 500 steps = 2.8363e-01, PNorm = 153.7423, GNorm = 0.2290
Meta loss on this task batch = 2.4831e-01, Meta loss averaged over last 500 steps = 2.8358e-01, PNorm = 153.7462, GNorm = 0.2482
Meta loss on this task batch = 3.0135e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 153.7501, GNorm = 0.2965
Meta loss on this task batch = 2.1623e-01, Meta loss averaged over last 500 steps = 2.8348e-01, PNorm = 153.7544, GNorm = 0.2818
Meta loss on this task batch = 2.8510e-01, Meta loss averaged over last 500 steps = 2.8353e-01, PNorm = 153.7585, GNorm = 0.2569
Meta loss on this task batch = 2.6032e-01, Meta loss averaged over last 500 steps = 2.8337e-01, PNorm = 153.7622, GNorm = 0.2642
Meta loss on this task batch = 3.5677e-01, Meta loss averaged over last 500 steps = 2.8353e-01, PNorm = 153.7653, GNorm = 0.3854
Meta loss on this task batch = 3.3929e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 153.7681, GNorm = 0.2877
Meta loss on this task batch = 2.2418e-01, Meta loss averaged over last 500 steps = 2.8347e-01, PNorm = 153.7713, GNorm = 0.2607
Meta loss on this task batch = 2.7188e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 153.7748, GNorm = 0.3325
Meta loss on this task batch = 3.0729e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 153.7783, GNorm = 0.3120
Meta loss on this task batch = 2.8463e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 153.7821, GNorm = 0.3523
Meta loss on this task batch = 2.6668e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 153.7864, GNorm = 0.2517
Meta loss on this task batch = 3.0105e-01, Meta loss averaged over last 500 steps = 2.8363e-01, PNorm = 153.7901, GNorm = 0.3197
Meta loss on this task batch = 3.1988e-01, Meta loss averaged over last 500 steps = 2.8365e-01, PNorm = 153.7943, GNorm = 0.2978
Meta loss on this task batch = 3.0868e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 153.7985, GNorm = 0.2773
Meta loss on this task batch = 2.7852e-01, Meta loss averaged over last 500 steps = 2.8374e-01, PNorm = 153.8026, GNorm = 0.2797
Meta loss on this task batch = 2.8224e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 153.8070, GNorm = 0.3312
Took 113.98369264602661 seconds to complete one epoch of meta training
Took 121.7909529209137 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469238
Epoch 718
Meta loss on this task batch = 2.6088e-01, Meta loss averaged over last 500 steps = 2.8349e-01, PNorm = 153.8117, GNorm = 0.2626
Meta loss on this task batch = 2.8325e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 153.8165, GNorm = 0.2237
Meta loss on this task batch = 2.8600e-01, Meta loss averaged over last 500 steps = 2.8364e-01, PNorm = 153.8216, GNorm = 0.3014
Meta loss on this task batch = 2.9221e-01, Meta loss averaged over last 500 steps = 2.8373e-01, PNorm = 153.8266, GNorm = 0.2906
Meta loss on this task batch = 2.5674e-01, Meta loss averaged over last 500 steps = 2.8367e-01, PNorm = 153.8322, GNorm = 0.2092
Meta loss on this task batch = 3.2247e-01, Meta loss averaged over last 500 steps = 2.8375e-01, PNorm = 153.8370, GNorm = 0.2926
Meta loss on this task batch = 2.9238e-01, Meta loss averaged over last 500 steps = 2.8370e-01, PNorm = 153.8407, GNorm = 0.3407
Meta loss on this task batch = 2.6599e-01, Meta loss averaged over last 500 steps = 2.8369e-01, PNorm = 153.8443, GNorm = 0.2677
Meta loss on this task batch = 2.8424e-01, Meta loss averaged over last 500 steps = 2.8374e-01, PNorm = 153.8488, GNorm = 0.2715
Meta loss on this task batch = 3.4991e-01, Meta loss averaged over last 500 steps = 2.8396e-01, PNorm = 153.8520, GNorm = 0.2740
Meta loss on this task batch = 2.7829e-01, Meta loss averaged over last 500 steps = 2.8398e-01, PNorm = 153.8549, GNorm = 0.2864
Meta loss on this task batch = 2.6744e-01, Meta loss averaged over last 500 steps = 2.8400e-01, PNorm = 153.8580, GNorm = 0.3267
Meta loss on this task batch = 2.5654e-01, Meta loss averaged over last 500 steps = 2.8398e-01, PNorm = 153.8623, GNorm = 0.2655
Meta loss on this task batch = 2.7885e-01, Meta loss averaged over last 500 steps = 2.8400e-01, PNorm = 153.8667, GNorm = 0.2703
Meta loss on this task batch = 3.1632e-01, Meta loss averaged over last 500 steps = 2.8407e-01, PNorm = 153.8712, GNorm = 0.3252
Meta loss on this task batch = 2.6706e-01, Meta loss averaged over last 500 steps = 2.8404e-01, PNorm = 153.8760, GNorm = 0.2758
Meta loss on this task batch = 2.6351e-01, Meta loss averaged over last 500 steps = 2.8398e-01, PNorm = 153.8810, GNorm = 0.3155
Meta loss on this task batch = 2.7067e-01, Meta loss averaged over last 500 steps = 2.8396e-01, PNorm = 153.8866, GNorm = 0.2689
Meta loss on this task batch = 2.6999e-01, Meta loss averaged over last 500 steps = 2.8386e-01, PNorm = 153.8927, GNorm = 0.2972
Took 114.16064310073853 seconds to complete one epoch of meta training
Took 122.02985644340515 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.432134
Epoch 719
Meta loss on this task batch = 2.5558e-01, Meta loss averaged over last 500 steps = 2.8391e-01, PNorm = 153.8988, GNorm = 0.2538
Meta loss on this task batch = 2.3549e-01, Meta loss averaged over last 500 steps = 2.8375e-01, PNorm = 153.9050, GNorm = 0.2951
Meta loss on this task batch = 2.6860e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 153.9116, GNorm = 0.2574
Meta loss on this task batch = 3.2903e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 153.9174, GNorm = 0.3069
Meta loss on this task batch = 3.2708e-01, Meta loss averaged over last 500 steps = 2.8377e-01, PNorm = 153.9216, GNorm = 0.3817
Meta loss on this task batch = 3.1534e-01, Meta loss averaged over last 500 steps = 2.8382e-01, PNorm = 153.9253, GNorm = 0.4214
Meta loss on this task batch = 2.1459e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 153.9294, GNorm = 0.3373
Meta loss on this task batch = 3.0066e-01, Meta loss averaged over last 500 steps = 2.8364e-01, PNorm = 153.9337, GNorm = 0.4251
Meta loss on this task batch = 2.3905e-01, Meta loss averaged over last 500 steps = 2.8351e-01, PNorm = 153.9379, GNorm = 0.2320
Meta loss on this task batch = 2.5839e-01, Meta loss averaged over last 500 steps = 2.8361e-01, PNorm = 153.9418, GNorm = 0.3304
Meta loss on this task batch = 2.9570e-01, Meta loss averaged over last 500 steps = 2.8365e-01, PNorm = 153.9462, GNorm = 0.3155
Meta loss on this task batch = 2.6474e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 153.9511, GNorm = 0.3505
Meta loss on this task batch = 3.3567e-01, Meta loss averaged over last 500 steps = 2.8368e-01, PNorm = 153.9568, GNorm = 0.3726
Meta loss on this task batch = 2.2011e-01, Meta loss averaged over last 500 steps = 2.8355e-01, PNorm = 153.9631, GNorm = 0.2833
Meta loss on this task batch = 2.7582e-01, Meta loss averaged over last 500 steps = 2.8347e-01, PNorm = 153.9700, GNorm = 0.2779
Meta loss on this task batch = 2.8167e-01, Meta loss averaged over last 500 steps = 2.8353e-01, PNorm = 153.9776, GNorm = 0.3233
Meta loss on this task batch = 2.8839e-01, Meta loss averaged over last 500 steps = 2.8358e-01, PNorm = 153.9852, GNorm = 0.3326
Meta loss on this task batch = 3.1964e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 153.9926, GNorm = 0.3043
Meta loss on this task batch = 2.8970e-01, Meta loss averaged over last 500 steps = 2.8364e-01, PNorm = 154.0000, GNorm = 0.3566
Took 111.36262893676758 seconds to complete one epoch of meta training
Took 119.21351361274719 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458969
Epoch 720
Meta loss on this task batch = 2.5246e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 154.0072, GNorm = 0.2721
Meta loss on this task batch = 3.1555e-01, Meta loss averaged over last 500 steps = 2.8378e-01, PNorm = 154.0134, GNorm = 0.4299
Meta loss on this task batch = 2.5082e-01, Meta loss averaged over last 500 steps = 2.8365e-01, PNorm = 154.0192, GNorm = 0.3391
Meta loss on this task batch = 2.5767e-01, Meta loss averaged over last 500 steps = 2.8355e-01, PNorm = 154.0247, GNorm = 0.2905
Meta loss on this task batch = 2.5887e-01, Meta loss averaged over last 500 steps = 2.8361e-01, PNorm = 154.0300, GNorm = 0.3138
Meta loss on this task batch = 2.4624e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 154.0355, GNorm = 0.2970
Meta loss on this task batch = 2.5786e-01, Meta loss averaged over last 500 steps = 2.8359e-01, PNorm = 154.0407, GNorm = 0.3188
Meta loss on this task batch = 2.6566e-01, Meta loss averaged over last 500 steps = 2.8343e-01, PNorm = 154.0457, GNorm = 0.2819
Meta loss on this task batch = 3.7658e-01, Meta loss averaged over last 500 steps = 2.8371e-01, PNorm = 154.0498, GNorm = 0.3467
Meta loss on this task batch = 2.5150e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 154.0538, GNorm = 0.2596
Meta loss on this task batch = 2.7569e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 154.0579, GNorm = 0.2708
Meta loss on this task batch = 2.7119e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 154.0626, GNorm = 0.2439
Meta loss on this task batch = 2.6078e-01, Meta loss averaged over last 500 steps = 2.8338e-01, PNorm = 154.0663, GNorm = 0.2760
Meta loss on this task batch = 3.2758e-01, Meta loss averaged over last 500 steps = 2.8355e-01, PNorm = 154.0692, GNorm = 0.4709
Meta loss on this task batch = 2.5353e-01, Meta loss averaged over last 500 steps = 2.8359e-01, PNorm = 154.0718, GNorm = 0.2524
Meta loss on this task batch = 3.4229e-01, Meta loss averaged over last 500 steps = 2.8372e-01, PNorm = 154.0752, GNorm = 0.3489
Meta loss on this task batch = 3.3958e-01, Meta loss averaged over last 500 steps = 2.8391e-01, PNorm = 154.0788, GNorm = 0.4830
Meta loss on this task batch = 3.0841e-01, Meta loss averaged over last 500 steps = 2.8384e-01, PNorm = 154.0823, GNorm = 0.2633
Meta loss on this task batch = 2.7978e-01, Meta loss averaged over last 500 steps = 2.8392e-01, PNorm = 154.0856, GNorm = 0.3010
Took 112.84787106513977 seconds to complete one epoch of meta training
Took 120.72055912017822 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.441711
Epoch 721
Meta loss on this task batch = 2.5032e-01, Meta loss averaged over last 500 steps = 2.8373e-01, PNorm = 154.0902, GNorm = 0.3521
Meta loss on this task batch = 2.4806e-01, Meta loss averaged over last 500 steps = 2.8372e-01, PNorm = 154.0954, GNorm = 0.2389
Meta loss on this task batch = 3.0239e-01, Meta loss averaged over last 500 steps = 2.8372e-01, PNorm = 154.1004, GNorm = 0.2845
Meta loss on this task batch = 3.0720e-01, Meta loss averaged over last 500 steps = 2.8379e-01, PNorm = 154.1048, GNorm = 0.3152
Meta loss on this task batch = 2.4867e-01, Meta loss averaged over last 500 steps = 2.8371e-01, PNorm = 154.1098, GNorm = 0.2666
Meta loss on this task batch = 3.0910e-01, Meta loss averaged over last 500 steps = 2.8383e-01, PNorm = 154.1150, GNorm = 0.3040
Meta loss on this task batch = 2.3721e-01, Meta loss averaged over last 500 steps = 2.8368e-01, PNorm = 154.1209, GNorm = 0.3485
Meta loss on this task batch = 2.3662e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 154.1269, GNorm = 0.2503
Meta loss on this task batch = 2.4991e-01, Meta loss averaged over last 500 steps = 2.8359e-01, PNorm = 154.1327, GNorm = 0.2588
Meta loss on this task batch = 2.7566e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 154.1376, GNorm = 0.2579
Meta loss on this task batch = 2.5318e-01, Meta loss averaged over last 500 steps = 2.8338e-01, PNorm = 154.1418, GNorm = 0.2506
Meta loss on this task batch = 2.3898e-01, Meta loss averaged over last 500 steps = 2.8325e-01, PNorm = 154.1461, GNorm = 0.3006
Meta loss on this task batch = 3.1226e-01, Meta loss averaged over last 500 steps = 2.8339e-01, PNorm = 154.1505, GNorm = 0.2845
Meta loss on this task batch = 3.4762e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 154.1542, GNorm = 0.3422
Meta loss on this task batch = 2.5672e-01, Meta loss averaged over last 500 steps = 2.8346e-01, PNorm = 154.1585, GNorm = 0.3704
Meta loss on this task batch = 2.6782e-01, Meta loss averaged over last 500 steps = 2.8342e-01, PNorm = 154.1626, GNorm = 0.3651
Meta loss on this task batch = 3.2784e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 154.1668, GNorm = 0.4362
Meta loss on this task batch = 3.1588e-01, Meta loss averaged over last 500 steps = 2.8352e-01, PNorm = 154.1711, GNorm = 0.3330
Meta loss on this task batch = 2.6969e-01, Meta loss averaged over last 500 steps = 2.8352e-01, PNorm = 154.1752, GNorm = 0.5353
Took 111.47745060920715 seconds to complete one epoch of meta training
Took 119.38311386108398 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452849
Epoch 722
Meta loss on this task batch = 2.5273e-01, Meta loss averaged over last 500 steps = 2.8339e-01, PNorm = 154.1790, GNorm = 0.3300
Meta loss on this task batch = 2.3444e-01, Meta loss averaged over last 500 steps = 2.8325e-01, PNorm = 154.1840, GNorm = 0.3123
Meta loss on this task batch = 3.3159e-01, Meta loss averaged over last 500 steps = 2.8342e-01, PNorm = 154.1888, GNorm = 0.3477
Meta loss on this task batch = 2.6234e-01, Meta loss averaged over last 500 steps = 2.8327e-01, PNorm = 154.1939, GNorm = 0.3045
Meta loss on this task batch = 3.2223e-01, Meta loss averaged over last 500 steps = 2.8345e-01, PNorm = 154.1997, GNorm = 0.2925
Meta loss on this task batch = 2.7829e-01, Meta loss averaged over last 500 steps = 2.8344e-01, PNorm = 154.2053, GNorm = 0.2985
Meta loss on this task batch = 2.7184e-01, Meta loss averaged over last 500 steps = 2.8339e-01, PNorm = 154.2109, GNorm = 0.2930
Meta loss on this task batch = 3.2772e-01, Meta loss averaged over last 500 steps = 2.8338e-01, PNorm = 154.2165, GNorm = 0.3491
Meta loss on this task batch = 3.1537e-01, Meta loss averaged over last 500 steps = 2.8339e-01, PNorm = 154.2220, GNorm = 0.3615
Meta loss on this task batch = 3.3303e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 154.2260, GNorm = 0.4650
Meta loss on this task batch = 3.0744e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 154.2304, GNorm = 0.3349
Meta loss on this task batch = 2.7970e-01, Meta loss averaged over last 500 steps = 2.8349e-01, PNorm = 154.2357, GNorm = 0.3645
Meta loss on this task batch = 2.8905e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 154.2412, GNorm = 0.3087
Meta loss on this task batch = 2.6781e-01, Meta loss averaged over last 500 steps = 2.8367e-01, PNorm = 154.2464, GNorm = 0.2836
Meta loss on this task batch = 2.8512e-01, Meta loss averaged over last 500 steps = 2.8359e-01, PNorm = 154.2509, GNorm = 0.3204
Meta loss on this task batch = 2.7765e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 154.2560, GNorm = 0.2580
Meta loss on this task batch = 2.9759e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 154.2607, GNorm = 0.3053
Meta loss on this task batch = 2.2284e-01, Meta loss averaged over last 500 steps = 2.8349e-01, PNorm = 154.2663, GNorm = 0.2278
Meta loss on this task batch = 2.7481e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 154.2718, GNorm = 0.4517
Took 111.75919032096863 seconds to complete one epoch of meta training
Took 118.69403553009033 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.442432
Epoch 723
Meta loss on this task batch = 2.8200e-01, Meta loss averaged over last 500 steps = 2.8345e-01, PNorm = 154.2769, GNorm = 0.2295
Meta loss on this task batch = 3.5962e-01, Meta loss averaged over last 500 steps = 2.8375e-01, PNorm = 154.2811, GNorm = 0.2777
Meta loss on this task batch = 3.2080e-01, Meta loss averaged over last 500 steps = 2.8380e-01, PNorm = 154.2845, GNorm = 0.4973
Meta loss on this task batch = 2.7845e-01, Meta loss averaged over last 500 steps = 2.8377e-01, PNorm = 154.2879, GNorm = 0.2601
Meta loss on this task batch = 3.1072e-01, Meta loss averaged over last 500 steps = 2.8391e-01, PNorm = 154.2904, GNorm = 0.4297
Meta loss on this task batch = 2.6082e-01, Meta loss averaged over last 500 steps = 2.8395e-01, PNorm = 154.2929, GNorm = 0.3034
Meta loss on this task batch = 3.1221e-01, Meta loss averaged over last 500 steps = 2.8397e-01, PNorm = 154.2949, GNorm = 0.2794
Meta loss on this task batch = 2.7418e-01, Meta loss averaged over last 500 steps = 2.8394e-01, PNorm = 154.2969, GNorm = 0.3833
Meta loss on this task batch = 2.8190e-01, Meta loss averaged over last 500 steps = 2.8396e-01, PNorm = 154.2996, GNorm = 0.2989
Meta loss on this task batch = 2.6917e-01, Meta loss averaged over last 500 steps = 2.8405e-01, PNorm = 154.3023, GNorm = 0.3123
Meta loss on this task batch = 3.6062e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 154.3046, GNorm = 0.3385
Meta loss on this task batch = 2.9291e-01, Meta loss averaged over last 500 steps = 2.8422e-01, PNorm = 154.3072, GNorm = 0.3716
Meta loss on this task batch = 2.8670e-01, Meta loss averaged over last 500 steps = 2.8424e-01, PNorm = 154.3091, GNorm = 0.4073
Meta loss on this task batch = 3.1840e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 154.3118, GNorm = 0.3703
Meta loss on this task batch = 3.0879e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 154.3154, GNorm = 0.3801
Meta loss on this task batch = 2.9186e-01, Meta loss averaged over last 500 steps = 2.8452e-01, PNorm = 154.3191, GNorm = 0.2559
Meta loss on this task batch = 2.7567e-01, Meta loss averaged over last 500 steps = 2.8454e-01, PNorm = 154.3230, GNorm = 0.2630
Meta loss on this task batch = 3.2787e-01, Meta loss averaged over last 500 steps = 2.8451e-01, PNorm = 154.3271, GNorm = 0.3224
Meta loss on this task batch = 1.9244e-01, Meta loss averaged over last 500 steps = 2.8426e-01, PNorm = 154.3318, GNorm = 0.2871
Took 110.93347215652466 seconds to complete one epoch of meta training
Took 118.86806082725525 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.434537
Epoch 724
Meta loss on this task batch = 3.5065e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 154.3355, GNorm = 0.3068
Meta loss on this task batch = 2.8074e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 154.3393, GNorm = 0.2383
Meta loss on this task batch = 3.6216e-01, Meta loss averaged over last 500 steps = 2.8467e-01, PNorm = 154.3422, GNorm = 0.4286
Meta loss on this task batch = 3.2041e-01, Meta loss averaged over last 500 steps = 2.8465e-01, PNorm = 154.3446, GNorm = 0.4291
Meta loss on this task batch = 2.5378e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 154.3468, GNorm = 0.3153
Meta loss on this task batch = 2.7495e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 154.3488, GNorm = 0.3374
Meta loss on this task batch = 2.7555e-01, Meta loss averaged over last 500 steps = 2.8453e-01, PNorm = 154.3505, GNorm = 0.3902
Meta loss on this task batch = 3.3924e-01, Meta loss averaged over last 500 steps = 2.8465e-01, PNorm = 154.3517, GNorm = 0.2869
Meta loss on this task batch = 2.8150e-01, Meta loss averaged over last 500 steps = 2.8463e-01, PNorm = 154.3531, GNorm = 0.3043
Meta loss on this task batch = 2.6174e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 154.3552, GNorm = 0.3343
Meta loss on this task batch = 2.6059e-01, Meta loss averaged over last 500 steps = 2.8434e-01, PNorm = 154.3577, GNorm = 0.2434
Meta loss on this task batch = 2.8535e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 154.3607, GNorm = 0.3022
Meta loss on this task batch = 2.9576e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 154.3642, GNorm = 0.3281
Meta loss on this task batch = 2.9448e-01, Meta loss averaged over last 500 steps = 2.8438e-01, PNorm = 154.3684, GNorm = 0.2459
Meta loss on this task batch = 2.8042e-01, Meta loss averaged over last 500 steps = 2.8430e-01, PNorm = 154.3734, GNorm = 0.2578
Meta loss on this task batch = 3.0347e-01, Meta loss averaged over last 500 steps = 2.8426e-01, PNorm = 154.3784, GNorm = 0.2955
Meta loss on this task batch = 2.6022e-01, Meta loss averaged over last 500 steps = 2.8424e-01, PNorm = 154.3838, GNorm = 0.2541
Meta loss on this task batch = 2.3171e-01, Meta loss averaged over last 500 steps = 2.8407e-01, PNorm = 154.3903, GNorm = 0.3777
Meta loss on this task batch = 3.7055e-01, Meta loss averaged over last 500 steps = 2.8426e-01, PNorm = 154.3965, GNorm = 0.5949
Took 110.95392036437988 seconds to complete one epoch of meta training
Took 118.50566387176514 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456562
Epoch 725
Meta loss on this task batch = 2.9586e-01, Meta loss averaged over last 500 steps = 2.8427e-01, PNorm = 154.4031, GNorm = 0.3866
Meta loss on this task batch = 3.0505e-01, Meta loss averaged over last 500 steps = 2.8433e-01, PNorm = 154.4092, GNorm = 0.3311
Meta loss on this task batch = 3.4012e-01, Meta loss averaged over last 500 steps = 2.8464e-01, PNorm = 154.4140, GNorm = 0.3069
Meta loss on this task batch = 2.7119e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 154.4191, GNorm = 0.2951
Meta loss on this task batch = 3.1750e-01, Meta loss averaged over last 500 steps = 2.8474e-01, PNorm = 154.4242, GNorm = 0.3687
Meta loss on this task batch = 3.1502e-01, Meta loss averaged over last 500 steps = 2.8484e-01, PNorm = 154.4288, GNorm = 0.3530
Meta loss on this task batch = 2.8392e-01, Meta loss averaged over last 500 steps = 2.8486e-01, PNorm = 154.4331, GNorm = 0.2505
Meta loss on this task batch = 2.5225e-01, Meta loss averaged over last 500 steps = 2.8492e-01, PNorm = 154.4378, GNorm = 0.2857
Meta loss on this task batch = 3.1835e-01, Meta loss averaged over last 500 steps = 2.8505e-01, PNorm = 154.4423, GNorm = 0.2834
Meta loss on this task batch = 2.5529e-01, Meta loss averaged over last 500 steps = 2.8491e-01, PNorm = 154.4470, GNorm = 0.3212
Meta loss on this task batch = 3.0485e-01, Meta loss averaged over last 500 steps = 2.8501e-01, PNorm = 154.4513, GNorm = 0.2880
Meta loss on this task batch = 3.1375e-01, Meta loss averaged over last 500 steps = 2.8509e-01, PNorm = 154.4547, GNorm = 0.3539
Meta loss on this task batch = 2.2551e-01, Meta loss averaged over last 500 steps = 2.8492e-01, PNorm = 154.4586, GNorm = 0.2158
Meta loss on this task batch = 2.8837e-01, Meta loss averaged over last 500 steps = 2.8490e-01, PNorm = 154.4628, GNorm = 0.3041
Meta loss on this task batch = 2.6415e-01, Meta loss averaged over last 500 steps = 2.8482e-01, PNorm = 154.4674, GNorm = 0.2936
Meta loss on this task batch = 2.9913e-01, Meta loss averaged over last 500 steps = 2.8484e-01, PNorm = 154.4712, GNorm = 0.4295
Meta loss on this task batch = 2.1349e-01, Meta loss averaged over last 500 steps = 2.8472e-01, PNorm = 154.4749, GNorm = 0.3356
Meta loss on this task batch = 2.0989e-01, Meta loss averaged over last 500 steps = 2.8460e-01, PNorm = 154.4794, GNorm = 0.2402
Meta loss on this task batch = 2.4072e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 154.4834, GNorm = 0.3364
Took 112.76264977455139 seconds to complete one epoch of meta training
Took 121.03864097595215 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451065
Epoch 726
Meta loss on this task batch = 2.6670e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 154.4872, GNorm = 0.2483
Meta loss on this task batch = 2.5699e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 154.4912, GNorm = 0.2601
Meta loss on this task batch = 3.6737e-01, Meta loss averaged over last 500 steps = 2.8459e-01, PNorm = 154.4944, GNorm = 0.3125
Meta loss on this task batch = 3.0330e-01, Meta loss averaged over last 500 steps = 2.8460e-01, PNorm = 154.4974, GNorm = 0.3525
Meta loss on this task batch = 2.8457e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 154.5001, GNorm = 0.2736
Meta loss on this task batch = 2.6753e-01, Meta loss averaged over last 500 steps = 2.8451e-01, PNorm = 154.5025, GNorm = 0.2743
Meta loss on this task batch = 2.8406e-01, Meta loss averaged over last 500 steps = 2.8456e-01, PNorm = 154.5043, GNorm = 0.3444
Meta loss on this task batch = 2.0683e-01, Meta loss averaged over last 500 steps = 2.8449e-01, PNorm = 154.5070, GNorm = 0.2384
Meta loss on this task batch = 2.7573e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 154.5102, GNorm = 0.2824
Meta loss on this task batch = 2.7407e-01, Meta loss averaged over last 500 steps = 2.8445e-01, PNorm = 154.5129, GNorm = 0.2502
Meta loss on this task batch = 2.6404e-01, Meta loss averaged over last 500 steps = 2.8447e-01, PNorm = 154.5156, GNorm = 0.3811
Meta loss on this task batch = 2.9341e-01, Meta loss averaged over last 500 steps = 2.8439e-01, PNorm = 154.5190, GNorm = 0.3148
Meta loss on this task batch = 3.2105e-01, Meta loss averaged over last 500 steps = 2.8458e-01, PNorm = 154.5229, GNorm = 0.4280
Meta loss on this task batch = 2.4711e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 154.5271, GNorm = 0.3835
Meta loss on this task batch = 2.9501e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 154.5310, GNorm = 0.4357
Meta loss on this task batch = 2.4331e-01, Meta loss averaged over last 500 steps = 2.8422e-01, PNorm = 154.5354, GNorm = 0.3426
Meta loss on this task batch = 2.6219e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 154.5401, GNorm = 0.2940
Meta loss on this task batch = 2.6536e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 154.5445, GNorm = 0.2569
Meta loss on this task batch = 2.5473e-01, Meta loss averaged over last 500 steps = 2.8407e-01, PNorm = 154.5480, GNorm = 0.5496
Took 112.6475727558136 seconds to complete one epoch of meta training
Took 120.54217743873596 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459582
Epoch 727
Meta loss on this task batch = 3.0349e-01, Meta loss averaged over last 500 steps = 2.8414e-01, PNorm = 154.5518, GNorm = 0.4342
Meta loss on this task batch = 2.9337e-01, Meta loss averaged over last 500 steps = 2.8411e-01, PNorm = 154.5552, GNorm = 0.2622
Meta loss on this task batch = 3.3105e-01, Meta loss averaged over last 500 steps = 2.8415e-01, PNorm = 154.5587, GNorm = 0.3688
Meta loss on this task batch = 3.4972e-01, Meta loss averaged over last 500 steps = 2.8438e-01, PNorm = 154.5618, GNorm = 0.3265
Meta loss on this task batch = 2.9021e-01, Meta loss averaged over last 500 steps = 2.8434e-01, PNorm = 154.5649, GNorm = 0.3175
Meta loss on this task batch = 2.4093e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 154.5683, GNorm = 0.3322
Meta loss on this task batch = 2.9568e-01, Meta loss averaged over last 500 steps = 2.8414e-01, PNorm = 154.5719, GNorm = 0.2591
Meta loss on this task batch = 2.9821e-01, Meta loss averaged over last 500 steps = 2.8405e-01, PNorm = 154.5757, GNorm = 0.2836
Meta loss on this task batch = 2.8091e-01, Meta loss averaged over last 500 steps = 2.8397e-01, PNorm = 154.5789, GNorm = 0.2599
Meta loss on this task batch = 3.0679e-01, Meta loss averaged over last 500 steps = 2.8400e-01, PNorm = 154.5823, GNorm = 0.3380
Meta loss on this task batch = 2.6429e-01, Meta loss averaged over last 500 steps = 2.8398e-01, PNorm = 154.5860, GNorm = 0.2793
Meta loss on this task batch = 2.4369e-01, Meta loss averaged over last 500 steps = 2.8396e-01, PNorm = 154.5892, GNorm = 0.4170
Meta loss on this task batch = 3.1617e-01, Meta loss averaged over last 500 steps = 2.8403e-01, PNorm = 154.5921, GNorm = 0.3210
Meta loss on this task batch = 2.5877e-01, Meta loss averaged over last 500 steps = 2.8407e-01, PNorm = 154.5958, GNorm = 0.3332
Meta loss on this task batch = 2.1831e-01, Meta loss averaged over last 500 steps = 2.8394e-01, PNorm = 154.5999, GNorm = 0.2584
Meta loss on this task batch = 2.8934e-01, Meta loss averaged over last 500 steps = 2.8394e-01, PNorm = 154.6045, GNorm = 0.2814
Meta loss on this task batch = 2.3922e-01, Meta loss averaged over last 500 steps = 2.8395e-01, PNorm = 154.6096, GNorm = 0.3481
Meta loss on this task batch = 2.6342e-01, Meta loss averaged over last 500 steps = 2.8391e-01, PNorm = 154.6150, GNorm = 0.2679
Meta loss on this task batch = 3.2820e-01, Meta loss averaged over last 500 steps = 2.8409e-01, PNorm = 154.6209, GNorm = 0.4056
Took 115.0923023223877 seconds to complete one epoch of meta training
Took 123.3283257484436 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.442718
Epoch 728
Meta loss on this task batch = 2.5211e-01, Meta loss averaged over last 500 steps = 2.8402e-01, PNorm = 154.6268, GNorm = 0.2563
Meta loss on this task batch = 3.0477e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 154.6323, GNorm = 0.2615
Meta loss on this task batch = 3.0702e-01, Meta loss averaged over last 500 steps = 2.8425e-01, PNorm = 154.6358, GNorm = 0.5050
Meta loss on this task batch = 2.7689e-01, Meta loss averaged over last 500 steps = 2.8424e-01, PNorm = 154.6390, GNorm = 0.2732
Meta loss on this task batch = 2.3594e-01, Meta loss averaged over last 500 steps = 2.8417e-01, PNorm = 154.6424, GNorm = 0.3700
Meta loss on this task batch = 3.6491e-01, Meta loss averaged over last 500 steps = 2.8415e-01, PNorm = 154.6456, GNorm = 0.4573
Meta loss on this task batch = 3.7097e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 154.6472, GNorm = 0.3369
Meta loss on this task batch = 2.9765e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 154.6491, GNorm = 0.3854
Meta loss on this task batch = 2.6796e-01, Meta loss averaged over last 500 steps = 2.8421e-01, PNorm = 154.6511, GNorm = 0.3884
Meta loss on this task batch = 2.5809e-01, Meta loss averaged over last 500 steps = 2.8415e-01, PNorm = 154.6536, GNorm = 0.2445
Meta loss on this task batch = 3.0541e-01, Meta loss averaged over last 500 steps = 2.8423e-01, PNorm = 154.6556, GNorm = 0.3422
Meta loss on this task batch = 3.2705e-01, Meta loss averaged over last 500 steps = 2.8437e-01, PNorm = 154.6572, GNorm = 0.4614
Meta loss on this task batch = 3.0142e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 154.6590, GNorm = 0.3908
Meta loss on this task batch = 2.7077e-01, Meta loss averaged over last 500 steps = 2.8433e-01, PNorm = 154.6611, GNorm = 0.3872
Meta loss on this task batch = 2.5728e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 154.6646, GNorm = 0.3305
Meta loss on this task batch = 2.6485e-01, Meta loss averaged over last 500 steps = 2.8427e-01, PNorm = 154.6688, GNorm = 0.2909
Meta loss on this task batch = 2.6395e-01, Meta loss averaged over last 500 steps = 2.8430e-01, PNorm = 154.6738, GNorm = 0.2750
Meta loss on this task batch = 2.9825e-01, Meta loss averaged over last 500 steps = 2.8423e-01, PNorm = 154.6788, GNorm = 0.2709
Meta loss on this task batch = 2.4308e-01, Meta loss averaged over last 500 steps = 2.8412e-01, PNorm = 154.6842, GNorm = 0.2822
Took 111.94217705726624 seconds to complete one epoch of meta training
Took 118.91323232650757 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452272
Epoch 729
Meta loss on this task batch = 2.5436e-01, Meta loss averaged over last 500 steps = 2.8395e-01, PNorm = 154.6899, GNorm = 0.3030
Meta loss on this task batch = 2.5188e-01, Meta loss averaged over last 500 steps = 2.8389e-01, PNorm = 154.6956, GNorm = 0.2878
Meta loss on this task batch = 2.5498e-01, Meta loss averaged over last 500 steps = 2.8398e-01, PNorm = 154.7015, GNorm = 0.2739
Meta loss on this task batch = 2.4818e-01, Meta loss averaged over last 500 steps = 2.8394e-01, PNorm = 154.7078, GNorm = 0.2825
Meta loss on this task batch = 2.8329e-01, Meta loss averaged over last 500 steps = 2.8384e-01, PNorm = 154.7141, GNorm = 0.3191
Meta loss on this task batch = 3.5169e-01, Meta loss averaged over last 500 steps = 2.8399e-01, PNorm = 154.7203, GNorm = 0.3471
Meta loss on this task batch = 2.7750e-01, Meta loss averaged over last 500 steps = 2.8391e-01, PNorm = 154.7265, GNorm = 0.2997
Meta loss on this task batch = 3.0767e-01, Meta loss averaged over last 500 steps = 2.8399e-01, PNorm = 154.7324, GNorm = 0.2693
Meta loss on this task batch = 3.0815e-01, Meta loss averaged over last 500 steps = 2.8396e-01, PNorm = 154.7385, GNorm = 0.3870
Meta loss on this task batch = 3.1286e-01, Meta loss averaged over last 500 steps = 2.8405e-01, PNorm = 154.7439, GNorm = 0.2630
Meta loss on this task batch = 3.1909e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 154.7492, GNorm = 0.2639
Meta loss on this task batch = 2.7908e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 154.7552, GNorm = 0.2674
Meta loss on this task batch = 2.5597e-01, Meta loss averaged over last 500 steps = 2.8414e-01, PNorm = 154.7614, GNorm = 0.2869
Meta loss on this task batch = 2.7883e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 154.7674, GNorm = 0.3213
Meta loss on this task batch = 2.5310e-01, Meta loss averaged over last 500 steps = 2.8417e-01, PNorm = 154.7732, GNorm = 0.2362
Meta loss on this task batch = 2.6951e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 154.7785, GNorm = 0.4163
Meta loss on this task batch = 2.5434e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 154.7843, GNorm = 0.2684
Meta loss on this task batch = 3.1267e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 154.7894, GNorm = 0.3656
Meta loss on this task batch = 2.8748e-01, Meta loss averaged over last 500 steps = 2.8417e-01, PNorm = 154.7940, GNorm = 0.4171
Took 113.51390433311462 seconds to complete one epoch of meta training
Took 121.47264790534973 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444094
Epoch 730
Meta loss on this task batch = 2.9955e-01, Meta loss averaged over last 500 steps = 2.8422e-01, PNorm = 154.7988, GNorm = 0.3210
Meta loss on this task batch = 3.1626e-01, Meta loss averaged over last 500 steps = 2.8438e-01, PNorm = 154.8036, GNorm = 0.3004
Meta loss on this task batch = 3.1538e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 154.8085, GNorm = 0.3985
Meta loss on this task batch = 3.6716e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 154.8117, GNorm = 0.5375
Meta loss on this task batch = 2.6125e-01, Meta loss averaged over last 500 steps = 2.8434e-01, PNorm = 154.8153, GNorm = 0.2958
Meta loss on this task batch = 2.9344e-01, Meta loss averaged over last 500 steps = 2.8445e-01, PNorm = 154.8193, GNorm = 0.3219
Meta loss on this task batch = 2.9722e-01, Meta loss averaged over last 500 steps = 2.8455e-01, PNorm = 154.8238, GNorm = 0.3818
Meta loss on this task batch = 2.9008e-01, Meta loss averaged over last 500 steps = 2.8455e-01, PNorm = 154.8262, GNorm = 0.4494
Meta loss on this task batch = 2.7397e-01, Meta loss averaged over last 500 steps = 2.8455e-01, PNorm = 154.8290, GNorm = 0.2723
Meta loss on this task batch = 3.0452e-01, Meta loss averaged over last 500 steps = 2.8456e-01, PNorm = 154.8318, GNorm = 0.3223
Meta loss on this task batch = 2.5222e-01, Meta loss averaged over last 500 steps = 2.8460e-01, PNorm = 154.8349, GNorm = 0.2649
Meta loss on this task batch = 2.9277e-01, Meta loss averaged over last 500 steps = 2.8463e-01, PNorm = 154.8380, GNorm = 0.3892
Meta loss on this task batch = 3.5145e-01, Meta loss averaged over last 500 steps = 2.8474e-01, PNorm = 154.8411, GNorm = 0.2967
Meta loss on this task batch = 2.6879e-01, Meta loss averaged over last 500 steps = 2.8474e-01, PNorm = 154.8442, GNorm = 0.3037
Meta loss on this task batch = 2.8339e-01, Meta loss averaged over last 500 steps = 2.8469e-01, PNorm = 154.8473, GNorm = 0.3593
Meta loss on this task batch = 2.9458e-01, Meta loss averaged over last 500 steps = 2.8480e-01, PNorm = 154.8515, GNorm = 0.4397
Meta loss on this task batch = 2.3643e-01, Meta loss averaged over last 500 steps = 2.8462e-01, PNorm = 154.8561, GNorm = 0.2512
Meta loss on this task batch = 3.2887e-01, Meta loss averaged over last 500 steps = 2.8474e-01, PNorm = 154.8602, GNorm = 0.3237
Meta loss on this task batch = 2.7860e-01, Meta loss averaged over last 500 steps = 2.8467e-01, PNorm = 154.8643, GNorm = 0.3355
Took 116.97249484062195 seconds to complete one epoch of meta training
Took 124.86491084098816 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457331
Epoch 731
Meta loss on this task batch = 2.8989e-01, Meta loss averaged over last 500 steps = 2.8470e-01, PNorm = 154.8684, GNorm = 0.2485
Meta loss on this task batch = 2.9083e-01, Meta loss averaged over last 500 steps = 2.8479e-01, PNorm = 154.8727, GNorm = 0.2741
Meta loss on this task batch = 3.0686e-01, Meta loss averaged over last 500 steps = 2.8489e-01, PNorm = 154.8773, GNorm = 0.2399
Meta loss on this task batch = 2.8483e-01, Meta loss averaged over last 500 steps = 2.8482e-01, PNorm = 154.8814, GNorm = 0.2250
Meta loss on this task batch = 2.7960e-01, Meta loss averaged over last 500 steps = 2.8493e-01, PNorm = 154.8855, GNorm = 0.2565
Meta loss on this task batch = 2.6262e-01, Meta loss averaged over last 500 steps = 2.8488e-01, PNorm = 154.8898, GNorm = 0.3133
Meta loss on this task batch = 2.8129e-01, Meta loss averaged over last 500 steps = 2.8484e-01, PNorm = 154.8943, GNorm = 0.2976
Meta loss on this task batch = 2.8504e-01, Meta loss averaged over last 500 steps = 2.8486e-01, PNorm = 154.8987, GNorm = 0.3080
Meta loss on this task batch = 2.6342e-01, Meta loss averaged over last 500 steps = 2.8489e-01, PNorm = 154.9034, GNorm = 0.2775
Meta loss on this task batch = 2.8239e-01, Meta loss averaged over last 500 steps = 2.8485e-01, PNorm = 154.9084, GNorm = 0.2924
Meta loss on this task batch = 3.3081e-01, Meta loss averaged over last 500 steps = 2.8487e-01, PNorm = 154.9133, GNorm = 0.3568
Meta loss on this task batch = 2.7172e-01, Meta loss averaged over last 500 steps = 2.8488e-01, PNorm = 154.9184, GNorm = 0.2578
Meta loss on this task batch = 3.5580e-01, Meta loss averaged over last 500 steps = 2.8508e-01, PNorm = 154.9232, GNorm = 0.3988
Meta loss on this task batch = 2.6221e-01, Meta loss averaged over last 500 steps = 2.8503e-01, PNorm = 154.9283, GNorm = 0.3165
Meta loss on this task batch = 2.5577e-01, Meta loss averaged over last 500 steps = 2.8490e-01, PNorm = 154.9332, GNorm = 0.2626
Meta loss on this task batch = 2.8312e-01, Meta loss averaged over last 500 steps = 2.8487e-01, PNorm = 154.9375, GNorm = 0.2977
Meta loss on this task batch = 3.3506e-01, Meta loss averaged over last 500 steps = 2.8504e-01, PNorm = 154.9420, GNorm = 0.2786
Meta loss on this task batch = 2.9367e-01, Meta loss averaged over last 500 steps = 2.8508e-01, PNorm = 154.9466, GNorm = 0.2665
Meta loss on this task batch = 3.2820e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 154.9512, GNorm = 0.4069
Took 113.46729731559753 seconds to complete one epoch of meta training
Took 121.3390326499939 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454874
Epoch 732
Meta loss on this task batch = 2.8686e-01, Meta loss averaged over last 500 steps = 2.8505e-01, PNorm = 154.9559, GNorm = 0.3001
Meta loss on this task batch = 3.1731e-01, Meta loss averaged over last 500 steps = 2.8517e-01, PNorm = 154.9603, GNorm = 0.3478
Meta loss on this task batch = 2.8004e-01, Meta loss averaged over last 500 steps = 2.8508e-01, PNorm = 154.9647, GNorm = 0.2569
Meta loss on this task batch = 2.8475e-01, Meta loss averaged over last 500 steps = 2.8504e-01, PNorm = 154.9693, GNorm = 0.2684
Meta loss on this task batch = 2.4098e-01, Meta loss averaged over last 500 steps = 2.8494e-01, PNorm = 154.9741, GNorm = 0.2313
Meta loss on this task batch = 3.0333e-01, Meta loss averaged over last 500 steps = 2.8504e-01, PNorm = 154.9776, GNorm = 0.3145
Meta loss on this task batch = 2.9371e-01, Meta loss averaged over last 500 steps = 2.8508e-01, PNorm = 154.9805, GNorm = 0.3132
Meta loss on this task batch = 2.7897e-01, Meta loss averaged over last 500 steps = 2.8509e-01, PNorm = 154.9837, GNorm = 0.3272
Meta loss on this task batch = 2.9014e-01, Meta loss averaged over last 500 steps = 2.8509e-01, PNorm = 154.9867, GNorm = 0.2847
Meta loss on this task batch = 2.6210e-01, Meta loss averaged over last 500 steps = 2.8508e-01, PNorm = 154.9898, GNorm = 0.2593
Meta loss on this task batch = 2.7150e-01, Meta loss averaged over last 500 steps = 2.8514e-01, PNorm = 154.9935, GNorm = 0.3876
Meta loss on this task batch = 2.9152e-01, Meta loss averaged over last 500 steps = 2.8512e-01, PNorm = 154.9970, GNorm = 0.3307
Meta loss on this task batch = 3.2443e-01, Meta loss averaged over last 500 steps = 2.8514e-01, PNorm = 155.0010, GNorm = 0.2943
Meta loss on this task batch = 2.8773e-01, Meta loss averaged over last 500 steps = 2.8516e-01, PNorm = 155.0054, GNorm = 0.3964
Meta loss on this task batch = 3.3020e-01, Meta loss averaged over last 500 steps = 2.8520e-01, PNorm = 155.0082, GNorm = 0.5910
Meta loss on this task batch = 3.0771e-01, Meta loss averaged over last 500 steps = 2.8519e-01, PNorm = 155.0110, GNorm = 0.2619
Meta loss on this task batch = 2.7577e-01, Meta loss averaged over last 500 steps = 2.8518e-01, PNorm = 155.0139, GNorm = 0.2598
Meta loss on this task batch = 2.5567e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 155.0158, GNorm = 0.5281
Meta loss on this task batch = 3.2297e-01, Meta loss averaged over last 500 steps = 2.8508e-01, PNorm = 155.0178, GNorm = 0.4220
Took 111.72518920898438 seconds to complete one epoch of meta training
Took 119.56672072410583 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.440093
Epoch 733
Meta loss on this task batch = 2.5785e-01, Meta loss averaged over last 500 steps = 2.8502e-01, PNorm = 155.0206, GNorm = 0.3685
Meta loss on this task batch = 2.6299e-01, Meta loss averaged over last 500 steps = 2.8501e-01, PNorm = 155.0235, GNorm = 0.2912
Meta loss on this task batch = 3.3486e-01, Meta loss averaged over last 500 steps = 2.8516e-01, PNorm = 155.0261, GNorm = 0.3478
Meta loss on this task batch = 3.1050e-01, Meta loss averaged over last 500 steps = 2.8519e-01, PNorm = 155.0294, GNorm = 0.6390
Meta loss on this task batch = 2.5672e-01, Meta loss averaged over last 500 steps = 2.8519e-01, PNorm = 155.0330, GNorm = 0.2586
Meta loss on this task batch = 2.7300e-01, Meta loss averaged over last 500 steps = 2.8523e-01, PNorm = 155.0376, GNorm = 0.3288
Meta loss on this task batch = 3.1089e-01, Meta loss averaged over last 500 steps = 2.8520e-01, PNorm = 155.0419, GNorm = 0.2669
Meta loss on this task batch = 2.8504e-01, Meta loss averaged over last 500 steps = 2.8522e-01, PNorm = 155.0463, GNorm = 0.2373
Meta loss on this task batch = 2.8532e-01, Meta loss averaged over last 500 steps = 2.8520e-01, PNorm = 155.0503, GNorm = 0.3360
Meta loss on this task batch = 2.8615e-01, Meta loss averaged over last 500 steps = 2.8515e-01, PNorm = 155.0540, GNorm = 0.3046
Meta loss on this task batch = 3.0904e-01, Meta loss averaged over last 500 steps = 2.8524e-01, PNorm = 155.0572, GNorm = 0.2830
Meta loss on this task batch = 2.8583e-01, Meta loss averaged over last 500 steps = 2.8521e-01, PNorm = 155.0605, GNorm = 0.3495
Meta loss on this task batch = 3.1187e-01, Meta loss averaged over last 500 steps = 2.8527e-01, PNorm = 155.0631, GNorm = 0.3766
Meta loss on this task batch = 3.1063e-01, Meta loss averaged over last 500 steps = 2.8542e-01, PNorm = 155.0658, GNorm = 0.3352
Meta loss on this task batch = 3.0708e-01, Meta loss averaged over last 500 steps = 2.8551e-01, PNorm = 155.0691, GNorm = 0.2818
Meta loss on this task batch = 3.1034e-01, Meta loss averaged over last 500 steps = 2.8557e-01, PNorm = 155.0717, GNorm = 0.2705
Meta loss on this task batch = 3.1693e-01, Meta loss averaged over last 500 steps = 2.8567e-01, PNorm = 155.0751, GNorm = 0.3014
Meta loss on this task batch = 2.9454e-01, Meta loss averaged over last 500 steps = 2.8544e-01, PNorm = 155.0790, GNorm = 0.3975
Meta loss on this task batch = 2.5926e-01, Meta loss averaged over last 500 steps = 2.8543e-01, PNorm = 155.0839, GNorm = 0.3776
Took 113.78669714927673 seconds to complete one epoch of meta training
Took 121.32704591751099 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.441469
Epoch 734
Meta loss on this task batch = 2.9957e-01, Meta loss averaged over last 500 steps = 2.8548e-01, PNorm = 155.0883, GNorm = 0.2803
Meta loss on this task batch = 2.6156e-01, Meta loss averaged over last 500 steps = 2.8547e-01, PNorm = 155.0932, GNorm = 0.3421
Meta loss on this task batch = 2.9933e-01, Meta loss averaged over last 500 steps = 2.8542e-01, PNorm = 155.0980, GNorm = 0.2950
Meta loss on this task batch = 3.7586e-01, Meta loss averaged over last 500 steps = 2.8559e-01, PNorm = 155.1016, GNorm = 0.3235
Meta loss on this task batch = 2.5674e-01, Meta loss averaged over last 500 steps = 2.8555e-01, PNorm = 155.1048, GNorm = 0.2958
Meta loss on this task batch = 2.7762e-01, Meta loss averaged over last 500 steps = 2.8560e-01, PNorm = 155.1082, GNorm = 0.2646
Meta loss on this task batch = 3.1063e-01, Meta loss averaged over last 500 steps = 2.8571e-01, PNorm = 155.1115, GNorm = 0.3231
Meta loss on this task batch = 3.1394e-01, Meta loss averaged over last 500 steps = 2.8578e-01, PNorm = 155.1146, GNorm = 0.3813
Meta loss on this task batch = 2.5216e-01, Meta loss averaged over last 500 steps = 2.8563e-01, PNorm = 155.1181, GNorm = 0.2652
Meta loss on this task batch = 3.1845e-01, Meta loss averaged over last 500 steps = 2.8578e-01, PNorm = 155.1220, GNorm = 0.4255
Meta loss on this task batch = 2.1467e-01, Meta loss averaged over last 500 steps = 2.8568e-01, PNorm = 155.1264, GNorm = 0.2361
Meta loss on this task batch = 3.3280e-01, Meta loss averaged over last 500 steps = 2.8586e-01, PNorm = 155.1303, GNorm = 0.3447
Meta loss on this task batch = 2.4306e-01, Meta loss averaged over last 500 steps = 2.8582e-01, PNorm = 155.1348, GNorm = 0.3256
Meta loss on this task batch = 3.3721e-01, Meta loss averaged over last 500 steps = 2.8589e-01, PNorm = 155.1391, GNorm = 0.4634
Meta loss on this task batch = 3.1447e-01, Meta loss averaged over last 500 steps = 2.8597e-01, PNorm = 155.1433, GNorm = 0.3457
Meta loss on this task batch = 3.2230e-01, Meta loss averaged over last 500 steps = 2.8589e-01, PNorm = 155.1475, GNorm = 0.3094
Meta loss on this task batch = 2.9751e-01, Meta loss averaged over last 500 steps = 2.8589e-01, PNorm = 155.1519, GNorm = 0.2812
Meta loss on this task batch = 2.9666e-01, Meta loss averaged over last 500 steps = 2.8589e-01, PNorm = 155.1557, GNorm = 0.3703
Meta loss on this task batch = 3.4593e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 155.1598, GNorm = 0.3983
Took 111.0306077003479 seconds to complete one epoch of meta training
Took 119.34778881072998 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456904
Epoch 735
Meta loss on this task batch = 3.2297e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 155.1632, GNorm = 0.3054
Meta loss on this task batch = 2.8375e-01, Meta loss averaged over last 500 steps = 2.8602e-01, PNorm = 155.1666, GNorm = 0.3229
Meta loss on this task batch = 2.7572e-01, Meta loss averaged over last 500 steps = 2.8600e-01, PNorm = 155.1693, GNorm = 0.3001
Meta loss on this task batch = 2.5179e-01, Meta loss averaged over last 500 steps = 2.8601e-01, PNorm = 155.1721, GNorm = 0.3404
Meta loss on this task batch = 2.1092e-01, Meta loss averaged over last 500 steps = 2.8585e-01, PNorm = 155.1756, GNorm = 0.2376
Meta loss on this task batch = 3.0703e-01, Meta loss averaged over last 500 steps = 2.8591e-01, PNorm = 155.1783, GNorm = 0.4032
Meta loss on this task batch = 2.8736e-01, Meta loss averaged over last 500 steps = 2.8601e-01, PNorm = 155.1810, GNorm = 0.3276
Meta loss on this task batch = 3.4755e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 155.1837, GNorm = 0.3074
Meta loss on this task batch = 2.7897e-01, Meta loss averaged over last 500 steps = 2.8602e-01, PNorm = 155.1873, GNorm = 0.3328
Meta loss on this task batch = 2.9255e-01, Meta loss averaged over last 500 steps = 2.8621e-01, PNorm = 155.1906, GNorm = 0.3387
Meta loss on this task batch = 3.1013e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 155.1937, GNorm = 0.3308
Meta loss on this task batch = 3.2329e-01, Meta loss averaged over last 500 steps = 2.8645e-01, PNorm = 155.1968, GNorm = 0.3214
Meta loss on this task batch = 2.6009e-01, Meta loss averaged over last 500 steps = 2.8628e-01, PNorm = 155.1999, GNorm = 0.2584
Meta loss on this task batch = 2.8395e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 155.2035, GNorm = 0.3245
Meta loss on this task batch = 2.4025e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 155.2073, GNorm = 0.2596
Meta loss on this task batch = 2.8010e-01, Meta loss averaged over last 500 steps = 2.8612e-01, PNorm = 155.2118, GNorm = 0.2356
Meta loss on this task batch = 3.1985e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 155.2162, GNorm = 0.2817
Meta loss on this task batch = 2.7729e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 155.2210, GNorm = 0.2389
Meta loss on this task batch = 3.4649e-01, Meta loss averaged over last 500 steps = 2.8628e-01, PNorm = 155.2249, GNorm = 0.3654
Took 114.97108316421509 seconds to complete one epoch of meta training
Took 122.11616349220276 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454599
Epoch 736
Meta loss on this task batch = 2.6559e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 155.2284, GNorm = 0.3949
Meta loss on this task batch = 3.4784e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 155.2306, GNorm = 0.3230
Meta loss on this task batch = 2.2810e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 155.2334, GNorm = 0.2493
Meta loss on this task batch = 3.1197e-01, Meta loss averaged over last 500 steps = 2.8611e-01, PNorm = 155.2358, GNorm = 0.2770
Meta loss on this task batch = 3.3481e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 155.2384, GNorm = 0.2842
Meta loss on this task batch = 2.8208e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 155.2411, GNorm = 0.2804
Meta loss on this task batch = 2.6343e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 155.2443, GNorm = 0.2865
Meta loss on this task batch = 2.8456e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 155.2478, GNorm = 0.2491
Meta loss on this task batch = 3.0653e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 155.2517, GNorm = 0.2966
Meta loss on this task batch = 3.4031e-01, Meta loss averaged over last 500 steps = 2.8643e-01, PNorm = 155.2551, GNorm = 0.3328
Meta loss on this task batch = 3.0118e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 155.2586, GNorm = 0.2643
Meta loss on this task batch = 2.7791e-01, Meta loss averaged over last 500 steps = 2.8632e-01, PNorm = 155.2620, GNorm = 0.4806
Meta loss on this task batch = 2.6672e-01, Meta loss averaged over last 500 steps = 2.8631e-01, PNorm = 155.2657, GNorm = 0.2963
Meta loss on this task batch = 3.2191e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 155.2695, GNorm = 0.2808
Meta loss on this task batch = 3.0318e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 155.2716, GNorm = 0.4430
Meta loss on this task batch = 3.4068e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 155.2738, GNorm = 0.3119
Meta loss on this task batch = 2.7801e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 155.2767, GNorm = 0.3438
Meta loss on this task batch = 2.3005e-01, Meta loss averaged over last 500 steps = 2.8643e-01, PNorm = 155.2810, GNorm = 0.2551
Meta loss on this task batch = 3.3375e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 155.2846, GNorm = 0.3425
Took 117.39722871780396 seconds to complete one epoch of meta training
Took 125.05228161811829 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475236
Epoch 737
Meta loss on this task batch = 3.3792e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 155.2878, GNorm = 0.3160
Meta loss on this task batch = 2.9265e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 155.2912, GNorm = 0.3116
Meta loss on this task batch = 2.5203e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 155.2948, GNorm = 0.2751
Meta loss on this task batch = 2.7790e-01, Meta loss averaged over last 500 steps = 2.8645e-01, PNorm = 155.2987, GNorm = 0.2583
Meta loss on this task batch = 3.1621e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 155.3021, GNorm = 0.2879
Meta loss on this task batch = 2.7584e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 155.3052, GNorm = 0.2786
Meta loss on this task batch = 2.7558e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 155.3080, GNorm = 0.2791
Meta loss on this task batch = 3.5841e-01, Meta loss averaged over last 500 steps = 2.8675e-01, PNorm = 155.3103, GNorm = 0.3357
Meta loss on this task batch = 3.0952e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 155.3126, GNorm = 0.3077
Meta loss on this task batch = 2.5232e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 155.3152, GNorm = 0.3254
Meta loss on this task batch = 2.7474e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 155.3185, GNorm = 0.2832
Meta loss on this task batch = 2.7415e-01, Meta loss averaged over last 500 steps = 2.8637e-01, PNorm = 155.3229, GNorm = 0.3954
Meta loss on this task batch = 2.7331e-01, Meta loss averaged over last 500 steps = 2.8640e-01, PNorm = 155.3267, GNorm = 0.3362
Meta loss on this task batch = 2.4531e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 155.3310, GNorm = 0.2617
Meta loss on this task batch = 3.5149e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 155.3352, GNorm = 0.3897
Meta loss on this task batch = 1.9827e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 155.3403, GNorm = 0.2897
Meta loss on this task batch = 2.5009e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 155.3459, GNorm = 0.3911
Meta loss on this task batch = 2.7504e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 155.3512, GNorm = 0.3179
Meta loss on this task batch = 2.1632e-01, Meta loss averaged over last 500 steps = 2.8628e-01, PNorm = 155.3564, GNorm = 0.2394
Took 112.54366159439087 seconds to complete one epoch of meta training
Took 120.28616499900818 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467293
Epoch 738
Meta loss on this task batch = 2.2950e-01, Meta loss averaged over last 500 steps = 2.8627e-01, PNorm = 155.3613, GNorm = 0.3901
Meta loss on this task batch = 3.1269e-01, Meta loss averaged over last 500 steps = 2.8632e-01, PNorm = 155.3653, GNorm = 0.5962
Meta loss on this task batch = 3.2664e-01, Meta loss averaged over last 500 steps = 2.8641e-01, PNorm = 155.3687, GNorm = 0.3519
Meta loss on this task batch = 3.2440e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 155.3715, GNorm = 0.3382
Meta loss on this task batch = 3.0850e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 155.3747, GNorm = 0.3346
Meta loss on this task batch = 2.5450e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 155.3779, GNorm = 0.2651
Meta loss on this task batch = 3.1323e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 155.3807, GNorm = 0.3616
Meta loss on this task batch = 2.9312e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 155.3833, GNorm = 0.2966
Meta loss on this task batch = 3.1135e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 155.3854, GNorm = 0.2849
Meta loss on this task batch = 2.7940e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 155.3882, GNorm = 0.2768
Meta loss on this task batch = 2.9795e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 155.3910, GNorm = 0.3379
Meta loss on this task batch = 3.0046e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 155.3940, GNorm = 0.3532
Meta loss on this task batch = 2.8137e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 155.3973, GNorm = 0.4147
Meta loss on this task batch = 2.9214e-01, Meta loss averaged over last 500 steps = 2.8672e-01, PNorm = 155.4009, GNorm = 0.3059
Meta loss on this task batch = 2.4659e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 155.4043, GNorm = 0.3649
Meta loss on this task batch = 3.2173e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 155.4076, GNorm = 0.3469
Meta loss on this task batch = 2.7183e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 155.4110, GNorm = 0.3541
Meta loss on this task batch = 3.0121e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 155.4145, GNorm = 0.2529
Meta loss on this task batch = 2.5753e-01, Meta loss averaged over last 500 steps = 2.8668e-01, PNorm = 155.4180, GNorm = 0.3102
Took 112.97510933876038 seconds to complete one epoch of meta training
Took 120.8651168346405 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483155
Epoch 739
Meta loss on this task batch = 2.3792e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 155.4212, GNorm = 0.2730
Meta loss on this task batch = 2.6889e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 155.4241, GNorm = 0.5156
Meta loss on this task batch = 2.7389e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 155.4270, GNorm = 0.2522
Meta loss on this task batch = 2.7075e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 155.4293, GNorm = 0.3295
Meta loss on this task batch = 2.8233e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 155.4320, GNorm = 0.2760
Meta loss on this task batch = 2.7707e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 155.4347, GNorm = 0.2453
Meta loss on this task batch = 3.2253e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 155.4376, GNorm = 0.3237
Meta loss on this task batch = 3.2137e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 155.4407, GNorm = 0.2870
Meta loss on this task batch = 3.3997e-01, Meta loss averaged over last 500 steps = 2.8700e-01, PNorm = 155.4440, GNorm = 0.3671
Meta loss on this task batch = 2.9437e-01, Meta loss averaged over last 500 steps = 2.8697e-01, PNorm = 155.4461, GNorm = 0.4317
Meta loss on this task batch = 2.7643e-01, Meta loss averaged over last 500 steps = 2.8692e-01, PNorm = 155.4482, GNorm = 0.3117
Meta loss on this task batch = 3.3561e-01, Meta loss averaged over last 500 steps = 2.8713e-01, PNorm = 155.4501, GNorm = 0.2803
Meta loss on this task batch = 2.9697e-01, Meta loss averaged over last 500 steps = 2.8718e-01, PNorm = 155.4529, GNorm = 0.3490
Meta loss on this task batch = 2.5483e-01, Meta loss averaged over last 500 steps = 2.8715e-01, PNorm = 155.4566, GNorm = 0.2543
Meta loss on this task batch = 2.6301e-01, Meta loss averaged over last 500 steps = 2.8701e-01, PNorm = 155.4603, GNorm = 0.2596
Meta loss on this task batch = 3.3862e-01, Meta loss averaged over last 500 steps = 2.8713e-01, PNorm = 155.4640, GNorm = 0.3885
Meta loss on this task batch = 2.8156e-01, Meta loss averaged over last 500 steps = 2.8708e-01, PNorm = 155.4682, GNorm = 0.2650
Meta loss on this task batch = 2.8498e-01, Meta loss averaged over last 500 steps = 2.8719e-01, PNorm = 155.4721, GNorm = 0.2670
Meta loss on this task batch = 2.3328e-01, Meta loss averaged over last 500 steps = 2.8705e-01, PNorm = 155.4766, GNorm = 0.2977
Took 111.7927520275116 seconds to complete one epoch of meta training
Took 120.21084332466125 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470377
Epoch 740
Meta loss on this task batch = 2.6547e-01, Meta loss averaged over last 500 steps = 2.8706e-01, PNorm = 155.4810, GNorm = 0.3445
Meta loss on this task batch = 2.4769e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 155.4852, GNorm = 0.2277
Meta loss on this task batch = 3.1090e-01, Meta loss averaged over last 500 steps = 2.8708e-01, PNorm = 155.4890, GNorm = 0.4184
Meta loss on this task batch = 3.1487e-01, Meta loss averaged over last 500 steps = 2.8715e-01, PNorm = 155.4921, GNorm = 0.4447
Meta loss on this task batch = 3.1160e-01, Meta loss averaged over last 500 steps = 2.8719e-01, PNorm = 155.4954, GNorm = 0.3350
Meta loss on this task batch = 2.2356e-01, Meta loss averaged over last 500 steps = 2.8701e-01, PNorm = 155.4987, GNorm = 0.2323
Meta loss on this task batch = 3.1119e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 155.5018, GNorm = 0.4676
Meta loss on this task batch = 2.2491e-01, Meta loss averaged over last 500 steps = 2.8695e-01, PNorm = 155.5054, GNorm = 0.2659
Meta loss on this task batch = 2.5208e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 155.5093, GNorm = 0.2126
Meta loss on this task batch = 3.1910e-01, Meta loss averaged over last 500 steps = 2.8696e-01, PNorm = 155.5130, GNorm = 0.3459
Meta loss on this task batch = 3.2876e-01, Meta loss averaged over last 500 steps = 2.8693e-01, PNorm = 155.5162, GNorm = 0.2906
Meta loss on this task batch = 2.7608e-01, Meta loss averaged over last 500 steps = 2.8691e-01, PNorm = 155.5187, GNorm = 0.3414
Meta loss on this task batch = 2.5140e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 155.5218, GNorm = 0.3346
Meta loss on this task batch = 2.7990e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 155.5245, GNorm = 0.2970
Meta loss on this task batch = 2.7354e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 155.5269, GNorm = 0.2789
Meta loss on this task batch = 2.9951e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 155.5297, GNorm = 0.3045
Meta loss on this task batch = 2.7690e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 155.5327, GNorm = 0.2668
Meta loss on this task batch = 2.8419e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 155.5357, GNorm = 0.3803
Meta loss on this task batch = 3.3479e-01, Meta loss averaged over last 500 steps = 2.8682e-01, PNorm = 155.5371, GNorm = 0.5183
Took 111.93274164199829 seconds to complete one epoch of meta training
Took 119.58438992500305 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494160
Epoch 741
Meta loss on this task batch = 2.9213e-01, Meta loss averaged over last 500 steps = 2.8690e-01, PNorm = 155.5388, GNorm = 0.3151
Meta loss on this task batch = 2.6520e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 155.5402, GNorm = 0.2782
Meta loss on this task batch = 3.1334e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 155.5419, GNorm = 0.3300
Meta loss on this task batch = 3.1861e-01, Meta loss averaged over last 500 steps = 2.8690e-01, PNorm = 155.5439, GNorm = 0.3143
Meta loss on this task batch = 2.8893e-01, Meta loss averaged over last 500 steps = 2.8695e-01, PNorm = 155.5471, GNorm = 0.4216
Meta loss on this task batch = 2.9567e-01, Meta loss averaged over last 500 steps = 2.8696e-01, PNorm = 155.5511, GNorm = 0.2629
Meta loss on this task batch = 2.9555e-01, Meta loss averaged over last 500 steps = 2.8705e-01, PNorm = 155.5553, GNorm = 0.2841
Meta loss on this task batch = 2.7631e-01, Meta loss averaged over last 500 steps = 2.8697e-01, PNorm = 155.5599, GNorm = 0.3305
Meta loss on this task batch = 2.2627e-01, Meta loss averaged over last 500 steps = 2.8687e-01, PNorm = 155.5655, GNorm = 0.2693
Meta loss on this task batch = 2.7252e-01, Meta loss averaged over last 500 steps = 2.8683e-01, PNorm = 155.5717, GNorm = 0.3256
Meta loss on this task batch = 2.6859e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 155.5779, GNorm = 0.2947
Meta loss on this task batch = 3.0359e-01, Meta loss averaged over last 500 steps = 2.8690e-01, PNorm = 155.5839, GNorm = 0.2478
Meta loss on this task batch = 2.5339e-01, Meta loss averaged over last 500 steps = 2.8690e-01, PNorm = 155.5907, GNorm = 0.3304
Meta loss on this task batch = 2.4806e-01, Meta loss averaged over last 500 steps = 2.8688e-01, PNorm = 155.5968, GNorm = 0.2992
Meta loss on this task batch = 2.8550e-01, Meta loss averaged over last 500 steps = 2.8703e-01, PNorm = 155.6031, GNorm = 0.2792
Meta loss on this task batch = 2.7433e-01, Meta loss averaged over last 500 steps = 2.8703e-01, PNorm = 155.6088, GNorm = 0.2884
Meta loss on this task batch = 2.6852e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 155.6145, GNorm = 0.2752
Meta loss on this task batch = 2.5198e-01, Meta loss averaged over last 500 steps = 2.8677e-01, PNorm = 155.6197, GNorm = 0.2185
Meta loss on this task batch = 2.7321e-01, Meta loss averaged over last 500 steps = 2.8677e-01, PNorm = 155.6250, GNorm = 0.3823
Took 112.72949075698853 seconds to complete one epoch of meta training
Took 119.73245525360107 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479555
Epoch 742
Meta loss on this task batch = 2.4158e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 155.6305, GNorm = 0.2325
Meta loss on this task batch = 2.9578e-01, Meta loss averaged over last 500 steps = 2.8668e-01, PNorm = 155.6348, GNorm = 0.3068
Meta loss on this task batch = 2.6567e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 155.6385, GNorm = 0.2415
Meta loss on this task batch = 2.3759e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 155.6421, GNorm = 0.3949
Meta loss on this task batch = 3.1620e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 155.6450, GNorm = 0.4077
Meta loss on this task batch = 2.9202e-01, Meta loss averaged over last 500 steps = 2.8637e-01, PNorm = 155.6483, GNorm = 0.3707
Meta loss on this task batch = 2.6638e-01, Meta loss averaged over last 500 steps = 2.8640e-01, PNorm = 155.6514, GNorm = 0.3757
Meta loss on this task batch = 3.1167e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 155.6541, GNorm = 0.3095
Meta loss on this task batch = 3.0798e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 155.6563, GNorm = 0.3179
Meta loss on this task batch = 2.8949e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 155.6590, GNorm = 0.2410
Meta loss on this task batch = 2.4267e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 155.6622, GNorm = 0.2707
Meta loss on this task batch = 2.9678e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 155.6651, GNorm = 0.3112
Meta loss on this task batch = 2.1320e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 155.6682, GNorm = 0.2292
Meta loss on this task batch = 2.9293e-01, Meta loss averaged over last 500 steps = 2.8645e-01, PNorm = 155.6725, GNorm = 0.4077
Meta loss on this task batch = 3.0153e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 155.6765, GNorm = 0.3642
Meta loss on this task batch = 3.5489e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 155.6804, GNorm = 0.3881
Meta loss on this task batch = 2.7507e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 155.6850, GNorm = 0.2995
Meta loss on this task batch = 2.3750e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 155.6905, GNorm = 0.2872
Meta loss on this task batch = 3.3717e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 155.6968, GNorm = 0.5671
Took 112.28131294250488 seconds to complete one epoch of meta training
Took 120.30669236183167 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451508
Epoch 743
Meta loss on this task batch = 2.3872e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 155.7032, GNorm = 0.2254
Meta loss on this task batch = 2.9946e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 155.7094, GNorm = 0.3061
Meta loss on this task batch = 3.7462e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 155.7151, GNorm = 0.3570
Meta loss on this task batch = 2.8601e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 155.7203, GNorm = 0.2946
Meta loss on this task batch = 2.7341e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 155.7262, GNorm = 0.4389
Meta loss on this task batch = 2.8874e-01, Meta loss averaged over last 500 steps = 2.8633e-01, PNorm = 155.7319, GNorm = 0.5228
Meta loss on this task batch = 2.5844e-01, Meta loss averaged over last 500 steps = 2.8639e-01, PNorm = 155.7372, GNorm = 0.3342
Meta loss on this task batch = 2.7561e-01, Meta loss averaged over last 500 steps = 2.8643e-01, PNorm = 155.7420, GNorm = 0.2816
Meta loss on this task batch = 2.6629e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 155.7467, GNorm = 0.2699
Meta loss on this task batch = 2.9311e-01, Meta loss averaged over last 500 steps = 2.8645e-01, PNorm = 155.7510, GNorm = 0.3312
Meta loss on this task batch = 2.4881e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 155.7554, GNorm = 0.3190
Meta loss on this task batch = 2.7206e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 155.7595, GNorm = 0.2798
Meta loss on this task batch = 3.1032e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 155.7637, GNorm = 0.2527
Meta loss on this task batch = 2.7535e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 155.7679, GNorm = 0.2590
Meta loss on this task batch = 2.8058e-01, Meta loss averaged over last 500 steps = 2.8631e-01, PNorm = 155.7721, GNorm = 0.2757
Meta loss on this task batch = 3.0478e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 155.7757, GNorm = 0.3648
Meta loss on this task batch = 2.8896e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 155.7796, GNorm = 0.2798
Meta loss on this task batch = 2.9348e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 155.7834, GNorm = 0.3220
Meta loss on this task batch = 2.6203e-01, Meta loss averaged over last 500 steps = 2.8643e-01, PNorm = 155.7879, GNorm = 0.3233
Took 110.47102737426758 seconds to complete one epoch of meta training
Took 118.48060870170593 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486270
Epoch 744
Meta loss on this task batch = 3.0613e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 155.7925, GNorm = 0.2880
Meta loss on this task batch = 3.2490e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 155.7967, GNorm = 0.2899
Meta loss on this task batch = 2.8163e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 155.8007, GNorm = 0.2805
Meta loss on this task batch = 2.4899e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 155.8054, GNorm = 0.2523
Meta loss on this task batch = 3.1875e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 155.8098, GNorm = 0.2783
Meta loss on this task batch = 3.0665e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 155.8141, GNorm = 0.2995
Meta loss on this task batch = 2.6638e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 155.8181, GNorm = 0.2716
Meta loss on this task batch = 2.7694e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 155.8226, GNorm = 0.3023
Meta loss on this task batch = 2.6463e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 155.8275, GNorm = 0.3063
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 155.8322, GNorm = 0.3276
Meta loss on this task batch = 2.7191e-01, Meta loss averaged over last 500 steps = 2.8641e-01, PNorm = 155.8367, GNorm = 0.4050
Meta loss on this task batch = 2.6701e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 155.8417, GNorm = 0.2216
Meta loss on this task batch = 2.6046e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 155.8467, GNorm = 0.2474
Meta loss on this task batch = 2.6857e-01, Meta loss averaged over last 500 steps = 2.8624e-01, PNorm = 155.8510, GNorm = 0.3071
Meta loss on this task batch = 3.1966e-01, Meta loss averaged over last 500 steps = 2.8631e-01, PNorm = 155.8548, GNorm = 0.2741
Meta loss on this task batch = 3.2136e-01, Meta loss averaged over last 500 steps = 2.8625e-01, PNorm = 155.8584, GNorm = 0.3251
Meta loss on this task batch = 3.0906e-01, Meta loss averaged over last 500 steps = 2.8631e-01, PNorm = 155.8616, GNorm = 0.2635
Meta loss on this task batch = 2.2275e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 155.8651, GNorm = 0.2346
Meta loss on this task batch = 2.6008e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 155.8688, GNorm = 0.3065
Took 109.2973883152008 seconds to complete one epoch of meta training
Took 117.31518363952637 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478715
Epoch 745
Meta loss on this task batch = 2.4276e-01, Meta loss averaged over last 500 steps = 2.8616e-01, PNorm = 155.8723, GNorm = 0.2426
Meta loss on this task batch = 2.6625e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 155.8761, GNorm = 0.2597
Meta loss on this task batch = 2.5121e-01, Meta loss averaged over last 500 steps = 2.8603e-01, PNorm = 155.8798, GNorm = 0.2341
Meta loss on this task batch = 2.7897e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 155.8827, GNorm = 0.2857
Meta loss on this task batch = 2.8086e-01, Meta loss averaged over last 500 steps = 2.8608e-01, PNorm = 155.8866, GNorm = 0.6166
Meta loss on this task batch = 2.5508e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 155.8904, GNorm = 0.2416
Meta loss on this task batch = 3.2199e-01, Meta loss averaged over last 500 steps = 2.8618e-01, PNorm = 155.8941, GNorm = 0.4794
Meta loss on this task batch = 3.1632e-01, Meta loss averaged over last 500 steps = 2.8634e-01, PNorm = 155.8978, GNorm = 0.2553
Meta loss on this task batch = 2.5139e-01, Meta loss averaged over last 500 steps = 2.8631e-01, PNorm = 155.9014, GNorm = 0.2417
Meta loss on this task batch = 2.8879e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 155.9055, GNorm = 0.3123
Meta loss on this task batch = 2.4597e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 155.9106, GNorm = 0.2756
Meta loss on this task batch = 3.0606e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 155.9161, GNorm = 0.3382
Meta loss on this task batch = 2.5934e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 155.9216, GNorm = 0.2925
Meta loss on this task batch = 2.6205e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 155.9264, GNorm = 0.3036
Meta loss on this task batch = 2.7963e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 155.9316, GNorm = 0.3671
Meta loss on this task batch = 2.3418e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 155.9369, GNorm = 0.2910
Meta loss on this task batch = 2.5963e-01, Meta loss averaged over last 500 steps = 2.8602e-01, PNorm = 155.9424, GNorm = 0.2249
Meta loss on this task batch = 3.4959e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 155.9474, GNorm = 0.3210
Meta loss on this task batch = 3.2088e-01, Meta loss averaged over last 500 steps = 2.8616e-01, PNorm = 155.9511, GNorm = 0.6248
Took 112.10469508171082 seconds to complete one epoch of meta training
Took 118.99030232429504 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468193
Epoch 746
Meta loss on this task batch = 2.9213e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 155.9548, GNorm = 0.3120
Meta loss on this task batch = 2.7597e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 155.9590, GNorm = 0.3080
Meta loss on this task batch = 2.7065e-01, Meta loss averaged over last 500 steps = 2.8628e-01, PNorm = 155.9631, GNorm = 0.2686
Meta loss on this task batch = 2.8276e-01, Meta loss averaged over last 500 steps = 2.8627e-01, PNorm = 155.9674, GNorm = 0.2546
Meta loss on this task batch = 3.3032e-01, Meta loss averaged over last 500 steps = 2.8629e-01, PNorm = 155.9718, GNorm = 0.3254
Meta loss on this task batch = 2.4086e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 155.9763, GNorm = 0.2510
Meta loss on this task batch = 2.5069e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 155.9808, GNorm = 0.3425
Meta loss on this task batch = 2.6543e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 155.9853, GNorm = 0.2512
Meta loss on this task batch = 2.2831e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 155.9901, GNorm = 0.3792
Meta loss on this task batch = 2.5812e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 155.9955, GNorm = 0.2680
Meta loss on this task batch = 2.5705e-01, Meta loss averaged over last 500 steps = 2.8604e-01, PNorm = 156.0005, GNorm = 0.2248
Meta loss on this task batch = 2.5276e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 156.0057, GNorm = 0.2625
Meta loss on this task batch = 3.2531e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 156.0098, GNorm = 0.3297
Meta loss on this task batch = 3.0392e-01, Meta loss averaged over last 500 steps = 2.8627e-01, PNorm = 156.0135, GNorm = 0.3829
Meta loss on this task batch = 3.1510e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 156.0162, GNorm = 0.3686
Meta loss on this task batch = 2.5839e-01, Meta loss averaged over last 500 steps = 2.8616e-01, PNorm = 156.0193, GNorm = 0.2424
Meta loss on this task batch = 2.9747e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 156.0219, GNorm = 0.2845
Meta loss on this task batch = 2.6303e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 156.0245, GNorm = 0.3034
Meta loss on this task batch = 2.3332e-01, Meta loss averaged over last 500 steps = 2.8613e-01, PNorm = 156.0273, GNorm = 0.3142
Took 111.30034232139587 seconds to complete one epoch of meta training
Took 119.07668900489807 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479306
Epoch 747
Meta loss on this task batch = 2.5934e-01, Meta loss averaged over last 500 steps = 2.8599e-01, PNorm = 156.0302, GNorm = 0.2576
Meta loss on this task batch = 3.1132e-01, Meta loss averaged over last 500 steps = 2.8611e-01, PNorm = 156.0323, GNorm = 0.2981
Meta loss on this task batch = 3.2267e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 156.0345, GNorm = 0.3053
Meta loss on this task batch = 2.7432e-01, Meta loss averaged over last 500 steps = 2.8594e-01, PNorm = 156.0372, GNorm = 0.3892
Meta loss on this task batch = 2.7299e-01, Meta loss averaged over last 500 steps = 2.8587e-01, PNorm = 156.0404, GNorm = 0.4703
Meta loss on this task batch = 2.4227e-01, Meta loss averaged over last 500 steps = 2.8579e-01, PNorm = 156.0441, GNorm = 0.2584
Meta loss on this task batch = 2.5223e-01, Meta loss averaged over last 500 steps = 2.8580e-01, PNorm = 156.0483, GNorm = 0.2487
Meta loss on this task batch = 2.6027e-01, Meta loss averaged over last 500 steps = 2.8582e-01, PNorm = 156.0522, GNorm = 0.2783
Meta loss on this task batch = 2.9258e-01, Meta loss averaged over last 500 steps = 2.8580e-01, PNorm = 156.0559, GNorm = 0.2665
Meta loss on this task batch = 2.7985e-01, Meta loss averaged over last 500 steps = 2.8575e-01, PNorm = 156.0597, GNorm = 0.3071
Meta loss on this task batch = 3.0763e-01, Meta loss averaged over last 500 steps = 2.8587e-01, PNorm = 156.0632, GNorm = 0.3427
Meta loss on this task batch = 2.8417e-01, Meta loss averaged over last 500 steps = 2.8582e-01, PNorm = 156.0663, GNorm = 0.2946
Meta loss on this task batch = 2.7112e-01, Meta loss averaged over last 500 steps = 2.8588e-01, PNorm = 156.0694, GNorm = 0.3113
Meta loss on this task batch = 3.1133e-01, Meta loss averaged over last 500 steps = 2.8603e-01, PNorm = 156.0732, GNorm = 0.3821
Meta loss on this task batch = 2.6152e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 156.0776, GNorm = 0.2430
Meta loss on this task batch = 2.3969e-01, Meta loss averaged over last 500 steps = 2.8599e-01, PNorm = 156.0823, GNorm = 0.3271
Meta loss on this task batch = 2.8747e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 156.0865, GNorm = 0.3877
Meta loss on this task batch = 3.0011e-01, Meta loss averaged over last 500 steps = 2.8618e-01, PNorm = 156.0907, GNorm = 0.3179
Meta loss on this task batch = 2.1376e-01, Meta loss averaged over last 500 steps = 2.8598e-01, PNorm = 156.0956, GNorm = 0.3544
Took 108.99435758590698 seconds to complete one epoch of meta training
Took 116.56891798973083 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476220
Epoch 748
Meta loss on this task batch = 2.7850e-01, Meta loss averaged over last 500 steps = 2.8584e-01, PNorm = 156.1001, GNorm = 0.2898
Meta loss on this task batch = 3.0105e-01, Meta loss averaged over last 500 steps = 2.8593e-01, PNorm = 156.1044, GNorm = 0.3695
Meta loss on this task batch = 2.7940e-01, Meta loss averaged over last 500 steps = 2.8595e-01, PNorm = 156.1081, GNorm = 0.3372
Meta loss on this task batch = 2.6799e-01, Meta loss averaged over last 500 steps = 2.8583e-01, PNorm = 156.1116, GNorm = 0.3189
Meta loss on this task batch = 2.6011e-01, Meta loss averaged over last 500 steps = 2.8572e-01, PNorm = 156.1151, GNorm = 0.3070
Meta loss on this task batch = 2.6575e-01, Meta loss averaged over last 500 steps = 2.8571e-01, PNorm = 156.1188, GNorm = 0.3271
Meta loss on this task batch = 2.2427e-01, Meta loss averaged over last 500 steps = 2.8566e-01, PNorm = 156.1226, GNorm = 0.2910
Meta loss on this task batch = 2.5087e-01, Meta loss averaged over last 500 steps = 2.8569e-01, PNorm = 156.1262, GNorm = 0.2903
Meta loss on this task batch = 2.2742e-01, Meta loss averaged over last 500 steps = 2.8548e-01, PNorm = 156.1304, GNorm = 0.3163
Meta loss on this task batch = 3.2249e-01, Meta loss averaged over last 500 steps = 2.8560e-01, PNorm = 156.1348, GNorm = 0.3330
Meta loss on this task batch = 2.6626e-01, Meta loss averaged over last 500 steps = 2.8549e-01, PNorm = 156.1402, GNorm = 0.3533
Meta loss on this task batch = 2.6284e-01, Meta loss averaged over last 500 steps = 2.8546e-01, PNorm = 156.1456, GNorm = 0.2796
Meta loss on this task batch = 3.1158e-01, Meta loss averaged over last 500 steps = 2.8554e-01, PNorm = 156.1504, GNorm = 0.3071
Meta loss on this task batch = 3.3964e-01, Meta loss averaged over last 500 steps = 2.8556e-01, PNorm = 156.1547, GNorm = 0.3791
Meta loss on this task batch = 3.3045e-01, Meta loss averaged over last 500 steps = 2.8559e-01, PNorm = 156.1591, GNorm = 0.2884
Meta loss on this task batch = 2.3270e-01, Meta loss averaged over last 500 steps = 2.8539e-01, PNorm = 156.1639, GNorm = 0.2975
Meta loss on this task batch = 3.2923e-01, Meta loss averaged over last 500 steps = 2.8544e-01, PNorm = 156.1682, GNorm = 0.2704
Meta loss on this task batch = 2.5502e-01, Meta loss averaged over last 500 steps = 2.8539e-01, PNorm = 156.1731, GNorm = 0.3505
Meta loss on this task batch = 2.8179e-01, Meta loss averaged over last 500 steps = 2.8537e-01, PNorm = 156.1776, GNorm = 0.2905
Took 110.9545967578888 seconds to complete one epoch of meta training
Took 118.58200478553772 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468659
Epoch 749
Meta loss on this task batch = 2.9610e-01, Meta loss averaged over last 500 steps = 2.8543e-01, PNorm = 156.1816, GNorm = 0.2740
Meta loss on this task batch = 2.6668e-01, Meta loss averaged over last 500 steps = 2.8539e-01, PNorm = 156.1851, GNorm = 0.4614
Meta loss on this task batch = 2.9397e-01, Meta loss averaged over last 500 steps = 2.8542e-01, PNorm = 156.1888, GNorm = 0.3484
Meta loss on this task batch = 3.2146e-01, Meta loss averaged over last 500 steps = 2.8547e-01, PNorm = 156.1920, GNorm = 0.4025
Meta loss on this task batch = 2.3805e-01, Meta loss averaged over last 500 steps = 2.8550e-01, PNorm = 156.1956, GNorm = 0.3262
Meta loss on this task batch = 2.7776e-01, Meta loss averaged over last 500 steps = 2.8551e-01, PNorm = 156.1994, GNorm = 0.2817
Meta loss on this task batch = 2.5794e-01, Meta loss averaged over last 500 steps = 2.8546e-01, PNorm = 156.2035, GNorm = 0.2414
Meta loss on this task batch = 2.2814e-01, Meta loss averaged over last 500 steps = 2.8520e-01, PNorm = 156.2080, GNorm = 0.2286
Meta loss on this task batch = 3.5391e-01, Meta loss averaged over last 500 steps = 2.8526e-01, PNorm = 156.2107, GNorm = 0.3932
Meta loss on this task batch = 3.1805e-01, Meta loss averaged over last 500 steps = 2.8534e-01, PNorm = 156.2128, GNorm = 0.4545
Meta loss on this task batch = 2.7269e-01, Meta loss averaged over last 500 steps = 2.8527e-01, PNorm = 156.2148, GNorm = 0.5448
Meta loss on this task batch = 3.0302e-01, Meta loss averaged over last 500 steps = 2.8535e-01, PNorm = 156.2174, GNorm = 0.5138
Meta loss on this task batch = 3.2068e-01, Meta loss averaged over last 500 steps = 2.8537e-01, PNorm = 156.2202, GNorm = 0.3314
Meta loss on this task batch = 2.2768e-01, Meta loss averaged over last 500 steps = 2.8527e-01, PNorm = 156.2251, GNorm = 0.2461
Meta loss on this task batch = 2.8574e-01, Meta loss averaged over last 500 steps = 2.8528e-01, PNorm = 156.2300, GNorm = 0.3443
Meta loss on this task batch = 3.1248e-01, Meta loss averaged over last 500 steps = 2.8537e-01, PNorm = 156.2345, GNorm = 0.2773
Meta loss on this task batch = 2.2716e-01, Meta loss averaged over last 500 steps = 2.8510e-01, PNorm = 156.2396, GNorm = 0.2906
Meta loss on this task batch = 3.2191e-01, Meta loss averaged over last 500 steps = 2.8516e-01, PNorm = 156.2448, GNorm = 0.3618
Meta loss on this task batch = 2.9591e-01, Meta loss averaged over last 500 steps = 2.8518e-01, PNorm = 156.2496, GNorm = 0.2925
Took 111.96445941925049 seconds to complete one epoch of meta training
Took 119.96320509910583 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.442635
Epoch 750
Meta loss on this task batch = 2.5912e-01, Meta loss averaged over last 500 steps = 2.8506e-01, PNorm = 156.2544, GNorm = 0.2734
Meta loss on this task batch = 2.6549e-01, Meta loss averaged over last 500 steps = 2.8497e-01, PNorm = 156.2593, GNorm = 0.2400
Meta loss on this task batch = 3.1594e-01, Meta loss averaged over last 500 steps = 2.8502e-01, PNorm = 156.2640, GNorm = 0.2574
Meta loss on this task batch = 2.5986e-01, Meta loss averaged over last 500 steps = 2.8499e-01, PNorm = 156.2688, GNorm = 0.2362
Meta loss on this task batch = 3.0529e-01, Meta loss averaged over last 500 steps = 2.8494e-01, PNorm = 156.2732, GNorm = 0.3134
Meta loss on this task batch = 2.9723e-01, Meta loss averaged over last 500 steps = 2.8515e-01, PNorm = 156.2778, GNorm = 0.3565
Meta loss on this task batch = 2.8532e-01, Meta loss averaged over last 500 steps = 2.8502e-01, PNorm = 156.2827, GNorm = 0.2947
Meta loss on this task batch = 3.5282e-01, Meta loss averaged over last 500 steps = 2.8517e-01, PNorm = 156.2872, GNorm = 0.3841
Meta loss on this task batch = 2.9074e-01, Meta loss averaged over last 500 steps = 2.8502e-01, PNorm = 156.2918, GNorm = 0.3783
Meta loss on this task batch = 3.2587e-01, Meta loss averaged over last 500 steps = 2.8504e-01, PNorm = 156.2960, GNorm = 0.5099
Meta loss on this task batch = 2.8135e-01, Meta loss averaged over last 500 steps = 2.8509e-01, PNorm = 156.3001, GNorm = 0.4053
Meta loss on this task batch = 3.0908e-01, Meta loss averaged over last 500 steps = 2.8516e-01, PNorm = 156.3046, GNorm = 0.3051
Meta loss on this task batch = 2.5797e-01, Meta loss averaged over last 500 steps = 2.8512e-01, PNorm = 156.3086, GNorm = 0.2581
Meta loss on this task batch = 2.7895e-01, Meta loss averaged over last 500 steps = 2.8500e-01, PNorm = 156.3123, GNorm = 0.4536
Meta loss on this task batch = 3.4106e-01, Meta loss averaged over last 500 steps = 2.8512e-01, PNorm = 156.3145, GNorm = 0.3158
Meta loss on this task batch = 3.0854e-01, Meta loss averaged over last 500 steps = 2.8522e-01, PNorm = 156.3157, GNorm = 0.2952
Meta loss on this task batch = 2.4067e-01, Meta loss averaged over last 500 steps = 2.8518e-01, PNorm = 156.3172, GNorm = 0.2782
Meta loss on this task batch = 2.4625e-01, Meta loss averaged over last 500 steps = 2.8510e-01, PNorm = 156.3193, GNorm = 0.2480
Meta loss on this task batch = 2.2646e-01, Meta loss averaged over last 500 steps = 2.8496e-01, PNorm = 156.3217, GNorm = 0.3371
Took 110.79079484939575 seconds to complete one epoch of meta training
Took 118.47838592529297 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473519
Epoch 751
Meta loss on this task batch = 2.7841e-01, Meta loss averaged over last 500 steps = 2.8493e-01, PNorm = 156.3241, GNorm = 0.2378
Meta loss on this task batch = 2.9210e-01, Meta loss averaged over last 500 steps = 2.8495e-01, PNorm = 156.3272, GNorm = 0.2967
Meta loss on this task batch = 2.4540e-01, Meta loss averaged over last 500 steps = 2.8483e-01, PNorm = 156.3308, GNorm = 0.3917
Meta loss on this task batch = 3.0127e-01, Meta loss averaged over last 500 steps = 2.8492e-01, PNorm = 156.3342, GNorm = 0.3068
Meta loss on this task batch = 2.6611e-01, Meta loss averaged over last 500 steps = 2.8499e-01, PNorm = 156.3381, GNorm = 0.4388
Meta loss on this task batch = 2.8942e-01, Meta loss averaged over last 500 steps = 2.8482e-01, PNorm = 156.3419, GNorm = 0.3606
Meta loss on this task batch = 2.5416e-01, Meta loss averaged over last 500 steps = 2.8474e-01, PNorm = 156.3458, GNorm = 0.3875
Meta loss on this task batch = 2.6055e-01, Meta loss averaged over last 500 steps = 2.8465e-01, PNorm = 156.3497, GNorm = 0.4114
Meta loss on this task batch = 2.5214e-01, Meta loss averaged over last 500 steps = 2.8447e-01, PNorm = 156.3541, GNorm = 0.2719
Meta loss on this task batch = 2.6471e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 156.3585, GNorm = 0.2674
Meta loss on this task batch = 2.9690e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 156.3627, GNorm = 0.5861
Meta loss on this task batch = 3.3562e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 156.3667, GNorm = 0.3833
Meta loss on this task batch = 3.0891e-01, Meta loss averaged over last 500 steps = 2.8451e-01, PNorm = 156.3709, GNorm = 0.3759
Meta loss on this task batch = 2.5226e-01, Meta loss averaged over last 500 steps = 2.8451e-01, PNorm = 156.3751, GNorm = 0.2681
Meta loss on this task batch = 2.5361e-01, Meta loss averaged over last 500 steps = 2.8438e-01, PNorm = 156.3790, GNorm = 0.3616
Meta loss on this task batch = 2.8463e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 156.3828, GNorm = 0.2927
Meta loss on this task batch = 2.5127e-01, Meta loss averaged over last 500 steps = 2.8433e-01, PNorm = 156.3873, GNorm = 0.2985
Meta loss on this task batch = 3.5081e-01, Meta loss averaged over last 500 steps = 2.8441e-01, PNorm = 156.3909, GNorm = 0.4372
Meta loss on this task batch = 2.3350e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 156.3949, GNorm = 0.3280
Took 111.34342169761658 seconds to complete one epoch of meta training
Took 119.16396427154541 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488072
Epoch 752
Meta loss on this task batch = 2.2306e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 156.3992, GNorm = 0.2710
Meta loss on this task batch = 3.2373e-01, Meta loss averaged over last 500 steps = 2.8441e-01, PNorm = 156.4032, GNorm = 0.3392
Meta loss on this task batch = 2.6712e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 156.4071, GNorm = 0.2712
Meta loss on this task batch = 2.7097e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 156.4112, GNorm = 0.3040
Meta loss on this task batch = 2.8064e-01, Meta loss averaged over last 500 steps = 2.8460e-01, PNorm = 156.4152, GNorm = 0.2850
Meta loss on this task batch = 3.0045e-01, Meta loss averaged over last 500 steps = 2.8472e-01, PNorm = 156.4191, GNorm = 0.3333
Meta loss on this task batch = 2.6594e-01, Meta loss averaged over last 500 steps = 2.8472e-01, PNorm = 156.4236, GNorm = 0.2827
Meta loss on this task batch = 2.5990e-01, Meta loss averaged over last 500 steps = 2.8473e-01, PNorm = 156.4278, GNorm = 0.3103
Meta loss on this task batch = 3.0256e-01, Meta loss averaged over last 500 steps = 2.8460e-01, PNorm = 156.4321, GNorm = 0.2918
Meta loss on this task batch = 2.4400e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 156.4371, GNorm = 0.2363
Meta loss on this task batch = 2.2811e-01, Meta loss averaged over last 500 steps = 2.8437e-01, PNorm = 156.4418, GNorm = 0.3131
Meta loss on this task batch = 3.4203e-01, Meta loss averaged over last 500 steps = 2.8452e-01, PNorm = 156.4461, GNorm = 0.5956
Meta loss on this task batch = 2.5393e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 156.4501, GNorm = 0.2696
Meta loss on this task batch = 2.5469e-01, Meta loss averaged over last 500 steps = 2.8455e-01, PNorm = 156.4536, GNorm = 0.3059
Meta loss on this task batch = 2.8429e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 156.4566, GNorm = 0.4116
Meta loss on this task batch = 3.2758e-01, Meta loss averaged over last 500 steps = 2.8468e-01, PNorm = 156.4591, GNorm = 0.3604
Meta loss on this task batch = 3.0324e-01, Meta loss averaged over last 500 steps = 2.8475e-01, PNorm = 156.4624, GNorm = 0.3078
Meta loss on this task batch = 3.0141e-01, Meta loss averaged over last 500 steps = 2.8477e-01, PNorm = 156.4663, GNorm = 0.2378
Meta loss on this task batch = 2.4538e-01, Meta loss averaged over last 500 steps = 2.8462e-01, PNorm = 156.4709, GNorm = 0.2807
Took 110.99550557136536 seconds to complete one epoch of meta training
Took 117.85436463356018 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454483
Epoch 753
Meta loss on this task batch = 2.7405e-01, Meta loss averaged over last 500 steps = 2.8467e-01, PNorm = 156.4754, GNorm = 0.2854
Meta loss on this task batch = 2.8758e-01, Meta loss averaged over last 500 steps = 2.8466e-01, PNorm = 156.4800, GNorm = 0.2858
Meta loss on this task batch = 2.7001e-01, Meta loss averaged over last 500 steps = 2.8471e-01, PNorm = 156.4851, GNorm = 0.3439
Meta loss on this task batch = 3.4261e-01, Meta loss averaged over last 500 steps = 2.8487e-01, PNorm = 156.4901, GNorm = 0.2926
Meta loss on this task batch = 2.3238e-01, Meta loss averaged over last 500 steps = 2.8481e-01, PNorm = 156.4948, GNorm = 0.2682
Meta loss on this task batch = 2.6247e-01, Meta loss averaged over last 500 steps = 2.8482e-01, PNorm = 156.4999, GNorm = 0.2605
Meta loss on this task batch = 2.9150e-01, Meta loss averaged over last 500 steps = 2.8480e-01, PNorm = 156.5058, GNorm = 0.2622
Meta loss on this task batch = 2.8331e-01, Meta loss averaged over last 500 steps = 2.8478e-01, PNorm = 156.5111, GNorm = 0.3315
Meta loss on this task batch = 2.8152e-01, Meta loss averaged over last 500 steps = 2.8468e-01, PNorm = 156.5167, GNorm = 0.2726
Meta loss on this task batch = 2.5733e-01, Meta loss averaged over last 500 steps = 2.8449e-01, PNorm = 156.5218, GNorm = 0.3708
Meta loss on this task batch = 3.0783e-01, Meta loss averaged over last 500 steps = 2.8453e-01, PNorm = 156.5262, GNorm = 0.3347
Meta loss on this task batch = 3.0716e-01, Meta loss averaged over last 500 steps = 2.8466e-01, PNorm = 156.5309, GNorm = 0.3346
Meta loss on this task batch = 2.7856e-01, Meta loss averaged over last 500 steps = 2.8463e-01, PNorm = 156.5348, GNorm = 0.4244
Meta loss on this task batch = 2.8346e-01, Meta loss averaged over last 500 steps = 2.8460e-01, PNorm = 156.5386, GNorm = 0.3366
Meta loss on this task batch = 2.7460e-01, Meta loss averaged over last 500 steps = 2.8459e-01, PNorm = 156.5433, GNorm = 0.3134
Meta loss on this task batch = 3.0186e-01, Meta loss averaged over last 500 steps = 2.8458e-01, PNorm = 156.5478, GNorm = 0.3025
Meta loss on this task batch = 2.9252e-01, Meta loss averaged over last 500 steps = 2.8463e-01, PNorm = 156.5523, GNorm = 0.3965
Meta loss on this task batch = 2.5677e-01, Meta loss averaged over last 500 steps = 2.8466e-01, PNorm = 156.5566, GNorm = 0.4339
Meta loss on this task batch = 2.5973e-01, Meta loss averaged over last 500 steps = 2.8455e-01, PNorm = 156.5606, GNorm = 0.3570
Took 113.84821820259094 seconds to complete one epoch of meta training
Took 122.27391242980957 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471764
Epoch 754
Meta loss on this task batch = 2.7192e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 156.5638, GNorm = 0.3041
Meta loss on this task batch = 2.6508e-01, Meta loss averaged over last 500 steps = 2.8466e-01, PNorm = 156.5673, GNorm = 0.3280
Meta loss on this task batch = 2.6801e-01, Meta loss averaged over last 500 steps = 2.8462e-01, PNorm = 156.5707, GNorm = 0.2748
Meta loss on this task batch = 2.5155e-01, Meta loss averaged over last 500 steps = 2.8465e-01, PNorm = 156.5735, GNorm = 0.2511
Meta loss on this task batch = 2.8480e-01, Meta loss averaged over last 500 steps = 2.8469e-01, PNorm = 156.5764, GNorm = 0.2980
Meta loss on this task batch = 2.7147e-01, Meta loss averaged over last 500 steps = 2.8458e-01, PNorm = 156.5795, GNorm = 0.4357
Meta loss on this task batch = 2.9195e-01, Meta loss averaged over last 500 steps = 2.8466e-01, PNorm = 156.5827, GNorm = 0.2601
Meta loss on this task batch = 2.8477e-01, Meta loss averaged over last 500 steps = 2.8462e-01, PNorm = 156.5858, GNorm = 0.3748
Meta loss on this task batch = 2.8860e-01, Meta loss averaged over last 500 steps = 2.8458e-01, PNorm = 156.5896, GNorm = 0.3266
Meta loss on this task batch = 2.8378e-01, Meta loss averaged over last 500 steps = 2.8459e-01, PNorm = 156.5937, GNorm = 0.3514
Meta loss on this task batch = 2.7607e-01, Meta loss averaged over last 500 steps = 2.8467e-01, PNorm = 156.5978, GNorm = 0.3537
Meta loss on this task batch = 2.9585e-01, Meta loss averaged over last 500 steps = 2.8454e-01, PNorm = 156.6019, GNorm = 0.4760
Meta loss on this task batch = 2.9323e-01, Meta loss averaged over last 500 steps = 2.8438e-01, PNorm = 156.6053, GNorm = 0.3089
Meta loss on this task batch = 2.4102e-01, Meta loss averaged over last 500 steps = 2.8427e-01, PNorm = 156.6085, GNorm = 0.2183
Meta loss on this task batch = 3.1733e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 156.6115, GNorm = 0.3777
Meta loss on this task batch = 2.4921e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 156.6153, GNorm = 0.2882
Meta loss on this task batch = 2.6631e-01, Meta loss averaged over last 500 steps = 2.8427e-01, PNorm = 156.6198, GNorm = 0.2855
Meta loss on this task batch = 3.3193e-01, Meta loss averaged over last 500 steps = 2.8428e-01, PNorm = 156.6243, GNorm = 0.3522
Meta loss on this task batch = 2.5277e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 156.6296, GNorm = 0.2241
Took 113.27802848815918 seconds to complete one epoch of meta training
Took 121.05563855171204 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465793
Epoch 755
Meta loss on this task batch = 3.2223e-01, Meta loss averaged over last 500 steps = 2.8428e-01, PNorm = 156.6342, GNorm = 0.4858
Meta loss on this task batch = 2.3534e-01, Meta loss averaged over last 500 steps = 2.8424e-01, PNorm = 156.6387, GNorm = 0.2753
Meta loss on this task batch = 2.7064e-01, Meta loss averaged over last 500 steps = 2.8425e-01, PNorm = 156.6436, GNorm = 0.2377
Meta loss on this task batch = 2.9690e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 156.6480, GNorm = 0.2414
Meta loss on this task batch = 2.8380e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 156.6524, GNorm = 0.4028
Meta loss on this task batch = 2.9821e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 156.6561, GNorm = 0.3089
Meta loss on this task batch = 2.9754e-01, Meta loss averaged over last 500 steps = 2.8449e-01, PNorm = 156.6600, GNorm = 0.3075
Meta loss on this task batch = 3.1385e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 156.6634, GNorm = 0.3704
Meta loss on this task batch = 3.3482e-01, Meta loss averaged over last 500 steps = 2.8477e-01, PNorm = 156.6664, GNorm = 0.3343
Meta loss on this task batch = 2.5247e-01, Meta loss averaged over last 500 steps = 2.8478e-01, PNorm = 156.6685, GNorm = 0.3168
Meta loss on this task batch = 2.6870e-01, Meta loss averaged over last 500 steps = 2.8475e-01, PNorm = 156.6712, GNorm = 0.2332
Meta loss on this task batch = 2.8443e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 156.6743, GNorm = 0.3216
Meta loss on this task batch = 2.5755e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 156.6772, GNorm = 0.2491
Meta loss on this task batch = 2.8621e-01, Meta loss averaged over last 500 steps = 2.8453e-01, PNorm = 156.6807, GNorm = 0.3443
Meta loss on this task batch = 2.9154e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 156.6846, GNorm = 0.3650
Meta loss on this task batch = 2.8716e-01, Meta loss averaged over last 500 steps = 2.8445e-01, PNorm = 156.6887, GNorm = 0.3387
Meta loss on this task batch = 2.3505e-01, Meta loss averaged over last 500 steps = 2.8428e-01, PNorm = 156.6926, GNorm = 0.3646
Meta loss on this task batch = 2.8466e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 156.6968, GNorm = 0.2206
Meta loss on this task batch = 2.6859e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 156.7017, GNorm = 0.3790
Took 112.79274916648865 seconds to complete one epoch of meta training
Took 121.47339534759521 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458768
Epoch 756
Meta loss on this task batch = 2.6499e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 156.7063, GNorm = 0.2778
Meta loss on this task batch = 3.0951e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 156.7102, GNorm = 0.3685
Meta loss on this task batch = 3.0005e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 156.7135, GNorm = 0.3033
Meta loss on this task batch = 2.2401e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 156.7170, GNorm = 0.2261
Meta loss on this task batch = 2.6250e-01, Meta loss averaged over last 500 steps = 2.8430e-01, PNorm = 156.7210, GNorm = 0.2769
Meta loss on this task batch = 2.8500e-01, Meta loss averaged over last 500 steps = 2.8430e-01, PNorm = 156.7246, GNorm = 0.3470
Meta loss on this task batch = 2.4463e-01, Meta loss averaged over last 500 steps = 2.8419e-01, PNorm = 156.7281, GNorm = 0.2258
Meta loss on this task batch = 2.8054e-01, Meta loss averaged over last 500 steps = 2.8411e-01, PNorm = 156.7313, GNorm = 0.2864
Meta loss on this task batch = 3.2123e-01, Meta loss averaged over last 500 steps = 2.8413e-01, PNorm = 156.7344, GNorm = 0.2521
Meta loss on this task batch = 2.5448e-01, Meta loss averaged over last 500 steps = 2.8390e-01, PNorm = 156.7373, GNorm = 0.3009
Meta loss on this task batch = 2.8186e-01, Meta loss averaged over last 500 steps = 2.8394e-01, PNorm = 156.7407, GNorm = 0.2964
Meta loss on this task batch = 2.8354e-01, Meta loss averaged over last 500 steps = 2.8392e-01, PNorm = 156.7437, GNorm = 0.3521
Meta loss on this task batch = 2.6318e-01, Meta loss averaged over last 500 steps = 2.8385e-01, PNorm = 156.7466, GNorm = 0.3760
Meta loss on this task batch = 2.7510e-01, Meta loss averaged over last 500 steps = 2.8382e-01, PNorm = 156.7492, GNorm = 0.3492
Meta loss on this task batch = 2.9234e-01, Meta loss averaged over last 500 steps = 2.8386e-01, PNorm = 156.7520, GNorm = 0.3189
Meta loss on this task batch = 2.8145e-01, Meta loss averaged over last 500 steps = 2.8381e-01, PNorm = 156.7554, GNorm = 0.3125
Meta loss on this task batch = 2.8798e-01, Meta loss averaged over last 500 steps = 2.8389e-01, PNorm = 156.7592, GNorm = 0.2776
Meta loss on this task batch = 2.8958e-01, Meta loss averaged over last 500 steps = 2.8388e-01, PNorm = 156.7633, GNorm = 0.2641
Meta loss on this task batch = 2.8738e-01, Meta loss averaged over last 500 steps = 2.8375e-01, PNorm = 156.7676, GNorm = 0.3841
Took 114.28017210960388 seconds to complete one epoch of meta training
Took 122.03966641426086 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474334
Epoch 757
Meta loss on this task batch = 2.2271e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 156.7725, GNorm = 0.2624
Meta loss on this task batch = 3.0670e-01, Meta loss averaged over last 500 steps = 2.8371e-01, PNorm = 156.7771, GNorm = 0.2214
Meta loss on this task batch = 2.7441e-01, Meta loss averaged over last 500 steps = 2.8367e-01, PNorm = 156.7820, GNorm = 0.2642
Meta loss on this task batch = 2.6892e-01, Meta loss averaged over last 500 steps = 2.8373e-01, PNorm = 156.7865, GNorm = 0.2814
Meta loss on this task batch = 2.4064e-01, Meta loss averaged over last 500 steps = 2.8355e-01, PNorm = 156.7916, GNorm = 0.2674
Meta loss on this task batch = 2.8746e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 156.7972, GNorm = 0.2716
Meta loss on this task batch = 2.3269e-01, Meta loss averaged over last 500 steps = 2.8346e-01, PNorm = 156.8040, GNorm = 0.2436
Meta loss on this task batch = 2.9191e-01, Meta loss averaged over last 500 steps = 2.8346e-01, PNorm = 156.8101, GNorm = 0.4277
Meta loss on this task batch = 2.8322e-01, Meta loss averaged over last 500 steps = 2.8341e-01, PNorm = 156.8159, GNorm = 0.2572
Meta loss on this task batch = 2.9671e-01, Meta loss averaged over last 500 steps = 2.8344e-01, PNorm = 156.8211, GNorm = 0.3035
Meta loss on this task batch = 2.5481e-01, Meta loss averaged over last 500 steps = 2.8339e-01, PNorm = 156.8272, GNorm = 0.3774
Meta loss on this task batch = 3.2583e-01, Meta loss averaged over last 500 steps = 2.8351e-01, PNorm = 156.8325, GNorm = 0.3529
Meta loss on this task batch = 2.8342e-01, Meta loss averaged over last 500 steps = 2.8352e-01, PNorm = 156.8378, GNorm = 0.3345
Meta loss on this task batch = 3.2327e-01, Meta loss averaged over last 500 steps = 2.8359e-01, PNorm = 156.8436, GNorm = 0.3733
Meta loss on this task batch = 2.6849e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 156.8490, GNorm = 0.3526
Meta loss on this task batch = 3.2855e-01, Meta loss averaged over last 500 steps = 2.8370e-01, PNorm = 156.8544, GNorm = 0.3855
Meta loss on this task batch = 2.6184e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 156.8591, GNorm = 0.4928
Meta loss on this task batch = 2.8306e-01, Meta loss averaged over last 500 steps = 2.8358e-01, PNorm = 156.8639, GNorm = 0.4027
Meta loss on this task batch = 3.5440e-01, Meta loss averaged over last 500 steps = 2.8358e-01, PNorm = 156.8678, GNorm = 0.3579
Took 113.63646268844604 seconds to complete one epoch of meta training
Took 121.51860904693604 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483362
Epoch 758
Meta loss on this task batch = 2.7564e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 156.8720, GNorm = 0.3097
Meta loss on this task batch = 2.5653e-01, Meta loss averaged over last 500 steps = 2.8361e-01, PNorm = 156.8758, GNorm = 0.2600
Meta loss on this task batch = 2.7204e-01, Meta loss averaged over last 500 steps = 2.8358e-01, PNorm = 156.8799, GNorm = 0.2979
Meta loss on this task batch = 3.0854e-01, Meta loss averaged over last 500 steps = 2.8353e-01, PNorm = 156.8841, GNorm = 0.2709
Meta loss on this task batch = 2.7535e-01, Meta loss averaged over last 500 steps = 2.8349e-01, PNorm = 156.8873, GNorm = 0.3583
Meta loss on this task batch = 3.0294e-01, Meta loss averaged over last 500 steps = 2.8344e-01, PNorm = 156.8908, GNorm = 0.5422
Meta loss on this task batch = 2.3867e-01, Meta loss averaged over last 500 steps = 2.8335e-01, PNorm = 156.8938, GNorm = 0.2519
Meta loss on this task batch = 2.9112e-01, Meta loss averaged over last 500 steps = 2.8330e-01, PNorm = 156.8975, GNorm = 0.2704
Meta loss on this task batch = 2.5816e-01, Meta loss averaged over last 500 steps = 2.8325e-01, PNorm = 156.9014, GNorm = 0.2851
Meta loss on this task batch = 3.0871e-01, Meta loss averaged over last 500 steps = 2.8330e-01, PNorm = 156.9058, GNorm = 0.2807
Meta loss on this task batch = 2.5916e-01, Meta loss averaged over last 500 steps = 2.8334e-01, PNorm = 156.9106, GNorm = 0.2562
Meta loss on this task batch = 2.8530e-01, Meta loss averaged over last 500 steps = 2.8330e-01, PNorm = 156.9154, GNorm = 0.3021
Meta loss on this task batch = 2.5508e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 156.9200, GNorm = 0.2527
Meta loss on this task batch = 2.4884e-01, Meta loss averaged over last 500 steps = 2.8316e-01, PNorm = 156.9249, GNorm = 0.2745
Meta loss on this task batch = 3.1713e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 156.9299, GNorm = 0.3241
Meta loss on this task batch = 2.8887e-01, Meta loss averaged over last 500 steps = 2.8327e-01, PNorm = 156.9351, GNorm = 0.3059
Meta loss on this task batch = 3.2638e-01, Meta loss averaged over last 500 steps = 2.8338e-01, PNorm = 156.9386, GNorm = 0.6443
Meta loss on this task batch = 2.4741e-01, Meta loss averaged over last 500 steps = 2.8329e-01, PNorm = 156.9421, GNorm = 0.2216
Meta loss on this task batch = 2.9431e-01, Meta loss averaged over last 500 steps = 2.8323e-01, PNorm = 156.9458, GNorm = 0.4171
Took 113.50691628456116 seconds to complete one epoch of meta training
Took 121.62771320343018 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474201
Epoch 759
Meta loss on this task batch = 3.4073e-01, Meta loss averaged over last 500 steps = 2.8334e-01, PNorm = 156.9494, GNorm = 0.4311
Meta loss on this task batch = 2.7686e-01, Meta loss averaged over last 500 steps = 2.8323e-01, PNorm = 156.9535, GNorm = 0.3632
Meta loss on this task batch = 3.4288e-01, Meta loss averaged over last 500 steps = 2.8330e-01, PNorm = 156.9574, GNorm = 0.2694
Meta loss on this task batch = 2.6965e-01, Meta loss averaged over last 500 steps = 2.8329e-01, PNorm = 156.9608, GNorm = 0.3140
Meta loss on this task batch = 2.0283e-01, Meta loss averaged over last 500 steps = 2.8318e-01, PNorm = 156.9648, GNorm = 0.2878
Meta loss on this task batch = 3.0316e-01, Meta loss averaged over last 500 steps = 2.8314e-01, PNorm = 156.9686, GNorm = 0.2869
Meta loss on this task batch = 2.9607e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 156.9726, GNorm = 0.2715
Meta loss on this task batch = 3.4506e-01, Meta loss averaged over last 500 steps = 2.8338e-01, PNorm = 156.9762, GNorm = 0.4512
Meta loss on this task batch = 2.6786e-01, Meta loss averaged over last 500 steps = 2.8325e-01, PNorm = 156.9802, GNorm = 0.3296
Meta loss on this task batch = 3.0231e-01, Meta loss averaged over last 500 steps = 2.8323e-01, PNorm = 156.9839, GNorm = 0.3251
Meta loss on this task batch = 2.9562e-01, Meta loss averaged over last 500 steps = 2.8331e-01, PNorm = 156.9878, GNorm = 0.2380
Meta loss on this task batch = 2.7685e-01, Meta loss averaged over last 500 steps = 2.8332e-01, PNorm = 156.9928, GNorm = 0.3523
Meta loss on this task batch = 2.1723e-01, Meta loss averaged over last 500 steps = 2.8313e-01, PNorm = 156.9984, GNorm = 0.2233
Meta loss on this task batch = 2.8441e-01, Meta loss averaged over last 500 steps = 2.8313e-01, PNorm = 157.0040, GNorm = 0.3370
Meta loss on this task batch = 2.6023e-01, Meta loss averaged over last 500 steps = 2.8308e-01, PNorm = 157.0098, GNorm = 0.2770
Meta loss on this task batch = 3.2486e-01, Meta loss averaged over last 500 steps = 2.8316e-01, PNorm = 157.0148, GNorm = 0.3199
Meta loss on this task batch = 3.0317e-01, Meta loss averaged over last 500 steps = 2.8315e-01, PNorm = 157.0189, GNorm = 0.3227
Meta loss on this task batch = 2.6138e-01, Meta loss averaged over last 500 steps = 2.8310e-01, PNorm = 157.0235, GNorm = 0.3274
Meta loss on this task batch = 1.8511e-01, Meta loss averaged over last 500 steps = 2.8284e-01, PNorm = 157.0280, GNorm = 0.2587
Took 112.99384355545044 seconds to complete one epoch of meta training
Took 121.31576490402222 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480239
Epoch 760
Meta loss on this task batch = 2.1705e-01, Meta loss averaged over last 500 steps = 2.8266e-01, PNorm = 157.0324, GNorm = 0.2447
Meta loss on this task batch = 2.4466e-01, Meta loss averaged over last 500 steps = 2.8253e-01, PNorm = 157.0368, GNorm = 0.2187
Meta loss on this task batch = 2.9626e-01, Meta loss averaged over last 500 steps = 2.8250e-01, PNorm = 157.0415, GNorm = 0.2890
Meta loss on this task batch = 2.4420e-01, Meta loss averaged over last 500 steps = 2.8236e-01, PNorm = 157.0466, GNorm = 0.2264
Meta loss on this task batch = 2.3143e-01, Meta loss averaged over last 500 steps = 2.8223e-01, PNorm = 157.0518, GNorm = 0.2316
Meta loss on this task batch = 2.5145e-01, Meta loss averaged over last 500 steps = 2.8222e-01, PNorm = 157.0572, GNorm = 0.2521
Meta loss on this task batch = 3.1254e-01, Meta loss averaged over last 500 steps = 2.8224e-01, PNorm = 157.0624, GNorm = 0.2903
Meta loss on this task batch = 2.7791e-01, Meta loss averaged over last 500 steps = 2.8227e-01, PNorm = 157.0674, GNorm = 0.2703
Meta loss on this task batch = 2.7049e-01, Meta loss averaged over last 500 steps = 2.8222e-01, PNorm = 157.0721, GNorm = 0.3752
Meta loss on this task batch = 2.2432e-01, Meta loss averaged over last 500 steps = 2.8191e-01, PNorm = 157.0769, GNorm = 0.3167
Meta loss on this task batch = 3.2363e-01, Meta loss averaged over last 500 steps = 2.8205e-01, PNorm = 157.0811, GNorm = 0.3432
Meta loss on this task batch = 3.0045e-01, Meta loss averaged over last 500 steps = 2.8209e-01, PNorm = 157.0845, GNorm = 0.3830
Meta loss on this task batch = 2.6827e-01, Meta loss averaged over last 500 steps = 2.8201e-01, PNorm = 157.0884, GNorm = 0.3208
Meta loss on this task batch = 2.7010e-01, Meta loss averaged over last 500 steps = 2.8192e-01, PNorm = 157.0911, GNorm = 0.3067
Meta loss on this task batch = 2.9323e-01, Meta loss averaged over last 500 steps = 2.8200e-01, PNorm = 157.0934, GNorm = 0.2655
Meta loss on this task batch = 3.1420e-01, Meta loss averaged over last 500 steps = 2.8199e-01, PNorm = 157.0950, GNorm = 0.3428
Meta loss on this task batch = 2.6241e-01, Meta loss averaged over last 500 steps = 2.8209e-01, PNorm = 157.0971, GNorm = 0.3093
Meta loss on this task batch = 2.9366e-01, Meta loss averaged over last 500 steps = 2.8201e-01, PNorm = 157.0989, GNorm = 0.3395
Meta loss on this task batch = 2.6789e-01, Meta loss averaged over last 500 steps = 2.8206e-01, PNorm = 157.1004, GNorm = 0.3633
Took 112.9907705783844 seconds to complete one epoch of meta training
Took 121.39646935462952 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478864
Epoch 761
Meta loss on this task batch = 2.6127e-01, Meta loss averaged over last 500 steps = 2.8191e-01, PNorm = 157.1016, GNorm = 0.3221
Meta loss on this task batch = 2.8322e-01, Meta loss averaged over last 500 steps = 2.8185e-01, PNorm = 157.1036, GNorm = 0.4905
Meta loss on this task batch = 2.7639e-01, Meta loss averaged over last 500 steps = 2.8176e-01, PNorm = 157.1058, GNorm = 0.3808
Meta loss on this task batch = 2.7831e-01, Meta loss averaged over last 500 steps = 2.8172e-01, PNorm = 157.1083, GNorm = 0.3124
Meta loss on this task batch = 3.4024e-01, Meta loss averaged over last 500 steps = 2.8180e-01, PNorm = 157.1109, GNorm = 0.4031
Meta loss on this task batch = 2.5816e-01, Meta loss averaged over last 500 steps = 2.8163e-01, PNorm = 157.1147, GNorm = 0.3299
Meta loss on this task batch = 2.8106e-01, Meta loss averaged over last 500 steps = 2.8154e-01, PNorm = 157.1189, GNorm = 0.3305
Meta loss on this task batch = 2.9267e-01, Meta loss averaged over last 500 steps = 2.8156e-01, PNorm = 157.1228, GNorm = 0.3179
Meta loss on this task batch = 2.7679e-01, Meta loss averaged over last 500 steps = 2.8156e-01, PNorm = 157.1265, GNorm = 0.2905
Meta loss on this task batch = 2.2675e-01, Meta loss averaged over last 500 steps = 2.8151e-01, PNorm = 157.1308, GNorm = 0.2365
Meta loss on this task batch = 3.7357e-01, Meta loss averaged over last 500 steps = 2.8184e-01, PNorm = 157.1347, GNorm = 0.4169
Meta loss on this task batch = 2.8243e-01, Meta loss averaged over last 500 steps = 2.8179e-01, PNorm = 157.1388, GNorm = 0.2742
Meta loss on this task batch = 2.3304e-01, Meta loss averaged over last 500 steps = 2.8168e-01, PNorm = 157.1433, GNorm = 0.2707
Meta loss on this task batch = 3.4465e-01, Meta loss averaged over last 500 steps = 2.8168e-01, PNorm = 157.1466, GNorm = 0.3162
Meta loss on this task batch = 2.6285e-01, Meta loss averaged over last 500 steps = 2.8164e-01, PNorm = 157.1494, GNorm = 0.4180
Meta loss on this task batch = 2.6978e-01, Meta loss averaged over last 500 steps = 2.8160e-01, PNorm = 157.1517, GNorm = 0.3329
Meta loss on this task batch = 2.3304e-01, Meta loss averaged over last 500 steps = 2.8144e-01, PNorm = 157.1547, GNorm = 0.2328
Meta loss on this task batch = 2.7232e-01, Meta loss averaged over last 500 steps = 2.8134e-01, PNorm = 157.1574, GNorm = 0.2493
Meta loss on this task batch = 3.2560e-01, Meta loss averaged over last 500 steps = 2.8147e-01, PNorm = 157.1604, GNorm = 0.3863
Took 113.27643179893494 seconds to complete one epoch of meta training
Took 121.03077673912048 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475612
Epoch 762
Meta loss on this task batch = 2.9606e-01, Meta loss averaged over last 500 steps = 2.8150e-01, PNorm = 157.1645, GNorm = 0.3325
Meta loss on this task batch = 2.7648e-01, Meta loss averaged over last 500 steps = 2.8157e-01, PNorm = 157.1683, GNorm = 0.3012
Meta loss on this task batch = 2.8989e-01, Meta loss averaged over last 500 steps = 2.8159e-01, PNorm = 157.1724, GNorm = 0.3143
Meta loss on this task batch = 2.9896e-01, Meta loss averaged over last 500 steps = 2.8155e-01, PNorm = 157.1766, GNorm = 0.3188
Meta loss on this task batch = 2.8792e-01, Meta loss averaged over last 500 steps = 2.8157e-01, PNorm = 157.1811, GNorm = 0.2671
Meta loss on this task batch = 3.6795e-01, Meta loss averaged over last 500 steps = 2.8161e-01, PNorm = 157.1856, GNorm = 0.3104
Meta loss on this task batch = 3.0041e-01, Meta loss averaged over last 500 steps = 2.8168e-01, PNorm = 157.1896, GNorm = 0.3012
Meta loss on this task batch = 2.8462e-01, Meta loss averaged over last 500 steps = 2.8156e-01, PNorm = 157.1937, GNorm = 0.2644
Meta loss on this task batch = 2.5215e-01, Meta loss averaged over last 500 steps = 2.8160e-01, PNorm = 157.1978, GNorm = 0.5461
Meta loss on this task batch = 2.3293e-01, Meta loss averaged over last 500 steps = 2.8145e-01, PNorm = 157.2024, GNorm = 0.2233
Meta loss on this task batch = 2.8118e-01, Meta loss averaged over last 500 steps = 2.8134e-01, PNorm = 157.2069, GNorm = 0.2970
Meta loss on this task batch = 2.8225e-01, Meta loss averaged over last 500 steps = 2.8134e-01, PNorm = 157.2116, GNorm = 0.2795
Meta loss on this task batch = 2.9975e-01, Meta loss averaged over last 500 steps = 2.8141e-01, PNorm = 157.2163, GNorm = 0.2871
Meta loss on this task batch = 2.8943e-01, Meta loss averaged over last 500 steps = 2.8142e-01, PNorm = 157.2210, GNorm = 0.3748
Meta loss on this task batch = 2.8157e-01, Meta loss averaged over last 500 steps = 2.8137e-01, PNorm = 157.2258, GNorm = 0.2612
Meta loss on this task batch = 2.7991e-01, Meta loss averaged over last 500 steps = 2.8125e-01, PNorm = 157.2304, GNorm = 0.3531
Meta loss on this task batch = 2.6035e-01, Meta loss averaged over last 500 steps = 2.8117e-01, PNorm = 157.2355, GNorm = 0.3739
Meta loss on this task batch = 2.5942e-01, Meta loss averaged over last 500 steps = 2.8113e-01, PNorm = 157.2412, GNorm = 0.3274
Meta loss on this task batch = 2.2988e-01, Meta loss averaged over last 500 steps = 2.8106e-01, PNorm = 157.2465, GNorm = 0.2846
Took 113.81970953941345 seconds to complete one epoch of meta training
Took 120.78708505630493 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474036
Epoch 763
Meta loss on this task batch = 2.9058e-01, Meta loss averaged over last 500 steps = 2.8099e-01, PNorm = 157.2513, GNorm = 0.3121
Meta loss on this task batch = 2.5887e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 157.2560, GNorm = 0.4141
Meta loss on this task batch = 2.9643e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 157.2602, GNorm = 0.3249
Meta loss on this task batch = 2.8799e-01, Meta loss averaged over last 500 steps = 2.8084e-01, PNorm = 157.2646, GNorm = 0.2403
Meta loss on this task batch = 2.6893e-01, Meta loss averaged over last 500 steps = 2.8092e-01, PNorm = 157.2690, GNorm = 0.3368
Meta loss on this task batch = 3.4546e-01, Meta loss averaged over last 500 steps = 2.8094e-01, PNorm = 157.2724, GNorm = 0.3340
Meta loss on this task batch = 3.0832e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 157.2758, GNorm = 0.3049
Meta loss on this task batch = 3.0160e-01, Meta loss averaged over last 500 steps = 2.8090e-01, PNorm = 157.2787, GNorm = 0.2668
Meta loss on this task batch = 2.8648e-01, Meta loss averaged over last 500 steps = 2.8097e-01, PNorm = 157.2817, GNorm = 0.2886
Meta loss on this task batch = 2.7495e-01, Meta loss averaged over last 500 steps = 2.8096e-01, PNorm = 157.2848, GNorm = 0.3107
Meta loss on this task batch = 3.0499e-01, Meta loss averaged over last 500 steps = 2.8094e-01, PNorm = 157.2881, GNorm = 0.3687
Meta loss on this task batch = 2.5972e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 157.2922, GNorm = 0.3144
Meta loss on this task batch = 2.7613e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 157.2967, GNorm = 0.2869
Meta loss on this task batch = 3.1776e-01, Meta loss averaged over last 500 steps = 2.8083e-01, PNorm = 157.3008, GNorm = 0.4028
Meta loss on this task batch = 2.8130e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 157.3047, GNorm = 0.3948
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 157.3083, GNorm = 0.3470
Meta loss on this task batch = 2.1709e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 157.3124, GNorm = 0.2418
Meta loss on this task batch = 2.3934e-01, Meta loss averaged over last 500 steps = 2.8069e-01, PNorm = 157.3156, GNorm = 0.3543
Meta loss on this task batch = 2.3962e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 157.3197, GNorm = 0.2917
Took 113.65976047515869 seconds to complete one epoch of meta training
Took 121.48924803733826 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468890
Epoch 764
Meta loss on this task batch = 2.8797e-01, Meta loss averaged over last 500 steps = 2.8071e-01, PNorm = 157.3240, GNorm = 0.3003
Meta loss on this task batch = 2.6329e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 157.3279, GNorm = 0.2944
Meta loss on this task batch = 2.9967e-01, Meta loss averaged over last 500 steps = 2.8074e-01, PNorm = 157.3319, GNorm = 0.2930
Meta loss on this task batch = 2.7203e-01, Meta loss averaged over last 500 steps = 2.8078e-01, PNorm = 157.3359, GNorm = 0.3748
Meta loss on this task batch = 2.1417e-01, Meta loss averaged over last 500 steps = 2.8066e-01, PNorm = 157.3406, GNorm = 0.2884
Meta loss on this task batch = 3.1908e-01, Meta loss averaged over last 500 steps = 2.8087e-01, PNorm = 157.3453, GNorm = 0.3483
Meta loss on this task batch = 2.3555e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 157.3493, GNorm = 0.2996
Meta loss on this task batch = 3.2213e-01, Meta loss averaged over last 500 steps = 2.8090e-01, PNorm = 157.3537, GNorm = 0.3737
Meta loss on this task batch = 2.7873e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 157.3586, GNorm = 0.2661
Meta loss on this task batch = 2.5900e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 157.3638, GNorm = 0.3495
Meta loss on this task batch = 2.7930e-01, Meta loss averaged over last 500 steps = 2.8061e-01, PNorm = 157.3689, GNorm = 0.2466
Meta loss on this task batch = 2.9499e-01, Meta loss averaged over last 500 steps = 2.8069e-01, PNorm = 157.3737, GNorm = 0.3491
Meta loss on this task batch = 3.5064e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 157.3775, GNorm = 0.3176
Meta loss on this task batch = 3.1447e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 157.3802, GNorm = 0.2790
Meta loss on this task batch = 2.5988e-01, Meta loss averaged over last 500 steps = 2.8071e-01, PNorm = 157.3831, GNorm = 0.2889
Meta loss on this task batch = 3.5189e-01, Meta loss averaged over last 500 steps = 2.8085e-01, PNorm = 157.3858, GNorm = 0.3774
Meta loss on this task batch = 2.2244e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 157.3888, GNorm = 0.2462
Meta loss on this task batch = 2.5868e-01, Meta loss averaged over last 500 steps = 2.8062e-01, PNorm = 157.3913, GNorm = 0.3670
Meta loss on this task batch = 2.3810e-01, Meta loss averaged over last 500 steps = 2.8053e-01, PNorm = 157.3938, GNorm = 0.3287
Took 113.56068658828735 seconds to complete one epoch of meta training
Took 121.34549474716187 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468848
Epoch 765
Meta loss on this task batch = 2.2716e-01, Meta loss averaged over last 500 steps = 2.8040e-01, PNorm = 157.3974, GNorm = 0.2915
Meta loss on this task batch = 2.9329e-01, Meta loss averaged over last 500 steps = 2.8049e-01, PNorm = 157.4009, GNorm = 0.3101
Meta loss on this task batch = 2.8965e-01, Meta loss averaged over last 500 steps = 2.8043e-01, PNorm = 157.4046, GNorm = 0.2951
Meta loss on this task batch = 2.8145e-01, Meta loss averaged over last 500 steps = 2.8045e-01, PNorm = 157.4083, GNorm = 0.2667
Meta loss on this task batch = 2.2257e-01, Meta loss averaged over last 500 steps = 2.8029e-01, PNorm = 157.4119, GNorm = 0.2343
Meta loss on this task batch = 2.3285e-01, Meta loss averaged over last 500 steps = 2.8024e-01, PNorm = 157.4154, GNorm = 0.2866
Meta loss on this task batch = 2.7957e-01, Meta loss averaged over last 500 steps = 2.8033e-01, PNorm = 157.4191, GNorm = 0.3841
Meta loss on this task batch = 2.8934e-01, Meta loss averaged over last 500 steps = 2.8037e-01, PNorm = 157.4231, GNorm = 0.2722
Meta loss on this task batch = 3.5794e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 157.4270, GNorm = 0.3120
Meta loss on this task batch = 3.1357e-01, Meta loss averaged over last 500 steps = 2.8062e-01, PNorm = 157.4302, GNorm = 0.3195
Meta loss on this task batch = 2.9096e-01, Meta loss averaged over last 500 steps = 2.8064e-01, PNorm = 157.4335, GNorm = 0.2950
Meta loss on this task batch = 2.8119e-01, Meta loss averaged over last 500 steps = 2.8065e-01, PNorm = 157.4371, GNorm = 0.2636
Meta loss on this task batch = 2.7766e-01, Meta loss averaged over last 500 steps = 2.8056e-01, PNorm = 157.4409, GNorm = 0.3180
Meta loss on this task batch = 3.0812e-01, Meta loss averaged over last 500 steps = 2.8053e-01, PNorm = 157.4446, GNorm = 0.4694
Meta loss on this task batch = 2.5857e-01, Meta loss averaged over last 500 steps = 2.8037e-01, PNorm = 157.4486, GNorm = 0.3463
Meta loss on this task batch = 2.8696e-01, Meta loss averaged over last 500 steps = 2.8035e-01, PNorm = 157.4522, GNorm = 0.3137
Meta loss on this task batch = 2.3559e-01, Meta loss averaged over last 500 steps = 2.8027e-01, PNorm = 157.4564, GNorm = 0.3197
Meta loss on this task batch = 2.7056e-01, Meta loss averaged over last 500 steps = 2.8014e-01, PNorm = 157.4607, GNorm = 0.3949
Meta loss on this task batch = 2.9803e-01, Meta loss averaged over last 500 steps = 2.8014e-01, PNorm = 157.4646, GNorm = 0.3156
Took 111.95891809463501 seconds to complete one epoch of meta training
Took 120.10270547866821 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498223
Epoch 766
Meta loss on this task batch = 2.6827e-01, Meta loss averaged over last 500 steps = 2.8017e-01, PNorm = 157.4680, GNorm = 0.2520
Meta loss on this task batch = 3.0271e-01, Meta loss averaged over last 500 steps = 2.8025e-01, PNorm = 157.4710, GNorm = 0.4301
Meta loss on this task batch = 2.5584e-01, Meta loss averaged over last 500 steps = 2.8008e-01, PNorm = 157.4745, GNorm = 0.3051
Meta loss on this task batch = 2.2291e-01, Meta loss averaged over last 500 steps = 2.7997e-01, PNorm = 157.4786, GNorm = 0.2618
Meta loss on this task batch = 3.4990e-01, Meta loss averaged over last 500 steps = 2.8010e-01, PNorm = 157.4826, GNorm = 0.3092
Meta loss on this task batch = 2.5561e-01, Meta loss averaged over last 500 steps = 2.8014e-01, PNorm = 157.4867, GNorm = 0.2984
Meta loss on this task batch = 2.9971e-01, Meta loss averaged over last 500 steps = 2.8021e-01, PNorm = 157.4905, GNorm = 0.4727
Meta loss on this task batch = 2.6380e-01, Meta loss averaged over last 500 steps = 2.8024e-01, PNorm = 157.4949, GNorm = 0.5977
Meta loss on this task batch = 2.6162e-01, Meta loss averaged over last 500 steps = 2.8014e-01, PNorm = 157.5002, GNorm = 0.2743
Meta loss on this task batch = 3.3013e-01, Meta loss averaged over last 500 steps = 2.8017e-01, PNorm = 157.5052, GNorm = 0.3257
Meta loss on this task batch = 3.0085e-01, Meta loss averaged over last 500 steps = 2.8015e-01, PNorm = 157.5101, GNorm = 0.3586
Meta loss on this task batch = 3.1060e-01, Meta loss averaged over last 500 steps = 2.8033e-01, PNorm = 157.5157, GNorm = 0.2547
Meta loss on this task batch = 2.4218e-01, Meta loss averaged over last 500 steps = 2.8019e-01, PNorm = 157.5217, GNorm = 0.4343
Meta loss on this task batch = 2.4567e-01, Meta loss averaged over last 500 steps = 2.8023e-01, PNorm = 157.5278, GNorm = 0.2341
Meta loss on this task batch = 2.4978e-01, Meta loss averaged over last 500 steps = 2.8023e-01, PNorm = 157.5345, GNorm = 0.2815
Meta loss on this task batch = 3.1239e-01, Meta loss averaged over last 500 steps = 2.8021e-01, PNorm = 157.5407, GNorm = 0.3759
Meta loss on this task batch = 2.6476e-01, Meta loss averaged over last 500 steps = 2.8008e-01, PNorm = 157.5466, GNorm = 0.2499
Meta loss on this task batch = 2.8641e-01, Meta loss averaged over last 500 steps = 2.8010e-01, PNorm = 157.5524, GNorm = 0.3623
Meta loss on this task batch = 3.3355e-01, Meta loss averaged over last 500 steps = 2.8027e-01, PNorm = 157.5571, GNorm = 0.5553
Took 113.71197533607483 seconds to complete one epoch of meta training
Took 121.46160984039307 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478358
Epoch 767
Meta loss on this task batch = 2.7995e-01, Meta loss averaged over last 500 steps = 2.8027e-01, PNorm = 157.5616, GNorm = 0.3309
Meta loss on this task batch = 2.5685e-01, Meta loss averaged over last 500 steps = 2.8024e-01, PNorm = 157.5663, GNorm = 0.2531
Meta loss on this task batch = 3.0469e-01, Meta loss averaged over last 500 steps = 2.8025e-01, PNorm = 157.5707, GNorm = 0.3840
Meta loss on this task batch = 2.7845e-01, Meta loss averaged over last 500 steps = 2.8025e-01, PNorm = 157.5749, GNorm = 0.2479
Meta loss on this task batch = 2.4331e-01, Meta loss averaged over last 500 steps = 2.8017e-01, PNorm = 157.5793, GNorm = 0.2667
Meta loss on this task batch = 2.6045e-01, Meta loss averaged over last 500 steps = 2.8002e-01, PNorm = 157.5839, GNorm = 0.2858
Meta loss on this task batch = 3.1090e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 157.5882, GNorm = 0.4156
Meta loss on this task batch = 3.5785e-01, Meta loss averaged over last 500 steps = 2.8024e-01, PNorm = 157.5918, GNorm = 0.4014
Meta loss on this task batch = 3.1128e-01, Meta loss averaged over last 500 steps = 2.8024e-01, PNorm = 157.5952, GNorm = 0.2931
Meta loss on this task batch = 2.6357e-01, Meta loss averaged over last 500 steps = 2.8013e-01, PNorm = 157.5981, GNorm = 0.2324
Meta loss on this task batch = 2.5566e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 157.6018, GNorm = 0.2486
Meta loss on this task batch = 2.9472e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 157.6055, GNorm = 0.2391
Meta loss on this task batch = 2.6846e-01, Meta loss averaged over last 500 steps = 2.8000e-01, PNorm = 157.6092, GNorm = 0.4327
Meta loss on this task batch = 2.9970e-01, Meta loss averaged over last 500 steps = 2.8005e-01, PNorm = 157.6134, GNorm = 0.2748
Meta loss on this task batch = 2.4781e-01, Meta loss averaged over last 500 steps = 2.8009e-01, PNorm = 157.6179, GNorm = 0.2269
Meta loss on this task batch = 2.8015e-01, Meta loss averaged over last 500 steps = 2.8011e-01, PNorm = 157.6223, GNorm = 0.2628
Meta loss on this task batch = 2.2304e-01, Meta loss averaged over last 500 steps = 2.8002e-01, PNorm = 157.6269, GNorm = 0.2528
Meta loss on this task batch = 2.8455e-01, Meta loss averaged over last 500 steps = 2.7998e-01, PNorm = 157.6315, GNorm = 0.2928
Meta loss on this task batch = 2.2547e-01, Meta loss averaged over last 500 steps = 2.7992e-01, PNorm = 157.6368, GNorm = 0.4089
Took 149.96429419517517 seconds to complete one epoch of meta training
Took 157.90976238250732 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470588
Epoch 768
Meta loss on this task batch = 2.8793e-01, Meta loss averaged over last 500 steps = 2.8000e-01, PNorm = 157.6418, GNorm = 0.2806
Meta loss on this task batch = 2.6821e-01, Meta loss averaged over last 500 steps = 2.7997e-01, PNorm = 157.6464, GNorm = 0.2404
Meta loss on this task batch = 3.3139e-01, Meta loss averaged over last 500 steps = 2.8008e-01, PNorm = 157.6507, GNorm = 0.3270
Meta loss on this task batch = 2.9687e-01, Meta loss averaged over last 500 steps = 2.8014e-01, PNorm = 157.6551, GNorm = 0.2736
Meta loss on this task batch = 2.7869e-01, Meta loss averaged over last 500 steps = 2.8019e-01, PNorm = 157.6596, GNorm = 0.2333
Meta loss on this task batch = 2.4211e-01, Meta loss averaged over last 500 steps = 2.8013e-01, PNorm = 157.6647, GNorm = 0.4591
Meta loss on this task batch = 2.7894e-01, Meta loss averaged over last 500 steps = 2.8021e-01, PNorm = 157.6697, GNorm = 0.2701
Meta loss on this task batch = 2.4491e-01, Meta loss averaged over last 500 steps = 2.8010e-01, PNorm = 157.6747, GNorm = 0.2959
Meta loss on this task batch = 3.2034e-01, Meta loss averaged over last 500 steps = 2.8021e-01, PNorm = 157.6794, GNorm = 0.3167
Meta loss on this task batch = 2.3992e-01, Meta loss averaged over last 500 steps = 2.8022e-01, PNorm = 157.6840, GNorm = 0.2544
Meta loss on this task batch = 3.5039e-01, Meta loss averaged over last 500 steps = 2.8029e-01, PNorm = 157.6874, GNorm = 0.4023
Meta loss on this task batch = 2.2334e-01, Meta loss averaged over last 500 steps = 2.8015e-01, PNorm = 157.6915, GNorm = 0.2225
Meta loss on this task batch = 3.3212e-01, Meta loss averaged over last 500 steps = 2.8028e-01, PNorm = 157.6951, GNorm = 0.4457
Meta loss on this task batch = 2.7060e-01, Meta loss averaged over last 500 steps = 2.8020e-01, PNorm = 157.6985, GNorm = 0.3087
Meta loss on this task batch = 2.7991e-01, Meta loss averaged over last 500 steps = 2.8014e-01, PNorm = 157.7018, GNorm = 0.3413
Meta loss on this task batch = 2.7281e-01, Meta loss averaged over last 500 steps = 2.8011e-01, PNorm = 157.7054, GNorm = 0.2712
Meta loss on this task batch = 2.4762e-01, Meta loss averaged over last 500 steps = 2.8012e-01, PNorm = 157.7093, GNorm = 0.2720
Meta loss on this task batch = 2.5060e-01, Meta loss averaged over last 500 steps = 2.8003e-01, PNorm = 157.7129, GNorm = 0.2454
Meta loss on this task batch = 2.3931e-01, Meta loss averaged over last 500 steps = 2.8008e-01, PNorm = 157.7168, GNorm = 0.2560
Took 111.85740852355957 seconds to complete one epoch of meta training
Took 119.68713402748108 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453445
Epoch 769
Meta loss on this task batch = 2.4780e-01, Meta loss averaged over last 500 steps = 2.7999e-01, PNorm = 157.7207, GNorm = 0.2690
Meta loss on this task batch = 2.7346e-01, Meta loss averaged over last 500 steps = 2.7993e-01, PNorm = 157.7251, GNorm = 0.3069
Meta loss on this task batch = 2.5186e-01, Meta loss averaged over last 500 steps = 2.7973e-01, PNorm = 157.7303, GNorm = 0.2228
Meta loss on this task batch = 2.5143e-01, Meta loss averaged over last 500 steps = 2.7968e-01, PNorm = 157.7363, GNorm = 0.4699
Meta loss on this task batch = 3.2206e-01, Meta loss averaged over last 500 steps = 2.7985e-01, PNorm = 157.7418, GNorm = 0.2605
Meta loss on this task batch = 3.0479e-01, Meta loss averaged over last 500 steps = 2.7978e-01, PNorm = 157.7468, GNorm = 0.3069
Meta loss on this task batch = 2.3513e-01, Meta loss averaged over last 500 steps = 2.7978e-01, PNorm = 157.7524, GNorm = 0.2553
Meta loss on this task batch = 3.0588e-01, Meta loss averaged over last 500 steps = 2.7979e-01, PNorm = 157.7575, GNorm = 0.3064
Meta loss on this task batch = 3.1713e-01, Meta loss averaged over last 500 steps = 2.7967e-01, PNorm = 157.7631, GNorm = 0.3048
Meta loss on this task batch = 2.7900e-01, Meta loss averaged over last 500 steps = 2.7966e-01, PNorm = 157.7690, GNorm = 0.2353
Meta loss on this task batch = 2.7655e-01, Meta loss averaged over last 500 steps = 2.7967e-01, PNorm = 157.7738, GNorm = 0.3941
Meta loss on this task batch = 2.9777e-01, Meta loss averaged over last 500 steps = 2.7968e-01, PNorm = 157.7779, GNorm = 0.4883
Meta loss on this task batch = 2.7709e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 157.7815, GNorm = 0.2948
Meta loss on this task batch = 3.0455e-01, Meta loss averaged over last 500 steps = 2.7978e-01, PNorm = 157.7846, GNorm = 0.2893
Meta loss on this task batch = 2.7408e-01, Meta loss averaged over last 500 steps = 2.7980e-01, PNorm = 157.7883, GNorm = 0.4327
Meta loss on this task batch = 2.6081e-01, Meta loss averaged over last 500 steps = 2.7973e-01, PNorm = 157.7917, GNorm = 0.3074
Meta loss on this task batch = 2.7774e-01, Meta loss averaged over last 500 steps = 2.7979e-01, PNorm = 157.7955, GNorm = 0.5710
Meta loss on this task batch = 2.9260e-01, Meta loss averaged over last 500 steps = 2.7983e-01, PNorm = 157.7997, GNorm = 0.2752
Meta loss on this task batch = 2.1328e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 157.8044, GNorm = 0.2657
Took 112.52235054969788 seconds to complete one epoch of meta training
Took 120.50827980041504 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461956
Epoch 770
Meta loss on this task batch = 2.3668e-01, Meta loss averaged over last 500 steps = 2.7956e-01, PNorm = 157.8091, GNorm = 0.2884
Meta loss on this task batch = 2.8581e-01, Meta loss averaged over last 500 steps = 2.7957e-01, PNorm = 157.8134, GNorm = 0.4840
Meta loss on this task batch = 2.3866e-01, Meta loss averaged over last 500 steps = 2.7944e-01, PNorm = 157.8179, GNorm = 0.2497
Meta loss on this task batch = 2.9358e-01, Meta loss averaged over last 500 steps = 2.7945e-01, PNorm = 157.8223, GNorm = 0.4123
Meta loss on this task batch = 2.4645e-01, Meta loss averaged over last 500 steps = 2.7935e-01, PNorm = 157.8272, GNorm = 0.2415
Meta loss on this task batch = 2.8379e-01, Meta loss averaged over last 500 steps = 2.7940e-01, PNorm = 157.8320, GNorm = 0.3337
Meta loss on this task batch = 2.2412e-01, Meta loss averaged over last 500 steps = 2.7923e-01, PNorm = 157.8374, GNorm = 0.2462
Meta loss on this task batch = 3.3481e-01, Meta loss averaged over last 500 steps = 2.7925e-01, PNorm = 157.8425, GNorm = 0.3664
Meta loss on this task batch = 2.9502e-01, Meta loss averaged over last 500 steps = 2.7928e-01, PNorm = 157.8479, GNorm = 0.4475
Meta loss on this task batch = 2.6654e-01, Meta loss averaged over last 500 steps = 2.7931e-01, PNorm = 157.8534, GNorm = 0.4028
Meta loss on this task batch = 3.1521e-01, Meta loss averaged over last 500 steps = 2.7931e-01, PNorm = 157.8590, GNorm = 0.3251
Meta loss on this task batch = 2.4716e-01, Meta loss averaged over last 500 steps = 2.7919e-01, PNorm = 157.8646, GNorm = 0.2599
Meta loss on this task batch = 2.5961e-01, Meta loss averaged over last 500 steps = 2.7917e-01, PNorm = 157.8698, GNorm = 0.3737
Meta loss on this task batch = 3.1565e-01, Meta loss averaged over last 500 steps = 2.7925e-01, PNorm = 157.8742, GNorm = 0.3265
Meta loss on this task batch = 2.7034e-01, Meta loss averaged over last 500 steps = 2.7926e-01, PNorm = 157.8785, GNorm = 0.2770
Meta loss on this task batch = 3.1160e-01, Meta loss averaged over last 500 steps = 2.7937e-01, PNorm = 157.8832, GNorm = 0.3528
Meta loss on this task batch = 2.8920e-01, Meta loss averaged over last 500 steps = 2.7940e-01, PNorm = 157.8877, GNorm = 0.3010
Meta loss on this task batch = 3.1661e-01, Meta loss averaged over last 500 steps = 2.7950e-01, PNorm = 157.8921, GNorm = 0.3914
Meta loss on this task batch = 2.7402e-01, Meta loss averaged over last 500 steps = 2.7953e-01, PNorm = 157.8964, GNorm = 0.3297
Took 114.85689067840576 seconds to complete one epoch of meta training
Took 122.60605144500732 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447937
Epoch 771
Meta loss on this task batch = 2.5974e-01, Meta loss averaged over last 500 steps = 2.7951e-01, PNorm = 157.9005, GNorm = 0.3028
Meta loss on this task batch = 2.7717e-01, Meta loss averaged over last 500 steps = 2.7943e-01, PNorm = 157.9048, GNorm = 0.2537
Meta loss on this task batch = 2.9638e-01, Meta loss averaged over last 500 steps = 2.7938e-01, PNorm = 157.9088, GNorm = 0.4420
Meta loss on this task batch = 3.0423e-01, Meta loss averaged over last 500 steps = 2.7937e-01, PNorm = 157.9131, GNorm = 0.2866
Meta loss on this task batch = 2.4446e-01, Meta loss averaged over last 500 steps = 2.7941e-01, PNorm = 157.9181, GNorm = 0.3007
Meta loss on this task batch = 2.8303e-01, Meta loss averaged over last 500 steps = 2.7946e-01, PNorm = 157.9234, GNorm = 0.2625
Meta loss on this task batch = 3.2132e-01, Meta loss averaged over last 500 steps = 2.7961e-01, PNorm = 157.9281, GNorm = 0.3678
Meta loss on this task batch = 2.6961e-01, Meta loss averaged over last 500 steps = 2.7962e-01, PNorm = 157.9331, GNorm = 0.3286
Meta loss on this task batch = 3.0833e-01, Meta loss averaged over last 500 steps = 2.7973e-01, PNorm = 157.9373, GNorm = 0.3287
Meta loss on this task batch = 2.9670e-01, Meta loss averaged over last 500 steps = 2.7977e-01, PNorm = 157.9413, GNorm = 0.4035
Meta loss on this task batch = 3.2931e-01, Meta loss averaged over last 500 steps = 2.7987e-01, PNorm = 157.9453, GNorm = 0.3391
Meta loss on this task batch = 2.9628e-01, Meta loss averaged over last 500 steps = 2.7995e-01, PNorm = 157.9492, GNorm = 0.3226
Meta loss on this task batch = 2.5072e-01, Meta loss averaged over last 500 steps = 2.7981e-01, PNorm = 157.9533, GNorm = 0.2746
Meta loss on this task batch = 2.4497e-01, Meta loss averaged over last 500 steps = 2.7966e-01, PNorm = 157.9569, GNorm = 0.2833
Meta loss on this task batch = 2.6208e-01, Meta loss averaged over last 500 steps = 2.7968e-01, PNorm = 157.9596, GNorm = 0.3864
Meta loss on this task batch = 2.6974e-01, Meta loss averaged over last 500 steps = 2.7965e-01, PNorm = 157.9628, GNorm = 0.3154
Meta loss on this task batch = 2.4255e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 157.9668, GNorm = 0.3155
Meta loss on this task batch = 2.6498e-01, Meta loss averaged over last 500 steps = 2.7956e-01, PNorm = 157.9707, GNorm = 0.3391
Meta loss on this task batch = 3.1888e-01, Meta loss averaged over last 500 steps = 2.7968e-01, PNorm = 157.9741, GNorm = 0.5038
Took 115.10019993782043 seconds to complete one epoch of meta training
Took 122.02920007705688 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472927
Epoch 772
Meta loss on this task batch = 2.6050e-01, Meta loss averaged over last 500 steps = 2.7967e-01, PNorm = 157.9780, GNorm = 0.3033
Meta loss on this task batch = 2.4406e-01, Meta loss averaged over last 500 steps = 2.7960e-01, PNorm = 157.9818, GNorm = 0.3082
Meta loss on this task batch = 2.6862e-01, Meta loss averaged over last 500 steps = 2.7967e-01, PNorm = 157.9865, GNorm = 0.3458
Meta loss on this task batch = 3.1078e-01, Meta loss averaged over last 500 steps = 2.7977e-01, PNorm = 157.9901, GNorm = 0.2990
Meta loss on this task batch = 2.5060e-01, Meta loss averaged over last 500 steps = 2.7958e-01, PNorm = 157.9938, GNorm = 0.2925
Meta loss on this task batch = 2.9014e-01, Meta loss averaged over last 500 steps = 2.7951e-01, PNorm = 157.9974, GNorm = 0.2744
Meta loss on this task batch = 3.0759e-01, Meta loss averaged over last 500 steps = 2.7954e-01, PNorm = 158.0012, GNorm = 0.3726
Meta loss on this task batch = 2.5332e-01, Meta loss averaged over last 500 steps = 2.7950e-01, PNorm = 158.0056, GNorm = 0.3085
Meta loss on this task batch = 3.0168e-01, Meta loss averaged over last 500 steps = 2.7956e-01, PNorm = 158.0093, GNorm = 0.3203
Meta loss on this task batch = 3.5180e-01, Meta loss averaged over last 500 steps = 2.7970e-01, PNorm = 158.0130, GNorm = 0.4107
Meta loss on this task batch = 2.8891e-01, Meta loss averaged over last 500 steps = 2.7962e-01, PNorm = 158.0170, GNorm = 0.3312
Meta loss on this task batch = 2.5507e-01, Meta loss averaged over last 500 steps = 2.7965e-01, PNorm = 158.0209, GNorm = 0.3335
Meta loss on this task batch = 2.5743e-01, Meta loss averaged over last 500 steps = 2.7966e-01, PNorm = 158.0246, GNorm = 0.3167
Meta loss on this task batch = 2.9808e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 158.0285, GNorm = 0.2552
Meta loss on this task batch = 2.3258e-01, Meta loss averaged over last 500 steps = 2.7973e-01, PNorm = 158.0327, GNorm = 0.3077
Meta loss on this task batch = 2.5016e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 158.0374, GNorm = 0.2379
Meta loss on this task batch = 2.5525e-01, Meta loss averaged over last 500 steps = 2.7971e-01, PNorm = 158.0424, GNorm = 0.2827
Meta loss on this task batch = 2.7947e-01, Meta loss averaged over last 500 steps = 2.7977e-01, PNorm = 158.0478, GNorm = 0.2562
Meta loss on this task batch = 2.8775e-01, Meta loss averaged over last 500 steps = 2.7969e-01, PNorm = 158.0525, GNorm = 0.4521
Took 115.0227279663086 seconds to complete one epoch of meta training
Took 123.21013736724854 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.425414
Epoch 773
Meta loss on this task batch = 2.8623e-01, Meta loss averaged over last 500 steps = 2.7966e-01, PNorm = 158.0568, GNorm = 0.2600
Meta loss on this task batch = 2.8528e-01, Meta loss averaged over last 500 steps = 2.7960e-01, PNorm = 158.0612, GNorm = 0.3306
Meta loss on this task batch = 2.7766e-01, Meta loss averaged over last 500 steps = 2.7963e-01, PNorm = 158.0656, GNorm = 0.3689
Meta loss on this task batch = 2.6792e-01, Meta loss averaged over last 500 steps = 2.7958e-01, PNorm = 158.0697, GNorm = 0.2288
Meta loss on this task batch = 1.9375e-01, Meta loss averaged over last 500 steps = 2.7944e-01, PNorm = 158.0749, GNorm = 0.2856
Meta loss on this task batch = 2.5837e-01, Meta loss averaged over last 500 steps = 2.7949e-01, PNorm = 158.0802, GNorm = 0.3384
Meta loss on this task batch = 2.8111e-01, Meta loss averaged over last 500 steps = 2.7953e-01, PNorm = 158.0851, GNorm = 0.3064
Meta loss on this task batch = 2.5544e-01, Meta loss averaged over last 500 steps = 2.7942e-01, PNorm = 158.0899, GNorm = 0.2552
Meta loss on this task batch = 2.2620e-01, Meta loss averaged over last 500 steps = 2.7923e-01, PNorm = 158.0947, GNorm = 0.2351
Meta loss on this task batch = 2.9104e-01, Meta loss averaged over last 500 steps = 2.7926e-01, PNorm = 158.0989, GNorm = 0.3062
Meta loss on this task batch = 3.5965e-01, Meta loss averaged over last 500 steps = 2.7943e-01, PNorm = 158.1017, GNorm = 0.3876
Meta loss on this task batch = 2.9850e-01, Meta loss averaged over last 500 steps = 2.7955e-01, PNorm = 158.1041, GNorm = 0.3318
Meta loss on this task batch = 2.4408e-01, Meta loss averaged over last 500 steps = 2.7953e-01, PNorm = 158.1071, GNorm = 0.2678
Meta loss on this task batch = 2.7739e-01, Meta loss averaged over last 500 steps = 2.7956e-01, PNorm = 158.1096, GNorm = 0.3452
Meta loss on this task batch = 3.3096e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 158.1117, GNorm = 0.4730
Meta loss on this task batch = 2.8094e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 158.1136, GNorm = 0.2599
Meta loss on this task batch = 2.6539e-01, Meta loss averaged over last 500 steps = 2.7956e-01, PNorm = 158.1155, GNorm = 0.2572
Meta loss on this task batch = 2.6016e-01, Meta loss averaged over last 500 steps = 2.7951e-01, PNorm = 158.1176, GNorm = 0.2721
Meta loss on this task batch = 2.7371e-01, Meta loss averaged over last 500 steps = 2.7951e-01, PNorm = 158.1205, GNorm = 0.3022
Took 114.53650712966919 seconds to complete one epoch of meta training
Took 122.37822794914246 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449490
Epoch 774
Meta loss on this task batch = 2.8215e-01, Meta loss averaged over last 500 steps = 2.7946e-01, PNorm = 158.1243, GNorm = 0.2725
Meta loss on this task batch = 2.6717e-01, Meta loss averaged over last 500 steps = 2.7947e-01, PNorm = 158.1283, GNorm = 0.2886
Meta loss on this task batch = 2.8333e-01, Meta loss averaged over last 500 steps = 2.7956e-01, PNorm = 158.1323, GNorm = 0.3166
Meta loss on this task batch = 2.7238e-01, Meta loss averaged over last 500 steps = 2.7952e-01, PNorm = 158.1368, GNorm = 0.2477
Meta loss on this task batch = 2.5010e-01, Meta loss averaged over last 500 steps = 2.7942e-01, PNorm = 158.1420, GNorm = 0.2911
Meta loss on this task batch = 3.3193e-01, Meta loss averaged over last 500 steps = 2.7966e-01, PNorm = 158.1462, GNorm = 0.2721
Meta loss on this task batch = 3.4655e-01, Meta loss averaged over last 500 steps = 2.7980e-01, PNorm = 158.1507, GNorm = 0.3328
Meta loss on this task batch = 2.9016e-01, Meta loss averaged over last 500 steps = 2.7978e-01, PNorm = 158.1543, GNorm = 0.2868
Meta loss on this task batch = 2.7492e-01, Meta loss averaged over last 500 steps = 2.7977e-01, PNorm = 158.1576, GNorm = 0.3249
Meta loss on this task batch = 2.4588e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 158.1609, GNorm = 0.2562
Meta loss on this task batch = 2.8424e-01, Meta loss averaged over last 500 steps = 2.7977e-01, PNorm = 158.1642, GNorm = 0.3295
Meta loss on this task batch = 2.9415e-01, Meta loss averaged over last 500 steps = 2.7983e-01, PNorm = 158.1677, GNorm = 0.3808
Meta loss on this task batch = 2.2117e-01, Meta loss averaged over last 500 steps = 2.7982e-01, PNorm = 158.1716, GNorm = 0.2587
Meta loss on this task batch = 2.5830e-01, Meta loss averaged over last 500 steps = 2.7984e-01, PNorm = 158.1758, GNorm = 0.2438
Meta loss on this task batch = 2.4174e-01, Meta loss averaged over last 500 steps = 2.7986e-01, PNorm = 158.1802, GNorm = 0.3329
Meta loss on this task batch = 3.2561e-01, Meta loss averaged over last 500 steps = 2.7987e-01, PNorm = 158.1839, GNorm = 0.4049
Meta loss on this task batch = 2.4987e-01, Meta loss averaged over last 500 steps = 2.7984e-01, PNorm = 158.1877, GNorm = 0.2797
Meta loss on this task batch = 2.4123e-01, Meta loss averaged over last 500 steps = 2.7979e-01, PNorm = 158.1920, GNorm = 0.3395
Meta loss on this task batch = 2.3415e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 158.1971, GNorm = 0.2722
Took 110.37531805038452 seconds to complete one epoch of meta training
Took 118.42170000076294 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462146
Epoch 775
Meta loss on this task batch = 3.1312e-01, Meta loss averaged over last 500 steps = 2.7959e-01, PNorm = 158.2022, GNorm = 0.3184
Meta loss on this task batch = 2.7572e-01, Meta loss averaged over last 500 steps = 2.7948e-01, PNorm = 158.2068, GNorm = 0.4054
Meta loss on this task batch = 2.3143e-01, Meta loss averaged over last 500 steps = 2.7948e-01, PNorm = 158.2118, GNorm = 0.2458
Meta loss on this task batch = 2.4248e-01, Meta loss averaged over last 500 steps = 2.7930e-01, PNorm = 158.2164, GNorm = 0.3007
Meta loss on this task batch = 3.2129e-01, Meta loss averaged over last 500 steps = 2.7943e-01, PNorm = 158.2204, GNorm = 0.3567
Meta loss on this task batch = 2.8360e-01, Meta loss averaged over last 500 steps = 2.7944e-01, PNorm = 158.2241, GNorm = 0.2769
Meta loss on this task batch = 2.7605e-01, Meta loss averaged over last 500 steps = 2.7940e-01, PNorm = 158.2284, GNorm = 0.2453
Meta loss on this task batch = 2.7744e-01, Meta loss averaged over last 500 steps = 2.7942e-01, PNorm = 158.2324, GNorm = 0.3817
Meta loss on this task batch = 2.5900e-01, Meta loss averaged over last 500 steps = 2.7935e-01, PNorm = 158.2360, GNorm = 0.3280
Meta loss on this task batch = 2.9869e-01, Meta loss averaged over last 500 steps = 2.7930e-01, PNorm = 158.2398, GNorm = 0.2525
Meta loss on this task batch = 2.6629e-01, Meta loss averaged over last 500 steps = 2.7936e-01, PNorm = 158.2434, GNorm = 0.3727
Meta loss on this task batch = 3.1668e-01, Meta loss averaged over last 500 steps = 2.7944e-01, PNorm = 158.2476, GNorm = 0.3704
Meta loss on this task batch = 2.8627e-01, Meta loss averaged over last 500 steps = 2.7949e-01, PNorm = 158.2522, GNorm = 0.3417
Meta loss on this task batch = 3.0551e-01, Meta loss averaged over last 500 steps = 2.7965e-01, PNorm = 158.2570, GNorm = 0.3277
Meta loss on this task batch = 2.9974e-01, Meta loss averaged over last 500 steps = 2.7954e-01, PNorm = 158.2618, GNorm = 0.3058
Meta loss on this task batch = 2.7454e-01, Meta loss averaged over last 500 steps = 2.7945e-01, PNorm = 158.2670, GNorm = 0.2355
Meta loss on this task batch = 2.9401e-01, Meta loss averaged over last 500 steps = 2.7950e-01, PNorm = 158.2722, GNorm = 0.3448
Meta loss on this task batch = 2.4955e-01, Meta loss averaged over last 500 steps = 2.7939e-01, PNorm = 158.2781, GNorm = 0.2929
Meta loss on this task batch = 3.1442e-01, Meta loss averaged over last 500 steps = 2.7938e-01, PNorm = 158.2837, GNorm = 0.3965
Took 140.92748308181763 seconds to complete one epoch of meta training
Took 147.9737572669983 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446706
Epoch 776
Meta loss on this task batch = 2.5375e-01, Meta loss averaged over last 500 steps = 2.7943e-01, PNorm = 158.2894, GNorm = 0.2924
Meta loss on this task batch = 2.4759e-01, Meta loss averaged over last 500 steps = 2.7935e-01, PNorm = 158.2960, GNorm = 0.3075
Meta loss on this task batch = 3.3740e-01, Meta loss averaged over last 500 steps = 2.7940e-01, PNorm = 158.3018, GNorm = 0.3799
Meta loss on this task batch = 2.7693e-01, Meta loss averaged over last 500 steps = 2.7950e-01, PNorm = 158.3075, GNorm = 0.2701
Meta loss on this task batch = 2.9593e-01, Meta loss averaged over last 500 steps = 2.7945e-01, PNorm = 158.3129, GNorm = 0.3556
Meta loss on this task batch = 2.5310e-01, Meta loss averaged over last 500 steps = 2.7936e-01, PNorm = 158.3186, GNorm = 0.2691
Meta loss on this task batch = 2.7548e-01, Meta loss averaged over last 500 steps = 2.7940e-01, PNorm = 158.3239, GNorm = 0.2760
Meta loss on this task batch = 2.6443e-01, Meta loss averaged over last 500 steps = 2.7940e-01, PNorm = 158.3294, GNorm = 0.2374
Meta loss on this task batch = 2.8313e-01, Meta loss averaged over last 500 steps = 2.7933e-01, PNorm = 158.3350, GNorm = 0.2902
Meta loss on this task batch = 3.1342e-01, Meta loss averaged over last 500 steps = 2.7944e-01, PNorm = 158.3407, GNorm = 0.2866
Meta loss on this task batch = 2.5917e-01, Meta loss averaged over last 500 steps = 2.7934e-01, PNorm = 158.3462, GNorm = 0.3337
Meta loss on this task batch = 2.3716e-01, Meta loss averaged over last 500 steps = 2.7922e-01, PNorm = 158.3517, GNorm = 0.3620
Meta loss on this task batch = 2.7657e-01, Meta loss averaged over last 500 steps = 2.7921e-01, PNorm = 158.3566, GNorm = 0.2910
Meta loss on this task batch = 2.3040e-01, Meta loss averaged over last 500 steps = 2.7896e-01, PNorm = 158.3616, GNorm = 0.2542
Meta loss on this task batch = 2.7854e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 158.3664, GNorm = 0.3007
Meta loss on this task batch = 2.5046e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 158.3718, GNorm = 0.2615
Meta loss on this task batch = 2.6683e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 158.3764, GNorm = 0.3237
Meta loss on this task batch = 3.4272e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 158.3812, GNorm = 0.3110
Meta loss on this task batch = 2.0622e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 158.3864, GNorm = 0.2653
Took 114.90122961997986 seconds to complete one epoch of meta training
Took 121.98444628715515 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477403
Epoch 777
Meta loss on this task batch = 2.8459e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 158.3905, GNorm = 0.3137
Meta loss on this task batch = 3.3403e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 158.3941, GNorm = 0.3463
Meta loss on this task batch = 2.9120e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 158.3977, GNorm = 0.2456
Meta loss on this task batch = 3.0765e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 158.4006, GNorm = 0.2726
Meta loss on this task batch = 2.7859e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 158.4037, GNorm = 0.3319
Meta loss on this task batch = 3.2597e-01, Meta loss averaged over last 500 steps = 2.7908e-01, PNorm = 158.4068, GNorm = 0.3405
Meta loss on this task batch = 2.5438e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 158.4104, GNorm = 0.2808
Meta loss on this task batch = 2.9651e-01, Meta loss averaged over last 500 steps = 2.7904e-01, PNorm = 158.4143, GNorm = 0.3147
Meta loss on this task batch = 2.8868e-01, Meta loss averaged over last 500 steps = 2.7913e-01, PNorm = 158.4177, GNorm = 0.2565
Meta loss on this task batch = 2.7288e-01, Meta loss averaged over last 500 steps = 2.7907e-01, PNorm = 158.4218, GNorm = 0.3058
Meta loss on this task batch = 2.4497e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 158.4265, GNorm = 0.2944
Meta loss on this task batch = 3.0174e-01, Meta loss averaged over last 500 steps = 2.7905e-01, PNorm = 158.4308, GNorm = 0.3036
Meta loss on this task batch = 2.7041e-01, Meta loss averaged over last 500 steps = 2.7909e-01, PNorm = 158.4344, GNorm = 0.3355
Meta loss on this task batch = 2.7627e-01, Meta loss averaged over last 500 steps = 2.7912e-01, PNorm = 158.4377, GNorm = 0.2569
Meta loss on this task batch = 2.6608e-01, Meta loss averaged over last 500 steps = 2.7915e-01, PNorm = 158.4405, GNorm = 0.2582
Meta loss on this task batch = 2.8445e-01, Meta loss averaged over last 500 steps = 2.7919e-01, PNorm = 158.4435, GNorm = 0.4020
Meta loss on this task batch = 2.2305e-01, Meta loss averaged over last 500 steps = 2.7904e-01, PNorm = 158.4468, GNorm = 0.2321
Meta loss on this task batch = 2.8508e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 158.4508, GNorm = 0.2950
Meta loss on this task batch = 2.6998e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 158.4544, GNorm = 0.4781
Took 124.8795690536499 seconds to complete one epoch of meta training
Took 132.85595321655273 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457357
Epoch 778
Meta loss on this task batch = 2.9803e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 158.4582, GNorm = 0.4313
Meta loss on this task batch = 2.6529e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 158.4621, GNorm = 0.2756
Meta loss on this task batch = 2.9607e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 158.4657, GNorm = 0.4593
Meta loss on this task batch = 2.5103e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 158.4700, GNorm = 0.4735
Meta loss on this task batch = 2.8938e-01, Meta loss averaged over last 500 steps = 2.7887e-01, PNorm = 158.4735, GNorm = 0.3063
Meta loss on this task batch = 2.5248e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 158.4774, GNorm = 0.2489
Meta loss on this task batch = 3.1189e-01, Meta loss averaged over last 500 steps = 2.7909e-01, PNorm = 158.4810, GNorm = 0.3196
Meta loss on this task batch = 2.0749e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 158.4850, GNorm = 0.2521
Meta loss on this task batch = 2.7992e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 158.4889, GNorm = 0.2999
Meta loss on this task batch = 2.2841e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 158.4936, GNorm = 0.2276
Meta loss on this task batch = 2.7938e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 158.4984, GNorm = 0.4138
Meta loss on this task batch = 3.2414e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 158.5028, GNorm = 0.2627
Meta loss on this task batch = 3.2052e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 158.5068, GNorm = 0.3757
Meta loss on this task batch = 2.9380e-01, Meta loss averaged over last 500 steps = 2.7902e-01, PNorm = 158.5101, GNorm = 0.3002
Meta loss on this task batch = 3.2014e-01, Meta loss averaged over last 500 steps = 2.7905e-01, PNorm = 158.5132, GNorm = 0.4106
Meta loss on this task batch = 2.8989e-01, Meta loss averaged over last 500 steps = 2.7915e-01, PNorm = 158.5160, GNorm = 0.2762
Meta loss on this task batch = 2.5411e-01, Meta loss averaged over last 500 steps = 2.7920e-01, PNorm = 158.5190, GNorm = 0.2526
Meta loss on this task batch = 2.7764e-01, Meta loss averaged over last 500 steps = 2.7907e-01, PNorm = 158.5216, GNorm = 0.2604
Meta loss on this task batch = 2.3200e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 158.5250, GNorm = 0.3613
Took 115.48967695236206 seconds to complete one epoch of meta training
Took 122.6900806427002 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478167
Epoch 779
Meta loss on this task batch = 2.8414e-01, Meta loss averaged over last 500 steps = 2.7908e-01, PNorm = 158.5288, GNorm = 0.3705
Meta loss on this task batch = 2.9977e-01, Meta loss averaged over last 500 steps = 2.7912e-01, PNorm = 158.5329, GNorm = 0.3781
Meta loss on this task batch = 3.0915e-01, Meta loss averaged over last 500 steps = 2.7908e-01, PNorm = 158.5369, GNorm = 0.2862
Meta loss on this task batch = 2.7448e-01, Meta loss averaged over last 500 steps = 2.7902e-01, PNorm = 158.5407, GNorm = 0.3402
Meta loss on this task batch = 2.3791e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 158.5450, GNorm = 0.2892
Meta loss on this task batch = 2.9190e-01, Meta loss averaged over last 500 steps = 2.7899e-01, PNorm = 158.5493, GNorm = 0.3225
Meta loss on this task batch = 2.9530e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 158.5532, GNorm = 0.2848
Meta loss on this task batch = 2.7255e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 158.5574, GNorm = 0.2513
Meta loss on this task batch = 1.8733e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 158.5618, GNorm = 0.1935
Meta loss on this task batch = 2.5227e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 158.5660, GNorm = 0.2758
Meta loss on this task batch = 3.4947e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 158.5690, GNorm = 0.5538
Meta loss on this task batch = 2.2656e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 158.5720, GNorm = 0.4470
Meta loss on this task batch = 3.3712e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 158.5743, GNorm = 0.3701
Meta loss on this task batch = 2.5749e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 158.5765, GNorm = 0.3676
Meta loss on this task batch = 2.7430e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 158.5791, GNorm = 0.3307
Meta loss on this task batch = 2.6824e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 158.5825, GNorm = 0.2942
Meta loss on this task batch = 2.9180e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 158.5859, GNorm = 0.2611
Meta loss on this task batch = 3.0783e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 158.5893, GNorm = 0.3094
Meta loss on this task batch = 3.1099e-01, Meta loss averaged over last 500 steps = 2.7890e-01, PNorm = 158.5931, GNorm = 0.3024
Took 128.28737688064575 seconds to complete one epoch of meta training
Took 136.41567087173462 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468128
Epoch 780
Meta loss on this task batch = 2.8921e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 158.5967, GNorm = 0.3213
Meta loss on this task batch = 2.8061e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 158.6002, GNorm = 0.2562
Meta loss on this task batch = 2.6172e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 158.6040, GNorm = 0.3742
Meta loss on this task batch = 2.8117e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 158.6083, GNorm = 0.2947
Meta loss on this task batch = 3.3780e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 158.6121, GNorm = 0.3226
Meta loss on this task batch = 2.6358e-01, Meta loss averaged over last 500 steps = 2.7899e-01, PNorm = 158.6161, GNorm = 0.2952
Meta loss on this task batch = 3.2544e-01, Meta loss averaged over last 500 steps = 2.7909e-01, PNorm = 158.6197, GNorm = 0.3090
Meta loss on this task batch = 2.6073e-01, Meta loss averaged over last 500 steps = 2.7909e-01, PNorm = 158.6233, GNorm = 0.2930
Meta loss on this task batch = 2.6022e-01, Meta loss averaged over last 500 steps = 2.7907e-01, PNorm = 158.6273, GNorm = 0.2609
Meta loss on this task batch = 2.4756e-01, Meta loss averaged over last 500 steps = 2.7906e-01, PNorm = 158.6316, GNorm = 0.2535
Meta loss on this task batch = 2.9393e-01, Meta loss averaged over last 500 steps = 2.7908e-01, PNorm = 158.6362, GNorm = 0.2987
Meta loss on this task batch = 2.9457e-01, Meta loss averaged over last 500 steps = 2.7913e-01, PNorm = 158.6398, GNorm = 0.3211
Meta loss on this task batch = 3.1043e-01, Meta loss averaged over last 500 steps = 2.7916e-01, PNorm = 158.6437, GNorm = 0.3004
Meta loss on this task batch = 2.6494e-01, Meta loss averaged over last 500 steps = 2.7912e-01, PNorm = 158.6480, GNorm = 0.2387
Meta loss on this task batch = 2.3982e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 158.6530, GNorm = 0.3019
Meta loss on this task batch = 2.7708e-01, Meta loss averaged over last 500 steps = 2.7901e-01, PNorm = 158.6581, GNorm = 0.4603
Meta loss on this task batch = 2.6109e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 158.6636, GNorm = 0.2681
Meta loss on this task batch = 2.6935e-01, Meta loss averaged over last 500 steps = 2.7893e-01, PNorm = 158.6688, GNorm = 0.2258
Meta loss on this task batch = 2.9118e-01, Meta loss averaged over last 500 steps = 2.7893e-01, PNorm = 158.6738, GNorm = 0.3012
Took 151.7251172065735 seconds to complete one epoch of meta training
Took 159.53108263015747 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463139
Epoch 781
Meta loss on this task batch = 2.8567e-01, Meta loss averaged over last 500 steps = 2.7902e-01, PNorm = 158.6786, GNorm = 0.2427
Meta loss on this task batch = 2.9578e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 158.6832, GNorm = 0.3035
Meta loss on this task batch = 3.0800e-01, Meta loss averaged over last 500 steps = 2.7909e-01, PNorm = 158.6885, GNorm = 0.2762
Meta loss on this task batch = 3.3116e-01, Meta loss averaged over last 500 steps = 2.7922e-01, PNorm = 158.6919, GNorm = 0.4549
Meta loss on this task batch = 2.0553e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 158.6959, GNorm = 0.2667
Meta loss on this task batch = 2.2509e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 158.7001, GNorm = 0.2632
Meta loss on this task batch = 2.6789e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 158.7040, GNorm = 0.2510
Meta loss on this task batch = 1.8964e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 158.7080, GNorm = 0.2594
Meta loss on this task batch = 2.2692e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 158.7118, GNorm = 0.5189
Meta loss on this task batch = 2.7369e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 158.7161, GNorm = 0.3465
Meta loss on this task batch = 2.8699e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 158.7195, GNorm = 0.2647
Meta loss on this task batch = 2.6241e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 158.7223, GNorm = 0.3389
Meta loss on this task batch = 3.0834e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 158.7247, GNorm = 0.4976
Meta loss on this task batch = 3.1389e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 158.7272, GNorm = 0.2998
Meta loss on this task batch = 2.6842e-01, Meta loss averaged over last 500 steps = 2.7840e-01, PNorm = 158.7289, GNorm = 0.3739
Meta loss on this task batch = 2.7333e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 158.7306, GNorm = 0.4326
Meta loss on this task batch = 2.8971e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 158.7321, GNorm = 0.2893
Meta loss on this task batch = 3.3110e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 158.7332, GNorm = 0.3441
Meta loss on this task batch = 3.2540e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 158.7337, GNorm = 0.3859
Took 113.41520023345947 seconds to complete one epoch of meta training
Took 120.81560516357422 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464055
Epoch 782
Meta loss on this task batch = 2.8108e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 158.7345, GNorm = 0.3843
Meta loss on this task batch = 3.5347e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 158.7353, GNorm = 0.4229
Meta loss on this task batch = 2.6387e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 158.7366, GNorm = 0.2667
Meta loss on this task batch = 3.0033e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 158.7385, GNorm = 0.2505
Meta loss on this task batch = 2.7534e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 158.7405, GNorm = 0.2721
Meta loss on this task batch = 2.6862e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 158.7432, GNorm = 0.2766
Meta loss on this task batch = 2.6949e-01, Meta loss averaged over last 500 steps = 2.7890e-01, PNorm = 158.7467, GNorm = 0.3421
Meta loss on this task batch = 2.7709e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 158.7509, GNorm = 0.2652
Meta loss on this task batch = 2.9839e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 158.7554, GNorm = 0.3162
Meta loss on this task batch = 2.4753e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 158.7605, GNorm = 0.3004
Meta loss on this task batch = 3.2387e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 158.7663, GNorm = 0.3476
Meta loss on this task batch = 2.0330e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 158.7730, GNorm = 0.2449
Meta loss on this task batch = 2.5190e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 158.7796, GNorm = 0.2720
Meta loss on this task batch = 2.5565e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 158.7859, GNorm = 0.3476
Meta loss on this task batch = 2.5708e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 158.7919, GNorm = 0.2593
Meta loss on this task batch = 2.6143e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 158.7979, GNorm = 0.3048
Meta loss on this task batch = 2.5295e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 158.8037, GNorm = 0.2752
Meta loss on this task batch = 2.8686e-01, Meta loss averaged over last 500 steps = 2.7864e-01, PNorm = 158.8097, GNorm = 0.2558
Meta loss on this task batch = 3.3996e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 158.8149, GNorm = 0.4268
Took 110.53031802177429 seconds to complete one epoch of meta training
Took 117.53990459442139 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467015
Epoch 783
Meta loss on this task batch = 2.1749e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 158.8196, GNorm = 0.3099
Meta loss on this task batch = 2.6218e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 158.8245, GNorm = 0.2510
Meta loss on this task batch = 2.3199e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 158.8297, GNorm = 0.3899
Meta loss on this task batch = 2.6013e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 158.8344, GNorm = 0.2739
Meta loss on this task batch = 3.5877e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 158.8385, GNorm = 0.3157
Meta loss on this task batch = 2.8822e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 158.8429, GNorm = 0.3147
Meta loss on this task batch = 2.6243e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 158.8468, GNorm = 0.2872
Meta loss on this task batch = 2.9361e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 158.8505, GNorm = 0.2696
Meta loss on this task batch = 3.1326e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 158.8542, GNorm = 0.2938
Meta loss on this task batch = 2.5597e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 158.8577, GNorm = 0.3230
Meta loss on this task batch = 2.9805e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 158.8610, GNorm = 0.3167
Meta loss on this task batch = 3.1978e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 158.8633, GNorm = 0.3584
Meta loss on this task batch = 3.1077e-01, Meta loss averaged over last 500 steps = 2.7904e-01, PNorm = 158.8665, GNorm = 0.2728
Meta loss on this task batch = 2.4745e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 158.8705, GNorm = 0.2309
Meta loss on this task batch = 2.3896e-01, Meta loss averaged over last 500 steps = 2.7887e-01, PNorm = 158.8750, GNorm = 0.2375
Meta loss on this task batch = 3.5061e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 158.8785, GNorm = 0.4254
Meta loss on this task batch = 2.8546e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 158.8822, GNorm = 0.4168
Meta loss on this task batch = 2.5131e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 158.8866, GNorm = 0.2617
Meta loss on this task batch = 2.9971e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 158.8919, GNorm = 0.3300
Took 113.66816258430481 seconds to complete one epoch of meta training
Took 121.86304831504822 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469167
Epoch 784
Meta loss on this task batch = 2.5825e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 158.8975, GNorm = 0.2433
Meta loss on this task batch = 2.8743e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 158.9035, GNorm = 0.3617
Meta loss on this task batch = 3.4072e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 158.9095, GNorm = 0.3327
Meta loss on this task batch = 2.5580e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 158.9157, GNorm = 0.2758
Meta loss on this task batch = 2.7667e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 158.9223, GNorm = 0.3199
Meta loss on this task batch = 2.7107e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 158.9286, GNorm = 0.2920
Meta loss on this task batch = 2.9525e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 158.9352, GNorm = 0.3178
Meta loss on this task batch = 2.6360e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 158.9419, GNorm = 0.4206
Meta loss on this task batch = 2.4002e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 158.9487, GNorm = 0.4784
Meta loss on this task batch = 2.4093e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 158.9553, GNorm = 0.2387
Meta loss on this task batch = 3.2100e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 158.9609, GNorm = 0.3492
Meta loss on this task batch = 2.7978e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 158.9662, GNorm = 0.4035
Meta loss on this task batch = 3.2710e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 158.9709, GNorm = 0.4419
Meta loss on this task batch = 2.9776e-01, Meta loss averaged over last 500 steps = 2.7875e-01, PNorm = 158.9746, GNorm = 0.3108
Meta loss on this task batch = 3.2035e-01, Meta loss averaged over last 500 steps = 2.7887e-01, PNorm = 158.9772, GNorm = 0.3412
Meta loss on this task batch = 2.7008e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 158.9798, GNorm = 0.3017
Meta loss on this task batch = 3.0687e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 158.9829, GNorm = 0.4275
Meta loss on this task batch = 2.3971e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 158.9866, GNorm = 0.2434
Meta loss on this task batch = 3.2665e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 158.9895, GNorm = 0.3597
Took 114.33981847763062 seconds to complete one epoch of meta training
Took 121.3674705028534 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479674
Epoch 785
Meta loss on this task batch = 2.5982e-01, Meta loss averaged over last 500 steps = 2.7896e-01, PNorm = 158.9924, GNorm = 0.2619
Meta loss on this task batch = 2.9294e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 158.9953, GNorm = 0.2799
Meta loss on this task batch = 3.0707e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 158.9983, GNorm = 0.4506
Meta loss on this task batch = 2.8102e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 159.0018, GNorm = 0.2595
Meta loss on this task batch = 2.6782e-01, Meta loss averaged over last 500 steps = 2.7890e-01, PNorm = 159.0059, GNorm = 0.2790
Meta loss on this task batch = 2.4612e-01, Meta loss averaged over last 500 steps = 2.7881e-01, PNorm = 159.0099, GNorm = 0.3343
Meta loss on this task batch = 2.8387e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 159.0143, GNorm = 0.3037
Meta loss on this task batch = 2.3551e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 159.0192, GNorm = 0.2651
Meta loss on this task batch = 2.5248e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 159.0240, GNorm = 0.3501
Meta loss on this task batch = 2.6212e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 159.0290, GNorm = 0.2413
Meta loss on this task batch = 2.4197e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 159.0337, GNorm = 0.2413
Meta loss on this task batch = 2.6794e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 159.0387, GNorm = 0.2536
Meta loss on this task batch = 2.8220e-01, Meta loss averaged over last 500 steps = 2.7839e-01, PNorm = 159.0438, GNorm = 0.2466
Meta loss on this task batch = 2.1391e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 159.0490, GNorm = 0.3083
Meta loss on this task batch = 3.1153e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 159.0540, GNorm = 0.3949
Meta loss on this task batch = 2.5699e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 159.0582, GNorm = 0.3514
Meta loss on this task batch = 2.9339e-01, Meta loss averaged over last 500 steps = 2.7812e-01, PNorm = 159.0618, GNorm = 0.5837
Meta loss on this task batch = 3.5041e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 159.0647, GNorm = 0.3757
Meta loss on this task batch = 3.2085e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 159.0674, GNorm = 0.6190
Took 115.14148139953613 seconds to complete one epoch of meta training
Took 122.13355946540833 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489924
Epoch 786
Meta loss on this task batch = 2.3675e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 159.0706, GNorm = 0.2700
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 159.0743, GNorm = 0.3674
Meta loss on this task batch = 2.9325e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 159.0779, GNorm = 0.2663
Meta loss on this task batch = 2.6966e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 159.0821, GNorm = 0.3229
Meta loss on this task batch = 2.2315e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 159.0859, GNorm = 0.2486
Meta loss on this task batch = 3.3543e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 159.0894, GNorm = 0.3566
Meta loss on this task batch = 2.9361e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 159.0926, GNorm = 0.2777
Meta loss on this task batch = 2.2170e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 159.0967, GNorm = 0.4357
Meta loss on this task batch = 3.3497e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 159.1009, GNorm = 0.3999
Meta loss on this task batch = 2.6831e-01, Meta loss averaged over last 500 steps = 2.7875e-01, PNorm = 159.1053, GNorm = 0.2913
Meta loss on this task batch = 3.0757e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 159.1104, GNorm = 0.2696
Meta loss on this task batch = 3.1244e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 159.1157, GNorm = 0.3856
Meta loss on this task batch = 2.6974e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 159.1204, GNorm = 0.3104
Meta loss on this task batch = 3.4632e-01, Meta loss averaged over last 500 steps = 2.7908e-01, PNorm = 159.1247, GNorm = 0.3117
Meta loss on this task batch = 1.9097e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 159.1290, GNorm = 0.2915
Meta loss on this task batch = 3.0570e-01, Meta loss averaged over last 500 steps = 2.7908e-01, PNorm = 159.1334, GNorm = 0.2635
Meta loss on this task batch = 2.5454e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 159.1379, GNorm = 0.2580
Meta loss on this task batch = 2.1842e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 159.1426, GNorm = 0.2858
Meta loss on this task batch = 2.5780e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 159.1477, GNorm = 0.3211
Took 115.61962914466858 seconds to complete one epoch of meta training
Took 123.85727882385254 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462558
Epoch 787
Meta loss on this task batch = 2.9468e-01, Meta loss averaged over last 500 steps = 2.7881e-01, PNorm = 159.1517, GNorm = 0.4224
Meta loss on this task batch = 2.9560e-01, Meta loss averaged over last 500 steps = 2.7881e-01, PNorm = 159.1557, GNorm = 0.3182
Meta loss on this task batch = 3.2323e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 159.1598, GNorm = 0.2965
Meta loss on this task batch = 2.5550e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 159.1640, GNorm = 0.2487
Meta loss on this task batch = 3.2308e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 159.1670, GNorm = 0.3192
Meta loss on this task batch = 2.2603e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 159.1707, GNorm = 0.2337
Meta loss on this task batch = 2.4363e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 159.1744, GNorm = 0.2592
Meta loss on this task batch = 2.9242e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 159.1781, GNorm = 0.2815
Meta loss on this task batch = 3.2738e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 159.1809, GNorm = 0.3107
Meta loss on this task batch = 2.8098e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 159.1837, GNorm = 0.2692
Meta loss on this task batch = 2.3123e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 159.1870, GNorm = 0.2502
Meta loss on this task batch = 3.2138e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 159.1900, GNorm = 0.3035
Meta loss on this task batch = 2.1727e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 159.1932, GNorm = 0.2519
Meta loss on this task batch = 3.2025e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 159.1968, GNorm = 0.3038
Meta loss on this task batch = 2.5973e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 159.2004, GNorm = 0.2607
Meta loss on this task batch = 3.2840e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 159.2041, GNorm = 0.3138
Meta loss on this task batch = 2.8274e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 159.2073, GNorm = 0.4732
Meta loss on this task batch = 2.4334e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 159.2112, GNorm = 0.4214
Meta loss on this task batch = 2.9401e-01, Meta loss averaged over last 500 steps = 2.7875e-01, PNorm = 159.2149, GNorm = 0.3337
Took 113.93780541419983 seconds to complete one epoch of meta training
Took 121.94129085540771 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486349
Epoch 788
Meta loss on this task batch = 3.5100e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 159.2184, GNorm = 0.3170
Meta loss on this task batch = 3.0660e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 159.2217, GNorm = 0.2745
Meta loss on this task batch = 2.7173e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 159.2252, GNorm = 0.3192
Meta loss on this task batch = 3.0683e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 159.2292, GNorm = 0.3088
Meta loss on this task batch = 2.6491e-01, Meta loss averaged over last 500 steps = 2.7899e-01, PNorm = 159.2337, GNorm = 0.3379
Meta loss on this task batch = 2.4691e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 159.2386, GNorm = 0.3221
Meta loss on this task batch = 2.7503e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 159.2441, GNorm = 0.3826
Meta loss on this task batch = 2.5115e-01, Meta loss averaged over last 500 steps = 2.7874e-01, PNorm = 159.2494, GNorm = 0.2218
Meta loss on this task batch = 3.0264e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 159.2550, GNorm = 0.2767
Meta loss on this task batch = 2.7457e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 159.2610, GNorm = 0.3325
Meta loss on this task batch = 2.9462e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 159.2667, GNorm = 0.2569
Meta loss on this task batch = 2.3923e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 159.2723, GNorm = 0.2756
Meta loss on this task batch = 2.7041e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 159.2778, GNorm = 0.2362
Meta loss on this task batch = 2.2179e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 159.2835, GNorm = 0.2251
Meta loss on this task batch = 3.0036e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 159.2887, GNorm = 0.4004
Meta loss on this task batch = 2.7690e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 159.2942, GNorm = 0.2531
Meta loss on this task batch = 2.7048e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 159.2995, GNorm = 0.3143
Meta loss on this task batch = 2.4391e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 159.3050, GNorm = 0.3634
Meta loss on this task batch = 3.1760e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 159.3098, GNorm = 0.4149
Took 125.3030195236206 seconds to complete one epoch of meta training
Took 133.16391038894653 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471930
Epoch 789
Meta loss on this task batch = 2.5917e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 159.3141, GNorm = 0.2524
Meta loss on this task batch = 3.2524e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 159.3181, GNorm = 0.3188
Meta loss on this task batch = 2.2866e-01, Meta loss averaged over last 500 steps = 2.7833e-01, PNorm = 159.3226, GNorm = 0.3588
Meta loss on this task batch = 2.6879e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 159.3274, GNorm = 0.3851
Meta loss on this task batch = 2.7547e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 159.3318, GNorm = 0.2834
Meta loss on this task batch = 2.7996e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 159.3361, GNorm = 0.3596
Meta loss on this task batch = 2.6012e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 159.3395, GNorm = 0.3784
Meta loss on this task batch = 2.8409e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 159.3421, GNorm = 0.3019
Meta loss on this task batch = 3.2210e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 159.3443, GNorm = 0.3451
Meta loss on this task batch = 2.5958e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 159.3475, GNorm = 0.3464
Meta loss on this task batch = 2.8075e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 159.3512, GNorm = 0.2920
Meta loss on this task batch = 2.4825e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 159.3555, GNorm = 0.2924
Meta loss on this task batch = 2.6643e-01, Meta loss averaged over last 500 steps = 2.7821e-01, PNorm = 159.3598, GNorm = 0.2754
Meta loss on this task batch = 2.6659e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 159.3638, GNorm = 0.2818
Meta loss on this task batch = 2.6392e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 159.3679, GNorm = 0.2551
Meta loss on this task batch = 2.8097e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 159.3719, GNorm = 0.2774
Meta loss on this task batch = 3.2446e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 159.3757, GNorm = 0.3819
Meta loss on this task batch = 2.4136e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 159.3797, GNorm = 0.3571
Meta loss on this task batch = 3.1155e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 159.3831, GNorm = 0.3606
Took 142.35959196090698 seconds to complete one epoch of meta training
Took 150.57761096954346 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467423
Epoch 790
Meta loss on this task batch = 2.4518e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 159.3875, GNorm = 0.3786
Meta loss on this task batch = 2.1179e-01, Meta loss averaged over last 500 steps = 2.7789e-01, PNorm = 159.3921, GNorm = 0.2172
Meta loss on this task batch = 3.1987e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 159.3966, GNorm = 0.4521
Meta loss on this task batch = 2.8522e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 159.4010, GNorm = 0.2743
Meta loss on this task batch = 2.7220e-01, Meta loss averaged over last 500 steps = 2.7812e-01, PNorm = 159.4056, GNorm = 0.3505
Meta loss on this task batch = 3.7626e-01, Meta loss averaged over last 500 steps = 2.7840e-01, PNorm = 159.4094, GNorm = 0.3445
Meta loss on this task batch = 2.7362e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 159.4136, GNorm = 0.2485
Meta loss on this task batch = 2.5103e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 159.4179, GNorm = 0.2932
Meta loss on this task batch = 2.7576e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 159.4218, GNorm = 0.2988
Meta loss on this task batch = 2.7227e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 159.4254, GNorm = 0.2700
Meta loss on this task batch = 2.9203e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 159.4282, GNorm = 0.2626
Meta loss on this task batch = 2.8796e-01, Meta loss averaged over last 500 steps = 2.7839e-01, PNorm = 159.4311, GNorm = 0.2633
Meta loss on this task batch = 2.3559e-01, Meta loss averaged over last 500 steps = 2.7839e-01, PNorm = 159.4342, GNorm = 0.2315
Meta loss on this task batch = 2.8147e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 159.4375, GNorm = 0.2845
Meta loss on this task batch = 2.7724e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 159.4406, GNorm = 0.2917
Meta loss on this task batch = 2.3587e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 159.4439, GNorm = 0.2670
Meta loss on this task batch = 2.6157e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 159.4477, GNorm = 0.3579
Meta loss on this task batch = 2.7322e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 159.4517, GNorm = 0.2819
Meta loss on this task batch = 2.8163e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 159.4553, GNorm = 0.4554
Took 126.9444637298584 seconds to complete one epoch of meta training
Took 134.3650345802307 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497266
Epoch 791
Meta loss on this task batch = 3.0942e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 159.4584, GNorm = 0.3558
Meta loss on this task batch = 2.2755e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 159.4622, GNorm = 0.2481
Meta loss on this task batch = 2.8257e-01, Meta loss averaged over last 500 steps = 2.7783e-01, PNorm = 159.4658, GNorm = 0.3129
Meta loss on this task batch = 2.9486e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 159.4691, GNorm = 0.3525
Meta loss on this task batch = 2.6386e-01, Meta loss averaged over last 500 steps = 2.7798e-01, PNorm = 159.4723, GNorm = 0.2609
Meta loss on this task batch = 2.7563e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 159.4761, GNorm = 0.3349
Meta loss on this task batch = 2.9444e-01, Meta loss averaged over last 500 steps = 2.7819e-01, PNorm = 159.4803, GNorm = 0.3905
Meta loss on this task batch = 2.8838e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 159.4839, GNorm = 0.3828
Meta loss on this task batch = 2.5402e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 159.4876, GNorm = 0.2716
Meta loss on this task batch = 2.3639e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 159.4920, GNorm = 0.2870
Meta loss on this task batch = 3.3215e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 159.4965, GNorm = 0.2992
Meta loss on this task batch = 2.6695e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 159.5012, GNorm = 0.3019
Meta loss on this task batch = 2.4088e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 159.5059, GNorm = 0.2715
Meta loss on this task batch = 3.4844e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 159.5095, GNorm = 0.3740
Meta loss on this task batch = 3.0048e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 159.5133, GNorm = 0.3168
Meta loss on this task batch = 2.4433e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 159.5176, GNorm = 0.2314
Meta loss on this task batch = 2.4261e-01, Meta loss averaged over last 500 steps = 2.7800e-01, PNorm = 159.5223, GNorm = 0.2957
Meta loss on this task batch = 2.7521e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 159.5267, GNorm = 0.2470
Meta loss on this task batch = 2.9708e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 159.5304, GNorm = 0.4053
Took 149.74492144584656 seconds to complete one epoch of meta training
Took 156.6948413848877 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472299
Epoch 792
Meta loss on this task batch = 2.9347e-01, Meta loss averaged over last 500 steps = 2.7800e-01, PNorm = 159.5340, GNorm = 0.2972
Meta loss on this task batch = 2.9506e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 159.5377, GNorm = 0.2434
Meta loss on this task batch = 3.0422e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 159.5408, GNorm = 0.3051
Meta loss on this task batch = 2.7705e-01, Meta loss averaged over last 500 steps = 2.7819e-01, PNorm = 159.5449, GNorm = 0.5354
Meta loss on this task batch = 3.6218e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 159.5489, GNorm = 0.3310
Meta loss on this task batch = 2.8510e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 159.5536, GNorm = 0.3124
Meta loss on this task batch = 3.0284e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 159.5585, GNorm = 0.3161
Meta loss on this task batch = 2.8167e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 159.5630, GNorm = 0.3152
Meta loss on this task batch = 2.9499e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 159.5683, GNorm = 0.2992
Meta loss on this task batch = 2.4053e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 159.5735, GNorm = 0.2747
Meta loss on this task batch = 2.7080e-01, Meta loss averaged over last 500 steps = 2.7833e-01, PNorm = 159.5774, GNorm = 0.2840
Meta loss on this task batch = 2.4816e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 159.5823, GNorm = 0.3322
Meta loss on this task batch = 3.1474e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 159.5867, GNorm = 0.3872
Meta loss on this task batch = 2.1571e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 159.5912, GNorm = 0.2673
Meta loss on this task batch = 2.5983e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 159.5952, GNorm = 0.3659
Meta loss on this task batch = 2.6352e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 159.5992, GNorm = 0.2933
Meta loss on this task batch = 2.9557e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 159.6033, GNorm = 0.4282
Meta loss on this task batch = 2.8805e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 159.6076, GNorm = 0.2756
Meta loss on this task batch = 2.3589e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 159.6116, GNorm = 0.4390
Took 117.71818518638611 seconds to complete one epoch of meta training
Took 126.3870644569397 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.445572
Epoch 793
Meta loss on this task batch = 3.3158e-01, Meta loss averaged over last 500 steps = 2.7821e-01, PNorm = 159.6156, GNorm = 0.4397
Meta loss on this task batch = 2.4684e-01, Meta loss averaged over last 500 steps = 2.7821e-01, PNorm = 159.6204, GNorm = 0.2425
Meta loss on this task batch = 2.6506e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 159.6256, GNorm = 0.3266
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 2.7817e-01, PNorm = 159.6306, GNorm = 0.3494
Meta loss on this task batch = 2.8220e-01, Meta loss averaged over last 500 steps = 2.7816e-01, PNorm = 159.6358, GNorm = 0.2876
Meta loss on this task batch = 2.3822e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 159.6413, GNorm = 0.2632
Meta loss on this task batch = 2.3271e-01, Meta loss averaged over last 500 steps = 2.7787e-01, PNorm = 159.6471, GNorm = 0.2387
Meta loss on this task batch = 2.7881e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 159.6533, GNorm = 0.2417
Meta loss on this task batch = 2.4902e-01, Meta loss averaged over last 500 steps = 2.7780e-01, PNorm = 159.6593, GNorm = 0.2486
Meta loss on this task batch = 3.3180e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 159.6649, GNorm = 0.3026
Meta loss on this task batch = 2.6383e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 159.6702, GNorm = 0.2436
Meta loss on this task batch = 2.7275e-01, Meta loss averaged over last 500 steps = 2.7798e-01, PNorm = 159.6757, GNorm = 0.3038
Meta loss on this task batch = 2.8282e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 159.6804, GNorm = 0.3121
Meta loss on this task batch = 3.0424e-01, Meta loss averaged over last 500 steps = 2.7781e-01, PNorm = 159.6845, GNorm = 0.2835
Meta loss on this task batch = 2.6666e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 159.6887, GNorm = 0.3531
Meta loss on this task batch = 3.6208e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 159.6917, GNorm = 0.5413
Meta loss on this task batch = 3.2669e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 159.6942, GNorm = 0.6728
Meta loss on this task batch = 3.0567e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 159.6967, GNorm = 0.3996
Meta loss on this task batch = 2.4538e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 159.6988, GNorm = 0.3250
Took 126.64126491546631 seconds to complete one epoch of meta training
Took 134.62273383140564 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453577
Epoch 794
Meta loss on this task batch = 2.2272e-01, Meta loss averaged over last 500 steps = 2.7789e-01, PNorm = 159.7012, GNorm = 0.2376
Meta loss on this task batch = 2.6928e-01, Meta loss averaged over last 500 steps = 2.7793e-01, PNorm = 159.7040, GNorm = 0.2618
Meta loss on this task batch = 2.6227e-01, Meta loss averaged over last 500 steps = 2.7789e-01, PNorm = 159.7069, GNorm = 0.2707
Meta loss on this task batch = 3.1726e-01, Meta loss averaged over last 500 steps = 2.7808e-01, PNorm = 159.7098, GNorm = 0.3100
Meta loss on this task batch = 2.8483e-01, Meta loss averaged over last 500 steps = 2.7808e-01, PNorm = 159.7132, GNorm = 0.2913
Meta loss on this task batch = 3.4247e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 159.7163, GNorm = 0.3839
Meta loss on this task batch = 2.9422e-01, Meta loss averaged over last 500 steps = 2.7833e-01, PNorm = 159.7198, GNorm = 0.2531
Meta loss on this task batch = 2.9428e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 159.7230, GNorm = 0.3080
Meta loss on this task batch = 2.2774e-01, Meta loss averaged over last 500 steps = 2.7817e-01, PNorm = 159.7262, GNorm = 0.2785
Meta loss on this task batch = 2.6044e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 159.7299, GNorm = 0.2556
Meta loss on this task batch = 2.8251e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 159.7334, GNorm = 0.2870
Meta loss on this task batch = 2.3113e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 159.7372, GNorm = 0.3461
Meta loss on this task batch = 3.4781e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 159.7407, GNorm = 0.2845
Meta loss on this task batch = 3.4019e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 159.7440, GNorm = 0.2758
Meta loss on this task batch = 3.3971e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 159.7464, GNorm = 0.3202
Meta loss on this task batch = 2.5635e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 159.7492, GNorm = 0.2540
Meta loss on this task batch = 2.4315e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 159.7527, GNorm = 0.3063
Meta loss on this task batch = 2.7927e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 159.7563, GNorm = 0.4126
Meta loss on this task batch = 2.4006e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 159.7596, GNorm = 0.2888
Took 120.19131207466125 seconds to complete one epoch of meta training
Took 128.39761757850647 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479069
Epoch 795
Meta loss on this task batch = 2.9662e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 159.7632, GNorm = 0.3536
Meta loss on this task batch = 2.4465e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 159.7672, GNorm = 0.3351
Meta loss on this task batch = 2.8116e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 159.7702, GNorm = 0.4462
Meta loss on this task batch = 2.6152e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 159.7735, GNorm = 0.2963
Meta loss on this task batch = 2.8878e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 159.7768, GNorm = 0.2817
Meta loss on this task batch = 2.4209e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 159.7806, GNorm = 0.2562
Meta loss on this task batch = 2.9071e-01, Meta loss averaged over last 500 steps = 2.7839e-01, PNorm = 159.7838, GNorm = 0.3276
Meta loss on this task batch = 2.5311e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 159.7872, GNorm = 0.4095
Meta loss on this task batch = 3.1064e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 159.7903, GNorm = 0.2920
Meta loss on this task batch = 2.7383e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 159.7931, GNorm = 0.3245
Meta loss on this task batch = 2.3460e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 159.7957, GNorm = 0.2623
Meta loss on this task batch = 2.6412e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 159.7986, GNorm = 0.3621
Meta loss on this task batch = 2.4255e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 159.8018, GNorm = 0.2716
Meta loss on this task batch = 3.4373e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 159.8046, GNorm = 0.3466
Meta loss on this task batch = 2.6167e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 159.8076, GNorm = 0.2704
Meta loss on this task batch = 2.6989e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 159.8109, GNorm = 0.2510
Meta loss on this task batch = 3.1119e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 159.8142, GNorm = 0.3012
Meta loss on this task batch = 2.6710e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 159.8175, GNorm = 0.2583
Meta loss on this task batch = 3.1648e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 159.8205, GNorm = 0.4641
Took 154.39438557624817 seconds to complete one epoch of meta training
Took 162.42194414138794 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471382
Epoch 796
Meta loss on this task batch = 3.3171e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 159.8238, GNorm = 0.3275
Meta loss on this task batch = 2.8202e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 159.8280, GNorm = 0.3201
Meta loss on this task batch = 2.5905e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 159.8326, GNorm = 0.3143
Meta loss on this task batch = 2.3828e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 159.8370, GNorm = 0.2259
Meta loss on this task batch = 2.8047e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 159.8416, GNorm = 0.2715
Meta loss on this task batch = 2.6460e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 159.8460, GNorm = 0.2625
Meta loss on this task batch = 3.0783e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 159.8505, GNorm = 0.3477
Meta loss on this task batch = 2.2639e-01, Meta loss averaged over last 500 steps = 2.7840e-01, PNorm = 159.8550, GNorm = 0.1940
Meta loss on this task batch = 3.2830e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 159.8594, GNorm = 0.2923
Meta loss on this task batch = 2.9012e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 159.8633, GNorm = 0.3125
Meta loss on this task batch = 2.3640e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 159.8670, GNorm = 0.2448
Meta loss on this task batch = 2.7647e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 159.8706, GNorm = 0.3597
Meta loss on this task batch = 2.7006e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 159.8750, GNorm = 0.2661
Meta loss on this task batch = 2.5758e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 159.8795, GNorm = 0.2941
Meta loss on this task batch = 2.6108e-01, Meta loss averaged over last 500 steps = 2.7840e-01, PNorm = 159.8843, GNorm = 0.4872
Meta loss on this task batch = 2.6933e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 159.8885, GNorm = 0.3026
Meta loss on this task batch = 2.7227e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 159.8934, GNorm = 0.3644
Meta loss on this task batch = 2.4723e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 159.8984, GNorm = 0.3734
Meta loss on this task batch = 3.0040e-01, Meta loss averaged over last 500 steps = 2.7840e-01, PNorm = 159.9034, GNorm = 0.4396
Took 111.21464896202087 seconds to complete one epoch of meta training
Took 119.77666521072388 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473217
Epoch 797
Meta loss on this task batch = 2.8953e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 159.9080, GNorm = 0.3230
Meta loss on this task batch = 3.3817e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 159.9115, GNorm = 0.3805
Meta loss on this task batch = 2.9034e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 159.9151, GNorm = 0.2949
Meta loss on this task batch = 2.9652e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 159.9188, GNorm = 0.2850
Meta loss on this task batch = 3.1139e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 159.9233, GNorm = 0.3834
Meta loss on this task batch = 2.7487e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 159.9281, GNorm = 0.2851
Meta loss on this task batch = 2.7431e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 159.9328, GNorm = 0.2568
Meta loss on this task batch = 2.9289e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 159.9374, GNorm = 0.3700
Meta loss on this task batch = 2.6984e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 159.9420, GNorm = 0.3082
Meta loss on this task batch = 2.7781e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 159.9462, GNorm = 0.2761
Meta loss on this task batch = 2.6467e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 159.9505, GNorm = 0.3770
Meta loss on this task batch = 2.8790e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 159.9548, GNorm = 0.3363
Meta loss on this task batch = 2.7787e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 159.9594, GNorm = 0.3100
Meta loss on this task batch = 2.6208e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 159.9648, GNorm = 0.2509
Meta loss on this task batch = 2.4159e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 159.9701, GNorm = 0.3004
Meta loss on this task batch = 2.5143e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 159.9755, GNorm = 0.2779
Meta loss on this task batch = 3.0194e-01, Meta loss averaged over last 500 steps = 2.7808e-01, PNorm = 159.9811, GNorm = 0.2849
Meta loss on this task batch = 3.4911e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 159.9857, GNorm = 0.3202
Meta loss on this task batch = 2.7674e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 159.9898, GNorm = 0.3034
Took 112.73991560935974 seconds to complete one epoch of meta training
Took 121.04766273498535 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489855
Epoch 798
Meta loss on this task batch = 3.3568e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 159.9933, GNorm = 0.3655
Meta loss on this task batch = 2.3540e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 159.9968, GNorm = 0.3567
Meta loss on this task batch = 3.1853e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 159.9995, GNorm = 0.3175
Meta loss on this task batch = 2.6247e-01, Meta loss averaged over last 500 steps = 2.7850e-01, PNorm = 160.0022, GNorm = 0.3050
Meta loss on this task batch = 3.0120e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 160.0056, GNorm = 0.2764
Meta loss on this task batch = 2.7861e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 160.0089, GNorm = 0.4443
Meta loss on this task batch = 2.8357e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 160.0128, GNorm = 0.2549
Meta loss on this task batch = 2.6602e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 160.0167, GNorm = 0.2816
Meta loss on this task batch = 3.0181e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 160.0204, GNorm = 0.2541
Meta loss on this task batch = 2.5455e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 160.0247, GNorm = 0.2313
Meta loss on this task batch = 2.8538e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 160.0296, GNorm = 0.2841
Meta loss on this task batch = 2.3968e-01, Meta loss averaged over last 500 steps = 2.7850e-01, PNorm = 160.0351, GNorm = 0.3197
Meta loss on this task batch = 2.5035e-01, Meta loss averaged over last 500 steps = 2.7839e-01, PNorm = 160.0406, GNorm = 0.2602
Meta loss on this task batch = 2.2361e-01, Meta loss averaged over last 500 steps = 2.7833e-01, PNorm = 160.0457, GNorm = 0.2601
Meta loss on this task batch = 3.3726e-01, Meta loss averaged over last 500 steps = 2.7840e-01, PNorm = 160.0504, GNorm = 0.3792
Meta loss on this task batch = 2.8232e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 160.0545, GNorm = 0.4752
Meta loss on this task batch = 2.7422e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 160.0586, GNorm = 0.3376
Meta loss on this task batch = 2.5774e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 160.0628, GNorm = 0.2881
Meta loss on this task batch = 2.7399e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 160.0679, GNorm = 0.4740
Took 112.76333713531494 seconds to complete one epoch of meta training
Took 120.70500636100769 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482118
Epoch 799
Meta loss on this task batch = 2.8168e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 160.0727, GNorm = 0.2720
Meta loss on this task batch = 3.0593e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 160.0774, GNorm = 0.3382
Meta loss on this task batch = 2.8259e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 160.0822, GNorm = 0.3497
Meta loss on this task batch = 2.8954e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 160.0876, GNorm = 0.3756
Meta loss on this task batch = 2.8576e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 160.0921, GNorm = 0.4285
Meta loss on this task batch = 2.6828e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 160.0961, GNorm = 0.4596
Meta loss on this task batch = 3.2141e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 160.0999, GNorm = 0.4651
Meta loss on this task batch = 2.1357e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 160.1042, GNorm = 0.2422
Meta loss on this task batch = 2.4267e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 160.1086, GNorm = 0.2687
Meta loss on this task batch = 2.8353e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 160.1128, GNorm = 0.2716
Meta loss on this task batch = 3.1418e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 160.1164, GNorm = 0.2815
Meta loss on this task batch = 2.6081e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 160.1209, GNorm = 0.3927
Meta loss on this task batch = 2.9652e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 160.1260, GNorm = 0.3526
Meta loss on this task batch = 2.5500e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 160.1312, GNorm = 0.2378
Meta loss on this task batch = 3.2471e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 160.1361, GNorm = 0.4517
Meta loss on this task batch = 2.5077e-01, Meta loss averaged over last 500 steps = 2.7877e-01, PNorm = 160.1416, GNorm = 0.3031
Meta loss on this task batch = 2.4465e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 160.1469, GNorm = 0.2162
Meta loss on this task batch = 2.7258e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 160.1529, GNorm = 0.2583
Meta loss on this task batch = 2.6799e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 160.1597, GNorm = 0.3398
Took 111.708571434021 seconds to complete one epoch of meta training
Took 120.00126099586487 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465504
Epoch 800
Meta loss on this task batch = 3.1225e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 160.1659, GNorm = 0.3272
Meta loss on this task batch = 2.7375e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 160.1719, GNorm = 0.3297
Meta loss on this task batch = 2.6110e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 160.1783, GNorm = 0.3441
Meta loss on this task batch = 3.2114e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 160.1828, GNorm = 0.3067
Meta loss on this task batch = 2.7050e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 160.1868, GNorm = 0.3170
Meta loss on this task batch = 2.8459e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 160.1909, GNorm = 0.3179
Meta loss on this task batch = 2.9580e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 160.1945, GNorm = 0.2850
Meta loss on this task batch = 2.7396e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 160.1985, GNorm = 0.3679
Meta loss on this task batch = 2.7685e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 160.2023, GNorm = 0.3389
Meta loss on this task batch = 2.8231e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 160.2058, GNorm = 0.3144
Meta loss on this task batch = 2.5587e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 160.2086, GNorm = 0.2636
Meta loss on this task batch = 2.7879e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 160.2111, GNorm = 0.3064
Meta loss on this task batch = 2.9050e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 160.2143, GNorm = 0.2456
Meta loss on this task batch = 2.5427e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 160.2177, GNorm = 0.2578
Meta loss on this task batch = 3.0489e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 160.2214, GNorm = 0.2909
Meta loss on this task batch = 2.4885e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 160.2255, GNorm = 0.2569
Meta loss on this task batch = 2.7749e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 160.2289, GNorm = 0.3158
Meta loss on this task batch = 2.6190e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 160.2330, GNorm = 0.2921
Meta loss on this task batch = 2.9476e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 160.2363, GNorm = 0.3163
Took 111.94180226325989 seconds to complete one epoch of meta training
Took 119.67697834968567 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486535
Epoch 801
Meta loss on this task batch = 2.3505e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 160.2400, GNorm = 0.2505
Meta loss on this task batch = 3.2668e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 160.2426, GNorm = 0.3378
Meta loss on this task batch = 2.9124e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 160.2454, GNorm = 0.3009
Meta loss on this task batch = 3.0352e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 160.2486, GNorm = 0.2685
Meta loss on this task batch = 2.8540e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 160.2513, GNorm = 0.2911
Meta loss on this task batch = 2.7053e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 160.2544, GNorm = 0.2782
Meta loss on this task batch = 2.7389e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 160.2583, GNorm = 0.3040
Meta loss on this task batch = 2.5977e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 160.2622, GNorm = 0.2796
Meta loss on this task batch = 2.3176e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 160.2662, GNorm = 0.2272
Meta loss on this task batch = 2.4216e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 160.2702, GNorm = 0.2498
Meta loss on this task batch = 2.8110e-01, Meta loss averaged over last 500 steps = 2.7864e-01, PNorm = 160.2741, GNorm = 0.2942
Meta loss on this task batch = 2.8518e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 160.2775, GNorm = 0.2846
Meta loss on this task batch = 2.4502e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 160.2815, GNorm = 0.2590
Meta loss on this task batch = 3.0802e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 160.2851, GNorm = 0.3491
Meta loss on this task batch = 2.7317e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 160.2884, GNorm = 0.2775
Meta loss on this task batch = 2.5094e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 160.2917, GNorm = 0.2436
Meta loss on this task batch = 2.3956e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 160.2955, GNorm = 0.2750
Meta loss on this task batch = 2.6976e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 160.2995, GNorm = 0.2789
Meta loss on this task batch = 3.3821e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 160.3030, GNorm = 0.4024
Took 113.22616696357727 seconds to complete one epoch of meta training
Took 121.2033007144928 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467554
Epoch 802
Meta loss on this task batch = 3.2552e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 160.3067, GNorm = 0.2743
Meta loss on this task batch = 2.9234e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 160.3100, GNorm = 0.2876
Meta loss on this task batch = 2.6909e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 160.3135, GNorm = 0.3907
Meta loss on this task batch = 2.6089e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 160.3168, GNorm = 0.2677
Meta loss on this task batch = 2.7175e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 160.3211, GNorm = 0.3656
Meta loss on this task batch = 2.5892e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 160.3262, GNorm = 0.2495
Meta loss on this task batch = 2.9831e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 160.3314, GNorm = 0.3232
Meta loss on this task batch = 3.3138e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 160.3361, GNorm = 0.3047
Meta loss on this task batch = 2.6278e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 160.3412, GNorm = 0.4662
Meta loss on this task batch = 2.5721e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 160.3469, GNorm = 0.4476
Meta loss on this task batch = 2.4327e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 160.3527, GNorm = 0.3301
Meta loss on this task batch = 2.8172e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 160.3586, GNorm = 0.3204
Meta loss on this task batch = 2.8070e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 160.3647, GNorm = 0.3497
Meta loss on this task batch = 3.4769e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 160.3700, GNorm = 0.6761
Meta loss on this task batch = 2.8424e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 160.3754, GNorm = 0.2710
Meta loss on this task batch = 2.1983e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 160.3800, GNorm = 0.2620
Meta loss on this task batch = 2.5202e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 160.3845, GNorm = 0.2722
Meta loss on this task batch = 2.1915e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 160.3889, GNorm = 0.2634
Meta loss on this task batch = 3.3418e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 160.3933, GNorm = 0.4065
Took 111.26036047935486 seconds to complete one epoch of meta training
Took 119.05108428001404 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463486
Epoch 803
Meta loss on this task batch = 2.9067e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 160.3975, GNorm = 0.3293
Meta loss on this task batch = 2.7403e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 160.4005, GNorm = 0.3007
Meta loss on this task batch = 2.1669e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 160.4041, GNorm = 0.2672
Meta loss on this task batch = 3.0378e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 160.4074, GNorm = 0.3319
Meta loss on this task batch = 3.0750e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 160.4098, GNorm = 0.2718
Meta loss on this task batch = 2.9362e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 160.4121, GNorm = 0.2792
Meta loss on this task batch = 2.4080e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 160.4145, GNorm = 0.3557
Meta loss on this task batch = 3.1901e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 160.4171, GNorm = 0.4005
Meta loss on this task batch = 3.0747e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 160.4192, GNorm = 0.3674
Meta loss on this task batch = 2.6365e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 160.4214, GNorm = 0.2967
Meta loss on this task batch = 2.9286e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 160.4237, GNorm = 0.3139
Meta loss on this task batch = 2.4333e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 160.4267, GNorm = 0.3337
Meta loss on this task batch = 2.8984e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 160.4297, GNorm = 0.3640
Meta loss on this task batch = 2.9072e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 160.4323, GNorm = 0.2895
Meta loss on this task batch = 2.7389e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 160.4343, GNorm = 0.2751
Meta loss on this task batch = 2.9876e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 160.4354, GNorm = 0.2714
Meta loss on this task batch = 2.6411e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 160.4368, GNorm = 0.2380
Meta loss on this task batch = 3.0219e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 160.4386, GNorm = 0.3097
Meta loss on this task batch = 2.7821e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 160.4401, GNorm = 0.4079
Took 114.21111297607422 seconds to complete one epoch of meta training
Took 122.03796815872192 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478102
Epoch 804
Meta loss on this task batch = 2.8256e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 160.4417, GNorm = 0.2815
Meta loss on this task batch = 2.8675e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 160.4437, GNorm = 0.3776
Meta loss on this task batch = 2.4708e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 160.4468, GNorm = 0.4214
Meta loss on this task batch = 2.4764e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 160.4501, GNorm = 0.3089
Meta loss on this task batch = 3.1820e-01, Meta loss averaged over last 500 steps = 2.7864e-01, PNorm = 160.4527, GNorm = 0.2749
Meta loss on this task batch = 2.6626e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 160.4555, GNorm = 0.3355
Meta loss on this task batch = 3.0864e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 160.4589, GNorm = 0.4251
Meta loss on this task batch = 2.6552e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 160.4623, GNorm = 0.2393
Meta loss on this task batch = 2.9623e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 160.4654, GNorm = 0.3376
Meta loss on this task batch = 2.5996e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 160.4688, GNorm = 0.2613
Meta loss on this task batch = 2.9130e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 160.4718, GNorm = 0.2334
Meta loss on this task batch = 2.8003e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 160.4746, GNorm = 0.3093
Meta loss on this task batch = 2.9832e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 160.4781, GNorm = 0.3492
Meta loss on this task batch = 3.1642e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 160.4806, GNorm = 0.5652
Meta loss on this task batch = 3.1118e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 160.4832, GNorm = 0.3639
Meta loss on this task batch = 2.7158e-01, Meta loss averaged over last 500 steps = 2.7907e-01, PNorm = 160.4866, GNorm = 0.2292
Meta loss on this task batch = 2.3482e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 160.4903, GNorm = 0.3102
Meta loss on this task batch = 2.8669e-01, Meta loss averaged over last 500 steps = 2.7890e-01, PNorm = 160.4946, GNorm = 0.3145
Meta loss on this task batch = 3.4343e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 160.4995, GNorm = 0.5313
Took 114.18478035926819 seconds to complete one epoch of meta training
Took 122.10798382759094 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461073
Epoch 805
Meta loss on this task batch = 3.3080e-01, Meta loss averaged over last 500 steps = 2.7902e-01, PNorm = 160.5039, GNorm = 0.3792
Meta loss on this task batch = 2.7383e-01, Meta loss averaged over last 500 steps = 2.7893e-01, PNorm = 160.5084, GNorm = 0.2411
Meta loss on this task batch = 2.9356e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 160.5126, GNorm = 0.2610
Meta loss on this task batch = 2.7127e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 160.5170, GNorm = 0.2693
Meta loss on this task batch = 2.7689e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 160.5216, GNorm = 0.2894
Meta loss on this task batch = 2.5832e-01, Meta loss averaged over last 500 steps = 2.7902e-01, PNorm = 160.5263, GNorm = 0.2628
Meta loss on this task batch = 2.9921e-01, Meta loss averaged over last 500 steps = 2.7905e-01, PNorm = 160.5310, GNorm = 0.3211
Meta loss on this task batch = 3.2773e-01, Meta loss averaged over last 500 steps = 2.7911e-01, PNorm = 160.5362, GNorm = 0.3095
Meta loss on this task batch = 2.6497e-01, Meta loss averaged over last 500 steps = 2.7902e-01, PNorm = 160.5420, GNorm = 0.3121
Meta loss on this task batch = 2.7201e-01, Meta loss averaged over last 500 steps = 2.7902e-01, PNorm = 160.5478, GNorm = 0.2710
Meta loss on this task batch = 3.0383e-01, Meta loss averaged over last 500 steps = 2.7915e-01, PNorm = 160.5533, GNorm = 0.2640
Meta loss on this task batch = 2.8189e-01, Meta loss averaged over last 500 steps = 2.7913e-01, PNorm = 160.5583, GNorm = 0.3278
Meta loss on this task batch = 2.5635e-01, Meta loss averaged over last 500 steps = 2.7905e-01, PNorm = 160.5641, GNorm = 0.3819
Meta loss on this task batch = 2.4548e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 160.5703, GNorm = 0.3537
Meta loss on this task batch = 2.8687e-01, Meta loss averaged over last 500 steps = 2.7920e-01, PNorm = 160.5764, GNorm = 0.2964
Meta loss on this task batch = 2.5790e-01, Meta loss averaged over last 500 steps = 2.7921e-01, PNorm = 160.5827, GNorm = 0.3014
Meta loss on this task batch = 2.3992e-01, Meta loss averaged over last 500 steps = 2.7899e-01, PNorm = 160.5885, GNorm = 0.2667
Meta loss on this task batch = 2.7543e-01, Meta loss averaged over last 500 steps = 2.7909e-01, PNorm = 160.5946, GNorm = 0.3101
Meta loss on this task batch = 2.7294e-01, Meta loss averaged over last 500 steps = 2.7896e-01, PNorm = 160.6003, GNorm = 0.3906
Took 114.87338328361511 seconds to complete one epoch of meta training
Took 122.57648849487305 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459156
Epoch 806
Meta loss on this task batch = 2.6617e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 160.6060, GNorm = 0.3329
Meta loss on this task batch = 2.9361e-01, Meta loss averaged over last 500 steps = 2.7901e-01, PNorm = 160.6115, GNorm = 0.4018
Meta loss on this task batch = 2.5230e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 160.6169, GNorm = 0.3127
Meta loss on this task batch = 2.5629e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 160.6219, GNorm = 0.2919
Meta loss on this task batch = 3.2375e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 160.6258, GNorm = 0.3218
Meta loss on this task batch = 3.1756e-01, Meta loss averaged over last 500 steps = 2.7896e-01, PNorm = 160.6300, GNorm = 0.3654
Meta loss on this task batch = 2.4673e-01, Meta loss averaged over last 500 steps = 2.7887e-01, PNorm = 160.6334, GNorm = 0.2610
Meta loss on this task batch = 3.1224e-01, Meta loss averaged over last 500 steps = 2.7893e-01, PNorm = 160.6368, GNorm = 0.2917
Meta loss on this task batch = 3.1542e-01, Meta loss averaged over last 500 steps = 2.7904e-01, PNorm = 160.6401, GNorm = 0.2632
Meta loss on this task batch = 2.6868e-01, Meta loss averaged over last 500 steps = 2.7902e-01, PNorm = 160.6438, GNorm = 0.2623
Meta loss on this task batch = 3.1160e-01, Meta loss averaged over last 500 steps = 2.7896e-01, PNorm = 160.6475, GNorm = 0.3958
Meta loss on this task batch = 2.5139e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 160.6515, GNorm = 0.2924
Meta loss on this task batch = 2.8841e-01, Meta loss averaged over last 500 steps = 2.7887e-01, PNorm = 160.6556, GNorm = 0.3077
Meta loss on this task batch = 2.7870e-01, Meta loss averaged over last 500 steps = 2.7890e-01, PNorm = 160.6591, GNorm = 0.3902
Meta loss on this task batch = 2.7117e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 160.6628, GNorm = 0.3224
Meta loss on this task batch = 2.4820e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 160.6662, GNorm = 0.2612
Meta loss on this task batch = 2.6371e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 160.6696, GNorm = 0.2678
Meta loss on this task batch = 2.6800e-01, Meta loss averaged over last 500 steps = 2.7881e-01, PNorm = 160.6729, GNorm = 0.2475
Meta loss on this task batch = 2.5640e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 160.6760, GNorm = 0.2695
Took 164.91591024398804 seconds to complete one epoch of meta training
Took 172.8014566898346 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472663
Epoch 807
Meta loss on this task batch = 2.4730e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 160.6791, GNorm = 0.2463
Meta loss on this task batch = 2.5907e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 160.6825, GNorm = 0.2334
Meta loss on this task batch = 2.3036e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 160.6858, GNorm = 0.2148
Meta loss on this task batch = 2.7835e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 160.6898, GNorm = 0.4131
Meta loss on this task batch = 3.3600e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 160.6933, GNorm = 0.2923
Meta loss on this task batch = 3.0248e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 160.6966, GNorm = 0.2948
Meta loss on this task batch = 2.8614e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 160.7003, GNorm = 0.2822
Meta loss on this task batch = 2.1441e-01, Meta loss averaged over last 500 steps = 2.7864e-01, PNorm = 160.7040, GNorm = 0.3473
Meta loss on this task batch = 2.5230e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 160.7071, GNorm = 0.4356
Meta loss on this task batch = 2.6634e-01, Meta loss averaged over last 500 steps = 2.7840e-01, PNorm = 160.7102, GNorm = 0.2691
Meta loss on this task batch = 2.8590e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 160.7137, GNorm = 0.2727
Meta loss on this task batch = 3.2977e-01, Meta loss averaged over last 500 steps = 2.7877e-01, PNorm = 160.7170, GNorm = 0.4047
Meta loss on this task batch = 2.6874e-01, Meta loss averaged over last 500 steps = 2.7877e-01, PNorm = 160.7203, GNorm = 0.3318
Meta loss on this task batch = 3.0192e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 160.7234, GNorm = 0.2662
Meta loss on this task batch = 3.0182e-01, Meta loss averaged over last 500 steps = 2.7915e-01, PNorm = 160.7273, GNorm = 0.3305
Meta loss on this task batch = 2.7632e-01, Meta loss averaged over last 500 steps = 2.7915e-01, PNorm = 160.7313, GNorm = 0.3385
Meta loss on this task batch = 2.3061e-01, Meta loss averaged over last 500 steps = 2.7904e-01, PNorm = 160.7353, GNorm = 0.2723
Meta loss on this task batch = 2.8770e-01, Meta loss averaged over last 500 steps = 2.7909e-01, PNorm = 160.7395, GNorm = 0.2653
Meta loss on this task batch = 3.3102e-01, Meta loss averaged over last 500 steps = 2.7913e-01, PNorm = 160.7431, GNorm = 0.4422
Took 178.28837990760803 seconds to complete one epoch of meta training
Took 186.6287624835968 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461936
Epoch 808
Meta loss on this task batch = 3.1015e-01, Meta loss averaged over last 500 steps = 2.7913e-01, PNorm = 160.7461, GNorm = 0.4536
Meta loss on this task batch = 2.7253e-01, Meta loss averaged over last 500 steps = 2.7914e-01, PNorm = 160.7494, GNorm = 0.2582
Meta loss on this task batch = 2.4572e-01, Meta loss averaged over last 500 steps = 2.7908e-01, PNorm = 160.7535, GNorm = 0.3417
Meta loss on this task batch = 2.5368e-01, Meta loss averaged over last 500 steps = 2.7901e-01, PNorm = 160.7579, GNorm = 0.2524
Meta loss on this task batch = 2.8388e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 160.7622, GNorm = 0.2693
Meta loss on this task batch = 2.9869e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 160.7666, GNorm = 0.4515
Meta loss on this task batch = 2.6044e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 160.7713, GNorm = 0.2748
Meta loss on this task batch = 2.5996e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 160.7761, GNorm = 0.4452
Meta loss on this task batch = 2.9630e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 160.7807, GNorm = 0.3481
Meta loss on this task batch = 2.4516e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 160.7857, GNorm = 0.2786
Meta loss on this task batch = 2.6226e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 160.7911, GNorm = 0.2583
Meta loss on this task batch = 2.6155e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 160.7964, GNorm = 0.3027
Meta loss on this task batch = 2.8428e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 160.8016, GNorm = 0.2849
Meta loss on this task batch = 2.7381e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 160.8068, GNorm = 0.3226
Meta loss on this task batch = 2.7666e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 160.8118, GNorm = 0.3024
Meta loss on this task batch = 3.2358e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 160.8160, GNorm = 0.2802
Meta loss on this task batch = 2.6142e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 160.8196, GNorm = 0.3227
Meta loss on this task batch = 2.8785e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 160.8240, GNorm = 0.4328
Meta loss on this task batch = 2.6655e-01, Meta loss averaged over last 500 steps = 2.7875e-01, PNorm = 160.8283, GNorm = 0.3317
Took 132.72017979621887 seconds to complete one epoch of meta training
Took 140.7008183002472 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481883
Epoch 809
Meta loss on this task batch = 2.6129e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 160.8325, GNorm = 0.2785
Meta loss on this task batch = 2.9725e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 160.8370, GNorm = 0.2672
Meta loss on this task batch = 3.0455e-01, Meta loss averaged over last 500 steps = 2.7893e-01, PNorm = 160.8410, GNorm = 0.3163
Meta loss on this task batch = 2.7801e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 160.8453, GNorm = 0.3239
Meta loss on this task batch = 2.3568e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 160.8487, GNorm = 0.2899
Meta loss on this task batch = 2.3164e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 160.8525, GNorm = 0.2893
Meta loss on this task batch = 2.7568e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 160.8559, GNorm = 0.2585
Meta loss on this task batch = 3.0191e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 160.8594, GNorm = 0.3324
Meta loss on this task batch = 2.8983e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 160.8629, GNorm = 0.4226
Meta loss on this task batch = 2.3962e-01, Meta loss averaged over last 500 steps = 2.7893e-01, PNorm = 160.8670, GNorm = 0.2763
Meta loss on this task batch = 2.4036e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 160.8714, GNorm = 0.2460
Meta loss on this task batch = 2.2086e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 160.8752, GNorm = 0.3636
Meta loss on this task batch = 2.9849e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 160.8785, GNorm = 0.3114
Meta loss on this task batch = 2.7387e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 160.8815, GNorm = 0.2848
Meta loss on this task batch = 3.0405e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 160.8847, GNorm = 0.2895
Meta loss on this task batch = 2.9058e-01, Meta loss averaged over last 500 steps = 2.7864e-01, PNorm = 160.8879, GNorm = 0.3279
Meta loss on this task batch = 2.6528e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 160.8916, GNorm = 0.2575
Meta loss on this task batch = 3.1690e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 160.8951, GNorm = 0.2832
Meta loss on this task batch = 2.3039e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 160.8992, GNorm = 0.3593
Took 136.32826209068298 seconds to complete one epoch of meta training
Took 143.66282296180725 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468435
Epoch 810
Meta loss on this task batch = 3.0549e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 160.9032, GNorm = 0.5237
Meta loss on this task batch = 3.2724e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 160.9066, GNorm = 0.4300
Meta loss on this task batch = 2.5349e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 160.9097, GNorm = 0.4052
Meta loss on this task batch = 3.1157e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 160.9119, GNorm = 0.2940
Meta loss on this task batch = 2.5305e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 160.9140, GNorm = 0.2599
Meta loss on this task batch = 2.3657e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 160.9165, GNorm = 0.2661
Meta loss on this task batch = 2.5730e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 160.9187, GNorm = 0.2870
Meta loss on this task batch = 2.6206e-01, Meta loss averaged over last 500 steps = 2.7839e-01, PNorm = 160.9212, GNorm = 0.3521
Meta loss on this task batch = 2.8994e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 160.9230, GNorm = 0.3325
Meta loss on this task batch = 2.7879e-01, Meta loss averaged over last 500 steps = 2.7833e-01, PNorm = 160.9249, GNorm = 0.2739
Meta loss on this task batch = 2.9822e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 160.9269, GNorm = 0.5389
Meta loss on this task batch = 2.9876e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 160.9291, GNorm = 0.3057
Meta loss on this task batch = 2.4098e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 160.9323, GNorm = 0.3788
Meta loss on this task batch = 2.7228e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 160.9360, GNorm = 0.2936
Meta loss on this task batch = 2.4219e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 160.9401, GNorm = 0.3102
Meta loss on this task batch = 3.2140e-01, Meta loss averaged over last 500 steps = 2.7850e-01, PNorm = 160.9443, GNorm = 0.3029
Meta loss on this task batch = 3.1897e-01, Meta loss averaged over last 500 steps = 2.7850e-01, PNorm = 160.9488, GNorm = 0.3053
Meta loss on this task batch = 3.2595e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 160.9531, GNorm = 0.2374
Meta loss on this task batch = 2.7967e-01, Meta loss averaged over last 500 steps = 2.7850e-01, PNorm = 160.9585, GNorm = 0.5079
Took 165.8605456352234 seconds to complete one epoch of meta training
Took 174.44530487060547 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481667
Epoch 811
Meta loss on this task batch = 3.1682e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 160.9630, GNorm = 0.2984
Meta loss on this task batch = 2.4256e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 160.9678, GNorm = 0.2362
Meta loss on this task batch = 2.4716e-01, Meta loss averaged over last 500 steps = 2.7833e-01, PNorm = 160.9727, GNorm = 0.2824
Meta loss on this task batch = 3.2697e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 160.9779, GNorm = 0.3253
Meta loss on this task batch = 2.9588e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 160.9835, GNorm = 0.3825
Meta loss on this task batch = 2.8590e-01, Meta loss averaged over last 500 steps = 2.7840e-01, PNorm = 160.9891, GNorm = 0.3111
Meta loss on this task batch = 2.5788e-01, Meta loss averaged over last 500 steps = 2.7840e-01, PNorm = 160.9946, GNorm = 0.2690
Meta loss on this task batch = 2.2892e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 161.0004, GNorm = 0.2872
Meta loss on this task batch = 2.7983e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 161.0061, GNorm = 0.2922
Meta loss on this task batch = 2.7267e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 161.0124, GNorm = 0.2217
Meta loss on this task batch = 2.9920e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 161.0192, GNorm = 0.2954
Meta loss on this task batch = 2.8458e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 161.0254, GNorm = 0.3486
Meta loss on this task batch = 2.9860e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 161.0298, GNorm = 0.4476
Meta loss on this task batch = 3.0623e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 161.0339, GNorm = 0.3178
Meta loss on this task batch = 2.3200e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 161.0377, GNorm = 0.3075
Meta loss on this task batch = 3.3261e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 161.0409, GNorm = 0.2955
Meta loss on this task batch = 2.2153e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 161.0443, GNorm = 0.2664
Meta loss on this task batch = 2.5655e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 161.0474, GNorm = 0.3524
Meta loss on this task batch = 2.6617e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 161.0506, GNorm = 0.3291
Took 142.38004517555237 seconds to complete one epoch of meta training
Took 150.3374102115631 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451585
Epoch 812
Meta loss on this task batch = 2.5464e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 161.0533, GNorm = 0.3093
Meta loss on this task batch = 2.8396e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 161.0563, GNorm = 0.3116
Meta loss on this task batch = 2.8991e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 161.0588, GNorm = 0.2896
Meta loss on this task batch = 2.8775e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 161.0620, GNorm = 0.3161
Meta loss on this task batch = 3.0553e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 161.0654, GNorm = 0.4256
Meta loss on this task batch = 2.7216e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 161.0690, GNorm = 0.2859
Meta loss on this task batch = 2.3175e-01, Meta loss averaged over last 500 steps = 2.7840e-01, PNorm = 161.0721, GNorm = 0.2707
Meta loss on this task batch = 2.7914e-01, Meta loss averaged over last 500 steps = 2.7839e-01, PNorm = 161.0758, GNorm = 0.3072
Meta loss on this task batch = 2.1752e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 161.0797, GNorm = 0.3849
Meta loss on this task batch = 2.6203e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 161.0845, GNorm = 0.3019
Meta loss on this task batch = 2.6902e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 161.0893, GNorm = 0.2576
Meta loss on this task batch = 3.0934e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 161.0936, GNorm = 0.3880
Meta loss on this task batch = 3.0700e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 161.0977, GNorm = 0.3635
Meta loss on this task batch = 3.1042e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 161.1014, GNorm = 0.2945
Meta loss on this task batch = 2.8067e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 161.1057, GNorm = 0.3010
Meta loss on this task batch = 2.5609e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 161.1092, GNorm = 0.2451
Meta loss on this task batch = 2.6413e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 161.1124, GNorm = 0.2771
Meta loss on this task batch = 3.0119e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 161.1156, GNorm = 0.3076
Meta loss on this task batch = 2.7121e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 161.1197, GNorm = 0.4594
Took 159.30508041381836 seconds to complete one epoch of meta training
Took 167.12005400657654 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486375
Epoch 813
Meta loss on this task batch = 2.8419e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 161.1242, GNorm = 0.3021
Meta loss on this task batch = 2.7751e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 161.1290, GNorm = 0.3015
Meta loss on this task batch = 2.8452e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 161.1341, GNorm = 0.2668
Meta loss on this task batch = 2.7138e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 161.1388, GNorm = 0.2997
Meta loss on this task batch = 2.2240e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 161.1436, GNorm = 0.3096
Meta loss on this task batch = 2.8183e-01, Meta loss averaged over last 500 steps = 2.7833e-01, PNorm = 161.1490, GNorm = 0.2787
Meta loss on this task batch = 2.8808e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 161.1538, GNorm = 0.5501
Meta loss on this task batch = 2.6783e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 161.1589, GNorm = 0.2786
Meta loss on this task batch = 2.9589e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 161.1638, GNorm = 0.3224
Meta loss on this task batch = 2.6767e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 161.1687, GNorm = 0.2972
Meta loss on this task batch = 2.8539e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 161.1736, GNorm = 0.3004
Meta loss on this task batch = 3.0698e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 161.1784, GNorm = 0.2732
Meta loss on this task batch = 2.5360e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 161.1838, GNorm = 0.2604
Meta loss on this task batch = 2.7118e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 161.1891, GNorm = 0.3063
Meta loss on this task batch = 2.7572e-01, Meta loss averaged over last 500 steps = 2.7819e-01, PNorm = 161.1945, GNorm = 0.3244
Meta loss on this task batch = 3.3850e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 161.1997, GNorm = 0.4105
Meta loss on this task batch = 2.9128e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 161.2047, GNorm = 0.4888
Meta loss on this task batch = 2.4957e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 161.2095, GNorm = 0.3377
Meta loss on this task batch = 3.2433e-01, Meta loss averaged over last 500 steps = 2.7850e-01, PNorm = 161.2135, GNorm = 0.6249
Took 129.1747419834137 seconds to complete one epoch of meta training
Took 137.09381103515625 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463574
Epoch 814
Meta loss on this task batch = 2.5290e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 161.2181, GNorm = 0.2837
Meta loss on this task batch = 3.2993e-01, Meta loss averaged over last 500 steps = 2.7850e-01, PNorm = 161.2223, GNorm = 0.2922
Meta loss on this task batch = 3.0000e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 161.2266, GNorm = 0.3648
Meta loss on this task batch = 2.2212e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 161.2310, GNorm = 0.2449
Meta loss on this task batch = 2.8872e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 161.2353, GNorm = 0.2542
Meta loss on this task batch = 2.5524e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 161.2392, GNorm = 0.2980
Meta loss on this task batch = 2.8473e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 161.2430, GNorm = 0.2704
Meta loss on this task batch = 2.5130e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 161.2471, GNorm = 0.3304
Meta loss on this task batch = 2.5267e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 161.2519, GNorm = 0.2523
Meta loss on this task batch = 2.6566e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 161.2561, GNorm = 0.2469
Meta loss on this task batch = 2.2876e-01, Meta loss averaged over last 500 steps = 2.7790e-01, PNorm = 161.2605, GNorm = 0.2862
Meta loss on this task batch = 2.7731e-01, Meta loss averaged over last 500 steps = 2.7796e-01, PNorm = 161.2655, GNorm = 0.3974
Meta loss on this task batch = 2.3412e-01, Meta loss averaged over last 500 steps = 2.7788e-01, PNorm = 161.2707, GNorm = 0.3031
Meta loss on this task batch = 2.9671e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 161.2751, GNorm = 0.3040
Meta loss on this task batch = 3.4712e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 161.2788, GNorm = 0.4856
Meta loss on this task batch = 2.9770e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 161.2821, GNorm = 0.4046
Meta loss on this task batch = 2.1353e-01, Meta loss averaged over last 500 steps = 2.7794e-01, PNorm = 161.2856, GNorm = 0.2409
Meta loss on this task batch = 2.9314e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 161.2890, GNorm = 0.2760
Meta loss on this task batch = 2.7337e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 161.2920, GNorm = 0.3121
Took 155.20886707305908 seconds to complete one epoch of meta training
Took 163.31710243225098 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484122
Epoch 815
Meta loss on this task batch = 2.6179e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 161.2958, GNorm = 0.3113
Meta loss on this task batch = 2.6189e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 161.3004, GNorm = 0.2445
Meta loss on this task batch = 2.8692e-01, Meta loss averaged over last 500 steps = 2.7808e-01, PNorm = 161.3051, GNorm = 0.2747
Meta loss on this task batch = 3.0804e-01, Meta loss averaged over last 500 steps = 2.7816e-01, PNorm = 161.3094, GNorm = 0.3097
Meta loss on this task batch = 3.1037e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 161.3135, GNorm = 0.2849
Meta loss on this task batch = 2.9703e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 161.3172, GNorm = 0.3462
Meta loss on this task batch = 3.0624e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 161.3202, GNorm = 0.2709
Meta loss on this task batch = 2.6617e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 161.3239, GNorm = 0.2599
Meta loss on this task batch = 2.5725e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 161.3279, GNorm = 0.2298
Meta loss on this task batch = 2.7011e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 161.3319, GNorm = 0.3481
Meta loss on this task batch = 3.0803e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 161.3357, GNorm = 0.2322
Meta loss on this task batch = 2.6646e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 161.3398, GNorm = 0.2951
Meta loss on this task batch = 3.1156e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 161.3439, GNorm = 0.2833
Meta loss on this task batch = 2.3667e-01, Meta loss averaged over last 500 steps = 2.7833e-01, PNorm = 161.3488, GNorm = 0.2486
Meta loss on this task batch = 2.8061e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 161.3536, GNorm = 0.2925
Meta loss on this task batch = 2.6731e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 161.3586, GNorm = 0.2880
Meta loss on this task batch = 2.5367e-01, Meta loss averaged over last 500 steps = 2.7821e-01, PNorm = 161.3639, GNorm = 0.2471
Meta loss on this task batch = 2.0912e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 161.3695, GNorm = 0.2272
Meta loss on this task batch = 2.6358e-01, Meta loss averaged over last 500 steps = 2.7812e-01, PNorm = 161.3751, GNorm = 0.4036
Took 124.20171332359314 seconds to complete one epoch of meta training
Took 131.24255514144897 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482209
Epoch 816
Meta loss on this task batch = 2.6955e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 161.3812, GNorm = 0.4287
Meta loss on this task batch = 3.2084e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 161.3868, GNorm = 0.3767
Meta loss on this task batch = 3.0329e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 161.3917, GNorm = 0.2448
Meta loss on this task batch = 2.5640e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 161.3961, GNorm = 0.2965
Meta loss on this task batch = 2.7990e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 161.4006, GNorm = 0.2588
Meta loss on this task batch = 2.3325e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 161.4043, GNorm = 0.2848
Meta loss on this task batch = 2.8047e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 161.4085, GNorm = 0.3322
Meta loss on this task batch = 2.8129e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 161.4125, GNorm = 0.3326
Meta loss on this task batch = 2.8592e-01, Meta loss averaged over last 500 steps = 2.7821e-01, PNorm = 161.4166, GNorm = 0.3347
Meta loss on this task batch = 2.9521e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 161.4203, GNorm = 0.2574
Meta loss on this task batch = 2.7447e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 161.4237, GNorm = 0.3755
Meta loss on this task batch = 2.9025e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 161.4264, GNorm = 0.3947
Meta loss on this task batch = 2.3115e-01, Meta loss averaged over last 500 steps = 2.7798e-01, PNorm = 161.4293, GNorm = 0.2150
Meta loss on this task batch = 2.6935e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 161.4321, GNorm = 0.4471
Meta loss on this task batch = 3.1960e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 161.4354, GNorm = 0.3395
Meta loss on this task batch = 2.3366e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 161.4387, GNorm = 0.2404
Meta loss on this task batch = 2.7422e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 161.4425, GNorm = 0.3148
Meta loss on this task batch = 2.9527e-01, Meta loss averaged over last 500 steps = 2.7801e-01, PNorm = 161.4459, GNorm = 0.3074
Meta loss on this task batch = 2.5396e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 161.4502, GNorm = 0.3433
Took 118.23126578330994 seconds to complete one epoch of meta training
Took 126.11899256706238 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460574
Epoch 817
Meta loss on this task batch = 2.7121e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 161.4546, GNorm = 0.3301
Meta loss on this task batch = 2.7329e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 161.4593, GNorm = 0.3344
Meta loss on this task batch = 3.0157e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 161.4637, GNorm = 0.3384
Meta loss on this task batch = 2.9444e-01, Meta loss averaged over last 500 steps = 2.7821e-01, PNorm = 161.4680, GNorm = 0.4617
Meta loss on this task batch = 2.9715e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 161.4723, GNorm = 0.4205
Meta loss on this task batch = 3.0804e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 161.4765, GNorm = 0.3606
Meta loss on this task batch = 1.8567e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 161.4810, GNorm = 0.2982
Meta loss on this task batch = 3.2608e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 161.4854, GNorm = 0.3269
Meta loss on this task batch = 2.4921e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 161.4899, GNorm = 0.3024
Meta loss on this task batch = 3.3282e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 161.4936, GNorm = 0.3357
Meta loss on this task batch = 2.6990e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 161.4969, GNorm = 0.2796
Meta loss on this task batch = 2.5008e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 161.5002, GNorm = 0.2764
Meta loss on this task batch = 2.8788e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 161.5030, GNorm = 0.3123
Meta loss on this task batch = 3.2098e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 161.5060, GNorm = 0.4316
Meta loss on this task batch = 2.8575e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 161.5092, GNorm = 0.3198
Meta loss on this task batch = 2.4146e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 161.5128, GNorm = 0.4454
Meta loss on this task batch = 2.8100e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 161.5162, GNorm = 0.3379
Meta loss on this task batch = 3.0828e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 161.5199, GNorm = 0.3154
Meta loss on this task batch = 2.7685e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 161.5242, GNorm = 0.4128
Took 116.10289120674133 seconds to complete one epoch of meta training
Took 123.12231278419495 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444480
Epoch 818
Meta loss on this task batch = 3.0417e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 161.5287, GNorm = 0.3186
Meta loss on this task batch = 3.1544e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 161.5333, GNorm = 0.2980
Meta loss on this task batch = 2.6403e-01, Meta loss averaged over last 500 steps = 2.7839e-01, PNorm = 161.5384, GNorm = 0.2862
Meta loss on this task batch = 2.6263e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 161.5437, GNorm = 0.3900
Meta loss on this task batch = 2.3234e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 161.5495, GNorm = 0.2289
Meta loss on this task batch = 2.7291e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 161.5551, GNorm = 0.2959
Meta loss on this task batch = 3.2702e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 161.5605, GNorm = 0.3631
Meta loss on this task batch = 2.2818e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 161.5657, GNorm = 0.2345
Meta loss on this task batch = 2.4031e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 161.5709, GNorm = 0.2229
Meta loss on this task batch = 2.5086e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 161.5762, GNorm = 0.2506
Meta loss on this task batch = 3.3270e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 161.5804, GNorm = 0.3093
Meta loss on this task batch = 2.5806e-01, Meta loss averaged over last 500 steps = 2.7794e-01, PNorm = 161.5842, GNorm = 0.2710
Meta loss on this task batch = 3.1222e-01, Meta loss averaged over last 500 steps = 2.7796e-01, PNorm = 161.5881, GNorm = 0.3683
Meta loss on this task batch = 2.9797e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 161.5913, GNorm = 0.2603
Meta loss on this task batch = 3.1626e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 161.5948, GNorm = 0.2910
Meta loss on this task batch = 3.0623e-01, Meta loss averaged over last 500 steps = 2.7816e-01, PNorm = 161.5981, GNorm = 0.3050
Meta loss on this task batch = 2.7257e-01, Meta loss averaged over last 500 steps = 2.7817e-01, PNorm = 161.6019, GNorm = 0.2778
Meta loss on this task batch = 3.7323e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 161.6046, GNorm = 0.4523
Meta loss on this task batch = 2.9520e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 161.6076, GNorm = 0.3488
Took 118.74377417564392 seconds to complete one epoch of meta training
Took 126.68128848075867 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453720
Epoch 819
Meta loss on this task batch = 3.1588e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 161.6106, GNorm = 0.2705
Meta loss on this task batch = 3.3094e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 161.6132, GNorm = 0.3393
Meta loss on this task batch = 2.3867e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 161.6162, GNorm = 0.3239
Meta loss on this task batch = 2.8031e-01, Meta loss averaged over last 500 steps = 2.7864e-01, PNorm = 161.6192, GNorm = 0.2533
Meta loss on this task batch = 2.0327e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 161.6233, GNorm = 0.3715
Meta loss on this task batch = 2.4226e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 161.6274, GNorm = 0.2276
Meta loss on this task batch = 2.2368e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 161.6319, GNorm = 0.2818
Meta loss on this task batch = 2.3928e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 161.6371, GNorm = 0.4053
Meta loss on this task batch = 3.0105e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 161.6416, GNorm = 0.3370
Meta loss on this task batch = 2.8068e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 161.6456, GNorm = 0.3455
Meta loss on this task batch = 3.1459e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 161.6486, GNorm = 0.4329
Meta loss on this task batch = 2.7204e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 161.6518, GNorm = 0.4276
Meta loss on this task batch = 3.5254e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 161.6558, GNorm = 0.3116
Meta loss on this task batch = 2.9124e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 161.6590, GNorm = 0.4380
Meta loss on this task batch = 2.6893e-01, Meta loss averaged over last 500 steps = 2.7874e-01, PNorm = 161.6616, GNorm = 0.2785
Meta loss on this task batch = 2.8213e-01, Meta loss averaged over last 500 steps = 2.7864e-01, PNorm = 161.6644, GNorm = 0.4523
Meta loss on this task batch = 3.0814e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 161.6673, GNorm = 0.4892
Meta loss on this task batch = 2.4406e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 161.6705, GNorm = 0.3656
Meta loss on this task batch = 2.6551e-01, Meta loss averaged over last 500 steps = 2.7864e-01, PNorm = 161.6739, GNorm = 0.3436
Took 112.86440348625183 seconds to complete one epoch of meta training
Took 120.90837979316711 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479759
Epoch 820
Meta loss on this task batch = 2.5890e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 161.6781, GNorm = 0.3860
Meta loss on this task batch = 2.3196e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 161.6830, GNorm = 0.2311
Meta loss on this task batch = 3.3505e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 161.6875, GNorm = 0.2943
Meta loss on this task batch = 2.7429e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 161.6917, GNorm = 0.4316
Meta loss on this task batch = 2.5587e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 161.6963, GNorm = 0.2935
Meta loss on this task batch = 2.8898e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 161.7016, GNorm = 0.3812
Meta loss on this task batch = 3.0313e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 161.7065, GNorm = 0.4020
Meta loss on this task batch = 2.6680e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 161.7119, GNorm = 0.3564
Meta loss on this task batch = 3.3407e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 161.7172, GNorm = 0.3702
Meta loss on this task batch = 2.5515e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 161.7227, GNorm = 0.2686
Meta loss on this task batch = 2.5915e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 161.7270, GNorm = 0.3997
Meta loss on this task batch = 2.8764e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 161.7318, GNorm = 0.3994
Meta loss on this task batch = 2.8292e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 161.7366, GNorm = 0.2753
Meta loss on this task batch = 3.0597e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 161.7422, GNorm = 0.3131
Meta loss on this task batch = 3.2560e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 161.7472, GNorm = 0.2741
Meta loss on this task batch = 2.8103e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 161.7528, GNorm = 0.4856
Meta loss on this task batch = 3.1250e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 161.7586, GNorm = 0.2588
Meta loss on this task batch = 2.6260e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 161.7647, GNorm = 0.2997
Meta loss on this task batch = 2.5640e-01, Meta loss averaged over last 500 steps = 2.7850e-01, PNorm = 161.7705, GNorm = 0.2879
Took 111.63717079162598 seconds to complete one epoch of meta training
Took 119.69819664955139 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473480
Epoch 821
Meta loss on this task batch = 2.6772e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 161.7763, GNorm = 0.3537
Meta loss on this task batch = 2.7399e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 161.7823, GNorm = 0.2851
Meta loss on this task batch = 3.0989e-01, Meta loss averaged over last 500 steps = 2.7833e-01, PNorm = 161.7878, GNorm = 0.2942
Meta loss on this task batch = 2.8422e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 161.7938, GNorm = 0.2839
Meta loss on this task batch = 2.8664e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 161.7996, GNorm = 0.2778
Meta loss on this task batch = 2.4894e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 161.8057, GNorm = 0.3482
Meta loss on this task batch = 2.5353e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 161.8111, GNorm = 0.2892
Meta loss on this task batch = 2.4864e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 161.8165, GNorm = 0.2780
Meta loss on this task batch = 3.0459e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 161.8212, GNorm = 0.2801
Meta loss on this task batch = 3.4763e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 161.8253, GNorm = 0.4199
Meta loss on this task batch = 3.1818e-01, Meta loss averaged over last 500 steps = 2.7864e-01, PNorm = 161.8288, GNorm = 0.3072
Meta loss on this task batch = 3.1438e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 161.8322, GNorm = 0.2866
Meta loss on this task batch = 2.8579e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 161.8361, GNorm = 0.2833
Meta loss on this task batch = 2.7400e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 161.8405, GNorm = 0.3061
Meta loss on this task batch = 2.6890e-01, Meta loss averaged over last 500 steps = 2.7874e-01, PNorm = 161.8444, GNorm = 0.2473
Meta loss on this task batch = 2.5066e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 161.8484, GNorm = 0.2834
Meta loss on this task batch = 2.5327e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 161.8529, GNorm = 0.2450
Meta loss on this task batch = 2.5389e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 161.8572, GNorm = 0.2867
Meta loss on this task batch = 3.2175e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 161.8605, GNorm = 0.5082
Took 112.60094928741455 seconds to complete one epoch of meta training
Took 120.91718101501465 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485961
Epoch 822
Meta loss on this task batch = 3.0159e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 161.8641, GNorm = 0.3218
Meta loss on this task batch = 2.7613e-01, Meta loss averaged over last 500 steps = 2.7881e-01, PNorm = 161.8672, GNorm = 0.3240
Meta loss on this task batch = 3.2672e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 161.8701, GNorm = 0.3310
Meta loss on this task batch = 3.0155e-01, Meta loss averaged over last 500 steps = 2.7890e-01, PNorm = 161.8728, GNorm = 0.3467
Meta loss on this task batch = 2.6245e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 161.8760, GNorm = 0.2179
Meta loss on this task batch = 2.9971e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 161.8789, GNorm = 0.4530
Meta loss on this task batch = 3.2804e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 161.8813, GNorm = 0.3628
Meta loss on this task batch = 2.6629e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 161.8836, GNorm = 0.2380
Meta loss on this task batch = 2.6826e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 161.8860, GNorm = 0.3557
Meta loss on this task batch = 3.1022e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 161.8890, GNorm = 0.3578
Meta loss on this task batch = 2.3546e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 161.8923, GNorm = 0.2494
Meta loss on this task batch = 2.4590e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 161.8958, GNorm = 0.2605
Meta loss on this task batch = 2.8657e-01, Meta loss averaged over last 500 steps = 2.7881e-01, PNorm = 161.8988, GNorm = 0.4330
Meta loss on this task batch = 2.9299e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 161.9021, GNorm = 0.3581
Meta loss on this task batch = 2.9537e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 161.9054, GNorm = 0.2705
Meta loss on this task batch = 2.4326e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 161.9093, GNorm = 0.3185
Meta loss on this task batch = 2.8221e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 161.9131, GNorm = 0.2910
Meta loss on this task batch = 2.2577e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 161.9170, GNorm = 0.2801
Meta loss on this task batch = 3.0061e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 161.9206, GNorm = 0.4866
Took 123.24691987037659 seconds to complete one epoch of meta training
Took 130.4453432559967 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482291
Epoch 823
Meta loss on this task batch = 2.9988e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 161.9239, GNorm = 0.3343
Meta loss on this task batch = 2.1584e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 161.9271, GNorm = 0.2714
Meta loss on this task batch = 2.1887e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 161.9308, GNorm = 0.2333
Meta loss on this task batch = 3.2768e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 161.9339, GNorm = 0.2505
Meta loss on this task batch = 3.2193e-01, Meta loss averaged over last 500 steps = 2.7899e-01, PNorm = 161.9366, GNorm = 0.3213
Meta loss on this task batch = 2.7155e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 161.9399, GNorm = 0.3607
Meta loss on this task batch = 2.3328e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 161.9432, GNorm = 0.2512
Meta loss on this task batch = 2.7000e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 161.9466, GNorm = 0.4653
Meta loss on this task batch = 2.7837e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 161.9495, GNorm = 0.3458
Meta loss on this task batch = 2.3974e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 161.9528, GNorm = 0.2839
Meta loss on this task batch = 2.6474e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 161.9562, GNorm = 0.2458
Meta loss on this task batch = 3.7056e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 161.9592, GNorm = 0.3754
Meta loss on this task batch = 2.3919e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 161.9623, GNorm = 0.3132
Meta loss on this task batch = 2.5830e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 161.9657, GNorm = 0.2962
Meta loss on this task batch = 3.1670e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 161.9687, GNorm = 0.3816
Meta loss on this task batch = 3.0341e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 161.9720, GNorm = 0.5027
Meta loss on this task batch = 2.9111e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 161.9757, GNorm = 0.4432
Meta loss on this task batch = 2.7669e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 161.9796, GNorm = 0.2593
Meta loss on this task batch = 3.0257e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 161.9825, GNorm = 0.5262
Took 118.66364669799805 seconds to complete one epoch of meta training
Took 127.16553616523743 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469369
Epoch 824
Meta loss on this task batch = 2.8458e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 161.9858, GNorm = 0.2732
Meta loss on this task batch = 2.7623e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 161.9898, GNorm = 0.3307
Meta loss on this task batch = 2.4455e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 161.9940, GNorm = 0.2230
Meta loss on this task batch = 2.8487e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 161.9978, GNorm = 0.2586
Meta loss on this task batch = 2.7749e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 162.0015, GNorm = 0.2833
Meta loss on this task batch = 2.9387e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 162.0055, GNorm = 0.2788
Meta loss on this task batch = 2.7126e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 162.0101, GNorm = 0.4960
Meta loss on this task batch = 2.8298e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 162.0148, GNorm = 0.2233
Meta loss on this task batch = 2.8775e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 162.0192, GNorm = 0.2898
Meta loss on this task batch = 3.0160e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 162.0237, GNorm = 0.2894
Meta loss on this task batch = 2.9175e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 162.0285, GNorm = 0.2886
Meta loss on this task batch = 2.9747e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 162.0334, GNorm = 0.3456
Meta loss on this task batch = 3.0363e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 162.0380, GNorm = 0.4981
Meta loss on this task batch = 3.2611e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 162.0423, GNorm = 0.4349
Meta loss on this task batch = 2.0807e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 162.0475, GNorm = 0.2952
Meta loss on this task batch = 2.8839e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 162.0533, GNorm = 0.4158
Meta loss on this task batch = 2.8061e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 162.0585, GNorm = 0.2605
Meta loss on this task batch = 2.5689e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 162.0645, GNorm = 0.2516
Meta loss on this task batch = 2.5840e-01, Meta loss averaged over last 500 steps = 2.7877e-01, PNorm = 162.0710, GNorm = 0.3373
Took 112.83125114440918 seconds to complete one epoch of meta training
Took 120.79713606834412 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481864
Epoch 825
Meta loss on this task batch = 2.1873e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 162.0777, GNorm = 0.2325
Meta loss on this task batch = 2.4741e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 162.0841, GNorm = 0.2793
Meta loss on this task batch = 2.2737e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 162.0907, GNorm = 0.2705
Meta loss on this task batch = 3.3377e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 162.0959, GNorm = 0.3381
Meta loss on this task batch = 2.4173e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 162.1009, GNorm = 0.2448
Meta loss on this task batch = 3.0336e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 162.1055, GNorm = 0.4999
Meta loss on this task batch = 2.8250e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 162.1099, GNorm = 0.4098
Meta loss on this task batch = 2.3059e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 162.1145, GNorm = 0.3248
Meta loss on this task batch = 2.6272e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 162.1181, GNorm = 0.4788
Meta loss on this task batch = 3.3505e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 162.1215, GNorm = 0.3246
Meta loss on this task batch = 2.3261e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 162.1257, GNorm = 0.3804
Meta loss on this task batch = 3.0313e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 162.1292, GNorm = 0.4303
Meta loss on this task batch = 2.9197e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 162.1335, GNorm = 0.2873
Meta loss on this task batch = 2.6678e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 162.1379, GNorm = 0.3805
Meta loss on this task batch = 3.2624e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 162.1421, GNorm = 0.4648
Meta loss on this task batch = 2.9991e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 162.1461, GNorm = 0.3386
Meta loss on this task batch = 2.7246e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 162.1505, GNorm = 0.3559
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 162.1547, GNorm = 0.2943
Meta loss on this task batch = 3.3173e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 162.1585, GNorm = 0.6254
Took 139.32457089424133 seconds to complete one epoch of meta training
Took 147.27785348892212 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482008
Epoch 826
Meta loss on this task batch = 2.8097e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 162.1622, GNorm = 0.3483
Meta loss on this task batch = 2.6725e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 162.1659, GNorm = 0.2875
Meta loss on this task batch = 2.4041e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 162.1699, GNorm = 0.3715
Meta loss on this task batch = 2.7092e-01, Meta loss averaged over last 500 steps = 2.7875e-01, PNorm = 162.1737, GNorm = 0.4083
Meta loss on this task batch = 2.6902e-01, Meta loss averaged over last 500 steps = 2.7874e-01, PNorm = 162.1781, GNorm = 0.2484
Meta loss on this task batch = 3.1650e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 162.1826, GNorm = 0.3337
Meta loss on this task batch = 2.4049e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 162.1870, GNorm = 0.2495
Meta loss on this task batch = 2.8242e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 162.1913, GNorm = 0.2538
Meta loss on this task batch = 3.0535e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 162.1951, GNorm = 0.3900
Meta loss on this task batch = 3.1319e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 162.1992, GNorm = 0.3914
Meta loss on this task batch = 2.9874e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 162.2023, GNorm = 0.2696
Meta loss on this task batch = 2.5305e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 162.2054, GNorm = 0.3269
Meta loss on this task batch = 2.1173e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 162.2092, GNorm = 0.2733
Meta loss on this task batch = 2.8410e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 162.2130, GNorm = 0.3294
Meta loss on this task batch = 3.6238e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 162.2176, GNorm = 0.3717
Meta loss on this task batch = 2.5724e-01, Meta loss averaged over last 500 steps = 2.7875e-01, PNorm = 162.2225, GNorm = 0.2621
Meta loss on this task batch = 2.8471e-01, Meta loss averaged over last 500 steps = 2.7881e-01, PNorm = 162.2277, GNorm = 0.3177
Meta loss on this task batch = 2.4260e-01, Meta loss averaged over last 500 steps = 2.7874e-01, PNorm = 162.2332, GNorm = 0.2494
Meta loss on this task batch = 2.9794e-01, Meta loss averaged over last 500 steps = 2.7875e-01, PNorm = 162.2383, GNorm = 0.3642
Took 114.75974941253662 seconds to complete one epoch of meta training
Took 122.61940836906433 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466575
Epoch 827
Meta loss on this task batch = 2.7709e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 162.2433, GNorm = 0.2664
Meta loss on this task batch = 2.7416e-01, Meta loss averaged over last 500 steps = 2.7874e-01, PNorm = 162.2476, GNorm = 0.2808
Meta loss on this task batch = 2.6358e-01, Meta loss averaged over last 500 steps = 2.7877e-01, PNorm = 162.2518, GNorm = 0.2594
Meta loss on this task batch = 2.8470e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 162.2556, GNorm = 0.3336
Meta loss on this task batch = 2.8989e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 162.2595, GNorm = 0.3482
Meta loss on this task batch = 2.0156e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 162.2633, GNorm = 0.2705
Meta loss on this task batch = 2.0419e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 162.2676, GNorm = 0.2666
Meta loss on this task batch = 3.2130e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 162.2714, GNorm = 0.3033
Meta loss on this task batch = 2.9518e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 162.2749, GNorm = 0.2754
Meta loss on this task batch = 2.4242e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 162.2780, GNorm = 0.2796
Meta loss on this task batch = 3.1747e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 162.2805, GNorm = 0.3905
Meta loss on this task batch = 2.5098e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 162.2829, GNorm = 0.2724
Meta loss on this task batch = 2.9291e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 162.2847, GNorm = 0.3806
Meta loss on this task batch = 2.5561e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 162.2861, GNorm = 0.3711
Meta loss on this task batch = 2.8309e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 162.2879, GNorm = 0.4218
Meta loss on this task batch = 2.5786e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 162.2903, GNorm = 0.2510
Meta loss on this task batch = 2.3714e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 162.2933, GNorm = 0.2228
Meta loss on this task batch = 3.2560e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 162.2960, GNorm = 0.3283
Meta loss on this task batch = 3.0637e-01, Meta loss averaged over last 500 steps = 2.7877e-01, PNorm = 162.2984, GNorm = 0.3685
Took 144.7086570262909 seconds to complete one epoch of meta training
Took 152.61761665344238 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477826
Epoch 828
Meta loss on this task batch = 2.8503e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 162.3019, GNorm = 0.3532
Meta loss on this task batch = 2.6743e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 162.3052, GNorm = 0.3218
Meta loss on this task batch = 2.4920e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 162.3087, GNorm = 0.3123
Meta loss on this task batch = 2.7595e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 162.3126, GNorm = 0.3375
Meta loss on this task batch = 2.8748e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 162.3170, GNorm = 0.2585
Meta loss on this task batch = 3.0649e-01, Meta loss averaged over last 500 steps = 2.7875e-01, PNorm = 162.3196, GNorm = 0.4305
Meta loss on this task batch = 2.6616e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 162.3221, GNorm = 0.2889
Meta loss on this task batch = 2.4905e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 162.3243, GNorm = 0.2867
Meta loss on this task batch = 2.2413e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 162.3274, GNorm = 0.3015
Meta loss on this task batch = 2.9084e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 162.3304, GNorm = 0.5229
Meta loss on this task batch = 3.2249e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 162.3328, GNorm = 0.3930
Meta loss on this task batch = 2.3960e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 162.3356, GNorm = 0.2944
Meta loss on this task batch = 2.7024e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 162.3384, GNorm = 0.4033
Meta loss on this task batch = 2.8535e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 162.3409, GNorm = 0.3161
Meta loss on this task batch = 3.4168e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 162.3430, GNorm = 0.3333
Meta loss on this task batch = 2.3605e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 162.3456, GNorm = 0.2600
Meta loss on this task batch = 3.3610e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 162.3478, GNorm = 0.3105
Meta loss on this task batch = 2.6186e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 162.3499, GNorm = 0.3126
Meta loss on this task batch = 2.2766e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 162.3524, GNorm = 0.2941
Took 112.32179951667786 seconds to complete one epoch of meta training
Took 120.39422821998596 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459195
Epoch 829
Meta loss on this task batch = 3.2723e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 162.3539, GNorm = 0.3367
Meta loss on this task batch = 2.4797e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 162.3559, GNorm = 0.2523
Meta loss on this task batch = 2.8946e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 162.3582, GNorm = 0.2853
Meta loss on this task batch = 2.8333e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 162.3606, GNorm = 0.2979
Meta loss on this task batch = 2.9404e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 162.3632, GNorm = 0.4308
Meta loss on this task batch = 2.6978e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 162.3659, GNorm = 0.2733
Meta loss on this task batch = 3.0405e-01, Meta loss averaged over last 500 steps = 2.7872e-01, PNorm = 162.3687, GNorm = 0.4131
Meta loss on this task batch = 2.9354e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 162.3719, GNorm = 0.2565
Meta loss on this task batch = 2.9615e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 162.3751, GNorm = 0.2574
Meta loss on this task batch = 2.9783e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 162.3787, GNorm = 0.3005
Meta loss on this task batch = 2.6899e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 162.3829, GNorm = 0.3301
Meta loss on this task batch = 3.0008e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 162.3865, GNorm = 0.3716
Meta loss on this task batch = 2.6375e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 162.3911, GNorm = 0.2863
Meta loss on this task batch = 3.2414e-01, Meta loss averaged over last 500 steps = 2.7890e-01, PNorm = 162.3958, GNorm = 0.3801
Meta loss on this task batch = 2.7996e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 162.4006, GNorm = 0.2954
Meta loss on this task batch = 2.5325e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 162.4054, GNorm = 0.2849
Meta loss on this task batch = 1.8983e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 162.4106, GNorm = 0.2797
Meta loss on this task batch = 2.7134e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 162.4152, GNorm = 0.3239
Meta loss on this task batch = 2.5067e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 162.4187, GNorm = 0.6509
Took 112.1551947593689 seconds to complete one epoch of meta training
Took 120.62828183174133 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471185
Epoch 830
Meta loss on this task batch = 2.3874e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 162.4221, GNorm = 0.2635
Meta loss on this task batch = 3.7552e-01, Meta loss averaged over last 500 steps = 2.7870e-01, PNorm = 162.4249, GNorm = 0.3969
Meta loss on this task batch = 2.8342e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 162.4284, GNorm = 0.3525
Meta loss on this task batch = 2.5530e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 162.4319, GNorm = 0.3454
Meta loss on this task batch = 2.5525e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 162.4353, GNorm = 0.3547
Meta loss on this task batch = 2.8980e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 162.4384, GNorm = 0.2709
Meta loss on this task batch = 2.6465e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 162.4416, GNorm = 0.2941
Meta loss on this task batch = 2.6066e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 162.4448, GNorm = 0.2683
Meta loss on this task batch = 2.9439e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 162.4485, GNorm = 0.3082
Meta loss on this task batch = 2.8011e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 162.4528, GNorm = 0.4158
Meta loss on this task batch = 2.3180e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 162.4567, GNorm = 0.4009
Meta loss on this task batch = 3.1416e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 162.4610, GNorm = 0.3549
Meta loss on this task batch = 2.2897e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 162.4654, GNorm = 0.2262
Meta loss on this task batch = 3.2894e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 162.4689, GNorm = 0.3268
Meta loss on this task batch = 2.2948e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 162.4729, GNorm = 0.2649
Meta loss on this task batch = 2.9432e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 162.4767, GNorm = 0.3176
Meta loss on this task batch = 3.6675e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 162.4807, GNorm = 0.5118
Meta loss on this task batch = 2.4564e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 162.4855, GNorm = 0.3503
Meta loss on this task batch = 2.0360e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 162.4899, GNorm = 0.4071
Took 114.26148128509521 seconds to complete one epoch of meta training
Took 122.32196283340454 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470971
Epoch 831
Meta loss on this task batch = 2.8135e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 162.4952, GNorm = 0.4927
Meta loss on this task batch = 2.4012e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 162.5005, GNorm = 0.2676
Meta loss on this task batch = 2.5160e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 162.5057, GNorm = 0.2347
Meta loss on this task batch = 2.7274e-01, Meta loss averaged over last 500 steps = 2.7819e-01, PNorm = 162.5105, GNorm = 0.3289
Meta loss on this task batch = 2.6598e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 162.5152, GNorm = 0.2709
Meta loss on this task batch = 2.8668e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 162.5198, GNorm = 0.3322
Meta loss on this task batch = 2.7638e-01, Meta loss averaged over last 500 steps = 2.7793e-01, PNorm = 162.5233, GNorm = 0.3315
Meta loss on this task batch = 3.1909e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 162.5263, GNorm = 0.3028
Meta loss on this task batch = 2.8613e-01, Meta loss averaged over last 500 steps = 2.7800e-01, PNorm = 162.5295, GNorm = 0.3126
Meta loss on this task batch = 1.9899e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 162.5326, GNorm = 0.2150
Meta loss on this task batch = 3.5304e-01, Meta loss averaged over last 500 steps = 2.7801e-01, PNorm = 162.5351, GNorm = 0.3366
Meta loss on this task batch = 3.1270e-01, Meta loss averaged over last 500 steps = 2.7812e-01, PNorm = 162.5371, GNorm = 0.3585
Meta loss on this task batch = 2.7478e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 162.5391, GNorm = 0.2700
Meta loss on this task batch = 2.3347e-01, Meta loss averaged over last 500 steps = 2.7788e-01, PNorm = 162.5417, GNorm = 0.2754
Meta loss on this task batch = 2.6088e-01, Meta loss averaged over last 500 steps = 2.7787e-01, PNorm = 162.5452, GNorm = 0.2716
Meta loss on this task batch = 2.8901e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 162.5495, GNorm = 0.3997
Meta loss on this task batch = 2.8819e-01, Meta loss averaged over last 500 steps = 2.7788e-01, PNorm = 162.5533, GNorm = 0.2970
Meta loss on this task batch = 2.9822e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 162.5572, GNorm = 0.3164
Meta loss on this task batch = 2.7884e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 162.5615, GNorm = 0.3552
Took 113.56278276443481 seconds to complete one epoch of meta training
Took 121.87031936645508 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459092
Epoch 832
Meta loss on this task batch = 2.2430e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 162.5660, GNorm = 0.2607
Meta loss on this task batch = 2.8529e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 162.5707, GNorm = 0.3721
Meta loss on this task batch = 2.7679e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 162.5758, GNorm = 0.5007
Meta loss on this task batch = 2.6970e-01, Meta loss averaged over last 500 steps = 2.7801e-01, PNorm = 162.5805, GNorm = 0.2623
Meta loss on this task batch = 2.6848e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 162.5854, GNorm = 0.3031
Meta loss on this task batch = 2.7269e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 162.5907, GNorm = 0.2985
Meta loss on this task batch = 2.8545e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 162.5952, GNorm = 0.3229
Meta loss on this task batch = 3.2144e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 162.5989, GNorm = 0.3846
Meta loss on this task batch = 2.5240e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 162.6024, GNorm = 0.3704
Meta loss on this task batch = 2.7929e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 162.6058, GNorm = 0.2617
Meta loss on this task batch = 2.9180e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 162.6091, GNorm = 0.2588
Meta loss on this task batch = 3.1282e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 162.6124, GNorm = 0.2904
Meta loss on this task batch = 2.7782e-01, Meta loss averaged over last 500 steps = 2.7812e-01, PNorm = 162.6150, GNorm = 0.4326
Meta loss on this task batch = 2.8397e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 162.6184, GNorm = 0.3637
Meta loss on this task batch = 2.7763e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 162.6226, GNorm = 0.3054
Meta loss on this task batch = 2.3660e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 162.6275, GNorm = 0.2642
Meta loss on this task batch = 2.5170e-01, Meta loss averaged over last 500 steps = 2.7780e-01, PNorm = 162.6323, GNorm = 0.2598
Meta loss on this task batch = 3.5200e-01, Meta loss averaged over last 500 steps = 2.7801e-01, PNorm = 162.6367, GNorm = 0.3561
Meta loss on this task batch = 2.7846e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 162.6406, GNorm = 0.3693
Took 116.30126309394836 seconds to complete one epoch of meta training
Took 123.00330591201782 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472146
Epoch 833
Meta loss on this task batch = 2.8104e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 162.6442, GNorm = 0.2505
Meta loss on this task batch = 2.3884e-01, Meta loss averaged over last 500 steps = 2.7793e-01, PNorm = 162.6481, GNorm = 0.3091
Meta loss on this task batch = 2.3599e-01, Meta loss averaged over last 500 steps = 2.7790e-01, PNorm = 162.6527, GNorm = 0.2470
Meta loss on this task batch = 2.7545e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 162.6575, GNorm = 0.2811
Meta loss on this task batch = 2.3700e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 162.6625, GNorm = 0.2363
Meta loss on this task batch = 2.2274e-01, Meta loss averaged over last 500 steps = 2.7780e-01, PNorm = 162.6671, GNorm = 0.2405
Meta loss on this task batch = 3.3416e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 162.6709, GNorm = 0.3919
Meta loss on this task batch = 2.3369e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 162.6751, GNorm = 0.3534
Meta loss on this task batch = 3.1556e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 162.6789, GNorm = 0.3826
Meta loss on this task batch = 2.5200e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 162.6830, GNorm = 0.2943
Meta loss on this task batch = 2.7540e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 162.6863, GNorm = 0.3025
Meta loss on this task batch = 3.2087e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 162.6899, GNorm = 0.3659
Meta loss on this task batch = 3.0222e-01, Meta loss averaged over last 500 steps = 2.7798e-01, PNorm = 162.6941, GNorm = 0.3193
Meta loss on this task batch = 2.6236e-01, Meta loss averaged over last 500 steps = 2.7808e-01, PNorm = 162.6983, GNorm = 0.3079
Meta loss on this task batch = 2.8513e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 162.7018, GNorm = 0.2600
Meta loss on this task batch = 2.6355e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 162.7051, GNorm = 0.3024
Meta loss on this task batch = 3.0871e-01, Meta loss averaged over last 500 steps = 2.7819e-01, PNorm = 162.7081, GNorm = 0.3005
Meta loss on this task batch = 2.8600e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 162.7112, GNorm = 0.3428
Meta loss on this task batch = 3.4161e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 162.7152, GNorm = 0.3868
Took 115.18261408805847 seconds to complete one epoch of meta training
Took 123.00381994247437 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470609
Epoch 834
Meta loss on this task batch = 2.4785e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 162.7200, GNorm = 0.2571
Meta loss on this task batch = 2.9083e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 162.7248, GNorm = 0.2771
Meta loss on this task batch = 2.8207e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 162.7294, GNorm = 0.3990
Meta loss on this task batch = 2.4373e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 162.7344, GNorm = 0.2544
Meta loss on this task batch = 2.5908e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 162.7397, GNorm = 0.2408
Meta loss on this task batch = 2.4699e-01, Meta loss averaged over last 500 steps = 2.7793e-01, PNorm = 162.7453, GNorm = 0.2551
Meta loss on this task batch = 2.5400e-01, Meta loss averaged over last 500 steps = 2.7781e-01, PNorm = 162.7509, GNorm = 0.3166
Meta loss on this task batch = 2.6220e-01, Meta loss averaged over last 500 steps = 2.7779e-01, PNorm = 162.7566, GNorm = 0.4133
Meta loss on this task batch = 2.7145e-01, Meta loss averaged over last 500 steps = 2.7784e-01, PNorm = 162.7626, GNorm = 0.2770
Meta loss on this task batch = 2.6827e-01, Meta loss averaged over last 500 steps = 2.7787e-01, PNorm = 162.7681, GNorm = 0.3124
Meta loss on this task batch = 2.5687e-01, Meta loss averaged over last 500 steps = 2.7782e-01, PNorm = 162.7733, GNorm = 0.2726
Meta loss on this task batch = 2.8579e-01, Meta loss averaged over last 500 steps = 2.7779e-01, PNorm = 162.7783, GNorm = 0.3678
Meta loss on this task batch = 2.3919e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 162.7829, GNorm = 0.2643
Meta loss on this task batch = 2.7585e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 162.7874, GNorm = 0.2210
Meta loss on this task batch = 2.3546e-01, Meta loss averaged over last 500 steps = 2.7766e-01, PNorm = 162.7913, GNorm = 0.2706
Meta loss on this task batch = 2.8575e-01, Meta loss averaged over last 500 steps = 2.7774e-01, PNorm = 162.7950, GNorm = 0.2891
Meta loss on this task batch = 3.6995e-01, Meta loss averaged over last 500 steps = 2.7796e-01, PNorm = 162.7979, GNorm = 0.3374
Meta loss on this task batch = 3.5249e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 162.7995, GNorm = 0.3628
Meta loss on this task batch = 2.4597e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 162.8020, GNorm = 0.4447
Took 121.05534648895264 seconds to complete one epoch of meta training
Took 129.1072483062744 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485537
Epoch 835
Meta loss on this task batch = 2.1452e-01, Meta loss averaged over last 500 steps = 2.7794e-01, PNorm = 162.8049, GNorm = 0.2874
Meta loss on this task batch = 3.1699e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 162.8078, GNorm = 0.2757
Meta loss on this task batch = 2.7160e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 162.8109, GNorm = 0.3026
Meta loss on this task batch = 3.0327e-01, Meta loss averaged over last 500 steps = 2.7801e-01, PNorm = 162.8140, GNorm = 0.2944
Meta loss on this task batch = 3.3088e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 162.8162, GNorm = 0.3189
Meta loss on this task batch = 2.6425e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 162.8187, GNorm = 0.3052
Meta loss on this task batch = 3.0041e-01, Meta loss averaged over last 500 steps = 2.7816e-01, PNorm = 162.8215, GNorm = 0.3380
Meta loss on this task batch = 2.9388e-01, Meta loss averaged over last 500 steps = 2.7816e-01, PNorm = 162.8248, GNorm = 0.2681
Meta loss on this task batch = 2.9178e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 162.8282, GNorm = 0.3177
Meta loss on this task batch = 2.2960e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 162.8320, GNorm = 0.2809
Meta loss on this task batch = 2.9278e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 162.8356, GNorm = 0.2922
Meta loss on this task batch = 2.9098e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 162.8403, GNorm = 0.3556
Meta loss on this task batch = 2.1820e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 162.8451, GNorm = 0.2202
Meta loss on this task batch = 2.3939e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 162.8501, GNorm = 0.2277
Meta loss on this task batch = 2.9460e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 162.8554, GNorm = 0.2503
Meta loss on this task batch = 3.2228e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 162.8605, GNorm = 0.2706
Meta loss on this task batch = 2.1936e-01, Meta loss averaged over last 500 steps = 2.7816e-01, PNorm = 162.8657, GNorm = 0.2180
Meta loss on this task batch = 2.9982e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 162.8708, GNorm = 0.2854
Meta loss on this task batch = 1.9926e-01, Meta loss averaged over last 500 steps = 2.7812e-01, PNorm = 162.8763, GNorm = 0.3305
Took 151.45659828186035 seconds to complete one epoch of meta training
Took 159.6624140739441 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498609
Epoch 836
Meta loss on this task batch = 1.9825e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 162.8815, GNorm = 0.3327
Meta loss on this task batch = 2.8457e-01, Meta loss averaged over last 500 steps = 2.7793e-01, PNorm = 162.8861, GNorm = 0.4483
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 162.8909, GNorm = 0.3234
Meta loss on this task batch = 2.5777e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 162.8953, GNorm = 0.2767
Meta loss on this task batch = 3.0856e-01, Meta loss averaged over last 500 steps = 2.7789e-01, PNorm = 162.8993, GNorm = 0.3493
Meta loss on this task batch = 2.4993e-01, Meta loss averaged over last 500 steps = 2.7793e-01, PNorm = 162.9034, GNorm = 0.2586
Meta loss on this task batch = 2.6835e-01, Meta loss averaged over last 500 steps = 2.7785e-01, PNorm = 162.9076, GNorm = 0.2787
Meta loss on this task batch = 3.2259e-01, Meta loss averaged over last 500 steps = 2.7785e-01, PNorm = 162.9117, GNorm = 0.3212
Meta loss on this task batch = 2.7831e-01, Meta loss averaged over last 500 steps = 2.7790e-01, PNorm = 162.9158, GNorm = 0.3080
Meta loss on this task batch = 3.2244e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 162.9192, GNorm = 0.3941
Meta loss on this task batch = 2.7604e-01, Meta loss averaged over last 500 steps = 2.7796e-01, PNorm = 162.9231, GNorm = 0.3158
Meta loss on this task batch = 3.4276e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 162.9264, GNorm = 0.4893
Meta loss on this task batch = 2.9121e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 162.9293, GNorm = 0.3248
Meta loss on this task batch = 2.8452e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 162.9323, GNorm = 0.4382
Meta loss on this task batch = 2.4451e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 162.9356, GNorm = 0.2998
Meta loss on this task batch = 2.2907e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 162.9392, GNorm = 0.2259
Meta loss on this task batch = 2.5628e-01, Meta loss averaged over last 500 steps = 2.7801e-01, PNorm = 162.9424, GNorm = 0.2657
Meta loss on this task batch = 3.1374e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 162.9460, GNorm = 0.3331
Meta loss on this task batch = 2.4073e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 162.9509, GNorm = 0.3007
Took 153.68961715698242 seconds to complete one epoch of meta training
Took 162.1610450744629 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476663
Epoch 837
Meta loss on this task batch = 2.0513e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 162.9562, GNorm = 0.2274
Meta loss on this task batch = 3.0825e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 162.9617, GNorm = 0.2483
Meta loss on this task batch = 2.7651e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 162.9667, GNorm = 0.2729
Meta loss on this task batch = 3.2740e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 162.9718, GNorm = 0.2754
Meta loss on this task batch = 2.7001e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 162.9767, GNorm = 0.2631
Meta loss on this task batch = 2.5832e-01, Meta loss averaged over last 500 steps = 2.7781e-01, PNorm = 162.9817, GNorm = 0.3229
Meta loss on this task batch = 3.0194e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 162.9862, GNorm = 0.4263
Meta loss on this task batch = 3.0956e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 162.9899, GNorm = 0.3624
Meta loss on this task batch = 2.1693e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 162.9934, GNorm = 0.2767
Meta loss on this task batch = 2.9038e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 162.9970, GNorm = 0.3418
Meta loss on this task batch = 2.7353e-01, Meta loss averaged over last 500 steps = 2.7774e-01, PNorm = 163.0004, GNorm = 0.2726
Meta loss on this task batch = 2.2307e-01, Meta loss averaged over last 500 steps = 2.7761e-01, PNorm = 163.0040, GNorm = 0.2193
Meta loss on this task batch = 2.7750e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 163.0074, GNorm = 0.2353
Meta loss on this task batch = 2.7501e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 163.0102, GNorm = 0.3647
Meta loss on this task batch = 2.6488e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 163.0129, GNorm = 0.3080
Meta loss on this task batch = 3.0938e-01, Meta loss averaged over last 500 steps = 2.7779e-01, PNorm = 163.0144, GNorm = 0.4209
Meta loss on this task batch = 2.5329e-01, Meta loss averaged over last 500 steps = 2.7770e-01, PNorm = 163.0160, GNorm = 0.2801
Meta loss on this task batch = 2.5821e-01, Meta loss averaged over last 500 steps = 2.7764e-01, PNorm = 163.0179, GNorm = 0.3035
Meta loss on this task batch = 2.6953e-01, Meta loss averaged over last 500 steps = 2.7759e-01, PNorm = 163.0193, GNorm = 0.3684
Took 122.35959458351135 seconds to complete one epoch of meta training
Took 130.90430402755737 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477252
Epoch 838
Meta loss on this task batch = 2.6345e-01, Meta loss averaged over last 500 steps = 2.7750e-01, PNorm = 163.0217, GNorm = 0.3604
Meta loss on this task batch = 3.1291e-01, Meta loss averaged over last 500 steps = 2.7766e-01, PNorm = 163.0236, GNorm = 0.3178
Meta loss on this task batch = 2.8991e-01, Meta loss averaged over last 500 steps = 2.7758e-01, PNorm = 163.0254, GNorm = 0.3236
Meta loss on this task batch = 2.8726e-01, Meta loss averaged over last 500 steps = 2.7771e-01, PNorm = 163.0269, GNorm = 0.3382
Meta loss on this task batch = 2.8317e-01, Meta loss averaged over last 500 steps = 2.7776e-01, PNorm = 163.0290, GNorm = 0.2639
Meta loss on this task batch = 2.8341e-01, Meta loss averaged over last 500 steps = 2.7780e-01, PNorm = 163.0312, GNorm = 0.3551
Meta loss on this task batch = 2.8649e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 163.0333, GNorm = 0.3214
Meta loss on this task batch = 2.4309e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 163.0352, GNorm = 0.2719
Meta loss on this task batch = 3.1707e-01, Meta loss averaged over last 500 steps = 2.7783e-01, PNorm = 163.0373, GNorm = 0.3847
Meta loss on this task batch = 2.9343e-01, Meta loss averaged over last 500 steps = 2.7784e-01, PNorm = 163.0391, GNorm = 0.2763
Meta loss on this task batch = 2.7993e-01, Meta loss averaged over last 500 steps = 2.7779e-01, PNorm = 163.0412, GNorm = 0.2662
Meta loss on this task batch = 2.9983e-01, Meta loss averaged over last 500 steps = 2.7785e-01, PNorm = 163.0442, GNorm = 0.4137
Meta loss on this task batch = 2.4975e-01, Meta loss averaged over last 500 steps = 2.7788e-01, PNorm = 163.0480, GNorm = 0.3296
Meta loss on this task batch = 2.3845e-01, Meta loss averaged over last 500 steps = 2.7780e-01, PNorm = 163.0525, GNorm = 0.2627
Meta loss on this task batch = 2.4248e-01, Meta loss averaged over last 500 steps = 2.7785e-01, PNorm = 163.0575, GNorm = 0.3088
Meta loss on this task batch = 2.6914e-01, Meta loss averaged over last 500 steps = 2.7787e-01, PNorm = 163.0636, GNorm = 0.3835
Meta loss on this task batch = 3.0487e-01, Meta loss averaged over last 500 steps = 2.7794e-01, PNorm = 163.0701, GNorm = 0.2680
Meta loss on this task batch = 2.7193e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 163.0765, GNorm = 0.3651
Meta loss on this task batch = 2.7523e-01, Meta loss averaged over last 500 steps = 2.7780e-01, PNorm = 163.0826, GNorm = 0.3125
Took 139.52785515785217 seconds to complete one epoch of meta training
Took 147.75617241859436 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467377
Epoch 839
Meta loss on this task batch = 3.5282e-01, Meta loss averaged over last 500 steps = 2.7788e-01, PNorm = 163.0884, GNorm = 0.2907
Meta loss on this task batch = 2.9242e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 163.0940, GNorm = 0.2882
Meta loss on this task batch = 2.7908e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 163.0993, GNorm = 0.2612
Meta loss on this task batch = 2.9843e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 163.1051, GNorm = 0.3894
Meta loss on this task batch = 3.0868e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 163.1107, GNorm = 0.3128
Meta loss on this task batch = 3.4175e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 163.1154, GNorm = 0.2859
Meta loss on this task batch = 3.2003e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 163.1198, GNorm = 0.4304
Meta loss on this task batch = 2.8056e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 163.1248, GNorm = 0.2599
Meta loss on this task batch = 2.6930e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 163.1302, GNorm = 0.4636
Meta loss on this task batch = 2.3483e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 163.1363, GNorm = 0.2091
Meta loss on this task batch = 2.8499e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 163.1420, GNorm = 0.2265
Meta loss on this task batch = 2.7045e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 163.1474, GNorm = 0.2763
Meta loss on this task batch = 2.5014e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 163.1531, GNorm = 0.4318
Meta loss on this task batch = 2.6223e-01, Meta loss averaged over last 500 steps = 2.7817e-01, PNorm = 163.1591, GNorm = 0.3938
Meta loss on this task batch = 2.8187e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 163.1653, GNorm = 0.3063
Meta loss on this task batch = 3.0292e-01, Meta loss averaged over last 500 steps = 2.7821e-01, PNorm = 163.1708, GNorm = 0.3074
Meta loss on this task batch = 2.7086e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 163.1758, GNorm = 0.2531
Meta loss on this task batch = 2.8303e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 163.1805, GNorm = 0.3025
Meta loss on this task batch = 2.1252e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 163.1850, GNorm = 0.3235
Took 123.84530591964722 seconds to complete one epoch of meta training
Took 131.78710985183716 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479584
Epoch 840
Meta loss on this task batch = 2.1922e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 163.1893, GNorm = 0.2613
Meta loss on this task batch = 2.9915e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 163.1930, GNorm = 0.3222
Meta loss on this task batch = 2.9478e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 163.1973, GNorm = 0.3119
Meta loss on this task batch = 2.1616e-01, Meta loss averaged over last 500 steps = 2.7776e-01, PNorm = 163.2023, GNorm = 0.2854
Meta loss on this task batch = 3.0406e-01, Meta loss averaged over last 500 steps = 2.7787e-01, PNorm = 163.2070, GNorm = 0.2606
Meta loss on this task batch = 2.6395e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 163.2114, GNorm = 0.2481
Meta loss on this task batch = 2.8731e-01, Meta loss averaged over last 500 steps = 2.7781e-01, PNorm = 163.2156, GNorm = 0.3112
Meta loss on this task batch = 3.0566e-01, Meta loss averaged over last 500 steps = 2.7777e-01, PNorm = 163.2196, GNorm = 0.2472
Meta loss on this task batch = 2.9341e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 163.2239, GNorm = 0.2745
Meta loss on this task batch = 2.9618e-01, Meta loss averaged over last 500 steps = 2.7790e-01, PNorm = 163.2279, GNorm = 0.2687
Meta loss on this task batch = 2.5298e-01, Meta loss averaged over last 500 steps = 2.7783e-01, PNorm = 163.2314, GNorm = 0.2402
Meta loss on this task batch = 2.9993e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 163.2342, GNorm = 0.3357
Meta loss on this task batch = 2.5823e-01, Meta loss averaged over last 500 steps = 2.7787e-01, PNorm = 163.2370, GNorm = 0.3077
Meta loss on this task batch = 2.7301e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 163.2404, GNorm = 0.3882
Meta loss on this task batch = 2.8190e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 163.2442, GNorm = 0.3081
Meta loss on this task batch = 2.3664e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 163.2482, GNorm = 0.2833
Meta loss on this task batch = 2.9594e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 163.2525, GNorm = 0.3146
Meta loss on this task batch = 3.3735e-01, Meta loss averaged over last 500 steps = 2.7816e-01, PNorm = 163.2562, GNorm = 0.2971
Meta loss on this task batch = 2.4196e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 163.2610, GNorm = 0.3439
Took 126.07874059677124 seconds to complete one epoch of meta training
Took 134.65091013908386 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461727
Epoch 841
Meta loss on this task batch = 2.4058e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 163.2658, GNorm = 0.3077
Meta loss on this task batch = 2.7612e-01, Meta loss averaged over last 500 steps = 2.7793e-01, PNorm = 163.2711, GNorm = 0.3111
Meta loss on this task batch = 3.0859e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 163.2770, GNorm = 0.3704
Meta loss on this task batch = 2.4572e-01, Meta loss averaged over last 500 steps = 2.7801e-01, PNorm = 163.2830, GNorm = 0.2873
Meta loss on this task batch = 2.3378e-01, Meta loss averaged over last 500 steps = 2.7789e-01, PNorm = 163.2890, GNorm = 0.2288
Meta loss on this task batch = 2.8659e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 163.2948, GNorm = 0.2867
Meta loss on this task batch = 3.2102e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 163.3000, GNorm = 0.6588
Meta loss on this task batch = 2.2496e-01, Meta loss averaged over last 500 steps = 2.7796e-01, PNorm = 163.3049, GNorm = 0.3912
Meta loss on this task batch = 2.8095e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 163.3099, GNorm = 0.3769
Meta loss on this task batch = 2.3469e-01, Meta loss averaged over last 500 steps = 2.7780e-01, PNorm = 163.3147, GNorm = 0.2637
Meta loss on this task batch = 2.7700e-01, Meta loss averaged over last 500 steps = 2.7774e-01, PNorm = 163.3191, GNorm = 0.2896
Meta loss on this task batch = 3.0993e-01, Meta loss averaged over last 500 steps = 2.7776e-01, PNorm = 163.3220, GNorm = 0.3487
Meta loss on this task batch = 3.4319e-01, Meta loss averaged over last 500 steps = 2.7784e-01, PNorm = 163.3246, GNorm = 0.4457
Meta loss on this task batch = 3.2699e-01, Meta loss averaged over last 500 steps = 2.7796e-01, PNorm = 163.3266, GNorm = 0.3665
Meta loss on this task batch = 2.6107e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 163.3294, GNorm = 0.2635
Meta loss on this task batch = 2.5507e-01, Meta loss averaged over last 500 steps = 2.7794e-01, PNorm = 163.3319, GNorm = 0.2718
Meta loss on this task batch = 2.8918e-01, Meta loss averaged over last 500 steps = 2.7790e-01, PNorm = 163.3350, GNorm = 0.2708
Meta loss on this task batch = 2.7717e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 163.3384, GNorm = 0.4843
Meta loss on this task batch = 2.8013e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 163.3418, GNorm = 0.3997
Took 138.84220266342163 seconds to complete one epoch of meta training
Took 145.7409746646881 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468036
Epoch 842
Meta loss on this task batch = 2.9982e-01, Meta loss averaged over last 500 steps = 2.7798e-01, PNorm = 163.3448, GNorm = 0.3965
Meta loss on this task batch = 2.4645e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 163.3483, GNorm = 0.2807
Meta loss on this task batch = 3.3038e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 163.3517, GNorm = 0.2755
Meta loss on this task batch = 3.2366e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 163.3559, GNorm = 0.4399
Meta loss on this task batch = 2.8673e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 163.3613, GNorm = 0.4188
Meta loss on this task batch = 2.7330e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 163.3665, GNorm = 0.3013
Meta loss on this task batch = 2.8244e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 163.3724, GNorm = 0.3708
Meta loss on this task batch = 2.7736e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 163.3776, GNorm = 0.4204
Meta loss on this task batch = 2.8872e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 163.3827, GNorm = 0.2583
Meta loss on this task batch = 2.5255e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 163.3879, GNorm = 0.2450
Meta loss on this task batch = 2.6564e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 163.3923, GNorm = 0.2707
Meta loss on this task batch = 2.5951e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 163.3964, GNorm = 0.2656
Meta loss on this task batch = 2.7666e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 163.3999, GNorm = 0.4246
Meta loss on this task batch = 2.6281e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 163.4031, GNorm = 0.2461
Meta loss on this task batch = 3.2090e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 163.4063, GNorm = 0.3217
Meta loss on this task batch = 2.9083e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 163.4092, GNorm = 0.3124
Meta loss on this task batch = 2.6167e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 163.4124, GNorm = 0.3275
Meta loss on this task batch = 2.2946e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 163.4154, GNorm = 0.3083
Meta loss on this task batch = 2.7168e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 163.4188, GNorm = 0.2928
Took 160.04907965660095 seconds to complete one epoch of meta training
Took 169.08750367164612 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470790
Epoch 843
Meta loss on this task batch = 2.2557e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 163.4223, GNorm = 0.2389
Meta loss on this task batch = 2.9949e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 163.4248, GNorm = 0.3376
Meta loss on this task batch = 3.1976e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 163.4274, GNorm = 0.4519
Meta loss on this task batch = 3.1102e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 163.4303, GNorm = 0.3371
Meta loss on this task batch = 2.3409e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 163.4337, GNorm = 0.2408
Meta loss on this task batch = 3.0441e-01, Meta loss averaged over last 500 steps = 2.7833e-01, PNorm = 163.4369, GNorm = 0.2975
Meta loss on this task batch = 2.7860e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 163.4394, GNorm = 0.4453
Meta loss on this task batch = 3.2537e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 163.4423, GNorm = 0.3718
Meta loss on this task batch = 2.6667e-01, Meta loss averaged over last 500 steps = 2.7838e-01, PNorm = 163.4459, GNorm = 0.2812
Meta loss on this task batch = 2.8021e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 163.4497, GNorm = 0.3341
Meta loss on this task batch = 2.8642e-01, Meta loss averaged over last 500 steps = 2.7833e-01, PNorm = 163.4533, GNorm = 0.2728
Meta loss on this task batch = 2.6370e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 163.4563, GNorm = 0.3600
Meta loss on this task batch = 2.3550e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 163.4594, GNorm = 0.3369
Meta loss on this task batch = 2.7907e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 163.4623, GNorm = 0.2552
Meta loss on this task batch = 2.7818e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 163.4654, GNorm = 0.3373
Meta loss on this task batch = 2.8839e-01, Meta loss averaged over last 500 steps = 2.7821e-01, PNorm = 163.4679, GNorm = 0.4557
Meta loss on this task batch = 2.8625e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 163.4699, GNorm = 0.2854
Meta loss on this task batch = 2.8268e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 163.4720, GNorm = 0.2689
Meta loss on this task batch = 1.9999e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 163.4743, GNorm = 0.2939
Took 118.2812147140503 seconds to complete one epoch of meta training
Took 125.54310750961304 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481926
Epoch 844
Meta loss on this task batch = 2.6352e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 163.4767, GNorm = 0.2945
Meta loss on this task batch = 2.9583e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 163.4785, GNorm = 0.2959
Meta loss on this task batch = 2.4489e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 163.4809, GNorm = 0.2190
Meta loss on this task batch = 2.0247e-01, Meta loss averaged over last 500 steps = 2.7789e-01, PNorm = 163.4836, GNorm = 0.2108
Meta loss on this task batch = 2.7554e-01, Meta loss averaged over last 500 steps = 2.7783e-01, PNorm = 163.4864, GNorm = 0.2733
Meta loss on this task batch = 2.9150e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 163.4892, GNorm = 0.3684
Meta loss on this task batch = 2.8846e-01, Meta loss averaged over last 500 steps = 2.7782e-01, PNorm = 163.4920, GNorm = 0.4250
Meta loss on this task batch = 2.1166e-01, Meta loss averaged over last 500 steps = 2.7762e-01, PNorm = 163.4957, GNorm = 0.2420
Meta loss on this task batch = 3.0098e-01, Meta loss averaged over last 500 steps = 2.7769e-01, PNorm = 163.4988, GNorm = 0.2910
Meta loss on this task batch = 2.7267e-01, Meta loss averaged over last 500 steps = 2.7771e-01, PNorm = 163.5025, GNorm = 0.3620
Meta loss on this task batch = 2.1702e-01, Meta loss averaged over last 500 steps = 2.7768e-01, PNorm = 163.5063, GNorm = 0.2663
Meta loss on this task batch = 3.0675e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 163.5096, GNorm = 0.2979
Meta loss on this task batch = 2.6817e-01, Meta loss averaged over last 500 steps = 2.7763e-01, PNorm = 163.5127, GNorm = 0.3687
Meta loss on this task batch = 3.4127e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 163.5155, GNorm = 0.4462
Meta loss on this task batch = 2.6155e-01, Meta loss averaged over last 500 steps = 2.7790e-01, PNorm = 163.5191, GNorm = 0.2644
Meta loss on this task batch = 3.0026e-01, Meta loss averaged over last 500 steps = 2.7800e-01, PNorm = 163.5224, GNorm = 0.2880
Meta loss on this task batch = 2.8192e-01, Meta loss averaged over last 500 steps = 2.7790e-01, PNorm = 163.5266, GNorm = 0.4651
Meta loss on this task batch = 2.3966e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 163.5309, GNorm = 0.2472
Meta loss on this task batch = 2.8879e-01, Meta loss averaged over last 500 steps = 2.7781e-01, PNorm = 163.5346, GNorm = 0.4303
Took 118.82112693786621 seconds to complete one epoch of meta training
Took 127.93688201904297 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456930
Epoch 845
Meta loss on this task batch = 2.7766e-01, Meta loss averaged over last 500 steps = 2.7777e-01, PNorm = 163.5383, GNorm = 0.3005
Meta loss on this task batch = 2.6881e-01, Meta loss averaged over last 500 steps = 2.7768e-01, PNorm = 163.5423, GNorm = 0.2610
Meta loss on this task batch = 2.4648e-01, Meta loss averaged over last 500 steps = 2.7756e-01, PNorm = 163.5466, GNorm = 0.2525
Meta loss on this task batch = 2.5642e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 163.5509, GNorm = 0.2636
Meta loss on this task batch = 3.2019e-01, Meta loss averaged over last 500 steps = 2.7742e-01, PNorm = 163.5554, GNorm = 0.3245
Meta loss on this task batch = 2.7925e-01, Meta loss averaged over last 500 steps = 2.7739e-01, PNorm = 163.5598, GNorm = 0.2437
Meta loss on this task batch = 2.5918e-01, Meta loss averaged over last 500 steps = 2.7727e-01, PNorm = 163.5647, GNorm = 0.2954
Meta loss on this task batch = 3.1383e-01, Meta loss averaged over last 500 steps = 2.7724e-01, PNorm = 163.5690, GNorm = 0.3195
Meta loss on this task batch = 2.9191e-01, Meta loss averaged over last 500 steps = 2.7735e-01, PNorm = 163.5728, GNorm = 0.3168
Meta loss on this task batch = 2.6221e-01, Meta loss averaged over last 500 steps = 2.7731e-01, PNorm = 163.5769, GNorm = 0.3272
Meta loss on this task batch = 2.6439e-01, Meta loss averaged over last 500 steps = 2.7743e-01, PNorm = 163.5812, GNorm = 0.3199
Meta loss on this task batch = 2.7903e-01, Meta loss averaged over last 500 steps = 2.7751e-01, PNorm = 163.5859, GNorm = 0.3153
Meta loss on this task batch = 2.7807e-01, Meta loss averaged over last 500 steps = 2.7761e-01, PNorm = 163.5897, GNorm = 0.3274
Meta loss on this task batch = 3.1654e-01, Meta loss averaged over last 500 steps = 2.7777e-01, PNorm = 163.5936, GNorm = 0.3177
Meta loss on this task batch = 2.4051e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 163.5982, GNorm = 0.3280
Meta loss on this task batch = 2.4303e-01, Meta loss averaged over last 500 steps = 2.7757e-01, PNorm = 163.6035, GNorm = 0.2639
Meta loss on this task batch = 2.9387e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 163.6090, GNorm = 0.2734
Meta loss on this task batch = 3.0948e-01, Meta loss averaged over last 500 steps = 2.7761e-01, PNorm = 163.6147, GNorm = 0.3918
Meta loss on this task batch = 3.2220e-01, Meta loss averaged over last 500 steps = 2.7755e-01, PNorm = 163.6196, GNorm = 0.3122
Took 121.21831798553467 seconds to complete one epoch of meta training
Took 129.43702626228333 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463510
Epoch 846
Meta loss on this task batch = 2.7324e-01, Meta loss averaged over last 500 steps = 2.7751e-01, PNorm = 163.6250, GNorm = 0.3179
Meta loss on this task batch = 3.0678e-01, Meta loss averaged over last 500 steps = 2.7759e-01, PNorm = 163.6298, GNorm = 0.3413
Meta loss on this task batch = 2.5533e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 163.6344, GNorm = 0.2367
Meta loss on this task batch = 3.2639e-01, Meta loss averaged over last 500 steps = 2.7757e-01, PNorm = 163.6390, GNorm = 0.3221
Meta loss on this task batch = 2.5291e-01, Meta loss averaged over last 500 steps = 2.7759e-01, PNorm = 163.6433, GNorm = 0.2429
Meta loss on this task batch = 3.1940e-01, Meta loss averaged over last 500 steps = 2.7769e-01, PNorm = 163.6468, GNorm = 0.3896
Meta loss on this task batch = 2.5808e-01, Meta loss averaged over last 500 steps = 2.7769e-01, PNorm = 163.6507, GNorm = 0.3202
Meta loss on this task batch = 2.2455e-01, Meta loss averaged over last 500 steps = 2.7768e-01, PNorm = 163.6548, GNorm = 0.2516
Meta loss on this task batch = 2.7223e-01, Meta loss averaged over last 500 steps = 2.7755e-01, PNorm = 163.6593, GNorm = 0.3024
Meta loss on this task batch = 3.2938e-01, Meta loss averaged over last 500 steps = 2.7766e-01, PNorm = 163.6630, GNorm = 0.3526
Meta loss on this task batch = 2.9781e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 163.6667, GNorm = 0.3247
Meta loss on this task batch = 2.8947e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 163.6701, GNorm = 0.3278
Meta loss on this task batch = 2.4956e-01, Meta loss averaged over last 500 steps = 2.7764e-01, PNorm = 163.6741, GNorm = 0.3249
Meta loss on this task batch = 3.1473e-01, Meta loss averaged over last 500 steps = 2.7774e-01, PNorm = 163.6785, GNorm = 0.4322
Meta loss on this task batch = 2.4448e-01, Meta loss averaged over last 500 steps = 2.7756e-01, PNorm = 163.6827, GNorm = 0.2690
Meta loss on this task batch = 3.0101e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 163.6866, GNorm = 0.4634
Meta loss on this task batch = 2.5234e-01, Meta loss averaged over last 500 steps = 2.7763e-01, PNorm = 163.6906, GNorm = 0.2655
Meta loss on this task batch = 2.0330e-01, Meta loss averaged over last 500 steps = 2.7747e-01, PNorm = 163.6953, GNorm = 0.3229
Meta loss on this task batch = 3.2743e-01, Meta loss averaged over last 500 steps = 2.7755e-01, PNorm = 163.6986, GNorm = 0.4736
Took 136.4547209739685 seconds to complete one epoch of meta training
Took 145.1865530014038 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461542
Epoch 847
Meta loss on this task batch = 3.0726e-01, Meta loss averaged over last 500 steps = 2.7756e-01, PNorm = 163.7018, GNorm = 0.3453
Meta loss on this task batch = 2.4588e-01, Meta loss averaged over last 500 steps = 2.7740e-01, PNorm = 163.7054, GNorm = 0.2502
Meta loss on this task batch = 2.5893e-01, Meta loss averaged over last 500 steps = 2.7735e-01, PNorm = 163.7095, GNorm = 0.4261
Meta loss on this task batch = 2.5189e-01, Meta loss averaged over last 500 steps = 2.7723e-01, PNorm = 163.7133, GNorm = 0.2411
Meta loss on this task batch = 2.4112e-01, Meta loss averaged over last 500 steps = 2.7719e-01, PNorm = 163.7175, GNorm = 0.2311
Meta loss on this task batch = 2.8241e-01, Meta loss averaged over last 500 steps = 2.7724e-01, PNorm = 163.7220, GNorm = 0.3152
Meta loss on this task batch = 3.0297e-01, Meta loss averaged over last 500 steps = 2.7731e-01, PNorm = 163.7263, GNorm = 0.2715
Meta loss on this task batch = 3.0564e-01, Meta loss averaged over last 500 steps = 2.7738e-01, PNorm = 163.7312, GNorm = 0.4917
Meta loss on this task batch = 3.3872e-01, Meta loss averaged over last 500 steps = 2.7743e-01, PNorm = 163.7353, GNorm = 0.4259
Meta loss on this task batch = 2.6065e-01, Meta loss averaged over last 500 steps = 2.7739e-01, PNorm = 163.7394, GNorm = 0.3219
Meta loss on this task batch = 2.4737e-01, Meta loss averaged over last 500 steps = 2.7731e-01, PNorm = 163.7433, GNorm = 0.2772
Meta loss on this task batch = 2.9646e-01, Meta loss averaged over last 500 steps = 2.7740e-01, PNorm = 163.7476, GNorm = 0.2849
Meta loss on this task batch = 2.5659e-01, Meta loss averaged over last 500 steps = 2.7741e-01, PNorm = 163.7524, GNorm = 0.2933
Meta loss on this task batch = 3.0068e-01, Meta loss averaged over last 500 steps = 2.7751e-01, PNorm = 163.7572, GNorm = 0.3633
Meta loss on this task batch = 2.7159e-01, Meta loss averaged over last 500 steps = 2.7745e-01, PNorm = 163.7624, GNorm = 0.2545
Meta loss on this task batch = 2.6685e-01, Meta loss averaged over last 500 steps = 2.7728e-01, PNorm = 163.7682, GNorm = 0.3299
Meta loss on this task batch = 2.2128e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 163.7745, GNorm = 0.2609
Meta loss on this task batch = 2.8942e-01, Meta loss averaged over last 500 steps = 2.7704e-01, PNorm = 163.7807, GNorm = 0.4953
Meta loss on this task batch = 3.0786e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 163.7863, GNorm = 0.2919
Took 120.24371457099915 seconds to complete one epoch of meta training
Took 128.52924966812134 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455527
Epoch 848
Meta loss on this task batch = 2.2971e-01, Meta loss averaged over last 500 steps = 2.7700e-01, PNorm = 163.7917, GNorm = 0.2880
Meta loss on this task batch = 2.3466e-01, Meta loss averaged over last 500 steps = 2.7693e-01, PNorm = 163.7975, GNorm = 0.3511
Meta loss on this task batch = 3.1142e-01, Meta loss averaged over last 500 steps = 2.7705e-01, PNorm = 163.8024, GNorm = 0.2735
Meta loss on this task batch = 2.6451e-01, Meta loss averaged over last 500 steps = 2.7707e-01, PNorm = 163.8077, GNorm = 0.2779
Meta loss on this task batch = 2.8808e-01, Meta loss averaged over last 500 steps = 2.7714e-01, PNorm = 163.8123, GNorm = 0.3327
Meta loss on this task batch = 2.9600e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 163.8168, GNorm = 0.5546
Meta loss on this task batch = 3.3454e-01, Meta loss averaged over last 500 steps = 2.7715e-01, PNorm = 163.8210, GNorm = 0.3583
Meta loss on this task batch = 2.7197e-01, Meta loss averaged over last 500 steps = 2.7715e-01, PNorm = 163.8248, GNorm = 0.4174
Meta loss on this task batch = 2.9839e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 163.8285, GNorm = 0.3473
Meta loss on this task batch = 2.7018e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 163.8315, GNorm = 0.2906
Meta loss on this task batch = 2.7775e-01, Meta loss averaged over last 500 steps = 2.7706e-01, PNorm = 163.8344, GNorm = 0.3247
Meta loss on this task batch = 2.5226e-01, Meta loss averaged over last 500 steps = 2.7696e-01, PNorm = 163.8378, GNorm = 0.2541
Meta loss on this task batch = 2.4218e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 163.8418, GNorm = 0.2499
Meta loss on this task batch = 2.7255e-01, Meta loss averaged over last 500 steps = 2.7680e-01, PNorm = 163.8461, GNorm = 0.3916
Meta loss on this task batch = 2.7184e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 163.8499, GNorm = 0.2771
Meta loss on this task batch = 2.6447e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 163.8536, GNorm = 0.2637
Meta loss on this task batch = 3.1101e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 163.8572, GNorm = 0.3751
Meta loss on this task batch = 2.5211e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 163.8608, GNorm = 0.2763
Meta loss on this task batch = 2.9692e-01, Meta loss averaged over last 500 steps = 2.7690e-01, PNorm = 163.8646, GNorm = 0.4454
Took 120.74861741065979 seconds to complete one epoch of meta training
Took 129.56816411018372 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464038
Epoch 849
Meta loss on this task batch = 2.4625e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 163.8683, GNorm = 0.3663
Meta loss on this task batch = 2.3166e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 163.8723, GNorm = 0.2465
Meta loss on this task batch = 3.1418e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 163.8758, GNorm = 0.3196
Meta loss on this task batch = 2.8536e-01, Meta loss averaged over last 500 steps = 2.7683e-01, PNorm = 163.8793, GNorm = 0.2977
Meta loss on this task batch = 2.7597e-01, Meta loss averaged over last 500 steps = 2.7693e-01, PNorm = 163.8823, GNorm = 0.3805
Meta loss on this task batch = 2.7661e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 163.8861, GNorm = 0.2672
Meta loss on this task batch = 3.3945e-01, Meta loss averaged over last 500 steps = 2.7696e-01, PNorm = 163.8894, GNorm = 0.4437
Meta loss on this task batch = 2.4127e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 163.8932, GNorm = 0.3377
Meta loss on this task batch = 2.7735e-01, Meta loss averaged over last 500 steps = 2.7713e-01, PNorm = 163.8969, GNorm = 0.3697
Meta loss on this task batch = 2.9171e-01, Meta loss averaged over last 500 steps = 2.7706e-01, PNorm = 163.9015, GNorm = 0.3066
Meta loss on this task batch = 2.7847e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 163.9067, GNorm = 0.2998
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 163.9116, GNorm = 0.2782
Meta loss on this task batch = 2.7439e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 163.9164, GNorm = 0.3618
Meta loss on this task batch = 2.3609e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 163.9220, GNorm = 0.4169
Meta loss on this task batch = 2.8333e-01, Meta loss averaged over last 500 steps = 2.7704e-01, PNorm = 163.9279, GNorm = 0.3579
Meta loss on this task batch = 2.0850e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 163.9340, GNorm = 0.2175
Meta loss on this task batch = 2.6191e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 163.9394, GNorm = 0.2918
Meta loss on this task batch = 3.0072e-01, Meta loss averaged over last 500 steps = 2.7683e-01, PNorm = 163.9448, GNorm = 0.2843
Meta loss on this task batch = 3.0056e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 163.9495, GNorm = 0.5569
Took 117.00071883201599 seconds to complete one epoch of meta training
Took 124.30987977981567 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.439377
Epoch 850
Meta loss on this task batch = 2.5128e-01, Meta loss averaged over last 500 steps = 2.7694e-01, PNorm = 163.9542, GNorm = 0.2326
Meta loss on this task batch = 2.6243e-01, Meta loss averaged over last 500 steps = 2.7683e-01, PNorm = 163.9588, GNorm = 0.3715
Meta loss on this task batch = 3.1800e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 163.9627, GNorm = 0.3107
Meta loss on this task batch = 3.5152e-01, Meta loss averaged over last 500 steps = 2.7698e-01, PNorm = 163.9652, GNorm = 0.3702
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 2.7698e-01, PNorm = 163.9681, GNorm = 0.2772
Meta loss on this task batch = 2.4839e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 163.9708, GNorm = 0.3063
Meta loss on this task batch = 2.8542e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 163.9728, GNorm = 0.3364
Meta loss on this task batch = 2.7637e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 163.9753, GNorm = 0.3650
Meta loss on this task batch = 2.8941e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 163.9779, GNorm = 0.3858
Meta loss on this task batch = 3.1806e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 163.9805, GNorm = 0.2753
Meta loss on this task batch = 2.1710e-01, Meta loss averaged over last 500 steps = 2.7691e-01, PNorm = 163.9833, GNorm = 0.3598
Meta loss on this task batch = 2.6664e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 163.9869, GNorm = 0.3616
Meta loss on this task batch = 2.3305e-01, Meta loss averaged over last 500 steps = 2.7678e-01, PNorm = 163.9905, GNorm = 0.2481
Meta loss on this task batch = 3.1869e-01, Meta loss averaged over last 500 steps = 2.7685e-01, PNorm = 163.9950, GNorm = 0.5262
Meta loss on this task batch = 2.0610e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 163.9995, GNorm = 0.3601
Meta loss on this task batch = 2.5732e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 164.0041, GNorm = 0.2437
Meta loss on this task batch = 2.5038e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 164.0093, GNorm = 0.2893
Meta loss on this task batch = 3.2513e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 164.0139, GNorm = 0.2782
Meta loss on this task batch = 2.6889e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 164.0182, GNorm = 0.2612
Took 123.69282484054565 seconds to complete one epoch of meta training
Took 131.73899865150452 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452740
Epoch 851
Meta loss on this task batch = 2.3943e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 164.0226, GNorm = 0.2538
Meta loss on this task batch = 2.9419e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 164.0274, GNorm = 0.3039
Meta loss on this task batch = 2.6821e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 164.0321, GNorm = 0.2460
Meta loss on this task batch = 2.9229e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 164.0369, GNorm = 0.3147
Meta loss on this task batch = 2.7176e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 164.0414, GNorm = 0.3183
Meta loss on this task batch = 3.1151e-01, Meta loss averaged over last 500 steps = 2.7662e-01, PNorm = 164.0466, GNorm = 0.3597
Meta loss on this task batch = 2.2928e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 164.0519, GNorm = 0.2449
Meta loss on this task batch = 2.8126e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 164.0574, GNorm = 0.3339
Meta loss on this task batch = 2.8394e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 164.0623, GNorm = 0.3802
Meta loss on this task batch = 2.8435e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 164.0673, GNorm = 0.4134
Meta loss on this task batch = 2.6063e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 164.0722, GNorm = 0.2911
Meta loss on this task batch = 2.6578e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 164.0774, GNorm = 0.4523
Meta loss on this task batch = 3.2598e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 164.0823, GNorm = 0.2527
Meta loss on this task batch = 2.9415e-01, Meta loss averaged over last 500 steps = 2.7690e-01, PNorm = 164.0870, GNorm = 0.2961
Meta loss on this task batch = 2.5278e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 164.0920, GNorm = 0.2930
Meta loss on this task batch = 2.6126e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 164.0972, GNorm = 0.2844
Meta loss on this task batch = 2.6721e-01, Meta loss averaged over last 500 steps = 2.7680e-01, PNorm = 164.1017, GNorm = 0.2789
Meta loss on this task batch = 2.4095e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 164.1061, GNorm = 0.3705
Meta loss on this task batch = 3.2919e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 164.1098, GNorm = 0.5549
Took 118.82610130310059 seconds to complete one epoch of meta training
Took 127.05726718902588 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469931
Epoch 852
Meta loss on this task batch = 2.4382e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 164.1136, GNorm = 0.2580
Meta loss on this task batch = 3.1165e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 164.1170, GNorm = 0.3011
Meta loss on this task batch = 2.7736e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 164.1206, GNorm = 0.3034
Meta loss on this task batch = 2.9061e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 164.1247, GNorm = 0.2976
Meta loss on this task batch = 2.6915e-01, Meta loss averaged over last 500 steps = 2.7662e-01, PNorm = 164.1288, GNorm = 0.3093
Meta loss on this task batch = 2.2412e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 164.1331, GNorm = 0.2209
Meta loss on this task batch = 2.3257e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 164.1378, GNorm = 0.2465
Meta loss on this task batch = 2.7522e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 164.1420, GNorm = 0.2715
Meta loss on this task batch = 2.5429e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 164.1460, GNorm = 0.2727
Meta loss on this task batch = 2.8111e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 164.1492, GNorm = 0.2832
Meta loss on this task batch = 2.7079e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 164.1526, GNorm = 0.2905
Meta loss on this task batch = 2.2860e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 164.1562, GNorm = 0.2815
Meta loss on this task batch = 3.1478e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 164.1596, GNorm = 0.2946
Meta loss on this task batch = 2.6555e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 164.1631, GNorm = 0.2380
Meta loss on this task batch = 2.7141e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 164.1666, GNorm = 0.3517
Meta loss on this task batch = 2.0276e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 164.1701, GNorm = 0.2352
Meta loss on this task batch = 3.1165e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 164.1736, GNorm = 0.3984
Meta loss on this task batch = 3.5665e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 164.1769, GNorm = 0.4688
Meta loss on this task batch = 3.2853e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 164.1808, GNorm = 0.3541
Took 117.15058875083923 seconds to complete one epoch of meta training
Took 125.40578603744507 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474526
Epoch 853
Meta loss on this task batch = 2.6348e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 164.1853, GNorm = 0.3125
Meta loss on this task batch = 2.5262e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 164.1894, GNorm = 0.2478
Meta loss on this task batch = 2.6044e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 164.1937, GNorm = 0.2594
Meta loss on this task batch = 2.3560e-01, Meta loss averaged over last 500 steps = 2.7614e-01, PNorm = 164.1982, GNorm = 0.3669
Meta loss on this task batch = 2.4928e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 164.2027, GNorm = 0.2383
Meta loss on this task batch = 3.1505e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 164.2069, GNorm = 0.3414
Meta loss on this task batch = 2.6883e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 164.2107, GNorm = 0.2712
Meta loss on this task batch = 3.0813e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 164.2145, GNorm = 0.2755
Meta loss on this task batch = 3.1044e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 164.2183, GNorm = 0.2839
Meta loss on this task batch = 3.0203e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 164.2226, GNorm = 0.2968
Meta loss on this task batch = 2.6973e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 164.2268, GNorm = 0.2676
Meta loss on this task batch = 1.9966e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 164.2319, GNorm = 0.2195
Meta loss on this task batch = 2.5644e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 164.2372, GNorm = 0.2881
Meta loss on this task batch = 2.7433e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 164.2432, GNorm = 0.4221
Meta loss on this task batch = 2.7652e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 164.2495, GNorm = 0.4408
Meta loss on this task batch = 3.0758e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 164.2553, GNorm = 0.2972
Meta loss on this task batch = 2.8414e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 164.2608, GNorm = 0.2963
Meta loss on this task batch = 2.4665e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 164.2665, GNorm = 0.2458
Meta loss on this task batch = 3.0932e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 164.2715, GNorm = 0.3364
Took 115.5712149143219 seconds to complete one epoch of meta training
Took 124.54425406455994 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470856
Epoch 854
Meta loss on this task batch = 2.8781e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 164.2764, GNorm = 0.6478
Meta loss on this task batch = 2.4333e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 164.2813, GNorm = 0.2565
Meta loss on this task batch = 2.7920e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 164.2853, GNorm = 0.4084
Meta loss on this task batch = 2.9780e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 164.2892, GNorm = 0.3036
Meta loss on this task batch = 2.9719e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 164.2922, GNorm = 0.3495
Meta loss on this task batch = 2.9814e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 164.2948, GNorm = 0.2995
Meta loss on this task batch = 2.1534e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 164.2976, GNorm = 0.4785
Meta loss on this task batch = 2.7653e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 164.2999, GNorm = 0.4038
Meta loss on this task batch = 2.1011e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 164.3023, GNorm = 0.3546
Meta loss on this task batch = 3.0486e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 164.3044, GNorm = 0.2592
Meta loss on this task batch = 2.5285e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 164.3072, GNorm = 0.2724
Meta loss on this task batch = 2.7812e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 164.3105, GNorm = 0.3033
Meta loss on this task batch = 2.3511e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 164.3133, GNorm = 0.2269
Meta loss on this task batch = 2.5993e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 164.3163, GNorm = 0.2385
Meta loss on this task batch = 3.2101e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 164.3189, GNorm = 0.4056
Meta loss on this task batch = 2.9602e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 164.3215, GNorm = 0.3226
Meta loss on this task batch = 2.8601e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 164.3237, GNorm = 0.3366
Meta loss on this task batch = 2.5011e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 164.3262, GNorm = 0.2988
Meta loss on this task batch = 3.6834e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 164.3277, GNorm = 0.6833
Took 121.08791494369507 seconds to complete one epoch of meta training
Took 129.011127948761 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470001
Epoch 855
Meta loss on this task batch = 2.8038e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 164.3298, GNorm = 0.2490
Meta loss on this task batch = 2.8537e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 164.3323, GNorm = 0.3986
Meta loss on this task batch = 2.8195e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 164.3348, GNorm = 0.3570
Meta loss on this task batch = 2.5320e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 164.3381, GNorm = 0.2413
Meta loss on this task batch = 2.4841e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 164.3419, GNorm = 0.2982
Meta loss on this task batch = 2.8141e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 164.3460, GNorm = 0.2642
Meta loss on this task batch = 2.5905e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 164.3507, GNorm = 0.2810
Meta loss on this task batch = 2.7659e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 164.3558, GNorm = 0.2838
Meta loss on this task batch = 2.3192e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 164.3622, GNorm = 0.2461
Meta loss on this task batch = 2.3982e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 164.3688, GNorm = 0.2681
Meta loss on this task batch = 2.4702e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 164.3756, GNorm = 0.2514
Meta loss on this task batch = 3.1291e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 164.3819, GNorm = 0.3145
Meta loss on this task batch = 2.7953e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 164.3880, GNorm = 0.2977
Meta loss on this task batch = 2.6498e-01, Meta loss averaged over last 500 steps = 2.7599e-01, PNorm = 164.3947, GNorm = 0.3574
Meta loss on this task batch = 2.7753e-01, Meta loss averaged over last 500 steps = 2.7596e-01, PNorm = 164.3999, GNorm = 0.3077
Meta loss on this task batch = 3.0125e-01, Meta loss averaged over last 500 steps = 2.7596e-01, PNorm = 164.4051, GNorm = 0.4729
Meta loss on this task batch = 3.1753e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 164.4100, GNorm = 0.2828
Meta loss on this task batch = 2.9571e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 164.4142, GNorm = 0.3110
Meta loss on this task batch = 3.1950e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 164.4183, GNorm = 0.3809
Took 121.21872425079346 seconds to complete one epoch of meta training
Took 131.09461903572083 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473582
Epoch 856
Meta loss on this task batch = 2.8434e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 164.4223, GNorm = 0.3829
Meta loss on this task batch = 2.6905e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 164.4264, GNorm = 0.2962
Meta loss on this task batch = 2.5888e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 164.4301, GNorm = 0.3384
Meta loss on this task batch = 2.7618e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 164.4335, GNorm = 0.3633
Meta loss on this task batch = 2.5125e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 164.4364, GNorm = 0.2610
Meta loss on this task batch = 3.1351e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 164.4388, GNorm = 0.3210
Meta loss on this task batch = 3.1103e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 164.4415, GNorm = 0.3166
Meta loss on this task batch = 2.8072e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 164.4438, GNorm = 0.3187
Meta loss on this task batch = 2.8065e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 164.4466, GNorm = 0.2632
Meta loss on this task batch = 2.1397e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 164.4498, GNorm = 0.2394
Meta loss on this task batch = 2.4772e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 164.4536, GNorm = 0.2471
Meta loss on this task batch = 2.7781e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 164.4568, GNorm = 0.3075
Meta loss on this task batch = 2.9808e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 164.4607, GNorm = 0.2979
Meta loss on this task batch = 3.2489e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 164.4623, GNorm = 0.5925
Meta loss on this task batch = 3.4297e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 164.4639, GNorm = 0.2741
Meta loss on this task batch = 2.4110e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 164.4663, GNorm = 0.2422
Meta loss on this task batch = 2.1930e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 164.4695, GNorm = 0.2380
Meta loss on this task batch = 2.5220e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 164.4732, GNorm = 0.3072
Meta loss on this task batch = 3.3106e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 164.4765, GNorm = 0.3394
Took 124.36148238182068 seconds to complete one epoch of meta training
Took 132.49537181854248 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469746
Epoch 857
Meta loss on this task batch = 2.8637e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 164.4792, GNorm = 0.3533
Meta loss on this task batch = 2.5377e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 164.4824, GNorm = 0.2511
Meta loss on this task batch = 2.9998e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 164.4854, GNorm = 0.2696
Meta loss on this task batch = 2.3114e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 164.4886, GNorm = 0.2661
Meta loss on this task batch = 2.3028e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 164.4925, GNorm = 0.2793
Meta loss on this task batch = 2.8854e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 164.4964, GNorm = 0.2579
Meta loss on this task batch = 2.7700e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 164.5001, GNorm = 0.3239
Meta loss on this task batch = 3.4229e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 164.5032, GNorm = 0.3635
Meta loss on this task batch = 2.2516e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 164.5067, GNorm = 0.2164
Meta loss on this task batch = 2.6107e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 164.5104, GNorm = 0.4205
Meta loss on this task batch = 2.7786e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 164.5140, GNorm = 0.3209
Meta loss on this task batch = 2.3964e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 164.5177, GNorm = 0.2866
Meta loss on this task batch = 2.6153e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 164.5215, GNorm = 0.2459
Meta loss on this task batch = 2.4093e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 164.5252, GNorm = 0.3099
Meta loss on this task batch = 3.1103e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 164.5283, GNorm = 0.3605
Meta loss on this task batch = 2.7497e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 164.5318, GNorm = 0.2969
Meta loss on this task batch = 3.3615e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 164.5353, GNorm = 0.2771
Meta loss on this task batch = 2.7438e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 164.5392, GNorm = 0.2623
Meta loss on this task batch = 2.8851e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 164.5437, GNorm = 0.3154
Took 118.54540133476257 seconds to complete one epoch of meta training
Took 127.56564140319824 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461754
Epoch 858
Meta loss on this task batch = 3.0989e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 164.5477, GNorm = 0.2909
Meta loss on this task batch = 2.3902e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 164.5517, GNorm = 0.2649
Meta loss on this task batch = 3.2218e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 164.5553, GNorm = 0.3909
Meta loss on this task batch = 2.5766e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 164.5587, GNorm = 0.2875
Meta loss on this task batch = 2.6347e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 164.5626, GNorm = 0.2632
Meta loss on this task batch = 3.0457e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 164.5660, GNorm = 0.2997
Meta loss on this task batch = 2.8865e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 164.5696, GNorm = 0.2690
Meta loss on this task batch = 2.8560e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 164.5738, GNorm = 0.3081
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 164.5786, GNorm = 0.3074
Meta loss on this task batch = 3.1548e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 164.5835, GNorm = 0.2568
Meta loss on this task batch = 2.4309e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 164.5885, GNorm = 0.2237
Meta loss on this task batch = 2.5148e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 164.5937, GNorm = 0.2807
Meta loss on this task batch = 2.6597e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 164.5992, GNorm = 0.3955
Meta loss on this task batch = 2.7316e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 164.6047, GNorm = 0.4017
Meta loss on this task batch = 2.7258e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 164.6103, GNorm = 0.4199
Meta loss on this task batch = 2.5429e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 164.6161, GNorm = 0.3010
Meta loss on this task batch = 2.4467e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 164.6221, GNorm = 0.2922
Meta loss on this task batch = 2.6467e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 164.6281, GNorm = 0.2912
Meta loss on this task batch = 3.1418e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 164.6333, GNorm = 0.3993
Took 124.67941665649414 seconds to complete one epoch of meta training
Took 133.1622986793518 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464972
Epoch 859
Meta loss on this task batch = 2.1255e-01, Meta loss averaged over last 500 steps = 2.7609e-01, PNorm = 164.6386, GNorm = 0.2772
Meta loss on this task batch = 2.9136e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 164.6433, GNorm = 0.3289
Meta loss on this task batch = 3.1196e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 164.6472, GNorm = 0.3172
Meta loss on this task batch = 2.9290e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 164.6511, GNorm = 0.2895
Meta loss on this task batch = 2.3260e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 164.6554, GNorm = 0.3159
Meta loss on this task batch = 2.5517e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 164.6600, GNorm = 0.2797
Meta loss on this task batch = 3.0149e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 164.6645, GNorm = 0.3412
Meta loss on this task batch = 2.8491e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 164.6681, GNorm = 0.3333
Meta loss on this task batch = 2.6159e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 164.6717, GNorm = 0.4922
Meta loss on this task batch = 3.2572e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 164.6745, GNorm = 0.2982
Meta loss on this task batch = 3.2464e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 164.6772, GNorm = 0.3548
Meta loss on this task batch = 2.9427e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 164.6799, GNorm = 0.3633
Meta loss on this task batch = 2.1702e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 164.6831, GNorm = 0.2768
Meta loss on this task batch = 3.0465e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 164.6861, GNorm = 0.3105
Meta loss on this task batch = 2.9744e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 164.6893, GNorm = 0.3267
Meta loss on this task batch = 2.9992e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 164.6930, GNorm = 0.4005
Meta loss on this task batch = 2.9452e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 164.6975, GNorm = 0.3032
Meta loss on this task batch = 3.1149e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 164.7017, GNorm = 0.2976
Meta loss on this task batch = 3.5446e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 164.7053, GNorm = 0.3983
Took 120.58420467376709 seconds to complete one epoch of meta training
Took 129.70164513587952 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486493
Epoch 860
Meta loss on this task batch = 2.5042e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 164.7088, GNorm = 0.3073
Meta loss on this task batch = 2.6398e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 164.7129, GNorm = 0.2760
Meta loss on this task batch = 2.8682e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 164.7164, GNorm = 0.3538
Meta loss on this task batch = 2.7650e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 164.7202, GNorm = 0.2762
Meta loss on this task batch = 2.2194e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 164.7242, GNorm = 0.2611
Meta loss on this task batch = 2.6027e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 164.7285, GNorm = 0.2615
Meta loss on this task batch = 2.8001e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 164.7333, GNorm = 0.2948
Meta loss on this task batch = 3.1465e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 164.7378, GNorm = 0.3487
Meta loss on this task batch = 2.7361e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 164.7422, GNorm = 0.3107
Meta loss on this task batch = 2.5855e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 164.7464, GNorm = 0.2600
Meta loss on this task batch = 3.2266e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 164.7503, GNorm = 0.3566
Meta loss on this task batch = 3.3287e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 164.7547, GNorm = 0.2835
Meta loss on this task batch = 2.2630e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 164.7589, GNorm = 0.3397
Meta loss on this task batch = 2.9787e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 164.7632, GNorm = 0.2812
Meta loss on this task batch = 2.0559e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 164.7683, GNorm = 0.2841
Meta loss on this task batch = 3.2457e-01, Meta loss averaged over last 500 steps = 2.7680e-01, PNorm = 164.7723, GNorm = 0.3213
Meta loss on this task batch = 3.1813e-01, Meta loss averaged over last 500 steps = 2.7692e-01, PNorm = 164.7760, GNorm = 0.3054
Meta loss on this task batch = 2.3942e-01, Meta loss averaged over last 500 steps = 2.7683e-01, PNorm = 164.7803, GNorm = 0.2615
Meta loss on this task batch = 2.3247e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 164.7848, GNorm = 0.3219
Took 127.59863495826721 seconds to complete one epoch of meta training
Took 135.60234093666077 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471743
Epoch 861
Meta loss on this task batch = 3.0424e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 164.7892, GNorm = 0.3198
Meta loss on this task batch = 2.8923e-01, Meta loss averaged over last 500 steps = 2.7698e-01, PNorm = 164.7944, GNorm = 0.3283
Meta loss on this task batch = 2.9568e-01, Meta loss averaged over last 500 steps = 2.7700e-01, PNorm = 164.7992, GNorm = 0.3561
Meta loss on this task batch = 2.6675e-01, Meta loss averaged over last 500 steps = 2.7680e-01, PNorm = 164.8041, GNorm = 0.3225
Meta loss on this task batch = 2.5708e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 164.8091, GNorm = 0.2701
Meta loss on this task batch = 2.8604e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 164.8137, GNorm = 0.3439
Meta loss on this task batch = 2.8601e-01, Meta loss averaged over last 500 steps = 2.7683e-01, PNorm = 164.8177, GNorm = 0.3627
Meta loss on this task batch = 2.6545e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 164.8217, GNorm = 0.3909
Meta loss on this task batch = 2.4133e-01, Meta loss averaged over last 500 steps = 2.7666e-01, PNorm = 164.8260, GNorm = 0.2437
Meta loss on this task batch = 2.6456e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 164.8295, GNorm = 0.5034
Meta loss on this task batch = 2.4766e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 164.8335, GNorm = 0.2908
Meta loss on this task batch = 2.7257e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 164.8377, GNorm = 0.2678
Meta loss on this task batch = 2.6158e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 164.8413, GNorm = 0.2873
Meta loss on this task batch = 3.1100e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 164.8451, GNorm = 0.3181
Meta loss on this task batch = 3.1312e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 164.8488, GNorm = 0.3025
Meta loss on this task batch = 2.7744e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 164.8524, GNorm = 0.2610
Meta loss on this task batch = 2.7290e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 164.8556, GNorm = 0.4226
Meta loss on this task batch = 2.4217e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 164.8586, GNorm = 0.2199
Meta loss on this task batch = 2.0826e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 164.8624, GNorm = 0.2714
Took 188.87335395812988 seconds to complete one epoch of meta training
Took 196.94999504089355 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486302
Epoch 862
Meta loss on this task batch = 2.9498e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 164.8660, GNorm = 0.2627
Meta loss on this task batch = 3.0466e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 164.8703, GNorm = 0.5702
Meta loss on this task batch = 3.2286e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 164.8739, GNorm = 0.3136
Meta loss on this task batch = 2.7787e-01, Meta loss averaged over last 500 steps = 2.7662e-01, PNorm = 164.8774, GNorm = 0.4146
Meta loss on this task batch = 2.8180e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 164.8809, GNorm = 0.3096
Meta loss on this task batch = 2.6717e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 164.8853, GNorm = 0.2784
Meta loss on this task batch = 2.5777e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 164.8902, GNorm = 0.2977
Meta loss on this task batch = 2.8866e-01, Meta loss averaged over last 500 steps = 2.7685e-01, PNorm = 164.8944, GNorm = 0.3102
Meta loss on this task batch = 2.4649e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 164.8981, GNorm = 0.2606
Meta loss on this task batch = 2.8144e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 164.9017, GNorm = 0.3077
Meta loss on this task batch = 2.2807e-01, Meta loss averaged over last 500 steps = 2.7666e-01, PNorm = 164.9051, GNorm = 0.4450
Meta loss on this task batch = 2.4718e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 164.9088, GNorm = 0.2674
Meta loss on this task batch = 2.3398e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 164.9129, GNorm = 0.2564
Meta loss on this task batch = 2.6020e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 164.9167, GNorm = 0.4611
Meta loss on this task batch = 2.6820e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 164.9200, GNorm = 0.4292
Meta loss on this task batch = 2.8380e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 164.9234, GNorm = 0.2917
Meta loss on this task batch = 3.0950e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 164.9270, GNorm = 0.4033
Meta loss on this task batch = 3.2556e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 164.9302, GNorm = 0.2662
Meta loss on this task batch = 2.8865e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 164.9335, GNorm = 0.5674
Took 305.82301330566406 seconds to complete one epoch of meta training
Took 313.80841732025146 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501463
Epoch 863
Meta loss on this task batch = 2.2330e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 164.9373, GNorm = 0.2371
Meta loss on this task batch = 3.6810e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 164.9399, GNorm = 0.4802
Meta loss on this task batch = 2.6099e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 164.9424, GNorm = 0.3063
Meta loss on this task batch = 2.2240e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 164.9455, GNorm = 0.2429
Meta loss on this task batch = 2.8593e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 164.9484, GNorm = 0.3812
Meta loss on this task batch = 2.2496e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 164.9511, GNorm = 0.2666
Meta loss on this task batch = 2.5922e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 164.9540, GNorm = 0.3381
Meta loss on this task batch = 2.4360e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 164.9574, GNorm = 0.3070
Meta loss on this task batch = 2.6816e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 164.9605, GNorm = 0.3311
Meta loss on this task batch = 2.9857e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 164.9636, GNorm = 0.5004
Meta loss on this task batch = 2.7483e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 164.9668, GNorm = 0.4160
Meta loss on this task batch = 3.0155e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 164.9705, GNorm = 0.5282
Meta loss on this task batch = 2.5393e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 164.9739, GNorm = 0.2610
Meta loss on this task batch = 2.8097e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 164.9777, GNorm = 0.3734
Meta loss on this task batch = 2.5763e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 164.9822, GNorm = 0.4292
Meta loss on this task batch = 2.6070e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 164.9871, GNorm = 0.2695
Meta loss on this task batch = 3.3656e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 164.9918, GNorm = 0.3020
Meta loss on this task batch = 2.6049e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 164.9969, GNorm = 0.3509
Meta loss on this task batch = 3.1811e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 165.0010, GNorm = 0.3325
Took 168.05638360977173 seconds to complete one epoch of meta training
Took 176.12075448036194 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460735
Epoch 864
Meta loss on this task batch = 2.7164e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 165.0059, GNorm = 0.2757
Meta loss on this task batch = 2.7322e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 165.0103, GNorm = 0.2640
Meta loss on this task batch = 2.1430e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 165.0150, GNorm = 0.2132
Meta loss on this task batch = 3.0952e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 165.0198, GNorm = 0.3270
Meta loss on this task batch = 2.6370e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 165.0240, GNorm = 0.2867
Meta loss on this task batch = 2.8993e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 165.0277, GNorm = 0.3566
Meta loss on this task batch = 2.9050e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 165.0322, GNorm = 0.3797
Meta loss on this task batch = 2.9476e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 165.0362, GNorm = 0.2599
Meta loss on this task batch = 2.7346e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 165.0402, GNorm = 0.3062
Meta loss on this task batch = 2.7486e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 165.0445, GNorm = 0.5087
Meta loss on this task batch = 2.7405e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 165.0487, GNorm = 0.5311
Meta loss on this task batch = 2.3260e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 165.0535, GNorm = 0.3036
Meta loss on this task batch = 3.3722e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 165.0581, GNorm = 0.3324
Meta loss on this task batch = 2.2420e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 165.0629, GNorm = 0.2480
Meta loss on this task batch = 3.1708e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 165.0672, GNorm = 0.3166
Meta loss on this task batch = 3.2896e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 165.0714, GNorm = 0.2705
Meta loss on this task batch = 2.5745e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 165.0757, GNorm = 0.3696
Meta loss on this task batch = 3.2116e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 165.0798, GNorm = 0.3016
Meta loss on this task batch = 3.1853e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 165.0836, GNorm = 0.4752
Took 124.42369270324707 seconds to complete one epoch of meta training
Took 133.60789465904236 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459602
Epoch 865
Meta loss on this task batch = 2.9071e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 165.0873, GNorm = 0.2682
Meta loss on this task batch = 2.6876e-01, Meta loss averaged over last 500 steps = 2.7683e-01, PNorm = 165.0907, GNorm = 0.3566
Meta loss on this task batch = 2.0455e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 165.0945, GNorm = 0.2735
Meta loss on this task batch = 3.1762e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 165.0993, GNorm = 0.3886
Meta loss on this task batch = 2.4650e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 165.1039, GNorm = 0.4003
Meta loss on this task batch = 2.6936e-01, Meta loss averaged over last 500 steps = 2.7666e-01, PNorm = 165.1084, GNorm = 0.2608
Meta loss on this task batch = 2.5153e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 165.1132, GNorm = 0.3080
Meta loss on this task batch = 2.6921e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 165.1183, GNorm = 0.3305
Meta loss on this task batch = 3.1699e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 165.1238, GNorm = 0.3614
Meta loss on this task batch = 2.9405e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 165.1292, GNorm = 0.3838
Meta loss on this task batch = 3.0001e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 165.1345, GNorm = 0.3390
Meta loss on this task batch = 2.9622e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 165.1398, GNorm = 0.2065
Meta loss on this task batch = 3.0820e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 165.1451, GNorm = 0.2870
Meta loss on this task batch = 2.9627e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 165.1493, GNorm = 0.2922
Meta loss on this task batch = 2.6992e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 165.1537, GNorm = 0.2437
Meta loss on this task batch = 3.2757e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 165.1578, GNorm = 0.2640
Meta loss on this task batch = 3.1354e-01, Meta loss averaged over last 500 steps = 2.7662e-01, PNorm = 165.1615, GNorm = 0.2517
Meta loss on this task batch = 2.4886e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 165.1648, GNorm = 0.2477
Meta loss on this task batch = 2.2817e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 165.1683, GNorm = 0.3253
Took 121.85351085662842 seconds to complete one epoch of meta training
Took 130.03954339027405 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.439131
Epoch 866
Meta loss on this task batch = 2.2485e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 165.1717, GNorm = 0.2858
Meta loss on this task batch = 2.5066e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 165.1746, GNorm = 0.2768
Meta loss on this task batch = 2.6449e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 165.1775, GNorm = 0.3857
Meta loss on this task batch = 3.1503e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 165.1800, GNorm = 0.4113
Meta loss on this task batch = 2.5410e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 165.1825, GNorm = 0.3016
Meta loss on this task batch = 2.6387e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 165.1861, GNorm = 0.3436
Meta loss on this task batch = 2.3977e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 165.1899, GNorm = 0.2695
Meta loss on this task batch = 2.7267e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 165.1938, GNorm = 0.3868
Meta loss on this task batch = 2.9195e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 165.1977, GNorm = 0.3304
Meta loss on this task batch = 3.1519e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 165.2018, GNorm = 0.4509
Meta loss on this task batch = 2.6909e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 165.2056, GNorm = 0.2630
Meta loss on this task batch = 3.0426e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 165.2099, GNorm = 0.3012
Meta loss on this task batch = 2.6633e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 165.2142, GNorm = 0.2336
Meta loss on this task batch = 2.6001e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 165.2185, GNorm = 0.2539
Meta loss on this task batch = 3.0458e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 165.2228, GNorm = 0.3118
Meta loss on this task batch = 3.1941e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 165.2265, GNorm = 0.3337
Meta loss on this task batch = 3.0216e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 165.2293, GNorm = 0.3368
Meta loss on this task batch = 2.7360e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 165.2322, GNorm = 0.2730
Meta loss on this task batch = 1.9867e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 165.2358, GNorm = 0.3332
Took 120.92581915855408 seconds to complete one epoch of meta training
Took 129.4588167667389 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447727
Epoch 867
Meta loss on this task batch = 2.7896e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 165.2395, GNorm = 0.2595
Meta loss on this task batch = 2.4480e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 165.2431, GNorm = 0.3128
Meta loss on this task batch = 2.8812e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 165.2466, GNorm = 0.2391
Meta loss on this task batch = 2.5004e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 165.2500, GNorm = 0.2702
Meta loss on this task batch = 3.1636e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 165.2520, GNorm = 0.3328
Meta loss on this task batch = 2.6272e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 165.2549, GNorm = 0.2543
Meta loss on this task batch = 3.2639e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 165.2580, GNorm = 0.4484
Meta loss on this task batch = 2.9976e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 165.2608, GNorm = 0.2717
Meta loss on this task batch = 2.6844e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 165.2632, GNorm = 0.3771
Meta loss on this task batch = 3.3612e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 165.2656, GNorm = 0.3659
Meta loss on this task batch = 2.5190e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 165.2679, GNorm = 0.2559
Meta loss on this task batch = 3.1143e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 165.2698, GNorm = 0.4243
Meta loss on this task batch = 2.7328e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 165.2725, GNorm = 0.3050
Meta loss on this task batch = 3.0046e-01, Meta loss averaged over last 500 steps = 2.7692e-01, PNorm = 165.2758, GNorm = 0.2610
Meta loss on this task batch = 2.4285e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 165.2796, GNorm = 0.2828
Meta loss on this task batch = 1.9180e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 165.2836, GNorm = 0.3309
Meta loss on this task batch = 2.6269e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 165.2877, GNorm = 0.3542
Meta loss on this task batch = 2.6101e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 165.2922, GNorm = 0.3534
Meta loss on this task batch = 2.7442e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 165.2972, GNorm = 0.4888
Took 124.98927164077759 seconds to complete one epoch of meta training
Took 133.01914620399475 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470413
Epoch 868
Meta loss on this task batch = 2.5513e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 165.3026, GNorm = 0.2753
Meta loss on this task batch = 3.1570e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 165.3073, GNorm = 0.2745
Meta loss on this task batch = 2.8667e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 165.3117, GNorm = 0.2750
Meta loss on this task batch = 2.5610e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 165.3162, GNorm = 0.3288
Meta loss on this task batch = 2.5457e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 165.3210, GNorm = 0.2970
Meta loss on this task batch = 2.4377e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 165.3258, GNorm = 0.2735
Meta loss on this task batch = 2.4732e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 165.3308, GNorm = 0.2572
Meta loss on this task batch = 2.7518e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 165.3356, GNorm = 0.3698
Meta loss on this task batch = 3.3356e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 165.3395, GNorm = 0.3864
Meta loss on this task batch = 2.5083e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 165.3432, GNorm = 0.3126
Meta loss on this task batch = 3.3493e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 165.3463, GNorm = 0.3191
Meta loss on this task batch = 2.8424e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 165.3491, GNorm = 0.2838
Meta loss on this task batch = 3.0341e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 165.3513, GNorm = 0.3514
Meta loss on this task batch = 3.0692e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 165.3533, GNorm = 0.3563
Meta loss on this task batch = 2.7236e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 165.3549, GNorm = 0.4083
Meta loss on this task batch = 2.7365e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 165.3564, GNorm = 0.3314
Meta loss on this task batch = 2.3954e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 165.3580, GNorm = 0.2633
Meta loss on this task batch = 2.8559e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 165.3596, GNorm = 0.3043
Meta loss on this task batch = 2.6487e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 165.3618, GNorm = 0.3577
Took 119.57290148735046 seconds to complete one epoch of meta training
Took 128.1973865032196 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464907
Epoch 869
Meta loss on this task batch = 2.7782e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 165.3642, GNorm = 0.3939
Meta loss on this task batch = 2.5796e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 165.3674, GNorm = 0.3854
Meta loss on this task batch = 2.7575e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 165.3706, GNorm = 0.3159
Meta loss on this task batch = 2.5292e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 165.3749, GNorm = 0.3454
Meta loss on this task batch = 2.4513e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 165.3796, GNorm = 0.3583
Meta loss on this task batch = 2.5520e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 165.3841, GNorm = 0.2195
Meta loss on this task batch = 2.1258e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 165.3895, GNorm = 0.2446
Meta loss on this task batch = 2.5753e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 165.3952, GNorm = 0.2708
Meta loss on this task batch = 2.4306e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 165.4007, GNorm = 0.2358
Meta loss on this task batch = 3.0681e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 165.4062, GNorm = 0.4360
Meta loss on this task batch = 2.6182e-01, Meta loss averaged over last 500 steps = 2.7599e-01, PNorm = 165.4117, GNorm = 0.2705
Meta loss on this task batch = 3.1039e-01, Meta loss averaged over last 500 steps = 2.7600e-01, PNorm = 165.4169, GNorm = 0.3121
Meta loss on this task batch = 3.2624e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 165.4222, GNorm = 0.3268
Meta loss on this task batch = 2.8699e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 165.4269, GNorm = 0.4480
Meta loss on this task batch = 3.2070e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 165.4312, GNorm = 0.3410
Meta loss on this task batch = 2.6194e-01, Meta loss averaged over last 500 steps = 2.7609e-01, PNorm = 165.4356, GNorm = 0.2993
Meta loss on this task batch = 2.9004e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 165.4389, GNorm = 0.2868
Meta loss on this task batch = 2.7676e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 165.4422, GNorm = 0.2916
Meta loss on this task batch = 2.9977e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 165.4457, GNorm = 0.2984
Took 117.22800421714783 seconds to complete one epoch of meta training
Took 124.85104942321777 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464891
Epoch 870
Meta loss on this task batch = 2.5534e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 165.4497, GNorm = 0.3675
Meta loss on this task batch = 2.8356e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 165.4541, GNorm = 0.2728
Meta loss on this task batch = 2.7390e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 165.4580, GNorm = 0.4879
Meta loss on this task batch = 2.7878e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 165.4616, GNorm = 0.3983
Meta loss on this task batch = 2.4596e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 165.4654, GNorm = 0.3935
Meta loss on this task batch = 3.4255e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 165.4695, GNorm = 0.3513
Meta loss on this task batch = 1.9949e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 165.4743, GNorm = 0.2051
Meta loss on this task batch = 2.8642e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 165.4787, GNorm = 0.2759
Meta loss on this task batch = 3.2329e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 165.4832, GNorm = 0.2564
Meta loss on this task batch = 2.5914e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 165.4877, GNorm = 0.2701
Meta loss on this task batch = 2.4307e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 165.4923, GNorm = 0.4357
Meta loss on this task batch = 2.7914e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 165.4970, GNorm = 0.2384
Meta loss on this task batch = 2.8474e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 165.5023, GNorm = 0.3088
Meta loss on this task batch = 2.8593e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 165.5074, GNorm = 0.4413
Meta loss on this task batch = 1.9866e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 165.5126, GNorm = 0.2511
Meta loss on this task batch = 2.8214e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 165.5174, GNorm = 0.2769
Meta loss on this task batch = 2.8536e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 165.5219, GNorm = 0.3006
Meta loss on this task batch = 2.8836e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 165.5259, GNorm = 0.2553
Meta loss on this task batch = 3.0169e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 165.5295, GNorm = 0.3357
Took 115.61556506156921 seconds to complete one epoch of meta training
Took 123.46381425857544 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462739
Epoch 871
Meta loss on this task batch = 2.4297e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 165.5326, GNorm = 0.3690
Meta loss on this task batch = 2.6527e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 165.5351, GNorm = 0.4190
Meta loss on this task batch = 2.8559e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 165.5374, GNorm = 0.3124
Meta loss on this task batch = 2.8280e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 165.5396, GNorm = 0.2704
Meta loss on this task batch = 2.7601e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 165.5416, GNorm = 0.2729
Meta loss on this task batch = 2.7590e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 165.5438, GNorm = 0.3497
Meta loss on this task batch = 2.2870e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 165.5465, GNorm = 0.2551
Meta loss on this task batch = 2.7156e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 165.5491, GNorm = 0.2802
Meta loss on this task batch = 2.3814e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 165.5516, GNorm = 0.4953
Meta loss on this task batch = 3.0402e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 165.5548, GNorm = 0.2885
Meta loss on this task batch = 3.0852e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 165.5573, GNorm = 0.3816
Meta loss on this task batch = 2.7777e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 165.5603, GNorm = 0.2840
Meta loss on this task batch = 3.0795e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 165.5629, GNorm = 0.3568
Meta loss on this task batch = 2.4039e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 165.5660, GNorm = 0.2571
Meta loss on this task batch = 3.2886e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 165.5689, GNorm = 0.2940
Meta loss on this task batch = 3.1286e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 165.5721, GNorm = 0.2570
Meta loss on this task batch = 3.0411e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 165.5751, GNorm = 0.2887
Meta loss on this task batch = 2.5979e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 165.5792, GNorm = 0.2714
Meta loss on this task batch = 3.0291e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 165.5838, GNorm = 0.3137
Took 118.94229531288147 seconds to complete one epoch of meta training
Took 127.11966395378113 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456177
Epoch 872
Meta loss on this task batch = 2.9295e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 165.5884, GNorm = 0.2711
Meta loss on this task batch = 2.8043e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 165.5926, GNorm = 0.4075
Meta loss on this task batch = 2.6350e-01, Meta loss averaged over last 500 steps = 2.7662e-01, PNorm = 165.5974, GNorm = 0.2962
Meta loss on this task batch = 2.3116e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 165.6025, GNorm = 0.3175
Meta loss on this task batch = 2.4357e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 165.6081, GNorm = 0.2498
Meta loss on this task batch = 2.8486e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 165.6134, GNorm = 0.2892
Meta loss on this task batch = 3.0896e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 165.6185, GNorm = 0.3217
Meta loss on this task batch = 2.8054e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 165.6230, GNorm = 0.2787
Meta loss on this task batch = 2.4857e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 165.6268, GNorm = 0.2851
Meta loss on this task batch = 2.8570e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 165.6320, GNorm = 0.3166
Meta loss on this task batch = 3.2659e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 165.6368, GNorm = 0.3672
Meta loss on this task batch = 2.8290e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 165.6410, GNorm = 0.3323
Meta loss on this task batch = 2.4266e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 165.6451, GNorm = 0.2426
Meta loss on this task batch = 2.5198e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 165.6496, GNorm = 0.2998
Meta loss on this task batch = 2.8954e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 165.6545, GNorm = 0.2984
Meta loss on this task batch = 2.9689e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 165.6586, GNorm = 0.3387
Meta loss on this task batch = 3.0043e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 165.6622, GNorm = 0.2915
Meta loss on this task batch = 2.2057e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 165.6663, GNorm = 0.2965
Meta loss on this task batch = 2.7694e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 165.6694, GNorm = 0.4132
Took 117.29182267189026 seconds to complete one epoch of meta training
Took 125.78191208839417 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.431204
Epoch 873
Meta loss on this task batch = 2.6345e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 165.6728, GNorm = 0.2936
Meta loss on this task batch = 2.6059e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 165.6761, GNorm = 0.3348
Meta loss on this task batch = 3.0440e-01, Meta loss averaged over last 500 steps = 2.7614e-01, PNorm = 165.6794, GNorm = 0.3319
Meta loss on this task batch = 2.7695e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 165.6826, GNorm = 0.3578
Meta loss on this task batch = 3.1287e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 165.6858, GNorm = 0.2632
Meta loss on this task batch = 2.6058e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 165.6895, GNorm = 0.3113
Meta loss on this task batch = 2.7781e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 165.6937, GNorm = 0.2783
Meta loss on this task batch = 2.5673e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 165.6976, GNorm = 0.3131
Meta loss on this task batch = 2.4941e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 165.7020, GNorm = 0.2871
Meta loss on this task batch = 2.7065e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 165.7065, GNorm = 0.2526
Meta loss on this task batch = 2.9469e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 165.7111, GNorm = 0.2971
Meta loss on this task batch = 2.6121e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 165.7162, GNorm = 0.3834
Meta loss on this task batch = 2.4034e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 165.7218, GNorm = 0.3989
Meta loss on this task batch = 2.6095e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 165.7276, GNorm = 0.2475
Meta loss on this task batch = 2.7093e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 165.7333, GNorm = 0.3957
Meta loss on this task batch = 3.0394e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 165.7386, GNorm = 0.3249
Meta loss on this task batch = 2.3880e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 165.7442, GNorm = 0.2325
Meta loss on this task batch = 3.1276e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 165.7496, GNorm = 0.2985
Meta loss on this task batch = 2.9197e-01, Meta loss averaged over last 500 steps = 2.7614e-01, PNorm = 165.7554, GNorm = 0.3310
Took 117.00227665901184 seconds to complete one epoch of meta training
Took 125.58574056625366 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470508
Epoch 874
Meta loss on this task batch = 2.5390e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 165.7610, GNorm = 0.2510
Meta loss on this task batch = 3.2489e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 165.7653, GNorm = 0.4244
Meta loss on this task batch = 2.5652e-01, Meta loss averaged over last 500 steps = 2.7614e-01, PNorm = 165.7693, GNorm = 0.2599
Meta loss on this task batch = 2.9007e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 165.7728, GNorm = 0.2419
Meta loss on this task batch = 3.0337e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 165.7764, GNorm = 0.3826
Meta loss on this task batch = 2.7411e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 165.7795, GNorm = 0.3197
Meta loss on this task batch = 2.7160e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 165.7826, GNorm = 0.2716
Meta loss on this task batch = 2.7189e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 165.7860, GNorm = 0.3444
Meta loss on this task batch = 2.6176e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 165.7892, GNorm = 0.2992
Meta loss on this task batch = 2.5849e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 165.7931, GNorm = 0.2931
Meta loss on this task batch = 2.7245e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 165.7972, GNorm = 0.3603
Meta loss on this task batch = 2.7501e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 165.8011, GNorm = 0.2224
Meta loss on this task batch = 2.3314e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 165.8055, GNorm = 0.2493
Meta loss on this task batch = 3.0330e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 165.8096, GNorm = 0.3529
Meta loss on this task batch = 3.0107e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 165.8133, GNorm = 0.2724
Meta loss on this task batch = 2.0153e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 165.8173, GNorm = 0.3077
Meta loss on this task batch = 2.7799e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 165.8208, GNorm = 0.2556
Meta loss on this task batch = 2.6880e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 165.8241, GNorm = 0.2731
Meta loss on this task batch = 3.1571e-01, Meta loss averaged over last 500 steps = 2.7612e-01, PNorm = 165.8269, GNorm = 0.4587
Took 120.7050895690918 seconds to complete one epoch of meta training
Took 130.0165765285492 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.442928
Epoch 875
Meta loss on this task batch = 2.4299e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 165.8297, GNorm = 0.2229
Meta loss on this task batch = 2.6040e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 165.8328, GNorm = 0.2310
Meta loss on this task batch = 2.4698e-01, Meta loss averaged over last 500 steps = 2.7600e-01, PNorm = 165.8359, GNorm = 0.2454
Meta loss on this task batch = 2.2995e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 165.8393, GNorm = 0.2483
Meta loss on this task batch = 2.8265e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 165.8428, GNorm = 0.2803
Meta loss on this task batch = 2.8245e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 165.8463, GNorm = 0.4106
Meta loss on this task batch = 2.9885e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 165.8498, GNorm = 0.3157
Meta loss on this task batch = 2.5901e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 165.8540, GNorm = 0.2495
Meta loss on this task batch = 3.1088e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 165.8572, GNorm = 0.3068
Meta loss on this task batch = 2.6612e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 165.8603, GNorm = 0.3678
Meta loss on this task batch = 2.5351e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 165.8641, GNorm = 0.2956
Meta loss on this task batch = 3.0302e-01, Meta loss averaged over last 500 steps = 2.7599e-01, PNorm = 165.8683, GNorm = 0.3002
Meta loss on this task batch = 3.0056e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 165.8717, GNorm = 0.3044
Meta loss on this task batch = 2.5081e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 165.8751, GNorm = 0.2799
Meta loss on this task batch = 3.0984e-01, Meta loss averaged over last 500 steps = 2.7600e-01, PNorm = 165.8788, GNorm = 0.3385
Meta loss on this task batch = 2.6902e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 165.8826, GNorm = 0.3660
Meta loss on this task batch = 3.0824e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 165.8866, GNorm = 0.3879
Meta loss on this task batch = 3.1113e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 165.8909, GNorm = 0.2861
Meta loss on this task batch = 2.2543e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 165.8957, GNorm = 0.3221
Took 122.67077231407166 seconds to complete one epoch of meta training
Took 130.96370458602905 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451337
Epoch 876
Meta loss on this task batch = 2.4357e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 165.9009, GNorm = 0.2520
Meta loss on this task batch = 2.6550e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 165.9065, GNorm = 0.3121
Meta loss on this task batch = 2.2944e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 165.9116, GNorm = 0.1985
Meta loss on this task batch = 2.9381e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 165.9163, GNorm = 0.3026
Meta loss on this task batch = 3.0513e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 165.9209, GNorm = 0.2803
Meta loss on this task batch = 2.9769e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 165.9253, GNorm = 0.2639
Meta loss on this task batch = 2.7308e-01, Meta loss averaged over last 500 steps = 2.7609e-01, PNorm = 165.9297, GNorm = 0.3366
Meta loss on this task batch = 2.8735e-01, Meta loss averaged over last 500 steps = 2.7614e-01, PNorm = 165.9340, GNorm = 0.3377
Meta loss on this task batch = 2.7327e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 165.9383, GNorm = 0.3147
Meta loss on this task batch = 3.0738e-01, Meta loss averaged over last 500 steps = 2.7596e-01, PNorm = 165.9420, GNorm = 0.4330
Meta loss on this task batch = 2.7226e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 165.9454, GNorm = 0.2994
Meta loss on this task batch = 2.6863e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 165.9493, GNorm = 0.2452
Meta loss on this task batch = 3.0823e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 165.9533, GNorm = 0.4254
Meta loss on this task batch = 3.1956e-01, Meta loss averaged over last 500 steps = 2.7612e-01, PNorm = 165.9578, GNorm = 0.2986
Meta loss on this task batch = 3.2065e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 165.9613, GNorm = 0.3029
Meta loss on this task batch = 3.0704e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 165.9655, GNorm = 0.3358
Meta loss on this task batch = 2.8057e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 165.9697, GNorm = 0.3205
Meta loss on this task batch = 2.2963e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 165.9741, GNorm = 0.2257
Meta loss on this task batch = 2.7463e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 165.9788, GNorm = 0.4112
Took 119.75473928451538 seconds to complete one epoch of meta training
Took 128.00375962257385 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470159
Epoch 877
Meta loss on this task batch = 3.2397e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 165.9833, GNorm = 0.3362
Meta loss on this task batch = 2.8101e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 165.9880, GNorm = 0.2789
Meta loss on this task batch = 3.1087e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 165.9928, GNorm = 0.2688
Meta loss on this task batch = 2.5047e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 165.9975, GNorm = 0.2528
Meta loss on this task batch = 2.7367e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 166.0013, GNorm = 0.3380
Meta loss on this task batch = 2.5081e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 166.0044, GNorm = 0.2849
Meta loss on this task batch = 2.4221e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 166.0079, GNorm = 0.2682
Meta loss on this task batch = 2.4298e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 166.0107, GNorm = 0.2665
Meta loss on this task batch = 2.0700e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 166.0146, GNorm = 0.2355
Meta loss on this task batch = 2.6592e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 166.0183, GNorm = 0.3092
Meta loss on this task batch = 2.7180e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 166.0223, GNorm = 0.2579
Meta loss on this task batch = 3.1871e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 166.0256, GNorm = 0.2993
Meta loss on this task batch = 2.8366e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 166.0282, GNorm = 0.2889
Meta loss on this task batch = 3.4694e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 166.0300, GNorm = 0.4966
Meta loss on this task batch = 3.0970e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 166.0320, GNorm = 0.3425
Meta loss on this task batch = 2.6832e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 166.0342, GNorm = 0.2282
Meta loss on this task batch = 3.2777e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 166.0373, GNorm = 0.3593
Meta loss on this task batch = 3.0288e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 166.0407, GNorm = 0.3882
Meta loss on this task batch = 2.4433e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 166.0448, GNorm = 0.3919
Took 120.09030413627625 seconds to complete one epoch of meta training
Took 127.98225712776184 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474667
Epoch 878
Meta loss on this task batch = 2.4844e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 166.0497, GNorm = 0.2468
Meta loss on this task batch = 2.4429e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 166.0546, GNorm = 0.2700
Meta loss on this task batch = 3.2713e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 166.0602, GNorm = 0.2723
Meta loss on this task batch = 2.9402e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 166.0661, GNorm = 0.3564
Meta loss on this task batch = 2.7871e-01, Meta loss averaged over last 500 steps = 2.7662e-01, PNorm = 166.0717, GNorm = 0.2960
Meta loss on this task batch = 2.9583e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 166.0762, GNorm = 0.3151
Meta loss on this task batch = 2.8750e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 166.0807, GNorm = 0.2779
Meta loss on this task batch = 2.9112e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 166.0850, GNorm = 0.2670
Meta loss on this task batch = 2.6478e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 166.0897, GNorm = 0.2763
Meta loss on this task batch = 2.7976e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 166.0951, GNorm = 0.2742
Meta loss on this task batch = 3.1579e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 166.0998, GNorm = 0.2747
Meta loss on this task batch = 2.4258e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 166.1046, GNorm = 0.3253
Meta loss on this task batch = 2.4680e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 166.1094, GNorm = 0.2867
Meta loss on this task batch = 2.4029e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 166.1146, GNorm = 0.2333
Meta loss on this task batch = 2.9339e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 166.1193, GNorm = 0.2594
Meta loss on this task batch = 2.5239e-01, Meta loss averaged over last 500 steps = 2.7666e-01, PNorm = 166.1233, GNorm = 0.3725
Meta loss on this task batch = 3.1349e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 166.1275, GNorm = 0.4644
Meta loss on this task batch = 2.3114e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 166.1325, GNorm = 0.2577
Meta loss on this task batch = 3.0416e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 166.1379, GNorm = 0.5932
Took 123.24212551116943 seconds to complete one epoch of meta training
Took 131.4501600265503 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465956
Epoch 879
Meta loss on this task batch = 2.8537e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 166.1435, GNorm = 0.3523
Meta loss on this task batch = 2.8748e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 166.1485, GNorm = 0.2683
Meta loss on this task batch = 2.8752e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 166.1538, GNorm = 0.2858
Meta loss on this task batch = 3.1726e-01, Meta loss averaged over last 500 steps = 2.7699e-01, PNorm = 166.1589, GNorm = 0.3711
Meta loss on this task batch = 2.5916e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 166.1644, GNorm = 0.2406
Meta loss on this task batch = 2.7809e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 166.1696, GNorm = 0.2952
Meta loss on this task batch = 2.5845e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 166.1750, GNorm = 0.2947
Meta loss on this task batch = 2.5849e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 166.1804, GNorm = 0.2988
Meta loss on this task batch = 2.7946e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 166.1859, GNorm = 0.3013
Meta loss on this task batch = 3.1349e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 166.1916, GNorm = 0.3609
Meta loss on this task batch = 2.5325e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 166.1974, GNorm = 0.3083
Meta loss on this task batch = 2.4993e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 166.2028, GNorm = 0.2699
Meta loss on this task batch = 2.4774e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 166.2079, GNorm = 0.2776
Meta loss on this task batch = 3.1418e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 166.2135, GNorm = 0.5206
Meta loss on this task batch = 2.0952e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 166.2193, GNorm = 0.2009
Meta loss on this task batch = 3.2044e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 166.2250, GNorm = 0.2814
Meta loss on this task batch = 2.7815e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 166.2307, GNorm = 0.2594
Meta loss on this task batch = 2.6325e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 166.2357, GNorm = 0.3894
Meta loss on this task batch = 2.3818e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 166.2408, GNorm = 0.2925
Took 121.61627793312073 seconds to complete one epoch of meta training
Took 130.65947771072388 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470791
Epoch 880
Meta loss on this task batch = 3.1238e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 166.2457, GNorm = 0.3601
Meta loss on this task batch = 2.6125e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 166.2504, GNorm = 0.2630
Meta loss on this task batch = 2.9061e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 166.2550, GNorm = 0.2446
Meta loss on this task batch = 2.7390e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 166.2593, GNorm = 0.2732
Meta loss on this task batch = 3.2933e-01, Meta loss averaged over last 500 steps = 2.7683e-01, PNorm = 166.2635, GNorm = 0.3207
Meta loss on this task batch = 2.8407e-01, Meta loss averaged over last 500 steps = 2.7678e-01, PNorm = 166.2675, GNorm = 0.2850
Meta loss on this task batch = 3.2157e-01, Meta loss averaged over last 500 steps = 2.7685e-01, PNorm = 166.2710, GNorm = 0.3952
Meta loss on this task batch = 2.5951e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 166.2751, GNorm = 0.3307
Meta loss on this task batch = 2.5942e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 166.2794, GNorm = 0.3010
Meta loss on this task batch = 3.2365e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 166.2834, GNorm = 0.2678
Meta loss on this task batch = 2.2294e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 166.2883, GNorm = 0.2226
Meta loss on this task batch = 2.6836e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 166.2934, GNorm = 0.2438
Meta loss on this task batch = 2.9009e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 166.2979, GNorm = 0.3457
Meta loss on this task batch = 2.5953e-01, Meta loss averaged over last 500 steps = 2.7680e-01, PNorm = 166.3027, GNorm = 0.2458
Meta loss on this task batch = 2.2915e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 166.3080, GNorm = 0.2265
Meta loss on this task batch = 2.7946e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 166.3131, GNorm = 0.3302
Meta loss on this task batch = 2.4923e-01, Meta loss averaged over last 500 steps = 2.7678e-01, PNorm = 166.3183, GNorm = 0.2684
Meta loss on this task batch = 3.2910e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 166.3235, GNorm = 0.5554
Meta loss on this task batch = 2.8938e-01, Meta loss averaged over last 500 steps = 2.7699e-01, PNorm = 166.3282, GNorm = 0.6213
Took 123.26271176338196 seconds to complete one epoch of meta training
Took 131.89963102340698 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485646
Epoch 881
Meta loss on this task batch = 3.1192e-01, Meta loss averaged over last 500 steps = 2.7710e-01, PNorm = 166.3325, GNorm = 0.3866
Meta loss on this task batch = 2.5449e-01, Meta loss averaged over last 500 steps = 2.7696e-01, PNorm = 166.3367, GNorm = 0.2776
Meta loss on this task batch = 2.7349e-01, Meta loss averaged over last 500 steps = 2.7692e-01, PNorm = 166.3402, GNorm = 0.3507
Meta loss on this task batch = 2.5310e-01, Meta loss averaged over last 500 steps = 2.7685e-01, PNorm = 166.3439, GNorm = 0.2605
Meta loss on this task batch = 3.1506e-01, Meta loss averaged over last 500 steps = 2.7698e-01, PNorm = 166.3472, GNorm = 0.3245
Meta loss on this task batch = 3.5901e-01, Meta loss averaged over last 500 steps = 2.7696e-01, PNorm = 166.3505, GNorm = 0.4476
Meta loss on this task batch = 3.2055e-01, Meta loss averaged over last 500 steps = 2.7704e-01, PNorm = 166.3538, GNorm = 0.2855
Meta loss on this task batch = 2.7082e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 166.3573, GNorm = 0.3538
Meta loss on this task batch = 2.4792e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 166.3612, GNorm = 0.3932
Meta loss on this task batch = 2.5494e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 166.3650, GNorm = 0.2494
Meta loss on this task batch = 2.3119e-01, Meta loss averaged over last 500 steps = 2.7692e-01, PNorm = 166.3692, GNorm = 0.2622
Meta loss on this task batch = 3.1510e-01, Meta loss averaged over last 500 steps = 2.7698e-01, PNorm = 166.3733, GNorm = 0.2997
Meta loss on this task batch = 2.3527e-01, Meta loss averaged over last 500 steps = 2.7693e-01, PNorm = 166.3778, GNorm = 0.2662
Meta loss on this task batch = 3.2134e-01, Meta loss averaged over last 500 steps = 2.7702e-01, PNorm = 166.3823, GNorm = 0.3433
Meta loss on this task batch = 2.2997e-01, Meta loss averaged over last 500 steps = 2.7702e-01, PNorm = 166.3868, GNorm = 0.2359
Meta loss on this task batch = 3.0419e-01, Meta loss averaged over last 500 steps = 2.7715e-01, PNorm = 166.3907, GNorm = 0.2884
Meta loss on this task batch = 2.9262e-01, Meta loss averaged over last 500 steps = 2.7724e-01, PNorm = 166.3943, GNorm = 0.3046
Meta loss on this task batch = 2.9316e-01, Meta loss averaged over last 500 steps = 2.7720e-01, PNorm = 166.3979, GNorm = 0.2469
Meta loss on this task batch = 2.9871e-01, Meta loss averaged over last 500 steps = 2.7724e-01, PNorm = 166.4017, GNorm = 0.3292
Took 119.58863568305969 seconds to complete one epoch of meta training
Took 127.30320763587952 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476847
Epoch 882
Meta loss on this task batch = 2.8388e-01, Meta loss averaged over last 500 steps = 2.7728e-01, PNorm = 166.4051, GNorm = 0.3025
Meta loss on this task batch = 2.5333e-01, Meta loss averaged over last 500 steps = 2.7723e-01, PNorm = 166.4089, GNorm = 0.2475
Meta loss on this task batch = 2.8055e-01, Meta loss averaged over last 500 steps = 2.7719e-01, PNorm = 166.4131, GNorm = 0.4499
Meta loss on this task batch = 2.2753e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 166.4170, GNorm = 0.2828
Meta loss on this task batch = 3.3797e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 166.4196, GNorm = 0.4532
Meta loss on this task batch = 2.7134e-01, Meta loss averaged over last 500 steps = 2.7700e-01, PNorm = 166.4219, GNorm = 0.2931
Meta loss on this task batch = 2.9008e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 166.4243, GNorm = 0.2714
Meta loss on this task batch = 2.5704e-01, Meta loss averaged over last 500 steps = 2.7698e-01, PNorm = 166.4272, GNorm = 0.2970
Meta loss on this task batch = 3.1069e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 166.4295, GNorm = 0.2943
Meta loss on this task batch = 2.2296e-01, Meta loss averaged over last 500 steps = 2.7698e-01, PNorm = 166.4324, GNorm = 0.2414
Meta loss on this task batch = 3.1643e-01, Meta loss averaged over last 500 steps = 2.7711e-01, PNorm = 166.4347, GNorm = 0.3487
Meta loss on this task batch = 2.4492e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 166.4376, GNorm = 0.2501
Meta loss on this task batch = 2.7870e-01, Meta loss averaged over last 500 steps = 2.7691e-01, PNorm = 166.4407, GNorm = 0.2597
Meta loss on this task batch = 3.1199e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 166.4434, GNorm = 0.4264
Meta loss on this task batch = 2.7992e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 166.4467, GNorm = 0.2499
Meta loss on this task batch = 2.8715e-01, Meta loss averaged over last 500 steps = 2.7712e-01, PNorm = 166.4498, GNorm = 0.3291
Meta loss on this task batch = 2.3669e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 166.4536, GNorm = 0.2637
Meta loss on this task batch = 2.5942e-01, Meta loss averaged over last 500 steps = 2.7706e-01, PNorm = 166.4579, GNorm = 0.2717
Meta loss on this task batch = 2.7821e-01, Meta loss averaged over last 500 steps = 2.7702e-01, PNorm = 166.4623, GNorm = 0.3372
Took 122.83524465560913 seconds to complete one epoch of meta training
Took 132.3170177936554 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477021
Epoch 883
Meta loss on this task batch = 3.2352e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 166.4672, GNorm = 0.3321
Meta loss on this task batch = 2.8486e-01, Meta loss averaged over last 500 steps = 2.7690e-01, PNorm = 166.4721, GNorm = 0.2567
Meta loss on this task batch = 2.7986e-01, Meta loss averaged over last 500 steps = 2.7698e-01, PNorm = 166.4770, GNorm = 0.3593
Meta loss on this task batch = 2.2360e-01, Meta loss averaged over last 500 steps = 2.7698e-01, PNorm = 166.4824, GNorm = 0.2810
Meta loss on this task batch = 2.7589e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 166.4882, GNorm = 0.2386
Meta loss on this task batch = 2.6647e-01, Meta loss averaged over last 500 steps = 2.7690e-01, PNorm = 166.4942, GNorm = 0.2917
Meta loss on this task batch = 2.8471e-01, Meta loss averaged over last 500 steps = 2.7690e-01, PNorm = 166.5000, GNorm = 0.3522
Meta loss on this task batch = 2.4913e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 166.5056, GNorm = 0.2920
Meta loss on this task batch = 3.1892e-01, Meta loss averaged over last 500 steps = 2.7693e-01, PNorm = 166.5105, GNorm = 0.3080
Meta loss on this task batch = 3.0863e-01, Meta loss averaged over last 500 steps = 2.7708e-01, PNorm = 166.5154, GNorm = 0.4080
Meta loss on this task batch = 2.7717e-01, Meta loss averaged over last 500 steps = 2.7718e-01, PNorm = 166.5190, GNorm = 0.3444
Meta loss on this task batch = 2.7514e-01, Meta loss averaged over last 500 steps = 2.7715e-01, PNorm = 166.5227, GNorm = 0.2396
Meta loss on this task batch = 2.8921e-01, Meta loss averaged over last 500 steps = 2.7717e-01, PNorm = 166.5264, GNorm = 0.3342
Meta loss on this task batch = 2.7089e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 166.5304, GNorm = 0.3027
Meta loss on this task batch = 2.9833e-01, Meta loss averaged over last 500 steps = 2.7718e-01, PNorm = 166.5340, GNorm = 0.3531
Meta loss on this task batch = 2.3900e-01, Meta loss averaged over last 500 steps = 2.7713e-01, PNorm = 166.5375, GNorm = 0.2762
Meta loss on this task batch = 2.4983e-01, Meta loss averaged over last 500 steps = 2.7708e-01, PNorm = 166.5413, GNorm = 0.2898
Meta loss on this task batch = 2.8519e-01, Meta loss averaged over last 500 steps = 2.7717e-01, PNorm = 166.5450, GNorm = 0.3261
Meta loss on this task batch = 2.9474e-01, Meta loss averaged over last 500 steps = 2.7724e-01, PNorm = 166.5486, GNorm = 0.4723
Took 122.35429000854492 seconds to complete one epoch of meta training
Took 131.85173845291138 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455947
Epoch 884
Meta loss on this task batch = 2.6660e-01, Meta loss averaged over last 500 steps = 2.7729e-01, PNorm = 166.5522, GNorm = 0.3343
Meta loss on this task batch = 3.0427e-01, Meta loss averaged over last 500 steps = 2.7727e-01, PNorm = 166.5553, GNorm = 0.2926
Meta loss on this task batch = 2.7401e-01, Meta loss averaged over last 500 steps = 2.7727e-01, PNorm = 166.5584, GNorm = 0.3134
Meta loss on this task batch = 2.7370e-01, Meta loss averaged over last 500 steps = 2.7715e-01, PNorm = 166.5619, GNorm = 0.3741
Meta loss on this task batch = 2.9379e-01, Meta loss averaged over last 500 steps = 2.7719e-01, PNorm = 166.5647, GNorm = 0.4155
Meta loss on this task batch = 2.4163e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 166.5679, GNorm = 0.2811
Meta loss on this task batch = 2.5882e-01, Meta loss averaged over last 500 steps = 2.7699e-01, PNorm = 166.5716, GNorm = 0.2295
Meta loss on this task batch = 2.0676e-01, Meta loss averaged over last 500 steps = 2.7692e-01, PNorm = 166.5753, GNorm = 0.2222
Meta loss on this task batch = 2.9702e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 166.5791, GNorm = 0.2644
Meta loss on this task batch = 2.8253e-01, Meta loss averaged over last 500 steps = 2.7692e-01, PNorm = 166.5830, GNorm = 0.2522
Meta loss on this task batch = 2.8538e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 166.5865, GNorm = 0.2514
Meta loss on this task batch = 2.7722e-01, Meta loss averaged over last 500 steps = 2.7691e-01, PNorm = 166.5898, GNorm = 0.3140
Meta loss on this task batch = 3.0461e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 166.5928, GNorm = 0.3167
Meta loss on this task batch = 2.8049e-01, Meta loss averaged over last 500 steps = 2.7693e-01, PNorm = 166.5962, GNorm = 0.3413
Meta loss on this task batch = 2.8699e-01, Meta loss averaged over last 500 steps = 2.7694e-01, PNorm = 166.5996, GNorm = 0.2519
Meta loss on this task batch = 2.0946e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 166.6031, GNorm = 0.3382
Meta loss on this task batch = 2.1762e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 166.6073, GNorm = 0.2435
Meta loss on this task batch = 2.9963e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 166.6108, GNorm = 0.4321
Meta loss on this task batch = 2.5011e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 166.6142, GNorm = 0.2856
Took 126.50805497169495 seconds to complete one epoch of meta training
Took 135.1405701637268 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468902
Epoch 885
Meta loss on this task batch = 3.5992e-01, Meta loss averaged over last 500 steps = 2.7691e-01, PNorm = 166.6169, GNorm = 0.4219
Meta loss on this task batch = 2.4768e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 166.6202, GNorm = 0.3378
Meta loss on this task batch = 2.2707e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 166.6229, GNorm = 0.2843
Meta loss on this task batch = 2.9907e-01, Meta loss averaged over last 500 steps = 2.7692e-01, PNorm = 166.6254, GNorm = 0.2582
Meta loss on this task batch = 2.4223e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 166.6284, GNorm = 0.2987
Meta loss on this task batch = 2.3082e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 166.6319, GNorm = 0.2616
Meta loss on this task batch = 2.7423e-01, Meta loss averaged over last 500 steps = 2.7683e-01, PNorm = 166.6353, GNorm = 0.2494
Meta loss on this task batch = 2.8363e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 166.6389, GNorm = 0.3006
Meta loss on this task batch = 2.8340e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 166.6428, GNorm = 0.3239
Meta loss on this task batch = 2.8932e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 166.6467, GNorm = 0.2998
Meta loss on this task batch = 2.3364e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 166.6512, GNorm = 0.3739
Meta loss on this task batch = 2.5631e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 166.6558, GNorm = 0.2713
Meta loss on this task batch = 2.4348e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 166.6608, GNorm = 0.2300
Meta loss on this task batch = 3.0766e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 166.6658, GNorm = 0.3107
Meta loss on this task batch = 3.1795e-01, Meta loss averaged over last 500 steps = 2.7680e-01, PNorm = 166.6701, GNorm = 0.2898
Meta loss on this task batch = 2.8789e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 166.6744, GNorm = 0.2647
Meta loss on this task batch = 3.2499e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 166.6776, GNorm = 0.3040
Meta loss on this task batch = 2.5974e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 166.6812, GNorm = 0.2774
Meta loss on this task batch = 2.4106e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 166.6849, GNorm = 0.3001
Took 142.33903741836548 seconds to complete one epoch of meta training
Took 150.78787660598755 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460632
Epoch 886
Meta loss on this task batch = 2.4065e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 166.6887, GNorm = 0.2374
Meta loss on this task batch = 2.2358e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 166.6925, GNorm = 0.2502
Meta loss on this task batch = 3.1186e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 166.6953, GNorm = 0.2862
Meta loss on this task batch = 2.6238e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 166.6979, GNorm = 0.4849
Meta loss on this task batch = 2.0957e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 166.7007, GNorm = 0.3200
Meta loss on this task batch = 3.0127e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 166.7037, GNorm = 0.3022
Meta loss on this task batch = 2.5321e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 166.7068, GNorm = 0.3247
Meta loss on this task batch = 2.6428e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 166.7097, GNorm = 0.4283
Meta loss on this task batch = 2.3592e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 166.7132, GNorm = 0.2625
Meta loss on this task batch = 1.9920e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 166.7174, GNorm = 0.2241
Meta loss on this task batch = 2.4907e-01, Meta loss averaged over last 500 steps = 2.7588e-01, PNorm = 166.7220, GNorm = 0.2542
Meta loss on this task batch = 2.4051e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 166.7267, GNorm = 0.3508
Meta loss on this task batch = 3.1547e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 166.7314, GNorm = 0.3394
Meta loss on this task batch = 2.4547e-01, Meta loss averaged over last 500 steps = 2.7577e-01, PNorm = 166.7361, GNorm = 0.2503
Meta loss on this task batch = 2.6775e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 166.7408, GNorm = 0.2834
Meta loss on this task batch = 3.1382e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 166.7450, GNorm = 0.3147
Meta loss on this task batch = 2.4903e-01, Meta loss averaged over last 500 steps = 2.7572e-01, PNorm = 166.7487, GNorm = 0.4950
Meta loss on this task batch = 3.1892e-01, Meta loss averaged over last 500 steps = 2.7570e-01, PNorm = 166.7508, GNorm = 0.3746
Meta loss on this task batch = 2.7456e-01, Meta loss averaged over last 500 steps = 2.7579e-01, PNorm = 166.7525, GNorm = 0.4014
Took 126.99216604232788 seconds to complete one epoch of meta training
Took 134.2738652229309 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474354
Epoch 887
Meta loss on this task batch = 2.8394e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 166.7543, GNorm = 0.3147
Meta loss on this task batch = 2.3972e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 166.7566, GNorm = 0.3193
Meta loss on this task batch = 3.1755e-01, Meta loss averaged over last 500 steps = 2.7582e-01, PNorm = 166.7586, GNorm = 0.2747
Meta loss on this task batch = 2.0948e-01, Meta loss averaged over last 500 steps = 2.7560e-01, PNorm = 166.7612, GNorm = 0.3577
Meta loss on this task batch = 2.1527e-01, Meta loss averaged over last 500 steps = 2.7555e-01, PNorm = 166.7641, GNorm = 0.2993
Meta loss on this task batch = 2.6200e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 166.7671, GNorm = 0.2913
Meta loss on this task batch = 2.9550e-01, Meta loss averaged over last 500 steps = 2.7559e-01, PNorm = 166.7703, GNorm = 0.3762
Meta loss on this task batch = 2.6088e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 166.7740, GNorm = 0.3482
Meta loss on this task batch = 2.8904e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 166.7773, GNorm = 0.3242
Meta loss on this task batch = 3.0747e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 166.7804, GNorm = 0.4024
Meta loss on this task batch = 3.2711e-01, Meta loss averaged over last 500 steps = 2.7575e-01, PNorm = 166.7839, GNorm = 0.3277
Meta loss on this task batch = 2.9147e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 166.7873, GNorm = 0.2763
Meta loss on this task batch = 2.7682e-01, Meta loss averaged over last 500 steps = 2.7574e-01, PNorm = 166.7908, GNorm = 0.5031
Meta loss on this task batch = 2.9878e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 166.7949, GNorm = 0.2822
Meta loss on this task batch = 2.2723e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 166.7992, GNorm = 0.2710
Meta loss on this task batch = 2.4621e-01, Meta loss averaged over last 500 steps = 2.7574e-01, PNorm = 166.8041, GNorm = 0.2696
Meta loss on this task batch = 2.9465e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 166.8090, GNorm = 0.3053
Meta loss on this task batch = 2.2009e-01, Meta loss averaged over last 500 steps = 2.7573e-01, PNorm = 166.8139, GNorm = 0.2923
Meta loss on this task batch = 2.6801e-01, Meta loss averaged over last 500 steps = 2.7574e-01, PNorm = 166.8181, GNorm = 0.5300
Took 136.96175718307495 seconds to complete one epoch of meta training
Took 146.14137625694275 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452835
Epoch 888
Meta loss on this task batch = 2.1434e-01, Meta loss averaged over last 500 steps = 2.7555e-01, PNorm = 166.8232, GNorm = 0.2517
Meta loss on this task batch = 2.6450e-01, Meta loss averaged over last 500 steps = 2.7545e-01, PNorm = 166.8278, GNorm = 0.2404
Meta loss on this task batch = 2.6390e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 166.8323, GNorm = 0.2900
Meta loss on this task batch = 2.4209e-01, Meta loss averaged over last 500 steps = 2.7536e-01, PNorm = 166.8371, GNorm = 0.2606
Meta loss on this task batch = 2.4738e-01, Meta loss averaged over last 500 steps = 2.7537e-01, PNorm = 166.8420, GNorm = 0.2719
Meta loss on this task batch = 3.0707e-01, Meta loss averaged over last 500 steps = 2.7557e-01, PNorm = 166.8457, GNorm = 0.4168
Meta loss on this task batch = 2.4161e-01, Meta loss averaged over last 500 steps = 2.7546e-01, PNorm = 166.8496, GNorm = 0.2950
Meta loss on this task batch = 3.1176e-01, Meta loss averaged over last 500 steps = 2.7548e-01, PNorm = 166.8530, GNorm = 0.3284
Meta loss on this task batch = 2.3462e-01, Meta loss averaged over last 500 steps = 2.7530e-01, PNorm = 166.8565, GNorm = 0.3232
Meta loss on this task batch = 2.7889e-01, Meta loss averaged over last 500 steps = 2.7530e-01, PNorm = 166.8592, GNorm = 0.3193
Meta loss on this task batch = 2.8897e-01, Meta loss averaged over last 500 steps = 2.7532e-01, PNorm = 166.8617, GNorm = 0.4839
Meta loss on this task batch = 3.1237e-01, Meta loss averaged over last 500 steps = 2.7541e-01, PNorm = 166.8641, GNorm = 0.3006
Meta loss on this task batch = 2.9111e-01, Meta loss averaged over last 500 steps = 2.7548e-01, PNorm = 166.8673, GNorm = 0.2809
Meta loss on this task batch = 2.9423e-01, Meta loss averaged over last 500 steps = 2.7549e-01, PNorm = 166.8709, GNorm = 0.2780
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 2.7551e-01, PNorm = 166.8751, GNorm = 0.4709
Meta loss on this task batch = 2.6099e-01, Meta loss averaged over last 500 steps = 2.7547e-01, PNorm = 166.8797, GNorm = 0.4171
Meta loss on this task batch = 3.1310e-01, Meta loss averaged over last 500 steps = 2.7564e-01, PNorm = 166.8841, GNorm = 0.2988
Meta loss on this task batch = 3.2228e-01, Meta loss averaged over last 500 steps = 2.7579e-01, PNorm = 166.8885, GNorm = 0.3170
Meta loss on this task batch = 2.9994e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 166.8921, GNorm = 0.3365
Took 126.48232674598694 seconds to complete one epoch of meta training
Took 135.26694011688232 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452779
Epoch 889
Meta loss on this task batch = 2.5980e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 166.8960, GNorm = 0.2649
Meta loss on this task batch = 2.2931e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 166.9004, GNorm = 0.3638
Meta loss on this task batch = 2.6436e-01, Meta loss averaged over last 500 steps = 2.7580e-01, PNorm = 166.9051, GNorm = 0.2107
Meta loss on this task batch = 3.1624e-01, Meta loss averaged over last 500 steps = 2.7582e-01, PNorm = 166.9101, GNorm = 0.3630
Meta loss on this task batch = 2.8779e-01, Meta loss averaged over last 500 steps = 2.7574e-01, PNorm = 166.9150, GNorm = 0.3324
Meta loss on this task batch = 3.1159e-01, Meta loss averaged over last 500 steps = 2.7579e-01, PNorm = 166.9195, GNorm = 0.3528
Meta loss on this task batch = 2.5535e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 166.9237, GNorm = 0.3267
Meta loss on this task batch = 2.4954e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 166.9284, GNorm = 0.3064
Meta loss on this task batch = 2.6391e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 166.9338, GNorm = 0.3916
Meta loss on this task batch = 2.7715e-01, Meta loss averaged over last 500 steps = 2.7573e-01, PNorm = 166.9389, GNorm = 0.2788
Meta loss on this task batch = 2.3542e-01, Meta loss averaged over last 500 steps = 2.7563e-01, PNorm = 166.9434, GNorm = 0.2357
Meta loss on this task batch = 2.9203e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 166.9479, GNorm = 0.2971
Meta loss on this task batch = 2.6369e-01, Meta loss averaged over last 500 steps = 2.7577e-01, PNorm = 166.9526, GNorm = 0.2881
Meta loss on this task batch = 2.5037e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 166.9570, GNorm = 0.2436
Meta loss on this task batch = 3.2790e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 166.9622, GNorm = 0.3610
Meta loss on this task batch = 2.7649e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 166.9672, GNorm = 0.3366
Meta loss on this task batch = 2.7638e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 166.9726, GNorm = 0.3148
Meta loss on this task batch = 2.8989e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 166.9779, GNorm = 0.2653
Meta loss on this task batch = 2.4009e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 166.9834, GNorm = 0.3228
Took 123.12665915489197 seconds to complete one epoch of meta training
Took 131.84894847869873 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452920
Epoch 890
Meta loss on this task batch = 2.6457e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 166.9887, GNorm = 0.3503
Meta loss on this task batch = 2.7535e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 166.9933, GNorm = 0.3567
Meta loss on this task batch = 2.3405e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 166.9976, GNorm = 0.3210
Meta loss on this task batch = 2.4837e-01, Meta loss averaged over last 500 steps = 2.7558e-01, PNorm = 167.0019, GNorm = 0.3076
Meta loss on this task batch = 2.9542e-01, Meta loss averaged over last 500 steps = 2.7565e-01, PNorm = 167.0061, GNorm = 0.2793
Meta loss on this task batch = 2.8419e-01, Meta loss averaged over last 500 steps = 2.7559e-01, PNorm = 167.0104, GNorm = 0.3370
Meta loss on this task batch = 2.3575e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 167.0149, GNorm = 0.3004
Meta loss on this task batch = 2.7672e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 167.0192, GNorm = 0.2751
Meta loss on this task batch = 2.6446e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 167.0235, GNorm = 0.2329
Meta loss on this task batch = 2.8799e-01, Meta loss averaged over last 500 steps = 2.7558e-01, PNorm = 167.0277, GNorm = 0.3630
Meta loss on this task batch = 2.4762e-01, Meta loss averaged over last 500 steps = 2.7555e-01, PNorm = 167.0316, GNorm = 0.2818
Meta loss on this task batch = 2.7011e-01, Meta loss averaged over last 500 steps = 2.7551e-01, PNorm = 167.0353, GNorm = 0.2787
Meta loss on this task batch = 3.0655e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 167.0386, GNorm = 0.2900
Meta loss on this task batch = 2.6825e-01, Meta loss averaged over last 500 steps = 2.7549e-01, PNorm = 167.0418, GNorm = 0.2839
Meta loss on this task batch = 3.2209e-01, Meta loss averaged over last 500 steps = 2.7558e-01, PNorm = 167.0455, GNorm = 0.2893
Meta loss on this task batch = 2.9749e-01, Meta loss averaged over last 500 steps = 2.7563e-01, PNorm = 167.0491, GNorm = 0.4069
Meta loss on this task batch = 2.7870e-01, Meta loss averaged over last 500 steps = 2.7564e-01, PNorm = 167.0522, GNorm = 0.2748
Meta loss on this task batch = 2.6718e-01, Meta loss averaged over last 500 steps = 2.7571e-01, PNorm = 167.0562, GNorm = 0.3231
Meta loss on this task batch = 3.2094e-01, Meta loss averaged over last 500 steps = 2.7568e-01, PNorm = 167.0598, GNorm = 0.6027
Took 127.36111283302307 seconds to complete one epoch of meta training
Took 136.12746691703796 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467282
Epoch 891
Meta loss on this task batch = 2.3908e-01, Meta loss averaged over last 500 steps = 2.7570e-01, PNorm = 167.0639, GNorm = 0.2766
Meta loss on this task batch = 2.9725e-01, Meta loss averaged over last 500 steps = 2.7567e-01, PNorm = 167.0682, GNorm = 0.3694
Meta loss on this task batch = 2.3824e-01, Meta loss averaged over last 500 steps = 2.7548e-01, PNorm = 167.0728, GNorm = 0.2328
Meta loss on this task batch = 2.9362e-01, Meta loss averaged over last 500 steps = 2.7556e-01, PNorm = 167.0777, GNorm = 0.2805
Meta loss on this task batch = 2.9417e-01, Meta loss averaged over last 500 steps = 2.7550e-01, PNorm = 167.0821, GNorm = 0.2943
Meta loss on this task batch = 2.1840e-01, Meta loss averaged over last 500 steps = 2.7530e-01, PNorm = 167.0867, GNorm = 0.2933
Meta loss on this task batch = 2.5005e-01, Meta loss averaged over last 500 steps = 2.7522e-01, PNorm = 167.0913, GNorm = 0.2633
Meta loss on this task batch = 2.9659e-01, Meta loss averaged over last 500 steps = 2.7528e-01, PNorm = 167.0961, GNorm = 0.2808
Meta loss on this task batch = 2.4783e-01, Meta loss averaged over last 500 steps = 2.7536e-01, PNorm = 167.1012, GNorm = 0.3156
Meta loss on this task batch = 2.5040e-01, Meta loss averaged over last 500 steps = 2.7523e-01, PNorm = 167.1061, GNorm = 0.2918
Meta loss on this task batch = 2.8652e-01, Meta loss averaged over last 500 steps = 2.7531e-01, PNorm = 167.1112, GNorm = 0.3049
Meta loss on this task batch = 2.8353e-01, Meta loss averaged over last 500 steps = 2.7534e-01, PNorm = 167.1160, GNorm = 0.3015
Meta loss on this task batch = 3.0479e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 167.1210, GNorm = 0.3062
Meta loss on this task batch = 3.3707e-01, Meta loss averaged over last 500 steps = 2.7558e-01, PNorm = 167.1254, GNorm = 0.3045
Meta loss on this task batch = 3.1585e-01, Meta loss averaged over last 500 steps = 2.7558e-01, PNorm = 167.1297, GNorm = 0.3718
Meta loss on this task batch = 3.0681e-01, Meta loss averaged over last 500 steps = 2.7560e-01, PNorm = 167.1333, GNorm = 0.3988
Meta loss on this task batch = 2.6221e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 167.1371, GNorm = 0.2260
Meta loss on this task batch = 2.4503e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 167.1412, GNorm = 0.2357
Meta loss on this task batch = 2.3254e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 167.1457, GNorm = 0.3220
Took 125.54365634918213 seconds to complete one epoch of meta training
Took 134.08270239830017 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457480
Epoch 892
Meta loss on this task batch = 2.4608e-01, Meta loss averaged over last 500 steps = 2.7517e-01, PNorm = 167.1501, GNorm = 0.2540
Meta loss on this task batch = 2.5866e-01, Meta loss averaged over last 500 steps = 2.7515e-01, PNorm = 167.1550, GNorm = 0.2643
Meta loss on this task batch = 2.3577e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 167.1601, GNorm = 0.2712
Meta loss on this task batch = 2.1335e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 167.1654, GNorm = 0.2753
Meta loss on this task batch = 3.4419e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 167.1701, GNorm = 0.2578
Meta loss on this task batch = 2.4407e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 167.1743, GNorm = 0.2597
Meta loss on this task batch = 2.6547e-01, Meta loss averaged over last 500 steps = 2.7507e-01, PNorm = 167.1781, GNorm = 0.3562
Meta loss on this task batch = 2.2727e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 167.1820, GNorm = 0.2554
Meta loss on this task batch = 2.8708e-01, Meta loss averaged over last 500 steps = 2.7507e-01, PNorm = 167.1858, GNorm = 0.5335
Meta loss on this task batch = 2.9665e-01, Meta loss averaged over last 500 steps = 2.7503e-01, PNorm = 167.1894, GNorm = 0.4269
Meta loss on this task batch = 2.8909e-01, Meta loss averaged over last 500 steps = 2.7510e-01, PNorm = 167.1928, GNorm = 0.4332
Meta loss on this task batch = 3.0717e-01, Meta loss averaged over last 500 steps = 2.7519e-01, PNorm = 167.1957, GNorm = 0.3634
Meta loss on this task batch = 2.9564e-01, Meta loss averaged over last 500 steps = 2.7530e-01, PNorm = 167.1984, GNorm = 0.5067
Meta loss on this task batch = 2.6321e-01, Meta loss averaged over last 500 steps = 2.7528e-01, PNorm = 167.2019, GNorm = 0.2489
Meta loss on this task batch = 2.6854e-01, Meta loss averaged over last 500 steps = 2.7523e-01, PNorm = 167.2052, GNorm = 0.3039
Meta loss on this task batch = 3.4157e-01, Meta loss averaged over last 500 steps = 2.7529e-01, PNorm = 167.2071, GNorm = 0.3948
Meta loss on this task batch = 3.0677e-01, Meta loss averaged over last 500 steps = 2.7536e-01, PNorm = 167.2096, GNorm = 0.3703
Meta loss on this task batch = 2.8268e-01, Meta loss averaged over last 500 steps = 2.7532e-01, PNorm = 167.2117, GNorm = 0.3215
Meta loss on this task batch = 2.7083e-01, Meta loss averaged over last 500 steps = 2.7533e-01, PNorm = 167.2141, GNorm = 0.3310
Took 120.64713406562805 seconds to complete one epoch of meta training
Took 129.51822543144226 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456254
Epoch 893
Meta loss on this task batch = 3.1693e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 167.2166, GNorm = 0.2852
Meta loss on this task batch = 2.9587e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 167.2189, GNorm = 0.2792
Meta loss on this task batch = 2.6481e-01, Meta loss averaged over last 500 steps = 2.7532e-01, PNorm = 167.2218, GNorm = 0.3337
Meta loss on this task batch = 2.3079e-01, Meta loss averaged over last 500 steps = 2.7517e-01, PNorm = 167.2253, GNorm = 0.2503
Meta loss on this task batch = 2.6023e-01, Meta loss averaged over last 500 steps = 2.7515e-01, PNorm = 167.2294, GNorm = 0.2859
Meta loss on this task batch = 2.7187e-01, Meta loss averaged over last 500 steps = 2.7529e-01, PNorm = 167.2336, GNorm = 0.2851
Meta loss on this task batch = 3.1054e-01, Meta loss averaged over last 500 steps = 2.7536e-01, PNorm = 167.2381, GNorm = 0.3631
Meta loss on this task batch = 3.1552e-01, Meta loss averaged over last 500 steps = 2.7550e-01, PNorm = 167.2415, GNorm = 0.2773
Meta loss on this task batch = 2.1488e-01, Meta loss averaged over last 500 steps = 2.7535e-01, PNorm = 167.2449, GNorm = 0.2542
Meta loss on this task batch = 2.3298e-01, Meta loss averaged over last 500 steps = 2.7532e-01, PNorm = 167.2481, GNorm = 0.3618
Meta loss on this task batch = 3.1885e-01, Meta loss averaged over last 500 steps = 2.7532e-01, PNorm = 167.2498, GNorm = 0.3097
Meta loss on this task batch = 3.4715e-01, Meta loss averaged over last 500 steps = 2.7549e-01, PNorm = 167.2517, GNorm = 0.3515
Meta loss on this task batch = 2.9333e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 167.2543, GNorm = 0.3630
Meta loss on this task batch = 2.0561e-01, Meta loss averaged over last 500 steps = 2.7524e-01, PNorm = 167.2572, GNorm = 0.2555
Meta loss on this task batch = 2.8015e-01, Meta loss averaged over last 500 steps = 2.7526e-01, PNorm = 167.2596, GNorm = 0.2833
Meta loss on this task batch = 2.8108e-01, Meta loss averaged over last 500 steps = 2.7515e-01, PNorm = 167.2618, GNorm = 0.2935
Meta loss on this task batch = 2.3968e-01, Meta loss averaged over last 500 steps = 2.7512e-01, PNorm = 167.2639, GNorm = 0.3995
Meta loss on this task batch = 1.8977e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 167.2666, GNorm = 0.2821
Meta loss on this task batch = 2.8851e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 167.2688, GNorm = 0.3629
Took 119.08657550811768 seconds to complete one epoch of meta training
Took 127.36576271057129 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463237
Epoch 894
Meta loss on this task batch = 2.1703e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 167.2708, GNorm = 0.2538
Meta loss on this task batch = 2.1512e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 167.2735, GNorm = 0.2779
Meta loss on this task batch = 2.6628e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 167.2759, GNorm = 0.2777
Meta loss on this task batch = 2.8677e-01, Meta loss averaged over last 500 steps = 2.7489e-01, PNorm = 167.2786, GNorm = 0.3239
Meta loss on this task batch = 3.2848e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 167.2810, GNorm = 0.4436
Meta loss on this task batch = 2.9093e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 167.2830, GNorm = 0.2868
Meta loss on this task batch = 2.7823e-01, Meta loss averaged over last 500 steps = 2.7510e-01, PNorm = 167.2853, GNorm = 0.2763
Meta loss on this task batch = 2.9785e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 167.2878, GNorm = 0.3787
Meta loss on this task batch = 3.4572e-01, Meta loss averaged over last 500 steps = 2.7518e-01, PNorm = 167.2891, GNorm = 0.3489
Meta loss on this task batch = 2.4606e-01, Meta loss averaged over last 500 steps = 2.7516e-01, PNorm = 167.2907, GNorm = 0.2807
Meta loss on this task batch = 2.2031e-01, Meta loss averaged over last 500 steps = 2.7509e-01, PNorm = 167.2927, GNorm = 0.3387
Meta loss on this task batch = 2.3536e-01, Meta loss averaged over last 500 steps = 2.7508e-01, PNorm = 167.2955, GNorm = 0.2712
Meta loss on this task batch = 2.9525e-01, Meta loss averaged over last 500 steps = 2.7517e-01, PNorm = 167.2977, GNorm = 0.3595
Meta loss on this task batch = 2.8764e-01, Meta loss averaged over last 500 steps = 2.7520e-01, PNorm = 167.2996, GNorm = 0.2928
Meta loss on this task batch = 2.6067e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 167.3021, GNorm = 0.2142
Meta loss on this task batch = 2.6079e-01, Meta loss averaged over last 500 steps = 2.7507e-01, PNorm = 167.3051, GNorm = 0.3939
Meta loss on this task batch = 2.7566e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 167.3081, GNorm = 0.2665
Meta loss on this task batch = 2.8200e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 167.3116, GNorm = 0.2291
Meta loss on this task batch = 3.1880e-01, Meta loss averaged over last 500 steps = 2.7498e-01, PNorm = 167.3146, GNorm = 0.3598
Took 117.3787214756012 seconds to complete one epoch of meta training
Took 125.6069130897522 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457509
Epoch 895
Meta loss on this task batch = 2.8429e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 167.3176, GNorm = 0.2817
Meta loss on this task batch = 2.8528e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 167.3205, GNorm = 0.2762
Meta loss on this task batch = 2.3375e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 167.3236, GNorm = 0.2704
Meta loss on this task batch = 2.6696e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 167.3271, GNorm = 0.2834
Meta loss on this task batch = 3.5609e-01, Meta loss averaged over last 500 steps = 2.7508e-01, PNorm = 167.3298, GNorm = 0.3638
Meta loss on this task batch = 3.0227e-01, Meta loss averaged over last 500 steps = 2.7515e-01, PNorm = 167.3316, GNorm = 0.3926
Meta loss on this task batch = 3.0375e-01, Meta loss averaged over last 500 steps = 2.7520e-01, PNorm = 167.3334, GNorm = 0.2788
Meta loss on this task batch = 2.9503e-01, Meta loss averaged over last 500 steps = 2.7528e-01, PNorm = 167.3353, GNorm = 0.3679
Meta loss on this task batch = 2.1349e-01, Meta loss averaged over last 500 steps = 2.7515e-01, PNorm = 167.3378, GNorm = 0.2872
Meta loss on this task batch = 3.0107e-01, Meta loss averaged over last 500 steps = 2.7525e-01, PNorm = 167.3405, GNorm = 0.2765
Meta loss on this task batch = 2.6657e-01, Meta loss averaged over last 500 steps = 2.7529e-01, PNorm = 167.3436, GNorm = 0.3186
Meta loss on this task batch = 2.4338e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 167.3466, GNorm = 0.2896
Meta loss on this task batch = 2.2603e-01, Meta loss averaged over last 500 steps = 2.7530e-01, PNorm = 167.3495, GNorm = 0.2529
Meta loss on this task batch = 2.4391e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 167.3521, GNorm = 0.2596
Meta loss on this task batch = 3.0897e-01, Meta loss averaged over last 500 steps = 2.7540e-01, PNorm = 167.3546, GNorm = 0.2986
Meta loss on this task batch = 2.3647e-01, Meta loss averaged over last 500 steps = 2.7526e-01, PNorm = 167.3574, GNorm = 0.3224
Meta loss on this task batch = 2.7612e-01, Meta loss averaged over last 500 steps = 2.7529e-01, PNorm = 167.3605, GNorm = 0.3167
Meta loss on this task batch = 3.3528e-01, Meta loss averaged over last 500 steps = 2.7534e-01, PNorm = 167.3639, GNorm = 0.3304
Meta loss on this task batch = 2.7802e-01, Meta loss averaged over last 500 steps = 2.7524e-01, PNorm = 167.3674, GNorm = 0.4192
Took 121.27982831001282 seconds to complete one epoch of meta training
Took 129.34607434272766 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471243
Epoch 896
Meta loss on this task batch = 2.6062e-01, Meta loss averaged over last 500 steps = 2.7519e-01, PNorm = 167.3711, GNorm = 0.2861
Meta loss on this task batch = 2.4920e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 167.3745, GNorm = 0.2908
Meta loss on this task batch = 2.7678e-01, Meta loss averaged over last 500 steps = 2.7508e-01, PNorm = 167.3779, GNorm = 0.2822
Meta loss on this task batch = 2.3378e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 167.3812, GNorm = 0.2555
Meta loss on this task batch = 2.9418e-01, Meta loss averaged over last 500 steps = 2.7500e-01, PNorm = 167.3847, GNorm = 0.2905
Meta loss on this task batch = 2.8273e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 167.3884, GNorm = 0.4060
Meta loss on this task batch = 2.2987e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 167.3924, GNorm = 0.3003
Meta loss on this task batch = 2.5531e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 167.3966, GNorm = 0.3166
Meta loss on this task batch = 3.2778e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 167.4002, GNorm = 0.3281
Meta loss on this task batch = 2.5398e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 167.4043, GNorm = 0.3025
Meta loss on this task batch = 2.6901e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 167.4082, GNorm = 0.2959
Meta loss on this task batch = 2.7787e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 167.4130, GNorm = 0.3325
Meta loss on this task batch = 2.7110e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 167.4175, GNorm = 0.2808
Meta loss on this task batch = 2.2229e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 167.4216, GNorm = 0.2658
Meta loss on this task batch = 2.6849e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 167.4256, GNorm = 0.4161
Meta loss on this task batch = 2.8109e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 167.4290, GNorm = 0.3113
Meta loss on this task batch = 2.3453e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 167.4329, GNorm = 0.2972
Meta loss on this task batch = 2.9585e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 167.4362, GNorm = 0.3032
Meta loss on this task batch = 3.0528e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 167.4391, GNorm = 0.3887
Took 125.22467160224915 seconds to complete one epoch of meta training
Took 133.06347346305847 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471441
Epoch 897
Meta loss on this task batch = 2.5239e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 167.4414, GNorm = 0.3447
Meta loss on this task batch = 2.8172e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 167.4433, GNorm = 0.3603
Meta loss on this task batch = 2.7569e-01, Meta loss averaged over last 500 steps = 2.7492e-01, PNorm = 167.4460, GNorm = 0.2881
Meta loss on this task batch = 2.9817e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 167.4489, GNorm = 0.2358
Meta loss on this task batch = 3.0351e-01, Meta loss averaged over last 500 steps = 2.7498e-01, PNorm = 167.4516, GNorm = 0.2680
Meta loss on this task batch = 2.5205e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 167.4545, GNorm = 0.2279
Meta loss on this task batch = 3.3719e-01, Meta loss averaged over last 500 steps = 2.7507e-01, PNorm = 167.4574, GNorm = 0.3739
Meta loss on this task batch = 2.8316e-01, Meta loss averaged over last 500 steps = 2.7510e-01, PNorm = 167.4606, GNorm = 0.4101
Meta loss on this task batch = 2.9129e-01, Meta loss averaged over last 500 steps = 2.7512e-01, PNorm = 167.4645, GNorm = 0.5572
Meta loss on this task batch = 3.2499e-01, Meta loss averaged over last 500 steps = 2.7520e-01, PNorm = 167.4681, GNorm = 0.3075
Meta loss on this task batch = 2.6510e-01, Meta loss averaged over last 500 steps = 2.7518e-01, PNorm = 167.4715, GNorm = 0.2936
Meta loss on this task batch = 2.3643e-01, Meta loss averaged over last 500 steps = 2.7510e-01, PNorm = 167.4751, GNorm = 0.2541
Meta loss on this task batch = 2.3087e-01, Meta loss averaged over last 500 steps = 2.7510e-01, PNorm = 167.4791, GNorm = 0.2600
Meta loss on this task batch = 2.2747e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 167.4833, GNorm = 0.5113
Meta loss on this task batch = 2.4492e-01, Meta loss averaged over last 500 steps = 2.7503e-01, PNorm = 167.4876, GNorm = 0.2871
Meta loss on this task batch = 2.5404e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 167.4920, GNorm = 0.3207
Meta loss on this task batch = 3.1994e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 167.4963, GNorm = 0.4286
Meta loss on this task batch = 2.3044e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 167.5010, GNorm = 0.2322
Meta loss on this task batch = 2.5559e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 167.5057, GNorm = 0.4276
Took 128.049959897995 seconds to complete one epoch of meta training
Took 136.76032185554504 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478496
Epoch 898
Meta loss on this task batch = 2.9210e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 167.5107, GNorm = 0.3264
Meta loss on this task batch = 2.3405e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 167.5163, GNorm = 0.3318
Meta loss on this task batch = 2.5821e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 167.5219, GNorm = 0.3459
Meta loss on this task batch = 2.8259e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 167.5268, GNorm = 0.4324
Meta loss on this task batch = 2.8492e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 167.5317, GNorm = 0.2926
Meta loss on this task batch = 3.2852e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 167.5360, GNorm = 0.4890
Meta loss on this task batch = 2.6169e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 167.5396, GNorm = 0.3494
Meta loss on this task batch = 2.4988e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 167.5430, GNorm = 0.2511
Meta loss on this task batch = 3.0239e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 167.5462, GNorm = 0.2526
Meta loss on this task batch = 2.4853e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 167.5496, GNorm = 0.3344
Meta loss on this task batch = 2.7085e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 167.5533, GNorm = 0.3285
Meta loss on this task batch = 2.3697e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 167.5571, GNorm = 0.2510
Meta loss on this task batch = 2.4063e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 167.5613, GNorm = 0.3408
Meta loss on this task batch = 2.7636e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 167.5659, GNorm = 0.3535
Meta loss on this task batch = 2.9034e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 167.5703, GNorm = 0.4480
Meta loss on this task batch = 2.8359e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 167.5746, GNorm = 0.4382
Meta loss on this task batch = 2.9698e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 167.5791, GNorm = 0.3550
Meta loss on this task batch = 3.0606e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 167.5838, GNorm = 0.4037
Meta loss on this task batch = 2.6451e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 167.5888, GNorm = 0.3389
Took 124.75305843353271 seconds to complete one epoch of meta training
Took 134.02845287322998 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453758
Epoch 899
Meta loss on this task batch = 2.7659e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 167.5939, GNorm = 0.3308
Meta loss on this task batch = 2.6965e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 167.5987, GNorm = 0.2853
Meta loss on this task batch = 3.2304e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 167.6029, GNorm = 0.2957
Meta loss on this task batch = 2.7534e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 167.6074, GNorm = 0.2660
Meta loss on this task batch = 2.2383e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 167.6123, GNorm = 0.2380
Meta loss on this task batch = 2.6673e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 167.6177, GNorm = 0.2532
Meta loss on this task batch = 2.5711e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 167.6232, GNorm = 0.3387
Meta loss on this task batch = 2.9462e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 167.6284, GNorm = 0.3460
Meta loss on this task batch = 2.7490e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 167.6334, GNorm = 0.4350
Meta loss on this task batch = 2.8025e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 167.6378, GNorm = 0.3222
Meta loss on this task batch = 2.5624e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 167.6418, GNorm = 0.2770
Meta loss on this task batch = 2.4138e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 167.6464, GNorm = 0.2903
Meta loss on this task batch = 2.8031e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 167.6506, GNorm = 0.3849
Meta loss on this task batch = 2.7477e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 167.6542, GNorm = 0.3087
Meta loss on this task batch = 2.2600e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 167.6584, GNorm = 0.2602
Meta loss on this task batch = 2.9160e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 167.6617, GNorm = 0.2970
Meta loss on this task batch = 2.8299e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 167.6650, GNorm = 0.2982
Meta loss on this task batch = 2.7729e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 167.6685, GNorm = 0.3017
Meta loss on this task batch = 2.8752e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 167.6721, GNorm = 0.4620
Took 124.64366459846497 seconds to complete one epoch of meta training
Took 133.78022718429565 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453481
Epoch 900
Meta loss on this task batch = 2.6231e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 167.6759, GNorm = 0.2960
Meta loss on this task batch = 2.9772e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 167.6788, GNorm = 0.4613
Meta loss on this task batch = 2.7199e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 167.6819, GNorm = 0.4664
Meta loss on this task batch = 3.0149e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 167.6855, GNorm = 0.2875
Meta loss on this task batch = 2.1923e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 167.6894, GNorm = 0.2598
Meta loss on this task batch = 2.7337e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 167.6928, GNorm = 0.3082
Meta loss on this task batch = 2.7132e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 167.6958, GNorm = 0.3255
Meta loss on this task batch = 2.9424e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 167.6980, GNorm = 0.2407
Meta loss on this task batch = 2.0972e-01, Meta loss averaged over last 500 steps = 2.7429e-01, PNorm = 167.7005, GNorm = 0.2746
Meta loss on this task batch = 3.0698e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 167.7021, GNorm = 0.3935
Meta loss on this task batch = 2.3342e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 167.7043, GNorm = 0.2779
Meta loss on this task batch = 2.3390e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 167.7070, GNorm = 0.2348
Meta loss on this task batch = 2.7830e-01, Meta loss averaged over last 500 steps = 2.7412e-01, PNorm = 167.7102, GNorm = 0.2384
Meta loss on this task batch = 3.2668e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 167.7131, GNorm = 0.3021
Meta loss on this task batch = 2.9796e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 167.7159, GNorm = 0.4175
Meta loss on this task batch = 2.6326e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 167.7190, GNorm = 0.2872
Meta loss on this task batch = 2.9024e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 167.7224, GNorm = 0.2566
Meta loss on this task batch = 3.0878e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 167.7260, GNorm = 0.2754
Meta loss on this task batch = 2.5948e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 167.7303, GNorm = 0.3197
Took 122.6632866859436 seconds to complete one epoch of meta training
Took 131.52853870391846 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.433237
Epoch 901
Meta loss on this task batch = 3.3428e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 167.7349, GNorm = 0.2867
Meta loss on this task batch = 2.5820e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 167.7401, GNorm = 0.2537
Meta loss on this task batch = 2.5150e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 167.7451, GNorm = 0.2903
Meta loss on this task batch = 2.4017e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 167.7506, GNorm = 0.2554
Meta loss on this task batch = 2.8571e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 167.7560, GNorm = 0.2411
Meta loss on this task batch = 2.4354e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 167.7615, GNorm = 0.2635
Meta loss on this task batch = 2.6442e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 167.7665, GNorm = 0.4326
Meta loss on this task batch = 2.7857e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 167.7715, GNorm = 0.3569
Meta loss on this task batch = 2.7515e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 167.7766, GNorm = 0.2871
Meta loss on this task batch = 2.6950e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 167.7817, GNorm = 0.3977
Meta loss on this task batch = 2.2833e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 167.7871, GNorm = 0.3684
Meta loss on this task batch = 3.0531e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 167.7922, GNorm = 0.4702
Meta loss on this task batch = 3.0988e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 167.7969, GNorm = 0.2863
Meta loss on this task batch = 3.3818e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 167.8007, GNorm = 0.4200
Meta loss on this task batch = 2.6962e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 167.8040, GNorm = 0.2958
Meta loss on this task batch = 2.5685e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 167.8078, GNorm = 0.2494
Meta loss on this task batch = 2.6354e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 167.8116, GNorm = 0.2793
Meta loss on this task batch = 2.4227e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 167.8158, GNorm = 0.3012
Meta loss on this task batch = 2.7704e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 167.8205, GNorm = 0.3116
Took 126.17767667770386 seconds to complete one epoch of meta training
Took 134.16377782821655 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444554
Epoch 902
Meta loss on this task batch = 2.7702e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 167.8249, GNorm = 0.2951
Meta loss on this task batch = 2.7287e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 167.8293, GNorm = 0.3187
Meta loss on this task batch = 2.1795e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 167.8338, GNorm = 0.2349
Meta loss on this task batch = 3.1043e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 167.8379, GNorm = 0.3444
Meta loss on this task batch = 2.4531e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 167.8419, GNorm = 0.3238
Meta loss on this task batch = 2.3374e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 167.8467, GNorm = 0.2892
Meta loss on this task batch = 2.4591e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 167.8513, GNorm = 0.3107
Meta loss on this task batch = 3.0853e-01, Meta loss averaged over last 500 steps = 2.7429e-01, PNorm = 167.8555, GNorm = 0.4674
Meta loss on this task batch = 2.6283e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 167.8599, GNorm = 0.3014
Meta loss on this task batch = 2.6480e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 167.8643, GNorm = 0.3724
Meta loss on this task batch = 2.9626e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 167.8679, GNorm = 0.3873
Meta loss on this task batch = 2.4828e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 167.8711, GNorm = 0.4098
Meta loss on this task batch = 2.3118e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 167.8746, GNorm = 0.3054
Meta loss on this task batch = 2.7537e-01, Meta loss averaged over last 500 steps = 2.7408e-01, PNorm = 167.8777, GNorm = 0.3929
Meta loss on this task batch = 3.2545e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 167.8805, GNorm = 0.4669
Meta loss on this task batch = 2.6627e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 167.8836, GNorm = 0.2971
Meta loss on this task batch = 2.7499e-01, Meta loss averaged over last 500 steps = 2.7411e-01, PNorm = 167.8865, GNorm = 0.3344
Meta loss on this task batch = 3.5063e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 167.8882, GNorm = 0.3777
Meta loss on this task batch = 3.2251e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 167.8904, GNorm = 0.3353
Took 126.72484421730042 seconds to complete one epoch of meta training
Took 134.73675894737244 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451767
Epoch 903
Meta loss on this task batch = 2.5403e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 167.8926, GNorm = 0.2832
Meta loss on this task batch = 3.1231e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 167.8954, GNorm = 0.4205
Meta loss on this task batch = 3.4533e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 167.8986, GNorm = 0.2887
Meta loss on this task batch = 3.0746e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 167.9022, GNorm = 0.4007
Meta loss on this task batch = 2.6079e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 167.9065, GNorm = 0.3126
Meta loss on this task batch = 2.9138e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 167.9110, GNorm = 0.3259
Meta loss on this task batch = 2.9919e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 167.9152, GNorm = 0.2902
Meta loss on this task batch = 3.2912e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 167.9188, GNorm = 0.2889
Meta loss on this task batch = 2.7363e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 167.9230, GNorm = 0.2774
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 167.9268, GNorm = 0.2498
Meta loss on this task batch = 2.8510e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 167.9311, GNorm = 0.2528
Meta loss on this task batch = 3.4295e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 167.9356, GNorm = 0.4511
Meta loss on this task batch = 2.8352e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 167.9403, GNorm = 0.3910
Meta loss on this task batch = 2.6757e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 167.9455, GNorm = 0.2917
Meta loss on this task batch = 2.4386e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 167.9512, GNorm = 0.2366
Meta loss on this task batch = 2.9818e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 167.9565, GNorm = 0.3394
Meta loss on this task batch = 2.6312e-01, Meta loss averaged over last 500 steps = 2.7482e-01, PNorm = 167.9613, GNorm = 0.2824
Meta loss on this task batch = 2.8285e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 167.9665, GNorm = 0.2933
Meta loss on this task batch = 2.6544e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 167.9715, GNorm = 0.3550
Took 125.74352669715881 seconds to complete one epoch of meta training
Took 134.26306462287903 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448095
Epoch 904
Meta loss on this task batch = 2.9800e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 167.9762, GNorm = 0.3486
Meta loss on this task batch = 3.1434e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 167.9799, GNorm = 0.4930
Meta loss on this task batch = 2.5467e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 167.9842, GNorm = 0.3465
Meta loss on this task batch = 2.5753e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 167.9890, GNorm = 0.2872
Meta loss on this task batch = 2.9603e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 167.9932, GNorm = 0.2790
Meta loss on this task batch = 2.5463e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 167.9975, GNorm = 0.2700
Meta loss on this task batch = 3.2278e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 168.0009, GNorm = 0.3123
Meta loss on this task batch = 2.3090e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 168.0047, GNorm = 0.2504
Meta loss on this task batch = 2.5809e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 168.0088, GNorm = 0.2969
Meta loss on this task batch = 2.9373e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 168.0132, GNorm = 0.3414
Meta loss on this task batch = 3.0266e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 168.0164, GNorm = 0.3668
Meta loss on this task batch = 3.4740e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 168.0198, GNorm = 0.5804
Meta loss on this task batch = 2.6371e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 168.0232, GNorm = 0.2963
Meta loss on this task batch = 2.7555e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 168.0274, GNorm = 0.2721
Meta loss on this task batch = 2.9741e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 168.0312, GNorm = 0.2539
Meta loss on this task batch = 3.0661e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 168.0347, GNorm = 0.3078
Meta loss on this task batch = 2.3669e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 168.0380, GNorm = 0.4057
Meta loss on this task batch = 2.7986e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 168.0418, GNorm = 0.2624
Meta loss on this task batch = 2.2792e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 168.0454, GNorm = 0.3395
Took 137.2919418811798 seconds to complete one epoch of meta training
Took 145.52297592163086 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462589
Epoch 905
Meta loss on this task batch = 3.1041e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 168.0491, GNorm = 0.3599
Meta loss on this task batch = 2.4512e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 168.0531, GNorm = 0.2595
Meta loss on this task batch = 2.8694e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 168.0568, GNorm = 0.3745
Meta loss on this task batch = 2.6964e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 168.0609, GNorm = 0.2524
Meta loss on this task batch = 2.5232e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 168.0652, GNorm = 0.2682
Meta loss on this task batch = 2.1999e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 168.0695, GNorm = 0.2531
Meta loss on this task batch = 3.1075e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 168.0742, GNorm = 0.3052
Meta loss on this task batch = 2.3448e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 168.0784, GNorm = 0.3212
Meta loss on this task batch = 3.2048e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 168.0820, GNorm = 0.4109
Meta loss on this task batch = 3.3563e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 168.0856, GNorm = 0.2919
Meta loss on this task batch = 2.1733e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 168.0899, GNorm = 0.3184
Meta loss on this task batch = 3.2325e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 168.0940, GNorm = 0.3946
Meta loss on this task batch = 2.4858e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 168.0984, GNorm = 0.2823
Meta loss on this task batch = 2.7505e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 168.1030, GNorm = 0.2671
Meta loss on this task batch = 2.5705e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 168.1081, GNorm = 0.3134
Meta loss on this task batch = 2.5226e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 168.1131, GNorm = 0.2668
Meta loss on this task batch = 3.1638e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 168.1174, GNorm = 0.3645
Meta loss on this task batch = 2.4128e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 168.1214, GNorm = 0.2947
Meta loss on this task batch = 2.3832e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 168.1257, GNorm = 0.3783
Took 123.33888459205627 seconds to complete one epoch of meta training
Took 132.2558674812317 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476480
Epoch 906
Meta loss on this task batch = 2.5450e-01, Meta loss averaged over last 500 steps = 2.7429e-01, PNorm = 168.1296, GNorm = 0.3733
Meta loss on this task batch = 2.9013e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 168.1337, GNorm = 0.3817
Meta loss on this task batch = 2.5110e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 168.1370, GNorm = 0.3974
Meta loss on this task batch = 2.7400e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 168.1399, GNorm = 0.2934
Meta loss on this task batch = 3.1616e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 168.1420, GNorm = 0.3784
Meta loss on this task batch = 2.6804e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 168.1442, GNorm = 0.2880
Meta loss on this task batch = 2.7273e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 168.1460, GNorm = 0.2797
Meta loss on this task batch = 3.0701e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 168.1469, GNorm = 0.3355
Meta loss on this task batch = 3.0260e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 168.1482, GNorm = 0.3426
Meta loss on this task batch = 2.7780e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 168.1498, GNorm = 0.3103
Meta loss on this task batch = 2.7391e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 168.1523, GNorm = 0.3388
Meta loss on this task batch = 2.9014e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 168.1551, GNorm = 0.3624
Meta loss on this task batch = 2.1348e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 168.1582, GNorm = 0.2544
Meta loss on this task batch = 2.8711e-01, Meta loss averaged over last 500 steps = 2.7425e-01, PNorm = 168.1611, GNorm = 0.2509
Meta loss on this task batch = 2.7387e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 168.1644, GNorm = 0.2676
Meta loss on this task batch = 2.6031e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 168.1677, GNorm = 0.2909
Meta loss on this task batch = 3.2601e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 168.1707, GNorm = 0.3827
Meta loss on this task batch = 2.9719e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 168.1736, GNorm = 0.2923
Meta loss on this task batch = 2.6865e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 168.1767, GNorm = 0.3138
Took 207.22777485847473 seconds to complete one epoch of meta training
Took 214.8219964504242 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456456
Epoch 907
Meta loss on this task batch = 2.8397e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 168.1801, GNorm = 0.3338
Meta loss on this task batch = 2.9253e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 168.1836, GNorm = 0.3059
Meta loss on this task batch = 3.0055e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 168.1868, GNorm = 0.3256
Meta loss on this task batch = 2.8544e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 168.1900, GNorm = 0.2722
Meta loss on this task batch = 2.1524e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 168.1934, GNorm = 0.2508
Meta loss on this task batch = 2.3615e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 168.1973, GNorm = 0.2397
Meta loss on this task batch = 2.5347e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 168.2011, GNorm = 0.2695
Meta loss on this task batch = 3.2300e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 168.2047, GNorm = 0.3513
Meta loss on this task batch = 2.4937e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 168.2082, GNorm = 0.2354
Meta loss on this task batch = 2.4390e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 168.2125, GNorm = 0.2764
Meta loss on this task batch = 2.4604e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 168.2166, GNorm = 0.2817
Meta loss on this task batch = 2.8036e-01, Meta loss averaged over last 500 steps = 2.7399e-01, PNorm = 168.2217, GNorm = 0.2800
Meta loss on this task batch = 2.1581e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 168.2268, GNorm = 0.2999
Meta loss on this task batch = 2.7375e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 168.2320, GNorm = 0.3201
Meta loss on this task batch = 3.1192e-01, Meta loss averaged over last 500 steps = 2.7391e-01, PNorm = 168.2372, GNorm = 0.3059
Meta loss on this task batch = 3.1434e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 168.2419, GNorm = 0.5120
Meta loss on this task batch = 2.9729e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 168.2464, GNorm = 0.3095
Meta loss on this task batch = 2.5877e-01, Meta loss averaged over last 500 steps = 2.7405e-01, PNorm = 168.2513, GNorm = 0.2621
Meta loss on this task batch = 2.4858e-01, Meta loss averaged over last 500 steps = 2.7408e-01, PNorm = 168.2564, GNorm = 0.3130
Took 125.31168746948242 seconds to complete one epoch of meta training
Took 133.71491837501526 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476180
Epoch 908
Meta loss on this task batch = 2.5442e-01, Meta loss averaged over last 500 steps = 2.7394e-01, PNorm = 168.2615, GNorm = 0.2780
Meta loss on this task batch = 2.3983e-01, Meta loss averaged over last 500 steps = 2.7396e-01, PNorm = 168.2668, GNorm = 0.3066
Meta loss on this task batch = 2.5783e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 168.2719, GNorm = 0.2629
Meta loss on this task batch = 2.1662e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 168.2767, GNorm = 0.2933
Meta loss on this task batch = 2.5241e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 168.2814, GNorm = 0.2820
Meta loss on this task batch = 2.4224e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 168.2856, GNorm = 0.2483
Meta loss on this task batch = 2.7777e-01, Meta loss averaged over last 500 steps = 2.7351e-01, PNorm = 168.2897, GNorm = 0.3295
Meta loss on this task batch = 2.7392e-01, Meta loss averaged over last 500 steps = 2.7355e-01, PNorm = 168.2935, GNorm = 0.3149
Meta loss on this task batch = 2.7133e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 168.2968, GNorm = 0.3247
Meta loss on this task batch = 3.0111e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 168.2998, GNorm = 0.3121
Meta loss on this task batch = 2.5825e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 168.3026, GNorm = 0.2839
Meta loss on this task batch = 2.8745e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 168.3049, GNorm = 0.2792
Meta loss on this task batch = 3.1661e-01, Meta loss averaged over last 500 steps = 2.7361e-01, PNorm = 168.3073, GNorm = 0.4443
Meta loss on this task batch = 2.2653e-01, Meta loss averaged over last 500 steps = 2.7355e-01, PNorm = 168.3104, GNorm = 0.3240
Meta loss on this task batch = 2.6632e-01, Meta loss averaged over last 500 steps = 2.7346e-01, PNorm = 168.3141, GNorm = 0.3754
Meta loss on this task batch = 2.5146e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 168.3178, GNorm = 0.2498
Meta loss on this task batch = 3.1079e-01, Meta loss averaged over last 500 steps = 2.7350e-01, PNorm = 168.3213, GNorm = 0.3271
Meta loss on this task batch = 3.5884e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 168.3240, GNorm = 0.4642
Meta loss on this task batch = 2.2192e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 168.3271, GNorm = 0.3268
Took 128.83463621139526 seconds to complete one epoch of meta training
Took 137.55256819725037 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459448
Epoch 909
Meta loss on this task batch = 2.6756e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 168.3304, GNorm = 0.3736
Meta loss on this task batch = 3.1412e-01, Meta loss averaged over last 500 steps = 2.7360e-01, PNorm = 168.3334, GNorm = 0.3909
Meta loss on this task batch = 3.4959e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 168.3363, GNorm = 0.3247
Meta loss on this task batch = 3.0105e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 168.3392, GNorm = 0.2673
Meta loss on this task batch = 2.9065e-01, Meta loss averaged over last 500 steps = 2.7391e-01, PNorm = 168.3421, GNorm = 0.3122
Meta loss on this task batch = 2.1535e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 168.3454, GNorm = 0.2637
Meta loss on this task batch = 2.5245e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 168.3494, GNorm = 0.2250
Meta loss on this task batch = 2.7175e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 168.3541, GNorm = 0.2807
Meta loss on this task batch = 2.9168e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 168.3590, GNorm = 0.2785
Meta loss on this task batch = 2.9093e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 168.3643, GNorm = 0.2938
Meta loss on this task batch = 2.4624e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 168.3698, GNorm = 0.3659
Meta loss on this task batch = 3.2727e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 168.3755, GNorm = 0.3624
Meta loss on this task batch = 1.9937e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 168.3815, GNorm = 0.2354
Meta loss on this task batch = 3.0015e-01, Meta loss averaged over last 500 steps = 2.7377e-01, PNorm = 168.3875, GNorm = 0.2743
Meta loss on this task batch = 3.0349e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 168.3933, GNorm = 0.2840
Meta loss on this task batch = 2.7852e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 168.3989, GNorm = 0.5625
Meta loss on this task batch = 2.4481e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 168.4042, GNorm = 0.3722
Meta loss on this task batch = 2.4091e-01, Meta loss averaged over last 500 steps = 2.7355e-01, PNorm = 168.4094, GNorm = 0.2446
Meta loss on this task batch = 3.1227e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 168.4152, GNorm = 0.4845
Took 128.50971150398254 seconds to complete one epoch of meta training
Took 137.1246316432953 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450504
Epoch 910
Meta loss on this task batch = 2.6754e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 168.4204, GNorm = 0.2608
Meta loss on this task batch = 3.4341e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 168.4244, GNorm = 0.3506
Meta loss on this task batch = 2.7306e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 168.4287, GNorm = 0.4362
Meta loss on this task batch = 2.6555e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 168.4330, GNorm = 0.2940
Meta loss on this task batch = 2.4498e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 168.4376, GNorm = 0.2660
Meta loss on this task batch = 3.2313e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 168.4415, GNorm = 0.4279
Meta loss on this task batch = 2.8419e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 168.4457, GNorm = 0.3417
Meta loss on this task batch = 2.7064e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 168.4501, GNorm = 0.2936
Meta loss on this task batch = 3.1168e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 168.4541, GNorm = 0.5362
Meta loss on this task batch = 3.1899e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 168.4573, GNorm = 0.4315
Meta loss on this task batch = 2.6997e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 168.4612, GNorm = 0.4182
Meta loss on this task batch = 2.3373e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 168.4646, GNorm = 0.2916
Meta loss on this task batch = 3.5274e-01, Meta loss averaged over last 500 steps = 2.7401e-01, PNorm = 168.4681, GNorm = 0.3453
Meta loss on this task batch = 2.5114e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 168.4720, GNorm = 0.2338
Meta loss on this task batch = 3.2294e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 168.4755, GNorm = 0.2964
Meta loss on this task batch = 2.5766e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 168.4791, GNorm = 0.3692
Meta loss on this task batch = 2.5322e-01, Meta loss averaged over last 500 steps = 2.7404e-01, PNorm = 168.4828, GNorm = 0.2798
Meta loss on this task batch = 2.9886e-01, Meta loss averaged over last 500 steps = 2.7408e-01, PNorm = 168.4864, GNorm = 0.3007
Meta loss on this task batch = 3.6294e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 168.4894, GNorm = 0.4622
Took 126.64058876037598 seconds to complete one epoch of meta training
Took 135.90792107582092 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.437301
Epoch 911
Meta loss on this task batch = 2.8726e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 168.4928, GNorm = 0.2895
Meta loss on this task batch = 2.9388e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 168.4971, GNorm = 0.5655
Meta loss on this task batch = 3.2178e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 168.5013, GNorm = 0.3641
Meta loss on this task batch = 2.6358e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 168.5056, GNorm = 0.2551
Meta loss on this task batch = 2.5651e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 168.5102, GNorm = 0.3580
Meta loss on this task batch = 2.7517e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 168.5148, GNorm = 0.3229
Meta loss on this task batch = 3.1903e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 168.5189, GNorm = 0.2928
Meta loss on this task batch = 2.7329e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 168.5231, GNorm = 0.2792
Meta loss on this task batch = 2.8675e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 168.5267, GNorm = 0.3152
Meta loss on this task batch = 2.7123e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 168.5307, GNorm = 0.2592
Meta loss on this task batch = 3.0766e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 168.5349, GNorm = 0.2876
Meta loss on this task batch = 3.1246e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 168.5389, GNorm = 0.5320
Meta loss on this task batch = 2.9155e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 168.5427, GNorm = 0.2631
Meta loss on this task batch = 2.5808e-01, Meta loss averaged over last 500 steps = 2.7482e-01, PNorm = 168.5468, GNorm = 0.2691
Meta loss on this task batch = 2.6061e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 168.5513, GNorm = 0.2737
Meta loss on this task batch = 3.2239e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 168.5556, GNorm = 0.2711
Meta loss on this task batch = 2.4204e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 168.5604, GNorm = 0.2501
Meta loss on this task batch = 2.3928e-01, Meta loss averaged over last 500 steps = 2.7482e-01, PNorm = 168.5657, GNorm = 0.2439
Meta loss on this task batch = 3.0770e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 168.5705, GNorm = 0.5453
Took 121.24624466896057 seconds to complete one epoch of meta training
Took 130.63260293006897 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462478
Epoch 912
Meta loss on this task batch = 2.9743e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 168.5755, GNorm = 0.3358
Meta loss on this task batch = 2.8869e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 168.5801, GNorm = 0.5273
Meta loss on this task batch = 2.7407e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 168.5839, GNorm = 0.2666
Meta loss on this task batch = 3.1712e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 168.5872, GNorm = 0.2639
Meta loss on this task batch = 2.7971e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 168.5909, GNorm = 0.2726
Meta loss on this task batch = 2.9287e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 168.5948, GNorm = 0.2789
Meta loss on this task batch = 2.3395e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 168.5991, GNorm = 0.2630
Meta loss on this task batch = 2.9012e-01, Meta loss averaged over last 500 steps = 2.7509e-01, PNorm = 168.6035, GNorm = 0.4095
Meta loss on this task batch = 2.8985e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 168.6073, GNorm = 0.3316
Meta loss on this task batch = 3.0180e-01, Meta loss averaged over last 500 steps = 2.7512e-01, PNorm = 168.6104, GNorm = 0.2579
Meta loss on this task batch = 3.1797e-01, Meta loss averaged over last 500 steps = 2.7534e-01, PNorm = 168.6136, GNorm = 0.3191
Meta loss on this task batch = 2.4585e-01, Meta loss averaged over last 500 steps = 2.7523e-01, PNorm = 168.6172, GNorm = 0.2946
Meta loss on this task batch = 2.7436e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 168.6205, GNorm = 0.2931
Meta loss on this task batch = 3.1642e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 168.6235, GNorm = 0.4535
Meta loss on this task batch = 2.9160e-01, Meta loss averaged over last 500 steps = 2.7549e-01, PNorm = 168.6258, GNorm = 0.3123
Meta loss on this task batch = 2.7874e-01, Meta loss averaged over last 500 steps = 2.7565e-01, PNorm = 168.6264, GNorm = 0.4287
Meta loss on this task batch = 2.4293e-01, Meta loss averaged over last 500 steps = 2.7564e-01, PNorm = 168.6279, GNorm = 0.2981
Meta loss on this task batch = 2.3209e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 168.6305, GNorm = 0.2900
Meta loss on this task batch = 3.0264e-01, Meta loss averaged over last 500 steps = 2.7559e-01, PNorm = 168.6336, GNorm = 0.3873
Took 124.21853446960449 seconds to complete one epoch of meta training
Took 133.17617082595825 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460355
Epoch 913
Meta loss on this task batch = 2.6591e-01, Meta loss averaged over last 500 steps = 2.7563e-01, PNorm = 168.6372, GNorm = 0.2742
Meta loss on this task batch = 2.7087e-01, Meta loss averaged over last 500 steps = 2.7564e-01, PNorm = 168.6406, GNorm = 0.2817
Meta loss on this task batch = 2.5421e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 168.6431, GNorm = 0.2944
Meta loss on this task batch = 2.7242e-01, Meta loss averaged over last 500 steps = 2.7557e-01, PNorm = 168.6459, GNorm = 0.3497
Meta loss on this task batch = 2.2121e-01, Meta loss averaged over last 500 steps = 2.7537e-01, PNorm = 168.6493, GNorm = 0.2128
Meta loss on this task batch = 2.5117e-01, Meta loss averaged over last 500 steps = 2.7533e-01, PNorm = 168.6518, GNorm = 0.2729
Meta loss on this task batch = 2.7836e-01, Meta loss averaged over last 500 steps = 2.7531e-01, PNorm = 168.6541, GNorm = 0.2865
Meta loss on this task batch = 3.2355e-01, Meta loss averaged over last 500 steps = 2.7548e-01, PNorm = 168.6556, GNorm = 0.2942
Meta loss on this task batch = 2.5166e-01, Meta loss averaged over last 500 steps = 2.7535e-01, PNorm = 168.6575, GNorm = 0.3359
Meta loss on this task batch = 2.5307e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 168.6596, GNorm = 0.2851
Meta loss on this task batch = 2.6285e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 168.6612, GNorm = 0.2960
Meta loss on this task batch = 2.8668e-01, Meta loss averaged over last 500 steps = 2.7558e-01, PNorm = 168.6626, GNorm = 0.4758
Meta loss on this task batch = 3.0687e-01, Meta loss averaged over last 500 steps = 2.7560e-01, PNorm = 168.6638, GNorm = 0.4205
Meta loss on this task batch = 3.0489e-01, Meta loss averaged over last 500 steps = 2.7569e-01, PNorm = 168.6655, GNorm = 0.2672
Meta loss on this task batch = 2.6332e-01, Meta loss averaged over last 500 steps = 2.7564e-01, PNorm = 168.6671, GNorm = 0.2932
Meta loss on this task batch = 3.4386e-01, Meta loss averaged over last 500 steps = 2.7571e-01, PNorm = 168.6682, GNorm = 0.3260
Meta loss on this task batch = 2.7786e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 168.6703, GNorm = 0.3815
Meta loss on this task batch = 2.7741e-01, Meta loss averaged over last 500 steps = 2.7559e-01, PNorm = 168.6732, GNorm = 0.3330
Meta loss on this task batch = 2.7117e-01, Meta loss averaged over last 500 steps = 2.7558e-01, PNorm = 168.6768, GNorm = 0.3127
Took 125.82690978050232 seconds to complete one epoch of meta training
Took 134.49103927612305 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457286
Epoch 914
Meta loss on this task batch = 3.2397e-01, Meta loss averaged over last 500 steps = 2.7563e-01, PNorm = 168.6804, GNorm = 0.3158
Meta loss on this task batch = 2.6595e-01, Meta loss averaged over last 500 steps = 2.7570e-01, PNorm = 168.6848, GNorm = 0.2901
Meta loss on this task batch = 2.9629e-01, Meta loss averaged over last 500 steps = 2.7580e-01, PNorm = 168.6895, GNorm = 0.3040
Meta loss on this task batch = 2.6233e-01, Meta loss averaged over last 500 steps = 2.7574e-01, PNorm = 168.6950, GNorm = 0.2550
Meta loss on this task batch = 2.8505e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 168.7003, GNorm = 0.4777
Meta loss on this task batch = 2.4632e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 168.7060, GNorm = 0.2756
Meta loss on this task batch = 2.3619e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 168.7124, GNorm = 0.2569
Meta loss on this task batch = 2.6896e-01, Meta loss averaged over last 500 steps = 2.7588e-01, PNorm = 168.7180, GNorm = 0.3155
Meta loss on this task batch = 2.5699e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 168.7231, GNorm = 0.2885
Meta loss on this task batch = 2.5885e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 168.7277, GNorm = 0.2649
Meta loss on this task batch = 2.4346e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 168.7327, GNorm = 0.2887
Meta loss on this task batch = 3.1505e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 168.7370, GNorm = 0.3124
Meta loss on this task batch = 2.9218e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 168.7410, GNorm = 0.3440
Meta loss on this task batch = 2.8069e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 168.7450, GNorm = 0.2973
Meta loss on this task batch = 2.8665e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 168.7487, GNorm = 0.4143
Meta loss on this task batch = 2.7115e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 168.7528, GNorm = 0.2842
Meta loss on this task batch = 2.8884e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 168.7569, GNorm = 0.4973
Meta loss on this task batch = 2.5366e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 168.7607, GNorm = 0.2411
Meta loss on this task batch = 3.0815e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 168.7639, GNorm = 0.4062
Took 124.54829478263855 seconds to complete one epoch of meta training
Took 133.04912328720093 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466084
Epoch 915
Meta loss on this task batch = 2.6837e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 168.7666, GNorm = 0.3543
Meta loss on this task batch = 2.6807e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 168.7694, GNorm = 0.4058
Meta loss on this task batch = 2.6599e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 168.7719, GNorm = 0.3568
Meta loss on this task batch = 2.6166e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 168.7746, GNorm = 0.3121
Meta loss on this task batch = 3.1386e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 168.7771, GNorm = 0.2824
Meta loss on this task batch = 2.6480e-01, Meta loss averaged over last 500 steps = 2.7574e-01, PNorm = 168.7787, GNorm = 0.2957
Meta loss on this task batch = 3.1204e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 168.7804, GNorm = 0.4068
Meta loss on this task batch = 2.7727e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 168.7824, GNorm = 0.3175
Meta loss on this task batch = 2.3414e-01, Meta loss averaged over last 500 steps = 2.7588e-01, PNorm = 168.7854, GNorm = 0.2293
Meta loss on this task batch = 2.5540e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 168.7885, GNorm = 0.2713
Meta loss on this task batch = 2.3894e-01, Meta loss averaged over last 500 steps = 2.7566e-01, PNorm = 168.7920, GNorm = 0.2600
Meta loss on this task batch = 2.9186e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 168.7953, GNorm = 0.2764
Meta loss on this task batch = 2.5857e-01, Meta loss averaged over last 500 steps = 2.7563e-01, PNorm = 168.7993, GNorm = 0.2369
Meta loss on this task batch = 2.5250e-01, Meta loss averaged over last 500 steps = 2.7563e-01, PNorm = 168.8033, GNorm = 0.2821
Meta loss on this task batch = 3.5115e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 168.8071, GNorm = 0.3545
Meta loss on this task batch = 3.2271e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 168.8107, GNorm = 0.3203
Meta loss on this task batch = 2.7125e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 168.8144, GNorm = 0.2806
Meta loss on this task batch = 2.9893e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 168.8182, GNorm = 0.2727
Meta loss on this task batch = 3.1156e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 168.8222, GNorm = 0.4291
Took 124.48102498054504 seconds to complete one epoch of meta training
Took 133.3240418434143 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469513
Epoch 916
Meta loss on this task batch = 2.7964e-01, Meta loss averaged over last 500 steps = 2.7614e-01, PNorm = 168.8265, GNorm = 0.2911
Meta loss on this task batch = 2.2395e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 168.8310, GNorm = 0.3007
Meta loss on this task batch = 2.6968e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 168.8356, GNorm = 0.2427
Meta loss on this task batch = 2.3162e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 168.8405, GNorm = 0.3453
Meta loss on this task batch = 2.8049e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 168.8452, GNorm = 0.2445
Meta loss on this task batch = 2.2438e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 168.8502, GNorm = 0.2491
Meta loss on this task batch = 3.0582e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 168.8546, GNorm = 0.3622
Meta loss on this task batch = 3.3660e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 168.8585, GNorm = 0.2866
Meta loss on this task batch = 2.4702e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 168.8629, GNorm = 0.2572
Meta loss on this task batch = 3.0749e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 168.8667, GNorm = 0.2866
Meta loss on this task batch = 3.0652e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 168.8703, GNorm = 0.3858
Meta loss on this task batch = 3.0332e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 168.8737, GNorm = 0.4048
Meta loss on this task batch = 2.3924e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 168.8771, GNorm = 0.2943
Meta loss on this task batch = 2.3904e-01, Meta loss averaged over last 500 steps = 2.7612e-01, PNorm = 168.8809, GNorm = 0.2489
Meta loss on this task batch = 2.3941e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 168.8850, GNorm = 0.3145
Meta loss on this task batch = 2.8458e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 168.8890, GNorm = 0.2639
Meta loss on this task batch = 2.6951e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 168.8924, GNorm = 0.2718
Meta loss on this task batch = 3.0371e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 168.8956, GNorm = 0.3608
Meta loss on this task batch = 2.5114e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 168.8992, GNorm = 0.3711
Took 126.19751811027527 seconds to complete one epoch of meta training
Took 134.51095724105835 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465335
Epoch 917
Meta loss on this task batch = 2.6477e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 168.9032, GNorm = 0.3289
Meta loss on this task batch = 3.2964e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 168.9067, GNorm = 0.3636
Meta loss on this task batch = 2.5366e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 168.9104, GNorm = 0.2357
Meta loss on this task batch = 2.5170e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 168.9139, GNorm = 0.3318
Meta loss on this task batch = 2.3553e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 168.9174, GNorm = 0.2595
Meta loss on this task batch = 2.7972e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 168.9208, GNorm = 0.3890
Meta loss on this task batch = 3.0724e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 168.9241, GNorm = 0.2801
Meta loss on this task batch = 2.3760e-01, Meta loss averaged over last 500 steps = 2.7580e-01, PNorm = 168.9272, GNorm = 0.3798
Meta loss on this task batch = 2.8024e-01, Meta loss averaged over last 500 steps = 2.7588e-01, PNorm = 168.9303, GNorm = 0.3180
Meta loss on this task batch = 2.2811e-01, Meta loss averaged over last 500 steps = 2.7575e-01, PNorm = 168.9336, GNorm = 0.2235
Meta loss on this task batch = 3.0582e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 168.9367, GNorm = 0.3065
Meta loss on this task batch = 2.9292e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 168.9395, GNorm = 0.4008
Meta loss on this task batch = 2.5211e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 168.9426, GNorm = 0.3188
Meta loss on this task batch = 2.6675e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 168.9463, GNorm = 0.2945
Meta loss on this task batch = 2.8496e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 168.9499, GNorm = 0.2861
Meta loss on this task batch = 2.8010e-01, Meta loss averaged over last 500 steps = 2.7600e-01, PNorm = 168.9537, GNorm = 0.2735
Meta loss on this task batch = 2.3869e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 168.9582, GNorm = 0.2633
Meta loss on this task batch = 2.7039e-01, Meta loss averaged over last 500 steps = 2.7588e-01, PNorm = 168.9622, GNorm = 0.3500
Meta loss on this task batch = 2.6665e-01, Meta loss averaged over last 500 steps = 2.7580e-01, PNorm = 168.9665, GNorm = 0.3860
Took 124.84362530708313 seconds to complete one epoch of meta training
Took 133.39996242523193 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482198
Epoch 918
Meta loss on this task batch = 2.5646e-01, Meta loss averaged over last 500 steps = 2.7564e-01, PNorm = 168.9711, GNorm = 0.2625
Meta loss on this task batch = 3.0284e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 168.9752, GNorm = 0.3826
Meta loss on this task batch = 2.7489e-01, Meta loss averaged over last 500 steps = 2.7555e-01, PNorm = 168.9800, GNorm = 0.2820
Meta loss on this task batch = 2.7132e-01, Meta loss averaged over last 500 steps = 2.7557e-01, PNorm = 168.9843, GNorm = 0.3209
Meta loss on this task batch = 2.7733e-01, Meta loss averaged over last 500 steps = 2.7564e-01, PNorm = 168.9887, GNorm = 0.2835
Meta loss on this task batch = 3.2146e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 168.9924, GNorm = 0.3318
Meta loss on this task batch = 2.6305e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 168.9967, GNorm = 0.3110
Meta loss on this task batch = 2.5978e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 169.0003, GNorm = 0.2811
Meta loss on this task batch = 2.5705e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 169.0041, GNorm = 0.2964
Meta loss on this task batch = 2.9520e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 169.0078, GNorm = 0.2284
Meta loss on this task batch = 2.4434e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 169.0114, GNorm = 0.2532
Meta loss on this task batch = 2.5194e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 169.0153, GNorm = 0.2561
Meta loss on this task batch = 2.5284e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 169.0195, GNorm = 0.2676
Meta loss on this task batch = 2.5757e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 169.0242, GNorm = 0.2899
Meta loss on this task batch = 2.7080e-01, Meta loss averaged over last 500 steps = 2.7588e-01, PNorm = 169.0295, GNorm = 0.3775
Meta loss on this task batch = 2.7949e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 169.0346, GNorm = 0.2824
Meta loss on this task batch = 2.8589e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 169.0394, GNorm = 0.2887
Meta loss on this task batch = 3.0043e-01, Meta loss averaged over last 500 steps = 2.7582e-01, PNorm = 169.0434, GNorm = 0.3414
Meta loss on this task batch = 3.4136e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 169.0469, GNorm = 0.5892
Took 125.47223544120789 seconds to complete one epoch of meta training
Took 134.31570291519165 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474566
Epoch 919
Meta loss on this task batch = 2.6496e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 169.0502, GNorm = 0.2831
Meta loss on this task batch = 2.6173e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 169.0532, GNorm = 0.2499
Meta loss on this task batch = 2.4337e-01, Meta loss averaged over last 500 steps = 2.7571e-01, PNorm = 169.0561, GNorm = 0.2785
Meta loss on this task batch = 2.1416e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 169.0593, GNorm = 0.2516
Meta loss on this task batch = 2.5869e-01, Meta loss averaged over last 500 steps = 2.7547e-01, PNorm = 169.0624, GNorm = 0.2947
Meta loss on this task batch = 3.2568e-01, Meta loss averaged over last 500 steps = 2.7558e-01, PNorm = 169.0644, GNorm = 0.3597
Meta loss on this task batch = 2.0312e-01, Meta loss averaged over last 500 steps = 2.7536e-01, PNorm = 169.0668, GNorm = 0.4282
Meta loss on this task batch = 2.7362e-01, Meta loss averaged over last 500 steps = 2.7531e-01, PNorm = 169.0695, GNorm = 0.3562
Meta loss on this task batch = 2.6241e-01, Meta loss averaged over last 500 steps = 2.7531e-01, PNorm = 169.0725, GNorm = 0.3173
Meta loss on this task batch = 2.9997e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 169.0757, GNorm = 0.3293
Meta loss on this task batch = 2.2899e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 169.0788, GNorm = 0.3412
Meta loss on this task batch = 2.8194e-01, Meta loss averaged over last 500 steps = 2.7540e-01, PNorm = 169.0818, GNorm = 0.2605
Meta loss on this task batch = 3.1337e-01, Meta loss averaged over last 500 steps = 2.7541e-01, PNorm = 169.0851, GNorm = 0.2814
Meta loss on this task batch = 2.9337e-01, Meta loss averaged over last 500 steps = 2.7536e-01, PNorm = 169.0889, GNorm = 0.2938
Meta loss on this task batch = 2.6309e-01, Meta loss averaged over last 500 steps = 2.7546e-01, PNorm = 169.0924, GNorm = 0.2934
Meta loss on this task batch = 3.4482e-01, Meta loss averaged over last 500 steps = 2.7568e-01, PNorm = 169.0960, GNorm = 0.3455
Meta loss on this task batch = 3.1638e-01, Meta loss averaged over last 500 steps = 2.7568e-01, PNorm = 169.0990, GNorm = 0.3064
Meta loss on this task batch = 3.0084e-01, Meta loss averaged over last 500 steps = 2.7559e-01, PNorm = 169.1014, GNorm = 0.3996
Meta loss on this task batch = 2.5342e-01, Meta loss averaged over last 500 steps = 2.7551e-01, PNorm = 169.1043, GNorm = 0.5656
Took 125.17303466796875 seconds to complete one epoch of meta training
Took 134.11760878562927 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485466
Epoch 920
Meta loss on this task batch = 2.4913e-01, Meta loss averaged over last 500 steps = 2.7559e-01, PNorm = 169.1069, GNorm = 0.2435
Meta loss on this task batch = 2.5588e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 169.1097, GNorm = 0.2498
Meta loss on this task batch = 2.7556e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 169.1127, GNorm = 0.2714
Meta loss on this task batch = 2.1730e-01, Meta loss averaged over last 500 steps = 2.7549e-01, PNorm = 169.1164, GNorm = 0.2547
Meta loss on this task batch = 3.0525e-01, Meta loss averaged over last 500 steps = 2.7572e-01, PNorm = 169.1200, GNorm = 0.3256
Meta loss on this task batch = 2.3150e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 169.1242, GNorm = 0.2954
Meta loss on this task batch = 2.5824e-01, Meta loss averaged over last 500 steps = 2.7569e-01, PNorm = 169.1288, GNorm = 0.2923
Meta loss on this task batch = 2.4023e-01, Meta loss averaged over last 500 steps = 2.7574e-01, PNorm = 169.1336, GNorm = 0.2737
Meta loss on this task batch = 3.1608e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 169.1385, GNorm = 0.3229
Meta loss on this task batch = 2.7207e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 169.1430, GNorm = 0.3995
Meta loss on this task batch = 2.6554e-01, Meta loss averaged over last 500 steps = 2.7568e-01, PNorm = 169.1478, GNorm = 0.3152
Meta loss on this task batch = 2.5138e-01, Meta loss averaged over last 500 steps = 2.7560e-01, PNorm = 169.1525, GNorm = 0.2900
Meta loss on this task batch = 2.5062e-01, Meta loss averaged over last 500 steps = 2.7555e-01, PNorm = 169.1562, GNorm = 0.2738
Meta loss on this task batch = 3.0080e-01, Meta loss averaged over last 500 steps = 2.7555e-01, PNorm = 169.1600, GNorm = 0.3298
Meta loss on this task batch = 2.7957e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 169.1638, GNorm = 0.3266
Meta loss on this task batch = 2.8576e-01, Meta loss averaged over last 500 steps = 2.7550e-01, PNorm = 169.1676, GNorm = 0.3366
Meta loss on this task batch = 3.0787e-01, Meta loss averaged over last 500 steps = 2.7568e-01, PNorm = 169.1712, GNorm = 0.3179
Meta loss on this task batch = 3.1543e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 169.1737, GNorm = 0.2891
Meta loss on this task batch = 2.4309e-01, Meta loss averaged over last 500 steps = 2.7573e-01, PNorm = 169.1762, GNorm = 0.3188
Took 121.5841920375824 seconds to complete one epoch of meta training
Took 130.0248727798462 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468817
Epoch 921
Meta loss on this task batch = 2.7034e-01, Meta loss averaged over last 500 steps = 2.7570e-01, PNorm = 169.1790, GNorm = 0.3029
Meta loss on this task batch = 3.1787e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 169.1809, GNorm = 0.3287
Meta loss on this task batch = 2.3632e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 169.1834, GNorm = 0.2643
Meta loss on this task batch = 2.7132e-01, Meta loss averaged over last 500 steps = 2.7575e-01, PNorm = 169.1854, GNorm = 0.2758
Meta loss on this task batch = 2.4869e-01, Meta loss averaged over last 500 steps = 2.7569e-01, PNorm = 169.1880, GNorm = 0.3888
Meta loss on this task batch = 2.4821e-01, Meta loss averaged over last 500 steps = 2.7555e-01, PNorm = 169.1910, GNorm = 0.2439
Meta loss on this task batch = 2.6295e-01, Meta loss averaged over last 500 steps = 2.7550e-01, PNorm = 169.1949, GNorm = 0.2574
Meta loss on this task batch = 2.5140e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 169.1992, GNorm = 0.2371
Meta loss on this task batch = 2.0446e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 169.2041, GNorm = 0.2462
Meta loss on this task batch = 2.6650e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 169.2089, GNorm = 0.3030
Meta loss on this task batch = 2.6742e-01, Meta loss averaged over last 500 steps = 2.7520e-01, PNorm = 169.2136, GNorm = 0.3578
Meta loss on this task batch = 2.9293e-01, Meta loss averaged over last 500 steps = 2.7518e-01, PNorm = 169.2182, GNorm = 0.4440
Meta loss on this task batch = 2.7324e-01, Meta loss averaged over last 500 steps = 2.7512e-01, PNorm = 169.2225, GNorm = 0.2637
Meta loss on this task batch = 3.7436e-01, Meta loss averaged over last 500 steps = 2.7528e-01, PNorm = 169.2265, GNorm = 0.5280
Meta loss on this task batch = 2.8068e-01, Meta loss averaged over last 500 steps = 2.7541e-01, PNorm = 169.2307, GNorm = 0.3062
Meta loss on this task batch = 2.8591e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 169.2345, GNorm = 0.3050
Meta loss on this task batch = 2.5742e-01, Meta loss averaged over last 500 steps = 2.7536e-01, PNorm = 169.2376, GNorm = 0.3177
Meta loss on this task batch = 3.0287e-01, Meta loss averaged over last 500 steps = 2.7548e-01, PNorm = 169.2398, GNorm = 0.4429
Meta loss on this task batch = 2.5072e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 169.2427, GNorm = 0.2846
Took 124.65122103691101 seconds to complete one epoch of meta training
Took 133.00408029556274 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446168
Epoch 922
Meta loss on this task batch = 2.8042e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 169.2456, GNorm = 0.4209
Meta loss on this task batch = 2.5062e-01, Meta loss averaged over last 500 steps = 2.7549e-01, PNorm = 169.2491, GNorm = 0.2978
Meta loss on this task batch = 2.6579e-01, Meta loss averaged over last 500 steps = 2.7555e-01, PNorm = 169.2530, GNorm = 0.4879
Meta loss on this task batch = 3.2115e-01, Meta loss averaged over last 500 steps = 2.7564e-01, PNorm = 169.2562, GNorm = 0.3820
Meta loss on this task batch = 3.1110e-01, Meta loss averaged over last 500 steps = 2.7559e-01, PNorm = 169.2593, GNorm = 0.3391
Meta loss on this task batch = 3.1941e-01, Meta loss averaged over last 500 steps = 2.7567e-01, PNorm = 169.2622, GNorm = 0.2828
Meta loss on this task batch = 3.2366e-01, Meta loss averaged over last 500 steps = 2.7580e-01, PNorm = 169.2648, GNorm = 0.3570
Meta loss on this task batch = 2.8751e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 169.2670, GNorm = 0.3011
Meta loss on this task batch = 3.0511e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 169.2695, GNorm = 0.4019
Meta loss on this task batch = 3.0387e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 169.2718, GNorm = 0.3104
Meta loss on this task batch = 2.7261e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 169.2743, GNorm = 0.2700
Meta loss on this task batch = 2.1413e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 169.2779, GNorm = 0.2592
Meta loss on this task batch = 2.4087e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 169.2815, GNorm = 0.2716
Meta loss on this task batch = 2.8190e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 169.2858, GNorm = 0.2672
Meta loss on this task batch = 3.0036e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 169.2901, GNorm = 0.2791
Meta loss on this task batch = 2.8466e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 169.2935, GNorm = 0.4549
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 169.2968, GNorm = 0.3175
Meta loss on this task batch = 3.0323e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 169.3003, GNorm = 0.3061
Meta loss on this task batch = 2.1958e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 169.3041, GNorm = 0.2822
Took 125.36135292053223 seconds to complete one epoch of meta training
Took 133.52789330482483 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449830
Epoch 923
Meta loss on this task batch = 2.3854e-01, Meta loss averaged over last 500 steps = 2.7599e-01, PNorm = 169.3089, GNorm = 0.3403
Meta loss on this task batch = 2.5805e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 169.3137, GNorm = 0.2407
Meta loss on this task batch = 2.7094e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 169.3177, GNorm = 0.2872
Meta loss on this task batch = 2.5798e-01, Meta loss averaged over last 500 steps = 2.7599e-01, PNorm = 169.3210, GNorm = 0.4662
Meta loss on this task batch = 2.4319e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 169.3243, GNorm = 0.3262
Meta loss on this task batch = 3.0305e-01, Meta loss averaged over last 500 steps = 2.7588e-01, PNorm = 169.3280, GNorm = 0.3516
Meta loss on this task batch = 3.1968e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 169.3323, GNorm = 0.4698
Meta loss on this task batch = 3.0419e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 169.3359, GNorm = 0.5185
Meta loss on this task batch = 2.7752e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 169.3399, GNorm = 0.2849
Meta loss on this task batch = 2.3228e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 169.3433, GNorm = 0.2557
Meta loss on this task batch = 2.9383e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 169.3470, GNorm = 0.4406
Meta loss on this task batch = 2.4137e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 169.3514, GNorm = 0.4139
Meta loss on this task batch = 2.4612e-01, Meta loss averaged over last 500 steps = 2.7571e-01, PNorm = 169.3559, GNorm = 0.3472
Meta loss on this task batch = 2.7265e-01, Meta loss averaged over last 500 steps = 2.7569e-01, PNorm = 169.3602, GNorm = 0.2384
Meta loss on this task batch = 3.1567e-01, Meta loss averaged over last 500 steps = 2.7574e-01, PNorm = 169.3643, GNorm = 0.2793
Meta loss on this task batch = 2.8949e-01, Meta loss averaged over last 500 steps = 2.7567e-01, PNorm = 169.3688, GNorm = 0.4702
Meta loss on this task batch = 3.5016e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 169.3728, GNorm = 0.3282
Meta loss on this task batch = 3.2027e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 169.3773, GNorm = 0.3789
Meta loss on this task batch = 3.2792e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 169.3817, GNorm = 0.5180
Took 120.43050599098206 seconds to complete one epoch of meta training
Took 128.6821870803833 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462951
Epoch 924
Meta loss on this task batch = 2.7354e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 169.3859, GNorm = 0.2528
Meta loss on this task batch = 2.5974e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 169.3903, GNorm = 0.2797
Meta loss on this task batch = 2.5963e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 169.3951, GNorm = 0.2930
Meta loss on this task batch = 2.4555e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 169.4002, GNorm = 0.2677
Meta loss on this task batch = 2.4053e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 169.4048, GNorm = 0.2662
Meta loss on this task batch = 2.7621e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 169.4094, GNorm = 0.2542
Meta loss on this task batch = 2.7110e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 169.4146, GNorm = 0.3440
Meta loss on this task batch = 2.6501e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 169.4192, GNorm = 0.2394
Meta loss on this task batch = 2.9433e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 169.4235, GNorm = 0.3527
Meta loss on this task batch = 2.8245e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 169.4281, GNorm = 0.3348
Meta loss on this task batch = 2.8193e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 169.4322, GNorm = 0.2657
Meta loss on this task batch = 3.0892e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 169.4364, GNorm = 0.3786
Meta loss on this task batch = 3.3412e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 169.4401, GNorm = 0.3411
Meta loss on this task batch = 2.6020e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 169.4437, GNorm = 0.2813
Meta loss on this task batch = 2.6017e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 169.4470, GNorm = 0.2561
Meta loss on this task batch = 2.6792e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 169.4509, GNorm = 0.2900
Meta loss on this task batch = 2.5943e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 169.4547, GNorm = 0.2347
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 169.4585, GNorm = 0.4606
Meta loss on this task batch = 2.6981e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 169.4624, GNorm = 0.3490
Took 124.14580059051514 seconds to complete one epoch of meta training
Took 132.53314995765686 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447243
Epoch 925
Meta loss on this task batch = 2.9668e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 169.4660, GNorm = 0.4307
Meta loss on this task batch = 2.7385e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 169.4696, GNorm = 0.2490
Meta loss on this task batch = 2.4609e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 169.4738, GNorm = 0.2627
Meta loss on this task batch = 2.4488e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 169.4782, GNorm = 0.2483
Meta loss on this task batch = 2.7712e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 169.4825, GNorm = 0.3026
Meta loss on this task batch = 3.1167e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 169.4856, GNorm = 0.3164
Meta loss on this task batch = 3.1540e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 169.4880, GNorm = 0.2642
Meta loss on this task batch = 3.3233e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 169.4904, GNorm = 0.3099
Meta loss on this task batch = 3.5363e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 169.4921, GNorm = 0.3292
Meta loss on this task batch = 2.5159e-01, Meta loss averaged over last 500 steps = 2.7662e-01, PNorm = 169.4939, GNorm = 0.3184
Meta loss on this task batch = 2.6023e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 169.4963, GNorm = 0.2802
Meta loss on this task batch = 2.2030e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 169.4989, GNorm = 0.2616
Meta loss on this task batch = 2.7127e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 169.5017, GNorm = 0.3862
Meta loss on this task batch = 2.9048e-01, Meta loss averaged over last 500 steps = 2.7662e-01, PNorm = 169.5044, GNorm = 0.2920
Meta loss on this task batch = 2.5586e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 169.5073, GNorm = 0.3498
Meta loss on this task batch = 2.0092e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 169.5102, GNorm = 0.2471
Meta loss on this task batch = 2.8023e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 169.5134, GNorm = 0.3298
Meta loss on this task batch = 3.0645e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 169.5167, GNorm = 0.2918
Meta loss on this task batch = 2.6555e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 169.5202, GNorm = 0.3911
Took 125.8756775856018 seconds to complete one epoch of meta training
Took 134.73913717269897 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460487
Epoch 926
Meta loss on this task batch = 2.7080e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 169.5238, GNorm = 0.2549
Meta loss on this task batch = 2.3027e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 169.5276, GNorm = 0.3797
Meta loss on this task batch = 2.7345e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 169.5317, GNorm = 0.2692
Meta loss on this task batch = 2.3375e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 169.5360, GNorm = 0.2451
Meta loss on this task batch = 2.4652e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 169.5401, GNorm = 0.2490
Meta loss on this task batch = 2.8038e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 169.5439, GNorm = 0.3212
Meta loss on this task batch = 2.5444e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 169.5477, GNorm = 0.3066
Meta loss on this task batch = 2.5551e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 169.5517, GNorm = 0.2412
Meta loss on this task batch = 2.6300e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 169.5555, GNorm = 0.3391
Meta loss on this task batch = 2.5758e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 169.5592, GNorm = 0.3207
Meta loss on this task batch = 3.1754e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 169.5629, GNorm = 0.4417
Meta loss on this task batch = 2.7630e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 169.5666, GNorm = 0.2707
Meta loss on this task batch = 2.6278e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 169.5701, GNorm = 0.2547
Meta loss on this task batch = 2.9335e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 169.5734, GNorm = 0.3261
Meta loss on this task batch = 3.1929e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 169.5765, GNorm = 0.3700
Meta loss on this task batch = 2.7248e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 169.5795, GNorm = 0.2969
Meta loss on this task batch = 2.8988e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 169.5830, GNorm = 0.2467
Meta loss on this task batch = 3.1932e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 169.5867, GNorm = 0.2964
Meta loss on this task batch = 2.4150e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 169.5905, GNorm = 0.3279
Took 128.98651599884033 seconds to complete one epoch of meta training
Took 137.27250599861145 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.434498
Epoch 927
Meta loss on this task batch = 2.9955e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 169.5942, GNorm = 0.2765
Meta loss on this task batch = 2.7603e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 169.5975, GNorm = 0.2629
Meta loss on this task batch = 2.5537e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 169.6010, GNorm = 0.2139
Meta loss on this task batch = 3.1611e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 169.6051, GNorm = 0.5087
Meta loss on this task batch = 2.6579e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 169.6093, GNorm = 0.3995
Meta loss on this task batch = 2.5778e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 169.6139, GNorm = 0.3536
Meta loss on this task batch = 2.6052e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 169.6182, GNorm = 0.2857
Meta loss on this task batch = 3.1844e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 169.6220, GNorm = 0.3860
Meta loss on this task batch = 3.1362e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 169.6247, GNorm = 0.3053
Meta loss on this task batch = 2.4192e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 169.6278, GNorm = 0.2806
Meta loss on this task batch = 3.2274e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 169.6307, GNorm = 0.3278
Meta loss on this task batch = 2.5075e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 169.6338, GNorm = 0.2975
Meta loss on this task batch = 2.6482e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 169.6371, GNorm = 0.3839
Meta loss on this task batch = 2.6128e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 169.6413, GNorm = 0.3318
Meta loss on this task batch = 3.3341e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 169.6454, GNorm = 0.4581
Meta loss on this task batch = 2.9937e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 169.6500, GNorm = 0.3223
Meta loss on this task batch = 2.3903e-01, Meta loss averaged over last 500 steps = 2.7690e-01, PNorm = 169.6546, GNorm = 0.2293
Meta loss on this task batch = 2.8836e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 169.6591, GNorm = 0.2633
Meta loss on this task batch = 2.4912e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 169.6638, GNorm = 0.2975
Took 123.20436000823975 seconds to complete one epoch of meta training
Took 132.93246054649353 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462578
Epoch 928
Meta loss on this task batch = 2.8926e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 169.6684, GNorm = 0.3022
Meta loss on this task batch = 3.0708e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 169.6728, GNorm = 0.4089
Meta loss on this task batch = 2.6534e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 169.6775, GNorm = 0.2315
Meta loss on this task batch = 2.8773e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 169.6820, GNorm = 0.2678
Meta loss on this task batch = 2.8006e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 169.6868, GNorm = 0.4227
Meta loss on this task batch = 2.3425e-01, Meta loss averaged over last 500 steps = 2.7678e-01, PNorm = 169.6912, GNorm = 0.2589
Meta loss on this task batch = 2.6968e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 169.6959, GNorm = 0.2995
Meta loss on this task batch = 2.8451e-01, Meta loss averaged over last 500 steps = 2.7678e-01, PNorm = 169.7003, GNorm = 0.3572
Meta loss on this task batch = 2.7264e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 169.7044, GNorm = 0.2788
Meta loss on this task batch = 2.9680e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 169.7085, GNorm = 0.2612
Meta loss on this task batch = 3.1496e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 169.7126, GNorm = 0.2860
Meta loss on this task batch = 2.6620e-01, Meta loss averaged over last 500 steps = 2.7707e-01, PNorm = 169.7168, GNorm = 0.2465
Meta loss on this task batch = 2.4652e-01, Meta loss averaged over last 500 steps = 2.7707e-01, PNorm = 169.7210, GNorm = 0.2777
Meta loss on this task batch = 2.6771e-01, Meta loss averaged over last 500 steps = 2.7699e-01, PNorm = 169.7252, GNorm = 0.2892
Meta loss on this task batch = 2.8480e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 169.7295, GNorm = 0.3095
Meta loss on this task batch = 2.3866e-01, Meta loss averaged over last 500 steps = 2.7698e-01, PNorm = 169.7343, GNorm = 0.3409
Meta loss on this task batch = 2.5287e-01, Meta loss averaged over last 500 steps = 2.7690e-01, PNorm = 169.7388, GNorm = 0.2401
Meta loss on this task batch = 2.5678e-01, Meta loss averaged over last 500 steps = 2.7691e-01, PNorm = 169.7435, GNorm = 0.3473
Meta loss on this task batch = 2.8181e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 169.7477, GNorm = 0.4317
Took 122.93960165977478 seconds to complete one epoch of meta training
Took 132.91252160072327 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478519
Epoch 929
Meta loss on this task batch = 2.6194e-01, Meta loss averaged over last 500 steps = 2.7699e-01, PNorm = 169.7516, GNorm = 0.2540
Meta loss on this task batch = 2.7996e-01, Meta loss averaged over last 500 steps = 2.7690e-01, PNorm = 169.7552, GNorm = 0.3542
Meta loss on this task batch = 2.9304e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 169.7591, GNorm = 0.2923
Meta loss on this task batch = 2.8413e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 169.7627, GNorm = 0.2473
Meta loss on this task batch = 3.0409e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 169.7662, GNorm = 0.3237
Meta loss on this task batch = 3.3193e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 169.7693, GNorm = 0.3357
Meta loss on this task batch = 2.8036e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 169.7718, GNorm = 0.4664
Meta loss on this task batch = 3.5311e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 169.7737, GNorm = 0.3669
Meta loss on this task batch = 3.0159e-01, Meta loss averaged over last 500 steps = 2.7694e-01, PNorm = 169.7758, GNorm = 0.3068
Meta loss on this task batch = 2.3284e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 169.7786, GNorm = 0.3296
Meta loss on this task batch = 2.4810e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 169.7814, GNorm = 0.3445
Meta loss on this task batch = 2.8262e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 169.7843, GNorm = 0.4818
Meta loss on this task batch = 2.4834e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 169.7880, GNorm = 0.3290
Meta loss on this task batch = 2.8421e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 169.7915, GNorm = 0.2384
Meta loss on this task batch = 2.5429e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 169.7956, GNorm = 0.2482
Meta loss on this task batch = 2.6230e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 169.7998, GNorm = 0.3135
Meta loss on this task batch = 2.6037e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 169.8046, GNorm = 0.2972
Meta loss on this task batch = 2.5921e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 169.8094, GNorm = 0.2674
Meta loss on this task batch = 2.4433e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 169.8144, GNorm = 0.3046
Took 129.1776659488678 seconds to complete one epoch of meta training
Took 137.58237433433533 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462889
Epoch 930
Meta loss on this task batch = 2.4027e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 169.8194, GNorm = 0.3665
Meta loss on this task batch = 2.8999e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 169.8237, GNorm = 0.2950
Meta loss on this task batch = 2.5840e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 169.8280, GNorm = 0.2679
Meta loss on this task batch = 3.6525e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 169.8308, GNorm = 0.5163
Meta loss on this task batch = 2.5401e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 169.8340, GNorm = 0.2315
Meta loss on this task batch = 2.4970e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 169.8374, GNorm = 0.2536
Meta loss on this task batch = 3.1514e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 169.8408, GNorm = 0.5823
Meta loss on this task batch = 3.1034e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 169.8449, GNorm = 0.3348
Meta loss on this task batch = 2.5777e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 169.8489, GNorm = 0.2789
Meta loss on this task batch = 2.5390e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 169.8529, GNorm = 0.3610
Meta loss on this task batch = 2.5658e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 169.8570, GNorm = 0.4755
Meta loss on this task batch = 2.3115e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 169.8610, GNorm = 0.2487
Meta loss on this task batch = 3.1199e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 169.8647, GNorm = 0.4021
Meta loss on this task batch = 2.7014e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 169.8691, GNorm = 0.3016
Meta loss on this task batch = 2.6645e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 169.8734, GNorm = 0.2848
Meta loss on this task batch = 2.5529e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 169.8780, GNorm = 0.3236
Meta loss on this task batch = 2.3815e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 169.8826, GNorm = 0.3365
Meta loss on this task batch = 2.9743e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 169.8862, GNorm = 0.3180
Meta loss on this task batch = 2.5068e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 169.8894, GNorm = 0.3574
Took 131.5656795501709 seconds to complete one epoch of meta training
Took 139.8919394016266 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475167
Epoch 931
Meta loss on this task batch = 3.0371e-01, Meta loss averaged over last 500 steps = 2.7600e-01, PNorm = 169.8927, GNorm = 0.2737
Meta loss on this task batch = 2.7856e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 169.8957, GNorm = 0.3155
Meta loss on this task batch = 2.7074e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 169.8985, GNorm = 0.2567
Meta loss on this task batch = 2.4566e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 169.9014, GNorm = 0.2636
Meta loss on this task batch = 2.9288e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 169.9039, GNorm = 0.2626
Meta loss on this task batch = 2.7720e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 169.9067, GNorm = 0.3589
Meta loss on this task batch = 1.9706e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 169.9095, GNorm = 0.2476
Meta loss on this task batch = 2.5092e-01, Meta loss averaged over last 500 steps = 2.7582e-01, PNorm = 169.9131, GNorm = 0.2621
Meta loss on this task batch = 3.0481e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 169.9165, GNorm = 0.2556
Meta loss on this task batch = 2.2554e-01, Meta loss averaged over last 500 steps = 2.7577e-01, PNorm = 169.9205, GNorm = 0.2854
Meta loss on this task batch = 2.5099e-01, Meta loss averaged over last 500 steps = 2.7577e-01, PNorm = 169.9251, GNorm = 0.4003
Meta loss on this task batch = 2.5900e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 169.9298, GNorm = 0.2401
Meta loss on this task batch = 3.2497e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 169.9341, GNorm = 0.3447
Meta loss on this task batch = 2.7963e-01, Meta loss averaged over last 500 steps = 2.7596e-01, PNorm = 169.9382, GNorm = 0.2766
Meta loss on this task batch = 2.7545e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 169.9415, GNorm = 0.3594
Meta loss on this task batch = 3.3060e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 169.9444, GNorm = 0.3284
Meta loss on this task batch = 2.4051e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 169.9476, GNorm = 0.3022
Meta loss on this task batch = 3.1533e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 169.9497, GNorm = 0.4053
Meta loss on this task batch = 2.9078e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 169.9521, GNorm = 0.4112
Took 123.13047909736633 seconds to complete one epoch of meta training
Took 131.4530806541443 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460516
Epoch 932
Meta loss on this task batch = 2.7573e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 169.9551, GNorm = 0.3118
Meta loss on this task batch = 2.9203e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 169.9579, GNorm = 0.3423
Meta loss on this task batch = 3.2496e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 169.9607, GNorm = 0.2564
Meta loss on this task batch = 3.0016e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 169.9629, GNorm = 0.2916
Meta loss on this task batch = 2.7363e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 169.9655, GNorm = 0.2942
Meta loss on this task batch = 2.5453e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 169.9682, GNorm = 0.3585
Meta loss on this task batch = 3.0169e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 169.9712, GNorm = 0.4273
Meta loss on this task batch = 2.4727e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 169.9747, GNorm = 0.2371
Meta loss on this task batch = 2.7309e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 169.9787, GNorm = 0.3355
Meta loss on this task batch = 2.5183e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 169.9826, GNorm = 0.2658
Meta loss on this task batch = 2.7738e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 169.9869, GNorm = 0.3141
Meta loss on this task batch = 2.4720e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 169.9912, GNorm = 0.2814
Meta loss on this task batch = 2.9765e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 169.9947, GNorm = 0.2844
Meta loss on this task batch = 2.2521e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 169.9987, GNorm = 0.2328
Meta loss on this task batch = 2.8650e-01, Meta loss averaged over last 500 steps = 2.7600e-01, PNorm = 170.0024, GNorm = 0.3112
Meta loss on this task batch = 2.9106e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 170.0056, GNorm = 0.3635
Meta loss on this task batch = 2.1825e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 170.0092, GNorm = 0.2432
Meta loss on this task batch = 2.5136e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 170.0132, GNorm = 0.2894
Meta loss on this task batch = 3.0094e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 170.0167, GNorm = 0.4000
Took 124.78571653366089 seconds to complete one epoch of meta training
Took 133.29223561286926 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453814
Epoch 933
Meta loss on this task batch = 2.5068e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 170.0201, GNorm = 0.2317
Meta loss on this task batch = 2.8419e-01, Meta loss averaged over last 500 steps = 2.7596e-01, PNorm = 170.0229, GNorm = 0.2728
Meta loss on this task batch = 2.9709e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 170.0249, GNorm = 0.4308
Meta loss on this task batch = 2.5213e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 170.0268, GNorm = 0.2533
Meta loss on this task batch = 2.5880e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 170.0292, GNorm = 0.2346
Meta loss on this task batch = 2.5863e-01, Meta loss averaged over last 500 steps = 2.7579e-01, PNorm = 170.0312, GNorm = 0.3679
Meta loss on this task batch = 3.2689e-01, Meta loss averaged over last 500 steps = 2.7588e-01, PNorm = 170.0331, GNorm = 0.3730
Meta loss on this task batch = 2.6567e-01, Meta loss averaged over last 500 steps = 2.7582e-01, PNorm = 170.0347, GNorm = 0.2781
Meta loss on this task batch = 3.3674e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 170.0365, GNorm = 0.4225
Meta loss on this task batch = 2.6974e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 170.0386, GNorm = 0.3313
Meta loss on this task batch = 2.5159e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 170.0407, GNorm = 0.2659
Meta loss on this task batch = 2.5525e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 170.0429, GNorm = 0.2639
Meta loss on this task batch = 2.6959e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 170.0455, GNorm = 0.2496
Meta loss on this task batch = 2.3888e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 170.0483, GNorm = 0.3288
Meta loss on this task batch = 2.6330e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 170.0513, GNorm = 0.2467
Meta loss on this task batch = 2.6329e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 170.0549, GNorm = 0.2860
Meta loss on this task batch = 3.1105e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 170.0586, GNorm = 0.3150
Meta loss on this task batch = 2.9658e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 170.0622, GNorm = 0.2740
Meta loss on this task batch = 3.5745e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 170.0660, GNorm = 0.4191
Took 126.27487349510193 seconds to complete one epoch of meta training
Took 135.15741062164307 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460372
Epoch 934
Meta loss on this task batch = 2.5319e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 170.0701, GNorm = 0.2552
Meta loss on this task batch = 3.1445e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 170.0743, GNorm = 0.3149
Meta loss on this task batch = 2.6887e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 170.0794, GNorm = 0.2764
Meta loss on this task batch = 2.8361e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 170.0847, GNorm = 0.2879
Meta loss on this task batch = 2.6060e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 170.0905, GNorm = 0.2336
Meta loss on this task batch = 2.6239e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 170.0962, GNorm = 0.2358
Meta loss on this task batch = 2.5379e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 170.1014, GNorm = 0.3783
Meta loss on this task batch = 3.0118e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 170.1059, GNorm = 0.4607
Meta loss on this task batch = 2.3188e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 170.1102, GNorm = 0.3125
Meta loss on this task batch = 2.5067e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 170.1147, GNorm = 0.3609
Meta loss on this task batch = 3.1732e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 170.1194, GNorm = 0.2918
Meta loss on this task batch = 2.6588e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 170.1245, GNorm = 0.3393
Meta loss on this task batch = 3.3138e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 170.1293, GNorm = 0.3561
Meta loss on this task batch = 3.3025e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 170.1335, GNorm = 0.3718
Meta loss on this task batch = 2.0505e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 170.1379, GNorm = 0.2738
Meta loss on this task batch = 2.2786e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 170.1426, GNorm = 0.2177
Meta loss on this task batch = 3.0451e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 170.1473, GNorm = 0.2817
Meta loss on this task batch = 2.5511e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 170.1525, GNorm = 0.2852
Meta loss on this task batch = 3.3759e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 170.1573, GNorm = 0.4523
Took 131.33815908432007 seconds to complete one epoch of meta training
Took 140.3921844959259 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462455
Epoch 935
Meta loss on this task batch = 3.2881e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 170.1622, GNorm = 0.4736
Meta loss on this task batch = 3.0959e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 170.1666, GNorm = 0.3820
Meta loss on this task batch = 2.5586e-01, Meta loss averaged over last 500 steps = 2.7685e-01, PNorm = 170.1710, GNorm = 0.2363
Meta loss on this task batch = 3.0660e-01, Meta loss averaged over last 500 steps = 2.7685e-01, PNorm = 170.1747, GNorm = 0.3616
Meta loss on this task batch = 3.1063e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 170.1778, GNorm = 0.3015
Meta loss on this task batch = 3.0819e-01, Meta loss averaged over last 500 steps = 2.7692e-01, PNorm = 170.1810, GNorm = 0.2477
Meta loss on this task batch = 2.8272e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 170.1851, GNorm = 0.3467
Meta loss on this task batch = 2.5609e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 170.1893, GNorm = 0.2672
Meta loss on this task batch = 3.0871e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 170.1936, GNorm = 0.2795
Meta loss on this task batch = 2.6528e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 170.1982, GNorm = 0.3657
Meta loss on this task batch = 2.4283e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 170.2031, GNorm = 0.3227
Meta loss on this task batch = 2.0294e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 170.2083, GNorm = 0.1849
Meta loss on this task batch = 2.6245e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 170.2144, GNorm = 0.3057
Meta loss on this task batch = 2.6945e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 170.2200, GNorm = 0.2323
Meta loss on this task batch = 2.7406e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 170.2252, GNorm = 0.2715
Meta loss on this task batch = 2.8222e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 170.2302, GNorm = 0.4528
Meta loss on this task batch = 2.7819e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 170.2353, GNorm = 0.2848
Meta loss on this task batch = 2.6723e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 170.2408, GNorm = 0.2877
Meta loss on this task batch = 2.8580e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 170.2459, GNorm = 0.3643
Took 127.30094718933105 seconds to complete one epoch of meta training
Took 137.0186791419983 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478041
Epoch 936
Meta loss on this task batch = 3.1711e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 170.2503, GNorm = 0.3496
Meta loss on this task batch = 1.8870e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 170.2548, GNorm = 0.3029
Meta loss on this task batch = 2.7220e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 170.2579, GNorm = 0.5124
Meta loss on this task batch = 2.9973e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 170.2616, GNorm = 0.2708
Meta loss on this task batch = 2.6189e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 170.2650, GNorm = 0.2845
Meta loss on this task batch = 2.5514e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 170.2689, GNorm = 0.3449
Meta loss on this task batch = 3.2033e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 170.2721, GNorm = 0.3247
Meta loss on this task batch = 2.7052e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 170.2757, GNorm = 0.3674
Meta loss on this task batch = 3.2005e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 170.2792, GNorm = 0.2685
Meta loss on this task batch = 2.8958e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 170.2828, GNorm = 0.3089
Meta loss on this task batch = 2.5252e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 170.2863, GNorm = 0.2773
Meta loss on this task batch = 2.9296e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 170.2899, GNorm = 0.2715
Meta loss on this task batch = 2.1131e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 170.2936, GNorm = 0.2511
Meta loss on this task batch = 2.6410e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 170.2975, GNorm = 0.3177
Meta loss on this task batch = 2.8239e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 170.3015, GNorm = 0.3226
Meta loss on this task batch = 2.2956e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 170.3054, GNorm = 0.2809
Meta loss on this task batch = 2.3968e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 170.3101, GNorm = 0.2663
Meta loss on this task batch = 3.0330e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 170.3143, GNorm = 0.3881
Meta loss on this task batch = 3.0899e-01, Meta loss averaged over last 500 steps = 2.7612e-01, PNorm = 170.3186, GNorm = 0.5359
Took 127.69622278213501 seconds to complete one epoch of meta training
Took 136.22493767738342 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460270
Epoch 937
Meta loss on this task batch = 2.7067e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 170.3230, GNorm = 0.2834
Meta loss on this task batch = 2.4299e-01, Meta loss averaged over last 500 steps = 2.7600e-01, PNorm = 170.3278, GNorm = 0.4844
Meta loss on this task batch = 2.4586e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 170.3326, GNorm = 0.2508
Meta loss on this task batch = 2.4458e-01, Meta loss averaged over last 500 steps = 2.7596e-01, PNorm = 170.3368, GNorm = 0.3316
Meta loss on this task batch = 2.4677e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 170.3403, GNorm = 0.3079
Meta loss on this task batch = 2.8374e-01, Meta loss averaged over last 500 steps = 2.7570e-01, PNorm = 170.3436, GNorm = 0.3709
Meta loss on this task batch = 2.6433e-01, Meta loss averaged over last 500 steps = 2.7565e-01, PNorm = 170.3471, GNorm = 0.4701
Meta loss on this task batch = 3.2438e-01, Meta loss averaged over last 500 steps = 2.7571e-01, PNorm = 170.3502, GNorm = 0.3213
Meta loss on this task batch = 2.7405e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 170.3533, GNorm = 0.2726
Meta loss on this task batch = 3.5487e-01, Meta loss averaged over last 500 steps = 2.7580e-01, PNorm = 170.3558, GNorm = 0.3424
Meta loss on this task batch = 3.0089e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 170.3570, GNorm = 0.3109
Meta loss on this task batch = 3.0483e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 170.3586, GNorm = 0.3236
Meta loss on this task batch = 3.0324e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 170.3601, GNorm = 0.2661
Meta loss on this task batch = 2.6372e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 170.3623, GNorm = 0.2301
Meta loss on this task batch = 2.5679e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 170.3649, GNorm = 0.2847
Meta loss on this task batch = 2.6327e-01, Meta loss averaged over last 500 steps = 2.7582e-01, PNorm = 170.3681, GNorm = 0.3109
Meta loss on this task batch = 2.7499e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 170.3715, GNorm = 0.2684
Meta loss on this task batch = 2.3258e-01, Meta loss averaged over last 500 steps = 2.7560e-01, PNorm = 170.3755, GNorm = 0.2877
Meta loss on this task batch = 2.5677e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 170.3796, GNorm = 0.3076
Took 124.9114818572998 seconds to complete one epoch of meta training
Took 133.70601725578308 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462207
Epoch 938
Meta loss on this task batch = 2.6172e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 170.3843, GNorm = 0.3009
Meta loss on this task batch = 3.1784e-01, Meta loss averaged over last 500 steps = 2.7565e-01, PNorm = 170.3893, GNorm = 0.3563
Meta loss on this task batch = 2.2560e-01, Meta loss averaged over last 500 steps = 2.7545e-01, PNorm = 170.3949, GNorm = 0.2530
Meta loss on this task batch = 2.8552e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 170.4004, GNorm = 0.2849
Meta loss on this task batch = 3.0226e-01, Meta loss averaged over last 500 steps = 2.7567e-01, PNorm = 170.4056, GNorm = 0.3162
Meta loss on this task batch = 2.6937e-01, Meta loss averaged over last 500 steps = 2.7559e-01, PNorm = 170.4099, GNorm = 0.2917
Meta loss on this task batch = 3.0662e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 170.4138, GNorm = 0.2739
Meta loss on this task batch = 3.3005e-01, Meta loss averaged over last 500 steps = 2.7569e-01, PNorm = 170.4169, GNorm = 0.3148
Meta loss on this task batch = 2.3727e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 170.4203, GNorm = 0.2564
Meta loss on this task batch = 3.1516e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 170.4238, GNorm = 0.3405
Meta loss on this task batch = 2.5406e-01, Meta loss averaged over last 500 steps = 2.7556e-01, PNorm = 170.4267, GNorm = 0.2574
Meta loss on this task batch = 2.2894e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 170.4298, GNorm = 0.2526
Meta loss on this task batch = 2.4949e-01, Meta loss averaged over last 500 steps = 2.7547e-01, PNorm = 170.4332, GNorm = 0.2494
Meta loss on this task batch = 2.5493e-01, Meta loss averaged over last 500 steps = 2.7540e-01, PNorm = 170.4361, GNorm = 0.3283
Meta loss on this task batch = 2.8394e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 170.4391, GNorm = 0.4092
Meta loss on this task batch = 2.5389e-01, Meta loss averaged over last 500 steps = 2.7529e-01, PNorm = 170.4420, GNorm = 0.3165
Meta loss on this task batch = 2.0326e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 170.4446, GNorm = 0.2932
Meta loss on this task batch = 2.0235e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 170.4474, GNorm = 0.2260
Meta loss on this task batch = 2.9697e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 170.4503, GNorm = 0.4196
Took 124.34762263298035 seconds to complete one epoch of meta training
Took 134.1739420890808 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469055
Epoch 939
Meta loss on this task batch = 2.5545e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 170.4529, GNorm = 0.4519
Meta loss on this task batch = 2.8179e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 170.4557, GNorm = 0.2472
Meta loss on this task batch = 2.4365e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 170.4584, GNorm = 0.2695
Meta loss on this task batch = 2.7184e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 170.4618, GNorm = 0.2704
Meta loss on this task batch = 2.8653e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 170.4651, GNorm = 0.2556
Meta loss on this task batch = 2.8499e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 170.4685, GNorm = 0.2669
Meta loss on this task batch = 3.0521e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 170.4716, GNorm = 0.4469
Meta loss on this task batch = 2.6661e-01, Meta loss averaged over last 500 steps = 2.7501e-01, PNorm = 170.4749, GNorm = 0.3049
Meta loss on this task batch = 2.8133e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 170.4782, GNorm = 0.3002
Meta loss on this task batch = 2.3821e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 170.4815, GNorm = 0.2560
Meta loss on this task batch = 2.3411e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 170.4857, GNorm = 0.3211
Meta loss on this task batch = 2.4826e-01, Meta loss averaged over last 500 steps = 2.7501e-01, PNorm = 170.4900, GNorm = 0.2547
Meta loss on this task batch = 2.4050e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 170.4943, GNorm = 0.2657
Meta loss on this task batch = 2.7388e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 170.4991, GNorm = 0.2792
Meta loss on this task batch = 3.1804e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 170.5035, GNorm = 0.4206
Meta loss on this task batch = 3.1483e-01, Meta loss averaged over last 500 steps = 2.7509e-01, PNorm = 170.5079, GNorm = 0.3659
Meta loss on this task batch = 2.4084e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 170.5124, GNorm = 0.2415
Meta loss on this task batch = 2.8228e-01, Meta loss averaged over last 500 steps = 2.7504e-01, PNorm = 170.5178, GNorm = 0.3644
Meta loss on this task batch = 2.4013e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 170.5229, GNorm = 0.3816
Took 124.63346529006958 seconds to complete one epoch of meta training
Took 133.71554327011108 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468139
Epoch 940
Meta loss on this task batch = 2.5433e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 170.5282, GNorm = 0.3429
Meta loss on this task batch = 2.6487e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 170.5335, GNorm = 0.2699
Meta loss on this task batch = 2.6548e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 170.5388, GNorm = 0.2714
Meta loss on this task batch = 3.0085e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 170.5438, GNorm = 0.3497
Meta loss on this task batch = 3.4358e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 170.5476, GNorm = 0.4323
Meta loss on this task batch = 3.3879e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 170.5512, GNorm = 0.4162
Meta loss on this task batch = 2.1967e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 170.5555, GNorm = 0.2363
Meta loss on this task batch = 2.2010e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 170.5603, GNorm = 0.3861
Meta loss on this task batch = 2.5494e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 170.5651, GNorm = 0.2627
Meta loss on this task batch = 3.2804e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 170.5689, GNorm = 0.2889
Meta loss on this task batch = 2.6319e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 170.5725, GNorm = 0.2911
Meta loss on this task batch = 2.7180e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 170.5766, GNorm = 0.2600
Meta loss on this task batch = 2.7204e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 170.5804, GNorm = 0.2678
Meta loss on this task batch = 2.9235e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 170.5840, GNorm = 0.2531
Meta loss on this task batch = 2.9583e-01, Meta loss averaged over last 500 steps = 2.7492e-01, PNorm = 170.5865, GNorm = 0.2979
Meta loss on this task batch = 3.0312e-01, Meta loss averaged over last 500 steps = 2.7501e-01, PNorm = 170.5893, GNorm = 0.2653
Meta loss on this task batch = 2.6420e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 170.5921, GNorm = 0.2647
Meta loss on this task batch = 2.4451e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 170.5954, GNorm = 0.2513
Meta loss on this task batch = 2.7732e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 170.5983, GNorm = 0.3404
Took 124.94946503639221 seconds to complete one epoch of meta training
Took 134.4510657787323 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462022
Epoch 941
Meta loss on this task batch = 3.0941e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 170.6014, GNorm = 0.3130
Meta loss on this task batch = 3.1627e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 170.6047, GNorm = 0.3689
Meta loss on this task batch = 2.3414e-01, Meta loss averaged over last 500 steps = 2.7492e-01, PNorm = 170.6087, GNorm = 0.4258
Meta loss on this task batch = 2.7721e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 170.6125, GNorm = 0.2601
Meta loss on this task batch = 2.5315e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 170.6171, GNorm = 0.2432
Meta loss on this task batch = 3.0930e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 170.6216, GNorm = 0.2618
Meta loss on this task batch = 3.1549e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 170.6257, GNorm = 0.3718
Meta loss on this task batch = 2.7757e-01, Meta loss averaged over last 500 steps = 2.7501e-01, PNorm = 170.6301, GNorm = 0.3101
Meta loss on this task batch = 2.4282e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 170.6345, GNorm = 0.2627
Meta loss on this task batch = 3.0972e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 170.6386, GNorm = 0.3119
Meta loss on this task batch = 3.0681e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 170.6425, GNorm = 0.3883
Meta loss on this task batch = 2.8425e-01, Meta loss averaged over last 500 steps = 2.7509e-01, PNorm = 170.6460, GNorm = 0.2906
Meta loss on this task batch = 3.5163e-01, Meta loss averaged over last 500 steps = 2.7517e-01, PNorm = 170.6491, GNorm = 0.4852
Meta loss on this task batch = 2.2873e-01, Meta loss averaged over last 500 steps = 2.7507e-01, PNorm = 170.6526, GNorm = 0.2745
Meta loss on this task batch = 2.6675e-01, Meta loss averaged over last 500 steps = 2.7513e-01, PNorm = 170.6558, GNorm = 0.3917
Meta loss on this task batch = 2.5116e-01, Meta loss averaged over last 500 steps = 2.7513e-01, PNorm = 170.6587, GNorm = 0.5286
Meta loss on this task batch = 2.4008e-01, Meta loss averaged over last 500 steps = 2.7513e-01, PNorm = 170.6616, GNorm = 0.2706
Meta loss on this task batch = 2.6324e-01, Meta loss averaged over last 500 steps = 2.7507e-01, PNorm = 170.6648, GNorm = 0.2992
Meta loss on this task batch = 2.5117e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 170.6679, GNorm = 0.3126
Took 123.61609292030334 seconds to complete one epoch of meta training
Took 132.13784098625183 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.443807
Epoch 942
Meta loss on this task batch = 3.2747e-01, Meta loss averaged over last 500 steps = 2.7521e-01, PNorm = 170.6708, GNorm = 0.2949
Meta loss on this task batch = 2.7503e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 170.6737, GNorm = 0.2960
Meta loss on this task batch = 2.7214e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 170.6762, GNorm = 0.4049
Meta loss on this task batch = 2.3544e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 170.6797, GNorm = 0.2403
Meta loss on this task batch = 2.2223e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 170.6831, GNorm = 0.4282
Meta loss on this task batch = 2.9032e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 170.6870, GNorm = 0.3124
Meta loss on this task batch = 2.4535e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 170.6914, GNorm = 0.2946
Meta loss on this task batch = 2.4590e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 170.6959, GNorm = 0.2440
Meta loss on this task batch = 3.2292e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 170.7006, GNorm = 0.6252
Meta loss on this task batch = 2.1052e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 170.7053, GNorm = 0.2641
Meta loss on this task batch = 2.6972e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 170.7100, GNorm = 0.3917
Meta loss on this task batch = 2.6126e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 170.7147, GNorm = 0.3167
Meta loss on this task batch = 2.7897e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 170.7187, GNorm = 0.3863
Meta loss on this task batch = 2.3677e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 170.7228, GNorm = 0.2351
Meta loss on this task batch = 2.4497e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 170.7267, GNorm = 0.3029
Meta loss on this task batch = 2.5264e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 170.7305, GNorm = 0.4036
Meta loss on this task batch = 3.3323e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 170.7343, GNorm = 0.3478
Meta loss on this task batch = 2.9170e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 170.7379, GNorm = 0.3802
Meta loss on this task batch = 3.1967e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 170.7411, GNorm = 0.4507
Took 130.8241798877716 seconds to complete one epoch of meta training
Took 139.7458200454712 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452078
Epoch 943
Meta loss on this task batch = 2.7097e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 170.7442, GNorm = 0.2962
Meta loss on this task batch = 2.6287e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 170.7478, GNorm = 0.2956
Meta loss on this task batch = 2.7265e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 170.7518, GNorm = 0.3905
Meta loss on this task batch = 3.1500e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 170.7558, GNorm = 0.3051
Meta loss on this task batch = 2.6272e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 170.7601, GNorm = 0.2790
Meta loss on this task batch = 2.9761e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 170.7645, GNorm = 0.4035
Meta loss on this task batch = 3.3249e-01, Meta loss averaged over last 500 steps = 2.7492e-01, PNorm = 170.7685, GNorm = 0.5112
Meta loss on this task batch = 2.1915e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 170.7730, GNorm = 0.3128
Meta loss on this task batch = 2.7724e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 170.7772, GNorm = 0.3004
Meta loss on this task batch = 2.5133e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 170.7811, GNorm = 0.3237
Meta loss on this task batch = 2.8745e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 170.7848, GNorm = 0.2442
Meta loss on this task batch = 3.1535e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 170.7878, GNorm = 0.3812
Meta loss on this task batch = 2.4564e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 170.7911, GNorm = 0.3154
Meta loss on this task batch = 2.5808e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 170.7940, GNorm = 0.2668
Meta loss on this task batch = 2.9264e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 170.7966, GNorm = 0.4085
Meta loss on this task batch = 3.0519e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 170.8000, GNorm = 0.7892
Meta loss on this task batch = 2.7063e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 170.8032, GNorm = 0.2588
Meta loss on this task batch = 2.1838e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 170.8069, GNorm = 0.2728
Meta loss on this task batch = 2.4709e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 170.8109, GNorm = 0.2693
Took 130.37789011001587 seconds to complete one epoch of meta training
Took 139.51084232330322 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.443413
Epoch 944
Meta loss on this task batch = 3.0994e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 170.8150, GNorm = 0.2359
Meta loss on this task batch = 2.4490e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 170.8189, GNorm = 0.3026
Meta loss on this task batch = 2.9775e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 170.8223, GNorm = 0.3250
Meta loss on this task batch = 2.9141e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 170.8256, GNorm = 0.2903
Meta loss on this task batch = 2.5043e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 170.8294, GNorm = 0.3174
Meta loss on this task batch = 2.9468e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 170.8329, GNorm = 0.3710
Meta loss on this task batch = 2.5204e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 170.8364, GNorm = 0.3072
Meta loss on this task batch = 2.8447e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 170.8400, GNorm = 0.2936
Meta loss on this task batch = 2.4861e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 170.8441, GNorm = 0.2808
Meta loss on this task batch = 2.8434e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 170.8483, GNorm = 0.2654
Meta loss on this task batch = 3.2717e-01, Meta loss averaged over last 500 steps = 2.7498e-01, PNorm = 170.8525, GNorm = 0.3414
Meta loss on this task batch = 2.7375e-01, Meta loss averaged over last 500 steps = 2.7489e-01, PNorm = 170.8575, GNorm = 0.2947
Meta loss on this task batch = 2.8740e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 170.8624, GNorm = 0.2665
Meta loss on this task batch = 2.8334e-01, Meta loss averaged over last 500 steps = 2.7498e-01, PNorm = 170.8671, GNorm = 0.2675
Meta loss on this task batch = 2.2443e-01, Meta loss averaged over last 500 steps = 2.7492e-01, PNorm = 170.8718, GNorm = 0.3309
Meta loss on this task batch = 2.6258e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 170.8768, GNorm = 0.2916
Meta loss on this task batch = 2.8343e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 170.8822, GNorm = 0.2691
Meta loss on this task batch = 3.1001e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 170.8869, GNorm = 0.3017
Meta loss on this task batch = 3.2002e-01, Meta loss averaged over last 500 steps = 2.7518e-01, PNorm = 170.8923, GNorm = 0.3195
Took 117.99977040290833 seconds to complete one epoch of meta training
Took 126.77861714363098 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463593
Epoch 945
Meta loss on this task batch = 2.7337e-01, Meta loss averaged over last 500 steps = 2.7521e-01, PNorm = 170.8972, GNorm = 0.3535
Meta loss on this task batch = 2.9895e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 170.9016, GNorm = 0.3422
Meta loss on this task batch = 2.5751e-01, Meta loss averaged over last 500 steps = 2.7523e-01, PNorm = 170.9061, GNorm = 0.2975
Meta loss on this task batch = 3.1077e-01, Meta loss averaged over last 500 steps = 2.7528e-01, PNorm = 170.9100, GNorm = 0.4589
Meta loss on this task batch = 2.5531e-01, Meta loss averaged over last 500 steps = 2.7519e-01, PNorm = 170.9143, GNorm = 0.2370
Meta loss on this task batch = 2.8444e-01, Meta loss averaged over last 500 steps = 2.7507e-01, PNorm = 170.9186, GNorm = 0.2932
Meta loss on this task batch = 2.2128e-01, Meta loss averaged over last 500 steps = 2.7498e-01, PNorm = 170.9229, GNorm = 0.3343
Meta loss on this task batch = 3.2109e-01, Meta loss averaged over last 500 steps = 2.7510e-01, PNorm = 170.9270, GNorm = 0.4288
Meta loss on this task batch = 2.8824e-01, Meta loss averaged over last 500 steps = 2.7519e-01, PNorm = 170.9311, GNorm = 0.3188
Meta loss on this task batch = 2.4882e-01, Meta loss averaged over last 500 steps = 2.7526e-01, PNorm = 170.9355, GNorm = 0.2612
Meta loss on this task batch = 3.2259e-01, Meta loss averaged over last 500 steps = 2.7539e-01, PNorm = 170.9395, GNorm = 0.2692
Meta loss on this task batch = 2.8004e-01, Meta loss averaged over last 500 steps = 2.7530e-01, PNorm = 170.9438, GNorm = 0.3174
Meta loss on this task batch = 2.8714e-01, Meta loss averaged over last 500 steps = 2.7547e-01, PNorm = 170.9481, GNorm = 0.3248
Meta loss on this task batch = 2.7344e-01, Meta loss averaged over last 500 steps = 2.7547e-01, PNorm = 170.9534, GNorm = 0.2804
Meta loss on this task batch = 2.7767e-01, Meta loss averaged over last 500 steps = 2.7550e-01, PNorm = 170.9584, GNorm = 0.2885
Meta loss on this task batch = 2.7903e-01, Meta loss averaged over last 500 steps = 2.7545e-01, PNorm = 170.9638, GNorm = 0.4091
Meta loss on this task batch = 2.6021e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 170.9690, GNorm = 0.2217
Meta loss on this task batch = 2.4301e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 170.9743, GNorm = 0.4400
Meta loss on this task batch = 2.8352e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 170.9791, GNorm = 0.3049
Took 119.63045883178711 seconds to complete one epoch of meta training
Took 128.27270460128784 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458345
Epoch 946
Meta loss on this task batch = 2.4087e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 170.9837, GNorm = 0.2520
Meta loss on this task batch = 2.7609e-01, Meta loss averaged over last 500 steps = 2.7530e-01, PNorm = 170.9887, GNorm = 0.2648
Meta loss on this task batch = 2.8794e-01, Meta loss averaged over last 500 steps = 2.7519e-01, PNorm = 170.9933, GNorm = 0.2572
Meta loss on this task batch = 2.9059e-01, Meta loss averaged over last 500 steps = 2.7514e-01, PNorm = 170.9981, GNorm = 0.2893
Meta loss on this task batch = 3.4327e-01, Meta loss averaged over last 500 steps = 2.7522e-01, PNorm = 171.0025, GNorm = 0.3087
Meta loss on this task batch = 2.4554e-01, Meta loss averaged over last 500 steps = 2.7520e-01, PNorm = 171.0070, GNorm = 0.2724
Meta loss on this task batch = 2.8322e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 171.0115, GNorm = 0.3596
Meta loss on this task batch = 2.1069e-01, Meta loss averaged over last 500 steps = 2.7518e-01, PNorm = 171.0161, GNorm = 0.3106
Meta loss on this task batch = 2.3950e-01, Meta loss averaged over last 500 steps = 2.7511e-01, PNorm = 171.0207, GNorm = 0.2434
Meta loss on this task batch = 2.9650e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 171.0246, GNorm = 0.4179
Meta loss on this task batch = 2.9396e-01, Meta loss averaged over last 500 steps = 2.7525e-01, PNorm = 171.0284, GNorm = 0.2937
Meta loss on this task batch = 3.0886e-01, Meta loss averaged over last 500 steps = 2.7540e-01, PNorm = 171.0323, GNorm = 0.3390
Meta loss on this task batch = 2.8420e-01, Meta loss averaged over last 500 steps = 2.7545e-01, PNorm = 171.0367, GNorm = 0.2907
Meta loss on this task batch = 2.4517e-01, Meta loss averaged over last 500 steps = 2.7546e-01, PNorm = 171.0408, GNorm = 0.3273
Meta loss on this task batch = 3.0212e-01, Meta loss averaged over last 500 steps = 2.7543e-01, PNorm = 171.0447, GNorm = 0.3015
Meta loss on this task batch = 2.8267e-01, Meta loss averaged over last 500 steps = 2.7546e-01, PNorm = 171.0484, GNorm = 0.2882
Meta loss on this task batch = 2.7445e-01, Meta loss averaged over last 500 steps = 2.7547e-01, PNorm = 171.0526, GNorm = 0.2877
Meta loss on this task batch = 2.3428e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 171.0571, GNorm = 0.2526
Meta loss on this task batch = 2.6695e-01, Meta loss averaged over last 500 steps = 2.7547e-01, PNorm = 171.0608, GNorm = 0.3650
Took 119.63403415679932 seconds to complete one epoch of meta training
Took 128.10556292533875 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466859
Epoch 947
Meta loss on this task batch = 2.6391e-01, Meta loss averaged over last 500 steps = 2.7540e-01, PNorm = 171.0651, GNorm = 0.3045
Meta loss on this task batch = 2.6640e-01, Meta loss averaged over last 500 steps = 2.7537e-01, PNorm = 171.0697, GNorm = 0.2723
Meta loss on this task batch = 2.7173e-01, Meta loss averaged over last 500 steps = 2.7534e-01, PNorm = 171.0740, GNorm = 0.2805
Meta loss on this task batch = 2.7031e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 171.0780, GNorm = 0.2297
Meta loss on this task batch = 2.5572e-01, Meta loss averaged over last 500 steps = 2.7515e-01, PNorm = 171.0819, GNorm = 0.3720
Meta loss on this task batch = 1.9969e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 171.0861, GNorm = 0.2192
Meta loss on this task batch = 2.9450e-01, Meta loss averaged over last 500 steps = 2.7511e-01, PNorm = 171.0900, GNorm = 0.4049
Meta loss on this task batch = 2.3950e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 171.0944, GNorm = 0.2743
Meta loss on this task batch = 2.5717e-01, Meta loss averaged over last 500 steps = 2.7500e-01, PNorm = 171.0989, GNorm = 0.3716
Meta loss on this task batch = 2.6087e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 171.1030, GNorm = 0.2953
Meta loss on this task batch = 2.8089e-01, Meta loss averaged over last 500 steps = 2.7504e-01, PNorm = 171.1071, GNorm = 0.3007
Meta loss on this task batch = 3.3187e-01, Meta loss averaged over last 500 steps = 2.7521e-01, PNorm = 171.1112, GNorm = 0.3328
Meta loss on this task batch = 2.8047e-01, Meta loss averaged over last 500 steps = 2.7524e-01, PNorm = 171.1151, GNorm = 0.2754
Meta loss on this task batch = 2.4057e-01, Meta loss averaged over last 500 steps = 2.7522e-01, PNorm = 171.1194, GNorm = 0.2689
Meta loss on this task batch = 3.1201e-01, Meta loss averaged over last 500 steps = 2.7543e-01, PNorm = 171.1232, GNorm = 0.3387
Meta loss on this task batch = 3.1897e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 171.1264, GNorm = 0.2863
Meta loss on this task batch = 2.5708e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 171.1293, GNorm = 0.3014
Meta loss on this task batch = 3.0534e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 171.1319, GNorm = 0.3007
Meta loss on this task batch = 2.8794e-01, Meta loss averaged over last 500 steps = 2.7557e-01, PNorm = 171.1335, GNorm = 0.4076
Took 123.9632420539856 seconds to complete one epoch of meta training
Took 132.12768507003784 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464670
Epoch 948
Meta loss on this task batch = 3.0771e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 171.1354, GNorm = 0.3227
Meta loss on this task batch = 2.2799e-01, Meta loss averaged over last 500 steps = 2.7533e-01, PNorm = 171.1373, GNorm = 0.3286
Meta loss on this task batch = 2.7614e-01, Meta loss averaged over last 500 steps = 2.7531e-01, PNorm = 171.1398, GNorm = 0.3077
Meta loss on this task batch = 2.6663e-01, Meta loss averaged over last 500 steps = 2.7533e-01, PNorm = 171.1421, GNorm = 0.3083
Meta loss on this task batch = 2.7258e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 171.1444, GNorm = 0.2756
Meta loss on this task batch = 2.4255e-01, Meta loss averaged over last 500 steps = 2.7526e-01, PNorm = 171.1468, GNorm = 0.3299
Meta loss on this task batch = 2.5726e-01, Meta loss averaged over last 500 steps = 2.7521e-01, PNorm = 171.1494, GNorm = 0.2486
Meta loss on this task batch = 2.3029e-01, Meta loss averaged over last 500 steps = 2.7517e-01, PNorm = 171.1519, GNorm = 0.2257
Meta loss on this task batch = 2.9867e-01, Meta loss averaged over last 500 steps = 2.7524e-01, PNorm = 171.1543, GNorm = 0.3153
Meta loss on this task batch = 2.8692e-01, Meta loss averaged over last 500 steps = 2.7517e-01, PNorm = 171.1563, GNorm = 0.3112
Meta loss on this task batch = 2.8100e-01, Meta loss averaged over last 500 steps = 2.7511e-01, PNorm = 171.1586, GNorm = 0.2664
Meta loss on this task batch = 2.9476e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 171.1614, GNorm = 0.4591
Meta loss on this task batch = 2.4337e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 171.1645, GNorm = 0.2667
Meta loss on this task batch = 3.0248e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 171.1683, GNorm = 0.3615
Meta loss on this task batch = 2.6357e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 171.1726, GNorm = 0.2929
Meta loss on this task batch = 2.5338e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 171.1773, GNorm = 0.2736
Meta loss on this task batch = 2.7299e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 171.1825, GNorm = 0.3827
Meta loss on this task batch = 2.8837e-01, Meta loss averaged over last 500 steps = 2.7489e-01, PNorm = 171.1880, GNorm = 0.3050
Meta loss on this task batch = 2.6308e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 171.1935, GNorm = 0.3239
Took 124.50173950195312 seconds to complete one epoch of meta training
Took 133.54285740852356 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458080
Epoch 949
Meta loss on this task batch = 2.3588e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 171.1991, GNorm = 0.3055
Meta loss on this task batch = 2.9048e-01, Meta loss averaged over last 500 steps = 2.7482e-01, PNorm = 171.2040, GNorm = 0.2683
Meta loss on this task batch = 2.5755e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 171.2094, GNorm = 0.2768
Meta loss on this task batch = 2.9412e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 171.2142, GNorm = 0.3403
Meta loss on this task batch = 2.4669e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 171.2184, GNorm = 0.2660
Meta loss on this task batch = 2.6820e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 171.2221, GNorm = 0.4190
Meta loss on this task batch = 3.1229e-01, Meta loss averaged over last 500 steps = 2.7492e-01, PNorm = 171.2258, GNorm = 0.2837
Meta loss on this task batch = 2.3966e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 171.2294, GNorm = 0.2458
Meta loss on this task batch = 2.7871e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 171.2328, GNorm = 0.2974
Meta loss on this task batch = 2.5545e-01, Meta loss averaged over last 500 steps = 2.7489e-01, PNorm = 171.2370, GNorm = 0.3164
Meta loss on this task batch = 2.6041e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 171.2407, GNorm = 0.3857
Meta loss on this task batch = 2.7194e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 171.2449, GNorm = 0.2689
Meta loss on this task batch = 3.1107e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 171.2484, GNorm = 0.3685
Meta loss on this task batch = 3.2272e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 171.2514, GNorm = 0.2787
Meta loss on this task batch = 2.7641e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 171.2547, GNorm = 0.3313
Meta loss on this task batch = 2.2771e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 171.2585, GNorm = 0.2686
Meta loss on this task batch = 2.9872e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 171.2622, GNorm = 0.3587
Meta loss on this task batch = 2.8294e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 171.2654, GNorm = 0.3176
Meta loss on this task batch = 3.0428e-01, Meta loss averaged over last 500 steps = 2.7508e-01, PNorm = 171.2689, GNorm = 0.3023
Took 127.56716728210449 seconds to complete one epoch of meta training
Took 136.90609097480774 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.441607
Epoch 950
Meta loss on this task batch = 3.0663e-01, Meta loss averaged over last 500 steps = 2.7515e-01, PNorm = 171.2717, GNorm = 0.3124
Meta loss on this task batch = 2.3590e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 171.2748, GNorm = 0.2672
Meta loss on this task batch = 2.7077e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 171.2778, GNorm = 0.2828
Meta loss on this task batch = 3.1532e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 171.2808, GNorm = 0.2991
Meta loss on this task batch = 2.3842e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 171.2841, GNorm = 0.2598
Meta loss on this task batch = 2.6998e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 171.2880, GNorm = 0.2799
Meta loss on this task batch = 3.3130e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 171.2909, GNorm = 0.3704
Meta loss on this task batch = 2.9919e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 171.2930, GNorm = 0.2532
Meta loss on this task batch = 2.3490e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 171.2956, GNorm = 0.3317
Meta loss on this task batch = 2.8205e-01, Meta loss averaged over last 500 steps = 2.7482e-01, PNorm = 171.2984, GNorm = 0.2641
Meta loss on this task batch = 2.6236e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 171.3012, GNorm = 0.2883
Meta loss on this task batch = 3.0798e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 171.3038, GNorm = 0.3096
Meta loss on this task batch = 3.2852e-01, Meta loss averaged over last 500 steps = 2.7504e-01, PNorm = 171.3061, GNorm = 0.3143
Meta loss on this task batch = 2.6425e-01, Meta loss averaged over last 500 steps = 2.7504e-01, PNorm = 171.3082, GNorm = 0.2821
Meta loss on this task batch = 2.8687e-01, Meta loss averaged over last 500 steps = 2.7503e-01, PNorm = 171.3102, GNorm = 0.3362
Meta loss on this task batch = 2.6825e-01, Meta loss averaged over last 500 steps = 2.7500e-01, PNorm = 171.3128, GNorm = 0.2961
Meta loss on this task batch = 2.7842e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 171.3152, GNorm = 0.4370
Meta loss on this task batch = 2.5225e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 171.3179, GNorm = 0.2918
Meta loss on this task batch = 2.0066e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 171.3220, GNorm = 0.2934
Took 126.28181052207947 seconds to complete one epoch of meta training
Took 134.76319670677185 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446269
Epoch 951
Meta loss on this task batch = 3.0164e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 171.3262, GNorm = 0.2709
Meta loss on this task batch = 2.6270e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 171.3299, GNorm = 0.3094
Meta loss on this task batch = 2.5889e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 171.3345, GNorm = 0.3531
Meta loss on this task batch = 2.9579e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 171.3399, GNorm = 0.3230
Meta loss on this task batch = 2.9678e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 171.3445, GNorm = 0.3946
Meta loss on this task batch = 2.6568e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 171.3484, GNorm = 0.3169
Meta loss on this task batch = 2.9696e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 171.3525, GNorm = 0.2379
Meta loss on this task batch = 3.2026e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 171.3561, GNorm = 0.5308
Meta loss on this task batch = 2.5672e-01, Meta loss averaged over last 500 steps = 2.7489e-01, PNorm = 171.3600, GNorm = 0.2736
Meta loss on this task batch = 2.6716e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 171.3638, GNorm = 0.3610
Meta loss on this task batch = 2.4845e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 171.3682, GNorm = 0.3592
Meta loss on this task batch = 2.6952e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 171.3728, GNorm = 0.2655
Meta loss on this task batch = 2.6568e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 171.3769, GNorm = 0.2332
Meta loss on this task batch = 2.3736e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 171.3812, GNorm = 0.2548
Meta loss on this task batch = 3.0612e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 171.3855, GNorm = 0.4313
Meta loss on this task batch = 2.7784e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 171.3898, GNorm = 0.3947
Meta loss on this task batch = 2.9148e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 171.3941, GNorm = 0.2501
Meta loss on this task batch = 2.5324e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 171.3979, GNorm = 0.2832
Meta loss on this task batch = 2.1440e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 171.4021, GNorm = 0.5428
Took 130.94281435012817 seconds to complete one epoch of meta training
Took 139.4800386428833 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456764
Epoch 952
Meta loss on this task batch = 2.5310e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 171.4065, GNorm = 0.2967
Meta loss on this task batch = 2.7292e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 171.4102, GNorm = 0.2882
Meta loss on this task batch = 2.9254e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 171.4143, GNorm = 0.3737
Meta loss on this task batch = 2.5221e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 171.4178, GNorm = 0.2770
Meta loss on this task batch = 2.4666e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 171.4214, GNorm = 0.2646
Meta loss on this task batch = 2.5020e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 171.4256, GNorm = 0.2865
Meta loss on this task batch = 2.8974e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 171.4288, GNorm = 0.4203
Meta loss on this task batch = 2.8936e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 171.4324, GNorm = 0.3220
Meta loss on this task batch = 2.3916e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 171.4359, GNorm = 0.3006
Meta loss on this task batch = 2.7052e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 171.4401, GNorm = 0.3989
Meta loss on this task batch = 3.3800e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 171.4432, GNorm = 0.2850
Meta loss on this task batch = 3.0926e-01, Meta loss averaged over last 500 steps = 2.7482e-01, PNorm = 171.4466, GNorm = 0.3793
Meta loss on this task batch = 2.9961e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 171.4494, GNorm = 0.3691
Meta loss on this task batch = 2.6301e-01, Meta loss averaged over last 500 steps = 2.7492e-01, PNorm = 171.4520, GNorm = 0.2515
Meta loss on this task batch = 3.1216e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 171.4551, GNorm = 0.2792
Meta loss on this task batch = 2.5453e-01, Meta loss averaged over last 500 steps = 2.7501e-01, PNorm = 171.4590, GNorm = 0.2959
Meta loss on this task batch = 2.7469e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 171.4629, GNorm = 0.2981
Meta loss on this task batch = 2.7355e-01, Meta loss averaged over last 500 steps = 2.7492e-01, PNorm = 171.4667, GNorm = 0.3208
Meta loss on this task batch = 3.2642e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 171.4708, GNorm = 0.4345
Took 127.01567649841309 seconds to complete one epoch of meta training
Took 136.32335233688354 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448184
Epoch 953
Meta loss on this task batch = 2.5626e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 171.4755, GNorm = 0.2760
Meta loss on this task batch = 2.8450e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 171.4799, GNorm = 0.3575
Meta loss on this task batch = 2.5167e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 171.4850, GNorm = 0.4040
Meta loss on this task batch = 2.7394e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 171.4903, GNorm = 0.3063
Meta loss on this task batch = 2.7505e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 171.4958, GNorm = 0.2856
Meta loss on this task batch = 2.5433e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 171.5011, GNorm = 0.3126
Meta loss on this task batch = 3.4239e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 171.5065, GNorm = 0.3416
Meta loss on this task batch = 2.4651e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 171.5110, GNorm = 0.2638
Meta loss on this task batch = 2.8947e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 171.5150, GNorm = 0.3138
Meta loss on this task batch = 3.0615e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 171.5190, GNorm = 0.3795
Meta loss on this task batch = 2.6726e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 171.5224, GNorm = 0.2810
Meta loss on this task batch = 2.8418e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 171.5253, GNorm = 0.2401
Meta loss on this task batch = 2.0821e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 171.5287, GNorm = 0.3191
Meta loss on this task batch = 3.1681e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 171.5318, GNorm = 0.2990
Meta loss on this task batch = 2.8301e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 171.5352, GNorm = 0.4703
Meta loss on this task batch = 2.8120e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 171.5388, GNorm = 0.3089
Meta loss on this task batch = 3.1670e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 171.5417, GNorm = 0.2915
Meta loss on this task batch = 2.6117e-01, Meta loss averaged over last 500 steps = 2.7482e-01, PNorm = 171.5446, GNorm = 0.3396
Meta loss on this task batch = 2.5380e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 171.5476, GNorm = 0.2816
Took 128.206778049469 seconds to complete one epoch of meta training
Took 136.86981296539307 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458563
Epoch 954
Meta loss on this task batch = 2.8806e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 171.5508, GNorm = 0.2772
Meta loss on this task batch = 2.6626e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 171.5536, GNorm = 0.3374
Meta loss on this task batch = 2.3690e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 171.5568, GNorm = 0.2699
Meta loss on this task batch = 2.2176e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 171.5605, GNorm = 0.2719
Meta loss on this task batch = 2.9751e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 171.5644, GNorm = 0.3121
Meta loss on this task batch = 2.8196e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 171.5683, GNorm = 0.3300
Meta loss on this task batch = 2.4816e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 171.5723, GNorm = 0.2982
Meta loss on this task batch = 2.8902e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 171.5765, GNorm = 0.2445
Meta loss on this task batch = 3.3633e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 171.5803, GNorm = 0.3619
Meta loss on this task batch = 2.2093e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 171.5843, GNorm = 0.2607
Meta loss on this task batch = 2.8637e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 171.5881, GNorm = 0.3950
Meta loss on this task batch = 2.7748e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 171.5916, GNorm = 0.2867
Meta loss on this task batch = 2.2581e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 171.5957, GNorm = 0.2247
Meta loss on this task batch = 2.7215e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 171.5999, GNorm = 0.2876
Meta loss on this task batch = 3.3115e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 171.6029, GNorm = 0.3812
Meta loss on this task batch = 2.5917e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 171.6056, GNorm = 0.3090
Meta loss on this task batch = 3.2082e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 171.6083, GNorm = 0.4073
Meta loss on this task batch = 3.0114e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 171.6109, GNorm = 0.2761
Meta loss on this task batch = 2.5039e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 171.6138, GNorm = 0.3131
Took 127.71738910675049 seconds to complete one epoch of meta training
Took 135.77712631225586 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446515
Epoch 955
Meta loss on this task batch = 2.0724e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 171.6170, GNorm = 0.2896
Meta loss on this task batch = 2.3914e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 171.6205, GNorm = 0.2882
Meta loss on this task batch = 2.6372e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 171.6241, GNorm = 0.4247
Meta loss on this task batch = 2.3842e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 171.6277, GNorm = 0.2140
Meta loss on this task batch = 3.0269e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 171.6323, GNorm = 0.3085
Meta loss on this task batch = 2.5629e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 171.6363, GNorm = 0.3877
Meta loss on this task batch = 2.4813e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 171.6398, GNorm = 0.3491
Meta loss on this task batch = 2.6264e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 171.6429, GNorm = 0.3387
Meta loss on this task batch = 2.7830e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 171.6463, GNorm = 0.2891
Meta loss on this task batch = 3.4745e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 171.6492, GNorm = 0.3404
Meta loss on this task batch = 2.9733e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 171.6521, GNorm = 0.2789
Meta loss on this task batch = 2.8738e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 171.6552, GNorm = 0.2480
Meta loss on this task batch = 3.3009e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 171.6582, GNorm = 0.3661
Meta loss on this task batch = 2.6746e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 171.6613, GNorm = 0.3718
Meta loss on this task batch = 2.7452e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 171.6640, GNorm = 0.2683
Meta loss on this task batch = 2.7150e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 171.6662, GNorm = 0.2723
Meta loss on this task batch = 2.1457e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 171.6696, GNorm = 0.2335
Meta loss on this task batch = 3.1720e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 171.6733, GNorm = 0.3762
Meta loss on this task batch = 3.2883e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 171.6769, GNorm = 0.3960
Took 133.2703239917755 seconds to complete one epoch of meta training
Took 142.54498052597046 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456571
Epoch 956
Meta loss on this task batch = 2.8601e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 171.6803, GNorm = 0.2388
Meta loss on this task batch = 2.5352e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 171.6843, GNorm = 0.3280
Meta loss on this task batch = 2.7276e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 171.6887, GNorm = 0.3262
Meta loss on this task batch = 2.7525e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 171.6928, GNorm = 0.2633
Meta loss on this task batch = 2.8921e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 171.6972, GNorm = 0.3966
Meta loss on this task batch = 2.7831e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 171.7017, GNorm = 0.3034
Meta loss on this task batch = 2.6863e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 171.7065, GNorm = 0.3654
Meta loss on this task batch = 2.9922e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 171.7114, GNorm = 0.2641
Meta loss on this task batch = 2.5106e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 171.7165, GNorm = 0.3779
Meta loss on this task batch = 2.5906e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 171.7214, GNorm = 0.2438
Meta loss on this task batch = 2.7212e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 171.7260, GNorm = 0.3057
Meta loss on this task batch = 3.0555e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 171.7312, GNorm = 0.3118
Meta loss on this task batch = 2.8573e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 171.7364, GNorm = 0.3118
Meta loss on this task batch = 2.2483e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 171.7411, GNorm = 0.2437
Meta loss on this task batch = 3.1613e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 171.7451, GNorm = 0.4061
Meta loss on this task batch = 3.1024e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 171.7488, GNorm = 0.3466
Meta loss on this task batch = 2.9740e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 171.7521, GNorm = 0.4302
Meta loss on this task batch = 2.3320e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 171.7550, GNorm = 0.2618
Meta loss on this task batch = 3.0160e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 171.7578, GNorm = 0.4160
Took 126.06179141998291 seconds to complete one epoch of meta training
Took 134.57776379585266 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446124
Epoch 957
Meta loss on this task batch = 2.5531e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 171.7606, GNorm = 0.2660
Meta loss on this task batch = 3.0337e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 171.7637, GNorm = 0.2673
Meta loss on this task batch = 2.4723e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 171.7666, GNorm = 0.2394
Meta loss on this task batch = 2.4408e-01, Meta loss averaged over last 500 steps = 2.7482e-01, PNorm = 171.7698, GNorm = 0.3860
Meta loss on this task batch = 3.0122e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 171.7727, GNorm = 0.3527
Meta loss on this task batch = 2.4662e-01, Meta loss averaged over last 500 steps = 2.7482e-01, PNorm = 171.7758, GNorm = 0.2416
Meta loss on this task batch = 2.5841e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 171.7792, GNorm = 0.2903
Meta loss on this task batch = 2.7639e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 171.7826, GNorm = 0.3142
Meta loss on this task batch = 2.9017e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 171.7858, GNorm = 0.2502
Meta loss on this task batch = 2.4498e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 171.7893, GNorm = 0.2388
Meta loss on this task batch = 2.7372e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 171.7925, GNorm = 0.3713
Meta loss on this task batch = 2.6583e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 171.7958, GNorm = 0.2567
Meta loss on this task batch = 2.2004e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 171.7994, GNorm = 0.2446
Meta loss on this task batch = 3.0765e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 171.8025, GNorm = 0.2924
Meta loss on this task batch = 3.2316e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 171.8056, GNorm = 0.2838
Meta loss on this task batch = 3.4758e-01, Meta loss averaged over last 500 steps = 2.7515e-01, PNorm = 171.8077, GNorm = 0.3300
Meta loss on this task batch = 2.7333e-01, Meta loss averaged over last 500 steps = 2.7519e-01, PNorm = 171.8095, GNorm = 0.3237
Meta loss on this task batch = 2.7517e-01, Meta loss averaged over last 500 steps = 2.7522e-01, PNorm = 171.8120, GNorm = 0.3414
Meta loss on this task batch = 2.3632e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 171.8149, GNorm = 0.3673
Took 135.42101383209229 seconds to complete one epoch of meta training
Took 143.8494462966919 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463256
Epoch 958
Meta loss on this task batch = 2.3700e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 171.8181, GNorm = 0.2107
Meta loss on this task batch = 2.7753e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 171.8216, GNorm = 0.2634
Meta loss on this task batch = 2.9856e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 171.8255, GNorm = 0.3447
Meta loss on this task batch = 3.0988e-01, Meta loss averaged over last 500 steps = 2.7504e-01, PNorm = 171.8298, GNorm = 0.3131
Meta loss on this task batch = 3.0237e-01, Meta loss averaged over last 500 steps = 2.7501e-01, PNorm = 171.8340, GNorm = 0.3145
Meta loss on this task batch = 2.6958e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 171.8383, GNorm = 0.2854
Meta loss on this task batch = 2.4269e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 171.8427, GNorm = 0.3879
Meta loss on this task batch = 2.7472e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 171.8471, GNorm = 0.2762
Meta loss on this task batch = 2.2162e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 171.8520, GNorm = 0.3238
Meta loss on this task batch = 2.8386e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 171.8566, GNorm = 0.3827
Meta loss on this task batch = 2.4120e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 171.8611, GNorm = 0.3224
Meta loss on this task batch = 3.0458e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 171.8653, GNorm = 0.2697
Meta loss on this task batch = 2.3806e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 171.8697, GNorm = 0.2348
Meta loss on this task batch = 2.7268e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 171.8746, GNorm = 0.2997
Meta loss on this task batch = 2.9410e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 171.8789, GNorm = 0.2755
Meta loss on this task batch = 2.4291e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 171.8836, GNorm = 0.2927
Meta loss on this task batch = 2.8586e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 171.8885, GNorm = 0.2845
Meta loss on this task batch = 2.8010e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 171.8936, GNorm = 0.2588
Meta loss on this task batch = 3.0338e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 171.8987, GNorm = 0.3845
Took 141.98266577720642 seconds to complete one epoch of meta training
Took 151.1802637577057 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459654
Epoch 959
Meta loss on this task batch = 2.6851e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 171.9035, GNorm = 0.3528
Meta loss on this task batch = 3.1649e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 171.9078, GNorm = 0.3135
Meta loss on this task batch = 2.8714e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 171.9116, GNorm = 0.2962
Meta loss on this task batch = 2.6043e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 171.9156, GNorm = 0.2363
Meta loss on this task batch = 2.3400e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 171.9195, GNorm = 0.4241
Meta loss on this task batch = 3.2113e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 171.9230, GNorm = 0.3445
Meta loss on this task batch = 2.5365e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 171.9267, GNorm = 0.2940
Meta loss on this task batch = 2.7563e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 171.9302, GNorm = 0.4729
Meta loss on this task batch = 2.3737e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 171.9340, GNorm = 0.2888
Meta loss on this task batch = 2.5545e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 171.9377, GNorm = 0.4431
Meta loss on this task batch = 3.3579e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 171.9414, GNorm = 0.2793
Meta loss on this task batch = 2.6317e-01, Meta loss averaged over last 500 steps = 2.7498e-01, PNorm = 171.9454, GNorm = 0.4028
Meta loss on this task batch = 2.4586e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 171.9491, GNorm = 0.2788
Meta loss on this task batch = 2.5268e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 171.9533, GNorm = 0.2335
Meta loss on this task batch = 2.8541e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 171.9569, GNorm = 0.2908
Meta loss on this task batch = 2.3335e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 171.9603, GNorm = 0.2416
Meta loss on this task batch = 2.4265e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 171.9640, GNorm = 0.2657
Meta loss on this task batch = 2.4491e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 171.9675, GNorm = 0.3166
Meta loss on this task batch = 3.2130e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 171.9704, GNorm = 0.3760
Took 142.10404515266418 seconds to complete one epoch of meta training
Took 150.90800166130066 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464043
Epoch 960
Meta loss on this task batch = 2.7668e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 171.9732, GNorm = 0.2851
Meta loss on this task batch = 2.6018e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 171.9763, GNorm = 0.3343
Meta loss on this task batch = 3.0278e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 171.9793, GNorm = 0.2953
Meta loss on this task batch = 2.8373e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 171.9823, GNorm = 0.2694
Meta loss on this task batch = 3.1432e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 171.9854, GNorm = 0.3565
Meta loss on this task batch = 2.4002e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 171.9894, GNorm = 0.2983
Meta loss on this task batch = 2.6914e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 171.9936, GNorm = 0.3212
Meta loss on this task batch = 2.5257e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 171.9980, GNorm = 0.2534
Meta loss on this task batch = 2.6367e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 172.0015, GNorm = 0.2993
Meta loss on this task batch = 2.7275e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 172.0052, GNorm = 0.4017
Meta loss on this task batch = 2.7790e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 172.0090, GNorm = 0.2986
Meta loss on this task batch = 2.7722e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 172.0133, GNorm = 0.3192
Meta loss on this task batch = 2.5312e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 172.0179, GNorm = 0.2367
Meta loss on this task batch = 3.0943e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 172.0224, GNorm = 0.2656
Meta loss on this task batch = 2.9518e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 172.0268, GNorm = 0.2903
Meta loss on this task batch = 2.5656e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 172.0306, GNorm = 0.3190
Meta loss on this task batch = 2.4991e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 172.0344, GNorm = 0.2361
Meta loss on this task batch = 2.6274e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 172.0384, GNorm = 0.3596
Meta loss on this task batch = 2.4377e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 172.0427, GNorm = 0.3109
Took 124.98074746131897 seconds to complete one epoch of meta training
Took 133.32836890220642 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471951
Epoch 961
Meta loss on this task batch = 2.7262e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 172.0473, GNorm = 0.2883
Meta loss on this task batch = 1.8334e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 172.0525, GNorm = 0.2585
Meta loss on this task batch = 3.0452e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 172.0570, GNorm = 0.3723
Meta loss on this task batch = 2.6203e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 172.0617, GNorm = 0.3168
Meta loss on this task batch = 2.4544e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 172.0669, GNorm = 0.2423
Meta loss on this task batch = 2.6840e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 172.0716, GNorm = 0.2691
Meta loss on this task batch = 3.3835e-01, Meta loss averaged over last 500 steps = 2.7412e-01, PNorm = 172.0753, GNorm = 0.2999
Meta loss on this task batch = 2.1833e-01, Meta loss averaged over last 500 steps = 2.7394e-01, PNorm = 172.0789, GNorm = 0.3454
Meta loss on this task batch = 2.7580e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 172.0824, GNorm = 0.3404
Meta loss on this task batch = 3.0488e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 172.0854, GNorm = 0.3788
Meta loss on this task batch = 2.8948e-01, Meta loss averaged over last 500 steps = 2.7393e-01, PNorm = 172.0873, GNorm = 0.3391
Meta loss on this task batch = 3.1398e-01, Meta loss averaged over last 500 steps = 2.7394e-01, PNorm = 172.0884, GNorm = 0.4074
Meta loss on this task batch = 3.0457e-01, Meta loss averaged over last 500 steps = 2.7399e-01, PNorm = 172.0887, GNorm = 0.2673
Meta loss on this task batch = 2.0803e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 172.0893, GNorm = 0.2381
Meta loss on this task batch = 3.0152e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 172.0905, GNorm = 0.4202
Meta loss on this task batch = 3.2630e-01, Meta loss averaged over last 500 steps = 2.7400e-01, PNorm = 172.0911, GNorm = 0.3313
Meta loss on this task batch = 2.5090e-01, Meta loss averaged over last 500 steps = 2.7401e-01, PNorm = 172.0932, GNorm = 0.3787
Meta loss on this task batch = 2.6655e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 172.0949, GNorm = 0.2611
Meta loss on this task batch = 2.4015e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 172.0975, GNorm = 0.2617
Took 132.23475527763367 seconds to complete one epoch of meta training
Took 141.2399342060089 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446092
Epoch 962
Meta loss on this task batch = 2.9827e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 172.1004, GNorm = 0.2693
Meta loss on this task batch = 3.0685e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 172.1032, GNorm = 0.3056
Meta loss on this task batch = 2.6430e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 172.1066, GNorm = 0.3431
Meta loss on this task batch = 2.8523e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 172.1104, GNorm = 0.2151
Meta loss on this task batch = 2.9769e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 172.1141, GNorm = 0.2913
Meta loss on this task batch = 2.5922e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 172.1178, GNorm = 0.2990
Meta loss on this task batch = 2.3409e-01, Meta loss averaged over last 500 steps = 2.7404e-01, PNorm = 172.1217, GNorm = 0.2403
Meta loss on this task batch = 2.3956e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 172.1263, GNorm = 0.3223
Meta loss on this task batch = 2.6952e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 172.1313, GNorm = 0.2576
Meta loss on this task batch = 2.9794e-01, Meta loss averaged over last 500 steps = 2.7413e-01, PNorm = 172.1360, GNorm = 0.3099
Meta loss on this task batch = 2.8011e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 172.1405, GNorm = 0.3481
Meta loss on this task batch = 2.4545e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 172.1441, GNorm = 0.3568
Meta loss on this task batch = 2.7178e-01, Meta loss averaged over last 500 steps = 2.7405e-01, PNorm = 172.1477, GNorm = 0.4069
Meta loss on this task batch = 2.9725e-01, Meta loss averaged over last 500 steps = 2.7411e-01, PNorm = 172.1512, GNorm = 0.4438
Meta loss on this task batch = 2.6684e-01, Meta loss averaged over last 500 steps = 2.7400e-01, PNorm = 172.1548, GNorm = 0.2524
Meta loss on this task batch = 2.7053e-01, Meta loss averaged over last 500 steps = 2.7396e-01, PNorm = 172.1586, GNorm = 0.2940
Meta loss on this task batch = 2.8701e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 172.1621, GNorm = 0.2918
Meta loss on this task batch = 2.7997e-01, Meta loss averaged over last 500 steps = 2.7401e-01, PNorm = 172.1657, GNorm = 0.2779
Meta loss on this task batch = 2.8041e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 172.1693, GNorm = 0.4876
Took 129.75094175338745 seconds to complete one epoch of meta training
Took 138.8638560771942 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469638
Epoch 963
Meta loss on this task batch = 2.9324e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 172.1737, GNorm = 0.2982
Meta loss on this task batch = 3.7656e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 172.1766, GNorm = 0.5542
Meta loss on this task batch = 3.1722e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 172.1794, GNorm = 0.3457
Meta loss on this task batch = 2.3317e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 172.1827, GNorm = 0.2503
Meta loss on this task batch = 2.6073e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 172.1866, GNorm = 0.4297
Meta loss on this task batch = 3.3516e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 172.1903, GNorm = 0.4946
Meta loss on this task batch = 3.1521e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 172.1936, GNorm = 0.3104
Meta loss on this task batch = 2.5514e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 172.1970, GNorm = 0.2683
Meta loss on this task batch = 2.2446e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 172.2008, GNorm = 0.2898
Meta loss on this task batch = 2.8805e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 172.2042, GNorm = 0.3641
Meta loss on this task batch = 3.0292e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 172.2081, GNorm = 0.4144
Meta loss on this task batch = 2.4058e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 172.2123, GNorm = 0.2350
Meta loss on this task batch = 2.6415e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 172.2166, GNorm = 0.2327
Meta loss on this task batch = 2.2156e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 172.2210, GNorm = 0.2751
Meta loss on this task batch = 2.7043e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.2253, GNorm = 0.2610
Meta loss on this task batch = 2.9104e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 172.2292, GNorm = 0.3091
Meta loss on this task batch = 2.1526e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 172.2336, GNorm = 0.2197
Meta loss on this task batch = 3.1181e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 172.2373, GNorm = 0.2892
Meta loss on this task batch = 2.8198e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 172.2402, GNorm = 0.3981
Took 130.04264616966248 seconds to complete one epoch of meta training
Took 137.4457037448883 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460300
Epoch 964
Meta loss on this task batch = 2.6267e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 172.2434, GNorm = 0.4138
Meta loss on this task batch = 2.8482e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 172.2466, GNorm = 0.3024
Meta loss on this task batch = 2.4097e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 172.2508, GNorm = 0.4054
Meta loss on this task batch = 2.4886e-01, Meta loss averaged over last 500 steps = 2.7412e-01, PNorm = 172.2553, GNorm = 0.2847
Meta loss on this task batch = 3.1758e-01, Meta loss averaged over last 500 steps = 2.7429e-01, PNorm = 172.2601, GNorm = 0.3254
Meta loss on this task batch = 2.9309e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 172.2648, GNorm = 0.2942
Meta loss on this task batch = 2.8869e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 172.2694, GNorm = 0.2456
Meta loss on this task batch = 2.8379e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 172.2742, GNorm = 0.2731
Meta loss on this task batch = 2.5787e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 172.2785, GNorm = 0.2749
Meta loss on this task batch = 3.4001e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 172.2826, GNorm = 0.3511
Meta loss on this task batch = 2.8492e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.2867, GNorm = 0.3191
Meta loss on this task batch = 2.5930e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 172.2908, GNorm = 0.3064
Meta loss on this task batch = 2.7422e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 172.2948, GNorm = 0.3016
Meta loss on this task batch = 2.2430e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 172.2990, GNorm = 0.3595
Meta loss on this task batch = 2.5560e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 172.3036, GNorm = 0.2761
Meta loss on this task batch = 2.8906e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 172.3080, GNorm = 0.6343
Meta loss on this task batch = 2.8671e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 172.3118, GNorm = 0.3431
Meta loss on this task batch = 2.3958e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 172.3156, GNorm = 0.2876
Meta loss on this task batch = 2.2470e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 172.3192, GNorm = 0.4119
Took 135.32645726203918 seconds to complete one epoch of meta training
Took 143.849858045578 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447657
Epoch 965
Meta loss on this task batch = 3.6302e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 172.3214, GNorm = 0.3797
Meta loss on this task batch = 2.8276e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 172.3240, GNorm = 0.3075
Meta loss on this task batch = 2.2267e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 172.3270, GNorm = 0.2853
Meta loss on this task batch = 2.1275e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 172.3307, GNorm = 0.3199
Meta loss on this task batch = 3.7751e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 172.3330, GNorm = 0.3474
Meta loss on this task batch = 2.6423e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 172.3359, GNorm = 0.2732
Meta loss on this task batch = 2.7906e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 172.3389, GNorm = 0.2615
Meta loss on this task batch = 2.1085e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 172.3422, GNorm = 0.2617
Meta loss on this task batch = 2.8493e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 172.3447, GNorm = 0.4563
Meta loss on this task batch = 1.9106e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.3473, GNorm = 0.2468
Meta loss on this task batch = 2.6503e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 172.3500, GNorm = 0.2581
Meta loss on this task batch = 2.9584e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 172.3527, GNorm = 0.3625
Meta loss on this task batch = 2.7877e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 172.3554, GNorm = 0.2584
Meta loss on this task batch = 2.3331e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 172.3582, GNorm = 0.2782
Meta loss on this task batch = 3.5188e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.3609, GNorm = 0.3107
Meta loss on this task batch = 3.0215e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 172.3633, GNorm = 0.2621
Meta loss on this task batch = 2.7034e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 172.3655, GNorm = 0.3074
Meta loss on this task batch = 2.8810e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 172.3675, GNorm = 0.3465
Meta loss on this task batch = 2.7431e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 172.3696, GNorm = 0.3003
Took 125.8871374130249 seconds to complete one epoch of meta training
Took 135.46769905090332 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461927
Epoch 966
Meta loss on this task batch = 2.6615e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 172.3727, GNorm = 0.2515
Meta loss on this task batch = 2.2815e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 172.3766, GNorm = 0.3583
Meta loss on this task batch = 3.3772e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 172.3794, GNorm = 0.4116
Meta loss on this task batch = 2.3099e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 172.3826, GNorm = 0.3169
Meta loss on this task batch = 2.7444e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 172.3857, GNorm = 0.3131
Meta loss on this task batch = 2.7331e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 172.3887, GNorm = 0.2968
Meta loss on this task batch = 2.4275e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 172.3921, GNorm = 0.2593
Meta loss on this task batch = 2.2749e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 172.3960, GNorm = 0.2417
Meta loss on this task batch = 2.5398e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 172.4001, GNorm = 0.2696
Meta loss on this task batch = 2.9863e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 172.4045, GNorm = 0.2798
Meta loss on this task batch = 2.8760e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.4085, GNorm = 0.3225
Meta loss on this task batch = 2.9444e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 172.4117, GNorm = 0.2657
Meta loss on this task batch = 2.4902e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 172.4142, GNorm = 0.2748
Meta loss on this task batch = 2.1872e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 172.4169, GNorm = 0.2521
Meta loss on this task batch = 3.0439e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 172.4201, GNorm = 0.3262
Meta loss on this task batch = 2.8068e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 172.4232, GNorm = 0.3349
Meta loss on this task batch = 3.1757e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 172.4251, GNorm = 0.3816
Meta loss on this task batch = 2.8632e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 172.4269, GNorm = 0.4167
Meta loss on this task batch = 3.5872e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 172.4294, GNorm = 0.3599
Took 126.20322489738464 seconds to complete one epoch of meta training
Took 134.65465092658997 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455916
Epoch 967
Meta loss on this task batch = 2.3882e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 172.4320, GNorm = 0.2844
Meta loss on this task batch = 2.6132e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 172.4355, GNorm = 0.2866
Meta loss on this task batch = 2.4502e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 172.4396, GNorm = 0.3245
Meta loss on this task batch = 2.2507e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 172.4441, GNorm = 0.2764
Meta loss on this task batch = 3.1563e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 172.4482, GNorm = 0.2734
Meta loss on this task batch = 2.7456e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 172.4524, GNorm = 0.3215
Meta loss on this task batch = 2.8801e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.4568, GNorm = 0.2701
Meta loss on this task batch = 3.1127e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 172.4617, GNorm = 0.4152
Meta loss on this task batch = 2.6300e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 172.4664, GNorm = 0.2408
Meta loss on this task batch = 2.7239e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 172.4712, GNorm = 0.4143
Meta loss on this task batch = 2.4135e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 172.4758, GNorm = 0.2639
Meta loss on this task batch = 2.6744e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 172.4798, GNorm = 0.4104
Meta loss on this task batch = 2.3751e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 172.4842, GNorm = 0.3228
Meta loss on this task batch = 2.8585e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 172.4889, GNorm = 0.3972
Meta loss on this task batch = 3.1307e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 172.4923, GNorm = 0.4420
Meta loss on this task batch = 2.3534e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 172.4962, GNorm = 0.2803
Meta loss on this task batch = 2.5624e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 172.4998, GNorm = 0.3207
Meta loss on this task batch = 3.5120e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 172.5031, GNorm = 0.4116
Meta loss on this task batch = 2.7625e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 172.5074, GNorm = 0.4911
Took 125.94384479522705 seconds to complete one epoch of meta training
Took 133.81226801872253 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456717
Epoch 968
Meta loss on this task batch = 2.8483e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 172.5113, GNorm = 0.2518
Meta loss on this task batch = 2.4649e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 172.5158, GNorm = 0.2903
Meta loss on this task batch = 2.3060e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 172.5199, GNorm = 0.2611
Meta loss on this task batch = 2.7711e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 172.5233, GNorm = 0.2580
Meta loss on this task batch = 2.9206e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 172.5261, GNorm = 0.4267
Meta loss on this task batch = 3.1205e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 172.5288, GNorm = 0.3190
Meta loss on this task batch = 2.6536e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 172.5310, GNorm = 0.2817
Meta loss on this task batch = 2.3549e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 172.5340, GNorm = 0.2876
Meta loss on this task batch = 2.5952e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 172.5374, GNorm = 0.3082
Meta loss on this task batch = 3.3566e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 172.5410, GNorm = 0.3127
Meta loss on this task batch = 2.9517e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 172.5443, GNorm = 0.4556
Meta loss on this task batch = 1.9605e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 172.5482, GNorm = 0.3378
Meta loss on this task batch = 2.6614e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 172.5520, GNorm = 0.2676
Meta loss on this task batch = 2.8428e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.5560, GNorm = 0.2552
Meta loss on this task batch = 3.0742e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 172.5596, GNorm = 0.3978
Meta loss on this task batch = 2.7969e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 172.5631, GNorm = 0.2874
Meta loss on this task batch = 2.8233e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 172.5669, GNorm = 0.2857
Meta loss on this task batch = 2.8772e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 172.5708, GNorm = 0.2536
Meta loss on this task batch = 2.9369e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 172.5755, GNorm = 0.3452
Took 164.95303773880005 seconds to complete one epoch of meta training
Took 174.41595339775085 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.439760
Epoch 969
Meta loss on this task batch = 2.8725e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 172.5801, GNorm = 0.2985
Meta loss on this task batch = 2.5891e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 172.5848, GNorm = 0.3927
Meta loss on this task batch = 3.0638e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 172.5900, GNorm = 0.3064
Meta loss on this task batch = 2.9566e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 172.5952, GNorm = 0.2517
Meta loss on this task batch = 2.8261e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 172.6001, GNorm = 0.3317
Meta loss on this task batch = 2.3113e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 172.6054, GNorm = 0.2861
Meta loss on this task batch = 2.3824e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 172.6110, GNorm = 0.2493
Meta loss on this task batch = 3.0776e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 172.6159, GNorm = 0.2959
Meta loss on this task batch = 2.4914e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 172.6209, GNorm = 0.2444
Meta loss on this task batch = 2.9151e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 172.6257, GNorm = 0.3121
Meta loss on this task batch = 2.4072e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 172.6305, GNorm = 0.2757
Meta loss on this task batch = 2.8323e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 172.6350, GNorm = 0.3045
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 172.6395, GNorm = 0.3233
Meta loss on this task batch = 2.5978e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 172.6436, GNorm = 0.5154
Meta loss on this task batch = 2.9809e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 172.6472, GNorm = 0.3003
Meta loss on this task batch = 2.9787e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 172.6509, GNorm = 0.2538
Meta loss on this task batch = 2.3763e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 172.6544, GNorm = 0.2972
Meta loss on this task batch = 2.9155e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.6578, GNorm = 0.2985
Meta loss on this task batch = 2.4589e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.6616, GNorm = 0.2830
Took 293.8798894882202 seconds to complete one epoch of meta training
Took 301.9114360809326 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.452474
Epoch 970
Meta loss on this task batch = 2.6925e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 172.6649, GNorm = 0.2486
Meta loss on this task batch = 3.1153e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 172.6678, GNorm = 0.2851
Meta loss on this task batch = 3.1300e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 172.6707, GNorm = 0.2438
Meta loss on this task batch = 2.8846e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 172.6731, GNorm = 0.2901
Meta loss on this task batch = 3.2521e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 172.6749, GNorm = 0.2912
Meta loss on this task batch = 2.5693e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 172.6763, GNorm = 0.2976
Meta loss on this task batch = 2.7291e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 172.6779, GNorm = 0.4922
Meta loss on this task batch = 2.2473e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 172.6802, GNorm = 0.3865
Meta loss on this task batch = 2.4926e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 172.6833, GNorm = 0.2557
Meta loss on this task batch = 2.4368e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 172.6867, GNorm = 0.3149
Meta loss on this task batch = 2.7077e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 172.6906, GNorm = 0.2705
Meta loss on this task batch = 2.9953e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 172.6942, GNorm = 0.3568
Meta loss on this task batch = 3.0199e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 172.6978, GNorm = 0.3663
Meta loss on this task batch = 2.6869e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 172.7012, GNorm = 0.3169
Meta loss on this task batch = 2.4092e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 172.7053, GNorm = 0.4122
Meta loss on this task batch = 2.4944e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 172.7099, GNorm = 0.2486
Meta loss on this task batch = 2.9696e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 172.7143, GNorm = 0.3476
Meta loss on this task batch = 2.2271e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 172.7193, GNorm = 0.3519
Meta loss on this task batch = 2.7868e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 172.7240, GNorm = 0.3841
Took 127.62018275260925 seconds to complete one epoch of meta training
Took 137.13128185272217 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457755
Epoch 971
Meta loss on this task batch = 2.7911e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 172.7290, GNorm = 0.3263
Meta loss on this task batch = 2.6845e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 172.7339, GNorm = 0.2587
Meta loss on this task batch = 2.5065e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 172.7383, GNorm = 0.2235
Meta loss on this task batch = 3.0699e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 172.7433, GNorm = 0.3074
Meta loss on this task batch = 2.4500e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 172.7478, GNorm = 0.3409
Meta loss on this task batch = 2.7146e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 172.7522, GNorm = 0.2854
Meta loss on this task batch = 2.6876e-01, Meta loss averaged over last 500 steps = 2.7425e-01, PNorm = 172.7566, GNorm = 0.3194
Meta loss on this task batch = 3.5456e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 172.7604, GNorm = 0.4075
Meta loss on this task batch = 2.7461e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 172.7639, GNorm = 0.3828
Meta loss on this task batch = 2.5623e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 172.7678, GNorm = 0.3499
Meta loss on this task batch = 2.6675e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 172.7720, GNorm = 0.3895
Meta loss on this task batch = 3.6241e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 172.7759, GNorm = 0.3198
Meta loss on this task batch = 3.3039e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 172.7798, GNorm = 0.3045
Meta loss on this task batch = 2.2069e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 172.7843, GNorm = 0.2605
Meta loss on this task batch = 2.5401e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 172.7895, GNorm = 0.3038
Meta loss on this task batch = 2.7467e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 172.7943, GNorm = 0.2369
Meta loss on this task batch = 2.4710e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 172.8000, GNorm = 0.2680
Meta loss on this task batch = 2.7999e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 172.8056, GNorm = 0.3278
Meta loss on this task batch = 2.1489e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 172.8114, GNorm = 0.2999
Took 172.3409662246704 seconds to complete one epoch of meta training
Took 180.5759129524231 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460865
Epoch 972
Meta loss on this task batch = 2.5317e-01, Meta loss averaged over last 500 steps = 2.7413e-01, PNorm = 172.8173, GNorm = 0.2789
Meta loss on this task batch = 2.9579e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 172.8218, GNorm = 0.4066
Meta loss on this task batch = 2.9570e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 172.8255, GNorm = 0.2721
Meta loss on this task batch = 3.5004e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 172.8287, GNorm = 0.4176
Meta loss on this task batch = 3.2606e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 172.8318, GNorm = 0.5195
Meta loss on this task batch = 2.1674e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 172.8346, GNorm = 0.3327
Meta loss on this task batch = 3.3584e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 172.8379, GNorm = 0.4211
Meta loss on this task batch = 2.5972e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 172.8416, GNorm = 0.2631
Meta loss on this task batch = 2.6639e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 172.8461, GNorm = 0.2570
Meta loss on this task batch = 3.3874e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 172.8498, GNorm = 0.3022
Meta loss on this task batch = 2.6470e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 172.8538, GNorm = 0.3005
Meta loss on this task batch = 2.4761e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 172.8576, GNorm = 0.2548
Meta loss on this task batch = 2.1396e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 172.8611, GNorm = 0.2335
Meta loss on this task batch = 2.8204e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 172.8650, GNorm = 0.2348
Meta loss on this task batch = 2.9248e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 172.8684, GNorm = 0.2872
Meta loss on this task batch = 2.8618e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 172.8716, GNorm = 0.2904
Meta loss on this task batch = 2.5499e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 172.8743, GNorm = 0.4071
Meta loss on this task batch = 2.8346e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 172.8765, GNorm = 0.3096
Meta loss on this task batch = 3.4459e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 172.8791, GNorm = 0.5989
Took 137.5881679058075 seconds to complete one epoch of meta training
Took 145.95463514328003 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448662
Epoch 973
Meta loss on this task batch = 2.9535e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 172.8818, GNorm = 0.2441
Meta loss on this task batch = 2.3120e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 172.8850, GNorm = 0.2432
Meta loss on this task batch = 2.7042e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 172.8885, GNorm = 0.2575
Meta loss on this task batch = 2.7958e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.8926, GNorm = 0.4158
Meta loss on this task batch = 3.0882e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 172.8962, GNorm = 0.3077
Meta loss on this task batch = 2.7741e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 172.9001, GNorm = 0.4267
Meta loss on this task batch = 2.2832e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 172.9046, GNorm = 0.3909
Meta loss on this task batch = 3.1965e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 172.9089, GNorm = 0.3184
Meta loss on this task batch = 2.1500e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 172.9134, GNorm = 0.3440
Meta loss on this task batch = 2.1350e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 172.9180, GNorm = 0.2539
Meta loss on this task batch = 2.8192e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 172.9222, GNorm = 0.4578
Meta loss on this task batch = 2.5186e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 172.9261, GNorm = 0.2913
Meta loss on this task batch = 2.5537e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 172.9306, GNorm = 0.3487
Meta loss on this task batch = 3.1282e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 172.9349, GNorm = 0.2974
Meta loss on this task batch = 2.2639e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 172.9394, GNorm = 0.2624
Meta loss on this task batch = 2.9445e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 172.9434, GNorm = 0.3240
Meta loss on this task batch = 3.0278e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 172.9470, GNorm = 0.4075
Meta loss on this task batch = 3.1924e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 172.9499, GNorm = 0.3513
Meta loss on this task batch = 2.4577e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 172.9530, GNorm = 0.3784
Took 121.0189425945282 seconds to complete one epoch of meta training
Took 129.07726573944092 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453190
Epoch 974
Meta loss on this task batch = 2.2287e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 172.9565, GNorm = 0.2568
Meta loss on this task batch = 2.6944e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 172.9599, GNorm = 0.2879
Meta loss on this task batch = 2.3346e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 172.9639, GNorm = 0.4277
Meta loss on this task batch = 3.0929e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 172.9678, GNorm = 0.3430
Meta loss on this task batch = 2.9259e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 172.9711, GNorm = 0.3521
Meta loss on this task batch = 3.1417e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.9743, GNorm = 0.3490
Meta loss on this task batch = 2.4797e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 172.9776, GNorm = 0.2954
Meta loss on this task batch = 2.8847e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 172.9809, GNorm = 0.2783
Meta loss on this task batch = 2.5476e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 172.9846, GNorm = 0.2780
Meta loss on this task batch = 2.6315e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 172.9884, GNorm = 0.3651
Meta loss on this task batch = 2.8380e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 172.9919, GNorm = 0.2844
Meta loss on this task batch = 2.6529e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 172.9957, GNorm = 0.2700
Meta loss on this task batch = 2.9581e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 172.9990, GNorm = 0.2895
Meta loss on this task batch = 2.5306e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 173.0021, GNorm = 0.3498
Meta loss on this task batch = 2.2396e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 173.0055, GNorm = 0.2736
Meta loss on this task batch = 2.6676e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 173.0091, GNorm = 0.2667
Meta loss on this task batch = 2.7900e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 173.0131, GNorm = 0.3109
Meta loss on this task batch = 2.6126e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 173.0174, GNorm = 0.2361
Meta loss on this task batch = 3.2348e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 173.0211, GNorm = 0.3588
Took 113.36829280853271 seconds to complete one epoch of meta training
Took 121.44757914543152 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469565
Epoch 975
Meta loss on this task batch = 3.1327e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 173.0247, GNorm = 0.3696
Meta loss on this task batch = 2.9634e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 173.0282, GNorm = 0.3403
Meta loss on this task batch = 2.3987e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 173.0322, GNorm = 0.2164
Meta loss on this task batch = 2.6366e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 173.0370, GNorm = 0.3102
Meta loss on this task batch = 2.6034e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 173.0412, GNorm = 0.2846
Meta loss on this task batch = 2.8946e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 173.0450, GNorm = 0.2465
Meta loss on this task batch = 2.6196e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 173.0490, GNorm = 0.2776
Meta loss on this task batch = 2.7837e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 173.0532, GNorm = 0.2865
Meta loss on this task batch = 2.3088e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 173.0571, GNorm = 0.2731
Meta loss on this task batch = 2.5647e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 173.0617, GNorm = 0.2705
Meta loss on this task batch = 3.0468e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 173.0662, GNorm = 0.3408
Meta loss on this task batch = 2.3431e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 173.0708, GNorm = 0.2179
Meta loss on this task batch = 2.6242e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 173.0753, GNorm = 0.2628
Meta loss on this task batch = 2.6798e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 173.0800, GNorm = 0.2526
Meta loss on this task batch = 2.7803e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 173.0842, GNorm = 0.2869
Meta loss on this task batch = 2.6868e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 173.0885, GNorm = 0.2763
Meta loss on this task batch = 2.7598e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 173.0926, GNorm = 0.3390
Meta loss on this task batch = 2.8335e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 173.0965, GNorm = 0.3065
Meta loss on this task batch = 3.1214e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 173.1001, GNorm = 0.3874
Took 111.13931775093079 seconds to complete one epoch of meta training
Took 119.0391275882721 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470408
Epoch 976
Meta loss on this task batch = 2.2962e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 173.1037, GNorm = 0.3061
Meta loss on this task batch = 2.8609e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 173.1076, GNorm = 0.3746
Meta loss on this task batch = 2.7216e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 173.1116, GNorm = 0.3387
Meta loss on this task batch = 2.4873e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 173.1150, GNorm = 0.3769
Meta loss on this task batch = 2.5704e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 173.1182, GNorm = 0.2949
Meta loss on this task batch = 3.0689e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 173.1210, GNorm = 0.2907
Meta loss on this task batch = 2.9643e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 173.1236, GNorm = 0.4256
Meta loss on this task batch = 2.8535e-01, Meta loss averaged over last 500 steps = 2.7441e-01, PNorm = 173.1260, GNorm = 0.3594
Meta loss on this task batch = 2.7524e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 173.1279, GNorm = 0.2649
Meta loss on this task batch = 2.2492e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 173.1299, GNorm = 0.2616
Meta loss on this task batch = 2.8965e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 173.1324, GNorm = 0.3367
Meta loss on this task batch = 3.2223e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 173.1348, GNorm = 0.4823
Meta loss on this task batch = 2.7047e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 173.1374, GNorm = 0.3185
Meta loss on this task batch = 2.4567e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 173.1407, GNorm = 0.2792
Meta loss on this task batch = 2.4343e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 173.1443, GNorm = 0.3787
Meta loss on this task batch = 3.3790e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 173.1475, GNorm = 0.3688
Meta loss on this task batch = 2.7635e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 173.1513, GNorm = 0.2496
Meta loss on this task batch = 2.7776e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 173.1551, GNorm = 0.3390
Meta loss on this task batch = 2.7542e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 173.1597, GNorm = 0.3567
Took 110.52722263336182 seconds to complete one epoch of meta training
Took 118.49888896942139 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468270
Epoch 977
Meta loss on this task batch = 2.9960e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 173.1636, GNorm = 0.2464
Meta loss on this task batch = 2.9449e-01, Meta loss averaged over last 500 steps = 2.7429e-01, PNorm = 173.1680, GNorm = 0.2866
Meta loss on this task batch = 2.5671e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 173.1731, GNorm = 0.2507
Meta loss on this task batch = 3.0046e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 173.1779, GNorm = 0.2788
Meta loss on this task batch = 3.2207e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 173.1827, GNorm = 0.2700
Meta loss on this task batch = 2.0909e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 173.1880, GNorm = 0.2525
Meta loss on this task batch = 2.2111e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 173.1934, GNorm = 0.2921
Meta loss on this task batch = 2.6502e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 173.1980, GNorm = 0.2876
Meta loss on this task batch = 3.0148e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 173.2026, GNorm = 0.2996
Meta loss on this task batch = 3.0592e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 173.2071, GNorm = 0.4898
Meta loss on this task batch = 2.9772e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 173.2115, GNorm = 0.3532
Meta loss on this task batch = 2.6873e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 173.2159, GNorm = 0.2417
Meta loss on this task batch = 2.8313e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 173.2205, GNorm = 0.2540
Meta loss on this task batch = 2.4273e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 173.2251, GNorm = 0.2856
Meta loss on this task batch = 2.4727e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 173.2301, GNorm = 0.3755
Meta loss on this task batch = 2.5761e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 173.2354, GNorm = 0.2933
Meta loss on this task batch = 2.9705e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 173.2408, GNorm = 0.4202
Meta loss on this task batch = 3.3547e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 173.2458, GNorm = 0.3238
Meta loss on this task batch = 2.6582e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 173.2502, GNorm = 0.2980
Took 111.02056384086609 seconds to complete one epoch of meta training
Took 118.48449873924255 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464242
Epoch 978
Meta loss on this task batch = 3.0409e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 173.2548, GNorm = 0.2736
Meta loss on this task batch = 2.8985e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 173.2593, GNorm = 0.3163
Meta loss on this task batch = 2.7106e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 173.2640, GNorm = 0.2700
Meta loss on this task batch = 3.0292e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 173.2695, GNorm = 0.2796
Meta loss on this task batch = 2.7118e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 173.2747, GNorm = 0.2805
Meta loss on this task batch = 3.3920e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 173.2791, GNorm = 0.3496
Meta loss on this task batch = 3.2064e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 173.2833, GNorm = 0.4082
Meta loss on this task batch = 2.9296e-01, Meta loss averaged over last 500 steps = 2.7501e-01, PNorm = 173.2877, GNorm = 0.2843
Meta loss on this task batch = 2.2844e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 173.2925, GNorm = 0.2557
Meta loss on this task batch = 2.8194e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 173.2977, GNorm = 0.2320
Meta loss on this task batch = 2.6582e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 173.3029, GNorm = 0.3091
Meta loss on this task batch = 2.6754e-01, Meta loss averaged over last 500 steps = 2.7501e-01, PNorm = 173.3087, GNorm = 0.2624
Meta loss on this task batch = 2.6533e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 173.3148, GNorm = 0.2738
Meta loss on this task batch = 2.9472e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 173.3204, GNorm = 0.2989
Meta loss on this task batch = 2.5048e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 173.3253, GNorm = 0.3937
Meta loss on this task batch = 2.5689e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 173.3306, GNorm = 0.3170
Meta loss on this task batch = 2.9808e-01, Meta loss averaged over last 500 steps = 2.7489e-01, PNorm = 173.3360, GNorm = 0.2675
Meta loss on this task batch = 2.5547e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 173.3419, GNorm = 0.2699
Meta loss on this task batch = 2.3827e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 173.3479, GNorm = 0.3547
Took 108.8757758140564 seconds to complete one epoch of meta training
Took 117.1914713382721 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450088
Epoch 979
Meta loss on this task batch = 2.8180e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 173.3539, GNorm = 0.2649
Meta loss on this task batch = 3.0195e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 173.3596, GNorm = 0.3965
Meta loss on this task batch = 2.4462e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 173.3646, GNorm = 0.3566
Meta loss on this task batch = 2.3855e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 173.3693, GNorm = 0.2578
Meta loss on this task batch = 3.0062e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 173.3738, GNorm = 0.4442
Meta loss on this task batch = 2.9816e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 173.3778, GNorm = 0.3493
Meta loss on this task batch = 3.1287e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 173.3805, GNorm = 0.4857
Meta loss on this task batch = 2.6860e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 173.3836, GNorm = 0.2977
Meta loss on this task batch = 3.0785e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 173.3864, GNorm = 0.2922
Meta loss on this task batch = 2.5741e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 173.3898, GNorm = 0.2584
Meta loss on this task batch = 2.5597e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 173.3934, GNorm = 0.3503
Meta loss on this task batch = 2.6999e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 173.3971, GNorm = 0.2833
Meta loss on this task batch = 3.5600e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 173.4004, GNorm = 0.3105
Meta loss on this task batch = 2.3819e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 173.4034, GNorm = 0.2674
Meta loss on this task batch = 2.5297e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 173.4062, GNorm = 0.3045
Meta loss on this task batch = 2.8980e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 173.4094, GNorm = 0.3324
Meta loss on this task batch = 2.7296e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 173.4131, GNorm = 0.4509
Meta loss on this task batch = 2.6645e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 173.4162, GNorm = 0.2957
Meta loss on this task batch = 3.5042e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 173.4181, GNorm = 0.6139
Took 109.16676306724548 seconds to complete one epoch of meta training
Took 117.01248908042908 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460699
Epoch 980
Meta loss on this task batch = 2.7933e-01, Meta loss averaged over last 500 steps = 2.7482e-01, PNorm = 173.4199, GNorm = 0.2947
Meta loss on this task batch = 2.5546e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 173.4217, GNorm = 0.2689
Meta loss on this task batch = 3.7013e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 173.4233, GNorm = 0.5052
Meta loss on this task batch = 2.9162e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 173.4249, GNorm = 0.3319
Meta loss on this task batch = 3.0917e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 173.4273, GNorm = 0.3176
Meta loss on this task batch = 2.3921e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 173.4308, GNorm = 0.4499
Meta loss on this task batch = 3.2394e-01, Meta loss averaged over last 500 steps = 2.7503e-01, PNorm = 173.4345, GNorm = 0.3043
Meta loss on this task batch = 2.7153e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 173.4384, GNorm = 0.3214
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 2.7513e-01, PNorm = 173.4421, GNorm = 0.3285
Meta loss on this task batch = 2.7763e-01, Meta loss averaged over last 500 steps = 2.7524e-01, PNorm = 173.4454, GNorm = 0.3814
Meta loss on this task batch = 2.5457e-01, Meta loss averaged over last 500 steps = 2.7516e-01, PNorm = 173.4494, GNorm = 0.2318
Meta loss on this task batch = 2.3061e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 173.4534, GNorm = 0.2315
Meta loss on this task batch = 2.7448e-01, Meta loss averaged over last 500 steps = 2.7511e-01, PNorm = 173.4575, GNorm = 0.2900
Meta loss on this task batch = 2.4574e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 173.4618, GNorm = 0.2509
Meta loss on this task batch = 2.4959e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 173.4666, GNorm = 0.3367
Meta loss on this task batch = 2.9204e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 173.4718, GNorm = 0.2481
Meta loss on this task batch = 2.6729e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 173.4776, GNorm = 0.5305
Meta loss on this task batch = 2.5072e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 173.4834, GNorm = 0.2387
Meta loss on this task batch = 2.4342e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 173.4897, GNorm = 0.3534
Took 109.76567912101746 seconds to complete one epoch of meta training
Took 117.34893465042114 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463803
Epoch 981
Meta loss on this task batch = 2.4604e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 173.4958, GNorm = 0.3516
Meta loss on this task batch = 2.5309e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 173.5021, GNorm = 0.3267
Meta loss on this task batch = 2.9438e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 173.5078, GNorm = 0.3599
Meta loss on this task batch = 2.4023e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 173.5140, GNorm = 0.2383
Meta loss on this task batch = 2.0982e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 173.5199, GNorm = 0.2643
Meta loss on this task batch = 3.1157e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 173.5257, GNorm = 0.3424
Meta loss on this task batch = 2.8807e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 173.5309, GNorm = 0.4332
Meta loss on this task batch = 2.5393e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 173.5355, GNorm = 0.2719
Meta loss on this task batch = 2.8845e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 173.5394, GNorm = 0.3811
Meta loss on this task batch = 2.7103e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 173.5428, GNorm = 0.2583
Meta loss on this task batch = 3.1417e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 173.5454, GNorm = 0.3302
Meta loss on this task batch = 2.6131e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 173.5477, GNorm = 0.2587
Meta loss on this task batch = 2.7508e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 173.5505, GNorm = 0.3723
Meta loss on this task batch = 2.4055e-01, Meta loss averaged over last 500 steps = 2.7492e-01, PNorm = 173.5537, GNorm = 0.3102
Meta loss on this task batch = 2.6008e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 173.5570, GNorm = 0.2951
Meta loss on this task batch = 3.0068e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 173.5599, GNorm = 0.4558
Meta loss on this task batch = 3.3153e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 173.5626, GNorm = 0.3081
Meta loss on this task batch = 2.9232e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 173.5650, GNorm = 0.3104
Meta loss on this task batch = 3.1442e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 173.5673, GNorm = 0.3904
Took 109.68682289123535 seconds to complete one epoch of meta training
Took 117.36193346977234 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460845
Epoch 982
Meta loss on this task batch = 2.4800e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 173.5700, GNorm = 0.3325
Meta loss on this task batch = 2.4729e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 173.5731, GNorm = 0.3007
Meta loss on this task batch = 2.4066e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 173.5765, GNorm = 0.3072
Meta loss on this task batch = 2.8068e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 173.5801, GNorm = 0.4115
Meta loss on this task batch = 2.9674e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 173.5839, GNorm = 0.3598
Meta loss on this task batch = 2.9253e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 173.5884, GNorm = 0.3164
Meta loss on this task batch = 3.0923e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 173.5927, GNorm = 0.3090
Meta loss on this task batch = 2.8420e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 173.5971, GNorm = 0.3632
Meta loss on this task batch = 2.5565e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 173.6020, GNorm = 0.3024
Meta loss on this task batch = 2.5865e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 173.6069, GNorm = 0.3814
Meta loss on this task batch = 2.9100e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 173.6114, GNorm = 0.3341
Meta loss on this task batch = 2.7691e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 173.6161, GNorm = 0.6082
Meta loss on this task batch = 2.8000e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 173.6204, GNorm = 0.2664
Meta loss on this task batch = 2.4775e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 173.6254, GNorm = 0.2590
Meta loss on this task batch = 2.9317e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 173.6301, GNorm = 0.2904
Meta loss on this task batch = 2.4356e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 173.6346, GNorm = 0.4454
Meta loss on this task batch = 2.4648e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 173.6390, GNorm = 0.3663
Meta loss on this task batch = 3.1402e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 173.6434, GNorm = 0.3337
Meta loss on this task batch = 3.0554e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 173.6473, GNorm = 0.4613
Took 112.56004810333252 seconds to complete one epoch of meta training
Took 120.33435678482056 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481828
Epoch 983
Meta loss on this task batch = 2.8435e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 173.6505, GNorm = 0.3166
Meta loss on this task batch = 2.6963e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 173.6535, GNorm = 0.2834
Meta loss on this task batch = 2.7878e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 173.6568, GNorm = 0.3018
Meta loss on this task batch = 2.6381e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 173.6611, GNorm = 0.3301
Meta loss on this task batch = 2.6940e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 173.6648, GNorm = 0.3428
Meta loss on this task batch = 2.1087e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 173.6691, GNorm = 0.2852
Meta loss on this task batch = 2.8589e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 173.6729, GNorm = 0.3147
Meta loss on this task batch = 2.5678e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 173.6763, GNorm = 0.2587
Meta loss on this task batch = 2.2911e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 173.6799, GNorm = 0.2705
Meta loss on this task batch = 3.2318e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 173.6827, GNorm = 0.4574
Meta loss on this task batch = 2.5815e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 173.6852, GNorm = 0.2850
Meta loss on this task batch = 2.9148e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 173.6869, GNorm = 0.3195
Meta loss on this task batch = 2.6876e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 173.6888, GNorm = 0.2872
Meta loss on this task batch = 2.9462e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 173.6909, GNorm = 0.2558
Meta loss on this task batch = 2.7589e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 173.6934, GNorm = 0.3641
Meta loss on this task batch = 2.8559e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 173.6960, GNorm = 0.3919
Meta loss on this task batch = 2.5479e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 173.6991, GNorm = 0.3277
Meta loss on this task batch = 2.5044e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 173.7020, GNorm = 0.3126
Meta loss on this task batch = 3.1794e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 173.7055, GNorm = 0.3471
Took 112.26324009895325 seconds to complete one epoch of meta training
Took 120.86910271644592 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454657
Epoch 984
Meta loss on this task batch = 3.1128e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 173.7086, GNorm = 0.2545
Meta loss on this task batch = 2.3786e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 173.7121, GNorm = 0.2303
Meta loss on this task batch = 3.2204e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 173.7159, GNorm = 0.2991
Meta loss on this task batch = 2.4939e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 173.7201, GNorm = 0.2686
Meta loss on this task batch = 2.5681e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 173.7248, GNorm = 0.2346
Meta loss on this task batch = 2.5431e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 173.7295, GNorm = 0.2105
Meta loss on this task batch = 2.7943e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 173.7344, GNorm = 0.3173
Meta loss on this task batch = 2.9155e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 173.7388, GNorm = 0.3947
Meta loss on this task batch = 2.9946e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 173.7432, GNorm = 0.2381
Meta loss on this task batch = 2.7881e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 173.7479, GNorm = 0.3684
Meta loss on this task batch = 2.5892e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 173.7532, GNorm = 0.3284
Meta loss on this task batch = 2.1273e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 173.7584, GNorm = 0.2710
Meta loss on this task batch = 2.7740e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 173.7638, GNorm = 0.3212
Meta loss on this task batch = 2.5043e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 173.7690, GNorm = 0.2484
Meta loss on this task batch = 2.9006e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 173.7736, GNorm = 0.3830
Meta loss on this task batch = 2.3716e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 173.7781, GNorm = 0.2782
Meta loss on this task batch = 2.5254e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 173.7826, GNorm = 0.2670
Meta loss on this task batch = 3.2879e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 173.7862, GNorm = 0.2663
Meta loss on this task batch = 2.6276e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 173.7897, GNorm = 0.3376
Took 112.7911741733551 seconds to complete one epoch of meta training
Took 120.70446443557739 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474300
Epoch 985
Meta loss on this task batch = 3.1936e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 173.7926, GNorm = 0.2785
Meta loss on this task batch = 2.4690e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 173.7952, GNorm = 0.2981
Meta loss on this task batch = 2.4932e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 173.7971, GNorm = 0.2365
Meta loss on this task batch = 2.2652e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 173.7994, GNorm = 0.2449
Meta loss on this task batch = 2.6139e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 173.8016, GNorm = 0.2794
Meta loss on this task batch = 2.7763e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 173.8042, GNorm = 0.2597
Meta loss on this task batch = 2.6058e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 173.8060, GNorm = 0.3091
Meta loss on this task batch = 2.7117e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 173.8073, GNorm = 0.4027
Meta loss on this task batch = 3.5203e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 173.8080, GNorm = 0.4465
Meta loss on this task batch = 2.9121e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 173.8090, GNorm = 0.2973
Meta loss on this task batch = 3.0997e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 173.8109, GNorm = 0.3493
Meta loss on this task batch = 2.9486e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 173.8130, GNorm = 0.4293
Meta loss on this task batch = 2.8762e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 173.8153, GNorm = 0.2464
Meta loss on this task batch = 2.2807e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 173.8178, GNorm = 0.3541
Meta loss on this task batch = 2.5870e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 173.8212, GNorm = 0.2831
Meta loss on this task batch = 2.8533e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 173.8246, GNorm = 0.2819
Meta loss on this task batch = 2.4040e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 173.8282, GNorm = 0.3428
Meta loss on this task batch = 2.8898e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 173.8316, GNorm = 0.2999
Meta loss on this task batch = 3.1644e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 173.8351, GNorm = 0.5466
Took 112.02460312843323 seconds to complete one epoch of meta training
Took 120.05520701408386 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458147
Epoch 986
Meta loss on this task batch = 2.5725e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 173.8389, GNorm = 0.2158
Meta loss on this task batch = 2.7038e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 173.8427, GNorm = 0.2586
Meta loss on this task batch = 2.4876e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 173.8464, GNorm = 0.2852
Meta loss on this task batch = 2.6510e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 173.8498, GNorm = 0.4161
Meta loss on this task batch = 2.7500e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 173.8526, GNorm = 0.4637
Meta loss on this task batch = 2.7285e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 173.8554, GNorm = 0.3338
Meta loss on this task batch = 2.9045e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 173.8576, GNorm = 0.2863
Meta loss on this task batch = 2.6818e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 173.8603, GNorm = 0.3428
Meta loss on this task batch = 3.3091e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 173.8627, GNorm = 0.3319
Meta loss on this task batch = 2.8732e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 173.8653, GNorm = 0.3293
Meta loss on this task batch = 2.8481e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 173.8680, GNorm = 0.2849
Meta loss on this task batch = 1.8361e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 173.8714, GNorm = 0.2373
Meta loss on this task batch = 2.8470e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 173.8746, GNorm = 0.2792
Meta loss on this task batch = 2.6698e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 173.8779, GNorm = 0.2862
Meta loss on this task batch = 2.8240e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 173.8815, GNorm = 0.2946
Meta loss on this task batch = 2.9273e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 173.8853, GNorm = 0.3417
Meta loss on this task batch = 2.7855e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 173.8891, GNorm = 0.3673
Meta loss on this task batch = 2.9292e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 173.8941, GNorm = 0.3578
Meta loss on this task batch = 3.2896e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 173.8987, GNorm = 0.3870
Took 111.21562123298645 seconds to complete one epoch of meta training
Took 118.52131485939026 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.441974
Epoch 987
Meta loss on this task batch = 2.7247e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 173.9036, GNorm = 0.2981
Meta loss on this task batch = 2.1192e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 173.9088, GNorm = 0.1958
Meta loss on this task batch = 2.6500e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 173.9141, GNorm = 0.2297
Meta loss on this task batch = 2.9477e-01, Meta loss averaged over last 500 steps = 2.7482e-01, PNorm = 173.9190, GNorm = 0.3230
Meta loss on this task batch = 2.9403e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 173.9245, GNorm = 0.2710
Meta loss on this task batch = 2.8462e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 173.9302, GNorm = 0.3069
Meta loss on this task batch = 2.3734e-01, Meta loss averaged over last 500 steps = 2.7489e-01, PNorm = 173.9353, GNorm = 0.2364
Meta loss on this task batch = 2.9147e-01, Meta loss averaged over last 500 steps = 2.7511e-01, PNorm = 173.9400, GNorm = 0.3417
Meta loss on this task batch = 2.3433e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 173.9450, GNorm = 0.2281
Meta loss on this task batch = 2.8924e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 173.9498, GNorm = 0.2938
Meta loss on this task batch = 2.6301e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 173.9543, GNorm = 0.3145
Meta loss on this task batch = 2.5713e-01, Meta loss averaged over last 500 steps = 2.7503e-01, PNorm = 173.9586, GNorm = 0.2649
Meta loss on this task batch = 2.5337e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 173.9633, GNorm = 0.2684
Meta loss on this task batch = 3.3550e-01, Meta loss averaged over last 500 steps = 2.7510e-01, PNorm = 173.9678, GNorm = 0.4278
Meta loss on this task batch = 2.7986e-01, Meta loss averaged over last 500 steps = 2.7511e-01, PNorm = 173.9715, GNorm = 0.3780
Meta loss on this task batch = 2.9698e-01, Meta loss averaged over last 500 steps = 2.7509e-01, PNorm = 173.9752, GNorm = 0.3320
Meta loss on this task batch = 2.6959e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 173.9784, GNorm = 0.3296
Meta loss on this task batch = 2.8192e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 173.9814, GNorm = 0.3021
Meta loss on this task batch = 3.3827e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 173.9834, GNorm = 0.3190
Took 109.23936247825623 seconds to complete one epoch of meta training
Took 117.34414958953857 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444371
Epoch 988
Meta loss on this task batch = 2.5371e-01, Meta loss averaged over last 500 steps = 2.7515e-01, PNorm = 173.9857, GNorm = 0.2529
Meta loss on this task batch = 2.5655e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 173.9889, GNorm = 0.2474
Meta loss on this task batch = 2.4302e-01, Meta loss averaged over last 500 steps = 2.7489e-01, PNorm = 173.9927, GNorm = 0.2483
Meta loss on this task batch = 2.8539e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 173.9961, GNorm = 0.3101
Meta loss on this task batch = 2.9898e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 173.9999, GNorm = 0.3808
Meta loss on this task batch = 2.7165e-01, Meta loss averaged over last 500 steps = 2.7509e-01, PNorm = 174.0037, GNorm = 0.2940
Meta loss on this task batch = 2.7423e-01, Meta loss averaged over last 500 steps = 2.7504e-01, PNorm = 174.0079, GNorm = 0.2700
Meta loss on this task batch = 2.5142e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 174.0124, GNorm = 0.3080
Meta loss on this task batch = 2.5410e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 174.0167, GNorm = 0.3834
Meta loss on this task batch = 2.3777e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 174.0210, GNorm = 0.3143
Meta loss on this task batch = 2.3626e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 174.0255, GNorm = 0.2596
Meta loss on this task batch = 2.9585e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 174.0292, GNorm = 0.4053
Meta loss on this task batch = 2.8665e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 174.0328, GNorm = 0.2907
Meta loss on this task batch = 2.5941e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 174.0362, GNorm = 0.3663
Meta loss on this task batch = 2.7314e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 174.0395, GNorm = 0.3307
Meta loss on this task batch = 3.4006e-01, Meta loss averaged over last 500 steps = 2.7500e-01, PNorm = 174.0423, GNorm = 0.3500
Meta loss on this task batch = 2.4864e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 174.0455, GNorm = 0.3031
Meta loss on this task batch = 3.0804e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 174.0485, GNorm = 0.3754
Meta loss on this task batch = 2.6324e-01, Meta loss averaged over last 500 steps = 2.7504e-01, PNorm = 174.0516, GNorm = 0.3436
Took 110.02864289283752 seconds to complete one epoch of meta training
Took 118.14499759674072 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467965
Epoch 989
Meta loss on this task batch = 2.7367e-01, Meta loss averaged over last 500 steps = 2.7500e-01, PNorm = 174.0546, GNorm = 0.2597
Meta loss on this task batch = 2.6496e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 174.0577, GNorm = 0.2881
Meta loss on this task batch = 2.9945e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 174.0605, GNorm = 0.3519
Meta loss on this task batch = 2.8290e-01, Meta loss averaged over last 500 steps = 2.7504e-01, PNorm = 174.0638, GNorm = 0.3269
Meta loss on this task batch = 3.0592e-01, Meta loss averaged over last 500 steps = 2.7509e-01, PNorm = 174.0672, GNorm = 0.2468
Meta loss on this task batch = 3.0561e-01, Meta loss averaged over last 500 steps = 2.7515e-01, PNorm = 174.0705, GNorm = 0.3342
Meta loss on this task batch = 3.1949e-01, Meta loss averaged over last 500 steps = 2.7520e-01, PNorm = 174.0739, GNorm = 0.2719
Meta loss on this task batch = 3.1046e-01, Meta loss averaged over last 500 steps = 2.7507e-01, PNorm = 174.0770, GNorm = 0.2992
Meta loss on this task batch = 2.5816e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 174.0805, GNorm = 0.2899
Meta loss on this task batch = 3.0089e-01, Meta loss averaged over last 500 steps = 2.7508e-01, PNorm = 174.0835, GNorm = 0.2750
Meta loss on this task batch = 3.0911e-01, Meta loss averaged over last 500 steps = 2.7518e-01, PNorm = 174.0866, GNorm = 0.3269
Meta loss on this task batch = 2.5171e-01, Meta loss averaged over last 500 steps = 2.7501e-01, PNorm = 174.0902, GNorm = 0.2511
Meta loss on this task batch = 2.6601e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 174.0932, GNorm = 0.4533
Meta loss on this task batch = 1.9304e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 174.0966, GNorm = 0.2535
Meta loss on this task batch = 2.5487e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 174.1005, GNorm = 0.2438
Meta loss on this task batch = 2.3206e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 174.1051, GNorm = 0.2636
Meta loss on this task batch = 2.5795e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 174.1094, GNorm = 0.2010
Meta loss on this task batch = 2.4592e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 174.1132, GNorm = 0.4222
Meta loss on this task batch = 2.6209e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 174.1168, GNorm = 0.4200
Took 111.05873966217041 seconds to complete one epoch of meta training
Took 119.10289192199707 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464528
Epoch 990
Meta loss on this task batch = 2.6726e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 174.1206, GNorm = 0.2593
Meta loss on this task batch = 2.9038e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 174.1239, GNorm = 0.3797
Meta loss on this task batch = 2.5510e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 174.1273, GNorm = 0.3343
Meta loss on this task batch = 2.0444e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 174.1311, GNorm = 0.2800
Meta loss on this task batch = 2.7697e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 174.1340, GNorm = 0.3184
Meta loss on this task batch = 2.4182e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 174.1367, GNorm = 0.2569
Meta loss on this task batch = 3.1089e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 174.1386, GNorm = 0.3251
Meta loss on this task batch = 2.9191e-01, Meta loss averaged over last 500 steps = 2.7465e-01, PNorm = 174.1411, GNorm = 0.3652
Meta loss on this task batch = 3.3469e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 174.1424, GNorm = 0.3848
Meta loss on this task batch = 3.0168e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 174.1440, GNorm = 0.4582
Meta loss on this task batch = 2.2996e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 174.1456, GNorm = 0.3482
Meta loss on this task batch = 2.7064e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 174.1478, GNorm = 0.2918
Meta loss on this task batch = 3.0402e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 174.1509, GNorm = 0.5566
Meta loss on this task batch = 3.0005e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 174.1543, GNorm = 0.3152
Meta loss on this task batch = 2.9815e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 174.1575, GNorm = 0.3465
Meta loss on this task batch = 2.3340e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 174.1609, GNorm = 0.5016
Meta loss on this task batch = 2.6675e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 174.1648, GNorm = 0.3095
Meta loss on this task batch = 2.8283e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 174.1687, GNorm = 0.2464
Meta loss on this task batch = 2.8543e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 174.1728, GNorm = 0.4171
Took 108.91268181800842 seconds to complete one epoch of meta training
Took 116.79651927947998 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456189
Epoch 991
Meta loss on this task batch = 2.7083e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 174.1775, GNorm = 0.3195
Meta loss on this task batch = 2.6716e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 174.1824, GNorm = 0.2284
Meta loss on this task batch = 2.7394e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 174.1877, GNorm = 0.2928
Meta loss on this task batch = 2.7360e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 174.1930, GNorm = 0.3199
Meta loss on this task batch = 2.6896e-01, Meta loss averaged over last 500 steps = 2.7481e-01, PNorm = 174.1985, GNorm = 0.3671
Meta loss on this task batch = 2.6257e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 174.2043, GNorm = 0.2674
Meta loss on this task batch = 2.7891e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 174.2102, GNorm = 0.2915
Meta loss on this task batch = 2.4561e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 174.2159, GNorm = 0.2543
Meta loss on this task batch = 2.9645e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 174.2211, GNorm = 0.3225
Meta loss on this task batch = 3.0049e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 174.2251, GNorm = 0.2728
Meta loss on this task batch = 2.6077e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 174.2291, GNorm = 0.2491
Meta loss on this task batch = 2.3421e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 174.2331, GNorm = 0.3033
Meta loss on this task batch = 2.4751e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 174.2380, GNorm = 0.4557
Meta loss on this task batch = 2.4208e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 174.2427, GNorm = 0.3018
Meta loss on this task batch = 2.2881e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 174.2472, GNorm = 0.3571
Meta loss on this task batch = 2.8019e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 174.2514, GNorm = 0.3266
Meta loss on this task batch = 2.6490e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 174.2554, GNorm = 0.2925
Meta loss on this task batch = 2.6237e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 174.2589, GNorm = 0.2861
Meta loss on this task batch = 3.1655e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 174.2617, GNorm = 0.3605
Took 109.55438995361328 seconds to complete one epoch of meta training
Took 117.62350082397461 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448341
Epoch 992
Meta loss on this task batch = 3.1502e-01, Meta loss averaged over last 500 steps = 2.7491e-01, PNorm = 174.2640, GNorm = 0.4871
Meta loss on this task batch = 2.7888e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 174.2660, GNorm = 0.3922
Meta loss on this task batch = 2.4913e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 174.2682, GNorm = 0.2634
Meta loss on this task batch = 2.4683e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 174.2707, GNorm = 0.2381
Meta loss on this task batch = 2.6484e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 174.2736, GNorm = 0.2869
Meta loss on this task batch = 3.0327e-01, Meta loss averaged over last 500 steps = 2.7462e-01, PNorm = 174.2760, GNorm = 0.3041
Meta loss on this task batch = 3.0189e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 174.2783, GNorm = 0.2994
Meta loss on this task batch = 2.1180e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 174.2808, GNorm = 0.3000
Meta loss on this task batch = 2.7458e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 174.2831, GNorm = 0.2965
Meta loss on this task batch = 2.8455e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 174.2861, GNorm = 0.3325
Meta loss on this task batch = 2.8911e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 174.2894, GNorm = 0.3078
Meta loss on this task batch = 2.8683e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 174.2930, GNorm = 0.3776
Meta loss on this task batch = 3.2181e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 174.2969, GNorm = 0.3514
Meta loss on this task batch = 2.8702e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 174.3012, GNorm = 0.2996
Meta loss on this task batch = 2.3311e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 174.3063, GNorm = 0.3821
Meta loss on this task batch = 2.8335e-01, Meta loss averaged over last 500 steps = 2.7490e-01, PNorm = 174.3114, GNorm = 0.2739
Meta loss on this task batch = 2.3365e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 174.3166, GNorm = 0.3681
Meta loss on this task batch = 2.4923e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 174.3210, GNorm = 0.2531
Meta loss on this task batch = 2.7247e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 174.3252, GNorm = 0.3086
Took 110.36229634284973 seconds to complete one epoch of meta training
Took 117.95023822784424 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469115
Epoch 993
Meta loss on this task batch = 2.3685e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 174.3297, GNorm = 0.2377
Meta loss on this task batch = 2.4275e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 174.3343, GNorm = 0.4125
Meta loss on this task batch = 2.6708e-01, Meta loss averaged over last 500 steps = 2.7464e-01, PNorm = 174.3393, GNorm = 0.3285
Meta loss on this task batch = 2.9787e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 174.3435, GNorm = 0.2649
Meta loss on this task batch = 2.5251e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 174.3483, GNorm = 0.4055
Meta loss on this task batch = 2.2140e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 174.3533, GNorm = 0.3522
Meta loss on this task batch = 2.8557e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 174.3583, GNorm = 0.3414
Meta loss on this task batch = 3.0598e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 174.3627, GNorm = 0.2970
Meta loss on this task batch = 2.3813e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 174.3673, GNorm = 0.2529
Meta loss on this task batch = 2.7880e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 174.3721, GNorm = 0.2477
Meta loss on this task batch = 2.7770e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 174.3753, GNorm = 0.3118
Meta loss on this task batch = 2.6033e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 174.3785, GNorm = 0.3079
Meta loss on this task batch = 2.3900e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 174.3817, GNorm = 0.4548
Meta loss on this task batch = 2.7992e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 174.3850, GNorm = 0.2695
Meta loss on this task batch = 3.0043e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 174.3883, GNorm = 0.2754
Meta loss on this task batch = 2.6077e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 174.3909, GNorm = 0.2785
Meta loss on this task batch = 3.0144e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 174.3932, GNorm = 0.3682
Meta loss on this task batch = 2.6371e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 174.3959, GNorm = 0.3077
Meta loss on this task batch = 3.0967e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 174.3982, GNorm = 0.3498
Took 111.96645832061768 seconds to complete one epoch of meta training
Took 119.54899954795837 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.449840
Epoch 994
Meta loss on this task batch = 3.3940e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 174.4002, GNorm = 0.4108
Meta loss on this task batch = 2.7174e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 174.4026, GNorm = 0.3618
Meta loss on this task batch = 2.9152e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 174.4055, GNorm = 0.4689
Meta loss on this task batch = 2.4126e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 174.4088, GNorm = 0.3068
Meta loss on this task batch = 2.5132e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 174.4125, GNorm = 0.2864
Meta loss on this task batch = 2.3128e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 174.4168, GNorm = 0.2677
Meta loss on this task batch = 2.8832e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 174.4208, GNorm = 0.3313
Meta loss on this task batch = 2.2820e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 174.4252, GNorm = 0.2244
Meta loss on this task batch = 2.5820e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 174.4296, GNorm = 0.2762
Meta loss on this task batch = 2.6089e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 174.4346, GNorm = 0.5504
Meta loss on this task batch = 2.8030e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 174.4398, GNorm = 0.3136
Meta loss on this task batch = 2.6492e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 174.4452, GNorm = 0.2936
Meta loss on this task batch = 3.0023e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 174.4501, GNorm = 0.2570
Meta loss on this task batch = 2.8609e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 174.4551, GNorm = 0.3504
Meta loss on this task batch = 2.7839e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 174.4608, GNorm = 0.2969
Meta loss on this task batch = 2.9346e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 174.4659, GNorm = 0.2916
Meta loss on this task batch = 3.1782e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 174.4707, GNorm = 0.3525
Meta loss on this task batch = 2.5412e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 174.4755, GNorm = 0.3076
Meta loss on this task batch = 2.2586e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 174.4800, GNorm = 0.2853
Took 110.82274603843689 seconds to complete one epoch of meta training
Took 117.60304403305054 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461659
Epoch 995
Meta loss on this task batch = 2.1183e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 174.4848, GNorm = 0.2584
Meta loss on this task batch = 3.1266e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 174.4892, GNorm = 0.3277
Meta loss on this task batch = 2.9015e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 174.4932, GNorm = 0.3146
Meta loss on this task batch = 2.7493e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 174.4967, GNorm = 0.2954
Meta loss on this task batch = 2.5481e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 174.5008, GNorm = 0.3266
Meta loss on this task batch = 3.0507e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 174.5046, GNorm = 0.3901
Meta loss on this task batch = 2.2245e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 174.5082, GNorm = 0.3347
Meta loss on this task batch = 2.8835e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 174.5118, GNorm = 0.2896
Meta loss on this task batch = 2.1641e-01, Meta loss averaged over last 500 steps = 2.7405e-01, PNorm = 174.5155, GNorm = 0.2987
Meta loss on this task batch = 2.6653e-01, Meta loss averaged over last 500 steps = 2.7399e-01, PNorm = 174.5194, GNorm = 0.3181
Meta loss on this task batch = 2.5969e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 174.5236, GNorm = 0.2530
Meta loss on this task batch = 2.5252e-01, Meta loss averaged over last 500 steps = 2.7399e-01, PNorm = 174.5278, GNorm = 0.3626
Meta loss on this task batch = 3.3346e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 174.5309, GNorm = 0.2782
Meta loss on this task batch = 2.5566e-01, Meta loss averaged over last 500 steps = 2.7408e-01, PNorm = 174.5343, GNorm = 0.2719
Meta loss on this task batch = 2.0289e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 174.5383, GNorm = 0.4420
Meta loss on this task batch = 3.1584e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 174.5419, GNorm = 0.3014
Meta loss on this task batch = 2.6592e-01, Meta loss averaged over last 500 steps = 2.7408e-01, PNorm = 174.5457, GNorm = 0.2714
Meta loss on this task batch = 2.9831e-01, Meta loss averaged over last 500 steps = 2.7411e-01, PNorm = 174.5497, GNorm = 0.3155
Meta loss on this task batch = 2.6812e-01, Meta loss averaged over last 500 steps = 2.7409e-01, PNorm = 174.5533, GNorm = 0.3290
Took 109.22916173934937 seconds to complete one epoch of meta training
Took 117.6141345500946 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454342
Epoch 996
Meta loss on this task batch = 2.6280e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 174.5569, GNorm = 0.3232
Meta loss on this task batch = 2.7156e-01, Meta loss averaged over last 500 steps = 2.7404e-01, PNorm = 174.5605, GNorm = 0.2829
Meta loss on this task batch = 2.0394e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 174.5645, GNorm = 0.2601
Meta loss on this task batch = 2.6978e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 174.5673, GNorm = 0.3563
Meta loss on this task batch = 2.7376e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 174.5702, GNorm = 0.2773
Meta loss on this task batch = 2.5987e-01, Meta loss averaged over last 500 steps = 2.7391e-01, PNorm = 174.5730, GNorm = 0.2693
Meta loss on this task batch = 2.2682e-01, Meta loss averaged over last 500 steps = 2.7383e-01, PNorm = 174.5757, GNorm = 0.3035
Meta loss on this task batch = 2.5072e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 174.5788, GNorm = 0.3565
Meta loss on this task batch = 2.9840e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 174.5826, GNorm = 0.2948
Meta loss on this task batch = 2.6582e-01, Meta loss averaged over last 500 steps = 2.7363e-01, PNorm = 174.5859, GNorm = 0.2739
Meta loss on this task batch = 3.0350e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 174.5890, GNorm = 0.3533
Meta loss on this task batch = 2.4367e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 174.5926, GNorm = 0.2891
Meta loss on this task batch = 2.8634e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 174.5959, GNorm = 0.2683
Meta loss on this task batch = 2.6774e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 174.5990, GNorm = 0.2547
Meta loss on this task batch = 3.2911e-01, Meta loss averaged over last 500 steps = 2.7383e-01, PNorm = 174.6022, GNorm = 0.3316
Meta loss on this task batch = 3.0016e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 174.6050, GNorm = 0.3684
Meta loss on this task batch = 2.7250e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 174.6078, GNorm = 0.3186
Meta loss on this task batch = 2.6102e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 174.6114, GNorm = 0.2898
Meta loss on this task batch = 2.4235e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 174.6150, GNorm = 0.3622
Took 110.8651978969574 seconds to complete one epoch of meta training
Took 119.03478336334229 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450532
Epoch 997
Meta loss on this task batch = 2.9079e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 174.6188, GNorm = 0.3172
Meta loss on this task batch = 2.4129e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 174.6229, GNorm = 0.2294
Meta loss on this task batch = 2.7515e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 174.6269, GNorm = 0.3440
Meta loss on this task batch = 2.5772e-01, Meta loss averaged over last 500 steps = 2.7377e-01, PNorm = 174.6312, GNorm = 0.2825
Meta loss on this task batch = 2.2994e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 174.6360, GNorm = 0.2614
Meta loss on this task batch = 2.7415e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 174.6408, GNorm = 0.2530
Meta loss on this task batch = 2.5975e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 174.6456, GNorm = 0.2663
Meta loss on this task batch = 2.9412e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 174.6504, GNorm = 0.3699
Meta loss on this task batch = 2.1990e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 174.6557, GNorm = 0.3030
Meta loss on this task batch = 3.1390e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 174.6607, GNorm = 0.2534
Meta loss on this task batch = 2.8167e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 174.6653, GNorm = 0.2913
Meta loss on this task batch = 2.8599e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 174.6693, GNorm = 0.4160
Meta loss on this task batch = 2.5141e-01, Meta loss averaged over last 500 steps = 2.7381e-01, PNorm = 174.6727, GNorm = 0.2500
Meta loss on this task batch = 2.7091e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 174.6760, GNorm = 0.3340
Meta loss on this task batch = 2.6563e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 174.6790, GNorm = 0.2644
Meta loss on this task batch = 2.5963e-01, Meta loss averaged over last 500 steps = 2.7363e-01, PNorm = 174.6819, GNorm = 0.3043
Meta loss on this task batch = 3.1073e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 174.6846, GNorm = 0.3201
Meta loss on this task batch = 2.7359e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 174.6875, GNorm = 0.2674
Meta loss on this task batch = 2.9111e-01, Meta loss averaged over last 500 steps = 2.7346e-01, PNorm = 174.6905, GNorm = 0.4199
Took 109.92055678367615 seconds to complete one epoch of meta training
Took 117.92794823646545 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468103
Epoch 998
Meta loss on this task batch = 2.3742e-01, Meta loss averaged over last 500 steps = 2.7350e-01, PNorm = 174.6939, GNorm = 0.2210
Meta loss on this task batch = 2.3034e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 174.6973, GNorm = 0.2540
Meta loss on this task batch = 2.4745e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 174.7013, GNorm = 0.3090
Meta loss on this task batch = 2.1519e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 174.7053, GNorm = 0.3002
Meta loss on this task batch = 3.1478e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 174.7098, GNorm = 0.3692
Meta loss on this task batch = 2.9079e-01, Meta loss averaged over last 500 steps = 2.7355e-01, PNorm = 174.7140, GNorm = 0.3754
Meta loss on this task batch = 2.9765e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 174.7180, GNorm = 0.2552
Meta loss on this task batch = 2.9753e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 174.7216, GNorm = 0.3051
Meta loss on this task batch = 2.6297e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 174.7248, GNorm = 0.3174
Meta loss on this task batch = 2.2814e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 174.7286, GNorm = 0.2524
Meta loss on this task batch = 2.9184e-01, Meta loss averaged over last 500 steps = 2.7327e-01, PNorm = 174.7322, GNorm = 0.3658
Meta loss on this task batch = 2.7300e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 174.7360, GNorm = 0.2596
Meta loss on this task batch = 2.3838e-01, Meta loss averaged over last 500 steps = 2.7319e-01, PNorm = 174.7402, GNorm = 0.3496
Meta loss on this task batch = 2.8191e-01, Meta loss averaged over last 500 steps = 2.7323e-01, PNorm = 174.7445, GNorm = 0.3129
Meta loss on this task batch = 2.1104e-01, Meta loss averaged over last 500 steps = 2.7312e-01, PNorm = 174.7491, GNorm = 0.4028
Meta loss on this task batch = 2.5931e-01, Meta loss averaged over last 500 steps = 2.7296e-01, PNorm = 174.7538, GNorm = 0.2379
Meta loss on this task batch = 3.0410e-01, Meta loss averaged over last 500 steps = 2.7304e-01, PNorm = 174.7580, GNorm = 0.4259
Meta loss on this task batch = 2.5817e-01, Meta loss averaged over last 500 steps = 2.7306e-01, PNorm = 174.7621, GNorm = 0.3285
Meta loss on this task batch = 2.4309e-01, Meta loss averaged over last 500 steps = 2.7312e-01, PNorm = 174.7667, GNorm = 0.3535
Took 110.04407620429993 seconds to complete one epoch of meta training
Took 117.48096799850464 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472038
Epoch 999
Meta loss on this task batch = 2.7044e-01, Meta loss averaged over last 500 steps = 2.7309e-01, PNorm = 174.7712, GNorm = 0.4160
Meta loss on this task batch = 2.9782e-01, Meta loss averaged over last 500 steps = 2.7311e-01, PNorm = 174.7752, GNorm = 0.2878
Meta loss on this task batch = 2.1741e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 174.7790, GNorm = 0.2553
Meta loss on this task batch = 2.8085e-01, Meta loss averaged over last 500 steps = 2.7302e-01, PNorm = 174.7824, GNorm = 0.3164
Meta loss on this task batch = 2.7506e-01, Meta loss averaged over last 500 steps = 2.7300e-01, PNorm = 174.7854, GNorm = 0.2774
Meta loss on this task batch = 2.3634e-01, Meta loss averaged over last 500 steps = 2.7279e-01, PNorm = 174.7885, GNorm = 0.2673
Meta loss on this task batch = 2.2446e-01, Meta loss averaged over last 500 steps = 2.7264e-01, PNorm = 174.7917, GNorm = 0.3156
Meta loss on this task batch = 2.5935e-01, Meta loss averaged over last 500 steps = 2.7270e-01, PNorm = 174.7947, GNorm = 0.2943
Meta loss on this task batch = 3.1804e-01, Meta loss averaged over last 500 steps = 2.7280e-01, PNorm = 174.7980, GNorm = 0.2840
Meta loss on this task batch = 2.6512e-01, Meta loss averaged over last 500 steps = 2.7277e-01, PNorm = 174.8009, GNorm = 0.2593
Meta loss on this task batch = 3.1212e-01, Meta loss averaged over last 500 steps = 2.7277e-01, PNorm = 174.8034, GNorm = 0.2711
Meta loss on this task batch = 2.6831e-01, Meta loss averaged over last 500 steps = 2.7276e-01, PNorm = 174.8057, GNorm = 0.2693
Meta loss on this task batch = 2.2288e-01, Meta loss averaged over last 500 steps = 2.7274e-01, PNorm = 174.8080, GNorm = 0.2520
Meta loss on this task batch = 2.5754e-01, Meta loss averaged over last 500 steps = 2.7262e-01, PNorm = 174.8102, GNorm = 0.2879
Meta loss on this task batch = 2.6141e-01, Meta loss averaged over last 500 steps = 2.7271e-01, PNorm = 174.8126, GNorm = 0.2794
Meta loss on this task batch = 2.4942e-01, Meta loss averaged over last 500 steps = 2.7279e-01, PNorm = 174.8158, GNorm = 0.2458
Meta loss on this task batch = 2.7042e-01, Meta loss averaged over last 500 steps = 2.7276e-01, PNorm = 174.8190, GNorm = 0.3380
Meta loss on this task batch = 3.0760e-01, Meta loss averaged over last 500 steps = 2.7287e-01, PNorm = 174.8217, GNorm = 0.2988
Meta loss on this task batch = 2.6363e-01, Meta loss averaged over last 500 steps = 2.7289e-01, PNorm = 174.8247, GNorm = 0.4301
Took 112.0982301235199 seconds to complete one epoch of meta training
Took 119.85566639900208 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461382
Best validation prc-auc = 0.521901 on epoch 246
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Beginning meta testing
Meta testing on task: CHEMBL1794355
New best model for test task CHEMBL1794355 at epoch 1 with val loss 0.5879532098770142
New best model for test task CHEMBL1794355 at epoch 2 with val loss 0.5830938220024109
New best model for test task CHEMBL1794355 at epoch 3 with val loss 0.5783840417861938
New best model for test task CHEMBL1794355 at epoch 4 with val loss 0.5736433267593384
New best model for test task CHEMBL1794355 at epoch 5 with val loss 0.5686453580856323
New best model for test task CHEMBL1794355 at epoch 6 with val loss 0.5637730956077576
New best model for test task CHEMBL1794355 at epoch 7 with val loss 0.5587974786758423
New best model for test task CHEMBL1794355 at epoch 8 with val loss 0.554086446762085
New best model for test task CHEMBL1794355 at epoch 9 with val loss 0.5492749810218811
New best model for test task CHEMBL1794355 at epoch 10 with val loss 0.5447543859481812
New best model for test task CHEMBL1794355 at epoch 11 with val loss 0.5401938557624817
New best model for test task CHEMBL1794355 at epoch 12 with val loss 0.5357526540756226
New best model for test task CHEMBL1794355 at epoch 13 with val loss 0.5313794016838074
New best model for test task CHEMBL1794355 at epoch 14 with val loss 0.5272440910339355
New best model for test task CHEMBL1794355 at epoch 15 with val loss 0.5231024622917175
New best model for test task CHEMBL1794355 at epoch 16 with val loss 0.5188983082771301
New best model for test task CHEMBL1794355 at epoch 17 with val loss 0.514831006526947
New best model for test task CHEMBL1794355 at epoch 18 with val loss 0.5109775066375732
New best model for test task CHEMBL1794355 at epoch 19 with val loss 0.5070004463195801
New best model for test task CHEMBL1794355 at epoch 20 with val loss 0.5031664967536926
New best model for test task CHEMBL1794355 at epoch 21 with val loss 0.4996749460697174
New best model for test task CHEMBL1794355 at epoch 22 with val loss 0.4958849847316742
New best model for test task CHEMBL1794355 at epoch 23 with val loss 0.49230659008026123
New best model for test task CHEMBL1794355 at epoch 24 with val loss 0.48880326747894287
New best model for test task CHEMBL1794355 at epoch 25 with val loss 0.48561516404151917
New best model for test task CHEMBL1794355 at epoch 26 with val loss 0.4822523891925812
New best model for test task CHEMBL1794355 at epoch 27 with val loss 0.47888511419296265
New best model for test task CHEMBL1794355 at epoch 28 with val loss 0.47565650939941406
New best model for test task CHEMBL1794355 at epoch 29 with val loss 0.47275808453559875
New best model for test task CHEMBL1794355 at epoch 30 with val loss 0.46973735094070435
Finished early stopping for task CHEMBL1794355, beginning testing
Meta testing on task: CHEMBL2098499
New best model for test task CHEMBL2098499 at epoch 1 with val loss 0.7010676860809326
Val loss: 0.7011206150054932
Val loss: 0.7012010812759399
Val loss: 0.7012449502944946
Val loss: 0.7012951970100403
Val loss: 0.7013358473777771
Val loss: 0.7014182209968567
Val loss: 0.7014744877815247
Val loss: 0.7015321254730225
Val loss: 0.7016022205352783
Val loss: 0.7016439437866211
Val loss: 0.7016962766647339
Val loss: 0.7017681002616882
Val loss: 0.7018328905105591
Val loss: 0.7018887996673584
Val loss: 0.7019386291503906
Val loss: 0.702005922794342
Val loss: 0.7020744681358337
Val loss: 0.7021296620368958
Val loss: 0.7021949887275696
Val loss: 0.7022655010223389
Val loss: 0.7023277282714844
Val loss: 0.7024085521697998
Val loss: 0.7024887204170227
Val loss: 0.7025498747825623
Val loss: 0.7026282548904419
Val loss: 0.7027175426483154
Val loss: 0.7027814388275146
Val loss: 0.7028684020042419
Val loss: 0.7029367089271545
Finished early stopping for task CHEMBL2098499, beginning testing
Meta testing on task: CHEMBL1738131
New best model for test task CHEMBL1738131 at epoch 1 with val loss 0.7039540708065033
Val loss: 0.7052567601203918
Val loss: 0.708537369966507
Val loss: 0.7058611512184143
Val loss: 0.7207525670528412
New best model for test task CHEMBL1738131 at epoch 6 with val loss 0.6931692659854889
Val loss: 0.7191183865070343
Val loss: 0.7062870860099792
Val loss: 0.7243117690086365
Val loss: 0.711742490530014
Val loss: 0.705917477607727
Val loss: 0.7196171283721924
Val loss: 0.7165705859661102
Val loss: 0.7174100279808044
Val loss: 0.7142275273799896
Val loss: 0.7186844646930695
Val loss: 0.7172839343547821
Val loss: 0.71329265832901
Val loss: 0.7097291052341461
Val loss: 0.7153827846050262
Val loss: 0.7184789776802063
Val loss: 0.7052196562290192
Val loss: 0.7152227759361267
Val loss: 0.7285653352737427
Val loss: 0.725687712430954
Val loss: 0.7025565505027771
Val loss: 0.7218846678733826
Val loss: 0.728486567735672
Val loss: 0.7364625632762909
Val loss: 0.7101708054542542
Finished early stopping for task CHEMBL1738131, beginning testing
Meta testing on task: CHEMBL918058
New best model for test task CHEMBL918058 at epoch 1 with val loss 0.7350482940673828
Val loss: 0.7353589534759521
Val loss: 0.7356507778167725
Val loss: 0.7359037399291992
Val loss: 0.7361817359924316
Val loss: 0.7364117503166199
Val loss: 0.7367137670516968
Val loss: 0.7370110750198364
Val loss: 0.7372596263885498
Val loss: 0.7375901937484741
Val loss: 0.7379472851753235
Val loss: 0.7381688952445984
Val loss: 0.7384740710258484
Val loss: 0.7387388348579407
Val loss: 0.7390093803405762
Val loss: 0.7393134832382202
Val loss: 0.7396039962768555
Val loss: 0.7398421168327332
Val loss: 0.7401520609855652
Val loss: 0.740387499332428
Val loss: 0.7406266331672668
Val loss: 0.7408787608146667
Val loss: 0.7411662340164185
Val loss: 0.7414336204528809
Val loss: 0.7417251467704773
Val loss: 0.7419732213020325
Val loss: 0.7422139048576355
Val loss: 0.7424310445785522
Val loss: 0.7427359819412231
Val loss: 0.7429898381233215
Finished early stopping for task CHEMBL918058, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL2095143
New best model for test task CHEMBL2095143 at epoch 1 with val loss 0.7851425409317017
New best model for test task CHEMBL2095143 at epoch 2 with val loss 0.7848494052886963
New best model for test task CHEMBL2095143 at epoch 3 with val loss 0.7846165299415588
New best model for test task CHEMBL2095143 at epoch 4 with val loss 0.7844249606132507
New best model for test task CHEMBL2095143 at epoch 5 with val loss 0.7842019200325012
New best model for test task CHEMBL2095143 at epoch 6 with val loss 0.7839747071266174
New best model for test task CHEMBL2095143 at epoch 7 with val loss 0.7836940884590149
New best model for test task CHEMBL2095143 at epoch 8 with val loss 0.7835445404052734
New best model for test task CHEMBL2095143 at epoch 9 with val loss 0.7833558320999146
New best model for test task CHEMBL2095143 at epoch 10 with val loss 0.7831258773803711
New best model for test task CHEMBL2095143 at epoch 11 with val loss 0.7829129099845886
New best model for test task CHEMBL2095143 at epoch 12 with val loss 0.7827233076095581
New best model for test task CHEMBL2095143 at epoch 13 with val loss 0.7825556397438049
New best model for test task CHEMBL2095143 at epoch 14 with val loss 0.7823401689529419
New best model for test task CHEMBL2095143 at epoch 15 with val loss 0.7821296453475952
New best model for test task CHEMBL2095143 at epoch 16 with val loss 0.7819181680679321
New best model for test task CHEMBL2095143 at epoch 17 with val loss 0.781722903251648
New best model for test task CHEMBL2095143 at epoch 18 with val loss 0.7814650535583496
New best model for test task CHEMBL2095143 at epoch 19 with val loss 0.7812333106994629
New best model for test task CHEMBL2095143 at epoch 20 with val loss 0.7810961008071899
New best model for test task CHEMBL2095143 at epoch 21 with val loss 0.7809192538261414
New best model for test task CHEMBL2095143 at epoch 22 with val loss 0.7806666493415833
New best model for test task CHEMBL2095143 at epoch 23 with val loss 0.7805784940719604
New best model for test task CHEMBL2095143 at epoch 24 with val loss 0.7804319858551025
New best model for test task CHEMBL2095143 at epoch 25 with val loss 0.780234158039093
New best model for test task CHEMBL2095143 at epoch 26 with val loss 0.780025064945221
New best model for test task CHEMBL2095143 at epoch 27 with val loss 0.7798941135406494
New best model for test task CHEMBL2095143 at epoch 28 with val loss 0.779626190662384
New best model for test task CHEMBL2095143 at epoch 29 with val loss 0.7793872356414795
New best model for test task CHEMBL2095143 at epoch 30 with val loss 0.7792415618896484
Finished early stopping for task CHEMBL2095143, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1614170
New best model for test task CHEMBL1614170 at epoch 1 with val loss 0.7013399004936218
Val loss: 0.7114932239055634
Val loss: 0.7060091197490692
Val loss: 0.7049678564071655
Val loss: 0.7048316299915314
Val loss: 0.7183900475502014
Val loss: 0.7136965692043304
Val loss: 0.7173359096050262
Val loss: 0.7081818580627441
Val loss: 0.7125046849250793
Val loss: 0.7205646634101868
Val loss: 0.7082004845142365
Val loss: 0.7287567257881165
Val loss: 0.7151521444320679
Val loss: 0.7127422392368317
Val loss: 0.7186915278434753
Val loss: 0.714701235294342
Val loss: 0.721868097782135
Val loss: 0.7217597663402557
Val loss: 0.7315057814121246
Val loss: 0.722409725189209
Val loss: 0.711357444524765
Val loss: 0.7278362810611725
Val loss: 0.7234525978565216
Val loss: 0.7298191785812378
Val loss: 0.7168210744857788
Val loss: 0.7278469800949097
Val loss: 0.7319675087928772
Val loss: 0.725795567035675
Val loss: 0.7299481332302094
Finished early stopping for task CHEMBL1614170, beginning testing
Meta testing on task: CHEMBL1963705
New best model for test task CHEMBL1963705 at epoch 1 with val loss 0.6012715895970663
Val loss: 0.6277872522672018
New best model for test task CHEMBL1963705 at epoch 3 with val loss 0.5894337097803751
Val loss: 0.6648054321606954
New best model for test task CHEMBL1963705 at epoch 5 with val loss 0.5893615086873373
Val loss: 0.6198526422182719
Val loss: 0.5972169041633606
Val loss: 0.5932720899581909
Val loss: 0.6246218880017599
Val loss: 0.5973395705223083
Val loss: 0.609808623790741
Val loss: 0.5969534118970236
Val loss: 0.6589877605438232
Val loss: 0.5924466649691263
Val loss: 0.6081288854281107
New best model for test task CHEMBL1963705 at epoch 16 with val loss 0.581862191359202
Val loss: 0.6189841230710348
Val loss: 0.6244815389315287
Val loss: 0.5896016160647074
Val loss: 0.6004306872685751
New best model for test task CHEMBL1963705 at epoch 21 with val loss 0.5807865858078003
New best model for test task CHEMBL1963705 at epoch 22 with val loss 0.5739288330078125
Val loss: 0.5956253608067831
New best model for test task CHEMBL1963705 at epoch 24 with val loss 0.5637768705685934
Val loss: 0.5649605492750803
Val loss: 0.587047298749288
Val loss: 0.5698738495508829
Val loss: 0.5772752364476522
Val loss: 0.6360582907994589
Val loss: 0.586529811223348
Finished early stopping for task CHEMBL1963705, beginning testing
Meta testing on task: CHEMBL1909212
New best model for test task CHEMBL1909212 at epoch 1 with val loss 0.7317670782407125
New best model for test task CHEMBL1909212 at epoch 2 with val loss 0.7305440902709961
New best model for test task CHEMBL1909212 at epoch 3 with val loss 0.7287585934003195
Val loss: 0.7312581737836202
Val loss: 0.7295389771461487
New best model for test task CHEMBL1909212 at epoch 6 with val loss 0.7284785509109497
New best model for test task CHEMBL1909212 at epoch 7 with val loss 0.7270452777544657
Val loss: 0.7311480244000753
Val loss: 0.7303951581319174
Val loss: 0.7288181583086649
New best model for test task CHEMBL1909212 at epoch 11 with val loss 0.7261091868082682
Val loss: 0.726877490679423
New best model for test task CHEMBL1909212 at epoch 13 with val loss 0.7242583235104879
Val loss: 0.7264517545700073
Val loss: 0.7243249615033468
Val loss: 0.7264050245285034
Val loss: 0.7253989974657694
Val loss: 0.7266918420791626
Val loss: 0.7242963910102844
Val loss: 0.7243900696436564
Val loss: 0.7244518001874288
Val loss: 0.7252016067504883
Val loss: 0.7255366245905558
Val loss: 0.7261640032132467
Val loss: 0.725061317284902
Val loss: 0.7254745165506998
New best model for test task CHEMBL1909212 at epoch 27 with val loss 0.723946213722229
New best model for test task CHEMBL1909212 at epoch 28 with val loss 0.7194245258967081
Val loss: 0.723187526067098
Val loss: 0.7200971245765686
Finished early stopping for task CHEMBL1909212, beginning testing
Meta testing on task: CHEMBL2114797
New best model for test task CHEMBL2114797 at epoch 1 with val loss 0.7833674550056458
Val loss: 0.7836110591888428
Val loss: 0.7838389277458191
Val loss: 0.7840861678123474
Val loss: 0.7843671441078186
Val loss: 0.7846066355705261
Val loss: 0.7848643660545349
Val loss: 0.7851302623748779
Val loss: 0.7853485941886902
Val loss: 0.785610020160675
Val loss: 0.7858189940452576
Val loss: 0.7860299944877625
Val loss: 0.7862634062767029
Val loss: 0.7865134477615356
Val loss: 0.7867909669876099
Val loss: 0.7870561480522156
Val loss: 0.7873461842536926
Val loss: 0.787611186504364
Val loss: 0.7878760695457458
Val loss: 0.7881808876991272
Val loss: 0.7884396910667419
Val loss: 0.7887397408485413
Val loss: 0.7890216708183289
Val loss: 0.7893052697181702
Val loss: 0.7895755171775818
Val loss: 0.7898613810539246
Val loss: 0.79010009765625
Val loss: 0.7903702855110168
Val loss: 0.7906475067138672
Val loss: 0.7908852696418762
Finished early stopping for task CHEMBL2114797, beginning testing
Meta testing on task: CHEMBL1963934
New best model for test task CHEMBL1963934 at epoch 1 with val loss 0.586707592010498
New best model for test task CHEMBL1963934 at epoch 2 with val loss 0.5825414657592773
New best model for test task CHEMBL1963934 at epoch 3 with val loss 0.5787830948829651
New best model for test task CHEMBL1963934 at epoch 4 with val loss 0.575070858001709
New best model for test task CHEMBL1963934 at epoch 5 with val loss 0.571419358253479
New best model for test task CHEMBL1963934 at epoch 6 with val loss 0.5677707195281982
New best model for test task CHEMBL1963934 at epoch 7 with val loss 0.5639265179634094
New best model for test task CHEMBL1963934 at epoch 8 with val loss 0.5602672100067139
New best model for test task CHEMBL1963934 at epoch 9 with val loss 0.5567594766616821
New best model for test task CHEMBL1963934 at epoch 10 with val loss 0.5530178546905518
New best model for test task CHEMBL1963934 at epoch 11 with val loss 0.549526035785675
New best model for test task CHEMBL1963934 at epoch 12 with val loss 0.5460127592086792
New best model for test task CHEMBL1963934 at epoch 13 with val loss 0.5424166917800903
New best model for test task CHEMBL1963934 at epoch 14 with val loss 0.5390567779541016
New best model for test task CHEMBL1963934 at epoch 15 with val loss 0.5359764099121094
New best model for test task CHEMBL1963934 at epoch 16 with val loss 0.5327408313751221
New best model for test task CHEMBL1963934 at epoch 17 with val loss 0.5295798778533936
New best model for test task CHEMBL1963934 at epoch 18 with val loss 0.5266919136047363
New best model for test task CHEMBL1963934 at epoch 19 with val loss 0.5237820148468018
New best model for test task CHEMBL1963934 at epoch 20 with val loss 0.5208038091659546
New best model for test task CHEMBL1963934 at epoch 21 with val loss 0.5177446007728577
New best model for test task CHEMBL1963934 at epoch 22 with val loss 0.514860987663269
New best model for test task CHEMBL1963934 at epoch 23 with val loss 0.5121448040008545
New best model for test task CHEMBL1963934 at epoch 24 with val loss 0.5095229148864746
New best model for test task CHEMBL1963934 at epoch 25 with val loss 0.5068315267562866
New best model for test task CHEMBL1963934 at epoch 26 with val loss 0.5038796663284302
New best model for test task CHEMBL1963934 at epoch 27 with val loss 0.5010061860084534
New best model for test task CHEMBL1963934 at epoch 28 with val loss 0.4984785318374634
New best model for test task CHEMBL1963934 at epoch 29 with val loss 0.4961569011211395
New best model for test task CHEMBL1963934 at epoch 30 with val loss 0.4934593141078949
Finished early stopping for task CHEMBL1963934, beginning testing
Meta testing on task: CHEMBL1909209
New best model for test task CHEMBL1909209 at epoch 1 with val loss 0.7107025980949402
New best model for test task CHEMBL1909209 at epoch 2 with val loss 0.7091624140739441
Val loss: 0.7094247539838155
New best model for test task CHEMBL1909209 at epoch 4 with val loss 0.7084171175956726
New best model for test task CHEMBL1909209 at epoch 5 with val loss 0.7080718080202738
Val loss: 0.7099503477414449
New best model for test task CHEMBL1909209 at epoch 7 with val loss 0.7066953579584757
Val loss: 0.7080957889556885
Val loss: 0.7099388043085734
Val loss: 0.7128323316574097
Val loss: 0.7110897699991862
Val loss: 0.7133415540059408
Val loss: 0.7103476723035177
Val loss: 0.7085105776786804
Val loss: 0.7106470664342245
Val loss: 0.7127422293027242
Val loss: 0.709009567896525
Val loss: 0.708603302637736
Val loss: 0.7107819318771362
Val loss: 0.7109023928642273
Val loss: 0.7100190122922262
Val loss: 0.7105528116226196
Val loss: 0.7117772301038107
Val loss: 0.7099754412968954
Val loss: 0.7086159984270731
Val loss: 0.7092565298080444
Val loss: 0.7096116344134012
Val loss: 0.7114981015523275
Val loss: 0.7085304260253906
Val loss: 0.7099878390630087
Finished early stopping for task CHEMBL1909209, beginning testing
Meta testing on task: CHEMBL1909211
New best model for test task CHEMBL1909211 at epoch 1 with val loss 0.7085906863212585
New best model for test task CHEMBL1909211 at epoch 2 with val loss 0.7080867290496826
Val loss: 0.7092364430427551
Val loss: 0.7089000542958578
Val loss: 0.7110761801401774
Val loss: 0.7099283734957377
Val loss: 0.7082433303197225
Val loss: 0.7129024068514506
Val loss: 0.7103280226389567
Val loss: 0.7111449440320333
Val loss: 0.7098473310470581
Val loss: 0.7103528380393982
New best model for test task CHEMBL1909211 at epoch 13 with val loss 0.7074010968208313
Val loss: 0.7138331333796183
Val loss: 0.7107080618540446
Val loss: 0.7103240688641866
Val loss: 0.7142815391222636
Val loss: 0.7117828726768494
Val loss: 0.7106481393178304
Val loss: 0.7109974026679993
Val loss: 0.7111339370409647
Val loss: 0.7111373742421468
Val loss: 0.7108554442723592
Val loss: 0.7106440862019857
Val loss: 0.7086788018544515
Val loss: 0.7087318102518717
Val loss: 0.7112786173820496
Val loss: 0.7143860260645548
Val loss: 0.7108990748723348
Val loss: 0.7133084932963053
Finished early stopping for task CHEMBL1909211, beginning testing
Meta testing on task: CHEMBL1909085
New best model for test task CHEMBL1909085 at epoch 1 with val loss 0.703647514184316
Val loss: 0.7047793666521708
Val loss: 0.7049573461214701
New best model for test task CHEMBL1909085 at epoch 4 with val loss 0.7003486553827921
Val loss: 0.7065358360608419
Val loss: 0.7085816264152527
Val loss: 0.7104290922482809
Val loss: 0.7035489281018575
Val loss: 0.7051870624224345
Val loss: 0.7030905882517496
Val loss: 0.7063964207967123
Val loss: 0.7037659883499146
Val loss: 0.7059071063995361
Val loss: 0.7044176856676737
Val loss: 0.7030026316642761
Val loss: 0.7020681500434875
Val loss: 0.707602063814799
New best model for test task CHEMBL1909085 at epoch 18 with val loss 0.6997703313827515
Val loss: 0.709914763768514
Val loss: 0.7083730101585388
Val loss: 0.7037815650304159
Val loss: 0.7030119697252909
Val loss: 0.7050437529881796
Val loss: 0.7068083683649699
Val loss: 0.7008223334948221
Val loss: 0.7100345094998678
Val loss: 0.7085847854614258
Val loss: 0.7077489693959554
Val loss: 0.7004856069882711
Val loss: 0.7034568985303243
Finished early stopping for task CHEMBL1909085, beginning testing
Meta testing on task: CHEMBL1738202
New best model for test task CHEMBL1738202 at epoch 1 with val loss 0.6588054895401001
New best model for test task CHEMBL1738202 at epoch 2 with val loss 0.6573642492294312
New best model for test task CHEMBL1738202 at epoch 3 with val loss 0.6559867858886719
New best model for test task CHEMBL1738202 at epoch 4 with val loss 0.6544961929321289
New best model for test task CHEMBL1738202 at epoch 5 with val loss 0.6530718803405762
New best model for test task CHEMBL1738202 at epoch 6 with val loss 0.6516613364219666
New best model for test task CHEMBL1738202 at epoch 7 with val loss 0.6502116322517395
New best model for test task CHEMBL1738202 at epoch 8 with val loss 0.6487048268318176
New best model for test task CHEMBL1738202 at epoch 9 with val loss 0.6472329497337341
New best model for test task CHEMBL1738202 at epoch 10 with val loss 0.6457073092460632
New best model for test task CHEMBL1738202 at epoch 11 with val loss 0.6441352367401123
New best model for test task CHEMBL1738202 at epoch 12 with val loss 0.6425026059150696
New best model for test task CHEMBL1738202 at epoch 13 with val loss 0.6409327387809753
New best model for test task CHEMBL1738202 at epoch 14 with val loss 0.6392998099327087
New best model for test task CHEMBL1738202 at epoch 15 with val loss 0.6376729011535645
New best model for test task CHEMBL1738202 at epoch 16 with val loss 0.6360111236572266
New best model for test task CHEMBL1738202 at epoch 17 with val loss 0.6344025135040283
New best model for test task CHEMBL1738202 at epoch 18 with val loss 0.6327380537986755
New best model for test task CHEMBL1738202 at epoch 19 with val loss 0.6310669779777527
New best model for test task CHEMBL1738202 at epoch 20 with val loss 0.6294230222702026
New best model for test task CHEMBL1738202 at epoch 21 with val loss 0.627724826335907
New best model for test task CHEMBL1738202 at epoch 22 with val loss 0.626029908657074
New best model for test task CHEMBL1738202 at epoch 23 with val loss 0.6242930889129639
New best model for test task CHEMBL1738202 at epoch 24 with val loss 0.6224925518035889
New best model for test task CHEMBL1738202 at epoch 25 with val loss 0.6206907033920288
New best model for test task CHEMBL1738202 at epoch 26 with val loss 0.6188687086105347
New best model for test task CHEMBL1738202 at epoch 27 with val loss 0.6170970797538757
New best model for test task CHEMBL1738202 at epoch 28 with val loss 0.6151981353759766
New best model for test task CHEMBL1738202 at epoch 29 with val loss 0.6133005023002625
New best model for test task CHEMBL1738202 at epoch 30 with val loss 0.6114190220832825
Finished early stopping for task CHEMBL1738202, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1738019
New best model for test task CHEMBL1738019 at epoch 1 with val loss 0.6468808650970459
New best model for test task CHEMBL1738019 at epoch 2 with val loss 0.6455778479576111
New best model for test task CHEMBL1738019 at epoch 3 with val loss 0.6440907716751099
New best model for test task CHEMBL1738019 at epoch 4 with val loss 0.6427633762359619
New best model for test task CHEMBL1738019 at epoch 5 with val loss 0.6412991285324097
New best model for test task CHEMBL1738019 at epoch 6 with val loss 0.6402125954627991
New best model for test task CHEMBL1738019 at epoch 7 with val loss 0.6387282609939575
New best model for test task CHEMBL1738019 at epoch 8 with val loss 0.6374546885490417
New best model for test task CHEMBL1738019 at epoch 9 with val loss 0.636243462562561
New best model for test task CHEMBL1738019 at epoch 10 with val loss 0.6351286172866821
New best model for test task CHEMBL1738019 at epoch 11 with val loss 0.6339350342750549
New best model for test task CHEMBL1738019 at epoch 12 with val loss 0.6327934861183167
New best model for test task CHEMBL1738019 at epoch 13 with val loss 0.6316351890563965
New best model for test task CHEMBL1738019 at epoch 14 with val loss 0.6303524971008301
New best model for test task CHEMBL1738019 at epoch 15 with val loss 0.6292188167572021
New best model for test task CHEMBL1738019 at epoch 16 with val loss 0.6280692219734192
New best model for test task CHEMBL1738019 at epoch 17 with val loss 0.6269030570983887
New best model for test task CHEMBL1738019 at epoch 18 with val loss 0.6258284449577332
New best model for test task CHEMBL1738019 at epoch 19 with val loss 0.6248032450675964
New best model for test task CHEMBL1738019 at epoch 20 with val loss 0.6238791942596436
New best model for test task CHEMBL1738019 at epoch 21 with val loss 0.6227320432662964
New best model for test task CHEMBL1738019 at epoch 22 with val loss 0.6216918230056763
New best model for test task CHEMBL1738019 at epoch 23 with val loss 0.6207436323165894
New best model for test task CHEMBL1738019 at epoch 24 with val loss 0.6196595430374146
New best model for test task CHEMBL1738019 at epoch 25 with val loss 0.6185792684555054
New best model for test task CHEMBL1738019 at epoch 26 with val loss 0.6175922155380249
New best model for test task CHEMBL1738019 at epoch 27 with val loss 0.6165362596511841
New best model for test task CHEMBL1738019 at epoch 28 with val loss 0.6155016422271729
New best model for test task CHEMBL1738019 at epoch 29 with val loss 0.6145675182342529
New best model for test task CHEMBL1738019 at epoch 30 with val loss 0.6135534644126892
Finished early stopping for task CHEMBL1738019, beginning testing
Meta testing on task: CHEMBL1909092
New best model for test task CHEMBL1909092 at epoch 1 with val loss 0.7411859631538391
Val loss: 0.7449094454447428
Val loss: 0.7466452121734619
Val loss: 0.7439427574475607
Val loss: 0.7449618776639303
New best model for test task CHEMBL1909092 at epoch 6 with val loss 0.7406841715176901
Val loss: 0.7431200941403707
Val loss: 0.7443313399950663
Val loss: 0.7453415195147196
Val loss: 0.7407216429710388
Val loss: 0.7411872545878092
New best model for test task CHEMBL1909092 at epoch 12 with val loss 0.7400028705596924
Val loss: 0.7407219409942627
Val loss: 0.7416525681813558
New best model for test task CHEMBL1909092 at epoch 15 with val loss 0.7361301382382711
Val loss: 0.7412230372428894
Val loss: 0.7425553798675537
Val loss: 0.7418975830078125
New best model for test task CHEMBL1909092 at epoch 19 with val loss 0.7352661689122518
Val loss: 0.7428197662035624
Val loss: 0.7364008625348409
Val loss: 0.7394694089889526
New best model for test task CHEMBL1909092 at epoch 23 with val loss 0.7349021633466085
Val loss: 0.739875058333079
Val loss: 0.736548920472463
New best model for test task CHEMBL1909092 at epoch 26 with val loss 0.7346075971921285
Val loss: 0.7349556883176168
Val loss: 0.7356279889742533
New best model for test task CHEMBL1909092 at epoch 29 with val loss 0.734231173992157
New best model for test task CHEMBL1909092 at epoch 30 with val loss 0.7329139709472656
Finished early stopping for task CHEMBL1909092, beginning testing
Meta testing on task: CHEMBL1909192
New best model for test task CHEMBL1909192 at epoch 1 with val loss 0.748557964960734
New best model for test task CHEMBL1909192 at epoch 2 with val loss 0.7431581616401672
Val loss: 0.7472228209177653
Val loss: 0.7451432347297668
Val loss: 0.7441264788309733
Val loss: 0.7463817993799845
Val loss: 0.7490272720654806
New best model for test task CHEMBL1909192 at epoch 8 with val loss 0.741451621055603
New best model for test task CHEMBL1909192 at epoch 9 with val loss 0.7394710779190063
Val loss: 0.7447310288747152
Val loss: 0.7406586011250814
Val loss: 0.7400306661923727
Val loss: 0.7412221233050028
New best model for test task CHEMBL1909192 at epoch 14 with val loss 0.7366513808568319
Val loss: 0.7385842204093933
Val loss: 0.7372149626413981
Val loss: 0.7374903559684753
New best model for test task CHEMBL1909192 at epoch 18 with val loss 0.7357228994369507
Val loss: 0.7387224038441976
New best model for test task CHEMBL1909192 at epoch 20 with val loss 0.7352374196052551
Val loss: 0.7362532019615173
Val loss: 0.7387627164522806
Val loss: 0.7366127769152323
New best model for test task CHEMBL1909192 at epoch 24 with val loss 0.7347267667452494
New best model for test task CHEMBL1909192 at epoch 25 with val loss 0.7330499887466431
New best model for test task CHEMBL1909192 at epoch 26 with val loss 0.7319587469100952
Val loss: 0.7358193596204122
Val loss: 0.7330715656280518
New best model for test task CHEMBL1909192 at epoch 29 with val loss 0.7300386428833008
Val loss: 0.7315914233525594
Finished early stopping for task CHEMBL1909192, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1614359
New best model for test task CHEMBL1614359 at epoch 1 with val loss 0.7030499577522278
New best model for test task CHEMBL1614359 at epoch 2 with val loss 0.6879819333553314
Val loss: 0.707525908946991
New best model for test task CHEMBL1614359 at epoch 4 with val loss 0.6651349067687988
Val loss: 0.6893972158432007
Val loss: 0.6811584830284119
Val loss: 0.7344529330730438
Val loss: 0.6829113960266113
Val loss: 0.721444845199585
Val loss: 0.6939626634120941
Val loss: 0.6862647235393524
Val loss: 0.699975311756134
Val loss: 0.6725369393825531
New best model for test task CHEMBL1614359 at epoch 14 with val loss 0.6534305810928345
Val loss: 0.6716746985912323
Val loss: 0.6703920960426331
Val loss: 0.6631278693675995
Val loss: 0.700591504573822
Val loss: 0.6839316189289093
Val loss: 0.6980878710746765
Val loss: 0.6742691993713379
New best model for test task CHEMBL1614359 at epoch 22 with val loss 0.647361159324646
Val loss: 0.6816091537475586
Val loss: 0.6966785192489624
Val loss: 0.6543926894664764
Val loss: 0.6740230321884155
Val loss: 0.6536626219749451
Val loss: 0.6711491048336029
Val loss: 0.6688449680805206
Val loss: 0.6741210520267487
Finished early stopping for task CHEMBL1614359, beginning testing
Meta testing on task: CHEMBL2028077
New best model for test task CHEMBL2028077 at epoch 1 with val loss 0.7482797503471375
New best model for test task CHEMBL2028077 at epoch 2 with val loss 0.7481454014778137
New best model for test task CHEMBL2028077 at epoch 3 with val loss 0.7479740381240845
New best model for test task CHEMBL2028077 at epoch 4 with val loss 0.7478502988815308
New best model for test task CHEMBL2028077 at epoch 5 with val loss 0.7476919889450073
New best model for test task CHEMBL2028077 at epoch 6 with val loss 0.7475187182426453
New best model for test task CHEMBL2028077 at epoch 7 with val loss 0.7473683953285217
New best model for test task CHEMBL2028077 at epoch 8 with val loss 0.7472313642501831
New best model for test task CHEMBL2028077 at epoch 9 with val loss 0.7470816373825073
New best model for test task CHEMBL2028077 at epoch 10 with val loss 0.7469695806503296
New best model for test task CHEMBL2028077 at epoch 11 with val loss 0.7468990087509155
New best model for test task CHEMBL2028077 at epoch 12 with val loss 0.7467911839485168
New best model for test task CHEMBL2028077 at epoch 13 with val loss 0.7466275691986084
New best model for test task CHEMBL2028077 at epoch 14 with val loss 0.7464925646781921
New best model for test task CHEMBL2028077 at epoch 15 with val loss 0.7463864684104919
New best model for test task CHEMBL2028077 at epoch 16 with val loss 0.7462112903594971
New best model for test task CHEMBL2028077 at epoch 17 with val loss 0.7461763620376587
New best model for test task CHEMBL2028077 at epoch 18 with val loss 0.7460235357284546
New best model for test task CHEMBL2028077 at epoch 19 with val loss 0.7459288239479065
New best model for test task CHEMBL2028077 at epoch 20 with val loss 0.7458239793777466
New best model for test task CHEMBL2028077 at epoch 21 with val loss 0.7457289099693298
New best model for test task CHEMBL2028077 at epoch 22 with val loss 0.745563268661499
New best model for test task CHEMBL2028077 at epoch 23 with val loss 0.7454437613487244
New best model for test task CHEMBL2028077 at epoch 24 with val loss 0.7453250288963318
New best model for test task CHEMBL2028077 at epoch 25 with val loss 0.7452104091644287
New best model for test task CHEMBL2028077 at epoch 26 with val loss 0.7451125979423523
New best model for test task CHEMBL2028077 at epoch 27 with val loss 0.7450143694877625
New best model for test task CHEMBL2028077 at epoch 28 with val loss 0.7448866963386536
New best model for test task CHEMBL2028077 at epoch 29 with val loss 0.7447487711906433
New best model for test task CHEMBL2028077 at epoch 30 with val loss 0.7446081042289734
Finished early stopping for task CHEMBL2028077, beginning testing
Meta testing on task: CHEMBL1794358
New best model for test task CHEMBL1794358 at epoch 1 with val loss 0.7214676737785339
New best model for test task CHEMBL1794358 at epoch 2 with val loss 0.7214212417602539
New best model for test task CHEMBL1794358 at epoch 3 with val loss 0.7213579416275024
New best model for test task CHEMBL1794358 at epoch 4 with val loss 0.7212634086608887
New best model for test task CHEMBL1794358 at epoch 5 with val loss 0.7211847305297852
New best model for test task CHEMBL1794358 at epoch 6 with val loss 0.7211264967918396
New best model for test task CHEMBL1794358 at epoch 7 with val loss 0.7210653424263
New best model for test task CHEMBL1794358 at epoch 8 with val loss 0.7209832072257996
New best model for test task CHEMBL1794358 at epoch 9 with val loss 0.7209083437919617
New best model for test task CHEMBL1794358 at epoch 10 with val loss 0.7208616733551025
New best model for test task CHEMBL1794358 at epoch 11 with val loss 0.7207784056663513
New best model for test task CHEMBL1794358 at epoch 12 with val loss 0.720710039138794
New best model for test task CHEMBL1794358 at epoch 13 with val loss 0.720668613910675
New best model for test task CHEMBL1794358 at epoch 14 with val loss 0.7206270098686218
New best model for test task CHEMBL1794358 at epoch 15 with val loss 0.7205857634544373
New best model for test task CHEMBL1794358 at epoch 16 with val loss 0.7205055356025696
New best model for test task CHEMBL1794358 at epoch 17 with val loss 0.7204386591911316
New best model for test task CHEMBL1794358 at epoch 18 with val loss 0.72038334608078
New best model for test task CHEMBL1794358 at epoch 19 with val loss 0.7203435301780701
New best model for test task CHEMBL1794358 at epoch 20 with val loss 0.7202870845794678
New best model for test task CHEMBL1794358 at epoch 21 with val loss 0.7201916575431824
New best model for test task CHEMBL1794358 at epoch 22 with val loss 0.7201125025749207
New best model for test task CHEMBL1794358 at epoch 23 with val loss 0.7200796008110046
New best model for test task CHEMBL1794358 at epoch 24 with val loss 0.7200291752815247
New best model for test task CHEMBL1794358 at epoch 25 with val loss 0.7199748158454895
New best model for test task CHEMBL1794358 at epoch 26 with val loss 0.7199198603630066
New best model for test task CHEMBL1794358 at epoch 27 with val loss 0.7198396325111389
New best model for test task CHEMBL1794358 at epoch 28 with val loss 0.7197627425193787
New best model for test task CHEMBL1794358 at epoch 29 with val loss 0.7196931838989258
New best model for test task CHEMBL1794358 at epoch 30 with val loss 0.719637393951416
Finished early stopping for task CHEMBL1794358, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1738021
New best model for test task CHEMBL1738021 at epoch 1 with val loss 0.6473912000656128
New best model for test task CHEMBL1738021 at epoch 2 with val loss 0.6467059254646301
New best model for test task CHEMBL1738021 at epoch 3 with val loss 0.6459752917289734
New best model for test task CHEMBL1738021 at epoch 4 with val loss 0.6452476978302002
New best model for test task CHEMBL1738021 at epoch 5 with val loss 0.6444316506385803
New best model for test task CHEMBL1738021 at epoch 6 with val loss 0.6436890959739685
New best model for test task CHEMBL1738021 at epoch 7 with val loss 0.6429228186607361
New best model for test task CHEMBL1738021 at epoch 8 with val loss 0.6421335935592651
New best model for test task CHEMBL1738021 at epoch 9 with val loss 0.6413097381591797
New best model for test task CHEMBL1738021 at epoch 10 with val loss 0.6405336260795593
New best model for test task CHEMBL1738021 at epoch 11 with val loss 0.6396715044975281
New best model for test task CHEMBL1738021 at epoch 12 with val loss 0.6388010382652283
New best model for test task CHEMBL1738021 at epoch 13 with val loss 0.6380859613418579
New best model for test task CHEMBL1738021 at epoch 14 with val loss 0.6372110247612
New best model for test task CHEMBL1738021 at epoch 15 with val loss 0.636391282081604
New best model for test task CHEMBL1738021 at epoch 16 with val loss 0.6354900002479553
New best model for test task CHEMBL1738021 at epoch 17 with val loss 0.6345706582069397
New best model for test task CHEMBL1738021 at epoch 18 with val loss 0.6336979269981384
New best model for test task CHEMBL1738021 at epoch 19 with val loss 0.6328085660934448
New best model for test task CHEMBL1738021 at epoch 20 with val loss 0.6318196654319763
New best model for test task CHEMBL1738021 at epoch 21 with val loss 0.6308852434158325
New best model for test task CHEMBL1738021 at epoch 22 with val loss 0.6300291419029236
New best model for test task CHEMBL1738021 at epoch 23 with val loss 0.6291007995605469
New best model for test task CHEMBL1738021 at epoch 24 with val loss 0.6282105445861816
New best model for test task CHEMBL1738021 at epoch 25 with val loss 0.6273273825645447
New best model for test task CHEMBL1738021 at epoch 26 with val loss 0.6262809038162231
New best model for test task CHEMBL1738021 at epoch 27 with val loss 0.6253489255905151
New best model for test task CHEMBL1738021 at epoch 28 with val loss 0.6243629455566406
New best model for test task CHEMBL1738021 at epoch 29 with val loss 0.6234831213951111
New best model for test task CHEMBL1738021 at epoch 30 with val loss 0.6225746870040894
Finished early stopping for task CHEMBL1738021, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL3215116
New best model for test task CHEMBL3215116 at epoch 1 with val loss 0.7042127847671509
Val loss: 0.7044322490692139
Val loss: 0.7045480608940125
Val loss: 0.704781174659729
Val loss: 0.704957902431488
Val loss: 0.7051072120666504
Val loss: 0.7052440643310547
Val loss: 0.7055053114891052
Val loss: 0.7056624889373779
Val loss: 0.7058035731315613
Val loss: 0.7059677839279175
Val loss: 0.7062051296234131
Val loss: 0.7063732147216797
Val loss: 0.7065386772155762
Val loss: 0.7066785097122192
Val loss: 0.7068861722946167
Val loss: 0.7070919871330261
Val loss: 0.7072039842605591
Val loss: 0.7075600624084473
Val loss: 0.7077481746673584
Val loss: 0.7079150080680847
Val loss: 0.7080663442611694
Val loss: 0.7083038687705994
Val loss: 0.708478569984436
Val loss: 0.7086496353149414
Val loss: 0.7087919116020203
Val loss: 0.7090104818344116
Val loss: 0.7091428637504578
Val loss: 0.7092949151992798
Val loss: 0.7094694972038269
Finished early stopping for task CHEMBL3215116, beginning testing
Meta testing on task: CHEMBL1614202
New best model for test task CHEMBL1614202 at epoch 1 with val loss 0.6231975555419922
New best model for test task CHEMBL1614202 at epoch 2 with val loss 0.6173372268676758
New best model for test task CHEMBL1614202 at epoch 3 with val loss 0.6113932132720947
New best model for test task CHEMBL1614202 at epoch 4 with val loss 0.6053690910339355
New best model for test task CHEMBL1614202 at epoch 5 with val loss 0.5993318557739258
New best model for test task CHEMBL1614202 at epoch 6 with val loss 0.5931840538978577
New best model for test task CHEMBL1614202 at epoch 7 with val loss 0.5872126221656799
New best model for test task CHEMBL1614202 at epoch 8 with val loss 0.581209123134613
New best model for test task CHEMBL1614202 at epoch 9 with val loss 0.5755013227462769
New best model for test task CHEMBL1614202 at epoch 10 with val loss 0.5698217153549194
New best model for test task CHEMBL1614202 at epoch 11 with val loss 0.5641469359397888
New best model for test task CHEMBL1614202 at epoch 12 with val loss 0.5586626529693604
New best model for test task CHEMBL1614202 at epoch 13 with val loss 0.5533503890037537
New best model for test task CHEMBL1614202 at epoch 14 with val loss 0.5478406548500061
New best model for test task CHEMBL1614202 at epoch 15 with val loss 0.5423336625099182
New best model for test task CHEMBL1614202 at epoch 16 with val loss 0.5374475121498108
New best model for test task CHEMBL1614202 at epoch 17 with val loss 0.532532811164856
New best model for test task CHEMBL1614202 at epoch 18 with val loss 0.527778685092926
New best model for test task CHEMBL1614202 at epoch 19 with val loss 0.5231152772903442
New best model for test task CHEMBL1614202 at epoch 20 with val loss 0.5185307264328003
New best model for test task CHEMBL1614202 at epoch 21 with val loss 0.5139620900154114
New best model for test task CHEMBL1614202 at epoch 22 with val loss 0.509513258934021
New best model for test task CHEMBL1614202 at epoch 23 with val loss 0.5053459405899048
New best model for test task CHEMBL1614202 at epoch 24 with val loss 0.5012240409851074
New best model for test task CHEMBL1614202 at epoch 25 with val loss 0.4973776936531067
New best model for test task CHEMBL1614202 at epoch 26 with val loss 0.4936622977256775
New best model for test task CHEMBL1614202 at epoch 27 with val loss 0.49023380875587463
New best model for test task CHEMBL1614202 at epoch 28 with val loss 0.48678842186927795
New best model for test task CHEMBL1614202 at epoch 29 with val loss 0.4833727180957794
New best model for test task CHEMBL1614202 at epoch 30 with val loss 0.480133593082428
Finished early stopping for task CHEMBL1614202, beginning testing
Meta testing on task: CHEMBL1794567
New best model for test task CHEMBL1794567 at epoch 1 with val loss 0.666935920715332
Val loss: 0.6683101952075958
New best model for test task CHEMBL1794567 at epoch 3 with val loss 0.6534522473812103
Val loss: 0.6558810770511627
New best model for test task CHEMBL1794567 at epoch 5 with val loss 0.6501368284225464
New best model for test task CHEMBL1794567 at epoch 6 with val loss 0.6453628838062286
New best model for test task CHEMBL1794567 at epoch 7 with val loss 0.6349026560783386
New best model for test task CHEMBL1794567 at epoch 8 with val loss 0.625944972038269
Val loss: 0.6276265978813171
Val loss: 0.6352291107177734
Val loss: 0.6380050480365753
New best model for test task CHEMBL1794567 at epoch 12 with val loss 0.604533314704895
Val loss: 0.6154865622520447
Val loss: 0.6212880313396454
New best model for test task CHEMBL1794567 at epoch 15 with val loss 0.5823273062705994
Val loss: 0.6085275113582611
Val loss: 0.6035192906856537
Val loss: 0.6247267723083496
Val loss: 0.5953482985496521
New best model for test task CHEMBL1794567 at epoch 20 with val loss 0.5763165056705475
New best model for test task CHEMBL1794567 at epoch 21 with val loss 0.5710393190383911
New best model for test task CHEMBL1794567 at epoch 22 with val loss 0.553640216588974
Val loss: 0.5908019244670868
Val loss: 0.5641741454601288
Val loss: 0.5649565756320953
Val loss: 0.5699290931224823
New best model for test task CHEMBL1794567 at epoch 27 with val loss 0.530052900314331
Val loss: 0.5532026290893555
Val loss: 0.5345478653907776
Val loss: 0.5369719862937927
Finished early stopping for task CHEMBL1794567, beginning testing
Meta testing on task: CHEMBL3215176
New best model for test task CHEMBL3215176 at epoch 1 with val loss 0.7280097007751465
Val loss: 0.7280274033546448
Val loss: 0.7280592322349548
Val loss: 0.7280900478363037
Val loss: 0.7281004786491394
Val loss: 0.7281219959259033
Val loss: 0.7281418442726135
Val loss: 0.7281615138053894
Val loss: 0.7282031774520874
Val loss: 0.7282398343086243
Val loss: 0.7282506823539734
Val loss: 0.7282788753509521
Val loss: 0.7283124923706055
Val loss: 0.7283322811126709
Val loss: 0.7283608913421631
Val loss: 0.7284190654754639
Val loss: 0.7284539937973022
Val loss: 0.7284842133522034
Val loss: 0.7284838557243347
Val loss: 0.7285160422325134
Val loss: 0.728547990322113
Val loss: 0.7285744547843933
Val loss: 0.728613555431366
Val loss: 0.7286539077758789
Val loss: 0.728663980960846
Val loss: 0.7287059426307678
Val loss: 0.7286868691444397
Val loss: 0.728681206703186
Val loss: 0.7286984920501709
Val loss: 0.7287498712539673
Finished early stopping for task CHEMBL3215176, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1963741
New best model for test task CHEMBL1963741 at epoch 1 with val loss 0.6789677937825521
New best model for test task CHEMBL1963741 at epoch 2 with val loss 0.677860697110494
Val loss: 0.6822041273117065
Val loss: 0.6826996008555094
Val loss: 0.6802213390668234
Val loss: 0.6811127066612244
Val loss: 0.6798432668050131
Val loss: 0.6812788645426432
Val loss: 0.6841299732526144
Val loss: 0.6831385493278503
Val loss: 0.6870793501536051
Val loss: 0.685904065767924
Val loss: 0.6853204766909281
Val loss: 0.6861426631609598
Val loss: 0.687035342057546
Val loss: 0.684620221455892
Val loss: 0.6863828102747599
Val loss: 0.6872018973032633
Val loss: 0.687649647394816
Val loss: 0.6905622283617655
Val loss: 0.6917886932690939
Val loss: 0.6917273998260498
Val loss: 0.6907689174016317
Val loss: 0.6905155777931213
Val loss: 0.6876498659451803
Val loss: 0.6896636088689169
Val loss: 0.695243239402771
Val loss: 0.6891690889994303
Val loss: 0.691770076751709
Val loss: 0.6933178504308065
Finished early stopping for task CHEMBL1963741, beginning testing
Took 623.1682937145233 seconds to complete meta testing
Model test prc-auc = 0.528195
1-fold cross validation
Seed 0 ==> test prc-auc = 0.528195
Overall test prc-auc = 0.528195 +/- 0.000000
Total running time was 127329.94920325279 seconds
