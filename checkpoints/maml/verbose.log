Fold 0
Command line
python chemprop/meta_train.py --meta_learning --data_path /home/apappu/thesis/molecule-metalearning/filtered_chembl/chembl_less_1024_more_128_645_tasks.csv --dataset_type classification --split_type scaffold_balanced --chembl_assay_metadata_pickle_path filtered_chembl/ --save_dir checkpoints/maml/ --results_save_dir results/maml/ --experiment_name maml
Args
{'ANIL': False,
 'FO_MAML': False,
 'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 32,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'chembl_assay_metadata_pickle_path': 'filtered_chembl/',
 'class_balance': False,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'data_path': '/home/apappu/thesis/molecule-metalearning/filtered_chembl/chembl_less_1024_more_128_645_tasks.csv',
 'dataset_type': 'classification',
 'depth': 3,
 'dropout': 0.2,
 'dummy': False,
 'ensemble_size': 1,
 'epochs': 1000,
 'experiment_name': 'maml',
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_size': None,
 'ffn_hidden_size': 400,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 300,
 'init_lr': 0.0001,
 'inner_loop_lr': 0.05,
 'kaiming': True,
 'log_frequency': 10,
 'loss_queue_window': 500,
 'max_data_size': None,
 'max_lr': 0.001,
 'meta_batch_size': 32,
 'meta_learning': True,
 'meta_test_epochs': 30,
 'meta_test_lr': 0.0001,
 'meta_test_split_sizes': (0.8, 0.1, 0.1),
 'meta_train_split_sizes': (0.8, 0.2, 0),
 'metric': 'prc-auc',
 'minimize_score': False,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': False,
 'num_folds': 1,
 'num_inner_gradient_steps': 2,
 'num_lrs': 1,
 'num_tasks': None,
 'num_workers': 0,
 'outer_loop_lr': 0.0015,
 'pytorch_seed': 0,
 'quiet': False,
 'results_save_dir': 'results/maml/',
 'save_dir': 'checkpoints/maml/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'scaffold_balanced',
 'target_columns': None,
 'task_names': None,
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 645
Class sizes
CHEMBL1033994 0: 41.98%, 1: 58.02%
CHEMBL1119333 0: 9.59%, 1: 90.41%
CHEMBL1217000 0: 52.41%, 1: 47.59%
CHEMBL1243965 0: 17.11%, 1: 82.89%
CHEMBL1243966 0: 41.96%, 1: 58.04%
CHEMBL1243967 0: 10.17%, 1: 89.83%
CHEMBL1243968 0: 15.06%, 1: 84.94%
CHEMBL1243970 0: 6.91%, 1: 93.09%
CHEMBL1243972 0: 9.92%, 1: 90.08%
CHEMBL1243976 0: 28.99%, 1: 71.01%
CHEMBL1246087 0: 33.88%, 1: 66.12%
CHEMBL1246088 0: 48.37%, 1: 51.63%
CHEMBL1613762 0: 2.45%, 1: 97.55%
CHEMBL1613779 0: 58.47%, 1: 41.53%
CHEMBL1613785 0: 43.22%, 1: 56.78%
CHEMBL1613787 0: 69.08%, 1: 30.92%
CHEMBL1613807 0: 95.99%, 1: 4.01%
CHEMBL1613813 0: 44.20%, 1: 55.80%
CHEMBL1613814 0: 53.85%, 1: 46.15%
CHEMBL1613817 0: 84.86%, 1: 15.14%
CHEMBL1613853 0: 41.86%, 1: 58.14%
CHEMBL1613861 0: 14.47%, 1: 85.53%
CHEMBL1613864 0: 5.19%, 1: 94.81%
CHEMBL1613867 0: 97.13%, 1: 2.87%
CHEMBL1613870 0: 15.80%, 1: 84.20%
CHEMBL1613871 0: 6.15%, 1: 93.85%
CHEMBL1613874 0: 65.78%, 1: 34.22%
CHEMBL1613876 0: 54.35%, 1: 45.65%
CHEMBL1613884 0: 21.51%, 1: 78.49%
CHEMBL1613890 0: 34.85%, 1: 65.15%
CHEMBL1613897 0: 46.64%, 1: 53.36%
CHEMBL1613898 0: 75.45%, 1: 24.55%
CHEMBL1613904 0: 50.77%, 1: 49.23%
CHEMBL1613907 0: 5.34%, 1: 94.66%
CHEMBL1613926 0: 30.15%, 1: 69.85%
CHEMBL1613928 0: 45.90%, 1: 54.10%
CHEMBL1613929 0: 81.25%, 1: 18.75%
CHEMBL1613941 0: 32.58%, 1: 67.42%
CHEMBL1613942 0: 41.43%, 1: 58.57%
CHEMBL1613947 0: 46.01%, 1: 53.99%
CHEMBL1613949 0: 60.13%, 1: 39.87%
CHEMBL1613950 0: 43.05%, 1: 56.95%
CHEMBL1613955 0: 54.04%, 1: 45.96%
CHEMBL1613962 0: 24.26%, 1: 75.74%
CHEMBL1613967 0: 44.59%, 1: 55.41%
CHEMBL1613981 0: 58.85%, 1: 41.15%
CHEMBL1613991 0: 14.98%, 1: 85.02%
CHEMBL1613997 0: 7.73%, 1: 92.27%
CHEMBL1614001 0: 98.28%, 1: 1.72%
CHEMBL1614004 0: 14.66%, 1: 85.34%
CHEMBL1614016 0: 12.22%, 1: 87.78%
CHEMBL1614030 0: 19.96%, 1: 80.04%
CHEMBL1614034 0: 2.63%, 1: 97.37%
CHEMBL1614035 0: 77.14%, 1: 22.86%
CHEMBL1614049 0: 39.63%, 1: 60.37%
CHEMBL1614053 0: 42.48%, 1: 57.52%
CHEMBL1614063 0: 95.18%, 1: 4.82%
CHEMBL1614065 0: 9.71%, 1: 90.29%
CHEMBL1614066 0: 94.74%, 1: 5.26%
CHEMBL1614069 0: 56.82%, 1: 43.18%
CHEMBL1614072 0: 50.36%, 1: 49.64%
CHEMBL1614084 0: 13.26%, 1: 86.74%
CHEMBL1614091 0: 4.43%, 1: 95.57%
CHEMBL1614092 0: 47.50%, 1: 52.50%
CHEMBL1614097 0: 46.99%, 1: 53.01%
CHEMBL1614098 0: 97.25%, 1: 2.75%
CHEMBL1614104 0: 7.74%, 1: 92.26%
CHEMBL1614105 0: 6.12%, 1: 93.88%
CHEMBL1614109 0: 59.87%, 1: 40.13%
CHEMBL1614128 0: 10.35%, 1: 89.65%
CHEMBL1614131 0: 97.14%, 1: 2.86%
CHEMBL1614132 0: 42.07%, 1: 57.93%
CHEMBL1614138 0: 67.83%, 1: 32.17%
CHEMBL1614155 0: 25.40%, 1: 74.60%
CHEMBL1614158 0: 74.52%, 1: 25.48%
CHEMBL1614167 0: 16.17%, 1: 83.83%
CHEMBL1614170 0: 68.86%, 1: 31.14%
CHEMBL1614171 0: 93.77%, 1: 6.23%
CHEMBL1614175 0: 80.66%, 1: 19.34%
CHEMBL1614185 0: 26.17%, 1: 73.83%
CHEMBL1614197 0: 5.26%, 1: 94.74%
CHEMBL1614199 0: 49.66%, 1: 50.34%
CHEMBL1614202 0: 7.32%, 1: 92.68%
CHEMBL1614215 0: 52.76%, 1: 47.24%
CHEMBL1614216 0: 48.52%, 1: 51.48%
CHEMBL1614218 0: 22.36%, 1: 77.64%
CHEMBL1614225 0: 62.50%, 1: 37.50%
CHEMBL1614244 0: 38.85%, 1: 61.15%
CHEMBL1614247 0: 44.34%, 1: 55.66%
CHEMBL1614252 0: 60.11%, 1: 39.89%
CHEMBL1614255 0: 61.26%, 1: 38.74%
CHEMBL1614259 0: 32.82%, 1: 67.18%
CHEMBL1614272 0: 67.71%, 1: 32.29%
CHEMBL1614276 0: 51.74%, 1: 48.26%
CHEMBL1614287 0: 85.53%, 1: 14.47%
CHEMBL1614288 0: 72.51%, 1: 27.49%
CHEMBL1614290 0: 4.51%, 1: 95.49%
CHEMBL1614295 0: 2.29%, 1: 97.71%
CHEMBL1614301 0: 19.74%, 1: 80.26%
CHEMBL1614304 0: 63.76%, 1: 36.24%
CHEMBL1614309 0: 27.66%, 1: 72.34%
CHEMBL1614311 0: 53.76%, 1: 46.24%
CHEMBL1614314 0: 94.70%, 1: 5.30%
CHEMBL1614319 0: 72.16%, 1: 27.84%
CHEMBL1614320 0: 29.79%, 1: 70.21%
CHEMBL1614321 0: 60.16%, 1: 39.84%
CHEMBL1614328 0: 46.64%, 1: 53.36%
CHEMBL1614329 0: 40.69%, 1: 59.31%
CHEMBL1614336 0: 47.01%, 1: 52.99%
CHEMBL1614344 0: 67.72%, 1: 32.28%
CHEMBL1614356 0: 43.90%, 1: 56.10%
CHEMBL1614359 0: 50.26%, 1: 49.74%
CHEMBL1614363 0: 53.19%, 1: 46.81%
CHEMBL1614385 0: 93.27%, 1: 6.73%
CHEMBL1614388 0: 56.19%, 1: 43.81%
CHEMBL1614393 0: 45.00%, 1: 55.00%
CHEMBL1614395 0: 11.32%, 1: 88.68%
CHEMBL1614403 0: 88.15%, 1: 11.85%
CHEMBL1614423 0: 57.84%, 1: 42.16%
CHEMBL1614425 0: 38.24%, 1: 61.76%
CHEMBL1614433 0: 60.77%, 1: 39.23%
CHEMBL1614434 0: 80.25%, 1: 19.75%
CHEMBL1614456 0: 3.36%, 1: 96.64%
CHEMBL1614466 0: 10.32%, 1: 89.68%
CHEMBL1614469 0: 24.90%, 1: 75.10%
CHEMBL1614477 0: 43.75%, 1: 56.25%
CHEMBL1614478 0: 12.50%, 1: 87.50%
CHEMBL1614480 0: 85.71%, 1: 14.29%
CHEMBL1614484 0: 9.01%, 1: 90.99%
CHEMBL1614492 0: 40.54%, 1: 59.46%
CHEMBL1614499 0: 44.47%, 1: 55.53%
CHEMBL1614503 0: 40.30%, 1: 59.70%
CHEMBL1614504 0: 17.80%, 1: 82.20%
CHEMBL1614509 0: 79.43%, 1: 20.57%
CHEMBL1614512 0: 62.79%, 1: 37.21%
CHEMBL1614514 0: 83.07%, 1: 16.93%
CHEMBL1614515 0: 19.61%, 1: 80.39%
CHEMBL1614516 0: 96.81%, 1: 3.19%
CHEMBL1614522 0: 28.55%, 1: 71.45%
CHEMBL1614524 0: 12.98%, 1: 87.02%
CHEMBL1614528 0: 40.91%, 1: 59.09%
CHEMBL1614547 0: 72.51%, 1: 27.49%
CHEMBL1614548 0: 17.95%, 1: 82.05%
CHEMBL1614549 0: 84.25%, 1: 15.75%
CHEMBL1614550 0: 9.30%, 1: 90.70%
CHEMBL1614554 0: 71.64%, 1: 28.36%
CHEMBL1676103 0: 39.85%, 1: 60.15%
CHEMBL1737860 0: 3.60%, 1: 96.40%
CHEMBL1737863 0: 36.27%, 1: 63.73%
CHEMBL1737865 0: 61.01%, 1: 38.99%
CHEMBL1737868 0: 79.41%, 1: 20.59%
CHEMBL1737910 0: 45.71%, 1: 54.29%
CHEMBL1737912 0: 26.51%, 1: 73.49%
CHEMBL1737942 0: 44.52%, 1: 55.48%
CHEMBL1737951 0: 51.83%, 1: 48.17%
CHEMBL1737961 0: 93.34%, 1: 6.66%
CHEMBL1737966 0: 21.36%, 1: 78.64%
CHEMBL1737967 0: 89.17%, 1: 10.83%
CHEMBL1737977 0: 13.87%, 1: 86.13%
CHEMBL1737978 0: 18.18%, 1: 81.82%
CHEMBL1737979 0: 8.94%, 1: 91.06%
CHEMBL1738019 0: 24.85%, 1: 75.15%
CHEMBL1738021 0: 10.87%, 1: 89.13%
CHEMBL1738025 0: 33.80%, 1: 66.20%
CHEMBL1738040 0: 73.48%, 1: 26.52%
CHEMBL1738043 0: 46.43%, 1: 53.57%
CHEMBL1738079 0: 3.88%, 1: 96.12%
CHEMBL1738080 0: 76.43%, 1: 23.57%
CHEMBL1738091 0: 89.77%, 1: 10.23%
CHEMBL1738097 0: 72.41%, 1: 27.59%
CHEMBL1738131 0: 68.59%, 1: 31.41%
CHEMBL1738164 0: 79.79%, 1: 20.21%
CHEMBL1738171 0: 85.45%, 1: 14.55%
CHEMBL1738183 0: 59.62%, 1: 40.38%
CHEMBL1738197 0: 36.90%, 1: 63.10%
CHEMBL1738202 0: 3.47%, 1: 96.53%
CHEMBL1738242 0: 16.55%, 1: 83.45%
CHEMBL1738249 0: 86.53%, 1: 13.47%
CHEMBL1738253 0: 7.33%, 1: 92.67%
CHEMBL1738319 0: 59.17%, 1: 40.83%
CHEMBL1738325 0: 96.86%, 1: 3.14%
CHEMBL1738362 0: 21.43%, 1: 78.57%
CHEMBL1738369 0: 56.34%, 1: 43.66%
CHEMBL1738371 0: 81.97%, 1: 18.03%
CHEMBL1738391 0: 68.38%, 1: 31.62%
CHEMBL1738400 0: 77.86%, 1: 22.14%
CHEMBL1738402 0: 54.19%, 1: 45.81%
CHEMBL1738407 0: 85.58%, 1: 14.42%
CHEMBL1738408 0: 2.18%, 1: 97.82%
CHEMBL1738414 0: 5.11%, 1: 94.89%
CHEMBL1738418 0: 68.85%, 1: 31.15%
CHEMBL1738422 0: 4.29%, 1: 95.71%
CHEMBL1738424 0: 77.50%, 1: 22.50%
CHEMBL1738430 0: 25.54%, 1: 74.46%
CHEMBL1738438 0: 2.76%, 1: 97.24%
CHEMBL1738482 0: 7.69%, 1: 92.31%
CHEMBL1738485 0: 17.65%, 1: 82.35%
CHEMBL1738494 0: 97.26%, 1: 2.74%
CHEMBL1738495 0: 1.70%, 1: 98.30%
CHEMBL1738497 0: 64.50%, 1: 35.50%
CHEMBL1738502 0: 15.20%, 1: 84.80%
CHEMBL1738510 0: 54.41%, 1: 45.59%
CHEMBL1738512 0: 3.05%, 1: 96.95%
CHEMBL1738513 0: 28.64%, 1: 71.36%
CHEMBL1738552 0: 64.70%, 1: 35.30%
CHEMBL1738575 0: 65.65%, 1: 34.35%
CHEMBL1738578 0: 23.30%, 1: 76.70%
CHEMBL1738579 0: 22.79%, 1: 77.21%
CHEMBL1738593 0: 17.65%, 1: 82.35%
CHEMBL1738599 0: 16.07%, 1: 83.93%
CHEMBL1738602 0: 42.70%, 1: 57.30%
CHEMBL1738610 0: 42.07%, 1: 57.93%
CHEMBL1738611 0: 2.23%, 1: 97.77%
CHEMBL1738632 0: 86.32%, 1: 13.68%
CHEMBL1738633 0: 4.05%, 1: 95.95%
CHEMBL1738639 0: 64.47%, 1: 35.53%
CHEMBL1738642 0: 34.82%, 1: 65.18%
CHEMBL1738670 0: 9.18%, 1: 90.82%
CHEMBL1738673 0: 12.98%, 1: 87.02%
CHEMBL1738679 0: 63.95%, 1: 36.05%
CHEMBL1738682 0: 62.45%, 1: 37.55%
CHEMBL1794296 0: 44.02%, 1: 55.98%
CHEMBL1794303 0: 2.71%, 1: 97.29%
CHEMBL1794320 0: 96.68%, 1: 3.32%
CHEMBL1794327 0: 91.59%, 1: 8.41%
CHEMBL1794336 0: 53.07%, 1: 46.93%
CHEMBL1794350 0: 3.44%, 1: 96.56%
CHEMBL1794355 0: 5.26%, 1: 94.74%
CHEMBL1794356 0: 13.92%, 1: 86.08%
CHEMBL1794358 0: 94.14%, 1: 5.86%
CHEMBL1794365 0: 7.12%, 1: 92.88%
CHEMBL1794383 0: 6.90%, 1: 93.10%
CHEMBL1794387 0: 65.04%, 1: 34.96%
CHEMBL1794393 0: 67.55%, 1: 32.45%
CHEMBL1794396 0: 3.96%, 1: 96.04%
CHEMBL1794410 0: 23.98%, 1: 76.02%
CHEMBL1794413 0: 6.02%, 1: 93.98%
CHEMBL1794438 0: 40.17%, 1: 59.83%
CHEMBL1794445 0: 87.84%, 1: 12.16%
CHEMBL1794452 0: 94.30%, 1: 5.70%
CHEMBL1794457 0: 92.98%, 1: 7.02%
CHEMBL1794460 0: 28.63%, 1: 71.37%
CHEMBL1794467 0: 18.39%, 1: 81.61%
CHEMBL1794475 0: 56.02%, 1: 43.98%
CHEMBL1794484 0: 44.93%, 1: 55.07%
CHEMBL1794494 0: 5.33%, 1: 94.67%
CHEMBL1794497 0: 8.47%, 1: 91.53%
CHEMBL1794499 0: 73.90%, 1: 26.10%
CHEMBL1794508 0: 94.63%, 1: 5.37%
CHEMBL1794516 0: 3.21%, 1: 96.79%
CHEMBL1794522 0: 58.65%, 1: 41.35%
CHEMBL1794528 0: 79.86%, 1: 20.14%
CHEMBL1794531 0: 92.03%, 1: 7.97%
CHEMBL1794548 0: 88.74%, 1: 11.26%
CHEMBL1794566 0: 6.05%, 1: 93.95%
CHEMBL1794567 0: 9.61%, 1: 90.39%
CHEMBL1794570 0: 92.79%, 1: 7.21%
CHEMBL1794571 0: 53.55%, 1: 46.45%
CHEMBL1794573 0: 10.54%, 1: 89.46%
CHEMBL1794574 0: 1.09%, 1: 98.91%
CHEMBL1794578 0: 46.99%, 1: 53.01%
CHEMBL1794581 0: 19.65%, 1: 80.35%
CHEMBL1863510 0: 2.14%, 1: 97.86%
CHEMBL1863512 0: 2.79%, 1: 97.21%
CHEMBL1909084 0: 99.28%, 1: 0.72%
CHEMBL1909085 0: 92.10%, 1: 7.90%
CHEMBL1909086 0: 91.52%, 1: 8.48%
CHEMBL1909087 0: 91.86%, 1: 8.14%
CHEMBL1909088 0: 87.97%, 1: 12.03%
CHEMBL1909089 0: 88.69%, 1: 11.31%
CHEMBL1909090 0: 90.81%, 1: 9.19%
CHEMBL1909091 0: 97.24%, 1: 2.76%
CHEMBL1909092 0: 97.14%, 1: 2.86%
CHEMBL1909093 0: 98.42%, 1: 1.58%
CHEMBL1909094 0: 90.85%, 1: 9.15%
CHEMBL1909095 0: 98.56%, 1: 1.44%
CHEMBL1909097 0: 99.28%, 1: 0.72%
CHEMBL1909102 0: 92.94%, 1: 7.06%
CHEMBL1909103 0: 97.23%, 1: 2.77%
CHEMBL1909104 0: 87.06%, 1: 12.94%
CHEMBL1909105 0: 88.70%, 1: 11.30%
CHEMBL1909106 0: 98.57%, 1: 1.43%
CHEMBL1909107 0: 97.86%, 1: 2.14%
CHEMBL1909108 0: 93.04%, 1: 6.96%
CHEMBL1909109 0: 90.35%, 1: 9.65%
CHEMBL1909110 0: 92.47%, 1: 7.53%
CHEMBL1909111 0: 94.03%, 1: 5.97%
CHEMBL1909112 0: 90.91%, 1: 9.09%
CHEMBL1909114 0: 96.50%, 1: 3.50%
CHEMBL1909115 0: 96.00%, 1: 4.00%
CHEMBL1909116 0: 96.98%, 1: 3.02%
CHEMBL1909121 0: 92.80%, 1: 7.20%
CHEMBL1909123 0: 99.16%, 1: 0.84%
CHEMBL1909124 0: 99.40%, 1: 0.60%
CHEMBL1909130 0: 95.56%, 1: 4.44%
CHEMBL1909131 0: 97.01%, 1: 2.99%
CHEMBL1909132 0: 96.15%, 1: 3.85%
CHEMBL1909134 0: 95.19%, 1: 4.81%
CHEMBL1909135 0: 95.76%, 1: 4.24%
CHEMBL1909136 0: 93.20%, 1: 6.80%
CHEMBL1909138 0: 96.16%, 1: 3.84%
CHEMBL1909139 0: 93.84%, 1: 6.16%
CHEMBL1909140 0: 93.87%, 1: 6.13%
CHEMBL1909141 0: 89.73%, 1: 10.27%
CHEMBL1909142 0: 99.04%, 1: 0.96%
CHEMBL1909143 0: 93.01%, 1: 6.99%
CHEMBL1909145 0: 97.97%, 1: 2.03%
CHEMBL1909148 0: 98.57%, 1: 1.43%
CHEMBL1909150 0: 95.10%, 1: 4.90%
CHEMBL1909156 0: 94.28%, 1: 5.72%
CHEMBL1909157 0: 96.38%, 1: 3.62%
CHEMBL1909158 0: 99.05%, 1: 0.95%
CHEMBL1909159 0: 93.18%, 1: 6.82%
CHEMBL1909165 0: 97.79%, 1: 2.21%
CHEMBL1909169 0: 98.32%, 1: 1.68%
CHEMBL1909170 0: 92.35%, 1: 7.65%
CHEMBL1909171 0: 93.29%, 1: 6.71%
CHEMBL1909172 0: 92.82%, 1: 7.18%
CHEMBL1909173 0: 92.38%, 1: 7.62%
CHEMBL1909174 0: 92.71%, 1: 7.29%
CHEMBL1909180 0: 98.20%, 1: 1.80%
CHEMBL1909181 0: 97.71%, 1: 2.29%
CHEMBL1909182 0: 96.99%, 1: 3.01%
CHEMBL1909184 0: 98.93%, 1: 1.07%
CHEMBL1909186 0: 99.52%, 1: 0.48%
CHEMBL1909190 0: 98.66%, 1: 1.34%
CHEMBL1909191 0: 94.99%, 1: 5.01%
CHEMBL1909192 0: 99.52%, 1: 0.48%
CHEMBL1909200 0: 97.97%, 1: 2.03%
CHEMBL1909201 0: 97.72%, 1: 2.28%
CHEMBL1909203 0: 98.05%, 1: 1.95%
CHEMBL1909204 0: 97.17%, 1: 2.83%
CHEMBL1909205 0: 98.67%, 1: 1.33%
CHEMBL1909206 0: 99.16%, 1: 0.84%
CHEMBL1909209 0: 92.34%, 1: 7.66%
CHEMBL1909210 0: 96.05%, 1: 3.95%
CHEMBL1909211 0: 88.80%, 1: 11.20%
CHEMBL1909212 0: 98.30%, 1: 1.70%
CHEMBL1909213 0: 99.28%, 1: 0.72%
CHEMBL1909214 0: 98.79%, 1: 1.21%
CHEMBL1909215 0: 95.61%, 1: 4.39%
CHEMBL1963686 0: 61.95%, 1: 38.05%
CHEMBL1963687 0: 64.42%, 1: 35.58%
CHEMBL1963688 0: 62.52%, 1: 37.48%
CHEMBL1963689 0: 89.69%, 1: 10.31%
CHEMBL1963690 0: 57.36%, 1: 42.64%
CHEMBL1963691 0: 49.33%, 1: 50.67%
CHEMBL1963692 0: 53.73%, 1: 46.27%
CHEMBL1963693 0: 71.00%, 1: 29.00%
CHEMBL1963694 0: 91.05%, 1: 8.95%
CHEMBL1963695 0: 74.90%, 1: 25.10%
CHEMBL1963696 0: 77.85%, 1: 22.15%
CHEMBL1963697 0: 74.06%, 1: 25.94%
CHEMBL1963698 0: 74.60%, 1: 25.40%
CHEMBL1963699 0: 83.18%, 1: 16.82%
CHEMBL1963701 0: 67.32%, 1: 32.68%
CHEMBL1963702 0: 83.82%, 1: 16.18%
CHEMBL1963703 0: 68.65%, 1: 31.35%
CHEMBL1963704 0: 65.28%, 1: 34.72%
CHEMBL1963705 0: 55.92%, 1: 44.08%
CHEMBL1963706 0: 53.38%, 1: 46.62%
CHEMBL1963707 0: 53.25%, 1: 46.75%
CHEMBL1963708 0: 48.77%, 1: 51.23%
CHEMBL1963710 0: 63.21%, 1: 36.79%
CHEMBL1963711 0: 94.72%, 1: 5.28%
CHEMBL1963712 0: 75.76%, 1: 24.24%
CHEMBL1963714 0: 74.21%, 1: 25.79%
CHEMBL1963715 0: 50.28%, 1: 49.72%
CHEMBL1963716 0: 91.91%, 1: 8.09%
CHEMBL1963717 0: 55.50%, 1: 44.50%
CHEMBL1963718 0: 65.45%, 1: 34.55%
CHEMBL1963719 0: 63.39%, 1: 36.61%
CHEMBL1963720 0: 69.73%, 1: 30.27%
CHEMBL1963721 0: 56.94%, 1: 43.06%
CHEMBL1963722 0: 49.40%, 1: 50.60%
CHEMBL1963723 0: 53.82%, 1: 46.18%
CHEMBL1963724 0: 67.32%, 1: 32.68%
CHEMBL1963725 0: 69.97%, 1: 30.03%
CHEMBL1963727 0: 56.59%, 1: 43.41%
CHEMBL1963728 0: 92.02%, 1: 7.98%
CHEMBL1963729 0: 84.64%, 1: 15.36%
CHEMBL1963731 0: 61.35%, 1: 38.65%
CHEMBL1963733 0: 79.77%, 1: 20.23%
CHEMBL1963734 0: 79.51%, 1: 20.49%
CHEMBL1963735 0: 71.66%, 1: 28.34%
CHEMBL1963736 0: 78.32%, 1: 21.68%
CHEMBL1963737 0: 86.79%, 1: 13.21%
CHEMBL1963738 0: 61.18%, 1: 38.82%
CHEMBL1963739 0: 84.32%, 1: 15.68%
CHEMBL1963740 0: 64.30%, 1: 35.70%
CHEMBL1963741 0: 67.03%, 1: 32.97%
CHEMBL1963742 0: 73.47%, 1: 26.53%
CHEMBL1963743 0: 66.24%, 1: 33.76%
CHEMBL1963744 0: 71.97%, 1: 28.03%
CHEMBL1963745 0: 67.16%, 1: 32.84%
CHEMBL1963746 0: 66.34%, 1: 33.66%
CHEMBL1963747 0: 81.33%, 1: 18.67%
CHEMBL1963748 0: 58.75%, 1: 41.25%
CHEMBL1963749 0: 55.02%, 1: 44.98%
CHEMBL1963750 0: 58.61%, 1: 41.39%
CHEMBL1963751 0: 70.90%, 1: 29.10%
CHEMBL1963752 0: 64.78%, 1: 35.22%
CHEMBL1963753 0: 88.02%, 1: 11.98%
CHEMBL1963754 0: 59.12%, 1: 40.88%
CHEMBL1963756 0: 61.66%, 1: 38.34%
CHEMBL1963757 0: 73.28%, 1: 26.72%
CHEMBL1963758 0: 82.56%, 1: 17.44%
CHEMBL1963759 0: 81.38%, 1: 18.62%
CHEMBL1963760 0: 86.36%, 1: 13.64%
CHEMBL1963761 0: 83.63%, 1: 16.37%
CHEMBL1963763 0: 72.14%, 1: 27.86%
CHEMBL1963764 0: 62.50%, 1: 37.50%
CHEMBL1963765 0: 86.41%, 1: 13.59%
CHEMBL1963766 0: 81.38%, 1: 18.62%
CHEMBL1963767 0: 77.76%, 1: 22.24%
CHEMBL1963768 0: 77.63%, 1: 22.37%
CHEMBL1963770 0: 89.44%, 1: 10.56%
CHEMBL1963771 0: 53.53%, 1: 46.47%
CHEMBL1963772 0: 48.99%, 1: 51.01%
CHEMBL1963773 0: 59.00%, 1: 41.00%
CHEMBL1963775 0: 76.92%, 1: 23.08%
CHEMBL1963776 0: 87.71%, 1: 12.29%
CHEMBL1963777 0: 63.92%, 1: 36.08%
CHEMBL1963778 0: 51.70%, 1: 48.30%
CHEMBL1963779 0: 54.69%, 1: 45.31%
CHEMBL1963780 0: 89.06%, 1: 10.94%
CHEMBL1963781 0: 94.31%, 1: 5.69%
CHEMBL1963782 0: 75.86%, 1: 24.14%
CHEMBL1963783 0: 57.36%, 1: 42.64%
CHEMBL1963785 0: 70.10%, 1: 29.90%
CHEMBL1963786 0: 50.08%, 1: 49.92%
CHEMBL1963787 0: 66.39%, 1: 33.61%
CHEMBL1963789 0: 67.34%, 1: 32.66%
CHEMBL1963790 0: 57.93%, 1: 42.07%
CHEMBL1963791 0: 82.08%, 1: 17.92%
CHEMBL1963792 0: 85.73%, 1: 14.27%
CHEMBL1963793 0: 65.48%, 1: 34.52%
CHEMBL1963794 0: 77.53%, 1: 22.47%
CHEMBL1963795 0: 46.98%, 1: 53.02%
CHEMBL1963796 0: 69.76%, 1: 30.24%
CHEMBL1963797 0: 83.64%, 1: 16.36%
CHEMBL1963798 0: 89.45%, 1: 10.55%
CHEMBL1963799 0: 43.54%, 1: 56.46%
CHEMBL1963800 0: 64.88%, 1: 35.12%
CHEMBL1963801 0: 75.90%, 1: 24.10%
CHEMBL1963802 0: 61.41%, 1: 38.59%
CHEMBL1963803 0: 87.44%, 1: 12.56%
CHEMBL1963804 0: 65.90%, 1: 34.10%
CHEMBL1963805 0: 53.77%, 1: 46.23%
CHEMBL1963806 0: 41.19%, 1: 58.81%
CHEMBL1963807 0: 36.83%, 1: 63.17%
CHEMBL1963808 0: 68.82%, 1: 31.18%
CHEMBL1963809 0: 79.97%, 1: 20.03%
CHEMBL1963810 0: 43.69%, 1: 56.31%
CHEMBL1963811 0: 73.98%, 1: 26.02%
CHEMBL1963812 0: 53.08%, 1: 46.92%
CHEMBL1963813 0: 79.19%, 1: 20.81%
CHEMBL1963814 0: 51.10%, 1: 48.90%
CHEMBL1963815 0: 71.80%, 1: 28.20%
CHEMBL1963816 0: 89.64%, 1: 10.36%
CHEMBL1963817 0: 71.90%, 1: 28.10%
CHEMBL1963818 0: 55.13%, 1: 44.87%
CHEMBL1963819 0: 62.64%, 1: 37.36%
CHEMBL1963820 0: 84.46%, 1: 15.54%
CHEMBL1963821 0: 79.31%, 1: 20.69%
CHEMBL1963822 0: 90.91%, 1: 9.09%
CHEMBL1963823 0: 76.61%, 1: 23.39%
CHEMBL1963824 0: 39.57%, 1: 60.43%
CHEMBL1963825 0: 51.12%, 1: 48.88%
CHEMBL1963826 0: 59.25%, 1: 40.75%
CHEMBL1963827 0: 50.74%, 1: 49.26%
CHEMBL1963828 0: 80.86%, 1: 19.14%
CHEMBL1963829 0: 86.41%, 1: 13.59%
CHEMBL1963831 0: 52.56%, 1: 47.44%
CHEMBL1963832 0: 72.98%, 1: 27.02%
CHEMBL1963833 0: 87.13%, 1: 12.87%
CHEMBL1963834 0: 59.69%, 1: 40.31%
CHEMBL1963836 0: 85.18%, 1: 14.82%
CHEMBL1963837 0: 26.01%, 1: 73.99%
CHEMBL1963838 0: 75.83%, 1: 24.17%
CHEMBL1963846 0: 85.07%, 1: 14.93%
CHEMBL1963867 0: 73.13%, 1: 26.87%
CHEMBL1963893 0: 85.61%, 1: 14.39%
CHEMBL1963898 0: 75.62%, 1: 24.38%
CHEMBL1963907 0: 64.35%, 1: 35.65%
CHEMBL1963910 0: 10.50%, 1: 89.50%
CHEMBL1963915 0: 15.00%, 1: 85.00%
CHEMBL1963916 0: 40.48%, 1: 59.52%
CHEMBL1963918 0: 73.35%, 1: 26.65%
CHEMBL1963930 0: 3.64%, 1: 96.36%
CHEMBL1963933 0: 93.91%, 1: 6.09%
CHEMBL1963934 0: 3.64%, 1: 96.36%
CHEMBL1963937 0: 39.27%, 1: 60.73%
CHEMBL1963938 0: 65.85%, 1: 34.15%
CHEMBL1963940 0: 33.79%, 1: 66.21%
CHEMBL1963947 0: 6.25%, 1: 93.75%
CHEMBL1963966 0: 3.29%, 1: 96.71%
CHEMBL1963968 0: 51.52%, 1: 48.48%
CHEMBL1963969 0: 25.83%, 1: 74.17%
CHEMBL1963971 0: 10.20%, 1: 89.80%
CHEMBL1963974 0: 88.26%, 1: 11.74%
CHEMBL1963983 0: 76.39%, 1: 23.61%
CHEMBL1964000 0: 76.77%, 1: 23.23%
CHEMBL1964005 0: 6.67%, 1: 93.33%
CHEMBL1964010 0: 76.06%, 1: 23.94%
CHEMBL1964015 0: 73.77%, 1: 26.23%
CHEMBL1964022 0: 76.61%, 1: 23.39%
CHEMBL1964023 0: 73.75%, 1: 26.25%
CHEMBL1964081 0: 47.98%, 1: 52.02%
CHEMBL1964095 0: 65.41%, 1: 34.59%
CHEMBL1964096 0: 11.36%, 1: 88.64%
CHEMBL1964100 0: 79.62%, 1: 20.38%
CHEMBL1964101 0: 34.62%, 1: 65.38%
CHEMBL1964102 0: 71.54%, 1: 28.46%
CHEMBL1964103 0: 51.58%, 1: 48.42%
CHEMBL1964104 0: 63.88%, 1: 36.12%
CHEMBL1964105 0: 64.53%, 1: 35.47%
CHEMBL1964106 0: 57.96%, 1: 42.04%
CHEMBL1964108 0: 69.62%, 1: 30.38%
CHEMBL1964111 0: 73.41%, 1: 26.59%
CHEMBL1964112 0: 87.59%, 1: 12.41%
CHEMBL1964114 0: 61.11%, 1: 38.89%
CHEMBL1964115 0: 43.36%, 1: 56.64%
CHEMBL1964116 0: 70.55%, 1: 29.45%
CHEMBL1964117 0: 63.66%, 1: 36.34%
CHEMBL1964118 0: 60.33%, 1: 39.67%
CHEMBL1964119 0: 57.23%, 1: 42.77%
CHEMBL2028073 0: 76.99%, 1: 23.01%
CHEMBL2028074 0: 74.88%, 1: 25.12%
CHEMBL2028075 0: 56.46%, 1: 43.54%
CHEMBL2028076 0: 85.04%, 1: 14.96%
CHEMBL2028077 0: 96.19%, 1: 3.81%
CHEMBL2095143 0: 93.77%, 1: 6.23%
CHEMBL2098499 0: 74.45%, 1: 25.55%
CHEMBL2114715 0: 60.31%, 1: 39.69%
CHEMBL2114716 0: 12.27%, 1: 87.73%
CHEMBL2114719 0: 77.41%, 1: 22.59%
CHEMBL2114725 0: 51.74%, 1: 48.26%
CHEMBL2114727 0: 16.96%, 1: 83.04%
CHEMBL2114728 0: 9.82%, 1: 90.18%
CHEMBL2114737 0: 87.93%, 1: 12.07%
CHEMBL2114742 0: 33.90%, 1: 66.10%
CHEMBL2114748 0: 59.44%, 1: 40.56%
CHEMBL2114752 0: 15.02%, 1: 84.98%
CHEMBL2114753 0: 8.36%, 1: 91.64%
CHEMBL2114761 0: 78.40%, 1: 21.60%
CHEMBL2114764 0: 42.92%, 1: 57.08%
CHEMBL2114771 0: 64.02%, 1: 35.98%
CHEMBL2114791 0: 1.99%, 1: 98.01%
CHEMBL2114797 0: 42.41%, 1: 57.59%
CHEMBL2114811 0: 10.16%, 1: 89.84%
CHEMBL2114814 0: 44.98%, 1: 55.02%
CHEMBL2114816 0: 37.94%, 1: 62.06%
CHEMBL2114818 0: 4.15%, 1: 95.85%
CHEMBL2114820 0: 16.31%, 1: 83.69%
CHEMBL2114821 0: 18.77%, 1: 81.23%
CHEMBL2114823 0: 20.90%, 1: 79.10%
CHEMBL2114825 0: 18.62%, 1: 81.38%
CHEMBL2114827 0: 93.80%, 1: 6.20%
CHEMBL2114829 0: 25.17%, 1: 74.83%
CHEMBL2114830 0: 20.17%, 1: 79.83%
CHEMBL2114839 0: 28.69%, 1: 71.31%
CHEMBL2114842 0: 62.00%, 1: 38.00%
CHEMBL2114844 0: 32.59%, 1: 67.41%
CHEMBL2114847 0: 13.84%, 1: 86.16%
CHEMBL2114850 0: 8.62%, 1: 91.38%
CHEMBL2114852 0: 3.84%, 1: 96.16%
CHEMBL2114857 0: 50.00%, 1: 50.00%
CHEMBL2114858 0: 32.36%, 1: 67.64%
CHEMBL2114863 0: 33.55%, 1: 66.45%
CHEMBL2114865 0: 3.43%, 1: 96.57%
CHEMBL2114872 0: 9.51%, 1: 90.49%
CHEMBL2114874 0: 28.03%, 1: 71.97%
CHEMBL2114882 0: 92.25%, 1: 7.75%
CHEMBL2114896 0: 51.22%, 1: 48.78%
CHEMBL2114899 0: 87.76%, 1: 12.24%
CHEMBL2114909 0: 1.94%, 1: 98.06%
CHEMBL2114916 0: 42.50%, 1: 57.50%
CHEMBL2114926 0: 41.57%, 1: 58.43%
CHEMBL2114928 0: 86.06%, 1: 13.94%
CHEMBL2114930 0: 4.68%, 1: 95.32%
CHEMBL2114931 0: 97.44%, 1: 2.56%
CHEMBL2114932 0: 4.35%, 1: 95.65%
CHEMBL2354206 0: 93.86%, 1: 6.14%
CHEMBL2354207 0: 15.46%, 1: 84.54%
CHEMBL2354217 0: 20.69%, 1: 79.31%
CHEMBL2354227 0: 21.70%, 1: 78.30%
CHEMBL2354228 0: 59.88%, 1: 40.12%
CHEMBL2354248 0: 96.55%, 1: 3.45%
CHEMBL2354256 0: 7.85%, 1: 92.15%
CHEMBL2354269 0: 9.42%, 1: 90.58%
CHEMBL2354274 0: 5.26%, 1: 94.74%
CHEMBL2354276 0: 75.30%, 1: 24.70%
CHEMBL2354289 0: 34.38%, 1: 65.62%
CHEMBL2354292 0: 77.91%, 1: 22.09%
CHEMBL2354303 0: 86.76%, 1: 13.24%
CHEMBL2354305 0: 14.37%, 1: 85.63%
CHEMBL2354308 0: 9.09%, 1: 90.91%
CHEMBL2378059 0: 2.84%, 1: 97.16%
CHEMBL2449559 0: 17.37%, 1: 82.63%
CHEMBL3214794 0: 64.60%, 1: 35.40%
CHEMBL3214801 0: 28.46%, 1: 71.54%
CHEMBL3214812 0: 33.19%, 1: 66.81%
CHEMBL3214816 0: 25.43%, 1: 74.57%
CHEMBL3214851 0: 77.02%, 1: 22.98%
CHEMBL3214906 0: 61.72%, 1: 38.28%
CHEMBL3214907 0: 48.12%, 1: 51.88%
CHEMBL3214929 0: 44.25%, 1: 55.75%
CHEMBL3214930 0: 79.15%, 1: 20.85%
CHEMBL3214944 0: 32.70%, 1: 67.30%
CHEMBL3214958 0: 69.31%, 1: 30.69%
CHEMBL3214959 0: 31.09%, 1: 68.91%
CHEMBL3214970 0: 20.09%, 1: 79.91%
CHEMBL3214992 0: 27.76%, 1: 72.24%
CHEMBL3214993 0: 32.48%, 1: 67.52%
CHEMBL3214997 0: 27.27%, 1: 72.73%
CHEMBL3215006 0: 73.18%, 1: 26.82%
CHEMBL3215013 0: 10.05%, 1: 89.95%
CHEMBL3215025 0: 7.49%, 1: 92.51%
CHEMBL3215034 0: 39.87%, 1: 60.13%
CHEMBL3215078 0: 41.84%, 1: 58.16%
CHEMBL3215092 0: 57.98%, 1: 42.02%
CHEMBL3215096 0: 30.30%, 1: 69.70%
CHEMBL3215112 0: 7.48%, 1: 92.52%
CHEMBL3215116 0: 83.87%, 1: 16.13%
CHEMBL3215128 0: 14.05%, 1: 85.95%
CHEMBL3215154 0: 44.44%, 1: 55.56%
CHEMBL3215157 0: 5.96%, 1: 94.04%
CHEMBL3215158 0: 12.59%, 1: 87.41%
CHEMBL3215171 0: 97.98%, 1: 2.02%
CHEMBL3215176 0: 91.72%, 1: 8.28%
CHEMBL3215185 0: 91.95%, 1: 8.05%
CHEMBL3215187 0: 7.98%, 1: 92.02%
CHEMBL3215216 0: 15.79%, 1: 84.21%
CHEMBL3215220 0: 91.18%, 1: 8.82%
CHEMBL3215227 0: 32.66%, 1: 67.34%
CHEMBL3215228 0: 88.71%, 1: 11.29%
CHEMBL3215276 0: 7.46%, 1: 92.54%
CHEMBL3215277 0: 90.53%, 1: 9.47%
CHEMBL3215288 0: 27.05%, 1: 72.95%
CHEMBL829401 0: 10.53%, 1: 89.47%
CHEMBL830839 0: 3.07%, 1: 96.93%
CHEMBL830842 0: 10.53%, 1: 89.47%
CHEMBL914418 0: 14.93%, 1: 85.07%
CHEMBL918058 0: 93.33%, 1: 6.67%
Total scaffolds = 77 | train scaffolds = 66 | val scaffolds = 11 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([0.75,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 133 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 99 | train scaffolds = 64 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan, 0.32608696,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0, 46,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([       nan,        nan, 0.33333333,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.4, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0, 31,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 60 | train scaffolds = 48 | val scaffolds = 12 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.90566038, 0.78787879,
       0.94117647, 0.91304348, 0.95918367, 0.8974359 , 0.65853659,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 53, 33, 51, 46, 49, 39, 41,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 3, 3, 3, 3, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 0.66666667, 0.5       ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 3, 2, 3, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, 1. , 1. , 1. , 1. , 1. , 0.6, 0.6, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 7, 4, 7, 6, 7, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 61 | train scaffolds = 42 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.84848485, 0.5625    ,
       0.87179487, 0.83333333, 0.97297297, 0.96153846, 0.71428571,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 33, 48, 39, 36, 37, 26, 28,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan, nan,  0.,  1.,  0.,  1.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  0., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0.,  1., nan,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  1.,  0., nan,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 1.        , 1.        ,
       1.        , 1.        , 1.        , 0.66666667, 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 64 | train scaffolds = 42 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.92156863, 0.66666667,
       0.92307692, 0.89285714, 0.98305085, 0.9047619 , 0.625     ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 51, 39, 65, 56, 59, 42, 48,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  1.,  0., nan,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, 1. , 0. , 1. , 0.5, 1. , 1. , 0. , nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 65 | train scaffolds = 48 | val scaffolds = 17 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.91304348, 0.72222222,
       0.92857143, 0.9       , 0.98113208, 0.95      , 0.67391304,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 46, 36, 56, 60, 53, 40, 46,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0., nan,  0.,  0., nan,  0.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  0.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 71 | train scaffolds = 49 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.95918367, 0.72972973,
       0.94915254, 0.96226415, 0.96969697, 0.93023256, 0.65306122,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 49, 37, 59, 53, 66, 43, 49,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 0.66666667, 0.5       ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 3, 2, 3, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 48 | train scaffolds = 26 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.97435897, 0.80769231,
       1.        , 0.975     , 0.97674419, 0.91304348, 0.80487805,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 39, 26, 42, 40, 43, 46, 41,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0., nan, nan,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 3, 3, 3, 3, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 1.        , 1.        ,
       1.        , 1.        , 1.        , 0.6       , 0.33333333,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 5, 3, 5, 4, 5, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 51 | train scaffolds = 30 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.97560976, 0.78571429,
       1.        , 0.95652174, 0.97959184, 0.95121951, 0.65384615,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 41, 28, 48, 46, 49, 41, 52,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 0, 2, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  0., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0., nan, nan,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 32 | train scaffolds = 21 | val scaffolds = 11 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.63492063, 0.44444444,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 63, 45,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([     nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan,      nan,      nan, 0.8     , 0.640625,
            nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan]), array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 105,  64,   0,
         0,   0,   0,   0,   0,   0,   0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.53846154, 0.47058824,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 39, 34,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 0.8 ,
       0.75,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0., nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 31 | train scaffolds = 15 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.53333333, 0.42      ,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 45, 50,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([     nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan,      nan,      nan, 0.703125, 0.625   ,
            nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 64, 72,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.47058824, 0.44736842,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34, 38,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 0.75,
       0.6 ,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10, 10,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0. , 0.5, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 245 | train scaffolds = 196 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 409 | train scaffolds = 339 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 98 | train scaffolds = 67 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.52173913,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 46,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.66666667,
       1.        ,        nan, 0.76      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 33,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.33333333,
              nan,        nan, 1.        ,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 213 | train scaffolds = 168 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
Total scaffolds = 94 | train scaffolds = 64 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.64705882,
              nan,        nan, 0.56410256,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34,  0,  0,
       39,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.84      ,
       1.        ,        nan, 0.65517241,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  1,  0,
       29,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 139 | train scaffolds = 111 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
Total scaffolds = 183 | train scaffolds = 146 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]))]
Total scaffolds = 71 | train scaffolds = 46 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86363636,
              nan,        nan, 0.81818182,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  0,  0,
       22,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.89473684,
       1.        ,        nan, 0.85      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19,  1,  0,
       20,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 188 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
Total scaffolds = 626 | train scaffolds = 501 | val scaffolds = 125 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 179 | train scaffolds = 142 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 84 | train scaffolds = 56 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.85714286,
              nan,        nan, 0.77272727,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  0,  0,
       22,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.90909091,
       1.        ,        nan, 0.9047619 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 446 | train scaffolds = 358 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 67 | train scaffolds = 44 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.84      ,
              nan,        nan, 0.83333333,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  0,  0,
       24,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95      ,
       1.        ,        nan, 0.85714286,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 1. , nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 115 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 74 | train scaffolds = 52 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.9 ,  nan,  nan, 0.75,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  0,  0,
       20,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.94736842,
       1.        ,        nan, 0.85714286,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0]))]
Total scaffolds = 31 | train scaffolds = 16 | val scaffolds = 15 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 53 | train scaffolds = 47 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 262 | train scaffolds = 209 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 119 | train scaffolds = 92 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 76 | train scaffolds = 50 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan, 0.  , 0.8 ,  nan,  nan, 0.72,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 25,  0,  0,
       25,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 52 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.73333333,
              nan,        nan, 0.67741935,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,  0,  0,
       31,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 147 | train scaffolds = 116 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 77 | train scaffolds = 50 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.71428571,
              nan,        nan, 0.65517241,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95652174,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 161 | train scaffolds = 128 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 53 | train scaffolds = 47 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 82 | train scaffolds = 54 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.75      ,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 208 | train scaffolds = 164 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 160 | train scaffolds = 123 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 257 | train scaffolds = 189 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
Total scaffolds = 744 | train scaffolds = 602 | val scaffolds = 142 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 448 | train scaffolds = 357 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 145 | train scaffolds = 110 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 85 | train scaffolds = 59 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.63636364,
              nan,        nan, 0.625     ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 33,  0,  0,
       32,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95652174,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0]))]
Total scaffolds = 77 | train scaffolds = 53 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.74193548,
              nan,        nan, 0.68965517,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91304348,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 232 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 224 | train scaffolds = 175 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 75 | train scaffolds = 50 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.72,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  0,  0,
       25,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86956522,
       1.        ,        nan, 0.7826087 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 240 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 113 | train scaffolds = 88 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       0.5, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 57 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.71428571,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86956522,
       1.        ,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       24,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]))]
Total scaffolds = 91 | train scaffolds = 59 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.63888889,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 36,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.78571429,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 1. , nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 273 | train scaffolds = 222 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
Total scaffolds = 283 | train scaffolds = 223 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 298 | train scaffolds = 247 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 204 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 81 | train scaffolds = 53 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.67647059,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0]))]
Total scaffolds = 122 | train scaffolds = 97 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 752 | train scaffolds = 591 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 656 | train scaffolds = 506 | val scaffolds = 150 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 188 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 297 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 506 | train scaffolds = 408 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 342 | train scaffolds = 270 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 120 | train scaffolds = 97 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 404 | train scaffolds = 318 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 78 | train scaffolds = 52 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.79166667,
              nan,        nan, 0.73913043,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 24,  0,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , 1. , nan, 0.9, nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  1,  0,
       20,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0]))]
Total scaffolds = 87 | train scaffolds = 58 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.75      ,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.79166667,
       1.        ,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  1,  0,
       24,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 81 | train scaffolds = 50 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.9       ,
              nan,        nan, 0.77777778,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,
        9,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11,  0,  0,
       11,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 115 | train scaffolds = 90 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 87 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 276 | train scaffolds = 220 | val scaffolds = 56 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 685 | train scaffolds = 555 | val scaffolds = 130 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 159 | train scaffolds = 127 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 147 | train scaffolds = 126 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 447 | train scaffolds = 355 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 270 | train scaffolds = 213 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 208 | train scaffolds = 165 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 57 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 298 | train scaffolds = 247 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 113 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 63 | train scaffolds = 47 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 272 | train scaffolds = 218 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 163 | train scaffolds = 129 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 342 | train scaffolds = 270 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 120 | train scaffolds = 95 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 86 | train scaffolds = 65 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.73913043,
              nan,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 23,  0,  0,
       20,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.92857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14,  1,  0,
       14,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 224 | train scaffolds = 179 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 83 | train scaffolds = 59 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([    nan,     nan,     nan,     nan,     nan,     nan,     nan,
           nan,     nan,     nan,     nan,     nan,     nan,     nan,
       0.6875 ,     nan,     nan, 0.65625,     nan,     nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 32,  0,  0,
       32,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91666667,
       1.        ,        nan, 0.76      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 37 | train scaffolds = 24 | val scaffolds = 13 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 270 | train scaffolds = 213 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 146 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 84 | train scaffolds = 58 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.76923077,
              nan,        nan, 0.7037037 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  0,  0,
       27,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.84, 1.  ,  nan, 0.76,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
Total scaffolds = 156 | train scaffolds = 119 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 426 | train scaffolds = 335 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 73 | train scaffolds = 44 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.85714286,
       1.        ,        nan, 0.92307692,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14,  1,  0,
       13,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.76470588,
              nan,        nan, 0.82352941,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 17,  0,  0,
       17,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 111 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, nan, nan, 0.5]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 644 | train scaffolds = 506 | val scaffolds = 138 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 346 | train scaffolds = 268 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 206 | train scaffolds = 159 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 75 | train scaffolds = 50 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.77419355,
              nan,        nan, 0.72413793,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 98 | train scaffolds = 76 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 154 | train scaffolds = 123 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 233 | train scaffolds = 186 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 77 | train scaffolds = 51 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.76666667,
              nan,        nan, 0.7       ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 30,  0,  0,
       30,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91304348,
       1.        ,        nan, 0.82608696,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 693 | train scaffolds = 558 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 98 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 177 | train scaffolds = 138 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 154 | train scaffolds = 123 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 112 | train scaffolds = 90 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 224 | train scaffolds = 179 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 76 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 113 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 262 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan, 1.        ,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 761 | train scaffolds = 597 | val scaffolds = 164 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 82 | train scaffolds = 57 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.73333333,
              nan,        nan, 0.67741935,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,  0,  0,
       31,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.90909091,
       1.        ,        nan, 0.7826087 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 344 | train scaffolds = 272 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 139 | train scaffolds = 111 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
Total scaffolds = 297 | train scaffolds = 232 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 124 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 726 | train scaffolds = 579 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 103 | train scaffolds = 79 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 577 | train scaffolds = 469 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 70 | train scaffolds = 48 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 417 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 151 | train scaffolds = 120 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 176 | train scaffolds = 136 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 57 | train scaffolds = 29 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 717 | train scaffolds = 571 | val scaffolds = 146 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 502 | train scaffolds = 404 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 113 | train scaffolds = 89 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 112 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 130 | train scaffolds = 103 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 103 | train scaffolds = 84 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 213 | train scaffolds = 168 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 151 | train scaffolds = 118 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 63 | train scaffolds = 41 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 223 | train scaffolds = 177 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 227 | train scaffolds = 177 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 417 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 265 | train scaffolds = 214 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 155 | train scaffolds = 125 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 169 | train scaffolds = 131 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 120 | train scaffolds = 98 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 236 | train scaffolds = 181 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan,        nan,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 464 | train scaffolds = 373 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 293 | train scaffolds = 238 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 163 | train scaffolds = 127 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 545 | train scaffolds = 441 | val scaffolds = 104 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 156 | train scaffolds = 126 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 118 | train scaffolds = 98 | val scaffolds = 20 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 218 | train scaffolds = 175 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 227 | train scaffolds = 177 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 150 | train scaffolds = 121 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 196 | train scaffolds = 170 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 260 | train scaffolds = 209 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 502 | train scaffolds = 404 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 65 | val scaffolds = 15 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 152 | train scaffolds = 124 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 452 | train scaffolds = 366 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 131 | train scaffolds = 102 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 219 | train scaffolds = 164 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan, 1.        ,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 367 | train scaffolds = 299 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 132 | train scaffolds = 105 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 772 | train scaffolds = 625 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 218 | train scaffolds = 175 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 228 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 399 | train scaffolds = 320 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 111 | train scaffolds = 90 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 772 | train scaffolds = 625 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 550 | train scaffolds = 434 | val scaffolds = 116 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 216 | train scaffolds = 172 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 372 | train scaffolds = 301 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 228 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 257 | train scaffolds = 200 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 318 | train scaffolds = 263 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 84 | train scaffolds = 61 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 407 | train scaffolds = 320 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 338 | train scaffolds = 268 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 279 | train scaffolds = 218 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 159 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 159 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 258 | train scaffolds = 196 | val scaffolds = 62 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 192 | train scaffolds = 154 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 346 | train scaffolds = 279 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 174 | train scaffolds = 141 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 327 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 229 | train scaffolds = 181 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 224 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 372 | train scaffolds = 301 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 196 | train scaffolds = 156 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 269 | train scaffolds = 214 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 159 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 780 | train scaffolds = 619 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 195 | train scaffolds = 158 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 191 | train scaffolds = 148 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 179 | train scaffolds = 137 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 413 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 405 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 400 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 485 | train scaffolds = 401 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 486 | train scaffolds = 404 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 406 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 481 | train scaffolds = 397 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 473 | train scaffolds = 392 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 402 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.75      ,
              nan,        nan, 0.66666667,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 481 | train scaffolds = 395 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 405 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 484 | train scaffolds = 402 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 400 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 414 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 486 | train scaffolds = 398 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 401 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 485 | train scaffolds = 401 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 477 | train scaffolds = 394 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 404 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 405 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 407 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 406 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 365 | val scaffolds = 123 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 409 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 360 | val scaffolds = 123 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 482 | train scaffolds = 404 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 400 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 401 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 407 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 402 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 407 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 403 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 485 | train scaffolds = 373 | val scaffolds = 112 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 405 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 402 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 400 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 400 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 408 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 411 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 409 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 405 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 409 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 403 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 404 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 479 | train scaffolds = 362 | val scaffolds = 117 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 408 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 405 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 375 | val scaffolds = 113 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 396 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 477 | train scaffolds = 391 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 399 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 401 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 482 | train scaffolds = 396 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 359 | train scaffolds = 282 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 253 | train scaffolds = 192 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 440 | train scaffolds = 371 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 294 | train scaffolds = 237 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 588 | train scaffolds = 455 | val scaffolds = 133 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 421 | val scaffolds = 105 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 252 | train scaffolds = 202 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 502 | train scaffolds = 385 | val scaffolds = 117 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 424 | train scaffolds = 332 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 311 | train scaffolds = 249 | val scaffolds = 62 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 349 | train scaffolds = 273 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 329 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 451 | train scaffolds = 369 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 542 | train scaffolds = 445 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 309 | train scaffolds = 246 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 437 | train scaffolds = 345 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 467 | train scaffolds = 374 | val scaffolds = 93 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 465 | train scaffolds = 374 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 433 | train scaffolds = 363 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 415 | train scaffolds = 328 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 353 | train scaffolds = 277 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 256 | train scaffolds = 199 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 301 | train scaffolds = 234 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 477 | train scaffolds = 388 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 436 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 303 | train scaffolds = 228 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 337 | train scaffolds = 272 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 532 | train scaffolds = 428 | val scaffolds = 104 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 536 | train scaffolds = 434 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 400 | train scaffolds = 327 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 254 | train scaffolds = 202 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 461 | train scaffolds = 384 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 465 | train scaffolds = 368 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 286 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 551 | train scaffolds = 448 | val scaffolds = 103 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 564 | train scaffolds = 454 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 308 | train scaffolds = 230 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 349 | train scaffolds = 276 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 432 | train scaffolds = 340 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 301 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 432 | train scaffolds = 348 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 569 | train scaffolds = 445 | val scaffolds = 124 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 434 | train scaffolds = 357 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 364 | train scaffolds = 298 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 445 | train scaffolds = 370 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 582 | train scaffolds = 460 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 508 | train scaffolds = 382 | val scaffolds = 126 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 340 | train scaffolds = 251 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 227 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 343 | train scaffolds = 278 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 539 | train scaffolds = 439 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 359 | train scaffolds = 293 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 388 | train scaffolds = 316 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 495 | train scaffolds = 392 | val scaffolds = 103 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 524 | train scaffolds = 429 | val scaffolds = 95 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 302 | train scaffolds = 224 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 356 | train scaffolds = 278 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 376 | train scaffolds = 296 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 441 | train scaffolds = 352 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 529 | train scaffolds = 427 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 256 | train scaffolds = 214 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 428 | train scaffolds = 355 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 249 | train scaffolds = 196 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 422 | train scaffolds = 326 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 286 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 362 | train scaffolds = 287 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 302 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 278 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 439 | train scaffolds = 343 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 379 | train scaffolds = 293 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 233 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 568 | train scaffolds = 453 | val scaffolds = 115 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 523 | train scaffolds = 401 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 314 | train scaffolds = 232 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 543 | train scaffolds = 444 | val scaffolds = 99 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 209 | train scaffolds = 163 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 234 | train scaffolds = 201 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 230 | train scaffolds = 182 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 538 | train scaffolds = 433 | val scaffolds = 105 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 347 | train scaffolds = 277 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 594 | train scaffolds = 466 | val scaffolds = 128 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 429 | train scaffolds = 353 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 439 | train scaffolds = 352 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 367 | train scaffolds = 299 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 362 | train scaffolds = 282 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 256 | train scaffolds = 205 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 589 | train scaffolds = 471 | val scaffolds = 118 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 404 | train scaffolds = 333 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 480 | train scaffolds = 379 | val scaffolds = 101 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 431 | train scaffolds = 356 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 339 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 370 | train scaffolds = 287 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 358 | train scaffolds = 281 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 331 | train scaffolds = 261 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 293 | train scaffolds = 239 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 273 | train scaffolds = 216 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 454 | train scaffolds = 372 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 436 | train scaffolds = 361 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 351 | train scaffolds = 261 | val scaffolds = 90 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 265 | train scaffolds = 217 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 330 | train scaffolds = 264 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 237 | train scaffolds = 186 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 250 | train scaffolds = 196 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 594 | train scaffolds = 474 | val scaffolds = 120 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 431 | train scaffolds = 353 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 382 | train scaffolds = 310 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 259 | train scaffolds = 207 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 398 | train scaffolds = 315 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 132 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 588 | train scaffolds = 457 | val scaffolds = 131 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 285 | train scaffolds = 237 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 410 | train scaffolds = 325 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 460 | train scaffolds = 350 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 367 | train scaffolds = 289 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 426 | train scaffolds = 355 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 369 | train scaffolds = 288 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 408 | train scaffolds = 343 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 550 | train scaffolds = 439 | val scaffolds = 111 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 288 | train scaffolds = 242 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 273 | train scaffolds = 220 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 448 | train scaffolds = 356 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 369 | train scaffolds = 283 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 317 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 442 | train scaffolds = 365 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 267 | train scaffolds = 209 | val scaffolds = 58 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 544 | train scaffolds = 442 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 395 | train scaffolds = 322 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 376 | train scaffolds = 301 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 422 | train scaffolds = 328 | val scaffolds = 94 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 603 | train scaffolds = 491 | val scaffolds = 112 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 200 | train scaffolds = 156 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 158 | train scaffolds = 120 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 442 | train scaffolds = 360 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 158 | train scaffolds = 120 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 155 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 171 | train scaffolds = 136 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 154 | train scaffolds = 127 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 383 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 571 | train scaffolds = 423 | val scaffolds = 148 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 162 | train scaffolds = 124 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 242 | train scaffolds = 188 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 535 | train scaffolds = 428 | val scaffolds = 107 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 110 | train scaffolds = 85 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 289 | train scaffolds = 228 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 630 | train scaffolds = 498 | val scaffolds = 132 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 189 | train scaffolds = 147 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 131 | train scaffolds = 104 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 343 | train scaffolds = 268 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 189 | train scaffolds = 147 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 596 | train scaffolds = 466 | val scaffolds = 130 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 358 | train scaffolds = 276 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 586 | train scaffolds = 464 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 343 | train scaffolds = 268 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 279 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 560 | train scaffolds = 451 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 436 | train scaffolds = 369 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 237 | train scaffolds = 189 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 562 | train scaffolds = 426 | val scaffolds = 136 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 368 | train scaffolds = 283 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 241 | train scaffolds = 203 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 233 | train scaffolds = 191 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 242 | train scaffolds = 189 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 198 | train scaffolds = 152 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 265 | train scaffolds = 218 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 194 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 324 | train scaffolds = 247 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 231 | train scaffolds = 174 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 300 | train scaffolds = 229 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 450 | train scaffolds = 359 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 176 | train scaffolds = 141 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 169 | train scaffolds = 135 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 172 | train scaffolds = 139 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 184 | train scaffolds = 145 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 98 | train scaffolds = 79 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 780 | train scaffolds = 623 | val scaffolds = 157 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 260 | train scaffolds = 199 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 318 | train scaffolds = 255 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 157 | train scaffolds = 124 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 160 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 120 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 192 | train scaffolds = 153 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 294 | train scaffolds = 236 | val scaffolds = 58 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 269 | train scaffolds = 212 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 155 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 214 | train scaffolds = 175 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 143 | train scaffolds = 111 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 192 | train scaffolds = 153 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 302 | train scaffolds = 253 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 713 | train scaffolds = 578 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 408 | train scaffolds = 319 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 120 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 232 | train scaffolds = 180 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 108 | train scaffolds = 85 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 152 | train scaffolds = 127 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 135 | train scaffolds = 80 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.65517241,
              nan,        nan, 0.66666667,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 29,  0,  0,
       27,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.82352941,
       1.        ,        nan, 0.8125    ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 17,  1,  0,
       16,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 1.  ,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 160 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 115 | train scaffolds = 87 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 314 | train scaffolds = 255 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 269 | train scaffolds = 212 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 121 | train scaffolds = 93 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 260 | train scaffolds = 208 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 351 | train scaffolds = 278 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 155 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 333 | train scaffolds = 270 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 619 | train scaffolds = 497 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 306 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 625 | train scaffolds = 487 | val scaffolds = 138 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 619 | train scaffolds = 497 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 128 | train scaffolds = 96 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 789 | train scaffolds = 628 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 713 | train scaffolds = 578 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 154 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 144 | train scaffolds = 110 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 164 | train scaffolds = 130 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 200 | train scaffolds = 156 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 116 | train scaffolds = 86 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 168 | train scaffolds = 137 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 298 | train scaffolds = 228 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 221 | train scaffolds = 174 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 221 | train scaffolds = 174 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 214 | train scaffolds = 168 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 179 | train scaffolds = 143 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 164 | train scaffolds = 130 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 570 | train scaffolds = 452 | val scaffolds = 118 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 56 | train scaffolds = 50 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 80 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
Total scaffolds = 104 | train scaffolds = 83 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
Total scaffolds = 197 | train scaffolds = 158 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 217 | train scaffolds = 182 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 612 | train scaffolds = 492 | val scaffolds = 120 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 113 | train scaffolds = 91 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 498 | train scaffolds = 397 | val scaffolds = 101 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 203 | train scaffolds = 160 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 176 | train scaffolds = 143 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 190 | train scaffolds = 152 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 353 | train scaffolds = 274 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 443 | train scaffolds = 352 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 396 | train scaffolds = 310 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 153 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 122 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 221 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 417 | train scaffolds = 336 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 104 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 190 | train scaffolds = 152 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 237 | train scaffolds = 184 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 130 | train scaffolds = 94 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.75      ,
              nan,        nan, 0.83333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 0, 0, 6, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 162 | train scaffolds = 132 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]))]
Total scaffolds = 193 | train scaffolds = 154 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 309 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 124 | train scaffolds = 99 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 239 | train scaffolds = 189 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 249 | train scaffolds = 184 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 1. , nan, 0.5]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 282 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 105 | train scaffolds = 81 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 207 | train scaffolds = 165 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 174 | train scaffolds = 129 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 239 | train scaffolds = 189 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 91 | train scaffolds = 73 | val scaffolds = 18 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 377 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 350 | train scaffolds = 276 | val scaffolds = 74 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 36 | train scaffolds = 20 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 41 | train scaffolds = 30 | val scaffolds = 11 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 36 | train scaffolds = 20 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 75 | train scaffolds = 57 | val scaffolds = 18 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 85 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 204 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 622 | train scaffolds = 493 | val scaffolds = 129 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 377 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 480 | train scaffolds = 356 | val scaffolds = 124 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 486 | train scaffolds = 403 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 406 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 480 | train scaffolds = 398 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 406 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 404 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 435 | train scaffolds = 355 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 431 | train scaffolds = 334 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 428 | train scaffolds = 342 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 279 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 383 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 232 | train scaffolds = 180 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 329 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 228 | train scaffolds = 183 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 448 | train scaffolds = 357 | val scaffolds = 40 | test scaffolds = 51
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 147 | train scaffolds = 125 | val scaffolds = 5 | test scaffolds = 17
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 364 | train scaffolds = 293 | val scaffolds = 32 | test scaffolds = 39
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 117 | train scaffolds = 91 | val scaffolds = 15 | test scaffolds = 11
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 104 | train scaffolds = 80 | val scaffolds = 12 | test scaffolds = 12
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 32 | test scaffolds = 40
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 117 | train scaffolds = 94 | val scaffolds = 9 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 198 | val scaffolds = 28 | test scaffolds = 25
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 20 | test scaffolds = 20
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 294 | val scaffolds = 34 | test scaffolds = 38
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 364 | val scaffolds = 59 | test scaffolds = 67
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
        1.,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 376 | val scaffolds = 53 | test scaffolds = 62
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 491 | train scaffolds = 375 | val scaffolds = 57 | test scaffolds = 59
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 489 | train scaffolds = 362 | val scaffolds = 63 | test scaffolds = 64
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 366 | val scaffolds = 68 | test scaffolds = 57
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 482 | train scaffolds = 354 | val scaffolds = 59 | test scaffolds = 69
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 410 | train scaffolds = 346 | val scaffolds = 31 | test scaffolds = 33
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 546 | train scaffolds = 443 | val scaffolds = 45 | test scaffolds = 58
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 111 | val scaffolds = 9 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 25 | test scaffolds = 24
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 240 | train scaffolds = 197 | val scaffolds = 18 | test scaffolds = 25
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 122 | train scaffolds = 95 | val scaffolds = 13 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 139 | train scaffolds = 105 | val scaffolds = 19 | test scaffolds = 15
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1.,  1., nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 6, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.6       ,
              nan,        nan, 0.66666667,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,
        9,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 217 | train scaffolds = 182 | val scaffolds = 21 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 131 | train scaffolds = 104 | val scaffolds = 13 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 69 | train scaffolds = 53 | val scaffolds = 7 | test scaffolds = 9
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Building model 
Number of parameters = 385,101
MAML(
  (module): MoleculeModel(
    (sigmoid): Sigmoid()
    (encoder): MPN(
      (encoder): MPNEncoder(
        (dropout_layer): Dropout(p=0.2, inplace=False)
        (act_func): ReLU()
        (W_i): Linear(in_features=147, out_features=300, bias=False)
        (W_h): Linear(in_features=300, out_features=300, bias=False)
        (W_o): Linear(in_features=433, out_features=300, bias=True)
      )
    )
    (ffn): Sequential(
      (0): Dropout(p=0.2, inplace=False)
      (1): Linear(in_features=300, out_features=400, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.2, inplace=False)
      (4): Linear(in_features=400, out_features=1, bias=True)
    )
  )
)
Moving maml model to cuda
Epoch 0
Meta loss on this task batch = 1.0796e+00, Meta loss averaged over last 500 steps = 1.0796e+00, PNorm = 51.0148, GNorm = 1.0000
Meta loss on this task batch = 6.0342e-01, Meta loss averaged over last 500 steps = 8.4153e-01, PNorm = 51.0201, GNorm = 1.0000
Meta loss on this task batch = 5.8038e-01, Meta loss averaged over last 500 steps = 7.5448e-01, PNorm = 51.0281, GNorm = 0.2402
Meta loss on this task batch = 5.4474e-01, Meta loss averaged over last 500 steps = 7.0204e-01, PNorm = 51.0350, GNorm = 0.3017
Meta loss on this task batch = 5.7594e-01, Meta loss averaged over last 500 steps = 6.7682e-01, PNorm = 51.0437, GNorm = 0.2738
Meta loss on this task batch = 5.1383e-01, Meta loss averaged over last 500 steps = 6.4966e-01, PNorm = 51.0533, GNorm = 0.1854
Meta loss on this task batch = 4.6552e-01, Meta loss averaged over last 500 steps = 6.2335e-01, PNorm = 51.0640, GNorm = 0.2327
Meta loss on this task batch = 5.7480e-01, Meta loss averaged over last 500 steps = 6.1728e-01, PNorm = 51.0724, GNorm = 0.5087
Meta loss on this task batch = 4.7247e-01, Meta loss averaged over last 500 steps = 6.0119e-01, PNorm = 51.0809, GNorm = 0.1468
Meta loss on this task batch = 4.9365e-01, Meta loss averaged over last 500 steps = 5.9044e-01, PNorm = 51.0901, GNorm = 0.1824
Meta loss on this task batch = 4.5585e-01, Meta loss averaged over last 500 steps = 5.7820e-01, PNorm = 51.0994, GNorm = 0.1360
Meta loss on this task batch = 4.7584e-01, Meta loss averaged over last 500 steps = 5.6967e-01, PNorm = 51.1087, GNorm = 0.1195
Meta loss on this task batch = 4.5498e-01, Meta loss averaged over last 500 steps = 5.6085e-01, PNorm = 51.1182, GNorm = 0.1555
Meta loss on this task batch = 4.0283e-01, Meta loss averaged over last 500 steps = 5.4956e-01, PNorm = 51.1275, GNorm = 0.1306
Meta loss on this task batch = 4.3288e-01, Meta loss averaged over last 500 steps = 5.4178e-01, PNorm = 51.1366, GNorm = 0.1510
Meta loss on this task batch = 4.5940e-01, Meta loss averaged over last 500 steps = 5.3664e-01, PNorm = 51.1457, GNorm = 0.1474
Meta loss on this task batch = 5.6992e-01, Meta loss averaged over last 500 steps = 5.3859e-01, PNorm = 51.1529, GNorm = 0.4649
Meta loss on this task batch = 5.1361e-01, Meta loss averaged over last 500 steps = 5.3721e-01, PNorm = 51.1594, GNorm = 0.2962
Meta loss on this task batch = 5.3055e-01, Meta loss averaged over last 500 steps = 5.3686e-01, PNorm = 51.1656, GNorm = 0.1430
Took 139.5722053050995 seconds to complete one epoch of meta training
Took 146.8538796901703 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.289476
Found better MAML checkpoint after meta validation, saving now
Epoch 1
Meta loss on this task batch = 5.2229e-01, Meta loss averaged over last 500 steps = 5.3613e-01, PNorm = 51.1725, GNorm = 0.2957
Meta loss on this task batch = 5.2699e-01, Meta loss averaged over last 500 steps = 5.3569e-01, PNorm = 51.1801, GNorm = 0.2872
Meta loss on this task batch = 5.0495e-01, Meta loss averaged over last 500 steps = 5.3429e-01, PNorm = 51.1881, GNorm = 0.2387
Meta loss on this task batch = 4.1857e-01, Meta loss averaged over last 500 steps = 5.2926e-01, PNorm = 51.1966, GNorm = 0.1743
Meta loss on this task batch = 5.1114e-01, Meta loss averaged over last 500 steps = 5.2851e-01, PNorm = 51.2036, GNorm = 0.3192
Meta loss on this task batch = 5.2726e-01, Meta loss averaged over last 500 steps = 5.2846e-01, PNorm = 51.2094, GNorm = 0.3544
Meta loss on this task batch = 5.0403e-01, Meta loss averaged over last 500 steps = 5.2752e-01, PNorm = 51.2143, GNorm = 0.1830
Meta loss on this task batch = 4.9558e-01, Meta loss averaged over last 500 steps = 5.2634e-01, PNorm = 51.2190, GNorm = 0.1101
Meta loss on this task batch = 4.3506e-01, Meta loss averaged over last 500 steps = 5.2308e-01, PNorm = 51.2235, GNorm = 0.1223
Meta loss on this task batch = 4.8186e-01, Meta loss averaged over last 500 steps = 5.2165e-01, PNorm = 51.2275, GNorm = 0.1521
Meta loss on this task batch = 4.3408e-01, Meta loss averaged over last 500 steps = 5.1874e-01, PNorm = 51.2325, GNorm = 0.2632
Meta loss on this task batch = 4.8147e-01, Meta loss averaged over last 500 steps = 5.1753e-01, PNorm = 51.2375, GNorm = 0.1061
Meta loss on this task batch = 5.1715e-01, Meta loss averaged over last 500 steps = 5.1752e-01, PNorm = 51.2419, GNorm = 0.1271
Meta loss on this task batch = 4.5633e-01, Meta loss averaged over last 500 steps = 5.1567e-01, PNorm = 51.2464, GNorm = 0.1092
Meta loss on this task batch = 4.7774e-01, Meta loss averaged over last 500 steps = 5.1455e-01, PNorm = 51.2511, GNorm = 0.1178
Meta loss on this task batch = 5.1343e-01, Meta loss averaged over last 500 steps = 5.1452e-01, PNorm = 51.2551, GNorm = 0.1380
Meta loss on this task batch = 4.8866e-01, Meta loss averaged over last 500 steps = 5.1380e-01, PNorm = 51.2582, GNorm = 0.2062
Meta loss on this task batch = 4.1442e-01, Meta loss averaged over last 500 steps = 5.1112e-01, PNorm = 51.2622, GNorm = 0.1777
Meta loss on this task batch = 5.0782e-01, Meta loss averaged over last 500 steps = 5.1103e-01, PNorm = 51.2664, GNorm = 0.1407
Took 142.5295729637146 seconds to complete one epoch of meta training
Took 152.03356623649597 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.313643
Found better MAML checkpoint after meta validation, saving now
Epoch 2
Meta loss on this task batch = 4.2845e-01, Meta loss averaged over last 500 steps = 5.0891e-01, PNorm = 51.2702, GNorm = 0.1320
Meta loss on this task batch = 4.6966e-01, Meta loss averaged over last 500 steps = 5.0793e-01, PNorm = 51.2741, GNorm = 0.1230
Meta loss on this task batch = 4.5882e-01, Meta loss averaged over last 500 steps = 5.0673e-01, PNorm = 51.2781, GNorm = 0.0903
Meta loss on this task batch = 4.8846e-01, Meta loss averaged over last 500 steps = 5.0630e-01, PNorm = 51.2814, GNorm = 0.1019
Meta loss on this task batch = 4.9918e-01, Meta loss averaged over last 500 steps = 5.0613e-01, PNorm = 51.2843, GNorm = 0.1099
Meta loss on this task batch = 4.2684e-01, Meta loss averaged over last 500 steps = 5.0433e-01, PNorm = 51.2879, GNorm = 0.1324
Meta loss on this task batch = 4.1839e-01, Meta loss averaged over last 500 steps = 5.0242e-01, PNorm = 51.2918, GNorm = 0.1117
Meta loss on this task batch = 4.9310e-01, Meta loss averaged over last 500 steps = 5.0222e-01, PNorm = 51.2946, GNorm = 0.1834
Meta loss on this task batch = 4.6903e-01, Meta loss averaged over last 500 steps = 5.0151e-01, PNorm = 51.2974, GNorm = 0.1046
Meta loss on this task batch = 5.2278e-01, Meta loss averaged over last 500 steps = 5.0195e-01, PNorm = 51.2998, GNorm = 0.1164
Meta loss on this task batch = 5.2828e-01, Meta loss averaged over last 500 steps = 5.0249e-01, PNorm = 51.3025, GNorm = 0.1260
Meta loss on this task batch = 4.8534e-01, Meta loss averaged over last 500 steps = 5.0215e-01, PNorm = 51.3049, GNorm = 0.1143
Meta loss on this task batch = 4.9927e-01, Meta loss averaged over last 500 steps = 5.0209e-01, PNorm = 51.3078, GNorm = 0.1047
Meta loss on this task batch = 4.7848e-01, Meta loss averaged over last 500 steps = 5.0164e-01, PNorm = 51.3108, GNorm = 0.1049
Meta loss on this task batch = 5.0382e-01, Meta loss averaged over last 500 steps = 5.0168e-01, PNorm = 51.3140, GNorm = 0.1285
Meta loss on this task batch = 4.9326e-01, Meta loss averaged over last 500 steps = 5.0152e-01, PNorm = 51.3172, GNorm = 0.1108
Meta loss on this task batch = 4.5853e-01, Meta loss averaged over last 500 steps = 5.0074e-01, PNorm = 51.3200, GNorm = 0.1164
Meta loss on this task batch = 4.6850e-01, Meta loss averaged over last 500 steps = 5.0017e-01, PNorm = 51.3225, GNorm = 0.1449
Meta loss on this task batch = 5.2066e-01, Meta loss averaged over last 500 steps = 5.0053e-01, PNorm = 51.3253, GNorm = 0.1112
Took 136.27737760543823 seconds to complete one epoch of meta training
Took 144.6601665019989 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.332932
Found better MAML checkpoint after meta validation, saving now
Epoch 3
Meta loss on this task batch = 4.8640e-01, Meta loss averaged over last 500 steps = 5.0028e-01, PNorm = 51.3282, GNorm = 0.0811
Meta loss on this task batch = 4.8767e-01, Meta loss averaged over last 500 steps = 5.0007e-01, PNorm = 51.3307, GNorm = 0.1011
Meta loss on this task batch = 4.9088e-01, Meta loss averaged over last 500 steps = 4.9991e-01, PNorm = 51.3334, GNorm = 0.1207
Meta loss on this task batch = 4.7454e-01, Meta loss averaged over last 500 steps = 4.9950e-01, PNorm = 51.3366, GNorm = 0.0908
Meta loss on this task batch = 4.7633e-01, Meta loss averaged over last 500 steps = 4.9913e-01, PNorm = 51.3406, GNorm = 0.1239
Meta loss on this task batch = 4.9401e-01, Meta loss averaged over last 500 steps = 4.9904e-01, PNorm = 51.3449, GNorm = 0.0966
Meta loss on this task batch = 5.2859e-01, Meta loss averaged over last 500 steps = 4.9951e-01, PNorm = 51.3487, GNorm = 0.1155
Meta loss on this task batch = 4.7537e-01, Meta loss averaged over last 500 steps = 4.9913e-01, PNorm = 51.3522, GNorm = 0.0840
Meta loss on this task batch = 4.0402e-01, Meta loss averaged over last 500 steps = 4.9769e-01, PNorm = 51.3557, GNorm = 0.0844
Meta loss on this task batch = 4.6159e-01, Meta loss averaged over last 500 steps = 4.9715e-01, PNorm = 51.3578, GNorm = 0.1999
Meta loss on this task batch = 4.4242e-01, Meta loss averaged over last 500 steps = 4.9635e-01, PNorm = 51.3592, GNorm = 0.1633
Meta loss on this task batch = 5.5096e-01, Meta loss averaged over last 500 steps = 4.9714e-01, PNorm = 51.3611, GNorm = 0.1171
Meta loss on this task batch = 4.2622e-01, Meta loss averaged over last 500 steps = 4.9613e-01, PNorm = 51.3638, GNorm = 0.0944
Meta loss on this task batch = 4.7242e-01, Meta loss averaged over last 500 steps = 4.9579e-01, PNorm = 51.3671, GNorm = 0.0960
Meta loss on this task batch = 3.7772e-01, Meta loss averaged over last 500 steps = 4.9415e-01, PNorm = 51.3731, GNorm = 0.3095
Meta loss on this task batch = 4.3639e-01, Meta loss averaged over last 500 steps = 4.9336e-01, PNorm = 51.3788, GNorm = 0.0928
Meta loss on this task batch = 5.1521e-01, Meta loss averaged over last 500 steps = 4.9366e-01, PNorm = 51.3831, GNorm = 0.1647
Meta loss on this task batch = 5.7900e-01, Meta loss averaged over last 500 steps = 4.9480e-01, PNorm = 51.3865, GNorm = 0.2056
Meta loss on this task batch = 6.1441e-01, Meta loss averaged over last 500 steps = 4.9637e-01, PNorm = 51.3878, GNorm = 0.2384
Took 130.33145880699158 seconds to complete one epoch of meta training
Took 138.59901332855225 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.329227
Epoch 4
Meta loss on this task batch = 4.6319e-01, Meta loss averaged over last 500 steps = 4.9594e-01, PNorm = 51.3891, GNorm = 0.0918
Meta loss on this task batch = 4.7681e-01, Meta loss averaged over last 500 steps = 4.9569e-01, PNorm = 51.3910, GNorm = 0.0896
Meta loss on this task batch = 4.7104e-01, Meta loss averaged over last 500 steps = 4.9538e-01, PNorm = 51.3944, GNorm = 0.1690
Meta loss on this task batch = 4.5547e-01, Meta loss averaged over last 500 steps = 4.9488e-01, PNorm = 51.3975, GNorm = 0.0955
Meta loss on this task batch = 4.9062e-01, Meta loss averaged over last 500 steps = 4.9483e-01, PNorm = 51.4005, GNorm = 0.0904
Meta loss on this task batch = 4.2756e-01, Meta loss averaged over last 500 steps = 4.9401e-01, PNorm = 51.4045, GNorm = 0.1129
Meta loss on this task batch = 4.3837e-01, Meta loss averaged over last 500 steps = 4.9334e-01, PNorm = 51.4082, GNorm = 0.0971
Meta loss on this task batch = 4.6513e-01, Meta loss averaged over last 500 steps = 4.9300e-01, PNorm = 51.4118, GNorm = 0.0887
Meta loss on this task batch = 3.9694e-01, Meta loss averaged over last 500 steps = 4.9187e-01, PNorm = 51.4161, GNorm = 0.0918
Meta loss on this task batch = 4.2149e-01, Meta loss averaged over last 500 steps = 4.9106e-01, PNorm = 51.4197, GNorm = 0.0748
Meta loss on this task batch = 4.3952e-01, Meta loss averaged over last 500 steps = 4.9046e-01, PNorm = 51.4238, GNorm = 0.0793
Meta loss on this task batch = 4.7195e-01, Meta loss averaged over last 500 steps = 4.9025e-01, PNorm = 51.4269, GNorm = 0.1289
Meta loss on this task batch = 4.3268e-01, Meta loss averaged over last 500 steps = 4.8961e-01, PNorm = 51.4302, GNorm = 0.0963
Meta loss on this task batch = 5.5045e-01, Meta loss averaged over last 500 steps = 4.9028e-01, PNorm = 51.4328, GNorm = 0.1063
Meta loss on this task batch = 5.6404e-01, Meta loss averaged over last 500 steps = 4.9109e-01, PNorm = 51.4341, GNorm = 0.1850
Meta loss on this task batch = 5.5855e-01, Meta loss averaged over last 500 steps = 4.9183e-01, PNorm = 51.4363, GNorm = 0.0919
Meta loss on this task batch = 4.7464e-01, Meta loss averaged over last 500 steps = 4.9164e-01, PNorm = 51.4389, GNorm = 0.1136
Meta loss on this task batch = 5.1754e-01, Meta loss averaged over last 500 steps = 4.9192e-01, PNorm = 51.4442, GNorm = 0.3136
Meta loss on this task batch = 5.3669e-01, Meta loss averaged over last 500 steps = 4.9239e-01, PNorm = 51.4495, GNorm = 0.1115
Took 130.6507658958435 seconds to complete one epoch of meta training
Took 138.72904300689697 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.326094
Epoch 5
Meta loss on this task batch = 4.7559e-01, Meta loss averaged over last 500 steps = 4.9221e-01, PNorm = 51.4548, GNorm = 0.0865
Meta loss on this task batch = 5.0933e-01, Meta loss averaged over last 500 steps = 4.9239e-01, PNorm = 51.4592, GNorm = 0.1096
Meta loss on this task batch = 5.0782e-01, Meta loss averaged over last 500 steps = 4.9255e-01, PNorm = 51.4626, GNorm = 0.1214
Meta loss on this task batch = 5.0362e-01, Meta loss averaged over last 500 steps = 4.9266e-01, PNorm = 51.4640, GNorm = 0.1914
Meta loss on this task batch = 4.6167e-01, Meta loss averaged over last 500 steps = 4.9235e-01, PNorm = 51.4660, GNorm = 0.0835
Meta loss on this task batch = 5.0220e-01, Meta loss averaged over last 500 steps = 4.9245e-01, PNorm = 51.4687, GNorm = 0.0899
Meta loss on this task batch = 4.6020e-01, Meta loss averaged over last 500 steps = 4.9213e-01, PNorm = 51.4721, GNorm = 0.1306
Meta loss on this task batch = 4.6160e-01, Meta loss averaged over last 500 steps = 4.9183e-01, PNorm = 51.4758, GNorm = 0.0866
Meta loss on this task batch = 4.8976e-01, Meta loss averaged over last 500 steps = 4.9181e-01, PNorm = 51.4796, GNorm = 0.0869
Meta loss on this task batch = 4.5887e-01, Meta loss averaged over last 500 steps = 4.9150e-01, PNorm = 51.4829, GNorm = 0.0936
Meta loss on this task batch = 5.3130e-01, Meta loss averaged over last 500 steps = 4.9188e-01, PNorm = 51.4857, GNorm = 0.1243
Meta loss on this task batch = 5.0465e-01, Meta loss averaged over last 500 steps = 4.9199e-01, PNorm = 51.4890, GNorm = 0.0971
Meta loss on this task batch = 5.0080e-01, Meta loss averaged over last 500 steps = 4.9208e-01, PNorm = 51.4926, GNorm = 0.0911
Meta loss on this task batch = 3.9710e-01, Meta loss averaged over last 500 steps = 4.9120e-01, PNorm = 51.4958, GNorm = 0.0814
Meta loss on this task batch = 4.9795e-01, Meta loss averaged over last 500 steps = 4.9127e-01, PNorm = 51.4989, GNorm = 0.1044
Meta loss on this task batch = 3.6872e-01, Meta loss averaged over last 500 steps = 4.9016e-01, PNorm = 51.5048, GNorm = 0.2500
Meta loss on this task batch = 4.7536e-01, Meta loss averaged over last 500 steps = 4.9003e-01, PNorm = 51.5110, GNorm = 0.0882
Meta loss on this task batch = 5.1880e-01, Meta loss averaged over last 500 steps = 4.9028e-01, PNorm = 51.5158, GNorm = 0.1493
Meta loss on this task batch = 4.1911e-01, Meta loss averaged over last 500 steps = 4.8966e-01, PNorm = 51.5209, GNorm = 0.0953
Took 130.9586660861969 seconds to complete one epoch of meta training
Took 139.55225944519043 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.353185
Found better MAML checkpoint after meta validation, saving now
Epoch 6
Meta loss on this task batch = 4.7258e-01, Meta loss averaged over last 500 steps = 4.8951e-01, PNorm = 51.5247, GNorm = 0.1450
Meta loss on this task batch = 5.0173e-01, Meta loss averaged over last 500 steps = 4.8962e-01, PNorm = 51.5271, GNorm = 0.2058
Meta loss on this task batch = 4.5008e-01, Meta loss averaged over last 500 steps = 4.8928e-01, PNorm = 51.5296, GNorm = 0.1137
Meta loss on this task batch = 4.7148e-01, Meta loss averaged over last 500 steps = 4.8913e-01, PNorm = 51.5335, GNorm = 0.1042
Meta loss on this task batch = 5.2762e-01, Meta loss averaged over last 500 steps = 4.8945e-01, PNorm = 51.5374, GNorm = 0.0782
Meta loss on this task batch = 4.4815e-01, Meta loss averaged over last 500 steps = 4.8911e-01, PNorm = 51.5421, GNorm = 0.0887
Meta loss on this task batch = 4.3765e-01, Meta loss averaged over last 500 steps = 4.8868e-01, PNorm = 51.5488, GNorm = 0.1516
Meta loss on this task batch = 4.8116e-01, Meta loss averaged over last 500 steps = 4.8862e-01, PNorm = 51.5568, GNorm = 0.1484
Meta loss on this task batch = 5.1596e-01, Meta loss averaged over last 500 steps = 4.8884e-01, PNorm = 51.5653, GNorm = 0.1004
Meta loss on this task batch = 4.8196e-01, Meta loss averaged over last 500 steps = 4.8879e-01, PNorm = 51.5708, GNorm = 0.1796
Meta loss on this task batch = 4.5897e-01, Meta loss averaged over last 500 steps = 4.8855e-01, PNorm = 51.5732, GNorm = 0.2301
Meta loss on this task batch = 4.8150e-01, Meta loss averaged over last 500 steps = 4.8849e-01, PNorm = 51.5755, GNorm = 0.0995
Meta loss on this task batch = 4.3403e-01, Meta loss averaged over last 500 steps = 4.8806e-01, PNorm = 51.5788, GNorm = 0.0882
Meta loss on this task batch = 5.1362e-01, Meta loss averaged over last 500 steps = 4.8826e-01, PNorm = 51.5820, GNorm = 0.0797
Meta loss on this task batch = 4.3090e-01, Meta loss averaged over last 500 steps = 4.8782e-01, PNorm = 51.5856, GNorm = 0.0766
Meta loss on this task batch = 4.4375e-01, Meta loss averaged over last 500 steps = 4.8748e-01, PNorm = 51.5914, GNorm = 0.1695
Meta loss on this task batch = 4.7118e-01, Meta loss averaged over last 500 steps = 4.8736e-01, PNorm = 51.5979, GNorm = 0.1040
Meta loss on this task batch = 5.1472e-01, Meta loss averaged over last 500 steps = 4.8756e-01, PNorm = 51.6048, GNorm = 0.0922
Meta loss on this task batch = 4.8823e-01, Meta loss averaged over last 500 steps = 4.8757e-01, PNorm = 51.6112, GNorm = 0.1100
Took 132.3993694782257 seconds to complete one epoch of meta training
Took 140.79300165176392 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.352379
Epoch 7
Meta loss on this task batch = 4.5481e-01, Meta loss averaged over last 500 steps = 4.8732e-01, PNorm = 51.6162, GNorm = 0.0953
Meta loss on this task batch = 4.5359e-01, Meta loss averaged over last 500 steps = 4.8707e-01, PNorm = 51.6220, GNorm = 0.0790
Meta loss on this task batch = 4.9393e-01, Meta loss averaged over last 500 steps = 4.8712e-01, PNorm = 51.6266, GNorm = 0.1051
Meta loss on this task batch = 4.6035e-01, Meta loss averaged over last 500 steps = 4.8693e-01, PNorm = 51.6304, GNorm = 0.0827
Meta loss on this task batch = 4.2956e-01, Meta loss averaged over last 500 steps = 4.8651e-01, PNorm = 51.6348, GNorm = 0.0863
Meta loss on this task batch = 4.5688e-01, Meta loss averaged over last 500 steps = 4.8630e-01, PNorm = 51.6385, GNorm = 0.0878
Meta loss on this task batch = 4.9710e-01, Meta loss averaged over last 500 steps = 4.8638e-01, PNorm = 51.6419, GNorm = 0.1085
Meta loss on this task batch = 4.0313e-01, Meta loss averaged over last 500 steps = 4.8579e-01, PNorm = 51.6450, GNorm = 0.0769
Meta loss on this task batch = 4.2128e-01, Meta loss averaged over last 500 steps = 4.8533e-01, PNorm = 51.6482, GNorm = 0.0764
Meta loss on this task batch = 5.6145e-01, Meta loss averaged over last 500 steps = 4.8586e-01, PNorm = 51.6516, GNorm = 0.0733
Meta loss on this task batch = 4.8400e-01, Meta loss averaged over last 500 steps = 4.8585e-01, PNorm = 51.6563, GNorm = 0.0975
Meta loss on this task batch = 4.5949e-01, Meta loss averaged over last 500 steps = 4.8567e-01, PNorm = 51.6628, GNorm = 0.1328
Meta loss on this task batch = 5.2914e-01, Meta loss averaged over last 500 steps = 4.8597e-01, PNorm = 51.6694, GNorm = 0.0742
Meta loss on this task batch = 3.8356e-01, Meta loss averaged over last 500 steps = 4.8527e-01, PNorm = 51.6770, GNorm = 0.0857
Meta loss on this task batch = 5.7182e-01, Meta loss averaged over last 500 steps = 4.8586e-01, PNorm = 51.6837, GNorm = 0.1361
Meta loss on this task batch = 4.6153e-01, Meta loss averaged over last 500 steps = 4.8569e-01, PNorm = 51.6902, GNorm = 0.0850
Meta loss on this task batch = 5.2155e-01, Meta loss averaged over last 500 steps = 4.8593e-01, PNorm = 51.6963, GNorm = 0.1049
Meta loss on this task batch = 4.4382e-01, Meta loss averaged over last 500 steps = 4.8565e-01, PNorm = 51.7016, GNorm = 0.0973
Meta loss on this task batch = 5.0852e-01, Meta loss averaged over last 500 steps = 4.8580e-01, PNorm = 51.7067, GNorm = 0.1044
Took 134.15558433532715 seconds to complete one epoch of meta training
Took 142.54520344734192 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.363205
Found better MAML checkpoint after meta validation, saving now
Epoch 8
Meta loss on this task batch = 4.5047e-01, Meta loss averaged over last 500 steps = 4.8557e-01, PNorm = 51.7115, GNorm = 0.0760
Meta loss on this task batch = 5.2811e-01, Meta loss averaged over last 500 steps = 4.8585e-01, PNorm = 51.7156, GNorm = 0.0855
Meta loss on this task batch = 4.7015e-01, Meta loss averaged over last 500 steps = 4.8575e-01, PNorm = 51.7202, GNorm = 0.0997
Meta loss on this task batch = 4.3327e-01, Meta loss averaged over last 500 steps = 4.8541e-01, PNorm = 51.7263, GNorm = 0.1410
Meta loss on this task batch = 5.1240e-01, Meta loss averaged over last 500 steps = 4.8558e-01, PNorm = 51.7319, GNorm = 0.0820
Meta loss on this task batch = 4.6074e-01, Meta loss averaged over last 500 steps = 4.8543e-01, PNorm = 51.7369, GNorm = 0.0986
Meta loss on this task batch = 4.3277e-01, Meta loss averaged over last 500 steps = 4.8509e-01, PNorm = 51.7403, GNorm = 0.1383
Meta loss on this task batch = 5.4721e-01, Meta loss averaged over last 500 steps = 4.8548e-01, PNorm = 51.7431, GNorm = 0.1549
Meta loss on this task batch = 4.8662e-01, Meta loss averaged over last 500 steps = 4.8549e-01, PNorm = 51.7453, GNorm = 0.1112
Meta loss on this task batch = 4.8038e-01, Meta loss averaged over last 500 steps = 4.8546e-01, PNorm = 51.7480, GNorm = 0.0813
Meta loss on this task batch = 5.0813e-01, Meta loss averaged over last 500 steps = 4.8560e-01, PNorm = 51.7534, GNorm = 0.2349
Meta loss on this task batch = 4.2268e-01, Meta loss averaged over last 500 steps = 4.8521e-01, PNorm = 51.7612, GNorm = 0.2140
Meta loss on this task batch = 5.0694e-01, Meta loss averaged over last 500 steps = 4.8534e-01, PNorm = 51.7687, GNorm = 0.0733
Meta loss on this task batch = 4.0456e-01, Meta loss averaged over last 500 steps = 4.8486e-01, PNorm = 51.7763, GNorm = 0.0764
Meta loss on this task batch = 4.9909e-01, Meta loss averaged over last 500 steps = 4.8494e-01, PNorm = 51.7837, GNorm = 0.0854
Meta loss on this task batch = 4.4367e-01, Meta loss averaged over last 500 steps = 4.8470e-01, PNorm = 51.7889, GNorm = 0.1865
Meta loss on this task batch = 4.5946e-01, Meta loss averaged over last 500 steps = 4.8455e-01, PNorm = 51.7923, GNorm = 0.1843
Meta loss on this task batch = 5.2251e-01, Meta loss averaged over last 500 steps = 4.8477e-01, PNorm = 51.7943, GNorm = 0.1700
Meta loss on this task batch = 4.3281e-01, Meta loss averaged over last 500 steps = 4.8447e-01, PNorm = 51.7981, GNorm = 0.1192
Took 133.31919956207275 seconds to complete one epoch of meta training
Took 142.00907135009766 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.366749
Found better MAML checkpoint after meta validation, saving now
Epoch 9
Meta loss on this task batch = 4.6790e-01, Meta loss averaged over last 500 steps = 4.8437e-01, PNorm = 51.8032, GNorm = 0.1370
Meta loss on this task batch = 5.0178e-01, Meta loss averaged over last 500 steps = 4.8447e-01, PNorm = 51.8097, GNorm = 0.1170
Meta loss on this task batch = 4.9341e-01, Meta loss averaged over last 500 steps = 4.8452e-01, PNorm = 51.8175, GNorm = 0.1122
Meta loss on this task batch = 4.6715e-01, Meta loss averaged over last 500 steps = 4.8442e-01, PNorm = 51.8261, GNorm = 0.1386
Meta loss on this task batch = 4.9678e-01, Meta loss averaged over last 500 steps = 4.8449e-01, PNorm = 51.8328, GNorm = 0.1525
Meta loss on this task batch = 4.8542e-01, Meta loss averaged over last 500 steps = 4.8450e-01, PNorm = 51.8396, GNorm = 0.0971
Meta loss on this task batch = 4.3850e-01, Meta loss averaged over last 500 steps = 4.8424e-01, PNorm = 51.8460, GNorm = 0.0854
Meta loss on this task batch = 5.1131e-01, Meta loss averaged over last 500 steps = 4.8439e-01, PNorm = 51.8525, GNorm = 0.1967
Meta loss on this task batch = 4.1231e-01, Meta loss averaged over last 500 steps = 4.8399e-01, PNorm = 51.8578, GNorm = 0.0903
Meta loss on this task batch = 5.2547e-01, Meta loss averaged over last 500 steps = 4.8422e-01, PNorm = 51.8620, GNorm = 0.1743
Meta loss on this task batch = 5.1343e-01, Meta loss averaged over last 500 steps = 4.8438e-01, PNorm = 51.8677, GNorm = 0.1169
Meta loss on this task batch = 4.5832e-01, Meta loss averaged over last 500 steps = 4.8424e-01, PNorm = 51.8747, GNorm = 0.1043
Meta loss on this task batch = 4.7261e-01, Meta loss averaged over last 500 steps = 4.8418e-01, PNorm = 51.8834, GNorm = 0.1903
Meta loss on this task batch = 4.2505e-01, Meta loss averaged over last 500 steps = 4.8386e-01, PNorm = 51.8939, GNorm = 0.1697
Meta loss on this task batch = 4.3690e-01, Meta loss averaged over last 500 steps = 4.8360e-01, PNorm = 51.9049, GNorm = 0.0918
Meta loss on this task batch = 4.7996e-01, Meta loss averaged over last 500 steps = 4.8358e-01, PNorm = 51.9141, GNorm = 0.1282
Meta loss on this task batch = 4.7746e-01, Meta loss averaged over last 500 steps = 4.8355e-01, PNorm = 51.9209, GNorm = 0.1560
Meta loss on this task batch = 4.3556e-01, Meta loss averaged over last 500 steps = 4.8330e-01, PNorm = 51.9270, GNorm = 0.1053
Meta loss on this task batch = 4.3768e-01, Meta loss averaged over last 500 steps = 4.8306e-01, PNorm = 51.9307, GNorm = 0.1373
Took 131.80545616149902 seconds to complete one epoch of meta training
Took 139.40714049339294 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.369478
Found better MAML checkpoint after meta validation, saving now
Epoch 10
Meta loss on this task batch = 4.5348e-01, Meta loss averaged over last 500 steps = 4.8290e-01, PNorm = 51.9348, GNorm = 0.0999
Meta loss on this task batch = 5.0304e-01, Meta loss averaged over last 500 steps = 4.8301e-01, PNorm = 51.9399, GNorm = 0.0825
Meta loss on this task batch = 4.3105e-01, Meta loss averaged over last 500 steps = 4.8274e-01, PNorm = 51.9466, GNorm = 0.1176
Meta loss on this task batch = 4.3437e-01, Meta loss averaged over last 500 steps = 4.8249e-01, PNorm = 51.9546, GNorm = 0.0944
Meta loss on this task batch = 4.9452e-01, Meta loss averaged over last 500 steps = 4.8255e-01, PNorm = 51.9627, GNorm = 0.0850
Meta loss on this task batch = 4.7403e-01, Meta loss averaged over last 500 steps = 4.8251e-01, PNorm = 51.9714, GNorm = 0.1048
Meta loss on this task batch = 3.9553e-01, Meta loss averaged over last 500 steps = 4.8207e-01, PNorm = 51.9801, GNorm = 0.0943
Meta loss on this task batch = 4.8703e-01, Meta loss averaged over last 500 steps = 4.8209e-01, PNorm = 51.9889, GNorm = 0.1266
Meta loss on this task batch = 4.6367e-01, Meta loss averaged over last 500 steps = 4.8200e-01, PNorm = 51.9956, GNorm = 0.1624
Meta loss on this task batch = 4.4185e-01, Meta loss averaged over last 500 steps = 4.8180e-01, PNorm = 52.0018, GNorm = 0.0799
Meta loss on this task batch = 5.1900e-01, Meta loss averaged over last 500 steps = 4.8198e-01, PNorm = 52.0059, GNorm = 0.1942
Meta loss on this task batch = 4.0536e-01, Meta loss averaged over last 500 steps = 4.8160e-01, PNorm = 52.0102, GNorm = 0.1042
Meta loss on this task batch = 5.0540e-01, Meta loss averaged over last 500 steps = 4.8172e-01, PNorm = 52.0137, GNorm = 0.1320
Meta loss on this task batch = 4.8691e-01, Meta loss averaged over last 500 steps = 4.8175e-01, PNorm = 52.0176, GNorm = 0.0849
Meta loss on this task batch = 4.2038e-01, Meta loss averaged over last 500 steps = 4.8145e-01, PNorm = 52.0237, GNorm = 0.1609
Meta loss on this task batch = 5.3629e-01, Meta loss averaged over last 500 steps = 4.8171e-01, PNorm = 52.0294, GNorm = 0.0851
Meta loss on this task batch = 4.7652e-01, Meta loss averaged over last 500 steps = 4.8169e-01, PNorm = 52.0363, GNorm = 0.1494
Meta loss on this task batch = 4.8801e-01, Meta loss averaged over last 500 steps = 4.8172e-01, PNorm = 52.0430, GNorm = 0.0806
Meta loss on this task batch = 5.0411e-01, Meta loss averaged over last 500 steps = 4.8183e-01, PNorm = 52.0504, GNorm = 0.1159
Took 132.86916208267212 seconds to complete one epoch of meta training
Took 141.09338235855103 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.356760
Epoch 11
Meta loss on this task batch = 4.6077e-01, Meta loss averaged over last 500 steps = 4.8173e-01, PNorm = 52.0582, GNorm = 0.0846
Meta loss on this task batch = 4.7070e-01, Meta loss averaged over last 500 steps = 4.8167e-01, PNorm = 52.0655, GNorm = 0.1017
Meta loss on this task batch = 4.4608e-01, Meta loss averaged over last 500 steps = 4.8151e-01, PNorm = 52.0729, GNorm = 0.0941
Meta loss on this task batch = 4.9590e-01, Meta loss averaged over last 500 steps = 4.8157e-01, PNorm = 52.0798, GNorm = 0.1029
Meta loss on this task batch = 5.0503e-01, Meta loss averaged over last 500 steps = 4.8168e-01, PNorm = 52.0848, GNorm = 0.1486
Meta loss on this task batch = 4.1614e-01, Meta loss averaged over last 500 steps = 4.8138e-01, PNorm = 52.0891, GNorm = 0.0975
Meta loss on this task batch = 4.4415e-01, Meta loss averaged over last 500 steps = 4.8121e-01, PNorm = 52.0938, GNorm = 0.0946
Meta loss on this task batch = 5.1290e-01, Meta loss averaged over last 500 steps = 4.8135e-01, PNorm = 52.0953, GNorm = 0.3239
Meta loss on this task batch = 4.3570e-01, Meta loss averaged over last 500 steps = 4.8114e-01, PNorm = 52.0980, GNorm = 0.0858
Meta loss on this task batch = 4.9660e-01, Meta loss averaged over last 500 steps = 4.8121e-01, PNorm = 52.1031, GNorm = 0.1818
Meta loss on this task batch = 4.4720e-01, Meta loss averaged over last 500 steps = 4.8106e-01, PNorm = 52.1104, GNorm = 0.2166
Meta loss on this task batch = 5.1303e-01, Meta loss averaged over last 500 steps = 4.8120e-01, PNorm = 52.1189, GNorm = 0.1322
Meta loss on this task batch = 4.6431e-01, Meta loss averaged over last 500 steps = 4.8113e-01, PNorm = 52.1272, GNorm = 0.1041
Meta loss on this task batch = 4.9879e-01, Meta loss averaged over last 500 steps = 4.8121e-01, PNorm = 52.1358, GNorm = 0.0867
Meta loss on this task batch = 4.6319e-01, Meta loss averaged over last 500 steps = 4.8113e-01, PNorm = 52.1431, GNorm = 0.1179
Meta loss on this task batch = 4.4014e-01, Meta loss averaged over last 500 steps = 4.8094e-01, PNorm = 52.1497, GNorm = 0.0848
Meta loss on this task batch = 4.6340e-01, Meta loss averaged over last 500 steps = 4.8087e-01, PNorm = 52.1558, GNorm = 0.1268
Meta loss on this task batch = 4.4370e-01, Meta loss averaged over last 500 steps = 4.8070e-01, PNorm = 52.1609, GNorm = 0.1583
Meta loss on this task batch = 4.6860e-01, Meta loss averaged over last 500 steps = 4.8065e-01, PNorm = 52.1667, GNorm = 0.1063
Took 139.00668835639954 seconds to complete one epoch of meta training
Took 148.18053555488586 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.365989
Epoch 12
Meta loss on this task batch = 4.8973e-01, Meta loss averaged over last 500 steps = 4.8069e-01, PNorm = 52.1716, GNorm = 0.1204
Meta loss on this task batch = 4.9948e-01, Meta loss averaged over last 500 steps = 4.8077e-01, PNorm = 52.1781, GNorm = 0.1317
Meta loss on this task batch = 4.8769e-01, Meta loss averaged over last 500 steps = 4.8080e-01, PNorm = 52.1862, GNorm = 0.1494
Meta loss on this task batch = 4.7005e-01, Meta loss averaged over last 500 steps = 4.8075e-01, PNorm = 52.1952, GNorm = 0.1106
Meta loss on this task batch = 4.1920e-01, Meta loss averaged over last 500 steps = 4.8049e-01, PNorm = 52.2046, GNorm = 0.0939
Meta loss on this task batch = 5.3847e-01, Meta loss averaged over last 500 steps = 4.8074e-01, PNorm = 52.2138, GNorm = 0.0924
Meta loss on this task batch = 5.4912e-01, Meta loss averaged over last 500 steps = 4.8103e-01, PNorm = 52.2219, GNorm = 0.1386
Meta loss on this task batch = 5.2330e-01, Meta loss averaged over last 500 steps = 4.8121e-01, PNorm = 52.2279, GNorm = 0.1832
Meta loss on this task batch = 4.0065e-01, Meta loss averaged over last 500 steps = 4.8087e-01, PNorm = 52.2337, GNorm = 0.1058
Meta loss on this task batch = 4.1620e-01, Meta loss averaged over last 500 steps = 4.8060e-01, PNorm = 52.2379, GNorm = 0.1686
Meta loss on this task batch = 4.5313e-01, Meta loss averaged over last 500 steps = 4.8048e-01, PNorm = 52.2436, GNorm = 0.1111
Meta loss on this task batch = 4.5363e-01, Meta loss averaged over last 500 steps = 4.8037e-01, PNorm = 52.2499, GNorm = 0.1343
Meta loss on this task batch = 4.7715e-01, Meta loss averaged over last 500 steps = 4.8036e-01, PNorm = 52.2573, GNorm = 0.0863
Meta loss on this task batch = 4.6041e-01, Meta loss averaged over last 500 steps = 4.8027e-01, PNorm = 52.2649, GNorm = 0.1054
Meta loss on this task batch = 3.5900e-01, Meta loss averaged over last 500 steps = 4.7977e-01, PNorm = 52.2717, GNorm = 0.0817
Meta loss on this task batch = 5.3268e-01, Meta loss averaged over last 500 steps = 4.7999e-01, PNorm = 52.2779, GNorm = 0.1313
Meta loss on this task batch = 4.8796e-01, Meta loss averaged over last 500 steps = 4.8002e-01, PNorm = 52.2828, GNorm = 0.1214
Meta loss on this task batch = 4.6274e-01, Meta loss averaged over last 500 steps = 4.7995e-01, PNorm = 52.2874, GNorm = 0.1113
Meta loss on this task batch = 4.0221e-01, Meta loss averaged over last 500 steps = 4.7964e-01, PNorm = 52.2941, GNorm = 0.1539
Took 138.21026372909546 seconds to complete one epoch of meta training
Took 146.80109071731567 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.362956
Epoch 13
Meta loss on this task batch = 4.4135e-01, Meta loss averaged over last 500 steps = 4.7948e-01, PNorm = 52.3018, GNorm = 0.0963
Meta loss on this task batch = 4.6980e-01, Meta loss averaged over last 500 steps = 4.7945e-01, PNorm = 52.3098, GNorm = 0.0897
Meta loss on this task batch = 4.1047e-01, Meta loss averaged over last 500 steps = 4.7917e-01, PNorm = 52.3199, GNorm = 0.1843
Meta loss on this task batch = 4.6692e-01, Meta loss averaged over last 500 steps = 4.7912e-01, PNorm = 52.3299, GNorm = 0.0794
Meta loss on this task batch = 4.6782e-01, Meta loss averaged over last 500 steps = 4.7908e-01, PNorm = 52.3391, GNorm = 0.1176
Meta loss on this task batch = 4.9793e-01, Meta loss averaged over last 500 steps = 4.7915e-01, PNorm = 52.3420, GNorm = 0.3042
Meta loss on this task batch = 4.3149e-01, Meta loss averaged over last 500 steps = 4.7896e-01, PNorm = 52.3444, GNorm = 0.0961
Meta loss on this task batch = 4.6064e-01, Meta loss averaged over last 500 steps = 4.7889e-01, PNorm = 52.3467, GNorm = 0.1121
Meta loss on this task batch = 4.7609e-01, Meta loss averaged over last 500 steps = 4.7888e-01, PNorm = 52.3513, GNorm = 0.1773
Meta loss on this task batch = 4.7269e-01, Meta loss averaged over last 500 steps = 4.7886e-01, PNorm = 52.3569, GNorm = 0.1088
Meta loss on this task batch = 5.0656e-01, Meta loss averaged over last 500 steps = 4.7896e-01, PNorm = 52.3637, GNorm = 0.1136
Meta loss on this task batch = 4.8502e-01, Meta loss averaged over last 500 steps = 4.7899e-01, PNorm = 52.3717, GNorm = 0.1093
Meta loss on this task batch = 4.6798e-01, Meta loss averaged over last 500 steps = 4.7894e-01, PNorm = 52.3811, GNorm = 0.1349
Meta loss on this task batch = 5.0309e-01, Meta loss averaged over last 500 steps = 4.7904e-01, PNorm = 52.3896, GNorm = 0.1418
Meta loss on this task batch = 4.4162e-01, Meta loss averaged over last 500 steps = 4.7889e-01, PNorm = 52.3978, GNorm = 0.1180
Meta loss on this task batch = 4.4355e-01, Meta loss averaged over last 500 steps = 4.7876e-01, PNorm = 52.4051, GNorm = 0.1260
Meta loss on this task batch = 5.1246e-01, Meta loss averaged over last 500 steps = 4.7889e-01, PNorm = 52.4115, GNorm = 0.1288
Meta loss on this task batch = 4.9434e-01, Meta loss averaged over last 500 steps = 4.7895e-01, PNorm = 52.4182, GNorm = 0.1520
Meta loss on this task batch = 4.3412e-01, Meta loss averaged over last 500 steps = 4.7878e-01, PNorm = 52.4257, GNorm = 0.1245
Took 133.73572492599487 seconds to complete one epoch of meta training
Took 142.51434898376465 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.372027
Found better MAML checkpoint after meta validation, saving now
Epoch 14
Meta loss on this task batch = 4.8405e-01, Meta loss averaged over last 500 steps = 4.7880e-01, PNorm = 52.4334, GNorm = 0.1032
Meta loss on this task batch = 4.9350e-01, Meta loss averaged over last 500 steps = 4.7885e-01, PNorm = 52.4409, GNorm = 0.1459
Meta loss on this task batch = 4.3711e-01, Meta loss averaged over last 500 steps = 4.7870e-01, PNorm = 52.4489, GNorm = 0.1009
Meta loss on this task batch = 4.1567e-01, Meta loss averaged over last 500 steps = 4.7846e-01, PNorm = 52.4571, GNorm = 0.0871
Meta loss on this task batch = 4.2295e-01, Meta loss averaged over last 500 steps = 4.7826e-01, PNorm = 52.4655, GNorm = 0.0969
Meta loss on this task batch = 4.4176e-01, Meta loss averaged over last 500 steps = 4.7812e-01, PNorm = 52.4740, GNorm = 0.0885
Meta loss on this task batch = 4.8341e-01, Meta loss averaged over last 500 steps = 4.7814e-01, PNorm = 52.4828, GNorm = 0.0911
Meta loss on this task batch = 4.8305e-01, Meta loss averaged over last 500 steps = 4.7816e-01, PNorm = 52.4927, GNorm = 0.1148
Meta loss on this task batch = 4.4515e-01, Meta loss averaged over last 500 steps = 4.7804e-01, PNorm = 52.5025, GNorm = 0.0993
Meta loss on this task batch = 5.0709e-01, Meta loss averaged over last 500 steps = 4.7815e-01, PNorm = 52.5115, GNorm = 0.0847
Meta loss on this task batch = 4.4886e-01, Meta loss averaged over last 500 steps = 4.7804e-01, PNorm = 52.5203, GNorm = 0.1079
Meta loss on this task batch = 4.0795e-01, Meta loss averaged over last 500 steps = 4.7779e-01, PNorm = 52.5289, GNorm = 0.0924
Meta loss on this task batch = 4.4569e-01, Meta loss averaged over last 500 steps = 4.7767e-01, PNorm = 52.5366, GNorm = 0.1144
Meta loss on this task batch = 4.8577e-01, Meta loss averaged over last 500 steps = 4.7770e-01, PNorm = 52.5423, GNorm = 0.1508
Meta loss on this task batch = 4.6732e-01, Meta loss averaged over last 500 steps = 4.7767e-01, PNorm = 52.5497, GNorm = 0.1560
Meta loss on this task batch = 5.1541e-01, Meta loss averaged over last 500 steps = 4.7780e-01, PNorm = 52.5569, GNorm = 0.0970
Meta loss on this task batch = 4.6458e-01, Meta loss averaged over last 500 steps = 4.7775e-01, PNorm = 52.5624, GNorm = 0.1468
Meta loss on this task batch = 4.5113e-01, Meta loss averaged over last 500 steps = 4.7766e-01, PNorm = 52.5682, GNorm = 0.1109
Meta loss on this task batch = 5.0159e-01, Meta loss averaged over last 500 steps = 4.7774e-01, PNorm = 52.5735, GNorm = 0.1420
Took 136.33532452583313 seconds to complete one epoch of meta training
Took 144.64205765724182 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.387767
Found better MAML checkpoint after meta validation, saving now
Epoch 15
Meta loss on this task batch = 4.5092e-01, Meta loss averaged over last 500 steps = 4.7765e-01, PNorm = 52.5796, GNorm = 0.0890
Meta loss on this task batch = 4.7159e-01, Meta loss averaged over last 500 steps = 4.7763e-01, PNorm = 52.5865, GNorm = 0.0886
Meta loss on this task batch = 4.3531e-01, Meta loss averaged over last 500 steps = 4.7748e-01, PNorm = 52.5947, GNorm = 0.0962
Meta loss on this task batch = 4.1593e-01, Meta loss averaged over last 500 steps = 4.7727e-01, PNorm = 52.6054, GNorm = 0.1445
Meta loss on this task batch = 4.8422e-01, Meta loss averaged over last 500 steps = 4.7729e-01, PNorm = 52.6163, GNorm = 0.1165
Meta loss on this task batch = 4.6569e-01, Meta loss averaged over last 500 steps = 4.7725e-01, PNorm = 52.6265, GNorm = 0.1208
Meta loss on this task batch = 5.1861e-01, Meta loss averaged over last 500 steps = 4.7739e-01, PNorm = 52.6345, GNorm = 0.2092
Meta loss on this task batch = 4.9560e-01, Meta loss averaged over last 500 steps = 4.7746e-01, PNorm = 52.6422, GNorm = 0.1249
Meta loss on this task batch = 4.8689e-01, Meta loss averaged over last 500 steps = 4.7749e-01, PNorm = 52.6488, GNorm = 0.1286
Meta loss on this task batch = 4.3496e-01, Meta loss averaged over last 500 steps = 4.7734e-01, PNorm = 52.6563, GNorm = 0.0916
Meta loss on this task batch = 4.4658e-01, Meta loss averaged over last 500 steps = 4.7724e-01, PNorm = 52.6647, GNorm = 0.1078
Meta loss on this task batch = 4.8722e-01, Meta loss averaged over last 500 steps = 4.7727e-01, PNorm = 52.6764, GNorm = 0.2478
Meta loss on this task batch = 3.9771e-01, Meta loss averaged over last 500 steps = 4.7701e-01, PNorm = 52.6893, GNorm = 0.1289
Meta loss on this task batch = 4.6522e-01, Meta loss averaged over last 500 steps = 4.7697e-01, PNorm = 52.7019, GNorm = 0.1286
Meta loss on this task batch = 4.3940e-01, Meta loss averaged over last 500 steps = 4.7684e-01, PNorm = 52.7138, GNorm = 0.0968
Meta loss on this task batch = 4.2029e-01, Meta loss averaged over last 500 steps = 4.7665e-01, PNorm = 52.7242, GNorm = 0.1102
Meta loss on this task batch = 4.5240e-01, Meta loss averaged over last 500 steps = 4.7657e-01, PNorm = 52.7310, GNorm = 0.2538
Meta loss on this task batch = 4.9409e-01, Meta loss averaged over last 500 steps = 4.7663e-01, PNorm = 52.7356, GNorm = 0.1619
Meta loss on this task batch = 5.5853e-01, Meta loss averaged over last 500 steps = 4.7690e-01, PNorm = 52.7381, GNorm = 0.1555
Took 131.72868990898132 seconds to complete one epoch of meta training
Took 140.253808259964 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.382302
Epoch 16
Meta loss on this task batch = 4.4912e-01, Meta loss averaged over last 500 steps = 4.7681e-01, PNorm = 52.7435, GNorm = 0.1273
Meta loss on this task batch = 4.1932e-01, Meta loss averaged over last 500 steps = 4.7662e-01, PNorm = 52.7529, GNorm = 0.1767
Meta loss on this task batch = 4.2003e-01, Meta loss averaged over last 500 steps = 4.7644e-01, PNorm = 52.7640, GNorm = 0.1419
Meta loss on this task batch = 4.4025e-01, Meta loss averaged over last 500 steps = 4.7632e-01, PNorm = 52.7765, GNorm = 0.1269
Meta loss on this task batch = 4.5930e-01, Meta loss averaged over last 500 steps = 4.7627e-01, PNorm = 52.7891, GNorm = 0.0918
Meta loss on this task batch = 4.5113e-01, Meta loss averaged over last 500 steps = 4.7618e-01, PNorm = 52.8003, GNorm = 0.0871
Meta loss on this task batch = 5.2069e-01, Meta loss averaged over last 500 steps = 4.7633e-01, PNorm = 52.8090, GNorm = 0.1562
Meta loss on this task batch = 4.7830e-01, Meta loss averaged over last 500 steps = 4.7633e-01, PNorm = 52.8163, GNorm = 0.1580
Meta loss on this task batch = 4.6487e-01, Meta loss averaged over last 500 steps = 4.7630e-01, PNorm = 52.8237, GNorm = 0.1241
Meta loss on this task batch = 4.9963e-01, Meta loss averaged over last 500 steps = 4.7637e-01, PNorm = 52.8284, GNorm = 0.1838
Meta loss on this task batch = 4.8170e-01, Meta loss averaged over last 500 steps = 4.7639e-01, PNorm = 52.8344, GNorm = 0.1091
Meta loss on this task batch = 4.3374e-01, Meta loss averaged over last 500 steps = 4.7625e-01, PNorm = 52.8408, GNorm = 0.1119
Meta loss on this task batch = 5.0042e-01, Meta loss averaged over last 500 steps = 4.7633e-01, PNorm = 52.8485, GNorm = 0.1389
Meta loss on this task batch = 4.7961e-01, Meta loss averaged over last 500 steps = 4.7634e-01, PNorm = 52.8573, GNorm = 0.0999
Meta loss on this task batch = 5.0202e-01, Meta loss averaged over last 500 steps = 4.7642e-01, PNorm = 52.8665, GNorm = 0.1504
Meta loss on this task batch = 3.7270e-01, Meta loss averaged over last 500 steps = 4.7610e-01, PNorm = 52.8755, GNorm = 0.0927
Meta loss on this task batch = 5.0227e-01, Meta loss averaged over last 500 steps = 4.7618e-01, PNorm = 52.8833, GNorm = 0.1484
Meta loss on this task batch = 4.8082e-01, Meta loss averaged over last 500 steps = 4.7619e-01, PNorm = 52.8912, GNorm = 0.2401
Meta loss on this task batch = 4.6852e-01, Meta loss averaged over last 500 steps = 4.7617e-01, PNorm = 52.8990, GNorm = 0.1248
Took 130.70877623558044 seconds to complete one epoch of meta training
Took 138.0767846107483 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.409696
Found better MAML checkpoint after meta validation, saving now
Epoch 17
Meta loss on this task batch = 3.9513e-01, Meta loss averaged over last 500 steps = 4.7592e-01, PNorm = 52.9074, GNorm = 0.0963
Meta loss on this task batch = 4.5614e-01, Meta loss averaged over last 500 steps = 4.7586e-01, PNorm = 52.9169, GNorm = 0.1163
Meta loss on this task batch = 5.0819e-01, Meta loss averaged over last 500 steps = 4.7596e-01, PNorm = 52.9259, GNorm = 0.1320
Meta loss on this task batch = 4.3760e-01, Meta loss averaged over last 500 steps = 4.7584e-01, PNorm = 52.9357, GNorm = 0.1007
Meta loss on this task batch = 4.7930e-01, Meta loss averaged over last 500 steps = 4.7585e-01, PNorm = 52.9460, GNorm = 0.0975
Meta loss on this task batch = 4.1869e-01, Meta loss averaged over last 500 steps = 4.7568e-01, PNorm = 52.9568, GNorm = 0.0978
Meta loss on this task batch = 5.1247e-01, Meta loss averaged over last 500 steps = 4.7579e-01, PNorm = 52.9661, GNorm = 0.1076
Meta loss on this task batch = 4.9216e-01, Meta loss averaged over last 500 steps = 4.7584e-01, PNorm = 52.9738, GNorm = 0.1363
Meta loss on this task batch = 4.8651e-01, Meta loss averaged over last 500 steps = 4.7587e-01, PNorm = 52.9809, GNorm = 0.1211
Meta loss on this task batch = 5.2746e-01, Meta loss averaged over last 500 steps = 4.7602e-01, PNorm = 52.9883, GNorm = 0.1022
Meta loss on this task batch = 4.0976e-01, Meta loss averaged over last 500 steps = 4.7583e-01, PNorm = 52.9956, GNorm = 0.1117
Meta loss on this task batch = 4.5214e-01, Meta loss averaged over last 500 steps = 4.7575e-01, PNorm = 53.0032, GNorm = 0.1337
Meta loss on this task batch = 3.7427e-01, Meta loss averaged over last 500 steps = 4.7545e-01, PNorm = 53.0117, GNorm = 0.1329
Meta loss on this task batch = 4.7708e-01, Meta loss averaged over last 500 steps = 4.7546e-01, PNorm = 53.0210, GNorm = 0.1148
Meta loss on this task batch = 4.7695e-01, Meta loss averaged over last 500 steps = 4.7546e-01, PNorm = 53.0280, GNorm = 0.1752
Meta loss on this task batch = 3.8220e-01, Meta loss averaged over last 500 steps = 4.7519e-01, PNorm = 53.0346, GNorm = 0.1126
Meta loss on this task batch = 5.0230e-01, Meta loss averaged over last 500 steps = 4.7527e-01, PNorm = 53.0417, GNorm = 0.1410
Meta loss on this task batch = 4.9233e-01, Meta loss averaged over last 500 steps = 4.7532e-01, PNorm = 53.0486, GNorm = 0.1288
Meta loss on this task batch = 4.9314e-01, Meta loss averaged over last 500 steps = 4.7537e-01, PNorm = 53.0560, GNorm = 0.1523
Took 137.20274782180786 seconds to complete one epoch of meta training
Took 146.22376227378845 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.387503
Epoch 18
Meta loss on this task batch = 4.7426e-01, Meta loss averaged over last 500 steps = 4.7537e-01, PNorm = 53.0642, GNorm = 0.1117
Meta loss on this task batch = 4.6005e-01, Meta loss averaged over last 500 steps = 4.7532e-01, PNorm = 53.0732, GNorm = 0.1036
Meta loss on this task batch = 4.7148e-01, Meta loss averaged over last 500 steps = 4.7531e-01, PNorm = 53.0827, GNorm = 0.1413
Meta loss on this task batch = 4.3603e-01, Meta loss averaged over last 500 steps = 4.7520e-01, PNorm = 53.0928, GNorm = 0.0849
Meta loss on this task batch = 5.3497e-01, Meta loss averaged over last 500 steps = 4.7537e-01, PNorm = 53.1034, GNorm = 0.1044
Meta loss on this task batch = 4.6586e-01, Meta loss averaged over last 500 steps = 4.7534e-01, PNorm = 53.1141, GNorm = 0.0890
Meta loss on this task batch = 4.9093e-01, Meta loss averaged over last 500 steps = 4.7539e-01, PNorm = 53.1252, GNorm = 0.1027
Meta loss on this task batch = 3.9585e-01, Meta loss averaged over last 500 steps = 4.7516e-01, PNorm = 53.1382, GNorm = 0.1782
Meta loss on this task batch = 4.6829e-01, Meta loss averaged over last 500 steps = 4.7514e-01, PNorm = 53.1509, GNorm = 0.1470
Meta loss on this task batch = 5.3619e-01, Meta loss averaged over last 500 steps = 4.7531e-01, PNorm = 53.1623, GNorm = 0.1045
Meta loss on this task batch = 3.9185e-01, Meta loss averaged over last 500 steps = 4.7508e-01, PNorm = 53.1733, GNorm = 0.0984
Meta loss on this task batch = 4.8466e-01, Meta loss averaged over last 500 steps = 4.7510e-01, PNorm = 53.1809, GNorm = 0.2691
Meta loss on this task batch = 4.5595e-01, Meta loss averaged over last 500 steps = 4.7505e-01, PNorm = 53.1850, GNorm = 0.1775
Meta loss on this task batch = 4.9833e-01, Meta loss averaged over last 500 steps = 4.7511e-01, PNorm = 53.1883, GNorm = 0.1276
Meta loss on this task batch = 4.7533e-01, Meta loss averaged over last 500 steps = 4.7512e-01, PNorm = 53.1926, GNorm = 0.0899
Meta loss on this task batch = 4.2278e-01, Meta loss averaged over last 500 steps = 4.7497e-01, PNorm = 53.2001, GNorm = 0.1980
Meta loss on this task batch = 4.2324e-01, Meta loss averaged over last 500 steps = 4.7483e-01, PNorm = 53.2102, GNorm = 0.1280
Meta loss on this task batch = 4.0900e-01, Meta loss averaged over last 500 steps = 4.7464e-01, PNorm = 53.2233, GNorm = 0.2049
Meta loss on this task batch = 4.6885e-01, Meta loss averaged over last 500 steps = 4.7463e-01, PNorm = 53.2356, GNorm = 0.1307
Took 135.16847276687622 seconds to complete one epoch of meta training
Took 144.0124921798706 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.401740
Epoch 19
Meta loss on this task batch = 5.1644e-01, Meta loss averaged over last 500 steps = 4.7474e-01, PNorm = 53.2465, GNorm = 0.1449
Meta loss on this task batch = 4.2926e-01, Meta loss averaged over last 500 steps = 4.7462e-01, PNorm = 53.2574, GNorm = 0.1073
Meta loss on this task batch = 5.6712e-01, Meta loss averaged over last 500 steps = 4.7487e-01, PNorm = 53.2684, GNorm = 0.2181
Meta loss on this task batch = 4.1011e-01, Meta loss averaged over last 500 steps = 4.7469e-01, PNorm = 53.2789, GNorm = 0.1213
Meta loss on this task batch = 5.2496e-01, Meta loss averaged over last 500 steps = 4.7483e-01, PNorm = 53.2884, GNorm = 0.1643
Meta loss on this task batch = 3.7588e-01, Meta loss averaged over last 500 steps = 4.7456e-01, PNorm = 53.3001, GNorm = 0.1445
Meta loss on this task batch = 5.1268e-01, Meta loss averaged over last 500 steps = 4.7466e-01, PNorm = 53.3126, GNorm = 0.1199
Meta loss on this task batch = 5.0608e-01, Meta loss averaged over last 500 steps = 4.7475e-01, PNorm = 53.3245, GNorm = 0.1122
Meta loss on this task batch = 4.1623e-01, Meta loss averaged over last 500 steps = 4.7459e-01, PNorm = 53.3350, GNorm = 0.1162
Meta loss on this task batch = 4.5105e-01, Meta loss averaged over last 500 steps = 4.7453e-01, PNorm = 53.3458, GNorm = 0.1148
Meta loss on this task batch = 5.0383e-01, Meta loss averaged over last 500 steps = 4.7461e-01, PNorm = 53.3565, GNorm = 0.1222
Meta loss on this task batch = 4.7411e-01, Meta loss averaged over last 500 steps = 4.7461e-01, PNorm = 53.3671, GNorm = 0.1198
Meta loss on this task batch = 4.5835e-01, Meta loss averaged over last 500 steps = 4.7456e-01, PNorm = 53.3761, GNorm = 0.1613
Meta loss on this task batch = 4.9773e-01, Meta loss averaged over last 500 steps = 4.7462e-01, PNorm = 53.3848, GNorm = 0.1073
Meta loss on this task batch = 4.2891e-01, Meta loss averaged over last 500 steps = 4.7450e-01, PNorm = 53.3919, GNorm = 0.1409
Meta loss on this task batch = 4.8643e-01, Meta loss averaged over last 500 steps = 4.7453e-01, PNorm = 53.3966, GNorm = 0.1434
Meta loss on this task batch = 3.8597e-01, Meta loss averaged over last 500 steps = 4.7430e-01, PNorm = 53.4011, GNorm = 0.1372
Meta loss on this task batch = 3.8126e-01, Meta loss averaged over last 500 steps = 4.7405e-01, PNorm = 53.4059, GNorm = 0.0914
Meta loss on this task batch = 3.8937e-01, Meta loss averaged over last 500 steps = 4.7383e-01, PNorm = 53.4116, GNorm = 0.1066
Took 131.44729447364807 seconds to complete one epoch of meta training
Took 139.67274117469788 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.400145
Epoch 20
Meta loss on this task batch = 4.0627e-01, Meta loss averaged over last 500 steps = 4.7365e-01, PNorm = 53.4192, GNorm = 0.1216
Meta loss on this task batch = 5.1113e-01, Meta loss averaged over last 500 steps = 4.7375e-01, PNorm = 53.4282, GNorm = 0.1228
Meta loss on this task batch = 4.5157e-01, Meta loss averaged over last 500 steps = 4.7369e-01, PNorm = 53.4400, GNorm = 0.1899
Meta loss on this task batch = 4.2572e-01, Meta loss averaged over last 500 steps = 4.7357e-01, PNorm = 53.4525, GNorm = 0.1107
Meta loss on this task batch = 5.0168e-01, Meta loss averaged over last 500 steps = 4.7364e-01, PNorm = 53.4639, GNorm = 0.1831
Meta loss on this task batch = 4.7952e-01, Meta loss averaged over last 500 steps = 4.7366e-01, PNorm = 53.4742, GNorm = 0.1964
Meta loss on this task batch = 4.8183e-01, Meta loss averaged over last 500 steps = 4.7368e-01, PNorm = 53.4826, GNorm = 0.2349
Meta loss on this task batch = 4.5496e-01, Meta loss averaged over last 500 steps = 4.7363e-01, PNorm = 53.4886, GNorm = 0.1944
Meta loss on this task batch = 4.7950e-01, Meta loss averaged over last 500 steps = 4.7365e-01, PNorm = 53.4967, GNorm = 0.1818
Meta loss on this task batch = 4.9544e-01, Meta loss averaged over last 500 steps = 4.7370e-01, PNorm = 53.5089, GNorm = 0.3089
Meta loss on this task batch = 4.1432e-01, Meta loss averaged over last 500 steps = 4.7355e-01, PNorm = 53.5231, GNorm = 0.1511
Meta loss on this task batch = 4.8607e-01, Meta loss averaged over last 500 steps = 4.7358e-01, PNorm = 53.5371, GNorm = 0.1526
Meta loss on this task batch = 4.7067e-01, Meta loss averaged over last 500 steps = 4.7357e-01, PNorm = 53.5515, GNorm = 0.1122
Meta loss on this task batch = 4.6266e-01, Meta loss averaged over last 500 steps = 4.7355e-01, PNorm = 53.5638, GNorm = 0.1561
Meta loss on this task batch = 4.0629e-01, Meta loss averaged over last 500 steps = 4.7338e-01, PNorm = 53.5756, GNorm = 0.1402
Meta loss on this task batch = 5.0889e-01, Meta loss averaged over last 500 steps = 4.7347e-01, PNorm = 53.5858, GNorm = 0.1982
Meta loss on this task batch = 4.3666e-01, Meta loss averaged over last 500 steps = 4.7337e-01, PNorm = 53.5943, GNorm = 0.1413
Meta loss on this task batch = 4.9737e-01, Meta loss averaged over last 500 steps = 4.7343e-01, PNorm = 53.6034, GNorm = 0.1235
Meta loss on this task batch = 4.1051e-01, Meta loss averaged over last 500 steps = 4.7328e-01, PNorm = 53.6148, GNorm = 0.1952
Took 131.84324145317078 seconds to complete one epoch of meta training
Took 140.83534812927246 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.398876
Epoch 21
Meta loss on this task batch = 4.4735e-01, Meta loss averaged over last 500 steps = 4.7321e-01, PNorm = 53.6272, GNorm = 0.1130
Meta loss on this task batch = 4.4768e-01, Meta loss averaged over last 500 steps = 4.7315e-01, PNorm = 53.6392, GNorm = 0.1080
Meta loss on this task batch = 4.9278e-01, Meta loss averaged over last 500 steps = 4.7320e-01, PNorm = 53.6501, GNorm = 0.1648
Meta loss on this task batch = 4.6208e-01, Meta loss averaged over last 500 steps = 4.7317e-01, PNorm = 53.6605, GNorm = 0.1168
Meta loss on this task batch = 5.0550e-01, Meta loss averaged over last 500 steps = 4.7325e-01, PNorm = 53.6698, GNorm = 0.1065
Meta loss on this task batch = 4.7091e-01, Meta loss averaged over last 500 steps = 4.7324e-01, PNorm = 53.6785, GNorm = 0.1202
Meta loss on this task batch = 5.0749e-01, Meta loss averaged over last 500 steps = 4.7333e-01, PNorm = 53.6876, GNorm = 0.1033
Meta loss on this task batch = 3.9827e-01, Meta loss averaged over last 500 steps = 4.7314e-01, PNorm = 53.6981, GNorm = 0.1207
Meta loss on this task batch = 4.6893e-01, Meta loss averaged over last 500 steps = 4.7313e-01, PNorm = 53.7081, GNorm = 0.1244
Meta loss on this task batch = 4.5999e-01, Meta loss averaged over last 500 steps = 4.7310e-01, PNorm = 53.7185, GNorm = 0.1129
Meta loss on this task batch = 3.9285e-01, Meta loss averaged over last 500 steps = 4.7290e-01, PNorm = 53.7284, GNorm = 0.1401
Meta loss on this task batch = 4.5108e-01, Meta loss averaged over last 500 steps = 4.7285e-01, PNorm = 53.7390, GNorm = 0.1071
Meta loss on this task batch = 5.2983e-01, Meta loss averaged over last 500 steps = 4.7299e-01, PNorm = 53.7455, GNorm = 0.2056
Meta loss on this task batch = 3.9174e-01, Meta loss averaged over last 500 steps = 4.7279e-01, PNorm = 53.7537, GNorm = 0.1473
Meta loss on this task batch = 4.5797e-01, Meta loss averaged over last 500 steps = 4.7276e-01, PNorm = 53.7624, GNorm = 0.1243
Meta loss on this task batch = 5.0306e-01, Meta loss averaged over last 500 steps = 4.7283e-01, PNorm = 53.7706, GNorm = 0.1379
Meta loss on this task batch = 4.4196e-01, Meta loss averaged over last 500 steps = 4.7276e-01, PNorm = 53.7789, GNorm = 0.1513
Meta loss on this task batch = 4.4405e-01, Meta loss averaged over last 500 steps = 4.7269e-01, PNorm = 53.7856, GNorm = 0.1795
Meta loss on this task batch = 3.7975e-01, Meta loss averaged over last 500 steps = 4.7246e-01, PNorm = 53.7936, GNorm = 0.1284
Took 132.90188837051392 seconds to complete one epoch of meta training
Took 141.2273178100586 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.420548
Found better MAML checkpoint after meta validation, saving now
Epoch 22
Meta loss on this task batch = 4.0241e-01, Meta loss averaged over last 500 steps = 4.7230e-01, PNorm = 53.8004, GNorm = 0.1393
Meta loss on this task batch = 5.5327e-01, Meta loss averaged over last 500 steps = 4.7249e-01, PNorm = 53.8067, GNorm = 0.1856
Meta loss on this task batch = 4.6368e-01, Meta loss averaged over last 500 steps = 4.7247e-01, PNorm = 53.8151, GNorm = 0.1375
Meta loss on this task batch = 5.2173e-01, Meta loss averaged over last 500 steps = 4.7259e-01, PNorm = 53.8248, GNorm = 0.1402
Meta loss on this task batch = 4.2863e-01, Meta loss averaged over last 500 steps = 4.7248e-01, PNorm = 53.8361, GNorm = 0.1242
Meta loss on this task batch = 4.6038e-01, Meta loss averaged over last 500 steps = 4.7245e-01, PNorm = 53.8491, GNorm = 0.1398
Meta loss on this task batch = 4.3846e-01, Meta loss averaged over last 500 steps = 4.7237e-01, PNorm = 53.8635, GNorm = 0.1322
Meta loss on this task batch = 4.3852e-01, Meta loss averaged over last 500 steps = 4.7229e-01, PNorm = 53.8761, GNorm = 0.1339
Meta loss on this task batch = 4.3092e-01, Meta loss averaged over last 500 steps = 4.7220e-01, PNorm = 53.8878, GNorm = 0.1145
Meta loss on this task batch = 3.9982e-01, Meta loss averaged over last 500 steps = 4.7203e-01, PNorm = 53.8970, GNorm = 0.1624
Meta loss on this task batch = 5.0414e-01, Meta loss averaged over last 500 steps = 4.7210e-01, PNorm = 53.9029, GNorm = 0.1706
Meta loss on this task batch = 4.9820e-01, Meta loss averaged over last 500 steps = 4.7216e-01, PNorm = 53.9080, GNorm = 0.1523
Meta loss on this task batch = 5.1712e-01, Meta loss averaged over last 500 steps = 4.7227e-01, PNorm = 53.9118, GNorm = 0.1415
Meta loss on this task batch = 4.4429e-01, Meta loss averaged over last 500 steps = 4.7220e-01, PNorm = 53.9179, GNorm = 0.1648
Meta loss on this task batch = 4.2578e-01, Meta loss averaged over last 500 steps = 4.7210e-01, PNorm = 53.9261, GNorm = 0.1263
Meta loss on this task batch = 4.3537e-01, Meta loss averaged over last 500 steps = 4.7201e-01, PNorm = 53.9355, GNorm = 0.1404
Meta loss on this task batch = 4.3373e-01, Meta loss averaged over last 500 steps = 4.7192e-01, PNorm = 53.9465, GNorm = 0.1314
Meta loss on this task batch = 4.5436e-01, Meta loss averaged over last 500 steps = 4.7188e-01, PNorm = 53.9587, GNorm = 0.1246
Meta loss on this task batch = 4.9484e-01, Meta loss averaged over last 500 steps = 4.7194e-01, PNorm = 53.9696, GNorm = 0.1904
Took 134.8848478794098 seconds to complete one epoch of meta training
Took 143.40869045257568 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.420118
Epoch 23
Meta loss on this task batch = 4.2532e-01, Meta loss averaged over last 500 steps = 4.7183e-01, PNorm = 53.9808, GNorm = 0.2066
Meta loss on this task batch = 4.3788e-01, Meta loss averaged over last 500 steps = 4.7175e-01, PNorm = 53.9910, GNorm = 0.1490
Meta loss on this task batch = 4.9440e-01, Meta loss averaged over last 500 steps = 4.7180e-01, PNorm = 53.9996, GNorm = 0.2013
Meta loss on this task batch = 4.6523e-01, Meta loss averaged over last 500 steps = 4.7179e-01, PNorm = 54.0073, GNorm = 0.1788
Meta loss on this task batch = 4.3981e-01, Meta loss averaged over last 500 steps = 4.7172e-01, PNorm = 54.0164, GNorm = 0.1169
Meta loss on this task batch = 4.8729e-01, Meta loss averaged over last 500 steps = 4.7175e-01, PNorm = 54.0269, GNorm = 0.1325
Meta loss on this task batch = 4.0209e-01, Meta loss averaged over last 500 steps = 4.7159e-01, PNorm = 54.0382, GNorm = 0.1070
Meta loss on this task batch = 4.4678e-01, Meta loss averaged over last 500 steps = 4.7154e-01, PNorm = 54.0499, GNorm = 0.1254
Meta loss on this task batch = 4.3374e-01, Meta loss averaged over last 500 steps = 4.7145e-01, PNorm = 54.0630, GNorm = 0.1818
Meta loss on this task batch = 4.4796e-01, Meta loss averaged over last 500 steps = 4.7140e-01, PNorm = 54.0772, GNorm = 0.1291
Meta loss on this task batch = 5.4084e-01, Meta loss averaged over last 500 steps = 4.7156e-01, PNorm = 54.0907, GNorm = 0.1325
Meta loss on this task batch = 4.4144e-01, Meta loss averaged over last 500 steps = 4.7149e-01, PNorm = 54.1038, GNorm = 0.1414
Meta loss on this task batch = 4.2260e-01, Meta loss averaged over last 500 steps = 4.7138e-01, PNorm = 54.1160, GNorm = 0.1153
Meta loss on this task batch = 4.8281e-01, Meta loss averaged over last 500 steps = 4.7141e-01, PNorm = 54.1254, GNorm = 0.1726
Meta loss on this task batch = 5.0206e-01, Meta loss averaged over last 500 steps = 4.7147e-01, PNorm = 54.1343, GNorm = 0.1330
Meta loss on this task batch = 4.8190e-01, Meta loss averaged over last 500 steps = 4.7150e-01, PNorm = 54.1430, GNorm = 0.1182
Meta loss on this task batch = 4.0597e-01, Meta loss averaged over last 500 steps = 4.7135e-01, PNorm = 54.1521, GNorm = 0.1012
Meta loss on this task batch = 4.7062e-01, Meta loss averaged over last 500 steps = 4.7135e-01, PNorm = 54.1618, GNorm = 0.1262
Meta loss on this task batch = 4.5471e-01, Meta loss averaged over last 500 steps = 4.7131e-01, PNorm = 54.1728, GNorm = 0.1436
Took 132.70775413513184 seconds to complete one epoch of meta training
Took 139.9181215763092 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.404581
Epoch 24
Meta loss on this task batch = 4.2057e-01, Meta loss averaged over last 500 steps = 4.7120e-01, PNorm = 54.1847, GNorm = 0.1172
Meta loss on this task batch = 4.3877e-01, Meta loss averaged over last 500 steps = 4.7113e-01, PNorm = 54.1963, GNorm = 0.0981
Meta loss on this task batch = 4.1784e-01, Meta loss averaged over last 500 steps = 4.7102e-01, PNorm = 54.2075, GNorm = 0.1171
Meta loss on this task batch = 4.7995e-01, Meta loss averaged over last 500 steps = 4.7104e-01, PNorm = 54.2197, GNorm = 0.1185
Meta loss on this task batch = 4.9651e-01, Meta loss averaged over last 500 steps = 4.7109e-01, PNorm = 54.2310, GNorm = 0.1922
Meta loss on this task batch = 4.6784e-01, Meta loss averaged over last 500 steps = 4.7108e-01, PNorm = 54.2421, GNorm = 0.1288
Meta loss on this task batch = 3.9760e-01, Meta loss averaged over last 500 steps = 4.7093e-01, PNorm = 54.2521, GNorm = 0.1183
Meta loss on this task batch = 5.1801e-01, Meta loss averaged over last 500 steps = 4.7103e-01, PNorm = 54.2606, GNorm = 0.1612
Meta loss on this task batch = 4.7281e-01, Meta loss averaged over last 500 steps = 4.7103e-01, PNorm = 54.2689, GNorm = 0.1059
Meta loss on this task batch = 4.7187e-01, Meta loss averaged over last 500 steps = 4.7103e-01, PNorm = 54.2781, GNorm = 0.1673
Meta loss on this task batch = 4.3429e-01, Meta loss averaged over last 500 steps = 4.7095e-01, PNorm = 54.2892, GNorm = 0.1334
Meta loss on this task batch = 4.8239e-01, Meta loss averaged over last 500 steps = 4.7098e-01, PNorm = 54.2994, GNorm = 0.1576
Meta loss on this task batch = 4.4205e-01, Meta loss averaged over last 500 steps = 4.7092e-01, PNorm = 54.3093, GNorm = 0.0967
Meta loss on this task batch = 5.0195e-01, Meta loss averaged over last 500 steps = 4.7098e-01, PNorm = 54.3201, GNorm = 0.1298
Meta loss on this task batch = 4.9097e-01, Meta loss averaged over last 500 steps = 4.7102e-01, PNorm = 54.3292, GNorm = 0.1161
Meta loss on this task batch = 4.2219e-01, Meta loss averaged over last 500 steps = 4.7092e-01, PNorm = 54.3410, GNorm = 0.1485
Meta loss on this task batch = 4.3590e-01, Meta loss averaged over last 500 steps = 4.7085e-01, PNorm = 54.3544, GNorm = 0.1211
Meta loss on this task batch = 4.6376e-01, Meta loss averaged over last 500 steps = 4.7083e-01, PNorm = 54.3663, GNorm = 0.1514
Meta loss on this task batch = 4.5748e-01, Meta loss averaged over last 500 steps = 4.7080e-01, PNorm = 54.3777, GNorm = 0.1503
Took 133.21287846565247 seconds to complete one epoch of meta training
Took 141.09152698516846 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.403556
Epoch 25
Meta loss on this task batch = 4.5536e-01, Meta loss averaged over last 500 steps = 4.7077e-01, PNorm = 54.3886, GNorm = 0.1324
Meta loss on this task batch = 4.3287e-01, Meta loss averaged over last 500 steps = 4.7069e-01, PNorm = 54.3989, GNorm = 0.1942
Meta loss on this task batch = 4.2792e-01, Meta loss averaged over last 500 steps = 4.7060e-01, PNorm = 54.4095, GNorm = 0.1147
Meta loss on this task batch = 4.5795e-01, Meta loss averaged over last 500 steps = 4.7058e-01, PNorm = 54.4201, GNorm = 0.1116
Meta loss on this task batch = 4.4860e-01, Meta loss averaged over last 500 steps = 4.7053e-01, PNorm = 54.4306, GNorm = 0.1267
Meta loss on this task batch = 4.2043e-01, Meta loss averaged over last 500 steps = 4.7043e-01, PNorm = 54.4412, GNorm = 0.1196
Meta loss on this task batch = 4.6370e-01, Meta loss averaged over last 500 steps = 4.7041e-01, PNorm = 54.4517, GNorm = 0.1275
Meta loss on this task batch = 4.3789e-01, Meta loss averaged over last 500 steps = 4.7035e-01, PNorm = 54.4646, GNorm = 0.1660
Meta loss on this task batch = 4.1731e-01, Meta loss averaged over last 500 steps = 4.7024e-01, PNorm = 54.4769, GNorm = 0.1195
Meta loss on this task batch = 4.0199e-01, Meta loss averaged over last 500 steps = 4.7009e-01, PNorm = 54.4884, GNorm = 0.1205
Meta loss on this task batch = 4.8900e-01, Meta loss averaged over last 500 steps = 4.7013e-01, PNorm = 54.4979, GNorm = 0.1351
Meta loss on this task batch = 4.3971e-01, Meta loss averaged over last 500 steps = 4.7007e-01, PNorm = 54.5077, GNorm = 0.1023
Meta loss on this task batch = 5.0801e-01, Meta loss averaged over last 500 steps = 4.7015e-01, PNorm = 54.5169, GNorm = 0.1618
Meta loss on this task batch = 4.2449e-01, Meta loss averaged over last 500 steps = 4.7006e-01, PNorm = 54.5270, GNorm = 0.1185
Meta loss on this task batch = 4.9359e-01, Meta loss averaged over last 500 steps = 4.7010e-01, PNorm = 54.5371, GNorm = 0.1518
Meta loss on this task batch = 4.5206e-01, Meta loss averaged over last 500 steps = 4.7007e-01, PNorm = 54.5476, GNorm = 0.1426
Meta loss on this task batch = 5.1595e-01, Meta loss averaged over last 500 steps = 4.7016e-01, PNorm = 54.5580, GNorm = 0.1261
Meta loss on this task batch = 4.5525e-01, Meta loss averaged over last 500 steps = 4.7013e-01, PNorm = 54.5693, GNorm = 0.1223
Meta loss on this task batch = 4.1983e-01, Meta loss averaged over last 500 steps = 4.7003e-01, PNorm = 54.5790, GNorm = 0.1373
Took 131.27260613441467 seconds to complete one epoch of meta training
Took 139.60286593437195 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.440442
Found better MAML checkpoint after meta validation, saving now
Epoch 26
Meta loss on this task batch = 3.9200e-01, Meta loss averaged over last 500 steps = 4.6987e-01, PNorm = 54.5893, GNorm = 0.1477
Meta loss on this task batch = 4.6769e-01, Meta loss averaged over last 500 steps = 4.6987e-01, PNorm = 54.6005, GNorm = 0.1564
Meta loss on this task batch = 4.0561e-01, Meta loss averaged over last 500 steps = 4.6974e-01, PNorm = 54.6113, GNorm = 0.1004
Meta loss on this task batch = 4.5494e-01, Meta loss averaged over last 500 steps = 4.6971e-01, PNorm = 54.6209, GNorm = 0.1432
Meta loss on this task batch = 4.6077e-01, Meta loss averaged over last 500 steps = 4.6969e-01, PNorm = 54.6297, GNorm = 0.1507
Meta loss on this task batch = 4.0928e-01, Meta loss averaged over last 500 steps = 4.6957e-01, PNorm = 54.6386, GNorm = 0.1004
Meta loss on this task batch = 4.8222e-01, Meta loss averaged over last 500 steps = 4.6837e-01, PNorm = 54.6464, GNorm = 0.1346
Meta loss on this task batch = 4.0573e-01, Meta loss averaged over last 500 steps = 4.6798e-01, PNorm = 54.6545, GNorm = 0.0997
Meta loss on this task batch = 4.6488e-01, Meta loss averaged over last 500 steps = 4.6775e-01, PNorm = 54.6631, GNorm = 0.1447
Meta loss on this task batch = 5.0794e-01, Meta loss averaged over last 500 steps = 4.6767e-01, PNorm = 54.6725, GNorm = 0.1142
Meta loss on this task batch = 4.2775e-01, Meta loss averaged over last 500 steps = 4.6738e-01, PNorm = 54.6821, GNorm = 0.0991
Meta loss on this task batch = 4.4002e-01, Meta loss averaged over last 500 steps = 4.6723e-01, PNorm = 54.6932, GNorm = 0.1200
Meta loss on this task batch = 5.2023e-01, Meta loss averaged over last 500 steps = 4.6734e-01, PNorm = 54.7041, GNorm = 0.1191
Meta loss on this task batch = 4.5081e-01, Meta loss averaged over last 500 steps = 4.6709e-01, PNorm = 54.7179, GNorm = 0.2011
Meta loss on this task batch = 4.1807e-01, Meta loss averaged over last 500 steps = 4.6698e-01, PNorm = 54.7323, GNorm = 0.1328
Meta loss on this task batch = 5.4599e-01, Meta loss averaged over last 500 steps = 4.6709e-01, PNorm = 54.7454, GNorm = 0.1807
Meta loss on this task batch = 4.6557e-01, Meta loss averaged over last 500 steps = 4.6711e-01, PNorm = 54.7582, GNorm = 0.1090
Meta loss on this task batch = 3.9943e-01, Meta loss averaged over last 500 steps = 4.6695e-01, PNorm = 54.7713, GNorm = 0.1376
Meta loss on this task batch = 4.6229e-01, Meta loss averaged over last 500 steps = 4.6697e-01, PNorm = 54.7840, GNorm = 0.1890
Took 133.15379905700684 seconds to complete one epoch of meta training
Took 141.93049478530884 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.443363
Found better MAML checkpoint after meta validation, saving now
Epoch 27
Meta loss on this task batch = 4.7088e-01, Meta loss averaged over last 500 steps = 4.6710e-01, PNorm = 54.7950, GNorm = 0.1792
Meta loss on this task batch = 3.7667e-01, Meta loss averaged over last 500 steps = 4.6699e-01, PNorm = 54.8049, GNorm = 0.1500
Meta loss on this task batch = 5.2127e-01, Meta loss averaged over last 500 steps = 4.6712e-01, PNorm = 54.8136, GNorm = 0.1573
Meta loss on this task batch = 3.9449e-01, Meta loss averaged over last 500 steps = 4.6676e-01, PNorm = 54.8247, GNorm = 0.1396
Meta loss on this task batch = 4.0965e-01, Meta loss averaged over last 500 steps = 4.6656e-01, PNorm = 54.8388, GNorm = 0.2117
Meta loss on this task batch = 5.0300e-01, Meta loss averaged over last 500 steps = 4.6650e-01, PNorm = 54.8531, GNorm = 0.1728
Meta loss on this task batch = 4.9097e-01, Meta loss averaged over last 500 steps = 4.6644e-01, PNorm = 54.8658, GNorm = 0.1524
Meta loss on this task batch = 5.1156e-01, Meta loss averaged over last 500 steps = 4.6641e-01, PNorm = 54.8772, GNorm = 0.1890
Meta loss on this task batch = 4.3915e-01, Meta loss averaged over last 500 steps = 4.6628e-01, PNorm = 54.8891, GNorm = 0.1273
Meta loss on this task batch = 4.6661e-01, Meta loss averaged over last 500 steps = 4.6637e-01, PNorm = 54.8991, GNorm = 0.1928
Meta loss on this task batch = 4.1470e-01, Meta loss averaged over last 500 steps = 4.6618e-01, PNorm = 54.9089, GNorm = 0.1175
Meta loss on this task batch = 4.1966e-01, Meta loss averaged over last 500 steps = 4.6596e-01, PNorm = 54.9174, GNorm = 0.1788
Meta loss on this task batch = 4.3175e-01, Meta loss averaged over last 500 steps = 4.6582e-01, PNorm = 54.9261, GNorm = 0.1170
Meta loss on this task batch = 4.8144e-01, Meta loss averaged over last 500 steps = 4.6579e-01, PNorm = 54.9343, GNorm = 0.1454
Meta loss on this task batch = 4.6926e-01, Meta loss averaged over last 500 steps = 4.6586e-01, PNorm = 54.9448, GNorm = 0.1453
Meta loss on this task batch = 4.7742e-01, Meta loss averaged over last 500 steps = 4.6585e-01, PNorm = 54.9559, GNorm = 0.0982
Meta loss on this task batch = 4.8440e-01, Meta loss averaged over last 500 steps = 4.6595e-01, PNorm = 54.9680, GNorm = 0.1321
Meta loss on this task batch = 4.5763e-01, Meta loss averaged over last 500 steps = 4.6590e-01, PNorm = 54.9824, GNorm = 0.1794
Meta loss on this task batch = 4.6831e-01, Meta loss averaged over last 500 steps = 4.6581e-01, PNorm = 54.9963, GNorm = 0.1116
Took 129.42540311813354 seconds to complete one epoch of meta training
Took 137.94622015953064 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.423184
Epoch 28
Meta loss on this task batch = 4.6277e-01, Meta loss averaged over last 500 steps = 4.6582e-01, PNorm = 55.0101, GNorm = 0.1241
Meta loss on this task batch = 4.5957e-01, Meta loss averaged over last 500 steps = 4.6578e-01, PNorm = 55.0233, GNorm = 0.1374
Meta loss on this task batch = 4.6488e-01, Meta loss averaged over last 500 steps = 4.6569e-01, PNorm = 55.0365, GNorm = 0.1009
Meta loss on this task batch = 4.6839e-01, Meta loss averaged over last 500 steps = 4.6565e-01, PNorm = 55.0495, GNorm = 0.1218
Meta loss on this task batch = 4.0009e-01, Meta loss averaged over last 500 steps = 4.6562e-01, PNorm = 55.0612, GNorm = 0.1208
Meta loss on this task batch = 4.7019e-01, Meta loss averaged over last 500 steps = 4.6554e-01, PNorm = 55.0731, GNorm = 0.1364
Meta loss on this task batch = 4.1637e-01, Meta loss averaged over last 500 steps = 4.6552e-01, PNorm = 55.0841, GNorm = 0.1267
Meta loss on this task batch = 4.2879e-01, Meta loss averaged over last 500 steps = 4.6544e-01, PNorm = 55.0947, GNorm = 0.1171
Meta loss on this task batch = 4.9169e-01, Meta loss averaged over last 500 steps = 4.6550e-01, PNorm = 55.1045, GNorm = 0.1497
Meta loss on this task batch = 3.8147e-01, Meta loss averaged over last 500 steps = 4.6529e-01, PNorm = 55.1146, GNorm = 0.1044
Meta loss on this task batch = 4.4296e-01, Meta loss averaged over last 500 steps = 4.6517e-01, PNorm = 55.1271, GNorm = 0.1979
Meta loss on this task batch = 4.6054e-01, Meta loss averaged over last 500 steps = 4.6524e-01, PNorm = 55.1403, GNorm = 0.1699
Meta loss on this task batch = 4.6345e-01, Meta loss averaged over last 500 steps = 4.6533e-01, PNorm = 55.1518, GNorm = 0.1408
Meta loss on this task batch = 4.6513e-01, Meta loss averaged over last 500 steps = 4.6528e-01, PNorm = 55.1610, GNorm = 0.1818
Meta loss on this task batch = 4.6973e-01, Meta loss averaged over last 500 steps = 4.6528e-01, PNorm = 55.1691, GNorm = 0.1764
Meta loss on this task batch = 4.3939e-01, Meta loss averaged over last 500 steps = 4.6511e-01, PNorm = 55.1763, GNorm = 0.1278
Meta loss on this task batch = 4.3098e-01, Meta loss averaged over last 500 steps = 4.6492e-01, PNorm = 55.1832, GNorm = 0.1383
Meta loss on this task batch = 5.0957e-01, Meta loss averaged over last 500 steps = 4.6496e-01, PNorm = 55.1915, GNorm = 0.1279
Meta loss on this task batch = 4.8629e-01, Meta loss averaged over last 500 steps = 4.6494e-01, PNorm = 55.2008, GNorm = 0.1358
Took 130.4792549610138 seconds to complete one epoch of meta training
Took 138.53862929344177 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.435260
Epoch 29
Meta loss on this task batch = 4.4575e-01, Meta loss averaged over last 500 steps = 4.6487e-01, PNorm = 55.2131, GNorm = 0.1854
Meta loss on this task batch = 5.2494e-01, Meta loss averaged over last 500 steps = 4.6492e-01, PNorm = 55.2270, GNorm = 0.1706
Meta loss on this task batch = 4.3742e-01, Meta loss averaged over last 500 steps = 4.6480e-01, PNorm = 55.2427, GNorm = 0.1372
Meta loss on this task batch = 4.8369e-01, Meta loss averaged over last 500 steps = 4.6485e-01, PNorm = 55.2582, GNorm = 0.1249
Meta loss on this task batch = 4.6578e-01, Meta loss averaged over last 500 steps = 4.6485e-01, PNorm = 55.2734, GNorm = 0.1108
Meta loss on this task batch = 4.2011e-01, Meta loss averaged over last 500 steps = 4.6465e-01, PNorm = 55.2872, GNorm = 0.1588
Meta loss on this task batch = 4.6842e-01, Meta loss averaged over last 500 steps = 4.6461e-01, PNorm = 55.2984, GNorm = 0.1933
Meta loss on this task batch = 4.4819e-01, Meta loss averaged over last 500 steps = 4.6453e-01, PNorm = 55.3102, GNorm = 0.1385
Meta loss on this task batch = 4.7435e-01, Meta loss averaged over last 500 steps = 4.6450e-01, PNorm = 55.3212, GNorm = 0.1305
Meta loss on this task batch = 4.3127e-01, Meta loss averaged over last 500 steps = 4.6441e-01, PNorm = 55.3322, GNorm = 0.1742
Meta loss on this task batch = 3.9455e-01, Meta loss averaged over last 500 steps = 4.6425e-01, PNorm = 55.3433, GNorm = 0.1230
Meta loss on this task batch = 4.3372e-01, Meta loss averaged over last 500 steps = 4.6413e-01, PNorm = 55.3549, GNorm = 0.1303
Meta loss on this task batch = 4.1648e-01, Meta loss averaged over last 500 steps = 4.6390e-01, PNorm = 55.3679, GNorm = 0.1580
Meta loss on this task batch = 4.6857e-01, Meta loss averaged over last 500 steps = 4.6389e-01, PNorm = 55.3808, GNorm = 0.1398
Meta loss on this task batch = 4.3113e-01, Meta loss averaged over last 500 steps = 4.6395e-01, PNorm = 55.3934, GNorm = 0.1108
Meta loss on this task batch = 4.1290e-01, Meta loss averaged over last 500 steps = 4.6385e-01, PNorm = 55.4042, GNorm = 0.1469
Meta loss on this task batch = 4.1242e-01, Meta loss averaged over last 500 steps = 4.6379e-01, PNorm = 55.4159, GNorm = 0.1163
Meta loss on this task batch = 4.6733e-01, Meta loss averaged over last 500 steps = 4.6362e-01, PNorm = 55.4270, GNorm = 0.1515
Meta loss on this task batch = 3.7335e-01, Meta loss averaged over last 500 steps = 4.6352e-01, PNorm = 55.4392, GNorm = 0.1280
Took 128.36849451065063 seconds to complete one epoch of meta training
Took 136.4524006843567 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467965
Found better MAML checkpoint after meta validation, saving now
Epoch 30
Meta loss on this task batch = 4.2531e-01, Meta loss averaged over last 500 steps = 4.6342e-01, PNorm = 55.4508, GNorm = 0.1309
Meta loss on this task batch = 4.7455e-01, Meta loss averaged over last 500 steps = 4.6361e-01, PNorm = 55.4624, GNorm = 0.1486
Meta loss on this task batch = 5.0281e-01, Meta loss averaged over last 500 steps = 4.6375e-01, PNorm = 55.4738, GNorm = 0.1513
Meta loss on this task batch = 3.6934e-01, Meta loss averaged over last 500 steps = 4.6346e-01, PNorm = 55.4867, GNorm = 0.1066
Meta loss on this task batch = 4.7817e-01, Meta loss averaged over last 500 steps = 4.6325e-01, PNorm = 55.4987, GNorm = 0.1985
Meta loss on this task batch = 4.9804e-01, Meta loss averaged over last 500 steps = 4.6302e-01, PNorm = 55.5115, GNorm = 0.1266
Meta loss on this task batch = 3.9410e-01, Meta loss averaged over last 500 steps = 4.6288e-01, PNorm = 55.5270, GNorm = 0.1957
Meta loss on this task batch = 4.8240e-01, Meta loss averaged over last 500 steps = 4.6289e-01, PNorm = 55.5415, GNorm = 0.1202
Meta loss on this task batch = 4.3980e-01, Meta loss averaged over last 500 steps = 4.6283e-01, PNorm = 55.5560, GNorm = 0.1510
Meta loss on this task batch = 4.5100e-01, Meta loss averaged over last 500 steps = 4.6282e-01, PNorm = 55.5706, GNorm = 0.1112
Meta loss on this task batch = 3.1696e-01, Meta loss averaged over last 500 steps = 4.6248e-01, PNorm = 55.5885, GNorm = 0.2048
Meta loss on this task batch = 5.0317e-01, Meta loss averaged over last 500 steps = 4.6263e-01, PNorm = 55.6043, GNorm = 0.1507
Meta loss on this task batch = 4.7750e-01, Meta loss averaged over last 500 steps = 4.6271e-01, PNorm = 55.6155, GNorm = 0.2016
Meta loss on this task batch = 3.7641e-01, Meta loss averaged over last 500 steps = 4.6253e-01, PNorm = 55.6278, GNorm = 0.1230
Meta loss on this task batch = 4.0627e-01, Meta loss averaged over last 500 steps = 4.6255e-01, PNorm = 55.6376, GNorm = 0.1803
Meta loss on this task batch = 4.5191e-01, Meta loss averaged over last 500 steps = 4.6261e-01, PNorm = 55.6465, GNorm = 0.1344
Meta loss on this task batch = 4.6324e-01, Meta loss averaged over last 500 steps = 4.6265e-01, PNorm = 55.6528, GNorm = 0.2012
Meta loss on this task batch = 4.4455e-01, Meta loss averaged over last 500 steps = 4.6260e-01, PNorm = 55.6608, GNorm = 0.1652
Meta loss on this task batch = 4.5761e-01, Meta loss averaged over last 500 steps = 4.6265e-01, PNorm = 55.6718, GNorm = 0.2052
Took 131.24915528297424 seconds to complete one epoch of meta training
Took 139.53481602668762 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453375
Epoch 31
Meta loss on this task batch = 4.6047e-01, Meta loss averaged over last 500 steps = 4.6247e-01, PNorm = 55.6848, GNorm = 0.2035
Meta loss on this task batch = 4.0512e-01, Meta loss averaged over last 500 steps = 4.6215e-01, PNorm = 55.6989, GNorm = 0.1166
Meta loss on this task batch = 4.8622e-01, Meta loss averaged over last 500 steps = 4.6201e-01, PNorm = 55.7133, GNorm = 0.1274
Meta loss on this task batch = 4.1776e-01, Meta loss averaged over last 500 steps = 4.6189e-01, PNorm = 55.7270, GNorm = 0.1406
Meta loss on this task batch = 4.9233e-01, Meta loss averaged over last 500 steps = 4.6184e-01, PNorm = 55.7398, GNorm = 0.1487
Meta loss on this task batch = 4.5051e-01, Meta loss averaged over last 500 steps = 4.6167e-01, PNorm = 55.7527, GNorm = 0.1178
Meta loss on this task batch = 4.7642e-01, Meta loss averaged over last 500 steps = 4.6167e-01, PNorm = 55.7640, GNorm = 0.2055
Meta loss on this task batch = 3.6195e-01, Meta loss averaged over last 500 steps = 4.6138e-01, PNorm = 55.7760, GNorm = 0.1100
Meta loss on this task batch = 4.3362e-01, Meta loss averaged over last 500 steps = 4.6123e-01, PNorm = 55.7876, GNorm = 0.1404
Meta loss on this task batch = 4.4675e-01, Meta loss averaged over last 500 steps = 4.6112e-01, PNorm = 55.7977, GNorm = 0.1818
Meta loss on this task batch = 4.5587e-01, Meta loss averaged over last 500 steps = 4.6110e-01, PNorm = 55.8069, GNorm = 0.1494
Meta loss on this task batch = 4.7118e-01, Meta loss averaged over last 500 steps = 4.6104e-01, PNorm = 55.8162, GNorm = 0.2008
Meta loss on this task batch = 4.2131e-01, Meta loss averaged over last 500 steps = 4.6096e-01, PNorm = 55.8278, GNorm = 0.1593
Meta loss on this task batch = 5.3049e-01, Meta loss averaged over last 500 steps = 4.6110e-01, PNorm = 55.8394, GNorm = 0.1453
Meta loss on this task batch = 3.7851e-01, Meta loss averaged over last 500 steps = 4.6088e-01, PNorm = 55.8535, GNorm = 0.1581
Meta loss on this task batch = 5.0602e-01, Meta loss averaged over last 500 steps = 4.6097e-01, PNorm = 55.8692, GNorm = 0.1713
Meta loss on this task batch = 4.5006e-01, Meta loss averaged over last 500 steps = 4.6081e-01, PNorm = 55.8837, GNorm = 0.1503
Meta loss on this task batch = 5.1857e-01, Meta loss averaged over last 500 steps = 4.6084e-01, PNorm = 55.8965, GNorm = 0.1918
Meta loss on this task batch = 4.0416e-01, Meta loss averaged over last 500 steps = 4.6065e-01, PNorm = 55.9092, GNorm = 0.1450
Took 130.77392268180847 seconds to complete one epoch of meta training
Took 138.86103892326355 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460996
Epoch 32
Meta loss on this task batch = 4.1594e-01, Meta loss averaged over last 500 steps = 4.6068e-01, PNorm = 55.9221, GNorm = 0.1447
Meta loss on this task batch = 4.6762e-01, Meta loss averaged over last 500 steps = 4.6062e-01, PNorm = 55.9353, GNorm = 0.1566
Meta loss on this task batch = 4.6865e-01, Meta loss averaged over last 500 steps = 4.6082e-01, PNorm = 55.9489, GNorm = 0.1707
Meta loss on this task batch = 4.6791e-01, Meta loss averaged over last 500 steps = 4.6081e-01, PNorm = 55.9616, GNorm = 0.1481
Meta loss on this task batch = 4.8077e-01, Meta loss averaged over last 500 steps = 4.6073e-01, PNorm = 55.9748, GNorm = 0.1178
Meta loss on this task batch = 4.8833e-01, Meta loss averaged over last 500 steps = 4.6087e-01, PNorm = 55.9872, GNorm = 0.1444
Meta loss on this task batch = 4.7958e-01, Meta loss averaged over last 500 steps = 4.6088e-01, PNorm = 55.9974, GNorm = 0.1937
Meta loss on this task batch = 4.1287e-01, Meta loss averaged over last 500 steps = 4.6071e-01, PNorm = 56.0078, GNorm = 0.1482
Meta loss on this task batch = 4.0841e-01, Meta loss averaged over last 500 steps = 4.6062e-01, PNorm = 56.0184, GNorm = 0.1356
Meta loss on this task batch = 4.3889e-01, Meta loss averaged over last 500 steps = 4.6056e-01, PNorm = 56.0279, GNorm = 0.1627
Meta loss on this task batch = 4.2544e-01, Meta loss averaged over last 500 steps = 4.6035e-01, PNorm = 56.0373, GNorm = 0.1348
Meta loss on this task batch = 4.3628e-01, Meta loss averaged over last 500 steps = 4.6033e-01, PNorm = 56.0476, GNorm = 0.1068
Meta loss on this task batch = 4.4448e-01, Meta loss averaged over last 500 steps = 4.6034e-01, PNorm = 56.0578, GNorm = 0.1250
Meta loss on this task batch = 4.1973e-01, Meta loss averaged over last 500 steps = 4.6022e-01, PNorm = 56.0685, GNorm = 0.1265
Meta loss on this task batch = 4.5639e-01, Meta loss averaged over last 500 steps = 4.6010e-01, PNorm = 56.0782, GNorm = 0.1262
Meta loss on this task batch = 4.7198e-01, Meta loss averaged over last 500 steps = 4.6008e-01, PNorm = 56.0889, GNorm = 0.1554
Meta loss on this task batch = 3.9878e-01, Meta loss averaged over last 500 steps = 4.5996e-01, PNorm = 56.1004, GNorm = 0.1246
Meta loss on this task batch = 4.2777e-01, Meta loss averaged over last 500 steps = 4.5985e-01, PNorm = 56.1129, GNorm = 0.1369
Meta loss on this task batch = 4.4736e-01, Meta loss averaged over last 500 steps = 4.5988e-01, PNorm = 56.1252, GNorm = 0.1583
Took 131.05385065078735 seconds to complete one epoch of meta training
Took 139.24125742912292 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476652
Found better MAML checkpoint after meta validation, saving now
Epoch 33
Meta loss on this task batch = 4.8529e-01, Meta loss averaged over last 500 steps = 4.5982e-01, PNorm = 56.1365, GNorm = 0.1321
Meta loss on this task batch = 4.6915e-01, Meta loss averaged over last 500 steps = 4.5990e-01, PNorm = 56.1469, GNorm = 0.1682
Meta loss on this task batch = 4.4934e-01, Meta loss averaged over last 500 steps = 4.5991e-01, PNorm = 56.1582, GNorm = 0.1414
Meta loss on this task batch = 3.7896e-01, Meta loss averaged over last 500 steps = 4.5973e-01, PNorm = 56.1685, GNorm = 0.1278
Meta loss on this task batch = 4.4496e-01, Meta loss averaged over last 500 steps = 4.5959e-01, PNorm = 56.1786, GNorm = 0.1214
Meta loss on this task batch = 4.1973e-01, Meta loss averaged over last 500 steps = 4.5945e-01, PNorm = 56.1884, GNorm = 0.1244
Meta loss on this task batch = 5.0239e-01, Meta loss averaged over last 500 steps = 4.5955e-01, PNorm = 56.1989, GNorm = 0.1546
Meta loss on this task batch = 4.3993e-01, Meta loss averaged over last 500 steps = 4.5952e-01, PNorm = 56.2109, GNorm = 0.1797
Meta loss on this task batch = 3.8030e-01, Meta loss averaged over last 500 steps = 4.5929e-01, PNorm = 56.2252, GNorm = 0.1358
Meta loss on this task batch = 4.3343e-01, Meta loss averaged over last 500 steps = 4.5924e-01, PNorm = 56.2395, GNorm = 0.1343
Meta loss on this task batch = 4.6167e-01, Meta loss averaged over last 500 steps = 4.5930e-01, PNorm = 56.2514, GNorm = 0.1830
Meta loss on this task batch = 4.2025e-01, Meta loss averaged over last 500 steps = 4.5923e-01, PNorm = 56.2650, GNorm = 0.1385
Meta loss on this task batch = 4.5913e-01, Meta loss averaged over last 500 steps = 4.5915e-01, PNorm = 56.2787, GNorm = 0.1474
Meta loss on this task batch = 4.1990e-01, Meta loss averaged over last 500 steps = 4.5919e-01, PNorm = 56.2916, GNorm = 0.1492
Meta loss on this task batch = 4.3099e-01, Meta loss averaged over last 500 steps = 4.5920e-01, PNorm = 56.3050, GNorm = 0.1086
Meta loss on this task batch = 3.8921e-01, Meta loss averaged over last 500 steps = 4.5886e-01, PNorm = 56.3178, GNorm = 0.1070
Meta loss on this task batch = 5.0557e-01, Meta loss averaged over last 500 steps = 4.5890e-01, PNorm = 56.3306, GNorm = 0.1552
Meta loss on this task batch = 4.8607e-01, Meta loss averaged over last 500 steps = 4.5896e-01, PNorm = 56.3426, GNorm = 0.1429
Meta loss on this task batch = 4.3089e-01, Meta loss averaged over last 500 steps = 4.5876e-01, PNorm = 56.3538, GNorm = 0.1468
Took 127.6837956905365 seconds to complete one epoch of meta training
Took 136.2991828918457 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474869
Epoch 34
Meta loss on this task batch = 4.2223e-01, Meta loss averaged over last 500 steps = 4.5884e-01, PNorm = 56.3654, GNorm = 0.1323
Meta loss on this task batch = 4.6913e-01, Meta loss averaged over last 500 steps = 4.5863e-01, PNorm = 56.3777, GNorm = 0.1381
Meta loss on this task batch = 5.1985e-01, Meta loss averaged over last 500 steps = 4.5875e-01, PNorm = 56.3899, GNorm = 0.1516
Meta loss on this task batch = 4.5528e-01, Meta loss averaged over last 500 steps = 4.5862e-01, PNorm = 56.4009, GNorm = 0.1570
Meta loss on this task batch = 4.4056e-01, Meta loss averaged over last 500 steps = 4.5861e-01, PNorm = 56.4128, GNorm = 0.1301
Meta loss on this task batch = 4.7143e-01, Meta loss averaged over last 500 steps = 4.5854e-01, PNorm = 56.4233, GNorm = 0.1262
Meta loss on this task batch = 4.3787e-01, Meta loss averaged over last 500 steps = 4.5851e-01, PNorm = 56.4345, GNorm = 0.1384
Meta loss on this task batch = 4.3506e-01, Meta loss averaged over last 500 steps = 4.5832e-01, PNorm = 56.4459, GNorm = 0.1569
Meta loss on this task batch = 4.6489e-01, Meta loss averaged over last 500 steps = 4.5831e-01, PNorm = 56.4573, GNorm = 0.1333
Meta loss on this task batch = 4.1927e-01, Meta loss averaged over last 500 steps = 4.5829e-01, PNorm = 56.4696, GNorm = 0.1459
Meta loss on this task batch = 4.7023e-01, Meta loss averaged over last 500 steps = 4.5820e-01, PNorm = 56.4795, GNorm = 0.1876
Meta loss on this task batch = 4.4482e-01, Meta loss averaged over last 500 steps = 4.5817e-01, PNorm = 56.4893, GNorm = 0.1523
Meta loss on this task batch = 4.4393e-01, Meta loss averaged over last 500 steps = 4.5819e-01, PNorm = 56.5000, GNorm = 0.1409
Meta loss on this task batch = 3.9588e-01, Meta loss averaged over last 500 steps = 4.5789e-01, PNorm = 56.5113, GNorm = 0.1262
Meta loss on this task batch = 4.0593e-01, Meta loss averaged over last 500 steps = 4.5773e-01, PNorm = 56.5221, GNorm = 0.1595
Meta loss on this task batch = 3.6969e-01, Meta loss averaged over last 500 steps = 4.5751e-01, PNorm = 56.5346, GNorm = 0.1669
Meta loss on this task batch = 4.5783e-01, Meta loss averaged over last 500 steps = 4.5741e-01, PNorm = 56.5461, GNorm = 0.1370
Meta loss on this task batch = 4.1497e-01, Meta loss averaged over last 500 steps = 4.5739e-01, PNorm = 56.5574, GNorm = 0.1597
Meta loss on this task batch = 4.6013e-01, Meta loss averaged over last 500 steps = 4.5730e-01, PNorm = 56.5663, GNorm = 0.2042
Took 128.99141716957092 seconds to complete one epoch of meta training
Took 136.87427735328674 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483704
Found better MAML checkpoint after meta validation, saving now
Epoch 35
Meta loss on this task batch = 4.2209e-01, Meta loss averaged over last 500 steps = 4.5733e-01, PNorm = 56.5749, GNorm = 0.1423
Meta loss on this task batch = 4.7198e-01, Meta loss averaged over last 500 steps = 4.5728e-01, PNorm = 56.5853, GNorm = 0.1331
Meta loss on this task batch = 4.4065e-01, Meta loss averaged over last 500 steps = 4.5727e-01, PNorm = 56.5953, GNorm = 0.1352
Meta loss on this task batch = 4.5141e-01, Meta loss averaged over last 500 steps = 4.5726e-01, PNorm = 56.6065, GNorm = 0.1318
Meta loss on this task batch = 3.8731e-01, Meta loss averaged over last 500 steps = 4.5699e-01, PNorm = 56.6207, GNorm = 0.1777
Meta loss on this task batch = 4.0668e-01, Meta loss averaged over last 500 steps = 4.5693e-01, PNorm = 56.6359, GNorm = 0.1468
Meta loss on this task batch = 4.4220e-01, Meta loss averaged over last 500 steps = 4.5688e-01, PNorm = 56.6519, GNorm = 0.1624
Meta loss on this task batch = 4.0377e-01, Meta loss averaged over last 500 steps = 4.5669e-01, PNorm = 56.6676, GNorm = 0.1307
Meta loss on this task batch = 4.8270e-01, Meta loss averaged over last 500 steps = 4.5666e-01, PNorm = 56.6823, GNorm = 0.2181
Meta loss on this task batch = 4.3421e-01, Meta loss averaged over last 500 steps = 4.5660e-01, PNorm = 56.6936, GNorm = 0.2080
Meta loss on this task batch = 4.6202e-01, Meta loss averaged over last 500 steps = 4.5653e-01, PNorm = 56.7043, GNorm = 0.1274
Meta loss on this task batch = 3.9894e-01, Meta loss averaged over last 500 steps = 4.5636e-01, PNorm = 56.7154, GNorm = 0.1311
Meta loss on this task batch = 4.6590e-01, Meta loss averaged over last 500 steps = 4.5641e-01, PNorm = 56.7256, GNorm = 0.1714
Meta loss on this task batch = 4.0096e-01, Meta loss averaged over last 500 steps = 4.5619e-01, PNorm = 56.7378, GNorm = 0.1616
Meta loss on this task batch = 4.5283e-01, Meta loss averaged over last 500 steps = 4.5627e-01, PNorm = 56.7515, GNorm = 0.1821
Meta loss on this task batch = 4.1517e-01, Meta loss averaged over last 500 steps = 4.5605e-01, PNorm = 56.7654, GNorm = 0.1358
Meta loss on this task batch = 4.5685e-01, Meta loss averaged over last 500 steps = 4.5594e-01, PNorm = 56.7806, GNorm = 0.1564
Meta loss on this task batch = 4.1618e-01, Meta loss averaged over last 500 steps = 4.5585e-01, PNorm = 56.7951, GNorm = 0.1644
Meta loss on this task batch = 4.6397e-01, Meta loss averaged over last 500 steps = 4.5584e-01, PNorm = 56.8091, GNorm = 0.1848
Took 126.84127354621887 seconds to complete one epoch of meta training
Took 135.06427931785583 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448476
Epoch 36
Meta loss on this task batch = 4.3214e-01, Meta loss averaged over last 500 steps = 4.5585e-01, PNorm = 56.8221, GNorm = 0.2177
Meta loss on this task batch = 4.2731e-01, Meta loss averaged over last 500 steps = 4.5583e-01, PNorm = 56.8349, GNorm = 0.1493
Meta loss on this task batch = 3.9351e-01, Meta loss averaged over last 500 steps = 4.5566e-01, PNorm = 56.8463, GNorm = 0.1534
Meta loss on this task batch = 4.4510e-01, Meta loss averaged over last 500 steps = 4.5559e-01, PNorm = 56.8575, GNorm = 0.1589
Meta loss on this task batch = 4.1793e-01, Meta loss averaged over last 500 steps = 4.5556e-01, PNorm = 56.8687, GNorm = 0.1642
Meta loss on this task batch = 4.5096e-01, Meta loss averaged over last 500 steps = 4.5558e-01, PNorm = 56.8820, GNorm = 0.1823
Meta loss on this task batch = 4.5181e-01, Meta loss averaged over last 500 steps = 4.5558e-01, PNorm = 56.8946, GNorm = 0.1605
Meta loss on this task batch = 4.0823e-01, Meta loss averaged over last 500 steps = 4.5539e-01, PNorm = 56.9066, GNorm = 0.1539
Meta loss on this task batch = 4.4895e-01, Meta loss averaged over last 500 steps = 4.5543e-01, PNorm = 56.9189, GNorm = 0.1450
Meta loss on this task batch = 4.9520e-01, Meta loss averaged over last 500 steps = 4.5555e-01, PNorm = 56.9306, GNorm = 0.1715
Meta loss on this task batch = 4.8455e-01, Meta loss averaged over last 500 steps = 4.5553e-01, PNorm = 56.9425, GNorm = 0.1331
Meta loss on this task batch = 4.9427e-01, Meta loss averaged over last 500 steps = 4.5557e-01, PNorm = 56.9549, GNorm = 0.1522
Meta loss on this task batch = 3.8041e-01, Meta loss averaged over last 500 steps = 4.5554e-01, PNorm = 56.9685, GNorm = 0.1305
Meta loss on this task batch = 4.6820e-01, Meta loss averaged over last 500 steps = 4.5550e-01, PNorm = 56.9825, GNorm = 0.1689
Meta loss on this task batch = 3.9405e-01, Meta loss averaged over last 500 steps = 4.5536e-01, PNorm = 56.9967, GNorm = 0.1246
Meta loss on this task batch = 4.6532e-01, Meta loss averaged over last 500 steps = 4.5541e-01, PNorm = 57.0112, GNorm = 0.1245
Meta loss on this task batch = 4.5419e-01, Meta loss averaged over last 500 steps = 4.5528e-01, PNorm = 57.0257, GNorm = 0.1532
Meta loss on this task batch = 4.4088e-01, Meta loss averaged over last 500 steps = 4.5535e-01, PNorm = 57.0381, GNorm = 0.1377
Meta loss on this task batch = 3.5267e-01, Meta loss averaged over last 500 steps = 4.5504e-01, PNorm = 57.0481, GNorm = 0.1504
Took 129.5122902393341 seconds to complete one epoch of meta training
Took 137.98276019096375 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466298
Epoch 37
Meta loss on this task batch = 4.1009e-01, Meta loss averaged over last 500 steps = 4.5489e-01, PNorm = 57.0582, GNorm = 0.1329
Meta loss on this task batch = 4.0520e-01, Meta loss averaged over last 500 steps = 4.5486e-01, PNorm = 57.0678, GNorm = 0.1253
Meta loss on this task batch = 4.3489e-01, Meta loss averaged over last 500 steps = 4.5466e-01, PNorm = 57.0787, GNorm = 0.1379
Meta loss on this task batch = 4.1810e-01, Meta loss averaged over last 500 steps = 4.5454e-01, PNorm = 57.0907, GNorm = 0.1300
Meta loss on this task batch = 4.0962e-01, Meta loss averaged over last 500 steps = 4.5438e-01, PNorm = 57.1039, GNorm = 0.1444
Meta loss on this task batch = 3.8501e-01, Meta loss averaged over last 500 steps = 4.5415e-01, PNorm = 57.1190, GNorm = 0.1697
Meta loss on this task batch = 4.5796e-01, Meta loss averaged over last 500 steps = 4.5414e-01, PNorm = 57.1339, GNorm = 0.1745
Meta loss on this task batch = 4.1002e-01, Meta loss averaged over last 500 steps = 4.5402e-01, PNorm = 57.1472, GNorm = 0.1517
Meta loss on this task batch = 4.1720e-01, Meta loss averaged over last 500 steps = 4.5396e-01, PNorm = 57.1583, GNorm = 0.1839
Meta loss on this task batch = 4.3257e-01, Meta loss averaged over last 500 steps = 4.5383e-01, PNorm = 57.1697, GNorm = 0.1382
Meta loss on this task batch = 3.5706e-01, Meta loss averaged over last 500 steps = 4.5354e-01, PNorm = 57.1803, GNorm = 0.1402
Meta loss on this task batch = 4.9485e-01, Meta loss averaged over last 500 steps = 4.5370e-01, PNorm = 57.1925, GNorm = 0.1480
Meta loss on this task batch = 5.3568e-01, Meta loss averaged over last 500 steps = 4.5388e-01, PNorm = 57.2021, GNorm = 0.1968
Meta loss on this task batch = 4.8109e-01, Meta loss averaged over last 500 steps = 4.5382e-01, PNorm = 57.2124, GNorm = 0.1527
Meta loss on this task batch = 4.4268e-01, Meta loss averaged over last 500 steps = 4.5383e-01, PNorm = 57.2237, GNorm = 0.1559
Meta loss on this task batch = 4.0301e-01, Meta loss averaged over last 500 steps = 4.5364e-01, PNorm = 57.2367, GNorm = 0.1846
Meta loss on this task batch = 4.8044e-01, Meta loss averaged over last 500 steps = 4.5371e-01, PNorm = 57.2496, GNorm = 0.1737
Meta loss on this task batch = 4.4812e-01, Meta loss averaged over last 500 steps = 4.5358e-01, PNorm = 57.2627, GNorm = 0.1420
Meta loss on this task batch = 4.9264e-01, Meta loss averaged over last 500 steps = 4.5364e-01, PNorm = 57.2765, GNorm = 0.1867
Took 125.87230157852173 seconds to complete one epoch of meta training
Took 134.94755220413208 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501195
Found better MAML checkpoint after meta validation, saving now
Epoch 38
Meta loss on this task batch = 5.1511e-01, Meta loss averaged over last 500 steps = 4.5367e-01, PNorm = 57.2878, GNorm = 0.2159
Meta loss on this task batch = 4.2234e-01, Meta loss averaged over last 500 steps = 4.5359e-01, PNorm = 57.2953, GNorm = 0.2219
Meta loss on this task batch = 4.5301e-01, Meta loss averaged over last 500 steps = 4.5361e-01, PNorm = 57.3026, GNorm = 0.1628
Meta loss on this task batch = 4.4784e-01, Meta loss averaged over last 500 steps = 4.5358e-01, PNorm = 57.3115, GNorm = 0.1467
Meta loss on this task batch = 4.2821e-01, Meta loss averaged over last 500 steps = 4.5355e-01, PNorm = 57.3245, GNorm = 0.2734
Meta loss on this task batch = 4.3974e-01, Meta loss averaged over last 500 steps = 4.5349e-01, PNorm = 57.3404, GNorm = 0.2271
Meta loss on this task batch = 4.3731e-01, Meta loss averaged over last 500 steps = 4.5339e-01, PNorm = 57.3564, GNorm = 0.1701
Meta loss on this task batch = 4.7931e-01, Meta loss averaged over last 500 steps = 4.5335e-01, PNorm = 57.3725, GNorm = 0.2305
Meta loss on this task batch = 4.9112e-01, Meta loss averaged over last 500 steps = 4.5335e-01, PNorm = 57.3893, GNorm = 0.1953
Meta loss on this task batch = 4.8876e-01, Meta loss averaged over last 500 steps = 4.5339e-01, PNorm = 57.4025, GNorm = 0.2667
Meta loss on this task batch = 3.9784e-01, Meta loss averaged over last 500 steps = 4.5335e-01, PNorm = 57.4140, GNorm = 0.1771
Meta loss on this task batch = 4.1107e-01, Meta loss averaged over last 500 steps = 4.5309e-01, PNorm = 57.4254, GNorm = 0.1573
Meta loss on this task batch = 4.4876e-01, Meta loss averaged over last 500 steps = 4.5289e-01, PNorm = 57.4360, GNorm = 0.1338
Meta loss on this task batch = 3.9638e-01, Meta loss averaged over last 500 steps = 4.5264e-01, PNorm = 57.4479, GNorm = 0.1164
Meta loss on this task batch = 4.6638e-01, Meta loss averaged over last 500 steps = 4.5277e-01, PNorm = 57.4609, GNorm = 0.1379
Meta loss on this task batch = 4.4542e-01, Meta loss averaged over last 500 steps = 4.5283e-01, PNorm = 57.4744, GNorm = 0.1575
Meta loss on this task batch = 4.4110e-01, Meta loss averaged over last 500 steps = 4.5281e-01, PNorm = 57.4887, GNorm = 0.1453
Meta loss on this task batch = 4.6902e-01, Meta loss averaged over last 500 steps = 4.5284e-01, PNorm = 57.5024, GNorm = 0.2232
Meta loss on this task batch = 4.4048e-01, Meta loss averaged over last 500 steps = 4.5276e-01, PNorm = 57.5162, GNorm = 0.1682
Took 130.5015869140625 seconds to complete one epoch of meta training
Took 138.32842540740967 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450457
Epoch 39
Meta loss on this task batch = 4.1519e-01, Meta loss averaged over last 500 steps = 4.5267e-01, PNorm = 57.5311, GNorm = 0.2050
Meta loss on this task batch = 4.6406e-01, Meta loss averaged over last 500 steps = 4.5288e-01, PNorm = 57.5444, GNorm = 0.1512
Meta loss on this task batch = 4.1099e-01, Meta loss averaged over last 500 steps = 4.5264e-01, PNorm = 57.5571, GNorm = 0.1354
Meta loss on this task batch = 4.7799e-01, Meta loss averaged over last 500 steps = 4.5262e-01, PNorm = 57.5691, GNorm = 0.1939
Meta loss on this task batch = 4.3666e-01, Meta loss averaged over last 500 steps = 4.5257e-01, PNorm = 57.5813, GNorm = 0.1713
Meta loss on this task batch = 4.7338e-01, Meta loss averaged over last 500 steps = 4.5271e-01, PNorm = 57.5932, GNorm = 0.1656
Meta loss on this task batch = 4.4060e-01, Meta loss averaged over last 500 steps = 4.5271e-01, PNorm = 57.6055, GNorm = 0.1426
Meta loss on this task batch = 4.5293e-01, Meta loss averaged over last 500 steps = 4.5267e-01, PNorm = 57.6173, GNorm = 0.1624
Meta loss on this task batch = 4.2672e-01, Meta loss averaged over last 500 steps = 4.5271e-01, PNorm = 57.6302, GNorm = 0.1481
Meta loss on this task batch = 4.9142e-01, Meta loss averaged over last 500 steps = 4.5276e-01, PNorm = 57.6429, GNorm = 0.1823
Meta loss on this task batch = 3.3413e-01, Meta loss averaged over last 500 steps = 4.5249e-01, PNorm = 57.6566, GNorm = 0.1248
Meta loss on this task batch = 4.7047e-01, Meta loss averaged over last 500 steps = 4.5243e-01, PNorm = 57.6693, GNorm = 0.1541
Meta loss on this task batch = 4.1082e-01, Meta loss averaged over last 500 steps = 4.5239e-01, PNorm = 57.6794, GNorm = 0.1987
Meta loss on this task batch = 4.0040e-01, Meta loss averaged over last 500 steps = 4.5227e-01, PNorm = 57.6906, GNorm = 0.1283
Meta loss on this task batch = 4.5143e-01, Meta loss averaged over last 500 steps = 4.5222e-01, PNorm = 57.7015, GNorm = 0.1627
Meta loss on this task batch = 4.6321e-01, Meta loss averaged over last 500 steps = 4.5220e-01, PNorm = 57.7125, GNorm = 0.1701
Meta loss on this task batch = 4.7007e-01, Meta loss averaged over last 500 steps = 4.5213e-01, PNorm = 57.7226, GNorm = 0.1711
Meta loss on this task batch = 4.0846e-01, Meta loss averaged over last 500 steps = 4.5198e-01, PNorm = 57.7339, GNorm = 0.1223
Meta loss on this task batch = 4.5499e-01, Meta loss averaged over last 500 steps = 4.5195e-01, PNorm = 57.7471, GNorm = 0.1688
Took 127.90610694885254 seconds to complete one epoch of meta training
Took 135.89458775520325 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501878
Found better MAML checkpoint after meta validation, saving now
Epoch 40
Meta loss on this task batch = 4.2137e-01, Meta loss averaged over last 500 steps = 4.5179e-01, PNorm = 57.7607, GNorm = 0.1311
Meta loss on this task batch = 4.4317e-01, Meta loss averaged over last 500 steps = 4.5179e-01, PNorm = 57.7747, GNorm = 0.1395
Meta loss on this task batch = 4.4906e-01, Meta loss averaged over last 500 steps = 4.5180e-01, PNorm = 57.7909, GNorm = 0.1436
Meta loss on this task batch = 4.6790e-01, Meta loss averaged over last 500 steps = 4.5171e-01, PNorm = 57.8052, GNorm = 0.1488
Meta loss on this task batch = 4.1590e-01, Meta loss averaged over last 500 steps = 4.5156e-01, PNorm = 57.8201, GNorm = 0.1778
Meta loss on this task batch = 4.7454e-01, Meta loss averaged over last 500 steps = 4.5164e-01, PNorm = 57.8351, GNorm = 0.1408
Meta loss on this task batch = 4.3370e-01, Meta loss averaged over last 500 steps = 4.5154e-01, PNorm = 57.8489, GNorm = 0.1978
Meta loss on this task batch = 4.1099e-01, Meta loss averaged over last 500 steps = 4.5137e-01, PNorm = 57.8621, GNorm = 0.1380
Meta loss on this task batch = 4.5775e-01, Meta loss averaged over last 500 steps = 4.5141e-01, PNorm = 57.8742, GNorm = 0.1497
Meta loss on this task batch = 4.7938e-01, Meta loss averaged over last 500 steps = 4.5154e-01, PNorm = 57.8854, GNorm = 0.1691
Meta loss on this task batch = 4.3027e-01, Meta loss averaged over last 500 steps = 4.5155e-01, PNorm = 57.8968, GNorm = 0.1776
Meta loss on this task batch = 4.4929e-01, Meta loss averaged over last 500 steps = 4.5157e-01, PNorm = 57.9081, GNorm = 0.1356
Meta loss on this task batch = 4.3341e-01, Meta loss averaged over last 500 steps = 4.5147e-01, PNorm = 57.9189, GNorm = 0.1518
Meta loss on this task batch = 3.8800e-01, Meta loss averaged over last 500 steps = 4.5128e-01, PNorm = 57.9303, GNorm = 0.1403
Meta loss on this task batch = 4.3742e-01, Meta loss averaged over last 500 steps = 4.5126e-01, PNorm = 57.9426, GNorm = 0.1643
Meta loss on this task batch = 4.0461e-01, Meta loss averaged over last 500 steps = 4.5106e-01, PNorm = 57.9570, GNorm = 0.1734
Meta loss on this task batch = 4.4853e-01, Meta loss averaged over last 500 steps = 4.5106e-01, PNorm = 57.9699, GNorm = 0.1868
Meta loss on this task batch = 4.2617e-01, Meta loss averaged over last 500 steps = 4.5109e-01, PNorm = 57.9819, GNorm = 0.1648
Meta loss on this task batch = 4.4674e-01, Meta loss averaged over last 500 steps = 4.5110e-01, PNorm = 57.9935, GNorm = 0.2372
Took 126.35011267662048 seconds to complete one epoch of meta training
Took 134.4318015575409 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464411
Epoch 41
Meta loss on this task batch = 4.3585e-01, Meta loss averaged over last 500 steps = 4.5100e-01, PNorm = 58.0054, GNorm = 0.2050
Meta loss on this task batch = 4.5205e-01, Meta loss averaged over last 500 steps = 4.5097e-01, PNorm = 58.0175, GNorm = 0.1725
Meta loss on this task batch = 4.4838e-01, Meta loss averaged over last 500 steps = 4.5083e-01, PNorm = 58.0301, GNorm = 0.1980
Meta loss on this task batch = 3.5522e-01, Meta loss averaged over last 500 steps = 4.5061e-01, PNorm = 58.0437, GNorm = 0.1431
Meta loss on this task batch = 4.1731e-01, Meta loss averaged over last 500 steps = 4.5055e-01, PNorm = 58.0578, GNorm = 0.1628
Meta loss on this task batch = 4.0123e-01, Meta loss averaged over last 500 steps = 4.5035e-01, PNorm = 58.0724, GNorm = 0.1632
Meta loss on this task batch = 4.4076e-01, Meta loss averaged over last 500 steps = 4.5033e-01, PNorm = 58.0874, GNorm = 0.1399
Meta loss on this task batch = 4.8846e-01, Meta loss averaged over last 500 steps = 4.5036e-01, PNorm = 58.1023, GNorm = 0.2013
Meta loss on this task batch = 4.3413e-01, Meta loss averaged over last 500 steps = 4.5036e-01, PNorm = 58.1146, GNorm = 0.2630
Meta loss on this task batch = 3.8776e-01, Meta loss averaged over last 500 steps = 4.5030e-01, PNorm = 58.1247, GNorm = 0.1899
Meta loss on this task batch = 4.4384e-01, Meta loss averaged over last 500 steps = 4.5022e-01, PNorm = 58.1345, GNorm = 0.1367
Meta loss on this task batch = 4.6285e-01, Meta loss averaged over last 500 steps = 4.5021e-01, PNorm = 58.1452, GNorm = 0.1576
Meta loss on this task batch = 4.2433e-01, Meta loss averaged over last 500 steps = 4.5003e-01, PNorm = 58.1556, GNorm = 0.1724
Meta loss on this task batch = 4.5787e-01, Meta loss averaged over last 500 steps = 4.4995e-01, PNorm = 58.1662, GNorm = 0.1771
Meta loss on this task batch = 5.0266e-01, Meta loss averaged over last 500 steps = 4.4998e-01, PNorm = 58.1766, GNorm = 0.1817
Meta loss on this task batch = 3.9934e-01, Meta loss averaged over last 500 steps = 4.4991e-01, PNorm = 58.1882, GNorm = 0.1519
Meta loss on this task batch = 4.0759e-01, Meta loss averaged over last 500 steps = 4.4983e-01, PNorm = 58.2001, GNorm = 0.1596
Meta loss on this task batch = 3.8970e-01, Meta loss averaged over last 500 steps = 4.4964e-01, PNorm = 58.2105, GNorm = 0.1630
Meta loss on this task batch = 5.2777e-01, Meta loss averaged over last 500 steps = 4.4990e-01, PNorm = 58.2189, GNorm = 0.2514
Took 125.29828333854675 seconds to complete one epoch of meta training
Took 133.3430724143982 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.439896
Epoch 42
Meta loss on this task batch = 4.0168e-01, Meta loss averaged over last 500 steps = 4.4977e-01, PNorm = 58.2298, GNorm = 0.1400
Meta loss on this task batch = 4.8893e-01, Meta loss averaged over last 500 steps = 4.4987e-01, PNorm = 58.2423, GNorm = 0.2019
Meta loss on this task batch = 4.7048e-01, Meta loss averaged over last 500 steps = 4.4997e-01, PNorm = 58.2564, GNorm = 0.1794
Meta loss on this task batch = 3.6644e-01, Meta loss averaged over last 500 steps = 4.4980e-01, PNorm = 58.2722, GNorm = 0.1920
Meta loss on this task batch = 3.6424e-01, Meta loss averaged over last 500 steps = 4.4954e-01, PNorm = 58.2888, GNorm = 0.1392
Meta loss on this task batch = 4.4674e-01, Meta loss averaged over last 500 steps = 4.4931e-01, PNorm = 58.3047, GNorm = 0.1375
Meta loss on this task batch = 3.7322e-01, Meta loss averaged over last 500 steps = 4.4916e-01, PNorm = 58.3201, GNorm = 0.1384
Meta loss on this task batch = 4.5484e-01, Meta loss averaged over last 500 steps = 4.4923e-01, PNorm = 58.3345, GNorm = 0.1560
Meta loss on this task batch = 4.5427e-01, Meta loss averaged over last 500 steps = 4.4930e-01, PNorm = 58.3467, GNorm = 0.2276
Meta loss on this task batch = 4.6047e-01, Meta loss averaged over last 500 steps = 4.4934e-01, PNorm = 58.3588, GNorm = 0.1432
Meta loss on this task batch = 4.2907e-01, Meta loss averaged over last 500 steps = 4.4928e-01, PNorm = 58.3724, GNorm = 0.2463
Meta loss on this task batch = 4.7217e-01, Meta loss averaged over last 500 steps = 4.4932e-01, PNorm = 58.3878, GNorm = 0.1712
Meta loss on this task batch = 4.3063e-01, Meta loss averaged over last 500 steps = 4.4914e-01, PNorm = 58.4040, GNorm = 0.1583
Meta loss on this task batch = 3.5661e-01, Meta loss averaged over last 500 steps = 4.4890e-01, PNorm = 58.4187, GNorm = 0.1536
Meta loss on this task batch = 4.6939e-01, Meta loss averaged over last 500 steps = 4.4891e-01, PNorm = 58.4299, GNorm = 0.2804
Meta loss on this task batch = 4.8090e-01, Meta loss averaged over last 500 steps = 4.4887e-01, PNorm = 58.4391, GNorm = 0.2424
Meta loss on this task batch = 4.6280e-01, Meta loss averaged over last 500 steps = 4.4883e-01, PNorm = 58.4476, GNorm = 0.1813
Meta loss on this task batch = 5.0270e-01, Meta loss averaged over last 500 steps = 4.4897e-01, PNorm = 58.4576, GNorm = 0.1837
Meta loss on this task batch = 4.4200e-01, Meta loss averaged over last 500 steps = 4.4886e-01, PNorm = 58.4703, GNorm = 0.1927
Took 126.5341968536377 seconds to complete one epoch of meta training
Took 133.62822151184082 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461611
Epoch 43
Meta loss on this task batch = 4.1379e-01, Meta loss averaged over last 500 steps = 4.4872e-01, PNorm = 58.4856, GNorm = 0.2107
Meta loss on this task batch = 4.2420e-01, Meta loss averaged over last 500 steps = 4.4857e-01, PNorm = 58.5022, GNorm = 0.1696
Meta loss on this task batch = 4.5720e-01, Meta loss averaged over last 500 steps = 4.4874e-01, PNorm = 58.5194, GNorm = 0.1420
Meta loss on this task batch = 4.2085e-01, Meta loss averaged over last 500 steps = 4.4857e-01, PNorm = 58.5369, GNorm = 0.2000
Meta loss on this task batch = 4.8217e-01, Meta loss averaged over last 500 steps = 4.4858e-01, PNorm = 58.5516, GNorm = 0.2513
Meta loss on this task batch = 4.3477e-01, Meta loss averaged over last 500 steps = 4.4851e-01, PNorm = 58.5667, GNorm = 0.1618
Meta loss on this task batch = 3.8041e-01, Meta loss averaged over last 500 steps = 4.4848e-01, PNorm = 58.5813, GNorm = 0.1331
Meta loss on this task batch = 4.7352e-01, Meta loss averaged over last 500 steps = 4.4851e-01, PNorm = 58.5935, GNorm = 0.1623
Meta loss on this task batch = 4.4554e-01, Meta loss averaged over last 500 steps = 4.4839e-01, PNorm = 58.6064, GNorm = 0.1422
Meta loss on this task batch = 4.3442e-01, Meta loss averaged over last 500 steps = 4.4838e-01, PNorm = 58.6198, GNorm = 0.1531
Meta loss on this task batch = 4.2922e-01, Meta loss averaged over last 500 steps = 4.4828e-01, PNorm = 58.6337, GNorm = 0.1716
Meta loss on this task batch = 3.9895e-01, Meta loss averaged over last 500 steps = 4.4824e-01, PNorm = 58.6481, GNorm = 0.1430
Meta loss on this task batch = 4.6973e-01, Meta loss averaged over last 500 steps = 4.4816e-01, PNorm = 58.6610, GNorm = 0.1779
Meta loss on this task batch = 4.2737e-01, Meta loss averaged over last 500 steps = 4.4803e-01, PNorm = 58.6751, GNorm = 0.1449
Meta loss on this task batch = 4.0405e-01, Meta loss averaged over last 500 steps = 4.4786e-01, PNorm = 58.6900, GNorm = 0.1432
Meta loss on this task batch = 4.3786e-01, Meta loss averaged over last 500 steps = 4.4768e-01, PNorm = 58.7041, GNorm = 0.1977
Meta loss on this task batch = 4.5638e-01, Meta loss averaged over last 500 steps = 4.4778e-01, PNorm = 58.7192, GNorm = 0.1676
Meta loss on this task batch = 4.2224e-01, Meta loss averaged over last 500 steps = 4.4772e-01, PNorm = 58.7325, GNorm = 0.1565
Meta loss on this task batch = 4.7330e-01, Meta loss averaged over last 500 steps = 4.4792e-01, PNorm = 58.7452, GNorm = 0.1805
Took 125.24031400680542 seconds to complete one epoch of meta training
Took 133.41441130638123 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466168
Epoch 44
Meta loss on this task batch = 4.0154e-01, Meta loss averaged over last 500 steps = 4.4776e-01, PNorm = 58.7607, GNorm = 0.2226
Meta loss on this task batch = 4.4157e-01, Meta loss averaged over last 500 steps = 4.4769e-01, PNorm = 58.7738, GNorm = 0.1652
Meta loss on this task batch = 3.6006e-01, Meta loss averaged over last 500 steps = 4.4765e-01, PNorm = 58.7862, GNorm = 0.1595
Meta loss on this task batch = 4.4404e-01, Meta loss averaged over last 500 steps = 4.4753e-01, PNorm = 58.7965, GNorm = 0.1977
Meta loss on this task batch = 4.1537e-01, Meta loss averaged over last 500 steps = 4.4738e-01, PNorm = 58.8082, GNorm = 0.1403
Meta loss on this task batch = 3.7902e-01, Meta loss averaged over last 500 steps = 4.4715e-01, PNorm = 58.8191, GNorm = 0.1380
Meta loss on this task batch = 4.0754e-01, Meta loss averaged over last 500 steps = 4.4702e-01, PNorm = 58.8301, GNorm = 0.1404
Meta loss on this task batch = 4.3475e-01, Meta loss averaged over last 500 steps = 4.4697e-01, PNorm = 58.8391, GNorm = 0.1627
Meta loss on this task batch = 3.9227e-01, Meta loss averaged over last 500 steps = 4.4681e-01, PNorm = 58.8483, GNorm = 0.1613
Meta loss on this task batch = 4.5023e-01, Meta loss averaged over last 500 steps = 4.4684e-01, PNorm = 58.8587, GNorm = 0.1431
Meta loss on this task batch = 4.7553e-01, Meta loss averaged over last 500 steps = 4.4672e-01, PNorm = 58.8693, GNorm = 0.1454
Meta loss on this task batch = 4.6278e-01, Meta loss averaged over last 500 steps = 4.4671e-01, PNorm = 58.8808, GNorm = 0.1551
Meta loss on this task batch = 3.9745e-01, Meta loss averaged over last 500 steps = 4.4652e-01, PNorm = 58.8952, GNorm = 0.1738
Meta loss on this task batch = 4.3659e-01, Meta loss averaged over last 500 steps = 4.4661e-01, PNorm = 58.9104, GNorm = 0.1705
Meta loss on this task batch = 4.9595e-01, Meta loss averaged over last 500 steps = 4.4666e-01, PNorm = 58.9268, GNorm = 0.1331
Meta loss on this task batch = 4.5266e-01, Meta loss averaged over last 500 steps = 4.4649e-01, PNorm = 58.9436, GNorm = 0.1989
Meta loss on this task batch = 4.9783e-01, Meta loss averaged over last 500 steps = 4.4671e-01, PNorm = 58.9592, GNorm = 0.1690
Meta loss on this task batch = 5.1667e-01, Meta loss averaged over last 500 steps = 4.4677e-01, PNorm = 58.9731, GNorm = 0.2443
Meta loss on this task batch = 4.3399e-01, Meta loss averaged over last 500 steps = 4.4673e-01, PNorm = 58.9867, GNorm = 0.1944
Took 126.00964736938477 seconds to complete one epoch of meta training
Took 134.0127592086792 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483376
Epoch 45
Meta loss on this task batch = 4.8097e-01, Meta loss averaged over last 500 steps = 4.4669e-01, PNorm = 59.0015, GNorm = 0.1658
Meta loss on this task batch = 4.5363e-01, Meta loss averaged over last 500 steps = 4.4665e-01, PNorm = 59.0183, GNorm = 0.1474
Meta loss on this task batch = 4.3953e-01, Meta loss averaged over last 500 steps = 4.4668e-01, PNorm = 59.0347, GNorm = 0.1665
Meta loss on this task batch = 4.7318e-01, Meta loss averaged over last 500 steps = 4.4678e-01, PNorm = 59.0515, GNorm = 0.1753
Meta loss on this task batch = 4.1659e-01, Meta loss averaged over last 500 steps = 4.4680e-01, PNorm = 59.0658, GNorm = 0.1842
Meta loss on this task batch = 3.9783e-01, Meta loss averaged over last 500 steps = 4.4666e-01, PNorm = 59.0779, GNorm = 0.1507
Meta loss on this task batch = 4.6260e-01, Meta loss averaged over last 500 steps = 4.4655e-01, PNorm = 59.0872, GNorm = 0.1778
Meta loss on this task batch = 4.8686e-01, Meta loss averaged over last 500 steps = 4.4666e-01, PNorm = 59.0942, GNorm = 0.1797
Meta loss on this task batch = 3.7611e-01, Meta loss averaged over last 500 steps = 4.4628e-01, PNorm = 59.1042, GNorm = 0.1583
Meta loss on this task batch = 4.3656e-01, Meta loss averaged over last 500 steps = 4.4633e-01, PNorm = 59.1150, GNorm = 0.1403
Meta loss on this task batch = 4.6223e-01, Meta loss averaged over last 500 steps = 4.4621e-01, PNorm = 59.1250, GNorm = 0.1990
Meta loss on this task batch = 3.2194e-01, Meta loss averaged over last 500 steps = 4.4610e-01, PNorm = 59.1363, GNorm = 0.1305
Meta loss on this task batch = 4.0719e-01, Meta loss averaged over last 500 steps = 4.4589e-01, PNorm = 59.1481, GNorm = 0.1767
Meta loss on this task batch = 4.4214e-01, Meta loss averaged over last 500 steps = 4.4576e-01, PNorm = 59.1619, GNorm = 0.1847
Meta loss on this task batch = 3.9594e-01, Meta loss averaged over last 500 steps = 4.4572e-01, PNorm = 59.1752, GNorm = 0.1639
Meta loss on this task batch = 4.4103e-01, Meta loss averaged over last 500 steps = 4.4570e-01, PNorm = 59.1867, GNorm = 0.1826
Meta loss on this task batch = 4.4886e-01, Meta loss averaged over last 500 steps = 4.4559e-01, PNorm = 59.1987, GNorm = 0.1623
Meta loss on this task batch = 4.5118e-01, Meta loss averaged over last 500 steps = 4.4554e-01, PNorm = 59.2101, GNorm = 0.1584
Meta loss on this task batch = 3.9601e-01, Meta loss averaged over last 500 steps = 4.4542e-01, PNorm = 59.2243, GNorm = 0.2230
Took 126.24458503723145 seconds to complete one epoch of meta training
Took 135.07088088989258 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503015
Found better MAML checkpoint after meta validation, saving now
Epoch 46
Meta loss on this task batch = 4.4371e-01, Meta loss averaged over last 500 steps = 4.4531e-01, PNorm = 59.2396, GNorm = 0.1660
Meta loss on this task batch = 4.1087e-01, Meta loss averaged over last 500 steps = 4.4528e-01, PNorm = 59.2535, GNorm = 0.2307
Meta loss on this task batch = 4.1874e-01, Meta loss averaged over last 500 steps = 4.4514e-01, PNorm = 59.2664, GNorm = 0.2398
Meta loss on this task batch = 3.8918e-01, Meta loss averaged over last 500 steps = 4.4515e-01, PNorm = 59.2810, GNorm = 0.1992
Meta loss on this task batch = 4.1583e-01, Meta loss averaged over last 500 steps = 4.4522e-01, PNorm = 59.2951, GNorm = 0.1572
Meta loss on this task batch = 4.2942e-01, Meta loss averaged over last 500 steps = 4.4530e-01, PNorm = 59.3103, GNorm = 0.1747
Meta loss on this task batch = 4.2192e-01, Meta loss averaged over last 500 steps = 4.4533e-01, PNorm = 59.3266, GNorm = 0.1719
Meta loss on this task batch = 4.8116e-01, Meta loss averaged over last 500 steps = 4.4527e-01, PNorm = 59.3418, GNorm = 0.1463
Meta loss on this task batch = 4.3205e-01, Meta loss averaged over last 500 steps = 4.4523e-01, PNorm = 59.3551, GNorm = 0.1772
Meta loss on this task batch = 4.4468e-01, Meta loss averaged over last 500 steps = 4.4527e-01, PNorm = 59.3670, GNorm = 0.1917
Meta loss on this task batch = 4.8354e-01, Meta loss averaged over last 500 steps = 4.4523e-01, PNorm = 59.3759, GNorm = 0.2282
Meta loss on this task batch = 4.3399e-01, Meta loss averaged over last 500 steps = 4.4514e-01, PNorm = 59.3857, GNorm = 0.1631
Meta loss on this task batch = 4.2107e-01, Meta loss averaged over last 500 steps = 4.4502e-01, PNorm = 59.3943, GNorm = 0.1818
Meta loss on this task batch = 4.1505e-01, Meta loss averaged over last 500 steps = 4.4494e-01, PNorm = 59.4037, GNorm = 0.1740
Meta loss on this task batch = 4.4501e-01, Meta loss averaged over last 500 steps = 4.4487e-01, PNorm = 59.4137, GNorm = 0.2539
Meta loss on this task batch = 4.7758e-01, Meta loss averaged over last 500 steps = 4.4483e-01, PNorm = 59.4229, GNorm = 0.1773
Meta loss on this task batch = 4.6020e-01, Meta loss averaged over last 500 steps = 4.4492e-01, PNorm = 59.4331, GNorm = 0.1778
Meta loss on this task batch = 4.1426e-01, Meta loss averaged over last 500 steps = 4.4478e-01, PNorm = 59.4438, GNorm = 0.1692
Meta loss on this task batch = 3.5433e-01, Meta loss averaged over last 500 steps = 4.4455e-01, PNorm = 59.4558, GNorm = 0.1604
Took 126.81964230537415 seconds to complete one epoch of meta training
Took 133.8014361858368 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457559
Epoch 47
Meta loss on this task batch = 4.4782e-01, Meta loss averaged over last 500 steps = 4.4452e-01, PNorm = 59.4690, GNorm = 0.1632
Meta loss on this task batch = 3.9288e-01, Meta loss averaged over last 500 steps = 4.4449e-01, PNorm = 59.4838, GNorm = 0.1635
Meta loss on this task batch = 3.8911e-01, Meta loss averaged over last 500 steps = 4.4425e-01, PNorm = 59.4998, GNorm = 0.1558
Meta loss on this task batch = 4.0739e-01, Meta loss averaged over last 500 steps = 4.4419e-01, PNorm = 59.5169, GNorm = 0.1330
Meta loss on this task batch = 4.3562e-01, Meta loss averaged over last 500 steps = 4.4407e-01, PNorm = 59.5343, GNorm = 0.1754
Meta loss on this task batch = 4.4186e-01, Meta loss averaged over last 500 steps = 4.4413e-01, PNorm = 59.5504, GNorm = 0.1583
Meta loss on this task batch = 4.1959e-01, Meta loss averaged over last 500 steps = 4.4408e-01, PNorm = 59.5665, GNorm = 0.1559
Meta loss on this task batch = 4.4813e-01, Meta loss averaged over last 500 steps = 4.4408e-01, PNorm = 59.5816, GNorm = 0.1735
Meta loss on this task batch = 3.9378e-01, Meta loss averaged over last 500 steps = 4.4388e-01, PNorm = 59.5971, GNorm = 0.1761
Meta loss on this task batch = 4.8097e-01, Meta loss averaged over last 500 steps = 4.4392e-01, PNorm = 59.6117, GNorm = 0.1624
Meta loss on this task batch = 3.9820e-01, Meta loss averaged over last 500 steps = 4.4370e-01, PNorm = 59.6254, GNorm = 0.2014
Meta loss on this task batch = 3.6670e-01, Meta loss averaged over last 500 steps = 4.4350e-01, PNorm = 59.6392, GNorm = 0.1574
Meta loss on this task batch = 4.1472e-01, Meta loss averaged over last 500 steps = 4.4331e-01, PNorm = 59.6504, GNorm = 0.2886
Meta loss on this task batch = 4.2436e-01, Meta loss averaged over last 500 steps = 4.4336e-01, PNorm = 59.6602, GNorm = 0.1615
Meta loss on this task batch = 4.0488e-01, Meta loss averaged over last 500 steps = 4.4323e-01, PNorm = 59.6687, GNorm = 0.1813
Meta loss on this task batch = 4.9751e-01, Meta loss averaged over last 500 steps = 4.4331e-01, PNorm = 59.6780, GNorm = 0.2032
Meta loss on this task batch = 4.7501e-01, Meta loss averaged over last 500 steps = 4.4347e-01, PNorm = 59.6891, GNorm = 0.1778
Meta loss on this task batch = 3.9923e-01, Meta loss averaged over last 500 steps = 4.4337e-01, PNorm = 59.7022, GNorm = 0.2242
Meta loss on this task batch = 4.9178e-01, Meta loss averaged over last 500 steps = 4.4329e-01, PNorm = 59.7166, GNorm = 0.1974
Took 125.4838764667511 seconds to complete one epoch of meta training
Took 133.289306640625 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467146
Epoch 48
Meta loss on this task batch = 3.8987e-01, Meta loss averaged over last 500 steps = 4.4329e-01, PNorm = 59.7313, GNorm = 0.1702
Meta loss on this task batch = 4.2497e-01, Meta loss averaged over last 500 steps = 4.4322e-01, PNorm = 59.7464, GNorm = 0.1521
Meta loss on this task batch = 4.0314e-01, Meta loss averaged over last 500 steps = 4.4302e-01, PNorm = 59.7605, GNorm = 0.1616
Meta loss on this task batch = 4.6046e-01, Meta loss averaged over last 500 steps = 4.4306e-01, PNorm = 59.7726, GNorm = 0.2374
Meta loss on this task batch = 3.9788e-01, Meta loss averaged over last 500 steps = 4.4297e-01, PNorm = 59.7842, GNorm = 0.2163
Meta loss on this task batch = 4.1522e-01, Meta loss averaged over last 500 steps = 4.4304e-01, PNorm = 59.7940, GNorm = 0.1876
Meta loss on this task batch = 4.3174e-01, Meta loss averaged over last 500 steps = 4.4310e-01, PNorm = 59.8036, GNorm = 0.1635
Meta loss on this task batch = 4.4453e-01, Meta loss averaged over last 500 steps = 4.4288e-01, PNorm = 59.8126, GNorm = 0.1820
Meta loss on this task batch = 4.5165e-01, Meta loss averaged over last 500 steps = 4.4286e-01, PNorm = 59.8240, GNorm = 0.1380
Meta loss on this task batch = 4.4109e-01, Meta loss averaged over last 500 steps = 4.4270e-01, PNorm = 59.8365, GNorm = 0.1523
Meta loss on this task batch = 4.1587e-01, Meta loss averaged over last 500 steps = 4.4267e-01, PNorm = 59.8499, GNorm = 0.1460
Meta loss on this task batch = 4.5904e-01, Meta loss averaged over last 500 steps = 4.4267e-01, PNorm = 59.8635, GNorm = 0.1551
Meta loss on this task batch = 4.8040e-01, Meta loss averaged over last 500 steps = 4.4275e-01, PNorm = 59.8784, GNorm = 0.1701
Meta loss on this task batch = 4.1116e-01, Meta loss averaged over last 500 steps = 4.4270e-01, PNorm = 59.8947, GNorm = 0.1742
Meta loss on this task batch = 3.3924e-01, Meta loss averaged over last 500 steps = 4.4251e-01, PNorm = 59.9128, GNorm = 0.1466
Meta loss on this task batch = 4.1993e-01, Meta loss averaged over last 500 steps = 4.4255e-01, PNorm = 59.9306, GNorm = 0.1632
Meta loss on this task batch = 4.3909e-01, Meta loss averaged over last 500 steps = 4.4242e-01, PNorm = 59.9458, GNorm = 0.1564
Meta loss on this task batch = 5.0895e-01, Meta loss averaged over last 500 steps = 4.4244e-01, PNorm = 59.9600, GNorm = 0.1870
Meta loss on this task batch = 4.5232e-01, Meta loss averaged over last 500 steps = 4.4231e-01, PNorm = 59.9723, GNorm = 0.1915
Took 126.57974433898926 seconds to complete one epoch of meta training
Took 134.63948702812195 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466503
Epoch 49
Meta loss on this task batch = 4.3596e-01, Meta loss averaged over last 500 steps = 4.4230e-01, PNorm = 59.9832, GNorm = 0.2371
Meta loss on this task batch = 4.3674e-01, Meta loss averaged over last 500 steps = 4.4232e-01, PNorm = 59.9930, GNorm = 0.1613
Meta loss on this task batch = 4.5627e-01, Meta loss averaged over last 500 steps = 4.4236e-01, PNorm = 60.0029, GNorm = 0.1680
Meta loss on this task batch = 3.8506e-01, Meta loss averaged over last 500 steps = 4.4226e-01, PNorm = 60.0155, GNorm = 0.2036
Meta loss on this task batch = 4.5763e-01, Meta loss averaged over last 500 steps = 4.4227e-01, PNorm = 60.0286, GNorm = 0.1736
Meta loss on this task batch = 4.7109e-01, Meta loss averaged over last 500 steps = 4.4222e-01, PNorm = 60.0428, GNorm = 0.1716
Meta loss on this task batch = 4.1745e-01, Meta loss averaged over last 500 steps = 4.4221e-01, PNorm = 60.0549, GNorm = 0.1826
Meta loss on this task batch = 4.6407e-01, Meta loss averaged over last 500 steps = 4.4226e-01, PNorm = 60.0668, GNorm = 0.1787
Meta loss on this task batch = 3.9982e-01, Meta loss averaged over last 500 steps = 4.4207e-01, PNorm = 60.0766, GNorm = 0.1863
Meta loss on this task batch = 4.2150e-01, Meta loss averaged over last 500 steps = 4.4198e-01, PNorm = 60.0858, GNorm = 0.1672
Meta loss on this task batch = 4.3853e-01, Meta loss averaged over last 500 steps = 4.4198e-01, PNorm = 60.0972, GNorm = 0.1713
Meta loss on this task batch = 3.9755e-01, Meta loss averaged over last 500 steps = 4.4180e-01, PNorm = 60.1079, GNorm = 0.2133
Meta loss on this task batch = 4.6178e-01, Meta loss averaged over last 500 steps = 4.4192e-01, PNorm = 60.1199, GNorm = 0.1360
Meta loss on this task batch = 4.6711e-01, Meta loss averaged over last 500 steps = 4.4196e-01, PNorm = 60.1326, GNorm = 0.1586
Meta loss on this task batch = 4.6564e-01, Meta loss averaged over last 500 steps = 4.4203e-01, PNorm = 60.1457, GNorm = 0.1606
Meta loss on this task batch = 3.8743e-01, Meta loss averaged over last 500 steps = 4.4190e-01, PNorm = 60.1570, GNorm = 0.2210
Meta loss on this task batch = 3.9566e-01, Meta loss averaged over last 500 steps = 4.4161e-01, PNorm = 60.1686, GNorm = 0.1447
Meta loss on this task batch = 4.0682e-01, Meta loss averaged over last 500 steps = 4.4154e-01, PNorm = 60.1818, GNorm = 0.1400
Meta loss on this task batch = 4.3416e-01, Meta loss averaged over last 500 steps = 4.4157e-01, PNorm = 60.1965, GNorm = 0.1790
Took 128.6657350063324 seconds to complete one epoch of meta training
Took 136.581538438797 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498230
Epoch 50
Meta loss on this task batch = 4.2954e-01, Meta loss averaged over last 500 steps = 4.4146e-01, PNorm = 60.2113, GNorm = 0.1621
Meta loss on this task batch = 4.3622e-01, Meta loss averaged over last 500 steps = 4.4133e-01, PNorm = 60.2260, GNorm = 0.1673
Meta loss on this task batch = 4.3218e-01, Meta loss averaged over last 500 steps = 4.4123e-01, PNorm = 60.2396, GNorm = 0.1637
Meta loss on this task batch = 4.9891e-01, Meta loss averaged over last 500 steps = 4.4142e-01, PNorm = 60.2528, GNorm = 0.1777
Meta loss on this task batch = 4.6962e-01, Meta loss averaged over last 500 steps = 4.4141e-01, PNorm = 60.2661, GNorm = 0.1761
Meta loss on this task batch = 4.0815e-01, Meta loss averaged over last 500 steps = 4.4132e-01, PNorm = 60.2819, GNorm = 0.2055
Meta loss on this task batch = 3.9040e-01, Meta loss averaged over last 500 steps = 4.4126e-01, PNorm = 60.2970, GNorm = 0.1666
Meta loss on this task batch = 4.4217e-01, Meta loss averaged over last 500 steps = 4.4127e-01, PNorm = 60.3122, GNorm = 0.2148
Meta loss on this task batch = 3.8994e-01, Meta loss averaged over last 500 steps = 4.4121e-01, PNorm = 60.3267, GNorm = 0.1715
Meta loss on this task batch = 4.2927e-01, Meta loss averaged over last 500 steps = 4.4111e-01, PNorm = 60.3380, GNorm = 0.2341
Meta loss on this task batch = 4.1001e-01, Meta loss averaged over last 500 steps = 4.4094e-01, PNorm = 60.3495, GNorm = 0.1782
Meta loss on this task batch = 4.8925e-01, Meta loss averaged over last 500 steps = 4.4098e-01, PNorm = 60.3588, GNorm = 0.2952
Meta loss on this task batch = 3.5992e-01, Meta loss averaged over last 500 steps = 4.4090e-01, PNorm = 60.3712, GNorm = 0.2064
Meta loss on this task batch = 3.6889e-01, Meta loss averaged over last 500 steps = 4.4061e-01, PNorm = 60.3848, GNorm = 0.1551
Meta loss on this task batch = 4.1320e-01, Meta loss averaged over last 500 steps = 4.4049e-01, PNorm = 60.3983, GNorm = 0.1872
Meta loss on this task batch = 4.5059e-01, Meta loss averaged over last 500 steps = 4.4044e-01, PNorm = 60.4123, GNorm = 0.2160
Meta loss on this task batch = 3.7527e-01, Meta loss averaged over last 500 steps = 4.4033e-01, PNorm = 60.4269, GNorm = 0.1544
Meta loss on this task batch = 3.8459e-01, Meta loss averaged over last 500 steps = 4.4013e-01, PNorm = 60.4415, GNorm = 0.1718
Meta loss on this task batch = 4.3832e-01, Meta loss averaged over last 500 steps = 4.4012e-01, PNorm = 60.4560, GNorm = 0.2189
Took 125.51913475990295 seconds to complete one epoch of meta training
Took 133.32905673980713 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453666
Epoch 51
Meta loss on this task batch = 4.0689e-01, Meta loss averaged over last 500 steps = 4.3993e-01, PNorm = 60.4711, GNorm = 0.1641
Meta loss on this task batch = 4.7188e-01, Meta loss averaged over last 500 steps = 4.3990e-01, PNorm = 60.4860, GNorm = 0.1847
Meta loss on this task batch = 3.9046e-01, Meta loss averaged over last 500 steps = 4.3983e-01, PNorm = 60.5015, GNorm = 0.1537
Meta loss on this task batch = 4.0658e-01, Meta loss averaged over last 500 steps = 4.3977e-01, PNorm = 60.5177, GNorm = 0.1741
Meta loss on this task batch = 4.1962e-01, Meta loss averaged over last 500 steps = 4.3969e-01, PNorm = 60.5339, GNorm = 0.1672
Meta loss on this task batch = 4.5597e-01, Meta loss averaged over last 500 steps = 4.3968e-01, PNorm = 60.5474, GNorm = 0.2216
Meta loss on this task batch = 4.3705e-01, Meta loss averaged over last 500 steps = 4.3965e-01, PNorm = 60.5588, GNorm = 0.1917
Meta loss on this task batch = 4.6465e-01, Meta loss averaged over last 500 steps = 4.3971e-01, PNorm = 60.5686, GNorm = 0.2130
Meta loss on this task batch = 4.0889e-01, Meta loss averaged over last 500 steps = 4.3967e-01, PNorm = 60.5782, GNorm = 0.1948
Meta loss on this task batch = 3.9075e-01, Meta loss averaged over last 500 steps = 4.3954e-01, PNorm = 60.5882, GNorm = 0.2180
Meta loss on this task batch = 3.9779e-01, Meta loss averaged over last 500 steps = 4.3943e-01, PNorm = 60.6004, GNorm = 0.1779
Meta loss on this task batch = 4.6078e-01, Meta loss averaged over last 500 steps = 4.3952e-01, PNorm = 60.6101, GNorm = 0.2316
Meta loss on this task batch = 4.0946e-01, Meta loss averaged over last 500 steps = 4.3941e-01, PNorm = 60.6214, GNorm = 0.1422
Meta loss on this task batch = 4.9842e-01, Meta loss averaged over last 500 steps = 4.3953e-01, PNorm = 60.6338, GNorm = 0.1821
Meta loss on this task batch = 4.6245e-01, Meta loss averaged over last 500 steps = 4.3962e-01, PNorm = 60.6467, GNorm = 0.1761
Meta loss on this task batch = 4.7651e-01, Meta loss averaged over last 500 steps = 4.3977e-01, PNorm = 60.6584, GNorm = 0.2465
Meta loss on this task batch = 4.3772e-01, Meta loss averaged over last 500 steps = 4.3966e-01, PNorm = 60.6701, GNorm = 0.1696
Meta loss on this task batch = 5.1135e-01, Meta loss averaged over last 500 steps = 4.3981e-01, PNorm = 60.6820, GNorm = 0.2083
Meta loss on this task batch = 3.3689e-01, Meta loss averaged over last 500 steps = 4.3947e-01, PNorm = 60.6979, GNorm = 0.2442
Took 126.11072278022766 seconds to complete one epoch of meta training
Took 133.91271686553955 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494333
Epoch 52
Meta loss on this task batch = 5.0924e-01, Meta loss averaged over last 500 steps = 4.3964e-01, PNorm = 60.7149, GNorm = 0.1950
Meta loss on this task batch = 3.7971e-01, Meta loss averaged over last 500 steps = 4.3941e-01, PNorm = 60.7308, GNorm = 0.1484
Meta loss on this task batch = 4.5088e-01, Meta loss averaged over last 500 steps = 4.3941e-01, PNorm = 60.7449, GNorm = 0.2126
Meta loss on this task batch = 4.6105e-01, Meta loss averaged over last 500 steps = 4.3930e-01, PNorm = 60.7567, GNorm = 0.2127
Meta loss on this task batch = 4.8030e-01, Meta loss averaged over last 500 steps = 4.3935e-01, PNorm = 60.7667, GNorm = 0.2065
Meta loss on this task batch = 4.3484e-01, Meta loss averaged over last 500 steps = 4.3938e-01, PNorm = 60.7769, GNorm = 0.1571
Meta loss on this task batch = 3.9171e-01, Meta loss averaged over last 500 steps = 4.3938e-01, PNorm = 60.7897, GNorm = 0.1762
Meta loss on this task batch = 4.7160e-01, Meta loss averaged over last 500 steps = 4.3938e-01, PNorm = 60.8051, GNorm = 0.1913
Meta loss on this task batch = 3.8587e-01, Meta loss averaged over last 500 steps = 4.3934e-01, PNorm = 60.8223, GNorm = 0.1729
Meta loss on this task batch = 3.8069e-01, Meta loss averaged over last 500 steps = 4.3919e-01, PNorm = 60.8423, GNorm = 0.2327
Meta loss on this task batch = 4.6703e-01, Meta loss averaged over last 500 steps = 4.3921e-01, PNorm = 60.8601, GNorm = 0.1854
Meta loss on this task batch = 4.3989e-01, Meta loss averaged over last 500 steps = 4.3927e-01, PNorm = 60.8777, GNorm = 0.1943
Meta loss on this task batch = 4.3845e-01, Meta loss averaged over last 500 steps = 4.3918e-01, PNorm = 60.8930, GNorm = 0.2794
Meta loss on this task batch = 3.9157e-01, Meta loss averaged over last 500 steps = 4.3915e-01, PNorm = 60.9051, GNorm = 0.2073
Meta loss on this task batch = 3.7723e-01, Meta loss averaged over last 500 steps = 4.3898e-01, PNorm = 60.9177, GNorm = 0.1580
Meta loss on this task batch = 3.7564e-01, Meta loss averaged over last 500 steps = 4.3871e-01, PNorm = 60.9307, GNorm = 0.1988
Meta loss on this task batch = 4.6821e-01, Meta loss averaged over last 500 steps = 4.3879e-01, PNorm = 60.9439, GNorm = 0.1778
Meta loss on this task batch = 4.0757e-01, Meta loss averaged over last 500 steps = 4.3873e-01, PNorm = 60.9588, GNorm = 0.1725
Meta loss on this task batch = 4.7525e-01, Meta loss averaged over last 500 steps = 4.3864e-01, PNorm = 60.9744, GNorm = 0.2238
Took 125.71827936172485 seconds to complete one epoch of meta training
Took 133.64798259735107 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477422
Epoch 53
Meta loss on this task batch = 4.2347e-01, Meta loss averaged over last 500 steps = 4.3858e-01, PNorm = 60.9916, GNorm = 0.1879
Meta loss on this task batch = 3.9179e-01, Meta loss averaged over last 500 steps = 4.3853e-01, PNorm = 61.0113, GNorm = 0.1933
Meta loss on this task batch = 3.5487e-01, Meta loss averaged over last 500 steps = 4.3815e-01, PNorm = 61.0314, GNorm = 0.1623
Meta loss on this task batch = 4.6539e-01, Meta loss averaged over last 500 steps = 4.3815e-01, PNorm = 61.0480, GNorm = 0.2175
Meta loss on this task batch = 3.7345e-01, Meta loss averaged over last 500 steps = 4.3810e-01, PNorm = 61.0633, GNorm = 0.1805
Meta loss on this task batch = 5.0280e-01, Meta loss averaged over last 500 steps = 4.3818e-01, PNorm = 61.0769, GNorm = 0.2886
Meta loss on this task batch = 4.1266e-01, Meta loss averaged over last 500 steps = 4.3806e-01, PNorm = 61.0852, GNorm = 0.3052
Meta loss on this task batch = 4.4088e-01, Meta loss averaged over last 500 steps = 4.3819e-01, PNorm = 61.0962, GNorm = 0.2632
Meta loss on this task batch = 3.7660e-01, Meta loss averaged over last 500 steps = 4.3790e-01, PNorm = 61.1069, GNorm = 0.1367
Meta loss on this task batch = 4.7038e-01, Meta loss averaged over last 500 steps = 4.3805e-01, PNorm = 61.1172, GNorm = 0.1938
Meta loss on this task batch = 4.4342e-01, Meta loss averaged over last 500 steps = 4.3812e-01, PNorm = 61.1306, GNorm = 0.2422
Meta loss on this task batch = 3.9025e-01, Meta loss averaged over last 500 steps = 4.3789e-01, PNorm = 61.1454, GNorm = 0.2124
Meta loss on this task batch = 4.4298e-01, Meta loss averaged over last 500 steps = 4.3780e-01, PNorm = 61.1610, GNorm = 0.1915
Meta loss on this task batch = 4.9303e-01, Meta loss averaged over last 500 steps = 4.3776e-01, PNorm = 61.1761, GNorm = 0.2167
Meta loss on this task batch = 4.6130e-01, Meta loss averaged over last 500 steps = 4.3781e-01, PNorm = 61.1904, GNorm = 0.1733
Meta loss on this task batch = 4.3842e-01, Meta loss averaged over last 500 steps = 4.3775e-01, PNorm = 61.2022, GNorm = 0.2474
Meta loss on this task batch = 4.3397e-01, Meta loss averaged over last 500 steps = 4.3779e-01, PNorm = 61.2134, GNorm = 0.2004
Meta loss on this task batch = 3.3487e-01, Meta loss averaged over last 500 steps = 4.3762e-01, PNorm = 61.2259, GNorm = 0.1713
Meta loss on this task batch = 4.5864e-01, Meta loss averaged over last 500 steps = 4.3767e-01, PNorm = 61.2369, GNorm = 0.2061
Took 125.65176320075989 seconds to complete one epoch of meta training
Took 133.23738193511963 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481837
Epoch 54
Meta loss on this task batch = 3.7497e-01, Meta loss averaged over last 500 steps = 4.3746e-01, PNorm = 61.2454, GNorm = 0.2574
Meta loss on this task batch = 4.3553e-01, Meta loss averaged over last 500 steps = 4.3739e-01, PNorm = 61.2544, GNorm = 0.1772
Meta loss on this task batch = 4.1048e-01, Meta loss averaged over last 500 steps = 4.3726e-01, PNorm = 61.2651, GNorm = 0.1648
Meta loss on this task batch = 4.2164e-01, Meta loss averaged over last 500 steps = 4.3713e-01, PNorm = 61.2767, GNorm = 0.1834
Meta loss on this task batch = 4.4475e-01, Meta loss averaged over last 500 steps = 4.3711e-01, PNorm = 61.2883, GNorm = 0.1473
Meta loss on this task batch = 4.6519e-01, Meta loss averaged over last 500 steps = 4.3710e-01, PNorm = 61.2998, GNorm = 0.1783
Meta loss on this task batch = 4.4702e-01, Meta loss averaged over last 500 steps = 4.3707e-01, PNorm = 61.3109, GNorm = 0.2549
Meta loss on this task batch = 4.0242e-01, Meta loss averaged over last 500 steps = 4.3695e-01, PNorm = 61.3239, GNorm = 0.1630
Meta loss on this task batch = 3.8918e-01, Meta loss averaged over last 500 steps = 4.3680e-01, PNorm = 61.3385, GNorm = 0.1840
Meta loss on this task batch = 4.4884e-01, Meta loss averaged over last 500 steps = 4.3676e-01, PNorm = 61.3555, GNorm = 0.2042
Meta loss on this task batch = 3.7380e-01, Meta loss averaged over last 500 steps = 4.3671e-01, PNorm = 61.3740, GNorm = 0.1969
Meta loss on this task batch = 3.9378e-01, Meta loss averaged over last 500 steps = 4.3656e-01, PNorm = 61.3918, GNorm = 0.1483
Meta loss on this task batch = 4.4996e-01, Meta loss averaged over last 500 steps = 4.3663e-01, PNorm = 61.4082, GNorm = 0.1822
Meta loss on this task batch = 4.2870e-01, Meta loss averaged over last 500 steps = 4.3663e-01, PNorm = 61.4212, GNorm = 0.2584
Meta loss on this task batch = 4.2637e-01, Meta loss averaged over last 500 steps = 4.3650e-01, PNorm = 61.4332, GNorm = 0.1817
Meta loss on this task batch = 4.3978e-01, Meta loss averaged over last 500 steps = 4.3661e-01, PNorm = 61.4454, GNorm = 0.2190
Meta loss on this task batch = 4.2422e-01, Meta loss averaged over last 500 steps = 4.3657e-01, PNorm = 61.4573, GNorm = 0.1456
Meta loss on this task batch = 3.7867e-01, Meta loss averaged over last 500 steps = 4.3641e-01, PNorm = 61.4691, GNorm = 0.1567
Meta loss on this task batch = 4.3359e-01, Meta loss averaged over last 500 steps = 4.3635e-01, PNorm = 61.4817, GNorm = 0.1928
Took 126.00351333618164 seconds to complete one epoch of meta training
Took 133.82056307792664 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495666
Epoch 55
Meta loss on this task batch = 4.3237e-01, Meta loss averaged over last 500 steps = 4.3629e-01, PNorm = 61.4930, GNorm = 0.1916
Meta loss on this task batch = 4.6459e-01, Meta loss averaged over last 500 steps = 4.3628e-01, PNorm = 61.5045, GNorm = 0.1723
Meta loss on this task batch = 4.1138e-01, Meta loss averaged over last 500 steps = 4.3622e-01, PNorm = 61.5163, GNorm = 0.1768
Meta loss on this task batch = 3.8119e-01, Meta loss averaged over last 500 steps = 4.3612e-01, PNorm = 61.5283, GNorm = 0.1664
Meta loss on this task batch = 4.4616e-01, Meta loss averaged over last 500 steps = 4.3599e-01, PNorm = 61.5391, GNorm = 0.1650
Meta loss on this task batch = 4.5421e-01, Meta loss averaged over last 500 steps = 4.3593e-01, PNorm = 61.5484, GNorm = 0.1689
Meta loss on this task batch = 4.1605e-01, Meta loss averaged over last 500 steps = 4.3587e-01, PNorm = 61.5590, GNorm = 0.1486
Meta loss on this task batch = 4.1185e-01, Meta loss averaged over last 500 steps = 4.3564e-01, PNorm = 61.5695, GNorm = 0.1265
Meta loss on this task batch = 4.0646e-01, Meta loss averaged over last 500 steps = 4.3558e-01, PNorm = 61.5798, GNorm = 0.1541
Meta loss on this task batch = 4.0685e-01, Meta loss averaged over last 500 steps = 4.3543e-01, PNorm = 61.5910, GNorm = 0.1583
Meta loss on this task batch = 3.8405e-01, Meta loss averaged over last 500 steps = 4.3526e-01, PNorm = 61.6034, GNorm = 0.1565
Meta loss on this task batch = 4.1172e-01, Meta loss averaged over last 500 steps = 4.3525e-01, PNorm = 61.6153, GNorm = 0.1738
Meta loss on this task batch = 5.1960e-01, Meta loss averaged over last 500 steps = 4.3535e-01, PNorm = 61.6247, GNorm = 0.2129
Meta loss on this task batch = 4.1708e-01, Meta loss averaged over last 500 steps = 4.3529e-01, PNorm = 61.6343, GNorm = 0.1651
Meta loss on this task batch = 4.0848e-01, Meta loss averaged over last 500 steps = 4.3516e-01, PNorm = 61.6444, GNorm = 0.1841
Meta loss on this task batch = 3.8593e-01, Meta loss averaged over last 500 steps = 4.3506e-01, PNorm = 61.6569, GNorm = 0.1834
Meta loss on this task batch = 4.5471e-01, Meta loss averaged over last 500 steps = 4.3519e-01, PNorm = 61.6700, GNorm = 0.1958
Meta loss on this task batch = 4.4302e-01, Meta loss averaged over last 500 steps = 4.3520e-01, PNorm = 61.6842, GNorm = 0.1674
Meta loss on this task batch = 4.0439e-01, Meta loss averaged over last 500 steps = 4.3518e-01, PNorm = 61.6971, GNorm = 0.1595
Took 128.56464457511902 seconds to complete one epoch of meta training
Took 136.70599746704102 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500832
Epoch 56
Meta loss on this task batch = 4.4169e-01, Meta loss averaged over last 500 steps = 4.3513e-01, PNorm = 61.7099, GNorm = 0.1588
Meta loss on this task batch = 3.6875e-01, Meta loss averaged over last 500 steps = 4.3500e-01, PNorm = 61.7231, GNorm = 0.1378
Meta loss on this task batch = 4.1993e-01, Meta loss averaged over last 500 steps = 4.3502e-01, PNorm = 61.7363, GNorm = 0.1835
Meta loss on this task batch = 3.4500e-01, Meta loss averaged over last 500 steps = 4.3488e-01, PNorm = 61.7488, GNorm = 0.1326
Meta loss on this task batch = 4.0637e-01, Meta loss averaged over last 500 steps = 4.3476e-01, PNorm = 61.7615, GNorm = 0.1682
Meta loss on this task batch = 4.3765e-01, Meta loss averaged over last 500 steps = 4.3489e-01, PNorm = 61.7741, GNorm = 0.1989
Meta loss on this task batch = 4.9020e-01, Meta loss averaged over last 500 steps = 4.3502e-01, PNorm = 61.7869, GNorm = 0.1975
Meta loss on this task batch = 4.2595e-01, Meta loss averaged over last 500 steps = 4.3492e-01, PNorm = 61.7992, GNorm = 0.1683
Meta loss on this task batch = 4.5749e-01, Meta loss averaged over last 500 steps = 4.3483e-01, PNorm = 61.8122, GNorm = 0.1764
Meta loss on this task batch = 3.8053e-01, Meta loss averaged over last 500 steps = 4.3485e-01, PNorm = 61.8246, GNorm = 0.1955
Meta loss on this task batch = 4.4523e-01, Meta loss averaged over last 500 steps = 4.3479e-01, PNorm = 61.8373, GNorm = 0.1847
Meta loss on this task batch = 4.0268e-01, Meta loss averaged over last 500 steps = 4.3459e-01, PNorm = 61.8504, GNorm = 0.2032
Meta loss on this task batch = 4.0633e-01, Meta loss averaged over last 500 steps = 4.3462e-01, PNorm = 61.8617, GNorm = 0.2297
Meta loss on this task batch = 4.2108e-01, Meta loss averaged over last 500 steps = 4.3450e-01, PNorm = 61.8740, GNorm = 0.1876
Meta loss on this task batch = 3.8954e-01, Meta loss averaged over last 500 steps = 4.3440e-01, PNorm = 61.8864, GNorm = 0.1881
Meta loss on this task batch = 3.7900e-01, Meta loss averaged over last 500 steps = 4.3425e-01, PNorm = 61.8964, GNorm = 0.1857
Meta loss on this task batch = 4.3705e-01, Meta loss averaged over last 500 steps = 4.3449e-01, PNorm = 61.9052, GNorm = 0.1826
Meta loss on this task batch = 4.2902e-01, Meta loss averaged over last 500 steps = 4.3434e-01, PNorm = 61.9150, GNorm = 0.2082
Meta loss on this task batch = 4.3411e-01, Meta loss averaged over last 500 steps = 4.3426e-01, PNorm = 61.9268, GNorm = 0.2555
Took 126.75016045570374 seconds to complete one epoch of meta training
Took 134.5174400806427 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481019
Epoch 57
Meta loss on this task batch = 4.2639e-01, Meta loss averaged over last 500 steps = 4.3436e-01, PNorm = 61.9383, GNorm = 0.1916
Meta loss on this task batch = 3.9333e-01, Meta loss averaged over last 500 steps = 4.3433e-01, PNorm = 61.9508, GNorm = 0.2047
Meta loss on this task batch = 4.2000e-01, Meta loss averaged over last 500 steps = 4.3427e-01, PNorm = 61.9628, GNorm = 0.1933
Meta loss on this task batch = 4.1225e-01, Meta loss averaged over last 500 steps = 4.3417e-01, PNorm = 61.9759, GNorm = 0.1741
Meta loss on this task batch = 3.9604e-01, Meta loss averaged over last 500 steps = 4.3407e-01, PNorm = 61.9905, GNorm = 0.1823
Meta loss on this task batch = 4.9005e-01, Meta loss averaged over last 500 steps = 4.3413e-01, PNorm = 62.0032, GNorm = 0.1711
Meta loss on this task batch = 4.0695e-01, Meta loss averaged over last 500 steps = 4.3403e-01, PNorm = 62.0155, GNorm = 0.1526
Meta loss on this task batch = 4.7171e-01, Meta loss averaged over last 500 steps = 4.3416e-01, PNorm = 62.0256, GNorm = 0.2918
Meta loss on this task batch = 4.1310e-01, Meta loss averaged over last 500 steps = 4.3401e-01, PNorm = 62.0343, GNorm = 0.1911
Meta loss on this task batch = 4.0672e-01, Meta loss averaged over last 500 steps = 4.3399e-01, PNorm = 62.0440, GNorm = 0.1901
Meta loss on this task batch = 4.2672e-01, Meta loss averaged over last 500 steps = 4.3386e-01, PNorm = 62.0551, GNorm = 0.1909
Meta loss on this task batch = 3.8425e-01, Meta loss averaged over last 500 steps = 4.3373e-01, PNorm = 62.0690, GNorm = 0.1660
Meta loss on this task batch = 3.2313e-01, Meta loss averaged over last 500 steps = 4.3342e-01, PNorm = 62.0841, GNorm = 0.1748
Meta loss on this task batch = 3.9811e-01, Meta loss averaged over last 500 steps = 4.3349e-01, PNorm = 62.1000, GNorm = 0.1661
Meta loss on this task batch = 4.3257e-01, Meta loss averaged over last 500 steps = 4.3349e-01, PNorm = 62.1149, GNorm = 0.2185
Meta loss on this task batch = 5.2472e-01, Meta loss averaged over last 500 steps = 4.3365e-01, PNorm = 62.1291, GNorm = 0.2081
Meta loss on this task batch = 3.8047e-01, Meta loss averaged over last 500 steps = 4.3350e-01, PNorm = 62.1437, GNorm = 0.1704
Meta loss on this task batch = 4.4522e-01, Meta loss averaged over last 500 steps = 4.3344e-01, PNorm = 62.1594, GNorm = 0.1814
Meta loss on this task batch = 4.4596e-01, Meta loss averaged over last 500 steps = 4.3349e-01, PNorm = 62.1755, GNorm = 0.2101
Took 128.0928976535797 seconds to complete one epoch of meta training
Took 136.0525999069214 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511941
Found better MAML checkpoint after meta validation, saving now
Epoch 58
Meta loss on this task batch = 4.2376e-01, Meta loss averaged over last 500 steps = 4.3328e-01, PNorm = 62.1931, GNorm = 0.2084
Meta loss on this task batch = 3.7557e-01, Meta loss averaged over last 500 steps = 4.3327e-01, PNorm = 62.2096, GNorm = 0.1802
Meta loss on this task batch = 4.3587e-01, Meta loss averaged over last 500 steps = 4.3313e-01, PNorm = 62.2248, GNorm = 0.1790
Meta loss on this task batch = 4.3478e-01, Meta loss averaged over last 500 steps = 4.3310e-01, PNorm = 62.2396, GNorm = 0.1701
Meta loss on this task batch = 5.0237e-01, Meta loss averaged over last 500 steps = 4.3307e-01, PNorm = 62.2509, GNorm = 0.2398
Meta loss on this task batch = 3.9693e-01, Meta loss averaged over last 500 steps = 4.3306e-01, PNorm = 62.2619, GNorm = 0.1469
Meta loss on this task batch = 4.3100e-01, Meta loss averaged over last 500 steps = 4.3309e-01, PNorm = 62.2719, GNorm = 0.1812
Meta loss on this task batch = 5.0737e-01, Meta loss averaged over last 500 steps = 4.3317e-01, PNorm = 62.2809, GNorm = 0.2033
Meta loss on this task batch = 3.9275e-01, Meta loss averaged over last 500 steps = 4.3301e-01, PNorm = 62.2911, GNorm = 0.2095
Meta loss on this task batch = 3.7355e-01, Meta loss averaged over last 500 steps = 4.3283e-01, PNorm = 62.3039, GNorm = 0.1907
Meta loss on this task batch = 3.5613e-01, Meta loss averaged over last 500 steps = 4.3258e-01, PNorm = 62.3186, GNorm = 0.1801
Meta loss on this task batch = 3.9579e-01, Meta loss averaged over last 500 steps = 4.3239e-01, PNorm = 62.3348, GNorm = 0.1754
Meta loss on this task batch = 4.3033e-01, Meta loss averaged over last 500 steps = 4.3229e-01, PNorm = 62.3498, GNorm = 0.1800
Meta loss on this task batch = 4.3728e-01, Meta loss averaged over last 500 steps = 4.3234e-01, PNorm = 62.3651, GNorm = 0.2233
Meta loss on this task batch = 3.9082e-01, Meta loss averaged over last 500 steps = 4.3231e-01, PNorm = 62.3815, GNorm = 0.1538
Meta loss on this task batch = 4.6974e-01, Meta loss averaged over last 500 steps = 4.3237e-01, PNorm = 62.3976, GNorm = 0.1900
Meta loss on this task batch = 4.1338e-01, Meta loss averaged over last 500 steps = 4.3234e-01, PNorm = 62.4110, GNorm = 0.1693
Meta loss on this task batch = 3.8221e-01, Meta loss averaged over last 500 steps = 4.3224e-01, PNorm = 62.4252, GNorm = 0.1609
Meta loss on this task batch = 4.4112e-01, Meta loss averaged over last 500 steps = 4.3223e-01, PNorm = 62.4409, GNorm = 0.2892
Took 127.51511454582214 seconds to complete one epoch of meta training
Took 135.30162596702576 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496419
Epoch 59
Meta loss on this task batch = 4.0379e-01, Meta loss averaged over last 500 steps = 4.3220e-01, PNorm = 62.4572, GNorm = 0.1685
Meta loss on this task batch = 4.1444e-01, Meta loss averaged over last 500 steps = 4.3211e-01, PNorm = 62.4742, GNorm = 0.1984
Meta loss on this task batch = 4.9156e-01, Meta loss averaged over last 500 steps = 4.3215e-01, PNorm = 62.4905, GNorm = 0.2051
Meta loss on this task batch = 4.3428e-01, Meta loss averaged over last 500 steps = 4.3222e-01, PNorm = 62.5049, GNorm = 0.2089
Meta loss on this task batch = 3.8504e-01, Meta loss averaged over last 500 steps = 4.3214e-01, PNorm = 62.5184, GNorm = 0.2044
Meta loss on this task batch = 4.6585e-01, Meta loss averaged over last 500 steps = 4.3217e-01, PNorm = 62.5313, GNorm = 0.1908
Meta loss on this task batch = 3.8398e-01, Meta loss averaged over last 500 steps = 4.3197e-01, PNorm = 62.5452, GNorm = 0.1868
Meta loss on this task batch = 4.7775e-01, Meta loss averaged over last 500 steps = 4.3199e-01, PNorm = 62.5574, GNorm = 0.2130
Meta loss on this task batch = 4.0356e-01, Meta loss averaged over last 500 steps = 4.3190e-01, PNorm = 62.5711, GNorm = 0.2053
Meta loss on this task batch = 3.7336e-01, Meta loss averaged over last 500 steps = 4.3189e-01, PNorm = 62.5845, GNorm = 0.1489
Meta loss on this task batch = 4.3539e-01, Meta loss averaged over last 500 steps = 4.3187e-01, PNorm = 62.5962, GNorm = 0.2140
Meta loss on this task batch = 3.8619e-01, Meta loss averaged over last 500 steps = 4.3180e-01, PNorm = 62.6078, GNorm = 0.1697
Meta loss on this task batch = 3.8935e-01, Meta loss averaged over last 500 steps = 4.3157e-01, PNorm = 62.6194, GNorm = 0.1930
Meta loss on this task batch = 4.4466e-01, Meta loss averaged over last 500 steps = 4.3158e-01, PNorm = 62.6306, GNorm = 0.2521
Meta loss on this task batch = 4.3231e-01, Meta loss averaged over last 500 steps = 4.3169e-01, PNorm = 62.6402, GNorm = 0.1750
Meta loss on this task batch = 3.8141e-01, Meta loss averaged over last 500 steps = 4.3158e-01, PNorm = 62.6506, GNorm = 0.1691
Meta loss on this task batch = 4.2787e-01, Meta loss averaged over last 500 steps = 4.3152e-01, PNorm = 62.6608, GNorm = 0.1693
Meta loss on this task batch = 4.6849e-01, Meta loss averaged over last 500 steps = 4.3161e-01, PNorm = 62.6715, GNorm = 0.1952
Meta loss on this task batch = 3.9484e-01, Meta loss averaged over last 500 steps = 4.3148e-01, PNorm = 62.6840, GNorm = 0.2056
Took 125.1721453666687 seconds to complete one epoch of meta training
Took 132.89819645881653 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500720
Epoch 60
Meta loss on this task batch = 4.2039e-01, Meta loss averaged over last 500 steps = 4.3149e-01, PNorm = 62.6973, GNorm = 0.1597
Meta loss on this task batch = 4.2197e-01, Meta loss averaged over last 500 steps = 4.3147e-01, PNorm = 62.7111, GNorm = 0.1441
Meta loss on this task batch = 3.7560e-01, Meta loss averaged over last 500 steps = 4.3144e-01, PNorm = 62.7247, GNorm = 0.1528
Meta loss on this task batch = 3.9719e-01, Meta loss averaged over last 500 steps = 4.3122e-01, PNorm = 62.7385, GNorm = 0.1513
Meta loss on this task batch = 3.8944e-01, Meta loss averaged over last 500 steps = 4.3103e-01, PNorm = 62.7539, GNorm = 0.1797
Meta loss on this task batch = 4.3637e-01, Meta loss averaged over last 500 steps = 4.3104e-01, PNorm = 62.7692, GNorm = 0.1548
Meta loss on this task batch = 3.6622e-01, Meta loss averaged over last 500 steps = 4.3093e-01, PNorm = 62.7854, GNorm = 0.1564
Meta loss on this task batch = 4.2865e-01, Meta loss averaged over last 500 steps = 4.3085e-01, PNorm = 62.7987, GNorm = 0.2469
Meta loss on this task batch = 4.3882e-01, Meta loss averaged over last 500 steps = 4.3069e-01, PNorm = 62.8112, GNorm = 0.1857
Meta loss on this task batch = 4.4458e-01, Meta loss averaged over last 500 steps = 4.3066e-01, PNorm = 62.8234, GNorm = 0.1713
Meta loss on this task batch = 4.2007e-01, Meta loss averaged over last 500 steps = 4.3062e-01, PNorm = 62.8350, GNorm = 0.1853
Meta loss on this task batch = 4.2195e-01, Meta loss averaged over last 500 steps = 4.3052e-01, PNorm = 62.8477, GNorm = 0.1867
Meta loss on this task batch = 4.8415e-01, Meta loss averaged over last 500 steps = 4.3062e-01, PNorm = 62.8607, GNorm = 0.2099
Meta loss on this task batch = 4.8980e-01, Meta loss averaged over last 500 steps = 4.3073e-01, PNorm = 62.8733, GNorm = 0.2282
Meta loss on this task batch = 4.5682e-01, Meta loss averaged over last 500 steps = 4.3071e-01, PNorm = 62.8880, GNorm = 0.2174
Meta loss on this task batch = 4.1377e-01, Meta loss averaged over last 500 steps = 4.3070e-01, PNorm = 62.9031, GNorm = 0.1713
Meta loss on this task batch = 3.6988e-01, Meta loss averaged over last 500 steps = 4.3050e-01, PNorm = 62.9179, GNorm = 0.1889
Meta loss on this task batch = 3.9517e-01, Meta loss averaged over last 500 steps = 4.3040e-01, PNorm = 62.9305, GNorm = 0.1803
Meta loss on this task batch = 4.2331e-01, Meta loss averaged over last 500 steps = 4.3036e-01, PNorm = 62.9414, GNorm = 0.2269
Took 126.20249390602112 seconds to complete one epoch of meta training
Took 134.1461477279663 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493537
Epoch 61
Meta loss on this task batch = 4.6451e-01, Meta loss averaged over last 500 steps = 4.3050e-01, PNorm = 62.9491, GNorm = 0.4740
Meta loss on this task batch = 4.4943e-01, Meta loss averaged over last 500 steps = 4.3058e-01, PNorm = 62.9563, GNorm = 0.2220
Meta loss on this task batch = 4.1187e-01, Meta loss averaged over last 500 steps = 4.3067e-01, PNorm = 62.9661, GNorm = 0.3597
Meta loss on this task batch = 4.6113e-01, Meta loss averaged over last 500 steps = 4.3067e-01, PNorm = 62.9783, GNorm = 0.1688
Meta loss on this task batch = 4.3020e-01, Meta loss averaged over last 500 steps = 4.3070e-01, PNorm = 62.9952, GNorm = 0.2687
Meta loss on this task batch = 4.4099e-01, Meta loss averaged over last 500 steps = 4.3067e-01, PNorm = 63.0124, GNorm = 0.2002
Meta loss on this task batch = 4.5617e-01, Meta loss averaged over last 500 steps = 4.3073e-01, PNorm = 63.0287, GNorm = 0.2488
Meta loss on this task batch = 4.0866e-01, Meta loss averaged over last 500 steps = 4.3061e-01, PNorm = 63.0453, GNorm = 0.1772
Meta loss on this task batch = 4.1841e-01, Meta loss averaged over last 500 steps = 4.3056e-01, PNorm = 63.0594, GNorm = 0.2231
Meta loss on this task batch = 4.0202e-01, Meta loss averaged over last 500 steps = 4.3046e-01, PNorm = 63.0722, GNorm = 0.1911
Meta loss on this task batch = 4.4478e-01, Meta loss averaged over last 500 steps = 4.3058e-01, PNorm = 63.0839, GNorm = 0.1965
Meta loss on this task batch = 3.5898e-01, Meta loss averaged over last 500 steps = 4.3048e-01, PNorm = 63.0953, GNorm = 0.1619
Meta loss on this task batch = 4.4237e-01, Meta loss averaged over last 500 steps = 4.3048e-01, PNorm = 63.1068, GNorm = 0.1722
Meta loss on this task batch = 4.6804e-01, Meta loss averaged over last 500 steps = 4.3061e-01, PNorm = 63.1193, GNorm = 0.1913
Meta loss on this task batch = 4.0108e-01, Meta loss averaged over last 500 steps = 4.3045e-01, PNorm = 63.1319, GNorm = 0.1790
Meta loss on this task batch = 4.5666e-01, Meta loss averaged over last 500 steps = 4.3049e-01, PNorm = 63.1417, GNorm = 0.2692
Meta loss on this task batch = 4.2095e-01, Meta loss averaged over last 500 steps = 4.3041e-01, PNorm = 63.1485, GNorm = 0.2521
Meta loss on this task batch = 3.9880e-01, Meta loss averaged over last 500 steps = 4.3041e-01, PNorm = 63.1557, GNorm = 0.1728
Meta loss on this task batch = 4.5306e-01, Meta loss averaged over last 500 steps = 4.3039e-01, PNorm = 63.1626, GNorm = 0.1947
Took 126.75780248641968 seconds to complete one epoch of meta training
Took 134.88153433799744 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499753
Epoch 62
Meta loss on this task batch = 4.1995e-01, Meta loss averaged over last 500 steps = 4.3042e-01, PNorm = 63.1708, GNorm = 0.2136
Meta loss on this task batch = 4.6180e-01, Meta loss averaged over last 500 steps = 4.3044e-01, PNorm = 63.1791, GNorm = 0.1709
Meta loss on this task batch = 3.7882e-01, Meta loss averaged over last 500 steps = 4.3037e-01, PNorm = 63.1874, GNorm = 0.1678
Meta loss on this task batch = 3.8231e-01, Meta loss averaged over last 500 steps = 4.3022e-01, PNorm = 63.1976, GNorm = 0.1788
Meta loss on this task batch = 4.5132e-01, Meta loss averaged over last 500 steps = 4.3029e-01, PNorm = 63.2062, GNorm = 0.1824
Meta loss on this task batch = 4.4060e-01, Meta loss averaged over last 500 steps = 4.3024e-01, PNorm = 63.2151, GNorm = 0.1870
Meta loss on this task batch = 4.0114e-01, Meta loss averaged over last 500 steps = 4.3018e-01, PNorm = 63.2246, GNorm = 0.1524
Meta loss on this task batch = 3.9491e-01, Meta loss averaged over last 500 steps = 4.3012e-01, PNorm = 63.2345, GNorm = 0.1610
Meta loss on this task batch = 3.6796e-01, Meta loss averaged over last 500 steps = 4.3007e-01, PNorm = 63.2436, GNorm = 0.1601
Meta loss on this task batch = 4.5687e-01, Meta loss averaged over last 500 steps = 4.3009e-01, PNorm = 63.2531, GNorm = 0.2300
Meta loss on this task batch = 4.3985e-01, Meta loss averaged over last 500 steps = 4.3013e-01, PNorm = 63.2638, GNorm = 0.1918
Meta loss on this task batch = 4.6676e-01, Meta loss averaged over last 500 steps = 4.3016e-01, PNorm = 63.2772, GNorm = 0.2244
Meta loss on this task batch = 4.3261e-01, Meta loss averaged over last 500 steps = 4.3013e-01, PNorm = 63.2906, GNorm = 0.1767
Meta loss on this task batch = 4.3332e-01, Meta loss averaged over last 500 steps = 4.3018e-01, PNorm = 63.3043, GNorm = 0.1802
Meta loss on this task batch = 4.8487e-01, Meta loss averaged over last 500 steps = 4.3025e-01, PNorm = 63.3179, GNorm = 0.2260
Meta loss on this task batch = 4.2486e-01, Meta loss averaged over last 500 steps = 4.3011e-01, PNorm = 63.3329, GNorm = 0.2218
Meta loss on this task batch = 3.5665e-01, Meta loss averaged over last 500 steps = 4.2985e-01, PNorm = 63.3481, GNorm = 0.1649
Meta loss on this task batch = 4.8345e-01, Meta loss averaged over last 500 steps = 4.2983e-01, PNorm = 63.3623, GNorm = 0.2361
Meta loss on this task batch = 4.3956e-01, Meta loss averaged over last 500 steps = 4.2995e-01, PNorm = 63.3765, GNorm = 0.1918
Took 126.64182305335999 seconds to complete one epoch of meta training
Took 134.87377071380615 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501187
Epoch 63
Meta loss on this task batch = 4.2047e-01, Meta loss averaged over last 500 steps = 4.2985e-01, PNorm = 63.3888, GNorm = 0.2103
Meta loss on this task batch = 3.3071e-01, Meta loss averaged over last 500 steps = 4.2973e-01, PNorm = 63.4029, GNorm = 0.1742
Meta loss on this task batch = 4.1072e-01, Meta loss averaged over last 500 steps = 4.2962e-01, PNorm = 63.4168, GNorm = 0.1826
Meta loss on this task batch = 4.0206e-01, Meta loss averaged over last 500 steps = 4.2951e-01, PNorm = 63.4322, GNorm = 0.1706
Meta loss on this task batch = 3.9043e-01, Meta loss averaged over last 500 steps = 4.2941e-01, PNorm = 63.4482, GNorm = 0.1823
Meta loss on this task batch = 4.1637e-01, Meta loss averaged over last 500 steps = 4.2954e-01, PNorm = 63.4637, GNorm = 0.1771
Meta loss on this task batch = 4.2875e-01, Meta loss averaged over last 500 steps = 4.2958e-01, PNorm = 63.4790, GNorm = 0.1877
Meta loss on this task batch = 4.6956e-01, Meta loss averaged over last 500 steps = 4.2971e-01, PNorm = 63.4945, GNorm = 0.1727
Meta loss on this task batch = 4.6908e-01, Meta loss averaged over last 500 steps = 4.2977e-01, PNorm = 63.5085, GNorm = 0.2160
Meta loss on this task batch = 3.8544e-01, Meta loss averaged over last 500 steps = 4.2971e-01, PNorm = 63.5220, GNorm = 0.1560
Meta loss on this task batch = 4.0439e-01, Meta loss averaged over last 500 steps = 4.2970e-01, PNorm = 63.5340, GNorm = 0.1765
Meta loss on this task batch = 3.8947e-01, Meta loss averaged over last 500 steps = 4.2971e-01, PNorm = 63.5465, GNorm = 0.1526
Meta loss on this task batch = 3.9856e-01, Meta loss averaged over last 500 steps = 4.2959e-01, PNorm = 63.5593, GNorm = 0.2356
Meta loss on this task batch = 4.2558e-01, Meta loss averaged over last 500 steps = 4.2962e-01, PNorm = 63.5698, GNorm = 0.2047
Meta loss on this task batch = 4.5933e-01, Meta loss averaged over last 500 steps = 4.2970e-01, PNorm = 63.5797, GNorm = 0.1974
Meta loss on this task batch = 4.1833e-01, Meta loss averaged over last 500 steps = 4.2967e-01, PNorm = 63.5901, GNorm = 0.2054
Meta loss on this task batch = 4.3378e-01, Meta loss averaged over last 500 steps = 4.2983e-01, PNorm = 63.6014, GNorm = 0.1936
Meta loss on this task batch = 4.3929e-01, Meta loss averaged over last 500 steps = 4.2972e-01, PNorm = 63.6132, GNorm = 0.1921
Meta loss on this task batch = 4.2032e-01, Meta loss averaged over last 500 steps = 4.2949e-01, PNorm = 63.6271, GNorm = 0.2020
Took 126.5759346485138 seconds to complete one epoch of meta training
Took 134.43120551109314 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463619
Epoch 64
Meta loss on this task batch = 4.0093e-01, Meta loss averaged over last 500 steps = 4.2933e-01, PNorm = 63.6429, GNorm = 0.1701
Meta loss on this task batch = 4.0950e-01, Meta loss averaged over last 500 steps = 4.2926e-01, PNorm = 63.6597, GNorm = 0.1835
Meta loss on this task batch = 4.1154e-01, Meta loss averaged over last 500 steps = 4.2928e-01, PNorm = 63.6765, GNorm = 0.1487
Meta loss on this task batch = 4.0017e-01, Meta loss averaged over last 500 steps = 4.2912e-01, PNorm = 63.6926, GNorm = 0.1885
Meta loss on this task batch = 3.7263e-01, Meta loss averaged over last 500 steps = 4.2897e-01, PNorm = 63.7097, GNorm = 0.1874
Meta loss on this task batch = 3.8546e-01, Meta loss averaged over last 500 steps = 4.2875e-01, PNorm = 63.7248, GNorm = 0.1975
Meta loss on this task batch = 4.5266e-01, Meta loss averaged over last 500 steps = 4.2863e-01, PNorm = 63.7400, GNorm = 0.2227
Meta loss on this task batch = 3.8672e-01, Meta loss averaged over last 500 steps = 4.2855e-01, PNorm = 63.7542, GNorm = 0.1670
Meta loss on this task batch = 4.3961e-01, Meta loss averaged over last 500 steps = 4.2853e-01, PNorm = 63.7667, GNorm = 0.1872
Meta loss on this task batch = 3.4622e-01, Meta loss averaged over last 500 steps = 4.2832e-01, PNorm = 63.7790, GNorm = 0.1646
Meta loss on this task batch = 4.3729e-01, Meta loss averaged over last 500 steps = 4.2834e-01, PNorm = 63.7904, GNorm = 0.2002
Meta loss on this task batch = 3.9723e-01, Meta loss averaged over last 500 steps = 4.2826e-01, PNorm = 63.8031, GNorm = 0.1713
Meta loss on this task batch = 4.2094e-01, Meta loss averaged over last 500 steps = 4.2823e-01, PNorm = 63.8162, GNorm = 0.1741
Meta loss on this task batch = 4.5464e-01, Meta loss averaged over last 500 steps = 4.2818e-01, PNorm = 63.8277, GNorm = 0.2114
Meta loss on this task batch = 4.3872e-01, Meta loss averaged over last 500 steps = 4.2807e-01, PNorm = 63.8388, GNorm = 0.1847
Meta loss on this task batch = 3.3048e-01, Meta loss averaged over last 500 steps = 4.2775e-01, PNorm = 63.8521, GNorm = 0.2056
Meta loss on this task batch = 5.1548e-01, Meta loss averaged over last 500 steps = 4.2799e-01, PNorm = 63.8650, GNorm = 0.2876
Meta loss on this task batch = 4.4379e-01, Meta loss averaged over last 500 steps = 4.2806e-01, PNorm = 63.8768, GNorm = 0.2064
Meta loss on this task batch = 3.8863e-01, Meta loss averaged over last 500 steps = 4.2793e-01, PNorm = 63.8927, GNorm = 0.2524
Took 125.82422232627869 seconds to complete one epoch of meta training
Took 133.9831063747406 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472549
Epoch 65
Meta loss on this task batch = 4.3605e-01, Meta loss averaged over last 500 steps = 4.2801e-01, PNorm = 63.9082, GNorm = 0.1810
Meta loss on this task batch = 4.3956e-01, Meta loss averaged over last 500 steps = 4.2796e-01, PNorm = 63.9236, GNorm = 0.1976
Meta loss on this task batch = 3.6049e-01, Meta loss averaged over last 500 steps = 4.2779e-01, PNorm = 63.9387, GNorm = 0.2010
Meta loss on this task batch = 4.2400e-01, Meta loss averaged over last 500 steps = 4.2776e-01, PNorm = 63.9524, GNorm = 0.1906
Meta loss on this task batch = 4.2190e-01, Meta loss averaged over last 500 steps = 4.2766e-01, PNorm = 63.9642, GNorm = 0.2150
Meta loss on this task batch = 4.5887e-01, Meta loss averaged over last 500 steps = 4.2770e-01, PNorm = 63.9767, GNorm = 0.2063
Meta loss on this task batch = 3.8680e-01, Meta loss averaged over last 500 steps = 4.2764e-01, PNorm = 63.9933, GNorm = 0.2499
Meta loss on this task batch = 4.4482e-01, Meta loss averaged over last 500 steps = 4.2760e-01, PNorm = 64.0107, GNorm = 0.1742
Meta loss on this task batch = 3.8014e-01, Meta loss averaged over last 500 steps = 4.2754e-01, PNorm = 64.0283, GNorm = 0.1811
Meta loss on this task batch = 3.4480e-01, Meta loss averaged over last 500 steps = 4.2728e-01, PNorm = 64.0480, GNorm = 0.2011
Meta loss on this task batch = 4.4716e-01, Meta loss averaged over last 500 steps = 4.2730e-01, PNorm = 64.0665, GNorm = 0.2072
Meta loss on this task batch = 4.3591e-01, Meta loss averaged over last 500 steps = 4.2722e-01, PNorm = 64.0828, GNorm = 0.2312
Meta loss on this task batch = 4.1151e-01, Meta loss averaged over last 500 steps = 4.2716e-01, PNorm = 64.0975, GNorm = 0.2031
Meta loss on this task batch = 4.0364e-01, Meta loss averaged over last 500 steps = 4.2707e-01, PNorm = 64.1075, GNorm = 0.2671
Meta loss on this task batch = 4.1842e-01, Meta loss averaged over last 500 steps = 4.2705e-01, PNorm = 64.1158, GNorm = 0.2163
Meta loss on this task batch = 4.0772e-01, Meta loss averaged over last 500 steps = 4.2688e-01, PNorm = 64.1162, GNorm = 0.3536
Meta loss on this task batch = 4.7983e-01, Meta loss averaged over last 500 steps = 4.2717e-01, PNorm = 64.1136, GNorm = 0.2234
Meta loss on this task batch = 4.1512e-01, Meta loss averaged over last 500 steps = 4.2706e-01, PNorm = 64.1139, GNorm = 0.1730
Meta loss on this task batch = 3.4034e-01, Meta loss averaged over last 500 steps = 4.2692e-01, PNorm = 64.1174, GNorm = 0.2319
Took 125.23567938804626 seconds to complete one epoch of meta training
Took 132.31566667556763 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465998
Epoch 66
Meta loss on this task batch = 4.4988e-01, Meta loss averaged over last 500 steps = 4.2702e-01, PNorm = 64.1227, GNorm = 0.1664
Meta loss on this task batch = 4.1555e-01, Meta loss averaged over last 500 steps = 4.2695e-01, PNorm = 64.1315, GNorm = 0.1741
Meta loss on this task batch = 4.0126e-01, Meta loss averaged over last 500 steps = 4.2682e-01, PNorm = 64.1427, GNorm = 0.1583
Meta loss on this task batch = 4.1180e-01, Meta loss averaged over last 500 steps = 4.2671e-01, PNorm = 64.1569, GNorm = 0.1947
Meta loss on this task batch = 4.8995e-01, Meta loss averaged over last 500 steps = 4.2687e-01, PNorm = 64.1691, GNorm = 0.2866
Meta loss on this task batch = 4.4731e-01, Meta loss averaged over last 500 steps = 4.2686e-01, PNorm = 64.1821, GNorm = 0.1701
Meta loss on this task batch = 4.1280e-01, Meta loss averaged over last 500 steps = 4.2684e-01, PNorm = 64.1983, GNorm = 0.2096
Meta loss on this task batch = 4.0281e-01, Meta loss averaged over last 500 steps = 4.2676e-01, PNorm = 64.2165, GNorm = 0.2230
Meta loss on this task batch = 3.2683e-01, Meta loss averaged over last 500 steps = 4.2651e-01, PNorm = 64.2360, GNorm = 0.2010
Meta loss on this task batch = 3.6291e-01, Meta loss averaged over last 500 steps = 4.2630e-01, PNorm = 64.2551, GNorm = 0.2116
Meta loss on this task batch = 3.7913e-01, Meta loss averaged over last 500 steps = 4.2623e-01, PNorm = 64.2726, GNorm = 0.1852
Meta loss on this task batch = 4.2512e-01, Meta loss averaged over last 500 steps = 4.2613e-01, PNorm = 64.2868, GNorm = 0.2865
Meta loss on this task batch = 4.0170e-01, Meta loss averaged over last 500 steps = 4.2607e-01, PNorm = 64.2995, GNorm = 0.2326
Meta loss on this task batch = 4.3638e-01, Meta loss averaged over last 500 steps = 4.2612e-01, PNorm = 64.3106, GNorm = 0.2266
Meta loss on this task batch = 4.4270e-01, Meta loss averaged over last 500 steps = 4.2609e-01, PNorm = 64.3228, GNorm = 0.2133
Meta loss on this task batch = 4.0590e-01, Meta loss averaged over last 500 steps = 4.2594e-01, PNorm = 64.3369, GNorm = 0.1860
Meta loss on this task batch = 4.3476e-01, Meta loss averaged over last 500 steps = 4.2595e-01, PNorm = 64.3515, GNorm = 0.2141
Meta loss on this task batch = 4.5891e-01, Meta loss averaged over last 500 steps = 4.2597e-01, PNorm = 64.3668, GNorm = 0.2326
Meta loss on this task batch = 3.9758e-01, Meta loss averaged over last 500 steps = 4.2590e-01, PNorm = 64.3818, GNorm = 0.2007
Took 127.48717093467712 seconds to complete one epoch of meta training
Took 135.80532050132751 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489688
Epoch 67
Meta loss on this task batch = 3.6577e-01, Meta loss averaged over last 500 steps = 4.2585e-01, PNorm = 64.3977, GNorm = 0.1439
Meta loss on this task batch = 3.6636e-01, Meta loss averaged over last 500 steps = 4.2571e-01, PNorm = 64.4146, GNorm = 0.1629
Meta loss on this task batch = 3.9080e-01, Meta loss averaged over last 500 steps = 4.2568e-01, PNorm = 64.4302, GNorm = 0.1797
Meta loss on this task batch = 3.7473e-01, Meta loss averaged over last 500 steps = 4.2553e-01, PNorm = 64.4443, GNorm = 0.1814
Meta loss on this task batch = 3.9450e-01, Meta loss averaged over last 500 steps = 4.2547e-01, PNorm = 64.4576, GNorm = 0.1668
Meta loss on this task batch = 3.8182e-01, Meta loss averaged over last 500 steps = 4.2534e-01, PNorm = 64.4710, GNorm = 0.1792
Meta loss on this task batch = 4.7163e-01, Meta loss averaged over last 500 steps = 4.2541e-01, PNorm = 64.4813, GNorm = 0.2588
Meta loss on this task batch = 4.1456e-01, Meta loss averaged over last 500 steps = 4.2534e-01, PNorm = 64.4920, GNorm = 0.2269
Meta loss on this task batch = 4.0411e-01, Meta loss averaged over last 500 steps = 4.2525e-01, PNorm = 64.5001, GNorm = 0.2095
Meta loss on this task batch = 4.5801e-01, Meta loss averaged over last 500 steps = 4.2546e-01, PNorm = 64.5084, GNorm = 0.1814
Meta loss on this task batch = 4.2905e-01, Meta loss averaged over last 500 steps = 4.2548e-01, PNorm = 64.5192, GNorm = 0.2030
Meta loss on this task batch = 4.2726e-01, Meta loss averaged over last 500 steps = 4.2553e-01, PNorm = 64.5321, GNorm = 0.2303
Meta loss on this task batch = 4.2857e-01, Meta loss averaged over last 500 steps = 4.2551e-01, PNorm = 64.5472, GNorm = 0.1867
Meta loss on this task batch = 4.4138e-01, Meta loss averaged over last 500 steps = 4.2541e-01, PNorm = 64.5636, GNorm = 0.1879
Meta loss on this task batch = 4.5001e-01, Meta loss averaged over last 500 steps = 4.2544e-01, PNorm = 64.5797, GNorm = 0.1726
Meta loss on this task batch = 4.2370e-01, Meta loss averaged over last 500 steps = 4.2552e-01, PNorm = 64.5962, GNorm = 0.2762
Meta loss on this task batch = 4.7630e-01, Meta loss averaged over last 500 steps = 4.2558e-01, PNorm = 64.6119, GNorm = 0.2442
Meta loss on this task batch = 4.2329e-01, Meta loss averaged over last 500 steps = 4.2550e-01, PNorm = 64.6255, GNorm = 0.1854
Meta loss on this task batch = 4.5838e-01, Meta loss averaged over last 500 steps = 4.2557e-01, PNorm = 64.6382, GNorm = 0.2135
Took 126.50668168067932 seconds to complete one epoch of meta training
Took 134.21904063224792 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494955
Epoch 68
Meta loss on this task batch = 4.3753e-01, Meta loss averaged over last 500 steps = 4.2553e-01, PNorm = 64.6515, GNorm = 0.1743
Meta loss on this task batch = 4.7250e-01, Meta loss averaged over last 500 steps = 4.2547e-01, PNorm = 64.6659, GNorm = 0.1899
Meta loss on this task batch = 4.2710e-01, Meta loss averaged over last 500 steps = 4.2552e-01, PNorm = 64.6802, GNorm = 0.1677
Meta loss on this task batch = 3.7035e-01, Meta loss averaged over last 500 steps = 4.2545e-01, PNorm = 64.6973, GNorm = 0.2100
Meta loss on this task batch = 4.0565e-01, Meta loss averaged over last 500 steps = 4.2548e-01, PNorm = 64.7151, GNorm = 0.2029
Meta loss on this task batch = 4.1062e-01, Meta loss averaged over last 500 steps = 4.2525e-01, PNorm = 64.7341, GNorm = 0.2543
Meta loss on this task batch = 3.9342e-01, Meta loss averaged over last 500 steps = 4.2523e-01, PNorm = 64.7534, GNorm = 0.1725
Meta loss on this task batch = 4.3519e-01, Meta loss averaged over last 500 steps = 4.2512e-01, PNorm = 64.7705, GNorm = 0.2159
Meta loss on this task batch = 3.9404e-01, Meta loss averaged over last 500 steps = 4.2497e-01, PNorm = 64.7820, GNorm = 0.3186
Meta loss on this task batch = 3.6248e-01, Meta loss averaged over last 500 steps = 4.2496e-01, PNorm = 64.7923, GNorm = 0.2260
Meta loss on this task batch = 3.6403e-01, Meta loss averaged over last 500 steps = 4.2496e-01, PNorm = 64.8035, GNorm = 0.1831
Meta loss on this task batch = 4.1355e-01, Meta loss averaged over last 500 steps = 4.2490e-01, PNorm = 64.8171, GNorm = 0.1946
Meta loss on this task batch = 4.4359e-01, Meta loss averaged over last 500 steps = 4.2504e-01, PNorm = 64.8295, GNorm = 0.2033
Meta loss on this task batch = 4.1490e-01, Meta loss averaged over last 500 steps = 4.2496e-01, PNorm = 64.8431, GNorm = 0.1988
Meta loss on this task batch = 4.3629e-01, Meta loss averaged over last 500 steps = 4.2492e-01, PNorm = 64.8563, GNorm = 0.2085
Meta loss on this task batch = 4.3058e-01, Meta loss averaged over last 500 steps = 4.2486e-01, PNorm = 64.8695, GNorm = 0.2356
Meta loss on this task batch = 4.3788e-01, Meta loss averaged over last 500 steps = 4.2488e-01, PNorm = 64.8832, GNorm = 0.1647
Meta loss on this task batch = 4.0827e-01, Meta loss averaged over last 500 steps = 4.2475e-01, PNorm = 64.8980, GNorm = 0.2210
Meta loss on this task batch = 4.6608e-01, Meta loss averaged over last 500 steps = 4.2482e-01, PNorm = 64.9125, GNorm = 0.2267
Took 126.28987383842468 seconds to complete one epoch of meta training
Took 134.39275360107422 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488248
Epoch 69
Meta loss on this task batch = 4.2867e-01, Meta loss averaged over last 500 steps = 4.2497e-01, PNorm = 64.9284, GNorm = 0.1979
Meta loss on this task batch = 3.6079e-01, Meta loss averaged over last 500 steps = 4.2475e-01, PNorm = 64.9429, GNorm = 0.2167
Meta loss on this task batch = 4.7880e-01, Meta loss averaged over last 500 steps = 4.2474e-01, PNorm = 64.9555, GNorm = 0.2402
Meta loss on this task batch = 4.1096e-01, Meta loss averaged over last 500 steps = 4.2464e-01, PNorm = 64.9647, GNorm = 0.2308
Meta loss on this task batch = 4.1551e-01, Meta loss averaged over last 500 steps = 4.2447e-01, PNorm = 64.9739, GNorm = 0.1989
Meta loss on this task batch = 4.1589e-01, Meta loss averaged over last 500 steps = 4.2441e-01, PNorm = 64.9841, GNorm = 0.1779
Meta loss on this task batch = 4.3089e-01, Meta loss averaged over last 500 steps = 4.2445e-01, PNorm = 64.9956, GNorm = 0.1861
Meta loss on this task batch = 4.5850e-01, Meta loss averaged over last 500 steps = 4.2452e-01, PNorm = 65.0069, GNorm = 0.1819
Meta loss on this task batch = 4.7211e-01, Meta loss averaged over last 500 steps = 4.2455e-01, PNorm = 65.0182, GNorm = 0.2366
Meta loss on this task batch = 4.0704e-01, Meta loss averaged over last 500 steps = 4.2452e-01, PNorm = 65.0298, GNorm = 0.1899
Meta loss on this task batch = 4.2579e-01, Meta loss averaged over last 500 steps = 4.2441e-01, PNorm = 65.0406, GNorm = 0.1841
Meta loss on this task batch = 3.9581e-01, Meta loss averaged over last 500 steps = 4.2433e-01, PNorm = 65.0512, GNorm = 0.1995
Meta loss on this task batch = 3.8624e-01, Meta loss averaged over last 500 steps = 4.2434e-01, PNorm = 65.0623, GNorm = 0.1665
Meta loss on this task batch = 3.6454e-01, Meta loss averaged over last 500 steps = 4.2412e-01, PNorm = 65.0750, GNorm = 0.1796
Meta loss on this task batch = 3.9535e-01, Meta loss averaged over last 500 steps = 4.2402e-01, PNorm = 65.0874, GNorm = 0.1547
Meta loss on this task batch = 3.9439e-01, Meta loss averaged over last 500 steps = 4.2394e-01, PNorm = 65.0999, GNorm = 0.1725
Meta loss on this task batch = 3.8129e-01, Meta loss averaged over last 500 steps = 4.2385e-01, PNorm = 65.1128, GNorm = 0.1721
Meta loss on this task batch = 4.3767e-01, Meta loss averaged over last 500 steps = 4.2392e-01, PNorm = 65.1250, GNorm = 0.2169
Meta loss on this task batch = 4.4324e-01, Meta loss averaged over last 500 steps = 4.2387e-01, PNorm = 65.1386, GNorm = 0.2103
Took 125.70816016197205 seconds to complete one epoch of meta training
Took 133.69075512886047 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503900
Epoch 70
Meta loss on this task batch = 3.9347e-01, Meta loss averaged over last 500 steps = 4.2380e-01, PNorm = 65.1523, GNorm = 0.1798
Meta loss on this task batch = 3.9646e-01, Meta loss averaged over last 500 steps = 4.2379e-01, PNorm = 65.1646, GNorm = 0.1608
Meta loss on this task batch = 3.8765e-01, Meta loss averaged over last 500 steps = 4.2369e-01, PNorm = 65.1759, GNorm = 0.1597
Meta loss on this task batch = 3.9321e-01, Meta loss averaged over last 500 steps = 4.2356e-01, PNorm = 65.1881, GNorm = 0.1886
Meta loss on this task batch = 4.1947e-01, Meta loss averaged over last 500 steps = 4.2356e-01, PNorm = 65.1999, GNorm = 0.1827
Meta loss on this task batch = 4.4226e-01, Meta loss averaged over last 500 steps = 4.2349e-01, PNorm = 65.2116, GNorm = 0.2016
Meta loss on this task batch = 4.4355e-01, Meta loss averaged over last 500 steps = 4.2358e-01, PNorm = 65.2244, GNorm = 0.2078
Meta loss on this task batch = 4.4668e-01, Meta loss averaged over last 500 steps = 4.2359e-01, PNorm = 65.2383, GNorm = 0.2611
Meta loss on this task batch = 4.2273e-01, Meta loss averaged over last 500 steps = 4.2371e-01, PNorm = 65.2531, GNorm = 0.1825
Meta loss on this task batch = 4.1447e-01, Meta loss averaged over last 500 steps = 4.2365e-01, PNorm = 65.2685, GNorm = 0.1925
Meta loss on this task batch = 4.4898e-01, Meta loss averaged over last 500 steps = 4.2372e-01, PNorm = 65.2833, GNorm = 0.2306
Meta loss on this task batch = 3.7075e-01, Meta loss averaged over last 500 steps = 4.2370e-01, PNorm = 65.2979, GNorm = 0.2155
Meta loss on this task batch = 3.9421e-01, Meta loss averaged over last 500 steps = 4.2368e-01, PNorm = 65.3121, GNorm = 0.1880
Meta loss on this task batch = 3.5570e-01, Meta loss averaged over last 500 steps = 4.2352e-01, PNorm = 65.3265, GNorm = 0.1885
Meta loss on this task batch = 4.1896e-01, Meta loss averaged over last 500 steps = 4.2357e-01, PNorm = 65.3407, GNorm = 0.2141
Meta loss on this task batch = 3.5613e-01, Meta loss averaged over last 500 steps = 4.2338e-01, PNorm = 65.3534, GNorm = 0.2024
Meta loss on this task batch = 4.0376e-01, Meta loss averaged over last 500 steps = 4.2324e-01, PNorm = 65.3654, GNorm = 0.2072
Meta loss on this task batch = 4.0494e-01, Meta loss averaged over last 500 steps = 4.2313e-01, PNorm = 65.3779, GNorm = 0.1726
Meta loss on this task batch = 4.7834e-01, Meta loss averaged over last 500 steps = 4.2329e-01, PNorm = 65.3902, GNorm = 0.2977
Took 125.31744694709778 seconds to complete one epoch of meta training
Took 133.54320192337036 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501940
Epoch 71
Meta loss on this task batch = 3.4513e-01, Meta loss averaged over last 500 steps = 4.2310e-01, PNorm = 65.4045, GNorm = 0.1820
Meta loss on this task batch = 4.2552e-01, Meta loss averaged over last 500 steps = 4.2296e-01, PNorm = 65.4185, GNorm = 0.1965
Meta loss on this task batch = 4.3300e-01, Meta loss averaged over last 500 steps = 4.2292e-01, PNorm = 65.4340, GNorm = 0.2029
Meta loss on this task batch = 4.0935e-01, Meta loss averaged over last 500 steps = 4.2275e-01, PNorm = 65.4512, GNorm = 0.2568
Meta loss on this task batch = 3.9478e-01, Meta loss averaged over last 500 steps = 4.2250e-01, PNorm = 65.4678, GNorm = 0.1807
Meta loss on this task batch = 4.3342e-01, Meta loss averaged over last 500 steps = 4.2250e-01, PNorm = 65.4835, GNorm = 0.1935
Meta loss on this task batch = 4.0108e-01, Meta loss averaged over last 500 steps = 4.2234e-01, PNorm = 65.4977, GNorm = 0.2487
Meta loss on this task batch = 4.1890e-01, Meta loss averaged over last 500 steps = 4.2227e-01, PNorm = 65.5106, GNorm = 0.2340
Meta loss on this task batch = 3.7571e-01, Meta loss averaged over last 500 steps = 4.2215e-01, PNorm = 65.5243, GNorm = 0.1977
Meta loss on this task batch = 3.3389e-01, Meta loss averaged over last 500 steps = 4.2187e-01, PNorm = 65.5391, GNorm = 0.1622
Meta loss on this task batch = 4.7938e-01, Meta loss averaged over last 500 steps = 4.2199e-01, PNorm = 65.5530, GNorm = 0.2332
Meta loss on this task batch = 3.8803e-01, Meta loss averaged over last 500 steps = 4.2197e-01, PNorm = 65.5653, GNorm = 0.2713
Meta loss on this task batch = 3.5132e-01, Meta loss averaged over last 500 steps = 4.2175e-01, PNorm = 65.5785, GNorm = 0.1933
Meta loss on this task batch = 4.0132e-01, Meta loss averaged over last 500 steps = 4.2158e-01, PNorm = 65.5927, GNorm = 0.2197
Meta loss on this task batch = 4.0202e-01, Meta loss averaged over last 500 steps = 4.2163e-01, PNorm = 65.6067, GNorm = 0.1840
Meta loss on this task batch = 4.3780e-01, Meta loss averaged over last 500 steps = 4.2163e-01, PNorm = 65.6187, GNorm = 0.1800
Meta loss on this task batch = 4.3520e-01, Meta loss averaged over last 500 steps = 4.2158e-01, PNorm = 65.6306, GNorm = 0.1929
Meta loss on this task batch = 3.3868e-01, Meta loss averaged over last 500 steps = 4.2161e-01, PNorm = 65.6434, GNorm = 0.2383
Meta loss on this task batch = 4.3995e-01, Meta loss averaged over last 500 steps = 4.2168e-01, PNorm = 65.6549, GNorm = 0.2176
Took 125.72332906723022 seconds to complete one epoch of meta training
Took 133.58772253990173 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.528209
Found better MAML checkpoint after meta validation, saving now
Epoch 72
Meta loss on this task batch = 4.2598e-01, Meta loss averaged over last 500 steps = 4.2165e-01, PNorm = 65.6656, GNorm = 0.2306
Meta loss on this task batch = 4.2182e-01, Meta loss averaged over last 500 steps = 4.2170e-01, PNorm = 65.6758, GNorm = 0.2134
Meta loss on this task batch = 4.4867e-01, Meta loss averaged over last 500 steps = 4.2171e-01, PNorm = 65.6863, GNorm = 0.2796
Meta loss on this task batch = 3.5018e-01, Meta loss averaged over last 500 steps = 4.2152e-01, PNorm = 65.6990, GNorm = 0.2148
Meta loss on this task batch = 4.0082e-01, Meta loss averaged over last 500 steps = 4.2141e-01, PNorm = 65.7131, GNorm = 0.1660
Meta loss on this task batch = 4.4952e-01, Meta loss averaged over last 500 steps = 4.2152e-01, PNorm = 65.7283, GNorm = 0.2350
Meta loss on this task batch = 4.5607e-01, Meta loss averaged over last 500 steps = 4.2155e-01, PNorm = 65.7432, GNorm = 0.2213
Meta loss on this task batch = 3.7336e-01, Meta loss averaged over last 500 steps = 4.2147e-01, PNorm = 65.7595, GNorm = 0.1767
Meta loss on this task batch = 4.2459e-01, Meta loss averaged over last 500 steps = 4.2148e-01, PNorm = 65.7734, GNorm = 0.2759
Meta loss on this task batch = 3.7656e-01, Meta loss averaged over last 500 steps = 4.2146e-01, PNorm = 65.7861, GNorm = 0.2228
Meta loss on this task batch = 4.1351e-01, Meta loss averaged over last 500 steps = 4.2145e-01, PNorm = 65.7992, GNorm = 0.2176
Meta loss on this task batch = 4.4300e-01, Meta loss averaged over last 500 steps = 4.2148e-01, PNorm = 65.8104, GNorm = 0.2632
Meta loss on this task batch = 3.5962e-01, Meta loss averaged over last 500 steps = 4.2136e-01, PNorm = 65.8218, GNorm = 0.1936
Meta loss on this task batch = 3.7322e-01, Meta loss averaged over last 500 steps = 4.2114e-01, PNorm = 65.8346, GNorm = 0.2353
Meta loss on this task batch = 4.7069e-01, Meta loss averaged over last 500 steps = 4.2122e-01, PNorm = 65.8472, GNorm = 0.1980
Meta loss on this task batch = 4.2602e-01, Meta loss averaged over last 500 steps = 4.2118e-01, PNorm = 65.8584, GNorm = 0.1842
Meta loss on this task batch = 3.9319e-01, Meta loss averaged over last 500 steps = 4.2100e-01, PNorm = 65.8700, GNorm = 0.1927
Meta loss on this task batch = 3.5832e-01, Meta loss averaged over last 500 steps = 4.2085e-01, PNorm = 65.8813, GNorm = 0.1739
Meta loss on this task batch = 3.4487e-01, Meta loss averaged over last 500 steps = 4.2070e-01, PNorm = 65.8932, GNorm = 0.2265
Took 125.73192262649536 seconds to complete one epoch of meta training
Took 133.73888397216797 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494157
Epoch 73
Meta loss on this task batch = 3.9877e-01, Meta loss averaged over last 500 steps = 4.2066e-01, PNorm = 65.9056, GNorm = 0.1922
Meta loss on this task batch = 4.3851e-01, Meta loss averaged over last 500 steps = 4.2065e-01, PNorm = 65.9205, GNorm = 0.2056
Meta loss on this task batch = 4.6137e-01, Meta loss averaged over last 500 steps = 4.2062e-01, PNorm = 65.9346, GNorm = 0.1966
Meta loss on this task batch = 4.3223e-01, Meta loss averaged over last 500 steps = 4.2056e-01, PNorm = 65.9486, GNorm = 0.2359
Meta loss on this task batch = 3.7076e-01, Meta loss averaged over last 500 steps = 4.2047e-01, PNorm = 65.9627, GNorm = 0.1854
Meta loss on this task batch = 3.7089e-01, Meta loss averaged over last 500 steps = 4.2051e-01, PNorm = 65.9762, GNorm = 0.1692
Meta loss on this task batch = 3.6941e-01, Meta loss averaged over last 500 steps = 4.2035e-01, PNorm = 65.9896, GNorm = 0.1752
Meta loss on this task batch = 4.1942e-01, Meta loss averaged over last 500 steps = 4.2040e-01, PNorm = 66.0018, GNorm = 0.2182
Meta loss on this task batch = 3.8701e-01, Meta loss averaged over last 500 steps = 4.2040e-01, PNorm = 66.0127, GNorm = 0.2073
Meta loss on this task batch = 4.3927e-01, Meta loss averaged over last 500 steps = 4.2046e-01, PNorm = 66.0202, GNorm = 0.2853
Meta loss on this task batch = 4.2801e-01, Meta loss averaged over last 500 steps = 4.2045e-01, PNorm = 66.0274, GNorm = 0.2011
Meta loss on this task batch = 4.5601e-01, Meta loss averaged over last 500 steps = 4.2048e-01, PNorm = 66.0336, GNorm = 0.2441
Meta loss on this task batch = 3.8411e-01, Meta loss averaged over last 500 steps = 4.2041e-01, PNorm = 66.0419, GNorm = 0.2124
Meta loss on this task batch = 3.4414e-01, Meta loss averaged over last 500 steps = 4.2020e-01, PNorm = 66.0524, GNorm = 0.1619
Meta loss on this task batch = 4.2568e-01, Meta loss averaged over last 500 steps = 4.2026e-01, PNorm = 66.0651, GNorm = 0.1924
Meta loss on this task batch = 4.2971e-01, Meta loss averaged over last 500 steps = 4.2016e-01, PNorm = 66.0790, GNorm = 0.2277
Meta loss on this task batch = 3.5604e-01, Meta loss averaged over last 500 steps = 4.2007e-01, PNorm = 66.0944, GNorm = 0.2262
Meta loss on this task batch = 4.0460e-01, Meta loss averaged over last 500 steps = 4.2015e-01, PNorm = 66.1103, GNorm = 0.2050
Meta loss on this task batch = 4.2175e-01, Meta loss averaged over last 500 steps = 4.2016e-01, PNorm = 66.1271, GNorm = 0.2455
Took 125.80641102790833 seconds to complete one epoch of meta training
Took 133.59212446212769 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493211
Epoch 74
Meta loss on this task batch = 4.0072e-01, Meta loss averaged over last 500 steps = 4.2012e-01, PNorm = 66.1431, GNorm = 0.2280
Meta loss on this task batch = 4.3126e-01, Meta loss averaged over last 500 steps = 4.2017e-01, PNorm = 66.1588, GNorm = 0.1852
Meta loss on this task batch = 4.1197e-01, Meta loss averaged over last 500 steps = 4.2000e-01, PNorm = 66.1746, GNorm = 0.2154
Meta loss on this task batch = 4.2144e-01, Meta loss averaged over last 500 steps = 4.1989e-01, PNorm = 66.1902, GNorm = 0.1862
Meta loss on this task batch = 3.7968e-01, Meta loss averaged over last 500 steps = 4.1985e-01, PNorm = 66.2041, GNorm = 0.1945
Meta loss on this task batch = 4.5149e-01, Meta loss averaged over last 500 steps = 4.1977e-01, PNorm = 66.2173, GNorm = 0.2001
Meta loss on this task batch = 3.6732e-01, Meta loss averaged over last 500 steps = 4.1973e-01, PNorm = 66.2320, GNorm = 0.1991
Meta loss on this task batch = 4.0484e-01, Meta loss averaged over last 500 steps = 4.1969e-01, PNorm = 66.2465, GNorm = 0.1678
Meta loss on this task batch = 4.2212e-01, Meta loss averaged over last 500 steps = 4.1972e-01, PNorm = 66.2600, GNorm = 0.1903
Meta loss on this task batch = 4.0511e-01, Meta loss averaged over last 500 steps = 4.1961e-01, PNorm = 66.2736, GNorm = 0.1991
Meta loss on this task batch = 3.3841e-01, Meta loss averaged over last 500 steps = 4.1950e-01, PNorm = 66.2887, GNorm = 0.1859
Meta loss on this task batch = 3.5645e-01, Meta loss averaged over last 500 steps = 4.1938e-01, PNorm = 66.3032, GNorm = 0.1970
Meta loss on this task batch = 3.3076e-01, Meta loss averaged over last 500 steps = 4.1918e-01, PNorm = 66.3180, GNorm = 0.1881
Meta loss on this task batch = 4.4283e-01, Meta loss averaged over last 500 steps = 4.1917e-01, PNorm = 66.3317, GNorm = 0.2343
Meta loss on this task batch = 5.0882e-01, Meta loss averaged over last 500 steps = 4.1929e-01, PNorm = 66.3423, GNorm = 0.2738
Meta loss on this task batch = 3.8144e-01, Meta loss averaged over last 500 steps = 4.1917e-01, PNorm = 66.3508, GNorm = 0.2568
Meta loss on this task batch = 3.9138e-01, Meta loss averaged over last 500 steps = 4.1912e-01, PNorm = 66.3610, GNorm = 0.2017
Meta loss on this task batch = 4.3087e-01, Meta loss averaged over last 500 steps = 4.1906e-01, PNorm = 66.3720, GNorm = 0.2087
Meta loss on this task batch = 3.6622e-01, Meta loss averaged over last 500 steps = 4.1883e-01, PNorm = 66.3848, GNorm = 0.2080
Took 126.50738620758057 seconds to complete one epoch of meta training
Took 134.3453826904297 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480575
Epoch 75
Meta loss on this task batch = 4.0020e-01, Meta loss averaged over last 500 steps = 4.1881e-01, PNorm = 66.3979, GNorm = 0.1783
Meta loss on this task batch = 3.9343e-01, Meta loss averaged over last 500 steps = 4.1892e-01, PNorm = 66.4118, GNorm = 0.1665
Meta loss on this task batch = 4.0324e-01, Meta loss averaged over last 500 steps = 4.1889e-01, PNorm = 66.4248, GNorm = 0.2303
Meta loss on this task batch = 3.4752e-01, Meta loss averaged over last 500 steps = 4.1870e-01, PNorm = 66.4389, GNorm = 0.1721
Meta loss on this task batch = 4.2023e-01, Meta loss averaged over last 500 steps = 4.1853e-01, PNorm = 66.4519, GNorm = 0.2128
Meta loss on this task batch = 4.1120e-01, Meta loss averaged over last 500 steps = 4.1844e-01, PNorm = 66.4646, GNorm = 0.1726
Meta loss on this task batch = 4.3584e-01, Meta loss averaged over last 500 steps = 4.1844e-01, PNorm = 66.4770, GNorm = 0.2281
Meta loss on this task batch = 3.8379e-01, Meta loss averaged over last 500 steps = 4.1834e-01, PNorm = 66.4894, GNorm = 0.1780
Meta loss on this task batch = 4.7198e-01, Meta loss averaged over last 500 steps = 4.1837e-01, PNorm = 66.5006, GNorm = 0.2305
Meta loss on this task batch = 4.3232e-01, Meta loss averaged over last 500 steps = 4.1846e-01, PNorm = 66.5104, GNorm = 0.2360
Meta loss on this task batch = 4.3082e-01, Meta loss averaged over last 500 steps = 4.1841e-01, PNorm = 66.5207, GNorm = 0.2040
Meta loss on this task batch = 4.0583e-01, Meta loss averaged over last 500 steps = 4.1828e-01, PNorm = 66.5299, GNorm = 0.2226
Meta loss on this task batch = 3.9871e-01, Meta loss averaged over last 500 steps = 4.1824e-01, PNorm = 66.5405, GNorm = 0.2000
Meta loss on this task batch = 3.8371e-01, Meta loss averaged over last 500 steps = 4.1808e-01, PNorm = 66.5530, GNorm = 0.2315
Meta loss on this task batch = 3.9511e-01, Meta loss averaged over last 500 steps = 4.1807e-01, PNorm = 66.5646, GNorm = 0.2037
Meta loss on this task batch = 4.1160e-01, Meta loss averaged over last 500 steps = 4.1805e-01, PNorm = 66.5774, GNorm = 0.2078
Meta loss on this task batch = 3.5954e-01, Meta loss averaged over last 500 steps = 4.1789e-01, PNorm = 66.5903, GNorm = 0.2202
Meta loss on this task batch = 3.4913e-01, Meta loss averaged over last 500 steps = 4.1780e-01, PNorm = 66.6021, GNorm = 0.2197
Meta loss on this task batch = 4.0451e-01, Meta loss averaged over last 500 steps = 4.1768e-01, PNorm = 66.6124, GNorm = 0.2425
Took 126.06395530700684 seconds to complete one epoch of meta training
Took 132.83144807815552 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488338
Epoch 76
Meta loss on this task batch = 3.9981e-01, Meta loss averaged over last 500 steps = 4.1755e-01, PNorm = 66.6213, GNorm = 0.2371
Meta loss on this task batch = 3.8815e-01, Meta loss averaged over last 500 steps = 4.1739e-01, PNorm = 66.6312, GNorm = 0.2158
Meta loss on this task batch = 4.4434e-01, Meta loss averaged over last 500 steps = 4.1751e-01, PNorm = 66.6404, GNorm = 0.2339
Meta loss on this task batch = 3.8696e-01, Meta loss averaged over last 500 steps = 4.1749e-01, PNorm = 66.6500, GNorm = 0.2014
Meta loss on this task batch = 4.1200e-01, Meta loss averaged over last 500 steps = 4.1750e-01, PNorm = 66.6600, GNorm = 0.1847
Meta loss on this task batch = 3.0350e-01, Meta loss averaged over last 500 steps = 4.1724e-01, PNorm = 66.6714, GNorm = 0.1760
Meta loss on this task batch = 3.8778e-01, Meta loss averaged over last 500 steps = 4.1716e-01, PNorm = 66.6851, GNorm = 0.2456
Meta loss on this task batch = 4.2329e-01, Meta loss averaged over last 500 steps = 4.1713e-01, PNorm = 66.6998, GNorm = 0.1876
Meta loss on this task batch = 4.0606e-01, Meta loss averaged over last 500 steps = 4.1708e-01, PNorm = 66.7143, GNorm = 0.2152
Meta loss on this task batch = 4.2962e-01, Meta loss averaged over last 500 steps = 4.1694e-01, PNorm = 66.7268, GNorm = 0.2033
Meta loss on this task batch = 4.6878e-01, Meta loss averaged over last 500 steps = 4.1694e-01, PNorm = 66.7374, GNorm = 0.2546
Meta loss on this task batch = 3.7594e-01, Meta loss averaged over last 500 steps = 4.1687e-01, PNorm = 66.7461, GNorm = 0.2221
Meta loss on this task batch = 3.9545e-01, Meta loss averaged over last 500 steps = 4.1688e-01, PNorm = 66.7552, GNorm = 0.1858
Meta loss on this task batch = 3.5765e-01, Meta loss averaged over last 500 steps = 4.1671e-01, PNorm = 66.7632, GNorm = 0.1672
Meta loss on this task batch = 3.9266e-01, Meta loss averaged over last 500 steps = 4.1672e-01, PNorm = 66.7722, GNorm = 0.2110
Meta loss on this task batch = 4.7347e-01, Meta loss averaged over last 500 steps = 4.1681e-01, PNorm = 66.7814, GNorm = 0.1806
Meta loss on this task batch = 3.7644e-01, Meta loss averaged over last 500 steps = 4.1674e-01, PNorm = 66.7923, GNorm = 0.2144
Meta loss on this task batch = 4.2610e-01, Meta loss averaged over last 500 steps = 4.1661e-01, PNorm = 66.8036, GNorm = 0.2083
Meta loss on this task batch = 4.2938e-01, Meta loss averaged over last 500 steps = 4.1675e-01, PNorm = 66.8138, GNorm = 0.2272
Took 131.72822403907776 seconds to complete one epoch of meta training
Took 140.55184388160706 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476967
Epoch 77
Meta loss on this task batch = 3.9938e-01, Meta loss averaged over last 500 steps = 4.1681e-01, PNorm = 66.8258, GNorm = 0.1981
Meta loss on this task batch = 3.4254e-01, Meta loss averaged over last 500 steps = 4.1667e-01, PNorm = 66.8400, GNorm = 0.1928
Meta loss on this task batch = 3.6937e-01, Meta loss averaged over last 500 steps = 4.1651e-01, PNorm = 66.8544, GNorm = 0.1865
Meta loss on this task batch = 4.0819e-01, Meta loss averaged over last 500 steps = 4.1658e-01, PNorm = 66.8660, GNorm = 0.2130
Meta loss on this task batch = 4.0116e-01, Meta loss averaged over last 500 steps = 4.1661e-01, PNorm = 66.8781, GNorm = 0.1884
Meta loss on this task batch = 4.2558e-01, Meta loss averaged over last 500 steps = 4.1658e-01, PNorm = 66.8900, GNorm = 0.1925
Meta loss on this task batch = 4.1429e-01, Meta loss averaged over last 500 steps = 4.1660e-01, PNorm = 66.9006, GNorm = 0.2143
Meta loss on this task batch = 4.3350e-01, Meta loss averaged over last 500 steps = 4.1652e-01, PNorm = 66.9089, GNorm = 0.2646
Meta loss on this task batch = 4.3725e-01, Meta loss averaged over last 500 steps = 4.1662e-01, PNorm = 66.9164, GNorm = 0.2234
Meta loss on this task batch = 3.8807e-01, Meta loss averaged over last 500 steps = 4.1658e-01, PNorm = 66.9228, GNorm = 0.2328
Meta loss on this task batch = 4.0410e-01, Meta loss averaged over last 500 steps = 4.1655e-01, PNorm = 66.9264, GNorm = 0.2272
Meta loss on this task batch = 4.8549e-01, Meta loss averaged over last 500 steps = 4.1661e-01, PNorm = 66.9314, GNorm = 0.2097
Meta loss on this task batch = 4.3309e-01, Meta loss averaged over last 500 steps = 4.1660e-01, PNorm = 66.9377, GNorm = 0.2053
Meta loss on this task batch = 4.2588e-01, Meta loss averaged over last 500 steps = 4.1652e-01, PNorm = 66.9455, GNorm = 0.2054
Meta loss on this task batch = 4.2159e-01, Meta loss averaged over last 500 steps = 4.1655e-01, PNorm = 66.9539, GNorm = 0.2402
Meta loss on this task batch = 4.4158e-01, Meta loss averaged over last 500 steps = 4.1665e-01, PNorm = 66.9625, GNorm = 0.1992
Meta loss on this task batch = 3.7592e-01, Meta loss averaged over last 500 steps = 4.1660e-01, PNorm = 66.9739, GNorm = 0.2311
Meta loss on this task batch = 3.7061e-01, Meta loss averaged over last 500 steps = 4.1642e-01, PNorm = 66.9848, GNorm = 0.1970
Meta loss on this task batch = 3.5125e-01, Meta loss averaged over last 500 steps = 4.1631e-01, PNorm = 66.9957, GNorm = 0.2171
Took 130.54264855384827 seconds to complete one epoch of meta training
Took 138.9291386604309 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478733
Epoch 78
Meta loss on this task batch = 3.9372e-01, Meta loss averaged over last 500 steps = 4.1610e-01, PNorm = 67.0077, GNorm = 0.1764
Meta loss on this task batch = 4.1505e-01, Meta loss averaged over last 500 steps = 4.1600e-01, PNorm = 67.0217, GNorm = 0.1742
Meta loss on this task batch = 3.4149e-01, Meta loss averaged over last 500 steps = 4.1573e-01, PNorm = 67.0365, GNorm = 0.1815
Meta loss on this task batch = 4.6484e-01, Meta loss averaged over last 500 steps = 4.1579e-01, PNorm = 67.0514, GNorm = 0.2022
Meta loss on this task batch = 3.5588e-01, Meta loss averaged over last 500 steps = 4.1548e-01, PNorm = 67.0665, GNorm = 0.1921
Meta loss on this task batch = 4.3537e-01, Meta loss averaged over last 500 steps = 4.1567e-01, PNorm = 67.0802, GNorm = 0.2300
Meta loss on this task batch = 4.4519e-01, Meta loss averaged over last 500 steps = 4.1555e-01, PNorm = 67.0956, GNorm = 0.2978
Meta loss on this task batch = 3.2251e-01, Meta loss averaged over last 500 steps = 4.1543e-01, PNorm = 67.1098, GNorm = 0.2104
Meta loss on this task batch = 4.3766e-01, Meta loss averaged over last 500 steps = 4.1540e-01, PNorm = 67.1242, GNorm = 0.2668
Meta loss on this task batch = 4.4817e-01, Meta loss averaged over last 500 steps = 4.1538e-01, PNorm = 67.1390, GNorm = 0.2172
Meta loss on this task batch = 3.5926e-01, Meta loss averaged over last 500 steps = 4.1514e-01, PNorm = 67.1542, GNorm = 0.2166
Meta loss on this task batch = 4.1499e-01, Meta loss averaged over last 500 steps = 4.1510e-01, PNorm = 67.1700, GNorm = 0.2324
Meta loss on this task batch = 4.6453e-01, Meta loss averaged over last 500 steps = 4.1524e-01, PNorm = 67.1854, GNorm = 0.2424
Meta loss on this task batch = 3.8638e-01, Meta loss averaged over last 500 steps = 4.1507e-01, PNorm = 67.2011, GNorm = 0.2030
Meta loss on this task batch = 3.7878e-01, Meta loss averaged over last 500 steps = 4.1506e-01, PNorm = 67.2173, GNorm = 0.2087
Meta loss on this task batch = 3.7770e-01, Meta loss averaged over last 500 steps = 4.1505e-01, PNorm = 67.2333, GNorm = 0.1755
Meta loss on this task batch = 3.6231e-01, Meta loss averaged over last 500 steps = 4.1484e-01, PNorm = 67.2491, GNorm = 0.2140
Meta loss on this task batch = 4.4033e-01, Meta loss averaged over last 500 steps = 4.1484e-01, PNorm = 67.2621, GNorm = 0.1930
Meta loss on this task batch = 3.8848e-01, Meta loss averaged over last 500 steps = 4.1474e-01, PNorm = 67.2748, GNorm = 0.2268
Took 132.40333199501038 seconds to complete one epoch of meta training
Took 139.96452450752258 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467914
Epoch 79
Meta loss on this task batch = 4.1064e-01, Meta loss averaged over last 500 steps = 4.1478e-01, PNorm = 67.2851, GNorm = 0.2667
Meta loss on this task batch = 4.5041e-01, Meta loss averaged over last 500 steps = 4.1493e-01, PNorm = 67.2937, GNorm = 0.2593
Meta loss on this task batch = 4.2176e-01, Meta loss averaged over last 500 steps = 4.1502e-01, PNorm = 67.3022, GNorm = 0.2308
Meta loss on this task batch = 3.7920e-01, Meta loss averaged over last 500 steps = 4.1484e-01, PNorm = 67.3127, GNorm = 0.2042
Meta loss on this task batch = 3.7272e-01, Meta loss averaged over last 500 steps = 4.1477e-01, PNorm = 67.3236, GNorm = 0.1891
Meta loss on this task batch = 3.3174e-01, Meta loss averaged over last 500 steps = 4.1449e-01, PNorm = 67.3353, GNorm = 0.1653
Meta loss on this task batch = 4.4758e-01, Meta loss averaged over last 500 steps = 4.1453e-01, PNorm = 67.3467, GNorm = 0.2022
Meta loss on this task batch = 4.2851e-01, Meta loss averaged over last 500 steps = 4.1461e-01, PNorm = 67.3576, GNorm = 0.2051
Meta loss on this task batch = 3.8999e-01, Meta loss averaged over last 500 steps = 4.1468e-01, PNorm = 67.3694, GNorm = 0.1724
Meta loss on this task batch = 4.1919e-01, Meta loss averaged over last 500 steps = 4.1458e-01, PNorm = 67.3829, GNorm = 0.2285
Meta loss on this task batch = 4.2612e-01, Meta loss averaged over last 500 steps = 4.1469e-01, PNorm = 67.3963, GNorm = 0.1955
Meta loss on this task batch = 3.8357e-01, Meta loss averaged over last 500 steps = 4.1445e-01, PNorm = 67.4079, GNorm = 0.2068
Meta loss on this task batch = 3.9326e-01, Meta loss averaged over last 500 steps = 4.1441e-01, PNorm = 67.4189, GNorm = 0.2065
Meta loss on this task batch = 4.3865e-01, Meta loss averaged over last 500 steps = 4.1441e-01, PNorm = 67.4284, GNorm = 0.2503
Meta loss on this task batch = 4.1750e-01, Meta loss averaged over last 500 steps = 4.1449e-01, PNorm = 67.4385, GNorm = 0.2173
Meta loss on this task batch = 4.4356e-01, Meta loss averaged over last 500 steps = 4.1444e-01, PNorm = 67.4496, GNorm = 0.2047
Meta loss on this task batch = 2.9738e-01, Meta loss averaged over last 500 steps = 4.1414e-01, PNorm = 67.4621, GNorm = 0.1767
Meta loss on this task batch = 4.6243e-01, Meta loss averaged over last 500 steps = 4.1429e-01, PNorm = 67.4734, GNorm = 0.2031
Meta loss on this task batch = 3.5494e-01, Meta loss averaged over last 500 steps = 4.1411e-01, PNorm = 67.4860, GNorm = 0.2316
Took 133.54339981079102 seconds to complete one epoch of meta training
Took 140.93715953826904 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488967
Epoch 80
Meta loss on this task batch = 3.2609e-01, Meta loss averaged over last 500 steps = 4.1378e-01, PNorm = 67.4990, GNorm = 0.1551
Meta loss on this task batch = 4.2322e-01, Meta loss averaged over last 500 steps = 4.1370e-01, PNorm = 67.5128, GNorm = 0.2274
Meta loss on this task batch = 3.8857e-01, Meta loss averaged over last 500 steps = 4.1360e-01, PNorm = 67.5263, GNorm = 0.2431
Meta loss on this task batch = 3.6425e-01, Meta loss averaged over last 500 steps = 4.1346e-01, PNorm = 67.5397, GNorm = 0.2135
Meta loss on this task batch = 4.3794e-01, Meta loss averaged over last 500 steps = 4.1367e-01, PNorm = 67.5528, GNorm = 0.2416
Meta loss on this task batch = 4.4391e-01, Meta loss averaged over last 500 steps = 4.1364e-01, PNorm = 67.5647, GNorm = 0.3211
Meta loss on this task batch = 4.3597e-01, Meta loss averaged over last 500 steps = 4.1376e-01, PNorm = 67.5762, GNorm = 0.2307
Meta loss on this task batch = 4.3005e-01, Meta loss averaged over last 500 steps = 4.1375e-01, PNorm = 67.5874, GNorm = 0.1917
Meta loss on this task batch = 3.7759e-01, Meta loss averaged over last 500 steps = 4.1369e-01, PNorm = 67.6022, GNorm = 0.2525
Meta loss on this task batch = 3.9710e-01, Meta loss averaged over last 500 steps = 4.1364e-01, PNorm = 67.6190, GNorm = 0.2455
Meta loss on this task batch = 4.2785e-01, Meta loss averaged over last 500 steps = 4.1360e-01, PNorm = 67.6376, GNorm = 0.2567
Meta loss on this task batch = 3.8842e-01, Meta loss averaged over last 500 steps = 4.1345e-01, PNorm = 67.6559, GNorm = 0.2395
Meta loss on this task batch = 3.8685e-01, Meta loss averaged over last 500 steps = 4.1333e-01, PNorm = 67.6722, GNorm = 0.2030
Meta loss on this task batch = 3.8839e-01, Meta loss averaged over last 500 steps = 4.1330e-01, PNorm = 67.6877, GNorm = 0.2242
Meta loss on this task batch = 4.6681e-01, Meta loss averaged over last 500 steps = 4.1346e-01, PNorm = 67.7006, GNorm = 0.2728
Meta loss on this task batch = 4.1420e-01, Meta loss averaged over last 500 steps = 4.1339e-01, PNorm = 67.7104, GNorm = 0.2233
Meta loss on this task batch = 3.7957e-01, Meta loss averaged over last 500 steps = 4.1340e-01, PNorm = 67.7195, GNorm = 0.2284
Meta loss on this task batch = 3.8174e-01, Meta loss averaged over last 500 steps = 4.1337e-01, PNorm = 67.7308, GNorm = 0.2511
Meta loss on this task batch = 4.6981e-01, Meta loss averaged over last 500 steps = 4.1341e-01, PNorm = 67.7421, GNorm = 0.2435
Took 132.24696278572083 seconds to complete one epoch of meta training
Took 141.05923008918762 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471344
Epoch 81
Meta loss on this task batch = 4.4585e-01, Meta loss averaged over last 500 steps = 4.1345e-01, PNorm = 67.7551, GNorm = 0.2009
Meta loss on this task batch = 3.9134e-01, Meta loss averaged over last 500 steps = 4.1338e-01, PNorm = 67.7689, GNorm = 0.2097
Meta loss on this task batch = 4.2714e-01, Meta loss averaged over last 500 steps = 4.1335e-01, PNorm = 67.7852, GNorm = 0.2397
Meta loss on this task batch = 3.7199e-01, Meta loss averaged over last 500 steps = 4.1325e-01, PNorm = 67.8036, GNorm = 0.2034
Meta loss on this task batch = 4.2484e-01, Meta loss averaged over last 500 steps = 4.1334e-01, PNorm = 67.8222, GNorm = 0.1831
Meta loss on this task batch = 3.7590e-01, Meta loss averaged over last 500 steps = 4.1323e-01, PNorm = 67.8388, GNorm = 0.1890
Meta loss on this task batch = 3.1193e-01, Meta loss averaged over last 500 steps = 4.1298e-01, PNorm = 67.8531, GNorm = 0.2167
Meta loss on this task batch = 3.9205e-01, Meta loss averaged over last 500 steps = 4.1284e-01, PNorm = 67.8644, GNorm = 0.2319
Meta loss on this task batch = 4.8302e-01, Meta loss averaged over last 500 steps = 4.1298e-01, PNorm = 67.8746, GNorm = 0.2632
Meta loss on this task batch = 4.3438e-01, Meta loss averaged over last 500 steps = 4.1309e-01, PNorm = 67.8833, GNorm = 0.2739
Meta loss on this task batch = 4.0976e-01, Meta loss averaged over last 500 steps = 4.1302e-01, PNorm = 67.8924, GNorm = 0.2017
Meta loss on this task batch = 4.1953e-01, Meta loss averaged over last 500 steps = 4.1295e-01, PNorm = 67.9021, GNorm = 0.2171
Meta loss on this task batch = 3.4894e-01, Meta loss averaged over last 500 steps = 4.1281e-01, PNorm = 67.9146, GNorm = 0.1951
Meta loss on this task batch = 4.0818e-01, Meta loss averaged over last 500 steps = 4.1281e-01, PNorm = 67.9290, GNorm = 0.2281
Meta loss on this task batch = 3.6785e-01, Meta loss averaged over last 500 steps = 4.1273e-01, PNorm = 67.9447, GNorm = 0.2186
Meta loss on this task batch = 4.2649e-01, Meta loss averaged over last 500 steps = 4.1277e-01, PNorm = 67.9607, GNorm = 0.2337
Meta loss on this task batch = 3.7690e-01, Meta loss averaged over last 500 steps = 4.1275e-01, PNorm = 67.9759, GNorm = 0.2215
Meta loss on this task batch = 4.8003e-01, Meta loss averaged over last 500 steps = 4.1289e-01, PNorm = 67.9896, GNorm = 0.2729
Meta loss on this task batch = 4.1980e-01, Meta loss averaged over last 500 steps = 4.1269e-01, PNorm = 68.0019, GNorm = 0.2737
Took 131.76115560531616 seconds to complete one epoch of meta training
Took 140.2759428024292 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488873
Epoch 82
Meta loss on this task batch = 4.0784e-01, Meta loss averaged over last 500 steps = 4.1267e-01, PNorm = 68.0131, GNorm = 0.2278
Meta loss on this task batch = 3.7837e-01, Meta loss averaged over last 500 steps = 4.1261e-01, PNorm = 68.0242, GNorm = 0.1789
Meta loss on this task batch = 4.3212e-01, Meta loss averaged over last 500 steps = 4.1270e-01, PNorm = 68.0360, GNorm = 0.2203
Meta loss on this task batch = 3.6456e-01, Meta loss averaged over last 500 steps = 4.1252e-01, PNorm = 68.0471, GNorm = 0.1985
Meta loss on this task batch = 3.4561e-01, Meta loss averaged over last 500 steps = 4.1233e-01, PNorm = 68.0608, GNorm = 0.1872
Meta loss on this task batch = 3.9233e-01, Meta loss averaged over last 500 steps = 4.1230e-01, PNorm = 68.0753, GNorm = 0.2208
Meta loss on this task batch = 4.0895e-01, Meta loss averaged over last 500 steps = 4.1224e-01, PNorm = 68.0904, GNorm = 0.1838
Meta loss on this task batch = 4.2691e-01, Meta loss averaged over last 500 steps = 4.1236e-01, PNorm = 68.1078, GNorm = 0.2465
Meta loss on this task batch = 4.1774e-01, Meta loss averaged over last 500 steps = 4.1235e-01, PNorm = 68.1250, GNorm = 0.2121
Meta loss on this task batch = 4.2809e-01, Meta loss averaged over last 500 steps = 4.1252e-01, PNorm = 68.1424, GNorm = 0.2062
Meta loss on this task batch = 3.8655e-01, Meta loss averaged over last 500 steps = 4.1248e-01, PNorm = 68.1565, GNorm = 0.2346
Meta loss on this task batch = 3.6358e-01, Meta loss averaged over last 500 steps = 4.1233e-01, PNorm = 68.1661, GNorm = 0.2471
Meta loss on this task batch = 3.5439e-01, Meta loss averaged over last 500 steps = 4.1206e-01, PNorm = 68.1767, GNorm = 0.2281
Meta loss on this task batch = 4.4454e-01, Meta loss averaged over last 500 steps = 4.1210e-01, PNorm = 68.1852, GNorm = 0.2657
Meta loss on this task batch = 3.5436e-01, Meta loss averaged over last 500 steps = 4.1189e-01, PNorm = 68.1916, GNorm = 0.2486
Meta loss on this task batch = 3.7459e-01, Meta loss averaged over last 500 steps = 4.1188e-01, PNorm = 68.1987, GNorm = 0.2164
Meta loss on this task batch = 3.7951e-01, Meta loss averaged over last 500 steps = 4.1175e-01, PNorm = 68.2044, GNorm = 0.2785
Meta loss on this task batch = 4.1864e-01, Meta loss averaged over last 500 steps = 4.1178e-01, PNorm = 68.2112, GNorm = 0.1956
Meta loss on this task batch = 3.4377e-01, Meta loss averaged over last 500 steps = 4.1165e-01, PNorm = 68.2204, GNorm = 0.2260
Took 133.71711540222168 seconds to complete one epoch of meta training
Took 141.98246884346008 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472814
Epoch 83
Meta loss on this task batch = 4.3786e-01, Meta loss averaged over last 500 steps = 4.1169e-01, PNorm = 68.2306, GNorm = 0.2401
Meta loss on this task batch = 3.9336e-01, Meta loss averaged over last 500 steps = 4.1169e-01, PNorm = 68.2424, GNorm = 0.2391
Meta loss on this task batch = 2.9393e-01, Meta loss averaged over last 500 steps = 4.1152e-01, PNorm = 68.2573, GNorm = 0.2232
Meta loss on this task batch = 3.4729e-01, Meta loss averaged over last 500 steps = 4.1134e-01, PNorm = 68.2722, GNorm = 0.1614
Meta loss on this task batch = 4.2049e-01, Meta loss averaged over last 500 steps = 4.1133e-01, PNorm = 68.2872, GNorm = 0.2242
Meta loss on this task batch = 4.3460e-01, Meta loss averaged over last 500 steps = 4.1133e-01, PNorm = 68.3004, GNorm = 0.4021
Meta loss on this task batch = 4.0483e-01, Meta loss averaged over last 500 steps = 4.1128e-01, PNorm = 68.3150, GNorm = 0.2378
Meta loss on this task batch = 3.3582e-01, Meta loss averaged over last 500 steps = 4.1117e-01, PNorm = 68.3301, GNorm = 0.2000
Meta loss on this task batch = 4.4320e-01, Meta loss averaged over last 500 steps = 4.1122e-01, PNorm = 68.3445, GNorm = 0.2244
Meta loss on this task batch = 3.9968e-01, Meta loss averaged over last 500 steps = 4.1119e-01, PNorm = 68.3603, GNorm = 0.2154
Meta loss on this task batch = 4.7726e-01, Meta loss averaged over last 500 steps = 4.1135e-01, PNorm = 68.3767, GNorm = 0.1976
Meta loss on this task batch = 3.8099e-01, Meta loss averaged over last 500 steps = 4.1114e-01, PNorm = 68.3941, GNorm = 0.1898
Meta loss on this task batch = 3.3971e-01, Meta loss averaged over last 500 steps = 4.1100e-01, PNorm = 68.4115, GNorm = 0.1773
Meta loss on this task batch = 3.7723e-01, Meta loss averaged over last 500 steps = 4.1081e-01, PNorm = 68.4303, GNorm = 0.2235
Meta loss on this task batch = 3.6819e-01, Meta loss averaged over last 500 steps = 4.1072e-01, PNorm = 68.4473, GNorm = 0.2198
Meta loss on this task batch = 4.3652e-01, Meta loss averaged over last 500 steps = 4.1078e-01, PNorm = 68.4631, GNorm = 0.2336
Meta loss on this task batch = 4.5712e-01, Meta loss averaged over last 500 steps = 4.1084e-01, PNorm = 68.4772, GNorm = 0.2274
Meta loss on this task batch = 3.6957e-01, Meta loss averaged over last 500 steps = 4.1081e-01, PNorm = 68.4921, GNorm = 0.2043
Meta loss on this task batch = 3.8181e-01, Meta loss averaged over last 500 steps = 4.1093e-01, PNorm = 68.5080, GNorm = 0.2621
Took 132.1792857646942 seconds to complete one epoch of meta training
Took 139.76478052139282 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507311
Epoch 84
Meta loss on this task batch = 4.1500e-01, Meta loss averaged over last 500 steps = 4.1096e-01, PNorm = 68.5217, GNorm = 0.2074
Meta loss on this task batch = 4.2309e-01, Meta loss averaged over last 500 steps = 4.1095e-01, PNorm = 68.5345, GNorm = 0.2176
Meta loss on this task batch = 3.3009e-01, Meta loss averaged over last 500 steps = 4.1056e-01, PNorm = 68.5465, GNorm = 0.1992
Meta loss on this task batch = 3.9198e-01, Meta loss averaged over last 500 steps = 4.1058e-01, PNorm = 68.5603, GNorm = 0.2067
Meta loss on this task batch = 3.3816e-01, Meta loss averaged over last 500 steps = 4.1036e-01, PNorm = 68.5727, GNorm = 0.2216
Meta loss on this task batch = 3.9120e-01, Meta loss averaged over last 500 steps = 4.1026e-01, PNorm = 68.5844, GNorm = 0.2198
Meta loss on this task batch = 4.0136e-01, Meta loss averaged over last 500 steps = 4.1021e-01, PNorm = 68.5947, GNorm = 0.2201
Meta loss on this task batch = 2.9417e-01, Meta loss averaged over last 500 steps = 4.1005e-01, PNorm = 68.6050, GNorm = 0.2036
Meta loss on this task batch = 4.0756e-01, Meta loss averaged over last 500 steps = 4.0999e-01, PNorm = 68.6153, GNorm = 0.1915
Meta loss on this task batch = 4.2285e-01, Meta loss averaged over last 500 steps = 4.0997e-01, PNorm = 68.6250, GNorm = 0.2508
Meta loss on this task batch = 4.3602e-01, Meta loss averaged over last 500 steps = 4.0983e-01, PNorm = 68.6339, GNorm = 0.2320
Meta loss on this task batch = 4.0362e-01, Meta loss averaged over last 500 steps = 4.0985e-01, PNorm = 68.6433, GNorm = 0.2225
Meta loss on this task batch = 4.6448e-01, Meta loss averaged over last 500 steps = 4.0991e-01, PNorm = 68.6507, GNorm = 0.2541
Meta loss on this task batch = 3.5534e-01, Meta loss averaged over last 500 steps = 4.0961e-01, PNorm = 68.6585, GNorm = 0.1860
Meta loss on this task batch = 4.0621e-01, Meta loss averaged over last 500 steps = 4.0964e-01, PNorm = 68.6667, GNorm = 0.2009
Meta loss on this task batch = 3.8439e-01, Meta loss averaged over last 500 steps = 4.0966e-01, PNorm = 68.6766, GNorm = 0.2198
Meta loss on this task batch = 4.2738e-01, Meta loss averaged over last 500 steps = 4.0980e-01, PNorm = 68.6860, GNorm = 0.2361
Meta loss on this task batch = 4.2402e-01, Meta loss averaged over last 500 steps = 4.0986e-01, PNorm = 68.6962, GNorm = 0.2307
Meta loss on this task batch = 4.2494e-01, Meta loss averaged over last 500 steps = 4.0985e-01, PNorm = 68.7082, GNorm = 0.2509
Took 133.80781936645508 seconds to complete one epoch of meta training
Took 142.3105092048645 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494362
Epoch 85
Meta loss on this task batch = 4.2281e-01, Meta loss averaged over last 500 steps = 4.0982e-01, PNorm = 68.7184, GNorm = 0.2427
Meta loss on this task batch = 3.9170e-01, Meta loss averaged over last 500 steps = 4.0982e-01, PNorm = 68.7290, GNorm = 0.2214
Meta loss on this task batch = 4.1227e-01, Meta loss averaged over last 500 steps = 4.0971e-01, PNorm = 68.7403, GNorm = 0.2202
Meta loss on this task batch = 3.5949e-01, Meta loss averaged over last 500 steps = 4.0960e-01, PNorm = 68.7536, GNorm = 0.2292
Meta loss on this task batch = 3.8058e-01, Meta loss averaged over last 500 steps = 4.0959e-01, PNorm = 68.7662, GNorm = 0.2448
Meta loss on this task batch = 3.9141e-01, Meta loss averaged over last 500 steps = 4.0949e-01, PNorm = 68.7777, GNorm = 0.2450
Meta loss on this task batch = 4.0625e-01, Meta loss averaged over last 500 steps = 4.0950e-01, PNorm = 68.7870, GNorm = 0.3623
Meta loss on this task batch = 3.9971e-01, Meta loss averaged over last 500 steps = 4.0947e-01, PNorm = 68.7952, GNorm = 0.2802
Meta loss on this task batch = 3.9472e-01, Meta loss averaged over last 500 steps = 4.0928e-01, PNorm = 68.8032, GNorm = 0.2483
Meta loss on this task batch = 4.2052e-01, Meta loss averaged over last 500 steps = 4.0925e-01, PNorm = 68.8114, GNorm = 0.2281
Meta loss on this task batch = 4.1930e-01, Meta loss averaged over last 500 steps = 4.0932e-01, PNorm = 68.8182, GNorm = 0.2490
Meta loss on this task batch = 4.3587e-01, Meta loss averaged over last 500 steps = 4.0926e-01, PNorm = 68.8273, GNorm = 0.2344
Meta loss on this task batch = 4.2789e-01, Meta loss averaged over last 500 steps = 4.0935e-01, PNorm = 68.8383, GNorm = 0.2488
Meta loss on this task batch = 4.2182e-01, Meta loss averaged over last 500 steps = 4.0923e-01, PNorm = 68.8507, GNorm = 0.2365
Meta loss on this task batch = 3.7886e-01, Meta loss averaged over last 500 steps = 4.0918e-01, PNorm = 68.8656, GNorm = 0.2787
Meta loss on this task batch = 3.8980e-01, Meta loss averaged over last 500 steps = 4.0922e-01, PNorm = 68.8815, GNorm = 0.2132
Meta loss on this task batch = 3.9871e-01, Meta loss averaged over last 500 steps = 4.0914e-01, PNorm = 68.8986, GNorm = 0.2335
Meta loss on this task batch = 4.2704e-01, Meta loss averaged over last 500 steps = 4.0923e-01, PNorm = 68.9161, GNorm = 0.2845
Meta loss on this task batch = 4.7765e-01, Meta loss averaged over last 500 steps = 4.0940e-01, PNorm = 68.9274, GNorm = 0.3715
Took 130.1104063987732 seconds to complete one epoch of meta training
Took 138.49270939826965 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510677
Epoch 86
Meta loss on this task batch = 3.7649e-01, Meta loss averaged over last 500 steps = 4.0927e-01, PNorm = 68.9372, GNorm = 0.2430
Meta loss on this task batch = 3.5743e-01, Meta loss averaged over last 500 steps = 4.0912e-01, PNorm = 68.9461, GNorm = 0.2181
Meta loss on this task batch = 4.0587e-01, Meta loss averaged over last 500 steps = 4.0916e-01, PNorm = 68.9545, GNorm = 0.2428
Meta loss on this task batch = 3.5898e-01, Meta loss averaged over last 500 steps = 4.0903e-01, PNorm = 68.9639, GNorm = 0.2091
Meta loss on this task batch = 3.9583e-01, Meta loss averaged over last 500 steps = 4.0888e-01, PNorm = 68.9743, GNorm = 0.2207
Meta loss on this task batch = 3.9878e-01, Meta loss averaged over last 500 steps = 4.0889e-01, PNorm = 68.9865, GNorm = 0.2251
Meta loss on this task batch = 4.3672e-01, Meta loss averaged over last 500 steps = 4.0892e-01, PNorm = 68.9980, GNorm = 0.2267
Meta loss on this task batch = 4.3810e-01, Meta loss averaged over last 500 steps = 4.0895e-01, PNorm = 69.0098, GNorm = 0.1809
Meta loss on this task batch = 4.5044e-01, Meta loss averaged over last 500 steps = 4.0910e-01, PNorm = 69.0207, GNorm = 0.2022
Meta loss on this task batch = 3.9085e-01, Meta loss averaged over last 500 steps = 4.0909e-01, PNorm = 69.0320, GNorm = 0.1905
Meta loss on this task batch = 3.4757e-01, Meta loss averaged over last 500 steps = 4.0901e-01, PNorm = 69.0450, GNorm = 0.1921
Meta loss on this task batch = 4.1400e-01, Meta loss averaged over last 500 steps = 4.0896e-01, PNorm = 69.0587, GNorm = 0.1696
Meta loss on this task batch = 4.1299e-01, Meta loss averaged over last 500 steps = 4.0906e-01, PNorm = 69.0724, GNorm = 0.1817
Meta loss on this task batch = 3.7548e-01, Meta loss averaged over last 500 steps = 4.0895e-01, PNorm = 69.0857, GNorm = 0.1811
Meta loss on this task batch = 4.6881e-01, Meta loss averaged over last 500 steps = 4.0901e-01, PNorm = 69.0989, GNorm = 0.2790
Meta loss on this task batch = 4.4442e-01, Meta loss averaged over last 500 steps = 4.0901e-01, PNorm = 69.1128, GNorm = 0.2170
Meta loss on this task batch = 3.6410e-01, Meta loss averaged over last 500 steps = 4.0890e-01, PNorm = 69.1282, GNorm = 0.1786
Meta loss on this task batch = 3.5637e-01, Meta loss averaged over last 500 steps = 4.0877e-01, PNorm = 69.1427, GNorm = 0.2127
Meta loss on this task batch = 4.3345e-01, Meta loss averaged over last 500 steps = 4.0867e-01, PNorm = 69.1551, GNorm = 0.2470
Took 132.93139481544495 seconds to complete one epoch of meta training
Took 141.36575531959534 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488028
Epoch 87
Meta loss on this task batch = 3.7683e-01, Meta loss averaged over last 500 steps = 4.0844e-01, PNorm = 69.1677, GNorm = 0.2010
Meta loss on this task batch = 3.6439e-01, Meta loss averaged over last 500 steps = 4.0825e-01, PNorm = 69.1817, GNorm = 0.2057
Meta loss on this task batch = 4.2953e-01, Meta loss averaged over last 500 steps = 4.0829e-01, PNorm = 69.1968, GNorm = 0.2245
Meta loss on this task batch = 4.1920e-01, Meta loss averaged over last 500 steps = 4.0838e-01, PNorm = 69.2114, GNorm = 0.2476
Meta loss on this task batch = 3.6937e-01, Meta loss averaged over last 500 steps = 4.0833e-01, PNorm = 69.2243, GNorm = 0.2075
Meta loss on this task batch = 4.4642e-01, Meta loss averaged over last 500 steps = 4.0838e-01, PNorm = 69.2370, GNorm = 0.2420
Meta loss on this task batch = 3.8708e-01, Meta loss averaged over last 500 steps = 4.0822e-01, PNorm = 69.2506, GNorm = 0.2163
Meta loss on this task batch = 4.4023e-01, Meta loss averaged over last 500 steps = 4.0821e-01, PNorm = 69.2628, GNorm = 0.2473
Meta loss on this task batch = 3.8500e-01, Meta loss averaged over last 500 steps = 4.0815e-01, PNorm = 69.2755, GNorm = 0.2133
Meta loss on this task batch = 4.3698e-01, Meta loss averaged over last 500 steps = 4.0810e-01, PNorm = 69.2893, GNorm = 0.2427
Meta loss on this task batch = 3.6261e-01, Meta loss averaged over last 500 steps = 4.0797e-01, PNorm = 69.3039, GNorm = 0.1751
Meta loss on this task batch = 3.6496e-01, Meta loss averaged over last 500 steps = 4.0782e-01, PNorm = 69.3194, GNorm = 0.1884
Meta loss on this task batch = 4.3114e-01, Meta loss averaged over last 500 steps = 4.0777e-01, PNorm = 69.3342, GNorm = 0.1912
Meta loss on this task batch = 3.9904e-01, Meta loss averaged over last 500 steps = 4.0775e-01, PNorm = 69.3459, GNorm = 0.2168
Meta loss on this task batch = 4.1299e-01, Meta loss averaged over last 500 steps = 4.0774e-01, PNorm = 69.3576, GNorm = 0.2307
Meta loss on this task batch = 3.8652e-01, Meta loss averaged over last 500 steps = 4.0771e-01, PNorm = 69.3688, GNorm = 0.2004
Meta loss on this task batch = 3.8625e-01, Meta loss averaged over last 500 steps = 4.0759e-01, PNorm = 69.3802, GNorm = 0.2038
Meta loss on this task batch = 3.8915e-01, Meta loss averaged over last 500 steps = 4.0765e-01, PNorm = 69.3901, GNorm = 0.2303
Meta loss on this task batch = 4.2433e-01, Meta loss averaged over last 500 steps = 4.0761e-01, PNorm = 69.4008, GNorm = 0.2545
Took 132.47585272789001 seconds to complete one epoch of meta training
Took 141.7829933166504 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491323
Epoch 88
Meta loss on this task batch = 3.7252e-01, Meta loss averaged over last 500 steps = 4.0742e-01, PNorm = 69.4119, GNorm = 0.2019
Meta loss on this task batch = 3.9109e-01, Meta loss averaged over last 500 steps = 4.0740e-01, PNorm = 69.4212, GNorm = 0.2278
Meta loss on this task batch = 3.3284e-01, Meta loss averaged over last 500 steps = 4.0715e-01, PNorm = 69.4298, GNorm = 0.2190
Meta loss on this task batch = 4.2436e-01, Meta loss averaged over last 500 steps = 4.0716e-01, PNorm = 69.4396, GNorm = 0.2077
Meta loss on this task batch = 3.4723e-01, Meta loss averaged over last 500 steps = 4.0706e-01, PNorm = 69.4499, GNorm = 0.2435
Meta loss on this task batch = 3.8711e-01, Meta loss averaged over last 500 steps = 4.0693e-01, PNorm = 69.4601, GNorm = 0.1998
Meta loss on this task batch = 4.3355e-01, Meta loss averaged over last 500 steps = 4.0695e-01, PNorm = 69.4697, GNorm = 0.2337
Meta loss on this task batch = 3.8389e-01, Meta loss averaged over last 500 steps = 4.0680e-01, PNorm = 69.4799, GNorm = 0.2295
Meta loss on this task batch = 4.1532e-01, Meta loss averaged over last 500 steps = 4.0687e-01, PNorm = 69.4893, GNorm = 0.1924
Meta loss on this task batch = 3.2988e-01, Meta loss averaged over last 500 steps = 4.0677e-01, PNorm = 69.5008, GNorm = 0.2109
Meta loss on this task batch = 4.1517e-01, Meta loss averaged over last 500 steps = 4.0669e-01, PNorm = 69.5109, GNorm = 0.2236
Meta loss on this task batch = 4.3418e-01, Meta loss averaged over last 500 steps = 4.0668e-01, PNorm = 69.5227, GNorm = 0.2211
Meta loss on this task batch = 3.6781e-01, Meta loss averaged over last 500 steps = 4.0661e-01, PNorm = 69.5351, GNorm = 0.1890
Meta loss on this task batch = 4.2808e-01, Meta loss averaged over last 500 steps = 4.0668e-01, PNorm = 69.5466, GNorm = 0.2017
Meta loss on this task batch = 3.4338e-01, Meta loss averaged over last 500 steps = 4.0663e-01, PNorm = 69.5591, GNorm = 0.1872
Meta loss on this task batch = 4.6013e-01, Meta loss averaged over last 500 steps = 4.0664e-01, PNorm = 69.5705, GNorm = 0.2298
Meta loss on this task batch = 4.6450e-01, Meta loss averaged over last 500 steps = 4.0669e-01, PNorm = 69.5812, GNorm = 0.2402
Meta loss on this task batch = 4.0572e-01, Meta loss averaged over last 500 steps = 4.0656e-01, PNorm = 69.5930, GNorm = 0.2023
Meta loss on this task batch = 3.9216e-01, Meta loss averaged over last 500 steps = 4.0648e-01, PNorm = 69.6050, GNorm = 0.3163
Took 135.1721420288086 seconds to complete one epoch of meta training
Took 143.5340371131897 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459722
Epoch 89
Meta loss on this task batch = 4.7658e-01, Meta loss averaged over last 500 steps = 4.0657e-01, PNorm = 69.6144, GNorm = 0.2697
Meta loss on this task batch = 3.7728e-01, Meta loss averaged over last 500 steps = 4.0636e-01, PNorm = 69.6250, GNorm = 0.1832
Meta loss on this task batch = 3.6628e-01, Meta loss averaged over last 500 steps = 4.0624e-01, PNorm = 69.6364, GNorm = 0.1728
Meta loss on this task batch = 3.9616e-01, Meta loss averaged over last 500 steps = 4.0632e-01, PNorm = 69.6486, GNorm = 0.2131
Meta loss on this task batch = 3.8589e-01, Meta loss averaged over last 500 steps = 4.0612e-01, PNorm = 69.6619, GNorm = 0.1881
Meta loss on this task batch = 3.4851e-01, Meta loss averaged over last 500 steps = 4.0594e-01, PNorm = 69.6764, GNorm = 0.1933
Meta loss on this task batch = 4.3692e-01, Meta loss averaged over last 500 steps = 4.0597e-01, PNorm = 69.6886, GNorm = 0.2398
Meta loss on this task batch = 3.8701e-01, Meta loss averaged over last 500 steps = 4.0609e-01, PNorm = 69.7002, GNorm = 0.2564
Meta loss on this task batch = 3.0840e-01, Meta loss averaged over last 500 steps = 4.0588e-01, PNorm = 69.7113, GNorm = 0.2023
Meta loss on this task batch = 3.6616e-01, Meta loss averaged over last 500 steps = 4.0581e-01, PNorm = 69.7229, GNorm = 0.2124
Meta loss on this task batch = 4.1347e-01, Meta loss averaged over last 500 steps = 4.0585e-01, PNorm = 69.7345, GNorm = 0.2263
Meta loss on this task batch = 4.2032e-01, Meta loss averaged over last 500 steps = 4.0586e-01, PNorm = 69.7467, GNorm = 0.2138
Meta loss on this task batch = 3.5747e-01, Meta loss averaged over last 500 steps = 4.0572e-01, PNorm = 69.7596, GNorm = 0.2263
Meta loss on this task batch = 4.1416e-01, Meta loss averaged over last 500 steps = 4.0561e-01, PNorm = 69.7725, GNorm = 0.2390
Meta loss on this task batch = 4.1088e-01, Meta loss averaged over last 500 steps = 4.0549e-01, PNorm = 69.7848, GNorm = 0.2525
Meta loss on this task batch = 4.1467e-01, Meta loss averaged over last 500 steps = 4.0555e-01, PNorm = 69.7969, GNorm = 0.2272
Meta loss on this task batch = 4.5025e-01, Meta loss averaged over last 500 steps = 4.0564e-01, PNorm = 69.8088, GNorm = 0.2240
Meta loss on this task batch = 4.3495e-01, Meta loss averaged over last 500 steps = 4.0573e-01, PNorm = 69.8208, GNorm = 0.2218
Meta loss on this task batch = 3.8918e-01, Meta loss averaged over last 500 steps = 4.0572e-01, PNorm = 69.8333, GNorm = 0.2444
Took 133.8241572380066 seconds to complete one epoch of meta training
Took 143.0509479045868 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505701
Epoch 90
Meta loss on this task batch = 4.1551e-01, Meta loss averaged over last 500 steps = 4.0570e-01, PNorm = 69.8465, GNorm = 0.1990
Meta loss on this task batch = 3.9890e-01, Meta loss averaged over last 500 steps = 4.0557e-01, PNorm = 69.8608, GNorm = 0.1988
Meta loss on this task batch = 4.1002e-01, Meta loss averaged over last 500 steps = 4.0556e-01, PNorm = 69.8756, GNorm = 0.2201
Meta loss on this task batch = 3.5103e-01, Meta loss averaged over last 500 steps = 4.0539e-01, PNorm = 69.8915, GNorm = 0.2488
Meta loss on this task batch = 3.9886e-01, Meta loss averaged over last 500 steps = 4.0531e-01, PNorm = 69.9069, GNorm = 0.1939
Meta loss on this task batch = 3.9236e-01, Meta loss averaged over last 500 steps = 4.0526e-01, PNorm = 69.9211, GNorm = 0.2007
Meta loss on this task batch = 3.9833e-01, Meta loss averaged over last 500 steps = 4.0525e-01, PNorm = 69.9335, GNorm = 0.2197
Meta loss on this task batch = 4.4696e-01, Meta loss averaged over last 500 steps = 4.0533e-01, PNorm = 69.9444, GNorm = 0.2770
Meta loss on this task batch = 3.4341e-01, Meta loss averaged over last 500 steps = 4.0519e-01, PNorm = 69.9540, GNorm = 0.2181
Meta loss on this task batch = 3.9063e-01, Meta loss averaged over last 500 steps = 4.0517e-01, PNorm = 69.9620, GNorm = 0.2408
Meta loss on this task batch = 3.9812e-01, Meta loss averaged over last 500 steps = 4.0522e-01, PNorm = 69.9715, GNorm = 0.2178
Meta loss on this task batch = 4.2556e-01, Meta loss averaged over last 500 steps = 4.0530e-01, PNorm = 69.9811, GNorm = 0.2149
Meta loss on this task batch = 3.8045e-01, Meta loss averaged over last 500 steps = 4.0516e-01, PNorm = 69.9929, GNorm = 0.2510
Meta loss on this task batch = 3.8585e-01, Meta loss averaged over last 500 steps = 4.0515e-01, PNorm = 70.0065, GNorm = 0.2487
Meta loss on this task batch = 3.7570e-01, Meta loss averaged over last 500 steps = 4.0503e-01, PNorm = 70.0201, GNorm = 0.2168
Meta loss on this task batch = 5.0518e-01, Meta loss averaged over last 500 steps = 4.0534e-01, PNorm = 70.0298, GNorm = 0.2703
Meta loss on this task batch = 4.1600e-01, Meta loss averaged over last 500 steps = 4.0530e-01, PNorm = 70.0381, GNorm = 0.2421
Meta loss on this task batch = 3.4958e-01, Meta loss averaged over last 500 steps = 4.0521e-01, PNorm = 70.0453, GNorm = 0.2331
Meta loss on this task batch = 4.0808e-01, Meta loss averaged over last 500 steps = 4.0518e-01, PNorm = 70.0526, GNorm = 0.3182
Took 134.93815875053406 seconds to complete one epoch of meta training
Took 144.23686623573303 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491600
Epoch 91
Meta loss on this task batch = 4.1591e-01, Meta loss averaged over last 500 steps = 4.0510e-01, PNorm = 70.0600, GNorm = 0.2830
Meta loss on this task batch = 3.4679e-01, Meta loss averaged over last 500 steps = 4.0492e-01, PNorm = 70.0683, GNorm = 0.2164
Meta loss on this task batch = 3.7021e-01, Meta loss averaged over last 500 steps = 4.0500e-01, PNorm = 70.0781, GNorm = 0.2379
Meta loss on this task batch = 4.5644e-01, Meta loss averaged over last 500 steps = 4.0488e-01, PNorm = 70.0890, GNorm = 0.2028
Meta loss on this task batch = 3.3889e-01, Meta loss averaged over last 500 steps = 4.0467e-01, PNorm = 70.1018, GNorm = 0.2246
Meta loss on this task batch = 4.0675e-01, Meta loss averaged over last 500 steps = 4.0471e-01, PNorm = 70.1153, GNorm = 0.1989
Meta loss on this task batch = 4.1610e-01, Meta loss averaged over last 500 steps = 4.0467e-01, PNorm = 70.1301, GNorm = 0.2430
Meta loss on this task batch = 4.1724e-01, Meta loss averaged over last 500 steps = 4.0462e-01, PNorm = 70.1440, GNorm = 0.2786
Meta loss on this task batch = 3.9749e-01, Meta loss averaged over last 500 steps = 4.0470e-01, PNorm = 70.1576, GNorm = 0.2026
Meta loss on this task batch = 4.2365e-01, Meta loss averaged over last 500 steps = 4.0470e-01, PNorm = 70.1714, GNorm = 0.2176
Meta loss on this task batch = 4.2731e-01, Meta loss averaged over last 500 steps = 4.0471e-01, PNorm = 70.1820, GNorm = 0.3086
Meta loss on this task batch = 4.3483e-01, Meta loss averaged over last 500 steps = 4.0466e-01, PNorm = 70.1921, GNorm = 0.2696
Meta loss on this task batch = 3.6987e-01, Meta loss averaged over last 500 steps = 4.0463e-01, PNorm = 70.2023, GNorm = 0.2077
Meta loss on this task batch = 5.2721e-01, Meta loss averaged over last 500 steps = 4.0479e-01, PNorm = 70.2114, GNorm = 0.3383
Meta loss on this task batch = 3.6977e-01, Meta loss averaged over last 500 steps = 4.0477e-01, PNorm = 70.2196, GNorm = 0.2525
Meta loss on this task batch = 3.3915e-01, Meta loss averaged over last 500 steps = 4.0476e-01, PNorm = 70.2290, GNorm = 0.1994
Meta loss on this task batch = 3.8237e-01, Meta loss averaged over last 500 steps = 4.0463e-01, PNorm = 70.2379, GNorm = 0.2274
Meta loss on this task batch = 3.6967e-01, Meta loss averaged over last 500 steps = 4.0450e-01, PNorm = 70.2476, GNorm = 0.2428
Meta loss on this task batch = 3.8425e-01, Meta loss averaged over last 500 steps = 4.0444e-01, PNorm = 70.2595, GNorm = 0.2487
Took 130.646906375885 seconds to complete one epoch of meta training
Took 139.52902364730835 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465920
Epoch 92
Meta loss on this task batch = 3.7038e-01, Meta loss averaged over last 500 steps = 4.0438e-01, PNorm = 70.2710, GNorm = 0.2202
Meta loss on this task batch = 3.3986e-01, Meta loss averaged over last 500 steps = 4.0422e-01, PNorm = 70.2834, GNorm = 0.1807
Meta loss on this task batch = 4.1543e-01, Meta loss averaged over last 500 steps = 4.0423e-01, PNorm = 70.2956, GNorm = 0.2105
Meta loss on this task batch = 4.1880e-01, Meta loss averaged over last 500 steps = 4.0411e-01, PNorm = 70.3067, GNorm = 0.2481
Meta loss on this task batch = 4.4224e-01, Meta loss averaged over last 500 steps = 4.0417e-01, PNorm = 70.3174, GNorm = 0.2464
Meta loss on this task batch = 3.9276e-01, Meta loss averaged over last 500 steps = 4.0427e-01, PNorm = 70.3275, GNorm = 0.1988
Meta loss on this task batch = 4.0500e-01, Meta loss averaged over last 500 steps = 4.0418e-01, PNorm = 70.3387, GNorm = 0.2232
Meta loss on this task batch = 3.8611e-01, Meta loss averaged over last 500 steps = 4.0412e-01, PNorm = 70.3514, GNorm = 0.2195
Meta loss on this task batch = 3.9613e-01, Meta loss averaged over last 500 steps = 4.0411e-01, PNorm = 70.3627, GNorm = 0.2497
Meta loss on this task batch = 4.2596e-01, Meta loss averaged over last 500 steps = 4.0414e-01, PNorm = 70.3741, GNorm = 0.1999
Meta loss on this task batch = 3.5805e-01, Meta loss averaged over last 500 steps = 4.0388e-01, PNorm = 70.3879, GNorm = 0.2320
Meta loss on this task batch = 3.7453e-01, Meta loss averaged over last 500 steps = 4.0373e-01, PNorm = 70.4022, GNorm = 0.1940
Meta loss on this task batch = 4.7215e-01, Meta loss averaged over last 500 steps = 4.0385e-01, PNorm = 70.4157, GNorm = 0.2747
Meta loss on this task batch = 4.0882e-01, Meta loss averaged over last 500 steps = 4.0386e-01, PNorm = 70.4285, GNorm = 0.2307
Meta loss on this task batch = 4.4738e-01, Meta loss averaged over last 500 steps = 4.0410e-01, PNorm = 70.4413, GNorm = 0.2615
Meta loss on this task batch = 3.4080e-01, Meta loss averaged over last 500 steps = 4.0406e-01, PNorm = 70.4545, GNorm = 0.2001
Meta loss on this task batch = 4.2438e-01, Meta loss averaged over last 500 steps = 4.0415e-01, PNorm = 70.4669, GNorm = 0.2186
Meta loss on this task batch = 3.8196e-01, Meta loss averaged over last 500 steps = 4.0406e-01, PNorm = 70.4784, GNorm = 0.2209
Meta loss on this task batch = 3.7726e-01, Meta loss averaged over last 500 steps = 4.0401e-01, PNorm = 70.4881, GNorm = 0.3082
Took 147.9348921775818 seconds to complete one epoch of meta training
Took 156.41087746620178 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493306
Epoch 93
Meta loss on this task batch = 2.9914e-01, Meta loss averaged over last 500 steps = 4.0374e-01, PNorm = 70.4992, GNorm = 0.2390
Meta loss on this task batch = 4.0221e-01, Meta loss averaged over last 500 steps = 4.0366e-01, PNorm = 70.5103, GNorm = 0.2134
Meta loss on this task batch = 4.1781e-01, Meta loss averaged over last 500 steps = 4.0368e-01, PNorm = 70.5215, GNorm = 0.2304
Meta loss on this task batch = 4.0477e-01, Meta loss averaged over last 500 steps = 4.0362e-01, PNorm = 70.5331, GNorm = 0.2325
Meta loss on this task batch = 4.1016e-01, Meta loss averaged over last 500 steps = 4.0352e-01, PNorm = 70.5467, GNorm = 0.2097
Meta loss on this task batch = 3.7373e-01, Meta loss averaged over last 500 steps = 4.0348e-01, PNorm = 70.5621, GNorm = 0.2270
Meta loss on this task batch = 3.6565e-01, Meta loss averaged over last 500 steps = 4.0348e-01, PNorm = 70.5787, GNorm = 0.2122
Meta loss on this task batch = 5.2119e-01, Meta loss averaged over last 500 steps = 4.0379e-01, PNorm = 70.5910, GNorm = 0.2900
Meta loss on this task batch = 3.7779e-01, Meta loss averaged over last 500 steps = 4.0376e-01, PNorm = 70.6035, GNorm = 0.2016
Meta loss on this task batch = 3.9544e-01, Meta loss averaged over last 500 steps = 4.0380e-01, PNorm = 70.6163, GNorm = 0.1898
Meta loss on this task batch = 3.3907e-01, Meta loss averaged over last 500 steps = 4.0369e-01, PNorm = 70.6300, GNorm = 0.2063
Meta loss on this task batch = 4.2128e-01, Meta loss averaged over last 500 steps = 4.0377e-01, PNorm = 70.6437, GNorm = 0.2485
Meta loss on this task batch = 4.2744e-01, Meta loss averaged over last 500 steps = 4.0368e-01, PNorm = 70.6561, GNorm = 0.3089
Meta loss on this task batch = 4.0351e-01, Meta loss averaged over last 500 steps = 4.0366e-01, PNorm = 70.6683, GNorm = 0.2216
Meta loss on this task batch = 3.8773e-01, Meta loss averaged over last 500 steps = 4.0363e-01, PNorm = 70.6787, GNorm = 0.2205
Meta loss on this task batch = 4.7939e-01, Meta loss averaged over last 500 steps = 4.0367e-01, PNorm = 70.6867, GNorm = 0.2228
Meta loss on this task batch = 3.6879e-01, Meta loss averaged over last 500 steps = 4.0355e-01, PNorm = 70.6969, GNorm = 0.2108
Meta loss on this task batch = 3.7655e-01, Meta loss averaged over last 500 steps = 4.0345e-01, PNorm = 70.7075, GNorm = 0.1945
Meta loss on this task batch = 3.4569e-01, Meta loss averaged over last 500 steps = 4.0328e-01, PNorm = 70.7194, GNorm = 0.2107
Took 234.84451031684875 seconds to complete one epoch of meta training
Took 244.2387890815735 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464830
Epoch 94
Meta loss on this task batch = 4.1278e-01, Meta loss averaged over last 500 steps = 4.0322e-01, PNorm = 70.7331, GNorm = 0.2348
Meta loss on this task batch = 4.0349e-01, Meta loss averaged over last 500 steps = 4.0313e-01, PNorm = 70.7475, GNorm = 0.2321
Meta loss on this task batch = 4.0707e-01, Meta loss averaged over last 500 steps = 4.0310e-01, PNorm = 70.7620, GNorm = 0.2777
Meta loss on this task batch = 3.4222e-01, Meta loss averaged over last 500 steps = 4.0283e-01, PNorm = 70.7758, GNorm = 0.1981
Meta loss on this task batch = 3.5680e-01, Meta loss averaged over last 500 steps = 4.0270e-01, PNorm = 70.7879, GNorm = 0.2250
Meta loss on this task batch = 4.5166e-01, Meta loss averaged over last 500 steps = 4.0268e-01, PNorm = 70.7993, GNorm = 0.2879
Meta loss on this task batch = 4.0328e-01, Meta loss averaged over last 500 steps = 4.0261e-01, PNorm = 70.8104, GNorm = 0.2028
Meta loss on this task batch = 3.2263e-01, Meta loss averaged over last 500 steps = 4.0232e-01, PNorm = 70.8214, GNorm = 0.1804
Meta loss on this task batch = 4.3469e-01, Meta loss averaged over last 500 steps = 4.0233e-01, PNorm = 70.8310, GNorm = 0.2285
Meta loss on this task batch = 4.1857e-01, Meta loss averaged over last 500 steps = 4.0243e-01, PNorm = 70.8399, GNorm = 0.2705
Meta loss on this task batch = 3.9204e-01, Meta loss averaged over last 500 steps = 4.0240e-01, PNorm = 70.8487, GNorm = 0.2213
Meta loss on this task batch = 4.2601e-01, Meta loss averaged over last 500 steps = 4.0243e-01, PNorm = 70.8586, GNorm = 0.2239
Meta loss on this task batch = 3.8591e-01, Meta loss averaged over last 500 steps = 4.0242e-01, PNorm = 70.8694, GNorm = 0.1978
Meta loss on this task batch = 3.7838e-01, Meta loss averaged over last 500 steps = 4.0230e-01, PNorm = 70.8821, GNorm = 0.2056
Meta loss on this task batch = 4.2408e-01, Meta loss averaged over last 500 steps = 4.0236e-01, PNorm = 70.8964, GNorm = 0.2241
Meta loss on this task batch = 3.9889e-01, Meta loss averaged over last 500 steps = 4.0243e-01, PNorm = 70.9112, GNorm = 0.2284
Meta loss on this task batch = 3.9061e-01, Meta loss averaged over last 500 steps = 4.0249e-01, PNorm = 70.9252, GNorm = 0.2140
Meta loss on this task batch = 3.6139e-01, Meta loss averaged over last 500 steps = 4.0238e-01, PNorm = 70.9383, GNorm = 0.1893
Meta loss on this task batch = 3.5887e-01, Meta loss averaged over last 500 steps = 4.0221e-01, PNorm = 70.9496, GNorm = 0.2420
Took 134.5739781856537 seconds to complete one epoch of meta training
Took 143.12006163597107 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463302
Epoch 95
Meta loss on this task batch = 3.8728e-01, Meta loss averaged over last 500 steps = 4.0216e-01, PNorm = 70.9611, GNorm = 0.2088
Meta loss on this task batch = 3.5484e-01, Meta loss averaged over last 500 steps = 4.0200e-01, PNorm = 70.9727, GNorm = 0.1906
Meta loss on this task batch = 3.2550e-01, Meta loss averaged over last 500 steps = 4.0179e-01, PNorm = 70.9845, GNorm = 0.2253
Meta loss on this task batch = 4.6169e-01, Meta loss averaged over last 500 steps = 4.0183e-01, PNorm = 70.9963, GNorm = 0.2467
Meta loss on this task batch = 4.6219e-01, Meta loss averaged over last 500 steps = 4.0194e-01, PNorm = 71.0050, GNorm = 0.3074
Meta loss on this task batch = 3.9208e-01, Meta loss averaged over last 500 steps = 4.0179e-01, PNorm = 71.0141, GNorm = 0.2144
Meta loss on this task batch = 4.1694e-01, Meta loss averaged over last 500 steps = 4.0177e-01, PNorm = 71.0239, GNorm = 0.2185
Meta loss on this task batch = 3.9854e-01, Meta loss averaged over last 500 steps = 4.0185e-01, PNorm = 71.0352, GNorm = 0.2204
Meta loss on this task batch = 4.2348e-01, Meta loss averaged over last 500 steps = 4.0173e-01, PNorm = 71.0472, GNorm = 0.2380
Meta loss on this task batch = 4.2864e-01, Meta loss averaged over last 500 steps = 4.0177e-01, PNorm = 71.0610, GNorm = 0.2303
Meta loss on this task batch = 4.1150e-01, Meta loss averaged over last 500 steps = 4.0176e-01, PNorm = 71.0748, GNorm = 0.2444
Meta loss on this task batch = 3.8212e-01, Meta loss averaged over last 500 steps = 4.0169e-01, PNorm = 71.0897, GNorm = 0.2527
Meta loss on this task batch = 3.6107e-01, Meta loss averaged over last 500 steps = 4.0155e-01, PNorm = 71.1055, GNorm = 0.1877
Meta loss on this task batch = 4.2802e-01, Meta loss averaged over last 500 steps = 4.0149e-01, PNorm = 71.1208, GNorm = 0.1920
Meta loss on this task batch = 3.6960e-01, Meta loss averaged over last 500 steps = 4.0129e-01, PNorm = 71.1350, GNorm = 0.2056
Meta loss on this task batch = 3.4097e-01, Meta loss averaged over last 500 steps = 4.0116e-01, PNorm = 71.1496, GNorm = 0.2230
Meta loss on this task batch = 3.5759e-01, Meta loss averaged over last 500 steps = 4.0102e-01, PNorm = 71.1644, GNorm = 0.2231
Meta loss on this task batch = 4.1294e-01, Meta loss averaged over last 500 steps = 4.0105e-01, PNorm = 71.1766, GNorm = 0.3191
Meta loss on this task batch = 3.8376e-01, Meta loss averaged over last 500 steps = 4.0105e-01, PNorm = 71.1872, GNorm = 0.3595
Took 132.814204454422 seconds to complete one epoch of meta training
Took 141.13087463378906 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506329
Epoch 96
Meta loss on this task batch = 4.1711e-01, Meta loss averaged over last 500 steps = 4.0115e-01, PNorm = 71.1986, GNorm = 0.2293
Meta loss on this task batch = 3.6204e-01, Meta loss averaged over last 500 steps = 4.0109e-01, PNorm = 71.2123, GNorm = 0.2316
Meta loss on this task batch = 3.9718e-01, Meta loss averaged over last 500 steps = 4.0109e-01, PNorm = 71.2262, GNorm = 0.2058
Meta loss on this task batch = 3.5989e-01, Meta loss averaged over last 500 steps = 4.0105e-01, PNorm = 71.2418, GNorm = 0.2456
Meta loss on this task batch = 4.2039e-01, Meta loss averaged over last 500 steps = 4.0102e-01, PNorm = 71.2566, GNorm = 0.2838
Meta loss on this task batch = 4.3486e-01, Meta loss averaged over last 500 steps = 4.0100e-01, PNorm = 71.2705, GNorm = 0.2253
Meta loss on this task batch = 4.1115e-01, Meta loss averaged over last 500 steps = 4.0103e-01, PNorm = 71.2845, GNorm = 0.2394
Meta loss on this task batch = 4.2120e-01, Meta loss averaged over last 500 steps = 4.0108e-01, PNorm = 71.2976, GNorm = 0.2861
Meta loss on this task batch = 3.2998e-01, Meta loss averaged over last 500 steps = 4.0097e-01, PNorm = 71.3107, GNorm = 0.2507
Meta loss on this task batch = 3.8557e-01, Meta loss averaged over last 500 steps = 4.0095e-01, PNorm = 71.3238, GNorm = 0.2215
Meta loss on this task batch = 3.5408e-01, Meta loss averaged over last 500 steps = 4.0082e-01, PNorm = 71.3355, GNorm = 0.2153
Meta loss on this task batch = 3.7347e-01, Meta loss averaged over last 500 steps = 4.0069e-01, PNorm = 71.3468, GNorm = 0.2260
Meta loss on this task batch = 3.7276e-01, Meta loss averaged over last 500 steps = 4.0054e-01, PNorm = 71.3581, GNorm = 0.2294
Meta loss on this task batch = 3.0447e-01, Meta loss averaged over last 500 steps = 4.0026e-01, PNorm = 71.3685, GNorm = 0.2065
Meta loss on this task batch = 4.6240e-01, Meta loss averaged over last 500 steps = 4.0034e-01, PNorm = 71.3791, GNorm = 0.2857
Meta loss on this task batch = 3.8023e-01, Meta loss averaged over last 500 steps = 4.0027e-01, PNorm = 71.3896, GNorm = 0.2143
Meta loss on this task batch = 3.7184e-01, Meta loss averaged over last 500 steps = 4.0012e-01, PNorm = 71.4018, GNorm = 0.2057
Meta loss on this task batch = 4.3439e-01, Meta loss averaged over last 500 steps = 4.0024e-01, PNorm = 71.4130, GNorm = 0.2160
Meta loss on this task batch = 4.4161e-01, Meta loss averaged over last 500 steps = 4.0034e-01, PNorm = 71.4254, GNorm = 0.2529
Took 132.7898907661438 seconds to complete one epoch of meta training
Took 141.1383011341095 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489147
Epoch 97
Meta loss on this task batch = 2.8652e-01, Meta loss averaged over last 500 steps = 4.0020e-01, PNorm = 71.4387, GNorm = 0.2125
Meta loss on this task batch = 3.6047e-01, Meta loss averaged over last 500 steps = 4.0008e-01, PNorm = 71.4516, GNorm = 0.1992
Meta loss on this task batch = 3.8119e-01, Meta loss averaged over last 500 steps = 4.0013e-01, PNorm = 71.4652, GNorm = 0.2087
Meta loss on this task batch = 4.4665e-01, Meta loss averaged over last 500 steps = 4.0022e-01, PNorm = 71.4788, GNorm = 0.2335
Meta loss on this task batch = 3.9098e-01, Meta loss averaged over last 500 steps = 4.0019e-01, PNorm = 71.4926, GNorm = 0.2153
Meta loss on this task batch = 3.7436e-01, Meta loss averaged over last 500 steps = 3.9998e-01, PNorm = 71.5055, GNorm = 0.1954
Meta loss on this task batch = 4.4511e-01, Meta loss averaged over last 500 steps = 4.0018e-01, PNorm = 71.5184, GNorm = 0.2510
Meta loss on this task batch = 4.0127e-01, Meta loss averaged over last 500 steps = 4.0013e-01, PNorm = 71.5314, GNorm = 0.2062
Meta loss on this task batch = 3.7121e-01, Meta loss averaged over last 500 steps = 4.0001e-01, PNorm = 71.5431, GNorm = 0.1969
Meta loss on this task batch = 3.8505e-01, Meta loss averaged over last 500 steps = 3.9996e-01, PNorm = 71.5547, GNorm = 0.2318
Meta loss on this task batch = 3.9821e-01, Meta loss averaged over last 500 steps = 3.9997e-01, PNorm = 71.5650, GNorm = 0.2696
Meta loss on this task batch = 4.2755e-01, Meta loss averaged over last 500 steps = 3.9996e-01, PNorm = 71.5755, GNorm = 0.2359
Meta loss on this task batch = 4.0142e-01, Meta loss averaged over last 500 steps = 3.9996e-01, PNorm = 71.5864, GNorm = 0.2146
Meta loss on this task batch = 4.3247e-01, Meta loss averaged over last 500 steps = 3.9998e-01, PNorm = 71.5979, GNorm = 0.2310
Meta loss on this task batch = 3.4380e-01, Meta loss averaged over last 500 steps = 3.9992e-01, PNorm = 71.6094, GNorm = 0.2017
Meta loss on this task batch = 4.6600e-01, Meta loss averaged over last 500 steps = 4.0019e-01, PNorm = 71.6209, GNorm = 0.2564
Meta loss on this task batch = 4.0883e-01, Meta loss averaged over last 500 steps = 4.0004e-01, PNorm = 71.6309, GNorm = 0.2789
Meta loss on this task batch = 3.5703e-01, Meta loss averaged over last 500 steps = 3.9998e-01, PNorm = 71.6410, GNorm = 0.2045
Meta loss on this task batch = 3.8143e-01, Meta loss averaged over last 500 steps = 4.0004e-01, PNorm = 71.6515, GNorm = 0.2773
Took 149.06633591651917 seconds to complete one epoch of meta training
Took 157.65543818473816 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515584
Epoch 98
Meta loss on this task batch = 3.3531e-01, Meta loss averaged over last 500 steps = 3.9991e-01, PNorm = 71.6634, GNorm = 0.2252
Meta loss on this task batch = 3.2614e-01, Meta loss averaged over last 500 steps = 3.9976e-01, PNorm = 71.6751, GNorm = 0.2146
Meta loss on this task batch = 2.7492e-01, Meta loss averaged over last 500 steps = 3.9943e-01, PNorm = 71.6883, GNorm = 0.1998
Meta loss on this task batch = 3.6599e-01, Meta loss averaged over last 500 steps = 3.9929e-01, PNorm = 71.7008, GNorm = 0.2012
Meta loss on this task batch = 4.3948e-01, Meta loss averaged over last 500 steps = 3.9950e-01, PNorm = 71.7125, GNorm = 0.2259
Meta loss on this task batch = 4.1183e-01, Meta loss averaged over last 500 steps = 3.9944e-01, PNorm = 71.7202, GNorm = 0.2944
Meta loss on this task batch = 4.3638e-01, Meta loss averaged over last 500 steps = 3.9946e-01, PNorm = 71.7267, GNorm = 0.2209
Meta loss on this task batch = 4.2989e-01, Meta loss averaged over last 500 steps = 3.9948e-01, PNorm = 71.7336, GNorm = 0.2232
Meta loss on this task batch = 3.6844e-01, Meta loss averaged over last 500 steps = 3.9932e-01, PNorm = 71.7418, GNorm = 0.1895
Meta loss on this task batch = 3.9145e-01, Meta loss averaged over last 500 steps = 3.9940e-01, PNorm = 71.7508, GNorm = 0.2309
Meta loss on this task batch = 4.3719e-01, Meta loss averaged over last 500 steps = 3.9947e-01, PNorm = 71.7599, GNorm = 0.2192
Meta loss on this task batch = 3.7363e-01, Meta loss averaged over last 500 steps = 3.9932e-01, PNorm = 71.7696, GNorm = 0.1907
Meta loss on this task batch = 4.5854e-01, Meta loss averaged over last 500 steps = 3.9932e-01, PNorm = 71.7792, GNorm = 0.2474
Meta loss on this task batch = 3.8852e-01, Meta loss averaged over last 500 steps = 3.9935e-01, PNorm = 71.7897, GNorm = 0.2072
Meta loss on this task batch = 3.9854e-01, Meta loss averaged over last 500 steps = 3.9930e-01, PNorm = 71.7994, GNorm = 0.2158
Meta loss on this task batch = 3.5489e-01, Meta loss averaged over last 500 steps = 3.9926e-01, PNorm = 71.8105, GNorm = 0.1967
Meta loss on this task batch = 4.3300e-01, Meta loss averaged over last 500 steps = 3.9930e-01, PNorm = 71.8209, GNorm = 0.2213
Meta loss on this task batch = 4.1060e-01, Meta loss averaged over last 500 steps = 3.9923e-01, PNorm = 71.8313, GNorm = 0.2674
Meta loss on this task batch = 3.3174e-01, Meta loss averaged over last 500 steps = 3.9918e-01, PNorm = 71.8447, GNorm = 0.2335
Took 136.0323178768158 seconds to complete one epoch of meta training
Took 144.55139565467834 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.443374
Epoch 99
Meta loss on this task batch = 4.2663e-01, Meta loss averaged over last 500 steps = 3.9928e-01, PNorm = 71.8585, GNorm = 0.2625
Meta loss on this task batch = 4.2592e-01, Meta loss averaged over last 500 steps = 3.9920e-01, PNorm = 71.8732, GNorm = 0.2539
Meta loss on this task batch = 4.3477e-01, Meta loss averaged over last 500 steps = 3.9921e-01, PNorm = 71.8887, GNorm = 0.2851
Meta loss on this task batch = 3.2327e-01, Meta loss averaged over last 500 steps = 3.9907e-01, PNorm = 71.9057, GNorm = 0.2178
Meta loss on this task batch = 4.7371e-01, Meta loss averaged over last 500 steps = 3.9930e-01, PNorm = 71.9208, GNorm = 0.2474
Meta loss on this task batch = 4.1739e-01, Meta loss averaged over last 500 steps = 3.9945e-01, PNorm = 71.9356, GNorm = 0.2460
Meta loss on this task batch = 3.8336e-01, Meta loss averaged over last 500 steps = 3.9942e-01, PNorm = 71.9487, GNorm = 0.2280
Meta loss on this task batch = 3.7048e-01, Meta loss averaged over last 500 steps = 3.9928e-01, PNorm = 71.9621, GNorm = 0.2278
Meta loss on this task batch = 3.7125e-01, Meta loss averaged over last 500 steps = 3.9910e-01, PNorm = 71.9741, GNorm = 0.2515
Meta loss on this task batch = 4.0287e-01, Meta loss averaged over last 500 steps = 3.9904e-01, PNorm = 71.9839, GNorm = 0.2446
Meta loss on this task batch = 4.2216e-01, Meta loss averaged over last 500 steps = 3.9915e-01, PNorm = 71.9931, GNorm = 0.2288
Meta loss on this task batch = 3.3276e-01, Meta loss averaged over last 500 steps = 3.9907e-01, PNorm = 72.0004, GNorm = 0.2444
Meta loss on this task batch = 4.0261e-01, Meta loss averaged over last 500 steps = 3.9914e-01, PNorm = 72.0080, GNorm = 0.2022
Meta loss on this task batch = 3.8892e-01, Meta loss averaged over last 500 steps = 3.9907e-01, PNorm = 72.0154, GNorm = 0.2262
Meta loss on this task batch = 4.1830e-01, Meta loss averaged over last 500 steps = 3.9914e-01, PNorm = 72.0242, GNorm = 0.2262
Meta loss on this task batch = 3.7841e-01, Meta loss averaged over last 500 steps = 3.9902e-01, PNorm = 72.0343, GNorm = 0.2258
Meta loss on this task batch = 3.3935e-01, Meta loss averaged over last 500 steps = 3.9884e-01, PNorm = 72.0460, GNorm = 0.2001
Meta loss on this task batch = 3.5046e-01, Meta loss averaged over last 500 steps = 3.9863e-01, PNorm = 72.0582, GNorm = 0.2032
Meta loss on this task batch = 4.2281e-01, Meta loss averaged over last 500 steps = 3.9870e-01, PNorm = 72.0711, GNorm = 0.2579
Took 132.90985584259033 seconds to complete one epoch of meta training
Took 140.51058173179626 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499108
Epoch 100
Meta loss on this task batch = 4.2838e-01, Meta loss averaged over last 500 steps = 3.9887e-01, PNorm = 72.0824, GNorm = 0.2336
Meta loss on this task batch = 4.2267e-01, Meta loss averaged over last 500 steps = 3.9887e-01, PNorm = 72.0935, GNorm = 0.2375
Meta loss on this task batch = 3.4528e-01, Meta loss averaged over last 500 steps = 3.9870e-01, PNorm = 72.1055, GNorm = 0.1818
Meta loss on this task batch = 3.5187e-01, Meta loss averaged over last 500 steps = 3.9869e-01, PNorm = 72.1171, GNorm = 0.2292
Meta loss on this task batch = 3.6820e-01, Meta loss averaged over last 500 steps = 3.9862e-01, PNorm = 72.1287, GNorm = 0.2401
Meta loss on this task batch = 4.2477e-01, Meta loss averaged over last 500 steps = 3.9862e-01, PNorm = 72.1384, GNorm = 0.2810
Meta loss on this task batch = 3.7864e-01, Meta loss averaged over last 500 steps = 3.9858e-01, PNorm = 72.1484, GNorm = 0.2323
Meta loss on this task batch = 4.2314e-01, Meta loss averaged over last 500 steps = 3.9856e-01, PNorm = 72.1559, GNorm = 0.2186
Meta loss on this task batch = 3.8560e-01, Meta loss averaged over last 500 steps = 3.9851e-01, PNorm = 72.1622, GNorm = 0.2348
Meta loss on this task batch = 3.6278e-01, Meta loss averaged over last 500 steps = 3.9839e-01, PNorm = 72.1706, GNorm = 0.2289
Meta loss on this task batch = 3.7081e-01, Meta loss averaged over last 500 steps = 3.9837e-01, PNorm = 72.1804, GNorm = 0.2340
Meta loss on this task batch = 4.1780e-01, Meta loss averaged over last 500 steps = 3.9831e-01, PNorm = 72.1892, GNorm = 0.2624
Meta loss on this task batch = 4.5973e-01, Meta loss averaged over last 500 steps = 3.9849e-01, PNorm = 72.1956, GNorm = 0.3093
Meta loss on this task batch = 4.2462e-01, Meta loss averaged over last 500 steps = 3.9853e-01, PNorm = 72.2037, GNorm = 0.2338
Meta loss on this task batch = 3.7207e-01, Meta loss averaged over last 500 steps = 3.9843e-01, PNorm = 72.2128, GNorm = 0.2393
Meta loss on this task batch = 4.3321e-01, Meta loss averaged over last 500 steps = 3.9849e-01, PNorm = 72.2226, GNorm = 0.2323
Meta loss on this task batch = 2.6943e-01, Meta loss averaged over last 500 steps = 3.9835e-01, PNorm = 72.2344, GNorm = 0.2018
Meta loss on this task batch = 4.3068e-01, Meta loss averaged over last 500 steps = 3.9850e-01, PNorm = 72.2437, GNorm = 0.3201
Meta loss on this task batch = 3.8603e-01, Meta loss averaged over last 500 steps = 3.9861e-01, PNorm = 72.2541, GNorm = 0.2613
Took 133.00479364395142 seconds to complete one epoch of meta training
Took 141.9935758113861 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501491
Epoch 101
Meta loss on this task batch = 3.8626e-01, Meta loss averaged over last 500 steps = 3.9850e-01, PNorm = 72.2621, GNorm = 0.2021
Meta loss on this task batch = 4.1720e-01, Meta loss averaged over last 500 steps = 3.9831e-01, PNorm = 72.2691, GNorm = 0.2472
Meta loss on this task batch = 3.9171e-01, Meta loss averaged over last 500 steps = 3.9833e-01, PNorm = 72.2741, GNorm = 0.2296
Meta loss on this task batch = 4.2813e-01, Meta loss averaged over last 500 steps = 3.9841e-01, PNorm = 72.2790, GNorm = 0.2522
Meta loss on this task batch = 3.6617e-01, Meta loss averaged over last 500 steps = 3.9828e-01, PNorm = 72.2845, GNorm = 0.2034
Meta loss on this task batch = 3.7209e-01, Meta loss averaged over last 500 steps = 3.9829e-01, PNorm = 72.2927, GNorm = 0.2100
Meta loss on this task batch = 3.6422e-01, Meta loss averaged over last 500 steps = 3.9822e-01, PNorm = 72.3030, GNorm = 0.2227
Meta loss on this task batch = 3.4084e-01, Meta loss averaged over last 500 steps = 3.9811e-01, PNorm = 72.3150, GNorm = 0.1870
Meta loss on this task batch = 4.0760e-01, Meta loss averaged over last 500 steps = 3.9812e-01, PNorm = 72.3269, GNorm = 0.1984
Meta loss on this task batch = 3.8063e-01, Meta loss averaged over last 500 steps = 3.9819e-01, PNorm = 72.3398, GNorm = 0.2297
Meta loss on this task batch = 3.3812e-01, Meta loss averaged over last 500 steps = 3.9802e-01, PNorm = 72.3534, GNorm = 0.2440
Meta loss on this task batch = 3.9141e-01, Meta loss averaged over last 500 steps = 3.9798e-01, PNorm = 72.3671, GNorm = 0.2302
Meta loss on this task batch = 4.4332e-01, Meta loss averaged over last 500 steps = 3.9800e-01, PNorm = 72.3778, GNorm = 0.3057
Meta loss on this task batch = 4.2056e-01, Meta loss averaged over last 500 steps = 3.9807e-01, PNorm = 72.3872, GNorm = 0.2215
Meta loss on this task batch = 3.5674e-01, Meta loss averaged over last 500 steps = 3.9784e-01, PNorm = 72.3971, GNorm = 0.2002
Meta loss on this task batch = 3.9663e-01, Meta loss averaged over last 500 steps = 3.9777e-01, PNorm = 72.4062, GNorm = 0.2534
Meta loss on this task batch = 4.5101e-01, Meta loss averaged over last 500 steps = 3.9781e-01, PNorm = 72.4131, GNorm = 0.2463
Meta loss on this task batch = 4.1169e-01, Meta loss averaged over last 500 steps = 3.9782e-01, PNorm = 72.4232, GNorm = 0.2189
Meta loss on this task batch = 3.9680e-01, Meta loss averaged over last 500 steps = 3.9782e-01, PNorm = 72.4344, GNorm = 0.2766
Took 131.28537797927856 seconds to complete one epoch of meta training
Took 139.5480535030365 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483565
Epoch 102
Meta loss on this task batch = 3.6937e-01, Meta loss averaged over last 500 steps = 3.9779e-01, PNorm = 72.4465, GNorm = 0.1888
Meta loss on this task batch = 4.2936e-01, Meta loss averaged over last 500 steps = 3.9786e-01, PNorm = 72.4587, GNorm = 0.2153
Meta loss on this task batch = 4.0899e-01, Meta loss averaged over last 500 steps = 3.9785e-01, PNorm = 72.4704, GNorm = 0.1992
Meta loss on this task batch = 3.6381e-01, Meta loss averaged over last 500 steps = 3.9786e-01, PNorm = 72.4827, GNorm = 0.2095
Meta loss on this task batch = 3.6591e-01, Meta loss averaged over last 500 steps = 3.9789e-01, PNorm = 72.4966, GNorm = 0.2257
Meta loss on this task batch = 3.6461e-01, Meta loss averaged over last 500 steps = 3.9781e-01, PNorm = 72.5086, GNorm = 0.2088
Meta loss on this task batch = 3.7127e-01, Meta loss averaged over last 500 steps = 3.9776e-01, PNorm = 72.5208, GNorm = 0.2310
Meta loss on this task batch = 4.0613e-01, Meta loss averaged over last 500 steps = 3.9779e-01, PNorm = 72.5314, GNorm = 0.2725
Meta loss on this task batch = 3.6286e-01, Meta loss averaged over last 500 steps = 3.9763e-01, PNorm = 72.5417, GNorm = 0.2119
Meta loss on this task batch = 3.9207e-01, Meta loss averaged over last 500 steps = 3.9764e-01, PNorm = 72.5521, GNorm = 0.2094
Meta loss on this task batch = 3.9597e-01, Meta loss averaged over last 500 steps = 3.9761e-01, PNorm = 72.5639, GNorm = 0.2407
Meta loss on this task batch = 4.0446e-01, Meta loss averaged over last 500 steps = 3.9781e-01, PNorm = 72.5758, GNorm = 0.2119
Meta loss on this task batch = 4.2622e-01, Meta loss averaged over last 500 steps = 3.9789e-01, PNorm = 72.5892, GNorm = 0.2286
Meta loss on this task batch = 3.7662e-01, Meta loss averaged over last 500 steps = 3.9779e-01, PNorm = 72.6015, GNorm = 0.2363
Meta loss on this task batch = 3.7964e-01, Meta loss averaged over last 500 steps = 3.9774e-01, PNorm = 72.6136, GNorm = 0.2348
Meta loss on this task batch = 3.4314e-01, Meta loss averaged over last 500 steps = 3.9757e-01, PNorm = 72.6270, GNorm = 0.2180
Meta loss on this task batch = 3.5536e-01, Meta loss averaged over last 500 steps = 3.9734e-01, PNorm = 72.6393, GNorm = 0.2101
Meta loss on this task batch = 4.2847e-01, Meta loss averaged over last 500 steps = 3.9745e-01, PNorm = 72.6509, GNorm = 0.2625
Meta loss on this task batch = 3.0770e-01, Meta loss averaged over last 500 steps = 3.9727e-01, PNorm = 72.6612, GNorm = 0.2597
Took 128.42957210540771 seconds to complete one epoch of meta training
Took 137.38824033737183 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495907
Epoch 103
Meta loss on this task batch = 3.5562e-01, Meta loss averaged over last 500 steps = 3.9727e-01, PNorm = 72.6730, GNorm = 0.2845
Meta loss on this task batch = 4.0851e-01, Meta loss averaged over last 500 steps = 3.9730e-01, PNorm = 72.6833, GNorm = 0.2586
Meta loss on this task batch = 3.7165e-01, Meta loss averaged over last 500 steps = 3.9710e-01, PNorm = 72.6935, GNorm = 0.2447
Meta loss on this task batch = 3.9067e-01, Meta loss averaged over last 500 steps = 3.9712e-01, PNorm = 72.7043, GNorm = 0.2196
Meta loss on this task batch = 3.5670e-01, Meta loss averaged over last 500 steps = 3.9698e-01, PNorm = 72.7162, GNorm = 0.1908
Meta loss on this task batch = 3.8898e-01, Meta loss averaged over last 500 steps = 3.9690e-01, PNorm = 72.7270, GNorm = 0.2086
Meta loss on this task batch = 4.1887e-01, Meta loss averaged over last 500 steps = 3.9694e-01, PNorm = 72.7385, GNorm = 0.2110
Meta loss on this task batch = 3.7079e-01, Meta loss averaged over last 500 steps = 3.9700e-01, PNorm = 72.7502, GNorm = 0.2015
Meta loss on this task batch = 3.7261e-01, Meta loss averaged over last 500 steps = 3.9701e-01, PNorm = 72.7632, GNorm = 0.2297
Meta loss on this task batch = 3.9842e-01, Meta loss averaged over last 500 steps = 3.9699e-01, PNorm = 72.7752, GNorm = 0.2336
Meta loss on this task batch = 4.1778e-01, Meta loss averaged over last 500 steps = 3.9702e-01, PNorm = 72.7869, GNorm = 0.2808
Meta loss on this task batch = 3.4183e-01, Meta loss averaged over last 500 steps = 3.9685e-01, PNorm = 72.7981, GNorm = 0.2248
Meta loss on this task batch = 3.7769e-01, Meta loss averaged over last 500 steps = 3.9678e-01, PNorm = 72.8102, GNorm = 0.1904
Meta loss on this task batch = 3.8775e-01, Meta loss averaged over last 500 steps = 3.9669e-01, PNorm = 72.8239, GNorm = 0.2301
Meta loss on this task batch = 4.0977e-01, Meta loss averaged over last 500 steps = 3.9663e-01, PNorm = 72.8377, GNorm = 0.2265
Meta loss on this task batch = 4.1994e-01, Meta loss averaged over last 500 steps = 3.9670e-01, PNorm = 72.8508, GNorm = 0.2595
Meta loss on this task batch = 3.5604e-01, Meta loss averaged over last 500 steps = 3.9660e-01, PNorm = 72.8616, GNorm = 0.2270
Meta loss on this task batch = 4.2671e-01, Meta loss averaged over last 500 steps = 3.9648e-01, PNorm = 72.8733, GNorm = 0.2361
Meta loss on this task batch = 3.0995e-01, Meta loss averaged over last 500 steps = 3.9624e-01, PNorm = 72.8838, GNorm = 0.2679
Took 133.5079963207245 seconds to complete one epoch of meta training
Took 140.9808943271637 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491777
Epoch 104
Meta loss on this task batch = 4.1803e-01, Meta loss averaged over last 500 steps = 3.9622e-01, PNorm = 72.8927, GNorm = 0.2496
Meta loss on this task batch = 3.7318e-01, Meta loss averaged over last 500 steps = 3.9612e-01, PNorm = 72.9018, GNorm = 0.2616
Meta loss on this task batch = 3.4949e-01, Meta loss averaged over last 500 steps = 3.9594e-01, PNorm = 72.9116, GNorm = 0.2258
Meta loss on this task batch = 3.6659e-01, Meta loss averaged over last 500 steps = 3.9592e-01, PNorm = 72.9212, GNorm = 0.2257
Meta loss on this task batch = 4.2567e-01, Meta loss averaged over last 500 steps = 3.9603e-01, PNorm = 72.9322, GNorm = 0.2523
Meta loss on this task batch = 4.2031e-01, Meta loss averaged over last 500 steps = 3.9617e-01, PNorm = 72.9428, GNorm = 0.2162
Meta loss on this task batch = 3.8399e-01, Meta loss averaged over last 500 steps = 3.9615e-01, PNorm = 72.9550, GNorm = 0.2348
Meta loss on this task batch = 3.6516e-01, Meta loss averaged over last 500 steps = 3.9605e-01, PNorm = 72.9671, GNorm = 0.2127
Meta loss on this task batch = 3.8778e-01, Meta loss averaged over last 500 steps = 3.9614e-01, PNorm = 72.9794, GNorm = 0.2434
Meta loss on this task batch = 4.4171e-01, Meta loss averaged over last 500 steps = 3.9610e-01, PNorm = 72.9894, GNorm = 0.2533
Meta loss on this task batch = 3.8556e-01, Meta loss averaged over last 500 steps = 3.9616e-01, PNorm = 72.9997, GNorm = 0.2180
Meta loss on this task batch = 4.3630e-01, Meta loss averaged over last 500 steps = 3.9616e-01, PNorm = 73.0103, GNorm = 0.2771
Meta loss on this task batch = 3.8005e-01, Meta loss averaged over last 500 steps = 3.9603e-01, PNorm = 73.0219, GNorm = 0.2406
Meta loss on this task batch = 4.3372e-01, Meta loss averaged over last 500 steps = 3.9625e-01, PNorm = 73.0342, GNorm = 0.2237
Meta loss on this task batch = 3.4088e-01, Meta loss averaged over last 500 steps = 3.9606e-01, PNorm = 73.0465, GNorm = 0.2068
Meta loss on this task batch = 4.1954e-01, Meta loss averaged over last 500 steps = 3.9600e-01, PNorm = 73.0601, GNorm = 0.2494
Meta loss on this task batch = 3.6331e-01, Meta loss averaged over last 500 steps = 3.9601e-01, PNorm = 73.0743, GNorm = 0.2111
Meta loss on this task batch = 3.8993e-01, Meta loss averaged over last 500 steps = 3.9596e-01, PNorm = 73.0896, GNorm = 0.2213
Meta loss on this task batch = 3.5280e-01, Meta loss averaged over last 500 steps = 3.9573e-01, PNorm = 73.1048, GNorm = 0.2558
Took 131.89318323135376 seconds to complete one epoch of meta training
Took 140.89373683929443 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494306
Epoch 105
Meta loss on this task batch = 3.9555e-01, Meta loss averaged over last 500 steps = 3.9575e-01, PNorm = 73.1191, GNorm = 0.2310
Meta loss on this task batch = 4.4584e-01, Meta loss averaged over last 500 steps = 3.9589e-01, PNorm = 73.1326, GNorm = 0.2744
Meta loss on this task batch = 3.4375e-01, Meta loss averaged over last 500 steps = 3.9582e-01, PNorm = 73.1436, GNorm = 0.2380
Meta loss on this task batch = 4.0045e-01, Meta loss averaged over last 500 steps = 3.9589e-01, PNorm = 73.1524, GNorm = 0.2489
Meta loss on this task batch = 3.7315e-01, Meta loss averaged over last 500 steps = 3.9576e-01, PNorm = 73.1624, GNorm = 0.2429
Meta loss on this task batch = 4.1461e-01, Meta loss averaged over last 500 steps = 3.9581e-01, PNorm = 73.1715, GNorm = 0.2532
Meta loss on this task batch = 3.6788e-01, Meta loss averaged over last 500 steps = 3.9573e-01, PNorm = 73.1820, GNorm = 0.2939
Meta loss on this task batch = 4.2113e-01, Meta loss averaged over last 500 steps = 3.9567e-01, PNorm = 73.1913, GNorm = 0.2601
Meta loss on this task batch = 3.8535e-01, Meta loss averaged over last 500 steps = 3.9560e-01, PNorm = 73.2017, GNorm = 0.2333
Meta loss on this task batch = 3.7998e-01, Meta loss averaged over last 500 steps = 3.9560e-01, PNorm = 73.2136, GNorm = 0.2460
Meta loss on this task batch = 4.0064e-01, Meta loss averaged over last 500 steps = 3.9565e-01, PNorm = 73.2252, GNorm = 0.2077
Meta loss on this task batch = 4.2078e-01, Meta loss averaged over last 500 steps = 3.9583e-01, PNorm = 73.2368, GNorm = 0.2422
Meta loss on this task batch = 3.2276e-01, Meta loss averaged over last 500 steps = 3.9558e-01, PNorm = 73.2492, GNorm = 0.2576
Meta loss on this task batch = 4.2072e-01, Meta loss averaged over last 500 steps = 3.9557e-01, PNorm = 73.2609, GNorm = 0.2618
Meta loss on this task batch = 3.5649e-01, Meta loss averaged over last 500 steps = 3.9550e-01, PNorm = 73.2736, GNorm = 0.2333
Meta loss on this task batch = 3.3202e-01, Meta loss averaged over last 500 steps = 3.9532e-01, PNorm = 73.2854, GNorm = 0.2410
Meta loss on this task batch = 3.7371e-01, Meta loss averaged over last 500 steps = 3.9522e-01, PNorm = 73.2971, GNorm = 0.2711
Meta loss on this task batch = 3.6376e-01, Meta loss averaged over last 500 steps = 3.9518e-01, PNorm = 73.3078, GNorm = 0.2623
Meta loss on this task batch = 4.0587e-01, Meta loss averaged over last 500 steps = 3.9520e-01, PNorm = 73.3201, GNorm = 0.3377
Took 131.59574174880981 seconds to complete one epoch of meta training
Took 140.20994544029236 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483573
Epoch 106
Meta loss on this task batch = 3.6004e-01, Meta loss averaged over last 500 steps = 3.9505e-01, PNorm = 73.3345, GNorm = 0.2434
Meta loss on this task batch = 3.9016e-01, Meta loss averaged over last 500 steps = 3.9499e-01, PNorm = 73.3478, GNorm = 0.2486
Meta loss on this task batch = 3.8527e-01, Meta loss averaged over last 500 steps = 3.9488e-01, PNorm = 73.3614, GNorm = 0.2268
Meta loss on this task batch = 3.8917e-01, Meta loss averaged over last 500 steps = 3.9506e-01, PNorm = 73.3758, GNorm = 0.2466
Meta loss on this task batch = 3.5829e-01, Meta loss averaged over last 500 steps = 3.9485e-01, PNorm = 73.3908, GNorm = 0.2154
Meta loss on this task batch = 3.4086e-01, Meta loss averaged over last 500 steps = 3.9482e-01, PNorm = 73.4052, GNorm = 0.2074
Meta loss on this task batch = 4.0028e-01, Meta loss averaged over last 500 steps = 3.9497e-01, PNorm = 73.4187, GNorm = 0.2248
Meta loss on this task batch = 3.5848e-01, Meta loss averaged over last 500 steps = 3.9484e-01, PNorm = 73.4327, GNorm = 0.2227
Meta loss on this task batch = 4.0508e-01, Meta loss averaged over last 500 steps = 3.9488e-01, PNorm = 73.4460, GNorm = 0.2625
Meta loss on this task batch = 3.5925e-01, Meta loss averaged over last 500 steps = 3.9487e-01, PNorm = 73.4578, GNorm = 0.2253
Meta loss on this task batch = 4.2159e-01, Meta loss averaged over last 500 steps = 3.9483e-01, PNorm = 73.4678, GNorm = 0.2823
Meta loss on this task batch = 3.7825e-01, Meta loss averaged over last 500 steps = 3.9470e-01, PNorm = 73.4761, GNorm = 0.2448
Meta loss on this task batch = 3.8236e-01, Meta loss averaged over last 500 steps = 3.9459e-01, PNorm = 73.4839, GNorm = 0.2505
Meta loss on this task batch = 4.0620e-01, Meta loss averaged over last 500 steps = 3.9455e-01, PNorm = 73.4904, GNorm = 0.2386
Meta loss on this task batch = 3.8979e-01, Meta loss averaged over last 500 steps = 3.9457e-01, PNorm = 73.4966, GNorm = 0.1991
Meta loss on this task batch = 4.0658e-01, Meta loss averaged over last 500 steps = 3.9459e-01, PNorm = 73.5038, GNorm = 0.2116
Meta loss on this task batch = 3.8923e-01, Meta loss averaged over last 500 steps = 3.9451e-01, PNorm = 73.5106, GNorm = 0.2299
Meta loss on this task batch = 3.2854e-01, Meta loss averaged over last 500 steps = 3.9439e-01, PNorm = 73.5181, GNorm = 0.2185
Meta loss on this task batch = 3.7424e-01, Meta loss averaged over last 500 steps = 3.9437e-01, PNorm = 73.5276, GNorm = 0.2352
Took 131.96742248535156 seconds to complete one epoch of meta training
Took 140.64689111709595 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485693
Epoch 107
Meta loss on this task batch = 4.4722e-01, Meta loss averaged over last 500 steps = 3.9449e-01, PNorm = 73.5375, GNorm = 0.2463
Meta loss on this task batch = 3.7215e-01, Meta loss averaged over last 500 steps = 3.9430e-01, PNorm = 73.5482, GNorm = 0.1937
Meta loss on this task batch = 3.7421e-01, Meta loss averaged over last 500 steps = 3.9422e-01, PNorm = 73.5574, GNorm = 0.2095
Meta loss on this task batch = 3.1953e-01, Meta loss averaged over last 500 steps = 3.9410e-01, PNorm = 73.5656, GNorm = 0.2134
Meta loss on this task batch = 3.5166e-01, Meta loss averaged over last 500 steps = 3.9404e-01, PNorm = 73.5742, GNorm = 0.2127
Meta loss on this task batch = 4.2030e-01, Meta loss averaged over last 500 steps = 3.9394e-01, PNorm = 73.5834, GNorm = 0.2597
Meta loss on this task batch = 3.4393e-01, Meta loss averaged over last 500 steps = 3.9373e-01, PNorm = 73.5904, GNorm = 0.2489
Meta loss on this task batch = 4.0038e-01, Meta loss averaged over last 500 steps = 3.9375e-01, PNorm = 73.5975, GNorm = 0.2247
Meta loss on this task batch = 3.3462e-01, Meta loss averaged over last 500 steps = 3.9357e-01, PNorm = 73.6049, GNorm = 0.2042
Meta loss on this task batch = 4.6875e-01, Meta loss averaged over last 500 steps = 3.9376e-01, PNorm = 73.6122, GNorm = 0.2865
Meta loss on this task batch = 3.4564e-01, Meta loss averaged over last 500 steps = 3.9360e-01, PNorm = 73.6218, GNorm = 0.1991
Meta loss on this task batch = 3.8893e-01, Meta loss averaged over last 500 steps = 3.9363e-01, PNorm = 73.6329, GNorm = 0.2311
Meta loss on this task batch = 3.7316e-01, Meta loss averaged over last 500 steps = 3.9375e-01, PNorm = 73.6446, GNorm = 0.2712
Meta loss on this task batch = 4.1466e-01, Meta loss averaged over last 500 steps = 3.9379e-01, PNorm = 73.6565, GNorm = 0.2474
Meta loss on this task batch = 4.0318e-01, Meta loss averaged over last 500 steps = 3.9364e-01, PNorm = 73.6689, GNorm = 0.2668
Meta loss on this task batch = 4.4368e-01, Meta loss averaged over last 500 steps = 3.9365e-01, PNorm = 73.6813, GNorm = 0.2242
Meta loss on this task batch = 4.1827e-01, Meta loss averaged over last 500 steps = 3.9367e-01, PNorm = 73.6962, GNorm = 0.2919
Meta loss on this task batch = 3.2144e-01, Meta loss averaged over last 500 steps = 3.9347e-01, PNorm = 73.7123, GNorm = 0.2188
Meta loss on this task batch = 3.3512e-01, Meta loss averaged over last 500 steps = 3.9345e-01, PNorm = 73.7285, GNorm = 0.2610
Took 131.86044120788574 seconds to complete one epoch of meta training
Took 140.60824871063232 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491209
Epoch 108
Meta loss on this task batch = 3.7139e-01, Meta loss averaged over last 500 steps = 3.9337e-01, PNorm = 73.7462, GNorm = 0.2193
Meta loss on this task batch = 4.0754e-01, Meta loss averaged over last 500 steps = 3.9345e-01, PNorm = 73.7634, GNorm = 0.2404
Meta loss on this task batch = 3.7571e-01, Meta loss averaged over last 500 steps = 3.9335e-01, PNorm = 73.7776, GNorm = 0.2432
Meta loss on this task batch = 4.0254e-01, Meta loss averaged over last 500 steps = 3.9340e-01, PNorm = 73.7905, GNorm = 0.2702
Meta loss on this task batch = 3.8421e-01, Meta loss averaged over last 500 steps = 3.9321e-01, PNorm = 73.8008, GNorm = 0.3080
Meta loss on this task batch = 3.6071e-01, Meta loss averaged over last 500 steps = 3.9309e-01, PNorm = 73.8098, GNorm = 0.2297
Meta loss on this task batch = 4.1970e-01, Meta loss averaged over last 500 steps = 3.9312e-01, PNorm = 73.8200, GNorm = 0.2582
Meta loss on this task batch = 3.1160e-01, Meta loss averaged over last 500 steps = 3.9298e-01, PNorm = 73.8301, GNorm = 0.2066
Meta loss on this task batch = 3.3313e-01, Meta loss averaged over last 500 steps = 3.9278e-01, PNorm = 73.8401, GNorm = 0.2076
Meta loss on this task batch = 3.8398e-01, Meta loss averaged over last 500 steps = 3.9282e-01, PNorm = 73.8501, GNorm = 0.2280
Meta loss on this task batch = 4.1023e-01, Meta loss averaged over last 500 steps = 3.9295e-01, PNorm = 73.8607, GNorm = 0.2262
Meta loss on this task batch = 4.0515e-01, Meta loss averaged over last 500 steps = 3.9298e-01, PNorm = 73.8707, GNorm = 0.2295
Meta loss on this task batch = 4.1262e-01, Meta loss averaged over last 500 steps = 3.9299e-01, PNorm = 73.8822, GNorm = 0.2333
Meta loss on this task batch = 3.8959e-01, Meta loss averaged over last 500 steps = 3.9291e-01, PNorm = 73.8944, GNorm = 0.1957
Meta loss on this task batch = 3.6146e-01, Meta loss averaged over last 500 steps = 3.9280e-01, PNorm = 73.9070, GNorm = 0.2395
Meta loss on this task batch = 3.7539e-01, Meta loss averaged over last 500 steps = 3.9269e-01, PNorm = 73.9195, GNorm = 0.2170
Meta loss on this task batch = 3.5633e-01, Meta loss averaged over last 500 steps = 3.9263e-01, PNorm = 73.9301, GNorm = 0.2668
Meta loss on this task batch = 3.7042e-01, Meta loss averaged over last 500 steps = 3.9265e-01, PNorm = 73.9391, GNorm = 0.2314
Meta loss on this task batch = 3.6304e-01, Meta loss averaged over last 500 steps = 3.9266e-01, PNorm = 73.9498, GNorm = 0.2646
Took 130.671462059021 seconds to complete one epoch of meta training
Took 139.3932499885559 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515702
Epoch 109
Meta loss on this task batch = 3.7953e-01, Meta loss averaged over last 500 steps = 3.9253e-01, PNorm = 73.9599, GNorm = 0.2266
Meta loss on this task batch = 3.3920e-01, Meta loss averaged over last 500 steps = 3.9250e-01, PNorm = 73.9692, GNorm = 0.2156
Meta loss on this task batch = 3.8542e-01, Meta loss averaged over last 500 steps = 3.9253e-01, PNorm = 73.9777, GNorm = 0.2382
Meta loss on this task batch = 3.6473e-01, Meta loss averaged over last 500 steps = 3.9250e-01, PNorm = 73.9854, GNorm = 0.2249
Meta loss on this task batch = 3.9793e-01, Meta loss averaged over last 500 steps = 3.9245e-01, PNorm = 73.9930, GNorm = 0.2430
Meta loss on this task batch = 3.8568e-01, Meta loss averaged over last 500 steps = 3.9254e-01, PNorm = 73.9995, GNorm = 0.2213
Meta loss on this task batch = 4.3800e-01, Meta loss averaged over last 500 steps = 3.9254e-01, PNorm = 74.0082, GNorm = 0.2533
Meta loss on this task batch = 4.1559e-01, Meta loss averaged over last 500 steps = 3.9258e-01, PNorm = 74.0158, GNorm = 0.2640
Meta loss on this task batch = 3.6451e-01, Meta loss averaged over last 500 steps = 3.9272e-01, PNorm = 74.0248, GNorm = 0.2349
Meta loss on this task batch = 4.4282e-01, Meta loss averaged over last 500 steps = 3.9291e-01, PNorm = 74.0339, GNorm = 0.2726
Meta loss on this task batch = 4.5182e-01, Meta loss averaged over last 500 steps = 3.9298e-01, PNorm = 74.0423, GNorm = 0.2938
Meta loss on this task batch = 3.3003e-01, Meta loss averaged over last 500 steps = 3.9277e-01, PNorm = 74.0500, GNorm = 0.3056
Meta loss on this task batch = 3.1325e-01, Meta loss averaged over last 500 steps = 3.9259e-01, PNorm = 74.0567, GNorm = 0.2453
Meta loss on this task batch = 4.1562e-01, Meta loss averaged over last 500 steps = 3.9274e-01, PNorm = 74.0600, GNorm = 0.4018
Meta loss on this task batch = 3.5696e-01, Meta loss averaged over last 500 steps = 3.9257e-01, PNorm = 74.0623, GNorm = 0.2691
Meta loss on this task batch = 4.4411e-01, Meta loss averaged over last 500 steps = 3.9266e-01, PNorm = 74.0649, GNorm = 0.2358
Meta loss on this task batch = 3.6539e-01, Meta loss averaged over last 500 steps = 3.9244e-01, PNorm = 74.0692, GNorm = 0.1908
Meta loss on this task batch = 4.1228e-01, Meta loss averaged over last 500 steps = 3.9250e-01, PNorm = 74.0758, GNorm = 0.2066
Meta loss on this task batch = 3.6461e-01, Meta loss averaged over last 500 steps = 3.9255e-01, PNorm = 74.0833, GNorm = 0.2413
Took 135.70753741264343 seconds to complete one epoch of meta training
Took 144.4377748966217 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483584
Epoch 110
Meta loss on this task batch = 3.6988e-01, Meta loss averaged over last 500 steps = 3.9254e-01, PNorm = 74.0932, GNorm = 0.2513
Meta loss on this task batch = 3.9491e-01, Meta loss averaged over last 500 steps = 3.9259e-01, PNorm = 74.1037, GNorm = 0.2568
Meta loss on this task batch = 3.9693e-01, Meta loss averaged over last 500 steps = 3.9251e-01, PNorm = 74.1157, GNorm = 0.3149
Meta loss on this task batch = 4.7758e-01, Meta loss averaged over last 500 steps = 3.9255e-01, PNorm = 74.1277, GNorm = 0.3414
Meta loss on this task batch = 4.3385e-01, Meta loss averaged over last 500 steps = 3.9268e-01, PNorm = 74.1408, GNorm = 0.2767
Meta loss on this task batch = 3.7326e-01, Meta loss averaged over last 500 steps = 3.9266e-01, PNorm = 74.1562, GNorm = 0.2019
Meta loss on this task batch = 3.6014e-01, Meta loss averaged over last 500 steps = 3.9255e-01, PNorm = 74.1739, GNorm = 0.2340
Meta loss on this task batch = 4.1159e-01, Meta loss averaged over last 500 steps = 3.9253e-01, PNorm = 74.1923, GNorm = 0.2580
Meta loss on this task batch = 2.9901e-01, Meta loss averaged over last 500 steps = 3.9247e-01, PNorm = 74.2127, GNorm = 0.2148
Meta loss on this task batch = 3.7452e-01, Meta loss averaged over last 500 steps = 3.9243e-01, PNorm = 74.2337, GNorm = 0.2336
Meta loss on this task batch = 4.5176e-01, Meta loss averaged over last 500 steps = 3.9266e-01, PNorm = 74.2527, GNorm = 0.2577
Meta loss on this task batch = 3.8643e-01, Meta loss averaged over last 500 steps = 3.9265e-01, PNorm = 74.2698, GNorm = 0.2247
Meta loss on this task batch = 3.2397e-01, Meta loss averaged over last 500 steps = 3.9249e-01, PNorm = 74.2853, GNorm = 0.2199
Meta loss on this task batch = 3.9477e-01, Meta loss averaged over last 500 steps = 3.9270e-01, PNorm = 74.3002, GNorm = 0.2406
Meta loss on this task batch = 3.6777e-01, Meta loss averaged over last 500 steps = 3.9262e-01, PNorm = 74.3130, GNorm = 0.2626
Meta loss on this task batch = 4.8048e-01, Meta loss averaged over last 500 steps = 3.9273e-01, PNorm = 74.3230, GNorm = 0.2576
Meta loss on this task batch = 3.6678e-01, Meta loss averaged over last 500 steps = 3.9259e-01, PNorm = 74.3338, GNorm = 0.2240
Meta loss on this task batch = 4.1835e-01, Meta loss averaged over last 500 steps = 3.9262e-01, PNorm = 74.3467, GNorm = 0.2093
Meta loss on this task batch = 3.6329e-01, Meta loss averaged over last 500 steps = 3.9242e-01, PNorm = 74.3611, GNorm = 0.2609
Took 130.4401662349701 seconds to complete one epoch of meta training
Took 138.21449518203735 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486670
Epoch 111
Meta loss on this task batch = 3.9726e-01, Meta loss averaged over last 500 steps = 3.9250e-01, PNorm = 74.3760, GNorm = 0.2507
Meta loss on this task batch = 3.2908e-01, Meta loss averaged over last 500 steps = 3.9235e-01, PNorm = 74.3927, GNorm = 0.2279
Meta loss on this task batch = 3.1774e-01, Meta loss averaged over last 500 steps = 3.9222e-01, PNorm = 74.4100, GNorm = 0.2322
Meta loss on this task batch = 3.6099e-01, Meta loss averaged over last 500 steps = 3.9208e-01, PNorm = 74.4272, GNorm = 0.2402
Meta loss on this task batch = 3.7096e-01, Meta loss averaged over last 500 steps = 3.9198e-01, PNorm = 74.4441, GNorm = 0.2596
Meta loss on this task batch = 3.6080e-01, Meta loss averaged over last 500 steps = 3.9185e-01, PNorm = 74.4607, GNorm = 0.2223
Meta loss on this task batch = 3.6207e-01, Meta loss averaged over last 500 steps = 3.9173e-01, PNorm = 74.4767, GNorm = 0.2507
Meta loss on this task batch = 3.4924e-01, Meta loss averaged over last 500 steps = 3.9164e-01, PNorm = 74.4928, GNorm = 0.2230
Meta loss on this task batch = 4.0566e-01, Meta loss averaged over last 500 steps = 3.9163e-01, PNorm = 74.5095, GNorm = 0.2535
Meta loss on this task batch = 3.8468e-01, Meta loss averaged over last 500 steps = 3.9168e-01, PNorm = 74.5250, GNorm = 0.2626
Meta loss on this task batch = 4.1203e-01, Meta loss averaged over last 500 steps = 3.9174e-01, PNorm = 74.5414, GNorm = 0.2671
Meta loss on this task batch = 3.6705e-01, Meta loss averaged over last 500 steps = 3.9169e-01, PNorm = 74.5579, GNorm = 0.2272
Meta loss on this task batch = 3.3487e-01, Meta loss averaged over last 500 steps = 3.9155e-01, PNorm = 74.5739, GNorm = 0.2330
Meta loss on this task batch = 3.8733e-01, Meta loss averaged over last 500 steps = 3.9153e-01, PNorm = 74.5883, GNorm = 0.2506
Meta loss on this task batch = 4.3145e-01, Meta loss averaged over last 500 steps = 3.9160e-01, PNorm = 74.6011, GNorm = 0.2720
Meta loss on this task batch = 4.1358e-01, Meta loss averaged over last 500 steps = 3.9159e-01, PNorm = 74.6118, GNorm = 0.2544
Meta loss on this task batch = 4.0129e-01, Meta loss averaged over last 500 steps = 3.9155e-01, PNorm = 74.6212, GNorm = 0.2819
Meta loss on this task batch = 4.5981e-01, Meta loss averaged over last 500 steps = 3.9160e-01, PNorm = 74.6285, GNorm = 0.2742
Meta loss on this task batch = 3.8239e-01, Meta loss averaged over last 500 steps = 3.9151e-01, PNorm = 74.6365, GNorm = 0.2886
Took 131.80206680297852 seconds to complete one epoch of meta training
Took 140.2314796447754 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482676
Epoch 112
Meta loss on this task batch = 3.8250e-01, Meta loss averaged over last 500 steps = 3.9143e-01, PNorm = 74.6467, GNorm = 0.2268
Meta loss on this task batch = 3.4435e-01, Meta loss averaged over last 500 steps = 3.9136e-01, PNorm = 74.6586, GNorm = 0.2256
Meta loss on this task batch = 4.1621e-01, Meta loss averaged over last 500 steps = 3.9141e-01, PNorm = 74.6706, GNorm = 0.2220
Meta loss on this task batch = 3.6832e-01, Meta loss averaged over last 500 steps = 3.9135e-01, PNorm = 74.6831, GNorm = 0.2287
Meta loss on this task batch = 3.7702e-01, Meta loss averaged over last 500 steps = 3.9125e-01, PNorm = 74.6952, GNorm = 0.2288
Meta loss on this task batch = 3.7251e-01, Meta loss averaged over last 500 steps = 3.9104e-01, PNorm = 74.7090, GNorm = 0.2207
Meta loss on this task batch = 4.2029e-01, Meta loss averaged over last 500 steps = 3.9113e-01, PNorm = 74.7224, GNorm = 0.2015
Meta loss on this task batch = 3.6608e-01, Meta loss averaged over last 500 steps = 3.9115e-01, PNorm = 74.7359, GNorm = 0.2116
Meta loss on this task batch = 3.8882e-01, Meta loss averaged over last 500 steps = 3.9111e-01, PNorm = 74.7494, GNorm = 0.2179
Meta loss on this task batch = 4.3706e-01, Meta loss averaged over last 500 steps = 3.9127e-01, PNorm = 74.7607, GNorm = 0.2148
Meta loss on this task batch = 3.6399e-01, Meta loss averaged over last 500 steps = 3.9120e-01, PNorm = 74.7724, GNorm = 0.2175
Meta loss on this task batch = 3.6009e-01, Meta loss averaged over last 500 steps = 3.9113e-01, PNorm = 74.7839, GNorm = 0.2027
Meta loss on this task batch = 3.5978e-01, Meta loss averaged over last 500 steps = 3.9097e-01, PNorm = 74.7960, GNorm = 0.2384
Meta loss on this task batch = 3.2546e-01, Meta loss averaged over last 500 steps = 3.9075e-01, PNorm = 74.8076, GNorm = 0.2185
Meta loss on this task batch = 3.8502e-01, Meta loss averaged over last 500 steps = 3.9062e-01, PNorm = 74.8179, GNorm = 0.2190
Meta loss on this task batch = 4.1466e-01, Meta loss averaged over last 500 steps = 3.9066e-01, PNorm = 74.8283, GNorm = 0.2858
Meta loss on this task batch = 3.8902e-01, Meta loss averaged over last 500 steps = 3.9075e-01, PNorm = 74.8356, GNorm = 0.2692
Meta loss on this task batch = 3.3521e-01, Meta loss averaged over last 500 steps = 3.9059e-01, PNorm = 74.8437, GNorm = 0.2203
Meta loss on this task batch = 3.7871e-01, Meta loss averaged over last 500 steps = 3.9052e-01, PNorm = 74.8525, GNorm = 0.3032
Took 130.50919461250305 seconds to complete one epoch of meta training
Took 139.36862325668335 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486478
Epoch 113
Meta loss on this task batch = 3.4622e-01, Meta loss averaged over last 500 steps = 3.9046e-01, PNorm = 74.8607, GNorm = 0.2167
Meta loss on this task batch = 4.2302e-01, Meta loss averaged over last 500 steps = 3.9037e-01, PNorm = 74.8694, GNorm = 0.2449
Meta loss on this task batch = 3.4754e-01, Meta loss averaged over last 500 steps = 3.9018e-01, PNorm = 74.8783, GNorm = 0.2349
Meta loss on this task batch = 3.7192e-01, Meta loss averaged over last 500 steps = 3.9019e-01, PNorm = 74.8866, GNorm = 0.2151
Meta loss on this task batch = 4.0586e-01, Meta loss averaged over last 500 steps = 3.9029e-01, PNorm = 74.8958, GNorm = 0.2570
Meta loss on this task batch = 4.2266e-01, Meta loss averaged over last 500 steps = 3.9027e-01, PNorm = 74.9053, GNorm = 0.2391
Meta loss on this task batch = 3.6590e-01, Meta loss averaged over last 500 steps = 3.9025e-01, PNorm = 74.9144, GNorm = 0.2764
Meta loss on this task batch = 4.1460e-01, Meta loss averaged over last 500 steps = 3.9035e-01, PNorm = 74.9224, GNorm = 0.2479
Meta loss on this task batch = 4.1024e-01, Meta loss averaged over last 500 steps = 3.9031e-01, PNorm = 74.9297, GNorm = 0.2405
Meta loss on this task batch = 4.4513e-01, Meta loss averaged over last 500 steps = 3.9036e-01, PNorm = 74.9367, GNorm = 0.2684
Meta loss on this task batch = 4.4869e-01, Meta loss averaged over last 500 steps = 3.9052e-01, PNorm = 74.9435, GNorm = 0.2325
Meta loss on this task batch = 3.2513e-01, Meta loss averaged over last 500 steps = 3.9028e-01, PNorm = 74.9532, GNorm = 0.2257
Meta loss on this task batch = 4.1368e-01, Meta loss averaged over last 500 steps = 3.9033e-01, PNorm = 74.9647, GNorm = 0.2371
Meta loss on this task batch = 3.4756e-01, Meta loss averaged over last 500 steps = 3.9015e-01, PNorm = 74.9759, GNorm = 0.2095
Meta loss on this task batch = 3.7266e-01, Meta loss averaged over last 500 steps = 3.9012e-01, PNorm = 74.9882, GNorm = 0.2448
Meta loss on this task batch = 3.1808e-01, Meta loss averaged over last 500 steps = 3.8988e-01, PNorm = 75.0010, GNorm = 0.2470
Meta loss on this task batch = 4.0255e-01, Meta loss averaged over last 500 steps = 3.8996e-01, PNorm = 75.0106, GNorm = 0.2983
Meta loss on this task batch = 4.1508e-01, Meta loss averaged over last 500 steps = 3.9006e-01, PNorm = 75.0183, GNorm = 0.3125
Meta loss on this task batch = 3.0121e-01, Meta loss averaged over last 500 steps = 3.8980e-01, PNorm = 75.0245, GNorm = 0.2896
Took 131.0568654537201 seconds to complete one epoch of meta training
Took 139.7438325881958 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503341
Epoch 114
Meta loss on this task batch = 3.2991e-01, Meta loss averaged over last 500 steps = 3.8967e-01, PNorm = 75.0306, GNorm = 0.2266
Meta loss on this task batch = 3.8823e-01, Meta loss averaged over last 500 steps = 3.8962e-01, PNorm = 75.0374, GNorm = 0.2216
Meta loss on this task batch = 3.8652e-01, Meta loss averaged over last 500 steps = 3.8962e-01, PNorm = 75.0452, GNorm = 0.2281
Meta loss on this task batch = 3.5292e-01, Meta loss averaged over last 500 steps = 3.8955e-01, PNorm = 75.0560, GNorm = 0.2500
Meta loss on this task batch = 3.7132e-01, Meta loss averaged over last 500 steps = 3.8951e-01, PNorm = 75.0666, GNorm = 0.2059
Meta loss on this task batch = 4.0010e-01, Meta loss averaged over last 500 steps = 3.8947e-01, PNorm = 75.0758, GNorm = 0.2338
Meta loss on this task batch = 3.3446e-01, Meta loss averaged over last 500 steps = 3.8939e-01, PNorm = 75.0855, GNorm = 0.2390
Meta loss on this task batch = 3.2204e-01, Meta loss averaged over last 500 steps = 3.8925e-01, PNorm = 75.0944, GNorm = 0.2053
Meta loss on this task batch = 3.9893e-01, Meta loss averaged over last 500 steps = 3.8938e-01, PNorm = 75.1020, GNorm = 0.2797
Meta loss on this task batch = 3.9606e-01, Meta loss averaged over last 500 steps = 3.8933e-01, PNorm = 75.1041, GNorm = 0.2918
Meta loss on this task batch = 4.0169e-01, Meta loss averaged over last 500 steps = 3.8944e-01, PNorm = 75.1070, GNorm = 0.2345
Meta loss on this task batch = 3.5552e-01, Meta loss averaged over last 500 steps = 3.8937e-01, PNorm = 75.1091, GNorm = 0.2258
Meta loss on this task batch = 4.0464e-01, Meta loss averaged over last 500 steps = 3.8932e-01, PNorm = 75.1110, GNorm = 0.2553
Meta loss on this task batch = 3.9291e-01, Meta loss averaged over last 500 steps = 3.8933e-01, PNorm = 75.1144, GNorm = 0.2217
Meta loss on this task batch = 4.1295e-01, Meta loss averaged over last 500 steps = 3.8933e-01, PNorm = 75.1197, GNorm = 0.2285
Meta loss on this task batch = 4.0956e-01, Meta loss averaged over last 500 steps = 3.8949e-01, PNorm = 75.1296, GNorm = 0.2905
Meta loss on this task batch = 4.3208e-01, Meta loss averaged over last 500 steps = 3.8952e-01, PNorm = 75.1389, GNorm = 0.2764
Meta loss on this task batch = 3.3752e-01, Meta loss averaged over last 500 steps = 3.8933e-01, PNorm = 75.1513, GNorm = 0.2517
Meta loss on this task batch = 2.9320e-01, Meta loss averaged over last 500 steps = 3.8918e-01, PNorm = 75.1632, GNorm = 0.2511
Took 402.0927758216858 seconds to complete one epoch of meta training
Took 411.0273344516754 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448740
Epoch 115
Meta loss on this task batch = 3.6849e-01, Meta loss averaged over last 500 steps = 3.8906e-01, PNorm = 75.1746, GNorm = 0.2560
Meta loss on this task batch = 3.8647e-01, Meta loss averaged over last 500 steps = 3.8915e-01, PNorm = 75.1835, GNorm = 0.2573
Meta loss on this task batch = 3.9155e-01, Meta loss averaged over last 500 steps = 3.8901e-01, PNorm = 75.1912, GNorm = 0.2524
Meta loss on this task batch = 4.0089e-01, Meta loss averaged over last 500 steps = 3.8888e-01, PNorm = 75.1968, GNorm = 0.3018
Meta loss on this task batch = 3.8199e-01, Meta loss averaged over last 500 steps = 3.8883e-01, PNorm = 75.2019, GNorm = 0.2780
Meta loss on this task batch = 3.7752e-01, Meta loss averaged over last 500 steps = 3.8881e-01, PNorm = 75.2085, GNorm = 0.2459
Meta loss on this task batch = 3.9001e-01, Meta loss averaged over last 500 steps = 3.8863e-01, PNorm = 75.2174, GNorm = 0.2348
Meta loss on this task batch = 3.6045e-01, Meta loss averaged over last 500 steps = 3.8860e-01, PNorm = 75.2284, GNorm = 0.2351
Meta loss on this task batch = 3.8195e-01, Meta loss averaged over last 500 steps = 3.8863e-01, PNorm = 75.2415, GNorm = 0.2162
Meta loss on this task batch = 4.4993e-01, Meta loss averaged over last 500 steps = 3.8874e-01, PNorm = 75.2534, GNorm = 0.2379
Meta loss on this task batch = 3.6962e-01, Meta loss averaged over last 500 steps = 3.8870e-01, PNorm = 75.2656, GNorm = 0.2438
Meta loss on this task batch = 3.1649e-01, Meta loss averaged over last 500 steps = 3.8864e-01, PNorm = 75.2800, GNorm = 0.1922
Meta loss on this task batch = 3.2339e-01, Meta loss averaged over last 500 steps = 3.8841e-01, PNorm = 75.2956, GNorm = 0.2094
Meta loss on this task batch = 3.4860e-01, Meta loss averaged over last 500 steps = 3.8834e-01, PNorm = 75.3103, GNorm = 0.2267
Meta loss on this task batch = 3.9061e-01, Meta loss averaged over last 500 steps = 3.8850e-01, PNorm = 75.3219, GNorm = 0.2929
Meta loss on this task batch = 4.0469e-01, Meta loss averaged over last 500 steps = 3.8858e-01, PNorm = 75.3327, GNorm = 0.3136
Meta loss on this task batch = 4.1723e-01, Meta loss averaged over last 500 steps = 3.8859e-01, PNorm = 75.3415, GNorm = 0.2704
Meta loss on this task batch = 3.6284e-01, Meta loss averaged over last 500 steps = 3.8847e-01, PNorm = 75.3497, GNorm = 0.2161
Meta loss on this task batch = 4.2373e-01, Meta loss averaged over last 500 steps = 3.8860e-01, PNorm = 75.3569, GNorm = 0.2370
Took 182.5081820487976 seconds to complete one epoch of meta training
Took 191.03452253341675 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497082
Epoch 116
Meta loss on this task batch = 4.0361e-01, Meta loss averaged over last 500 steps = 3.8858e-01, PNorm = 75.3669, GNorm = 0.2500
Meta loss on this task batch = 3.3547e-01, Meta loss averaged over last 500 steps = 3.8843e-01, PNorm = 75.3757, GNorm = 0.3999
Meta loss on this task batch = 4.2370e-01, Meta loss averaged over last 500 steps = 3.8845e-01, PNorm = 75.3851, GNorm = 0.2695
Meta loss on this task batch = 3.9234e-01, Meta loss averaged over last 500 steps = 3.8833e-01, PNorm = 75.3954, GNorm = 0.2294
Meta loss on this task batch = 4.3589e-01, Meta loss averaged over last 500 steps = 3.8834e-01, PNorm = 75.4063, GNorm = 0.2511
Meta loss on this task batch = 3.6878e-01, Meta loss averaged over last 500 steps = 3.8829e-01, PNorm = 75.4175, GNorm = 0.2492
Meta loss on this task batch = 3.6386e-01, Meta loss averaged over last 500 steps = 3.8819e-01, PNorm = 75.4292, GNorm = 0.2512
Meta loss on this task batch = 3.4993e-01, Meta loss averaged over last 500 steps = 3.8809e-01, PNorm = 75.4396, GNorm = 0.1961
Meta loss on this task batch = 4.2001e-01, Meta loss averaged over last 500 steps = 3.8811e-01, PNorm = 75.4488, GNorm = 0.2633
Meta loss on this task batch = 3.4039e-01, Meta loss averaged over last 500 steps = 3.8809e-01, PNorm = 75.4588, GNorm = 0.2729
Meta loss on this task batch = 3.7075e-01, Meta loss averaged over last 500 steps = 3.8804e-01, PNorm = 75.4681, GNorm = 0.2440
Meta loss on this task batch = 3.5669e-01, Meta loss averaged over last 500 steps = 3.8796e-01, PNorm = 75.4791, GNorm = 0.2150
Meta loss on this task batch = 3.7522e-01, Meta loss averaged over last 500 steps = 3.8792e-01, PNorm = 75.4905, GNorm = 0.3062
Meta loss on this task batch = 3.7350e-01, Meta loss averaged over last 500 steps = 3.8777e-01, PNorm = 75.5020, GNorm = 0.2376
Meta loss on this task batch = 3.8469e-01, Meta loss averaged over last 500 steps = 3.8785e-01, PNorm = 75.5132, GNorm = 0.2572
Meta loss on this task batch = 4.0751e-01, Meta loss averaged over last 500 steps = 3.8789e-01, PNorm = 75.5246, GNorm = 0.2581
Meta loss on this task batch = 3.3494e-01, Meta loss averaged over last 500 steps = 3.8776e-01, PNorm = 75.5370, GNorm = 0.2456
Meta loss on this task batch = 3.5008e-01, Meta loss averaged over last 500 steps = 3.8761e-01, PNorm = 75.5499, GNorm = 0.2227
Meta loss on this task batch = 4.2310e-01, Meta loss averaged over last 500 steps = 3.8770e-01, PNorm = 75.5654, GNorm = 0.2777
Took 145.66794848442078 seconds to complete one epoch of meta training
Took 154.04021859169006 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465028
Epoch 117
Meta loss on this task batch = 4.5174e-01, Meta loss averaged over last 500 steps = 3.8783e-01, PNorm = 75.5783, GNorm = 0.3023
Meta loss on this task batch = 3.7813e-01, Meta loss averaged over last 500 steps = 3.8783e-01, PNorm = 75.5908, GNorm = 0.2429
Meta loss on this task batch = 4.4154e-01, Meta loss averaged over last 500 steps = 3.8771e-01, PNorm = 75.6010, GNorm = 0.2996
Meta loss on this task batch = 3.7851e-01, Meta loss averaged over last 500 steps = 3.8763e-01, PNorm = 75.6111, GNorm = 0.2106
Meta loss on this task batch = 3.9311e-01, Meta loss averaged over last 500 steps = 3.8772e-01, PNorm = 75.6212, GNorm = 0.2503
Meta loss on this task batch = 3.6831e-01, Meta loss averaged over last 500 steps = 3.8764e-01, PNorm = 75.6314, GNorm = 0.2391
Meta loss on this task batch = 3.4752e-01, Meta loss averaged over last 500 steps = 3.8750e-01, PNorm = 75.6415, GNorm = 0.2280
Meta loss on this task batch = 3.4902e-01, Meta loss averaged over last 500 steps = 3.8751e-01, PNorm = 75.6520, GNorm = 0.2056
Meta loss on this task batch = 3.7360e-01, Meta loss averaged over last 500 steps = 3.8751e-01, PNorm = 75.6621, GNorm = 0.2353
Meta loss on this task batch = 3.8510e-01, Meta loss averaged over last 500 steps = 3.8737e-01, PNorm = 75.6730, GNorm = 0.2265
Meta loss on this task batch = 3.3086e-01, Meta loss averaged over last 500 steps = 3.8735e-01, PNorm = 75.6834, GNorm = 0.2461
Meta loss on this task batch = 3.1176e-01, Meta loss averaged over last 500 steps = 3.8716e-01, PNorm = 75.6934, GNorm = 0.2184
Meta loss on this task batch = 3.4574e-01, Meta loss averaged over last 500 steps = 3.8702e-01, PNorm = 75.7039, GNorm = 0.2292
Meta loss on this task batch = 3.7277e-01, Meta loss averaged over last 500 steps = 3.8693e-01, PNorm = 75.7149, GNorm = 0.2434
Meta loss on this task batch = 3.5526e-01, Meta loss averaged over last 500 steps = 3.8685e-01, PNorm = 75.7288, GNorm = 0.2701
Meta loss on this task batch = 4.5453e-01, Meta loss averaged over last 500 steps = 3.8691e-01, PNorm = 75.7429, GNorm = 0.2521
Meta loss on this task batch = 3.7387e-01, Meta loss averaged over last 500 steps = 3.8680e-01, PNorm = 75.7579, GNorm = 0.2447
Meta loss on this task batch = 3.6856e-01, Meta loss averaged over last 500 steps = 3.8667e-01, PNorm = 75.7722, GNorm = 0.2606
Meta loss on this task batch = 3.5160e-01, Meta loss averaged over last 500 steps = 3.8664e-01, PNorm = 75.7857, GNorm = 0.2594
Took 214.2766695022583 seconds to complete one epoch of meta training
Took 222.9128315448761 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478970
Epoch 118
Meta loss on this task batch = 3.6183e-01, Meta loss averaged over last 500 steps = 3.8630e-01, PNorm = 75.7984, GNorm = 0.2249
Meta loss on this task batch = 3.3421e-01, Meta loss averaged over last 500 steps = 3.8623e-01, PNorm = 75.8104, GNorm = 0.2528
Meta loss on this task batch = 3.3829e-01, Meta loss averaged over last 500 steps = 3.8623e-01, PNorm = 75.8211, GNorm = 0.2441
Meta loss on this task batch = 3.9888e-01, Meta loss averaged over last 500 steps = 3.8626e-01, PNorm = 75.8309, GNorm = 0.2468
Meta loss on this task batch = 4.0116e-01, Meta loss averaged over last 500 steps = 3.8633e-01, PNorm = 75.8404, GNorm = 0.2411
Meta loss on this task batch = 4.0298e-01, Meta loss averaged over last 500 steps = 3.8636e-01, PNorm = 75.8502, GNorm = 0.2832
Meta loss on this task batch = 3.1619e-01, Meta loss averaged over last 500 steps = 3.8626e-01, PNorm = 75.8606, GNorm = 0.1897
Meta loss on this task batch = 3.2825e-01, Meta loss averaged over last 500 steps = 3.8623e-01, PNorm = 75.8722, GNorm = 0.2185
Meta loss on this task batch = 4.7949e-01, Meta loss averaged over last 500 steps = 3.8636e-01, PNorm = 75.8827, GNorm = 0.2728
Meta loss on this task batch = 3.2479e-01, Meta loss averaged over last 500 steps = 3.8617e-01, PNorm = 75.8950, GNorm = 0.2335
Meta loss on this task batch = 3.9963e-01, Meta loss averaged over last 500 steps = 3.8609e-01, PNorm = 75.9084, GNorm = 0.2499
Meta loss on this task batch = 3.9557e-01, Meta loss averaged over last 500 steps = 3.8609e-01, PNorm = 75.9232, GNorm = 0.2276
Meta loss on this task batch = 4.1685e-01, Meta loss averaged over last 500 steps = 3.8612e-01, PNorm = 75.9368, GNorm = 0.2386
Meta loss on this task batch = 4.1907e-01, Meta loss averaged over last 500 steps = 3.8618e-01, PNorm = 75.9504, GNorm = 0.3029
Meta loss on this task batch = 3.7926e-01, Meta loss averaged over last 500 steps = 3.8615e-01, PNorm = 75.9636, GNorm = 0.2610
Meta loss on this task batch = 3.4180e-01, Meta loss averaged over last 500 steps = 3.8598e-01, PNorm = 75.9756, GNorm = 0.2493
Meta loss on this task batch = 3.5714e-01, Meta loss averaged over last 500 steps = 3.8598e-01, PNorm = 75.9875, GNorm = 0.1969
Meta loss on this task batch = 4.0135e-01, Meta loss averaged over last 500 steps = 3.8603e-01, PNorm = 75.9994, GNorm = 0.2210
Meta loss on this task batch = 4.3135e-01, Meta loss averaged over last 500 steps = 3.8595e-01, PNorm = 76.0083, GNorm = 0.3245
Took 131.78039240837097 seconds to complete one epoch of meta training
Took 140.13007164001465 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503423
Epoch 119
Meta loss on this task batch = 4.0652e-01, Meta loss averaged over last 500 steps = 3.8595e-01, PNorm = 76.0161, GNorm = 0.2790
Meta loss on this task batch = 3.4703e-01, Meta loss averaged over last 500 steps = 3.8575e-01, PNorm = 76.0252, GNorm = 0.2544
Meta loss on this task batch = 3.4003e-01, Meta loss averaged over last 500 steps = 3.8574e-01, PNorm = 76.0359, GNorm = 0.2462
Meta loss on this task batch = 3.7245e-01, Meta loss averaged over last 500 steps = 3.8564e-01, PNorm = 76.0454, GNorm = 0.2599
Meta loss on this task batch = 3.7360e-01, Meta loss averaged over last 500 steps = 3.8562e-01, PNorm = 76.0549, GNorm = 0.2320
Meta loss on this task batch = 4.4559e-01, Meta loss averaged over last 500 steps = 3.8576e-01, PNorm = 76.0641, GNorm = 0.3525
Meta loss on this task batch = 3.7921e-01, Meta loss averaged over last 500 steps = 3.8592e-01, PNorm = 76.0731, GNorm = 0.2655
Meta loss on this task batch = 3.8129e-01, Meta loss averaged over last 500 steps = 3.8588e-01, PNorm = 76.0832, GNorm = 0.2837
Meta loss on this task batch = 3.6681e-01, Meta loss averaged over last 500 steps = 3.8578e-01, PNorm = 76.0946, GNorm = 0.2419
Meta loss on this task batch = 3.5189e-01, Meta loss averaged over last 500 steps = 3.8567e-01, PNorm = 76.1065, GNorm = 0.2301
Meta loss on this task batch = 3.5556e-01, Meta loss averaged over last 500 steps = 3.8556e-01, PNorm = 76.1198, GNorm = 0.2375
Meta loss on this task batch = 4.0563e-01, Meta loss averaged over last 500 steps = 3.8563e-01, PNorm = 76.1333, GNorm = 0.2404
Meta loss on this task batch = 4.2793e-01, Meta loss averaged over last 500 steps = 3.8575e-01, PNorm = 76.1464, GNorm = 0.2602
Meta loss on this task batch = 3.2914e-01, Meta loss averaged over last 500 steps = 3.8537e-01, PNorm = 76.1578, GNorm = 0.2270
Meta loss on this task batch = 4.3311e-01, Meta loss averaged over last 500 steps = 3.8548e-01, PNorm = 76.1662, GNorm = 0.3192
Meta loss on this task batch = 3.5063e-01, Meta loss averaged over last 500 steps = 3.8539e-01, PNorm = 76.1723, GNorm = 0.2901
Meta loss on this task batch = 3.4120e-01, Meta loss averaged over last 500 steps = 3.8539e-01, PNorm = 76.1788, GNorm = 0.2681
Meta loss on this task batch = 3.0124e-01, Meta loss averaged over last 500 steps = 3.8515e-01, PNorm = 76.1858, GNorm = 0.2036
Meta loss on this task batch = 3.7584e-01, Meta loss averaged over last 500 steps = 3.8505e-01, PNorm = 76.1939, GNorm = 0.2770
Took 134.166601896286 seconds to complete one epoch of meta training
Took 142.7582552433014 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498373
Epoch 120
Meta loss on this task batch = 3.8891e-01, Meta loss averaged over last 500 steps = 3.8502e-01, PNorm = 76.2031, GNorm = 0.2942
Meta loss on this task batch = 3.6729e-01, Meta loss averaged over last 500 steps = 3.8498e-01, PNorm = 76.2131, GNorm = 0.2430
Meta loss on this task batch = 3.3828e-01, Meta loss averaged over last 500 steps = 3.8470e-01, PNorm = 76.2249, GNorm = 0.2105
Meta loss on this task batch = 3.1791e-01, Meta loss averaged over last 500 steps = 3.8459e-01, PNorm = 76.2380, GNorm = 0.2311
Meta loss on this task batch = 3.8713e-01, Meta loss averaged over last 500 steps = 3.8462e-01, PNorm = 76.2529, GNorm = 0.3240
Meta loss on this task batch = 3.8335e-01, Meta loss averaged over last 500 steps = 3.8469e-01, PNorm = 76.2690, GNorm = 0.2631
Meta loss on this task batch = 3.8580e-01, Meta loss averaged over last 500 steps = 3.8464e-01, PNorm = 76.2851, GNorm = 0.2644
Meta loss on this task batch = 4.1362e-01, Meta loss averaged over last 500 steps = 3.8466e-01, PNorm = 76.3002, GNorm = 0.3494
Meta loss on this task batch = 4.2058e-01, Meta loss averaged over last 500 steps = 3.8468e-01, PNorm = 76.3150, GNorm = 0.2952
Meta loss on this task batch = 4.3904e-01, Meta loss averaged over last 500 steps = 3.8488e-01, PNorm = 76.3292, GNorm = 0.2419
Meta loss on this task batch = 3.9352e-01, Meta loss averaged over last 500 steps = 3.8495e-01, PNorm = 76.3426, GNorm = 0.2185
Meta loss on this task batch = 3.1244e-01, Meta loss averaged over last 500 steps = 3.8467e-01, PNorm = 76.3562, GNorm = 0.2706
Meta loss on this task batch = 3.2985e-01, Meta loss averaged over last 500 steps = 3.8453e-01, PNorm = 76.3694, GNorm = 0.2248
Meta loss on this task batch = 3.8188e-01, Meta loss averaged over last 500 steps = 3.8464e-01, PNorm = 76.3826, GNorm = 0.2386
Meta loss on this task batch = 3.6435e-01, Meta loss averaged over last 500 steps = 3.8450e-01, PNorm = 76.3960, GNorm = 0.2347
Meta loss on this task batch = 4.0508e-01, Meta loss averaged over last 500 steps = 3.8448e-01, PNorm = 76.4081, GNorm = 0.2555
Meta loss on this task batch = 3.4654e-01, Meta loss averaged over last 500 steps = 3.8439e-01, PNorm = 76.4207, GNorm = 0.2270
Meta loss on this task batch = 4.2707e-01, Meta loss averaged over last 500 steps = 3.8439e-01, PNorm = 76.4328, GNorm = 0.2477
Meta loss on this task batch = 3.7493e-01, Meta loss averaged over last 500 steps = 3.8437e-01, PNorm = 76.4447, GNorm = 0.3137
Took 133.4314260482788 seconds to complete one epoch of meta training
Took 142.05315589904785 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489117
Epoch 121
Meta loss on this task batch = 3.9220e-01, Meta loss averaged over last 500 steps = 3.8439e-01, PNorm = 76.4561, GNorm = 0.2615
Meta loss on this task batch = 4.0711e-01, Meta loss averaged over last 500 steps = 3.8436e-01, PNorm = 76.4689, GNorm = 0.2384
Meta loss on this task batch = 3.9117e-01, Meta loss averaged over last 500 steps = 3.8434e-01, PNorm = 76.4830, GNorm = 0.2395
Meta loss on this task batch = 3.6329e-01, Meta loss averaged over last 500 steps = 3.8429e-01, PNorm = 76.4967, GNorm = 0.2184
Meta loss on this task batch = 4.1818e-01, Meta loss averaged over last 500 steps = 3.8440e-01, PNorm = 76.5091, GNorm = 0.2582
Meta loss on this task batch = 3.6916e-01, Meta loss averaged over last 500 steps = 3.8442e-01, PNorm = 76.5228, GNorm = 0.2685
Meta loss on this task batch = 2.7730e-01, Meta loss averaged over last 500 steps = 3.8420e-01, PNorm = 76.5372, GNorm = 0.2000
Meta loss on this task batch = 3.3071e-01, Meta loss averaged over last 500 steps = 3.8416e-01, PNorm = 76.5497, GNorm = 0.2741
Meta loss on this task batch = 3.5652e-01, Meta loss averaged over last 500 steps = 3.8422e-01, PNorm = 76.5588, GNorm = 0.2753
Meta loss on this task batch = 3.8008e-01, Meta loss averaged over last 500 steps = 3.8405e-01, PNorm = 76.5654, GNorm = 0.3176
Meta loss on this task batch = 3.3860e-01, Meta loss averaged over last 500 steps = 3.8381e-01, PNorm = 76.5717, GNorm = 0.2629
Meta loss on this task batch = 4.3290e-01, Meta loss averaged over last 500 steps = 3.8389e-01, PNorm = 76.5774, GNorm = 0.3115
Meta loss on this task batch = 4.0517e-01, Meta loss averaged over last 500 steps = 3.8387e-01, PNorm = 76.5838, GNorm = 0.2848
Meta loss on this task batch = 4.1125e-01, Meta loss averaged over last 500 steps = 3.8389e-01, PNorm = 76.5896, GNorm = 0.2290
Meta loss on this task batch = 3.6059e-01, Meta loss averaged over last 500 steps = 3.8376e-01, PNorm = 76.5969, GNorm = 0.2303
Meta loss on this task batch = 4.2323e-01, Meta loss averaged over last 500 steps = 3.8375e-01, PNorm = 76.6016, GNorm = 0.2976
Meta loss on this task batch = 4.0497e-01, Meta loss averaged over last 500 steps = 3.8374e-01, PNorm = 76.6067, GNorm = 0.2200
Meta loss on this task batch = 2.9392e-01, Meta loss averaged over last 500 steps = 3.8356e-01, PNorm = 76.6142, GNorm = 0.2218
Meta loss on this task batch = 3.6470e-01, Meta loss averaged over last 500 steps = 3.8357e-01, PNorm = 76.6246, GNorm = 0.2665
Took 132.32089352607727 seconds to complete one epoch of meta training
Took 141.18929481506348 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493854
Epoch 122
Meta loss on this task batch = 3.7960e-01, Meta loss averaged over last 500 steps = 3.8348e-01, PNorm = 76.6363, GNorm = 0.2443
Meta loss on this task batch = 4.1530e-01, Meta loss averaged over last 500 steps = 3.8357e-01, PNorm = 76.6492, GNorm = 0.2778
Meta loss on this task batch = 3.4367e-01, Meta loss averaged over last 500 steps = 3.8357e-01, PNorm = 76.6645, GNorm = 0.2148
Meta loss on this task batch = 3.4661e-01, Meta loss averaged over last 500 steps = 3.8355e-01, PNorm = 76.6812, GNorm = 0.2183
Meta loss on this task batch = 3.4263e-01, Meta loss averaged over last 500 steps = 3.8341e-01, PNorm = 76.6968, GNorm = 0.2336
Meta loss on this task batch = 3.9104e-01, Meta loss averaged over last 500 steps = 3.8342e-01, PNorm = 76.7103, GNorm = 0.2586
Meta loss on this task batch = 4.2757e-01, Meta loss averaged over last 500 steps = 3.8344e-01, PNorm = 76.7196, GNorm = 0.3671
Meta loss on this task batch = 3.9530e-01, Meta loss averaged over last 500 steps = 3.8351e-01, PNorm = 76.7262, GNorm = 0.2637
Meta loss on this task batch = 3.8153e-01, Meta loss averaged over last 500 steps = 3.8348e-01, PNorm = 76.7337, GNorm = 0.2254
Meta loss on this task batch = 3.7384e-01, Meta loss averaged over last 500 steps = 3.8351e-01, PNorm = 76.7408, GNorm = 0.2392
Meta loss on this task batch = 3.8556e-01, Meta loss averaged over last 500 steps = 3.8344e-01, PNorm = 76.7484, GNorm = 0.2609
Meta loss on this task batch = 3.4927e-01, Meta loss averaged over last 500 steps = 3.8327e-01, PNorm = 76.7572, GNorm = 0.2498
Meta loss on this task batch = 3.9273e-01, Meta loss averaged over last 500 steps = 3.8323e-01, PNorm = 76.7677, GNorm = 0.2637
Meta loss on this task batch = 3.1248e-01, Meta loss averaged over last 500 steps = 3.8301e-01, PNorm = 76.7787, GNorm = 0.2385
Meta loss on this task batch = 4.3437e-01, Meta loss averaged over last 500 steps = 3.8322e-01, PNorm = 76.7890, GNorm = 0.2736
Meta loss on this task batch = 3.6471e-01, Meta loss averaged over last 500 steps = 3.8318e-01, PNorm = 76.8000, GNorm = 0.2595
Meta loss on this task batch = 4.1978e-01, Meta loss averaged over last 500 steps = 3.8331e-01, PNorm = 76.8095, GNorm = 0.3467
Meta loss on this task batch = 3.4805e-01, Meta loss averaged over last 500 steps = 3.8326e-01, PNorm = 76.8182, GNorm = 0.2250
Meta loss on this task batch = 4.1052e-01, Meta loss averaged over last 500 steps = 3.8334e-01, PNorm = 76.8284, GNorm = 0.3004
Took 132.7781081199646 seconds to complete one epoch of meta training
Took 140.19205236434937 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468698
Epoch 123
Meta loss on this task batch = 3.8912e-01, Meta loss averaged over last 500 steps = 3.8351e-01, PNorm = 76.8395, GNorm = 0.2192
Meta loss on this task batch = 3.2460e-01, Meta loss averaged over last 500 steps = 3.8323e-01, PNorm = 76.8524, GNorm = 0.2230
Meta loss on this task batch = 4.0497e-01, Meta loss averaged over last 500 steps = 3.8328e-01, PNorm = 76.8656, GNorm = 0.2214
Meta loss on this task batch = 4.1257e-01, Meta loss averaged over last 500 steps = 3.8336e-01, PNorm = 76.8803, GNorm = 0.2295
Meta loss on this task batch = 3.2497e-01, Meta loss averaged over last 500 steps = 3.8314e-01, PNorm = 76.8955, GNorm = 0.2352
Meta loss on this task batch = 4.1377e-01, Meta loss averaged over last 500 steps = 3.8309e-01, PNorm = 76.9096, GNorm = 0.2550
Meta loss on this task batch = 3.8529e-01, Meta loss averaged over last 500 steps = 3.8328e-01, PNorm = 76.9234, GNorm = 0.2604
Meta loss on this task batch = 3.5634e-01, Meta loss averaged over last 500 steps = 3.8328e-01, PNorm = 76.9357, GNorm = 0.2431
Meta loss on this task batch = 3.8572e-01, Meta loss averaged over last 500 steps = 3.8328e-01, PNorm = 76.9466, GNorm = 0.2573
Meta loss on this task batch = 3.8468e-01, Meta loss averaged over last 500 steps = 3.8316e-01, PNorm = 76.9561, GNorm = 0.3110
Meta loss on this task batch = 2.8991e-01, Meta loss averaged over last 500 steps = 3.8296e-01, PNorm = 76.9668, GNorm = 0.2152
Meta loss on this task batch = 3.5751e-01, Meta loss averaged over last 500 steps = 3.8292e-01, PNorm = 76.9780, GNorm = 0.2346
Meta loss on this task batch = 3.1190e-01, Meta loss averaged over last 500 steps = 3.8266e-01, PNorm = 76.9892, GNorm = 0.1993
Meta loss on this task batch = 4.2902e-01, Meta loss averaged over last 500 steps = 3.8271e-01, PNorm = 76.9992, GNorm = 0.2975
Meta loss on this task batch = 4.0572e-01, Meta loss averaged over last 500 steps = 3.8278e-01, PNorm = 77.0079, GNorm = 0.2574
Meta loss on this task batch = 3.8105e-01, Meta loss averaged over last 500 steps = 3.8277e-01, PNorm = 77.0152, GNorm = 0.2716
Meta loss on this task batch = 3.7067e-01, Meta loss averaged over last 500 steps = 3.8272e-01, PNorm = 77.0225, GNorm = 0.2508
Meta loss on this task batch = 4.1597e-01, Meta loss averaged over last 500 steps = 3.8270e-01, PNorm = 77.0288, GNorm = 0.2316
Meta loss on this task batch = 4.0039e-01, Meta loss averaged over last 500 steps = 3.8269e-01, PNorm = 77.0329, GNorm = 0.3599
Took 133.16005563735962 seconds to complete one epoch of meta training
Took 141.08317875862122 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485704
Epoch 124
Meta loss on this task batch = 3.7249e-01, Meta loss averaged over last 500 steps = 3.8257e-01, PNorm = 77.0370, GNorm = 0.2251
Meta loss on this task batch = 3.3944e-01, Meta loss averaged over last 500 steps = 3.8257e-01, PNorm = 77.0426, GNorm = 0.2570
Meta loss on this task batch = 3.6298e-01, Meta loss averaged over last 500 steps = 3.8236e-01, PNorm = 77.0495, GNorm = 0.2316
Meta loss on this task batch = 4.0141e-01, Meta loss averaged over last 500 steps = 3.8234e-01, PNorm = 77.0556, GNorm = 0.2536
Meta loss on this task batch = 3.8794e-01, Meta loss averaged over last 500 steps = 3.8241e-01, PNorm = 77.0588, GNorm = 0.2967
Meta loss on this task batch = 3.7740e-01, Meta loss averaged over last 500 steps = 3.8240e-01, PNorm = 77.0628, GNorm = 0.2522
Meta loss on this task batch = 3.4647e-01, Meta loss averaged over last 500 steps = 3.8242e-01, PNorm = 77.0682, GNorm = 0.2165
Meta loss on this task batch = 3.7468e-01, Meta loss averaged over last 500 steps = 3.8252e-01, PNorm = 77.0756, GNorm = 0.2476
Meta loss on this task batch = 3.1184e-01, Meta loss averaged over last 500 steps = 3.8259e-01, PNorm = 77.0853, GNorm = 0.1950
Meta loss on this task batch = 4.2022e-01, Meta loss averaged over last 500 steps = 3.8270e-01, PNorm = 77.0954, GNorm = 0.2560
Meta loss on this task batch = 3.2922e-01, Meta loss averaged over last 500 steps = 3.8248e-01, PNorm = 77.1075, GNorm = 0.2256
Meta loss on this task batch = 3.4325e-01, Meta loss averaged over last 500 steps = 3.8234e-01, PNorm = 77.1198, GNorm = 0.2243
Meta loss on this task batch = 4.3386e-01, Meta loss averaged over last 500 steps = 3.8234e-01, PNorm = 77.1326, GNorm = 0.2527
Meta loss on this task batch = 3.9567e-01, Meta loss averaged over last 500 steps = 3.8227e-01, PNorm = 77.1453, GNorm = 0.2527
Meta loss on this task batch = 3.4848e-01, Meta loss averaged over last 500 steps = 3.8223e-01, PNorm = 77.1589, GNorm = 0.2398
Meta loss on this task batch = 3.9072e-01, Meta loss averaged over last 500 steps = 3.8223e-01, PNorm = 77.1721, GNorm = 0.2535
Meta loss on this task batch = 3.6886e-01, Meta loss averaged over last 500 steps = 3.8209e-01, PNorm = 77.1854, GNorm = 0.2481
Meta loss on this task batch = 3.9200e-01, Meta loss averaged over last 500 steps = 3.8213e-01, PNorm = 77.1992, GNorm = 0.2462
Meta loss on this task batch = 4.0774e-01, Meta loss averaged over last 500 steps = 3.8203e-01, PNorm = 77.2103, GNorm = 0.2927
Took 133.34492373466492 seconds to complete one epoch of meta training
Took 141.56160402297974 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509940
Epoch 125
Meta loss on this task batch = 4.2877e-01, Meta loss averaged over last 500 steps = 3.8211e-01, PNorm = 77.2203, GNorm = 0.2725
Meta loss on this task batch = 4.1718e-01, Meta loss averaged over last 500 steps = 3.8214e-01, PNorm = 77.2316, GNorm = 0.2642
Meta loss on this task batch = 3.5327e-01, Meta loss averaged over last 500 steps = 3.8214e-01, PNorm = 77.2435, GNorm = 0.2575
Meta loss on this task batch = 3.8457e-01, Meta loss averaged over last 500 steps = 3.8204e-01, PNorm = 77.2557, GNorm = 0.2493
Meta loss on this task batch = 4.7406e-01, Meta loss averaged over last 500 steps = 3.8217e-01, PNorm = 77.2657, GNorm = 0.2856
Meta loss on this task batch = 3.0902e-01, Meta loss averaged over last 500 steps = 3.8213e-01, PNorm = 77.2766, GNorm = 0.2358
Meta loss on this task batch = 3.9121e-01, Meta loss averaged over last 500 steps = 3.8205e-01, PNorm = 77.2862, GNorm = 0.2354
Meta loss on this task batch = 3.8072e-01, Meta loss averaged over last 500 steps = 3.8196e-01, PNorm = 77.2961, GNorm = 0.2567
Meta loss on this task batch = 3.6309e-01, Meta loss averaged over last 500 steps = 3.8182e-01, PNorm = 77.3053, GNorm = 0.2435
Meta loss on this task batch = 4.1274e-01, Meta loss averaged over last 500 steps = 3.8200e-01, PNorm = 77.3118, GNorm = 0.2461
Meta loss on this task batch = 3.5052e-01, Meta loss averaged over last 500 steps = 3.8175e-01, PNorm = 77.3181, GNorm = 0.2411
Meta loss on this task batch = 3.5647e-01, Meta loss averaged over last 500 steps = 3.8163e-01, PNorm = 77.3260, GNorm = 0.2319
Meta loss on this task batch = 3.3694e-01, Meta loss averaged over last 500 steps = 3.8154e-01, PNorm = 77.3351, GNorm = 0.2300
Meta loss on this task batch = 3.7162e-01, Meta loss averaged over last 500 steps = 3.8154e-01, PNorm = 77.3455, GNorm = 0.3088
Meta loss on this task batch = 3.1422e-01, Meta loss averaged over last 500 steps = 3.8143e-01, PNorm = 77.3576, GNorm = 0.2228
Meta loss on this task batch = 3.2077e-01, Meta loss averaged over last 500 steps = 3.8126e-01, PNorm = 77.3702, GNorm = 0.2069
Meta loss on this task batch = 3.3830e-01, Meta loss averaged over last 500 steps = 3.8109e-01, PNorm = 77.3830, GNorm = 0.2333
Meta loss on this task batch = 3.8855e-01, Meta loss averaged over last 500 steps = 3.8121e-01, PNorm = 77.3940, GNorm = 0.2705
Meta loss on this task batch = 4.0410e-01, Meta loss averaged over last 500 steps = 3.8121e-01, PNorm = 77.4046, GNorm = 0.3006
Took 131.2846999168396 seconds to complete one epoch of meta training
Took 139.9121539592743 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506231
Epoch 126
Meta loss on this task batch = 3.6672e-01, Meta loss averaged over last 500 steps = 3.8117e-01, PNorm = 77.4143, GNorm = 0.2611
Meta loss on this task batch = 3.3495e-01, Meta loss averaged over last 500 steps = 3.8100e-01, PNorm = 77.4233, GNorm = 0.2407
Meta loss on this task batch = 3.6248e-01, Meta loss averaged over last 500 steps = 3.8097e-01, PNorm = 77.4322, GNorm = 0.2739
Meta loss on this task batch = 3.7042e-01, Meta loss averaged over last 500 steps = 3.8103e-01, PNorm = 77.4401, GNorm = 0.2749
Meta loss on this task batch = 4.1844e-01, Meta loss averaged over last 500 steps = 3.8116e-01, PNorm = 77.4465, GNorm = 0.3979
Meta loss on this task batch = 4.0577e-01, Meta loss averaged over last 500 steps = 3.8113e-01, PNorm = 77.4492, GNorm = 0.2909
Meta loss on this task batch = 3.9940e-01, Meta loss averaged over last 500 steps = 3.8107e-01, PNorm = 77.4511, GNorm = 0.2506
Meta loss on this task batch = 3.8634e-01, Meta loss averaged over last 500 steps = 3.8100e-01, PNorm = 77.4549, GNorm = 0.2606
Meta loss on this task batch = 3.3160e-01, Meta loss averaged over last 500 steps = 3.8097e-01, PNorm = 77.4620, GNorm = 0.2350
Meta loss on this task batch = 3.3567e-01, Meta loss averaged over last 500 steps = 3.8094e-01, PNorm = 77.4708, GNorm = 0.2437
Meta loss on this task batch = 3.4564e-01, Meta loss averaged over last 500 steps = 3.8090e-01, PNorm = 77.4831, GNorm = 0.2613
Meta loss on this task batch = 3.7368e-01, Meta loss averaged over last 500 steps = 3.8079e-01, PNorm = 77.4972, GNorm = 0.2543
Meta loss on this task batch = 3.2359e-01, Meta loss averaged over last 500 steps = 3.8068e-01, PNorm = 77.5102, GNorm = 0.2338
Meta loss on this task batch = 4.2407e-01, Meta loss averaged over last 500 steps = 3.8068e-01, PNorm = 77.5203, GNorm = 0.3067
Meta loss on this task batch = 3.5969e-01, Meta loss averaged over last 500 steps = 3.8063e-01, PNorm = 77.5293, GNorm = 0.2478
Meta loss on this task batch = 3.9112e-01, Meta loss averaged over last 500 steps = 3.8069e-01, PNorm = 77.5387, GNorm = 0.2311
Meta loss on this task batch = 3.7316e-01, Meta loss averaged over last 500 steps = 3.8069e-01, PNorm = 77.5494, GNorm = 0.2496
Meta loss on this task batch = 4.1488e-01, Meta loss averaged over last 500 steps = 3.8069e-01, PNorm = 77.5609, GNorm = 0.2277
Meta loss on this task batch = 3.5386e-01, Meta loss averaged over last 500 steps = 3.8048e-01, PNorm = 77.5734, GNorm = 0.2851
Took 132.99012327194214 seconds to complete one epoch of meta training
Took 141.54228591918945 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507202
Epoch 127
Meta loss on this task batch = 3.4731e-01, Meta loss averaged over last 500 steps = 3.8032e-01, PNorm = 77.5862, GNorm = 0.2202
Meta loss on this task batch = 4.2609e-01, Meta loss averaged over last 500 steps = 3.8043e-01, PNorm = 77.5964, GNorm = 0.2717
Meta loss on this task batch = 4.1196e-01, Meta loss averaged over last 500 steps = 3.8039e-01, PNorm = 77.6068, GNorm = 0.2313
Meta loss on this task batch = 3.7704e-01, Meta loss averaged over last 500 steps = 3.8060e-01, PNorm = 77.6177, GNorm = 0.2564
Meta loss on this task batch = 3.7234e-01, Meta loss averaged over last 500 steps = 3.8049e-01, PNorm = 77.6289, GNorm = 0.2341
Meta loss on this task batch = 4.1755e-01, Meta loss averaged over last 500 steps = 3.8055e-01, PNorm = 77.6397, GNorm = 0.2617
Meta loss on this task batch = 3.3578e-01, Meta loss averaged over last 500 steps = 3.8045e-01, PNorm = 77.6498, GNorm = 0.2111
Meta loss on this task batch = 4.3343e-01, Meta loss averaged over last 500 steps = 3.8048e-01, PNorm = 77.6608, GNorm = 0.2726
Meta loss on this task batch = 3.4999e-01, Meta loss averaged over last 500 steps = 3.8040e-01, PNorm = 77.6711, GNorm = 0.2117
Meta loss on this task batch = 3.8844e-01, Meta loss averaged over last 500 steps = 3.8032e-01, PNorm = 77.6819, GNorm = 0.2143
Meta loss on this task batch = 3.3474e-01, Meta loss averaged over last 500 steps = 3.8025e-01, PNorm = 77.6925, GNorm = 0.2219
Meta loss on this task batch = 3.6354e-01, Meta loss averaged over last 500 steps = 3.8024e-01, PNorm = 77.7037, GNorm = 0.2236
Meta loss on this task batch = 3.4318e-01, Meta loss averaged over last 500 steps = 3.8020e-01, PNorm = 77.7148, GNorm = 0.2273
Meta loss on this task batch = 3.5071e-01, Meta loss averaged over last 500 steps = 3.8022e-01, PNorm = 77.7264, GNorm = 0.2372
Meta loss on this task batch = 4.0573e-01, Meta loss averaged over last 500 steps = 3.8021e-01, PNorm = 77.7363, GNorm = 0.2886
Meta loss on this task batch = 3.8940e-01, Meta loss averaged over last 500 steps = 3.8023e-01, PNorm = 77.7444, GNorm = 0.2674
Meta loss on this task batch = 2.6080e-01, Meta loss averaged over last 500 steps = 3.8007e-01, PNorm = 77.7530, GNorm = 0.2111
Meta loss on this task batch = 3.5749e-01, Meta loss averaged over last 500 steps = 3.8001e-01, PNorm = 77.7603, GNorm = 0.2478
Meta loss on this task batch = 3.4243e-01, Meta loss averaged over last 500 steps = 3.7981e-01, PNorm = 77.7678, GNorm = 0.2554
Took 133.01239895820618 seconds to complete one epoch of meta training
Took 141.89453268051147 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491904
Epoch 128
Meta loss on this task batch = 4.0674e-01, Meta loss averaged over last 500 steps = 3.7978e-01, PNorm = 77.7749, GNorm = 0.2840
Meta loss on this task batch = 3.2390e-01, Meta loss averaged over last 500 steps = 3.7971e-01, PNorm = 77.7831, GNorm = 0.2109
Meta loss on this task batch = 3.6381e-01, Meta loss averaged over last 500 steps = 3.7965e-01, PNorm = 77.7900, GNorm = 0.2726
Meta loss on this task batch = 3.9278e-01, Meta loss averaged over last 500 steps = 3.7953e-01, PNorm = 77.7952, GNorm = 0.2951
Meta loss on this task batch = 3.4842e-01, Meta loss averaged over last 500 steps = 3.7940e-01, PNorm = 77.8021, GNorm = 0.2329
Meta loss on this task batch = 3.2643e-01, Meta loss averaged over last 500 steps = 3.7926e-01, PNorm = 77.8101, GNorm = 0.2274
Meta loss on this task batch = 3.6222e-01, Meta loss averaged over last 500 steps = 3.7925e-01, PNorm = 77.8166, GNorm = 0.2353
Meta loss on this task batch = 4.2335e-01, Meta loss averaged over last 500 steps = 3.7924e-01, PNorm = 77.8224, GNorm = 0.2458
Meta loss on this task batch = 4.0371e-01, Meta loss averaged over last 500 steps = 3.7923e-01, PNorm = 77.8271, GNorm = 0.2558
Meta loss on this task batch = 3.6170e-01, Meta loss averaged over last 500 steps = 3.7922e-01, PNorm = 77.8321, GNorm = 0.2544
Meta loss on this task batch = 3.6185e-01, Meta loss averaged over last 500 steps = 3.7921e-01, PNorm = 77.8382, GNorm = 0.2648
Meta loss on this task batch = 3.8000e-01, Meta loss averaged over last 500 steps = 3.7924e-01, PNorm = 77.8452, GNorm = 0.2304
Meta loss on this task batch = 3.8021e-01, Meta loss averaged over last 500 steps = 3.7926e-01, PNorm = 77.8532, GNorm = 0.2659
Meta loss on this task batch = 3.7954e-01, Meta loss averaged over last 500 steps = 3.7921e-01, PNorm = 77.8616, GNorm = 0.2307
Meta loss on this task batch = 3.5523e-01, Meta loss averaged over last 500 steps = 3.7919e-01, PNorm = 77.8702, GNorm = 0.2163
Meta loss on this task batch = 3.2993e-01, Meta loss averaged over last 500 steps = 3.7907e-01, PNorm = 77.8794, GNorm = 0.2344
Meta loss on this task batch = 3.9403e-01, Meta loss averaged over last 500 steps = 3.7907e-01, PNorm = 77.8888, GNorm = 0.2248
Meta loss on this task batch = 3.8056e-01, Meta loss averaged over last 500 steps = 3.7902e-01, PNorm = 77.8995, GNorm = 0.2428
Meta loss on this task batch = 3.5937e-01, Meta loss averaged over last 500 steps = 3.7888e-01, PNorm = 77.9120, GNorm = 0.2559
Took 134.5621509552002 seconds to complete one epoch of meta training
Took 143.1573212146759 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501889
Epoch 129
Meta loss on this task batch = 3.9598e-01, Meta loss averaged over last 500 steps = 3.7892e-01, PNorm = 77.9252, GNorm = 0.2389
Meta loss on this task batch = 3.5920e-01, Meta loss averaged over last 500 steps = 3.7888e-01, PNorm = 77.9381, GNorm = 0.2461
Meta loss on this task batch = 3.8195e-01, Meta loss averaged over last 500 steps = 3.7896e-01, PNorm = 77.9493, GNorm = 0.2383
Meta loss on this task batch = 3.3504e-01, Meta loss averaged over last 500 steps = 3.7892e-01, PNorm = 77.9617, GNorm = 0.1979
Meta loss on this task batch = 3.2015e-01, Meta loss averaged over last 500 steps = 3.7870e-01, PNorm = 77.9750, GNorm = 0.2132
Meta loss on this task batch = 3.7257e-01, Meta loss averaged over last 500 steps = 3.7883e-01, PNorm = 77.9880, GNorm = 0.2402
Meta loss on this task batch = 3.6688e-01, Meta loss averaged over last 500 steps = 3.7885e-01, PNorm = 77.9999, GNorm = 0.2282
Meta loss on this task batch = 3.7984e-01, Meta loss averaged over last 500 steps = 3.7880e-01, PNorm = 78.0105, GNorm = 0.2635
Meta loss on this task batch = 3.8640e-01, Meta loss averaged over last 500 steps = 3.7883e-01, PNorm = 78.0171, GNorm = 0.2541
Meta loss on this task batch = 3.6637e-01, Meta loss averaged over last 500 steps = 3.7878e-01, PNorm = 78.0249, GNorm = 0.2493
Meta loss on this task batch = 3.7414e-01, Meta loss averaged over last 500 steps = 3.7881e-01, PNorm = 78.0324, GNorm = 0.2694
Meta loss on this task batch = 2.9317e-01, Meta loss averaged over last 500 steps = 3.7862e-01, PNorm = 78.0420, GNorm = 0.2267
Meta loss on this task batch = 3.2998e-01, Meta loss averaged over last 500 steps = 3.7844e-01, PNorm = 78.0509, GNorm = 0.2432
Meta loss on this task batch = 3.4657e-01, Meta loss averaged over last 500 steps = 3.7839e-01, PNorm = 78.0619, GNorm = 0.2530
Meta loss on this task batch = 3.6248e-01, Meta loss averaged over last 500 steps = 3.7837e-01, PNorm = 78.0720, GNorm = 0.2636
Meta loss on this task batch = 3.5508e-01, Meta loss averaged over last 500 steps = 3.7829e-01, PNorm = 78.0816, GNorm = 0.2398
Meta loss on this task batch = 4.1266e-01, Meta loss averaged over last 500 steps = 3.7828e-01, PNorm = 78.0921, GNorm = 0.2737
Meta loss on this task batch = 3.7144e-01, Meta loss averaged over last 500 steps = 3.7834e-01, PNorm = 78.0993, GNorm = 0.2860
Meta loss on this task batch = 2.9430e-01, Meta loss averaged over last 500 steps = 3.7817e-01, PNorm = 78.1070, GNorm = 0.2782
Took 131.99086666107178 seconds to complete one epoch of meta training
Took 140.48292756080627 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506385
Epoch 130
Meta loss on this task batch = 3.9826e-01, Meta loss averaged over last 500 steps = 3.7819e-01, PNorm = 78.1119, GNorm = 0.3274
Meta loss on this task batch = 4.3094e-01, Meta loss averaged over last 500 steps = 3.7823e-01, PNorm = 78.1102, GNorm = 0.3628
Meta loss on this task batch = 3.3793e-01, Meta loss averaged over last 500 steps = 3.7807e-01, PNorm = 78.1098, GNorm = 0.2639
Meta loss on this task batch = 3.0652e-01, Meta loss averaged over last 500 steps = 3.7797e-01, PNorm = 78.1107, GNorm = 0.2096
Meta loss on this task batch = 3.5039e-01, Meta loss averaged over last 500 steps = 3.7782e-01, PNorm = 78.1149, GNorm = 0.2335
Meta loss on this task batch = 3.4214e-01, Meta loss averaged over last 500 steps = 3.7788e-01, PNorm = 78.1230, GNorm = 0.2556
Meta loss on this task batch = 3.3911e-01, Meta loss averaged over last 500 steps = 3.7772e-01, PNorm = 78.1337, GNorm = 0.2321
Meta loss on this task batch = 3.3458e-01, Meta loss averaged over last 500 steps = 3.7765e-01, PNorm = 78.1447, GNorm = 0.2178
Meta loss on this task batch = 3.9023e-01, Meta loss averaged over last 500 steps = 3.7773e-01, PNorm = 78.1561, GNorm = 0.2637
Meta loss on this task batch = 3.6843e-01, Meta loss averaged over last 500 steps = 3.7773e-01, PNorm = 78.1654, GNorm = 0.2354
Meta loss on this task batch = 3.5548e-01, Meta loss averaged over last 500 steps = 3.7759e-01, PNorm = 78.1756, GNorm = 0.2400
Meta loss on this task batch = 3.4954e-01, Meta loss averaged over last 500 steps = 3.7745e-01, PNorm = 78.1870, GNorm = 0.2232
Meta loss on this task batch = 4.2234e-01, Meta loss averaged over last 500 steps = 3.7753e-01, PNorm = 78.1971, GNorm = 0.2791
Meta loss on this task batch = 4.1604e-01, Meta loss averaged over last 500 steps = 3.7763e-01, PNorm = 78.2063, GNorm = 0.3035
Meta loss on this task batch = 3.2615e-01, Meta loss averaged over last 500 steps = 3.7751e-01, PNorm = 78.2160, GNorm = 0.2698
Meta loss on this task batch = 3.7984e-01, Meta loss averaged over last 500 steps = 3.7738e-01, PNorm = 78.2249, GNorm = 0.2619
Meta loss on this task batch = 3.5107e-01, Meta loss averaged over last 500 steps = 3.7731e-01, PNorm = 78.2371, GNorm = 0.2663
Meta loss on this task batch = 4.0892e-01, Meta loss averaged over last 500 steps = 3.7726e-01, PNorm = 78.2486, GNorm = 0.2482
Meta loss on this task batch = 3.4878e-01, Meta loss averaged over last 500 steps = 3.7720e-01, PNorm = 78.2603, GNorm = 0.3227
Took 133.07908940315247 seconds to complete one epoch of meta training
Took 141.86836075782776 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473341
Epoch 131
Meta loss on this task batch = 3.8152e-01, Meta loss averaged over last 500 steps = 3.7709e-01, PNorm = 78.2734, GNorm = 0.2631
Meta loss on this task batch = 3.4200e-01, Meta loss averaged over last 500 steps = 3.7709e-01, PNorm = 78.2867, GNorm = 0.2706
Meta loss on this task batch = 3.5904e-01, Meta loss averaged over last 500 steps = 3.7697e-01, PNorm = 78.2973, GNorm = 0.2708
Meta loss on this task batch = 3.7970e-01, Meta loss averaged over last 500 steps = 3.7701e-01, PNorm = 78.3075, GNorm = 0.2781
Meta loss on this task batch = 3.9568e-01, Meta loss averaged over last 500 steps = 3.7702e-01, PNorm = 78.3161, GNorm = 0.3389
Meta loss on this task batch = 3.6779e-01, Meta loss averaged over last 500 steps = 3.7705e-01, PNorm = 78.3256, GNorm = 0.2653
Meta loss on this task batch = 3.4652e-01, Meta loss averaged over last 500 steps = 3.7695e-01, PNorm = 78.3349, GNorm = 0.2060
Meta loss on this task batch = 3.0973e-01, Meta loss averaged over last 500 steps = 3.7668e-01, PNorm = 78.3455, GNorm = 0.2305
Meta loss on this task batch = 3.6187e-01, Meta loss averaged over last 500 steps = 3.7671e-01, PNorm = 78.3574, GNorm = 0.2696
Meta loss on this task batch = 3.7090e-01, Meta loss averaged over last 500 steps = 3.7665e-01, PNorm = 78.3710, GNorm = 0.2236
Meta loss on this task batch = 3.8305e-01, Meta loss averaged over last 500 steps = 3.7667e-01, PNorm = 78.3859, GNorm = 0.2381
Meta loss on this task batch = 3.3918e-01, Meta loss averaged over last 500 steps = 3.7652e-01, PNorm = 78.3989, GNorm = 0.2518
Meta loss on this task batch = 3.4426e-01, Meta loss averaged over last 500 steps = 3.7648e-01, PNorm = 78.4121, GNorm = 0.2300
Meta loss on this task batch = 3.9749e-01, Meta loss averaged over last 500 steps = 3.7643e-01, PNorm = 78.4218, GNorm = 0.2807
Meta loss on this task batch = 4.3881e-01, Meta loss averaged over last 500 steps = 3.7653e-01, PNorm = 78.4288, GNorm = 0.2879
Meta loss on this task batch = 3.4591e-01, Meta loss averaged over last 500 steps = 3.7647e-01, PNorm = 78.4368, GNorm = 0.2282
Meta loss on this task batch = 3.6778e-01, Meta loss averaged over last 500 steps = 3.7640e-01, PNorm = 78.4437, GNorm = 0.2543
Meta loss on this task batch = 4.2277e-01, Meta loss averaged over last 500 steps = 3.7640e-01, PNorm = 78.4512, GNorm = 0.2822
Meta loss on this task batch = 3.8878e-01, Meta loss averaged over last 500 steps = 3.7654e-01, PNorm = 78.4578, GNorm = 0.3149
Took 131.03434038162231 seconds to complete one epoch of meta training
Took 139.13479924201965 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479309
Epoch 132
Meta loss on this task batch = 3.1962e-01, Meta loss averaged over last 500 steps = 3.7633e-01, PNorm = 78.4653, GNorm = 0.2230
Meta loss on this task batch = 3.7396e-01, Meta loss averaged over last 500 steps = 3.7637e-01, PNorm = 78.4732, GNorm = 0.2510
Meta loss on this task batch = 3.6187e-01, Meta loss averaged over last 500 steps = 3.7643e-01, PNorm = 78.4837, GNorm = 0.2620
Meta loss on this task batch = 3.5134e-01, Meta loss averaged over last 500 steps = 3.7638e-01, PNorm = 78.4964, GNorm = 0.2583
Meta loss on this task batch = 3.6337e-01, Meta loss averaged over last 500 steps = 3.7638e-01, PNorm = 78.5103, GNorm = 0.2538
Meta loss on this task batch = 4.2791e-01, Meta loss averaged over last 500 steps = 3.7643e-01, PNorm = 78.5228, GNorm = 0.2631
Meta loss on this task batch = 3.3787e-01, Meta loss averaged over last 500 steps = 3.7638e-01, PNorm = 78.5369, GNorm = 0.2218
Meta loss on this task batch = 3.9359e-01, Meta loss averaged over last 500 steps = 3.7639e-01, PNorm = 78.5494, GNorm = 0.2616
Meta loss on this task batch = 4.0678e-01, Meta loss averaged over last 500 steps = 3.7643e-01, PNorm = 78.5582, GNorm = 0.2830
Meta loss on this task batch = 3.1672e-01, Meta loss averaged over last 500 steps = 3.7629e-01, PNorm = 78.5678, GNorm = 0.2185
Meta loss on this task batch = 3.2048e-01, Meta loss averaged over last 500 steps = 3.7621e-01, PNorm = 78.5775, GNorm = 0.2290
Meta loss on this task batch = 3.6817e-01, Meta loss averaged over last 500 steps = 3.7627e-01, PNorm = 78.5885, GNorm = 0.2699
Meta loss on this task batch = 3.9286e-01, Meta loss averaged over last 500 steps = 3.7625e-01, PNorm = 78.5991, GNorm = 0.2209
Meta loss on this task batch = 3.7088e-01, Meta loss averaged over last 500 steps = 3.7628e-01, PNorm = 78.6099, GNorm = 0.2473
Meta loss on this task batch = 3.6173e-01, Meta loss averaged over last 500 steps = 3.7619e-01, PNorm = 78.6210, GNorm = 0.2370
Meta loss on this task batch = 3.4745e-01, Meta loss averaged over last 500 steps = 3.7617e-01, PNorm = 78.6336, GNorm = 0.2405
Meta loss on this task batch = 4.1410e-01, Meta loss averaged over last 500 steps = 3.7615e-01, PNorm = 78.6467, GNorm = 0.2569
Meta loss on this task batch = 3.7404e-01, Meta loss averaged over last 500 steps = 3.7614e-01, PNorm = 78.6593, GNorm = 0.2340
Meta loss on this task batch = 3.7355e-01, Meta loss averaged over last 500 steps = 3.7613e-01, PNorm = 78.6732, GNorm = 0.2865
Took 131.7353003025055 seconds to complete one epoch of meta training
Took 140.448180437088 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460908
Epoch 133
Meta loss on this task batch = 3.9348e-01, Meta loss averaged over last 500 steps = 3.7610e-01, PNorm = 78.6863, GNorm = 0.2647
Meta loss on this task batch = 4.4671e-01, Meta loss averaged over last 500 steps = 3.7621e-01, PNorm = 78.6977, GNorm = 0.2796
Meta loss on this task batch = 3.4688e-01, Meta loss averaged over last 500 steps = 3.7610e-01, PNorm = 78.7086, GNorm = 0.2100
Meta loss on this task batch = 3.9421e-01, Meta loss averaged over last 500 steps = 3.7611e-01, PNorm = 78.7188, GNorm = 0.2767
Meta loss on this task batch = 3.3397e-01, Meta loss averaged over last 500 steps = 3.7612e-01, PNorm = 78.7299, GNorm = 0.2239
Meta loss on this task batch = 4.1897e-01, Meta loss averaged over last 500 steps = 3.7621e-01, PNorm = 78.7403, GNorm = 0.3259
Meta loss on this task batch = 3.5885e-01, Meta loss averaged over last 500 steps = 3.7603e-01, PNorm = 78.7511, GNorm = 0.2243
Meta loss on this task batch = 3.6913e-01, Meta loss averaged over last 500 steps = 3.7602e-01, PNorm = 78.7616, GNorm = 0.3120
Meta loss on this task batch = 3.3658e-01, Meta loss averaged over last 500 steps = 3.7595e-01, PNorm = 78.7733, GNorm = 0.2711
Meta loss on this task batch = 3.1164e-01, Meta loss averaged over last 500 steps = 3.7593e-01, PNorm = 78.7848, GNorm = 0.2343
Meta loss on this task batch = 3.3426e-01, Meta loss averaged over last 500 steps = 3.7590e-01, PNorm = 78.7955, GNorm = 0.2244
Meta loss on this task batch = 3.4933e-01, Meta loss averaged over last 500 steps = 3.7575e-01, PNorm = 78.8038, GNorm = 0.2626
Meta loss on this task batch = 3.8290e-01, Meta loss averaged over last 500 steps = 3.7583e-01, PNorm = 78.8099, GNorm = 0.2694
Meta loss on this task batch = 2.9268e-01, Meta loss averaged over last 500 steps = 3.7562e-01, PNorm = 78.8139, GNorm = 0.2460
Meta loss on this task batch = 3.5249e-01, Meta loss averaged over last 500 steps = 3.7565e-01, PNorm = 78.8159, GNorm = 0.2963
Meta loss on this task batch = 3.5448e-01, Meta loss averaged over last 500 steps = 3.7542e-01, PNorm = 78.8169, GNorm = 0.2401
Meta loss on this task batch = 3.6988e-01, Meta loss averaged over last 500 steps = 3.7547e-01, PNorm = 78.8155, GNorm = 0.3172
Meta loss on this task batch = 3.6349e-01, Meta loss averaged over last 500 steps = 3.7542e-01, PNorm = 78.8171, GNorm = 0.2549
Meta loss on this task batch = 3.7820e-01, Meta loss averaged over last 500 steps = 3.7543e-01, PNorm = 78.8195, GNorm = 0.3077
Took 133.0148229598999 seconds to complete one epoch of meta training
Took 141.24771356582642 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.428846
Epoch 134
Meta loss on this task batch = 3.9460e-01, Meta loss averaged over last 500 steps = 3.7539e-01, PNorm = 78.8234, GNorm = 0.2359
Meta loss on this task batch = 3.0020e-01, Meta loss averaged over last 500 steps = 3.7519e-01, PNorm = 78.8309, GNorm = 0.2416
Meta loss on this task batch = 3.8472e-01, Meta loss averaged over last 500 steps = 3.7507e-01, PNorm = 78.8397, GNorm = 0.2391
Meta loss on this task batch = 4.2687e-01, Meta loss averaged over last 500 steps = 3.7509e-01, PNorm = 78.8509, GNorm = 0.2775
Meta loss on this task batch = 3.1641e-01, Meta loss averaged over last 500 steps = 3.7508e-01, PNorm = 78.8623, GNorm = 0.2178
Meta loss on this task batch = 3.4903e-01, Meta loss averaged over last 500 steps = 3.7510e-01, PNorm = 78.8740, GNorm = 0.2294
Meta loss on this task batch = 3.1617e-01, Meta loss averaged over last 500 steps = 3.7499e-01, PNorm = 78.8860, GNorm = 0.2136
Meta loss on this task batch = 3.9848e-01, Meta loss averaged over last 500 steps = 3.7497e-01, PNorm = 78.8966, GNorm = 0.2742
Meta loss on this task batch = 3.7416e-01, Meta loss averaged over last 500 steps = 3.7497e-01, PNorm = 78.9065, GNorm = 0.2517
Meta loss on this task batch = 3.2312e-01, Meta loss averaged over last 500 steps = 3.7481e-01, PNorm = 78.9153, GNorm = 0.2367
Meta loss on this task batch = 3.4741e-01, Meta loss averaged over last 500 steps = 3.7474e-01, PNorm = 78.9257, GNorm = 0.2582
Meta loss on this task batch = 3.7620e-01, Meta loss averaged over last 500 steps = 3.7477e-01, PNorm = 78.9330, GNorm = 0.2620
Meta loss on this task batch = 3.6453e-01, Meta loss averaged over last 500 steps = 3.7466e-01, PNorm = 78.9388, GNorm = 0.2487
Meta loss on this task batch = 4.0687e-01, Meta loss averaged over last 500 steps = 3.7485e-01, PNorm = 78.9444, GNorm = 0.3068
Meta loss on this task batch = 3.5567e-01, Meta loss averaged over last 500 steps = 3.7490e-01, PNorm = 78.9520, GNorm = 0.2291
Meta loss on this task batch = 3.8684e-01, Meta loss averaged over last 500 steps = 3.7490e-01, PNorm = 78.9611, GNorm = 0.2670
Meta loss on this task batch = 4.4317e-01, Meta loss averaged over last 500 steps = 3.7497e-01, PNorm = 78.9705, GNorm = 0.2778
Meta loss on this task batch = 3.8331e-01, Meta loss averaged over last 500 steps = 3.7492e-01, PNorm = 78.9807, GNorm = 0.2447
Meta loss on this task batch = 3.3286e-01, Meta loss averaged over last 500 steps = 3.7476e-01, PNorm = 78.9931, GNorm = 0.2972
Took 131.5350067615509 seconds to complete one epoch of meta training
Took 140.67925238609314 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476609
Epoch 135
Meta loss on this task batch = 3.7450e-01, Meta loss averaged over last 500 steps = 3.7473e-01, PNorm = 79.0045, GNorm = 0.2398
Meta loss on this task batch = 3.5503e-01, Meta loss averaged over last 500 steps = 3.7472e-01, PNorm = 79.0154, GNorm = 0.2401
Meta loss on this task batch = 4.4551e-01, Meta loss averaged over last 500 steps = 3.7486e-01, PNorm = 79.0260, GNorm = 0.3197
Meta loss on this task batch = 3.8607e-01, Meta loss averaged over last 500 steps = 3.7492e-01, PNorm = 79.0353, GNorm = 0.3079
Meta loss on this task batch = 3.3808e-01, Meta loss averaged over last 500 steps = 3.7486e-01, PNorm = 79.0439, GNorm = 0.2338
Meta loss on this task batch = 4.2042e-01, Meta loss averaged over last 500 steps = 3.7497e-01, PNorm = 79.0531, GNorm = 0.3144
Meta loss on this task batch = 4.1029e-01, Meta loss averaged over last 500 steps = 3.7503e-01, PNorm = 79.0623, GNorm = 0.2588
Meta loss on this task batch = 3.1799e-01, Meta loss averaged over last 500 steps = 3.7499e-01, PNorm = 79.0737, GNorm = 0.2367
Meta loss on this task batch = 2.9689e-01, Meta loss averaged over last 500 steps = 3.7481e-01, PNorm = 79.0879, GNorm = 0.2375
Meta loss on this task batch = 3.4421e-01, Meta loss averaged over last 500 steps = 3.7477e-01, PNorm = 79.1029, GNorm = 0.2435
Meta loss on this task batch = 3.4225e-01, Meta loss averaged over last 500 steps = 3.7466e-01, PNorm = 79.1176, GNorm = 0.2189
Meta loss on this task batch = 2.9707e-01, Meta loss averaged over last 500 steps = 3.7448e-01, PNorm = 79.1327, GNorm = 0.2158
Meta loss on this task batch = 3.6253e-01, Meta loss averaged over last 500 steps = 3.7433e-01, PNorm = 79.1475, GNorm = 0.2411
Meta loss on this task batch = 3.8187e-01, Meta loss averaged over last 500 steps = 3.7426e-01, PNorm = 79.1613, GNorm = 0.2369
Meta loss on this task batch = 3.6485e-01, Meta loss averaged over last 500 steps = 3.7427e-01, PNorm = 79.1755, GNorm = 0.2393
Meta loss on this task batch = 3.2707e-01, Meta loss averaged over last 500 steps = 3.7403e-01, PNorm = 79.1896, GNorm = 0.2576
Meta loss on this task batch = 3.6931e-01, Meta loss averaged over last 500 steps = 3.7387e-01, PNorm = 79.2037, GNorm = 0.2767
Meta loss on this task batch = 3.7116e-01, Meta loss averaged over last 500 steps = 3.7395e-01, PNorm = 79.2166, GNorm = 0.2646
Meta loss on this task batch = 3.3393e-01, Meta loss averaged over last 500 steps = 3.7399e-01, PNorm = 79.2273, GNorm = 0.2634
Took 135.17510795593262 seconds to complete one epoch of meta training
Took 142.41476607322693 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477679
Epoch 136
Meta loss on this task batch = 3.8991e-01, Meta loss averaged over last 500 steps = 3.7394e-01, PNorm = 79.2370, GNorm = 0.2349
Meta loss on this task batch = 3.0125e-01, Meta loss averaged over last 500 steps = 3.7383e-01, PNorm = 79.2475, GNorm = 0.2456
Meta loss on this task batch = 4.2621e-01, Meta loss averaged over last 500 steps = 3.7379e-01, PNorm = 79.2587, GNorm = 0.3057
Meta loss on this task batch = 3.3969e-01, Meta loss averaged over last 500 steps = 3.7374e-01, PNorm = 79.2702, GNorm = 0.2703
Meta loss on this task batch = 3.7319e-01, Meta loss averaged over last 500 steps = 3.7366e-01, PNorm = 79.2806, GNorm = 0.2515
Meta loss on this task batch = 3.3824e-01, Meta loss averaged over last 500 steps = 3.7361e-01, PNorm = 79.2903, GNorm = 0.2709
Meta loss on this task batch = 3.2296e-01, Meta loss averaged over last 500 steps = 3.7352e-01, PNorm = 79.3004, GNorm = 0.2687
Meta loss on this task batch = 3.4127e-01, Meta loss averaged over last 500 steps = 3.7341e-01, PNorm = 79.3096, GNorm = 0.2551
Meta loss on this task batch = 4.1201e-01, Meta loss averaged over last 500 steps = 3.7344e-01, PNorm = 79.3179, GNorm = 0.2828
Meta loss on this task batch = 3.8046e-01, Meta loss averaged over last 500 steps = 3.7325e-01, PNorm = 79.3251, GNorm = 0.2905
Meta loss on this task batch = 3.9319e-01, Meta loss averaged over last 500 steps = 3.7316e-01, PNorm = 79.3334, GNorm = 0.2821
Meta loss on this task batch = 3.5852e-01, Meta loss averaged over last 500 steps = 3.7314e-01, PNorm = 79.3428, GNorm = 0.2380
Meta loss on this task batch = 3.7859e-01, Meta loss averaged over last 500 steps = 3.7317e-01, PNorm = 79.3533, GNorm = 0.2468
Meta loss on this task batch = 3.3348e-01, Meta loss averaged over last 500 steps = 3.7302e-01, PNorm = 79.3668, GNorm = 0.2532
Meta loss on this task batch = 3.4190e-01, Meta loss averaged over last 500 steps = 3.7310e-01, PNorm = 79.3816, GNorm = 0.2213
Meta loss on this task batch = 3.5196e-01, Meta loss averaged over last 500 steps = 3.7306e-01, PNorm = 79.3970, GNorm = 0.2285
Meta loss on this task batch = 3.4243e-01, Meta loss averaged over last 500 steps = 3.7284e-01, PNorm = 79.4134, GNorm = 0.2313
Meta loss on this task batch = 3.6353e-01, Meta loss averaged over last 500 steps = 3.7279e-01, PNorm = 79.4280, GNorm = 0.2769
Meta loss on this task batch = 3.8974e-01, Meta loss averaged over last 500 steps = 3.7292e-01, PNorm = 79.4398, GNorm = 0.3537
Took 128.86469531059265 seconds to complete one epoch of meta training
Took 137.32221913337708 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504042
Epoch 137
Meta loss on this task batch = 3.5352e-01, Meta loss averaged over last 500 steps = 3.7284e-01, PNorm = 79.4501, GNorm = 0.2489
Meta loss on this task batch = 3.9996e-01, Meta loss averaged over last 500 steps = 3.7291e-01, PNorm = 79.4571, GNorm = 0.3224
Meta loss on this task batch = 3.2896e-01, Meta loss averaged over last 500 steps = 3.7260e-01, PNorm = 79.4643, GNorm = 0.2220
Meta loss on this task batch = 3.9834e-01, Meta loss averaged over last 500 steps = 3.7267e-01, PNorm = 79.4709, GNorm = 0.2760
Meta loss on this task batch = 3.2008e-01, Meta loss averaged over last 500 steps = 3.7247e-01, PNorm = 79.4767, GNorm = 0.2346
Meta loss on this task batch = 3.8288e-01, Meta loss averaged over last 500 steps = 3.7251e-01, PNorm = 79.4819, GNorm = 0.2350
Meta loss on this task batch = 3.4584e-01, Meta loss averaged over last 500 steps = 3.7241e-01, PNorm = 79.4894, GNorm = 0.2419
Meta loss on this task batch = 3.6241e-01, Meta loss averaged over last 500 steps = 3.7247e-01, PNorm = 79.4976, GNorm = 0.2288
Meta loss on this task batch = 3.8811e-01, Meta loss averaged over last 500 steps = 3.7261e-01, PNorm = 79.5071, GNorm = 0.2502
Meta loss on this task batch = 3.3961e-01, Meta loss averaged over last 500 steps = 3.7257e-01, PNorm = 79.5188, GNorm = 0.2394
Meta loss on this task batch = 3.2199e-01, Meta loss averaged over last 500 steps = 3.7247e-01, PNorm = 79.5323, GNorm = 0.2666
Meta loss on this task batch = 3.2407e-01, Meta loss averaged over last 500 steps = 3.7240e-01, PNorm = 79.5451, GNorm = 0.2257
Meta loss on this task batch = 3.6512e-01, Meta loss averaged over last 500 steps = 3.7240e-01, PNorm = 79.5570, GNorm = 0.3011
Meta loss on this task batch = 4.3045e-01, Meta loss averaged over last 500 steps = 3.7257e-01, PNorm = 79.5669, GNorm = 0.2933
Meta loss on this task batch = 3.8400e-01, Meta loss averaged over last 500 steps = 3.7252e-01, PNorm = 79.5750, GNorm = 0.2665
Meta loss on this task batch = 3.6572e-01, Meta loss averaged over last 500 steps = 3.7249e-01, PNorm = 79.5807, GNorm = 0.3647
Meta loss on this task batch = 3.8298e-01, Meta loss averaged over last 500 steps = 3.7243e-01, PNorm = 79.5877, GNorm = 0.2761
Meta loss on this task batch = 3.6748e-01, Meta loss averaged over last 500 steps = 3.7243e-01, PNorm = 79.5971, GNorm = 0.2504
Meta loss on this task batch = 3.9054e-01, Meta loss averaged over last 500 steps = 3.7254e-01, PNorm = 79.6090, GNorm = 0.3306
Took 134.6704068183899 seconds to complete one epoch of meta training
Took 143.04157400131226 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502597
Epoch 138
Meta loss on this task batch = 3.9231e-01, Meta loss averaged over last 500 steps = 3.7255e-01, PNorm = 79.6213, GNorm = 0.2614
Meta loss on this task batch = 3.7285e-01, Meta loss averaged over last 500 steps = 3.7243e-01, PNorm = 79.6343, GNorm = 0.3023
Meta loss on this task batch = 4.0014e-01, Meta loss averaged over last 500 steps = 3.7241e-01, PNorm = 79.6434, GNorm = 0.2722
Meta loss on this task batch = 4.0536e-01, Meta loss averaged over last 500 steps = 3.7241e-01, PNorm = 79.6496, GNorm = 0.2995
Meta loss on this task batch = 3.8374e-01, Meta loss averaged over last 500 steps = 3.7226e-01, PNorm = 79.6559, GNorm = 0.2845
Meta loss on this task batch = 3.5279e-01, Meta loss averaged over last 500 steps = 3.7220e-01, PNorm = 79.6618, GNorm = 0.2613
Meta loss on this task batch = 3.9977e-01, Meta loss averaged over last 500 steps = 3.7224e-01, PNorm = 79.6646, GNorm = 0.3418
Meta loss on this task batch = 3.4496e-01, Meta loss averaged over last 500 steps = 3.7224e-01, PNorm = 79.6685, GNorm = 0.2371
Meta loss on this task batch = 3.2951e-01, Meta loss averaged over last 500 steps = 3.7207e-01, PNorm = 79.6742, GNorm = 0.2202
Meta loss on this task batch = 3.6166e-01, Meta loss averaged over last 500 steps = 3.7205e-01, PNorm = 79.6806, GNorm = 0.2283
Meta loss on this task batch = 3.6065e-01, Meta loss averaged over last 500 steps = 3.7202e-01, PNorm = 79.6866, GNorm = 0.3086
Meta loss on this task batch = 3.5121e-01, Meta loss averaged over last 500 steps = 3.7198e-01, PNorm = 79.6945, GNorm = 0.3018
Meta loss on this task batch = 3.8326e-01, Meta loss averaged over last 500 steps = 3.7190e-01, PNorm = 79.7044, GNorm = 0.2392
Meta loss on this task batch = 3.5241e-01, Meta loss averaged over last 500 steps = 3.7188e-01, PNorm = 79.7152, GNorm = 0.2360
Meta loss on this task batch = 3.0421e-01, Meta loss averaged over last 500 steps = 3.7171e-01, PNorm = 79.7270, GNorm = 0.2334
Meta loss on this task batch = 3.7433e-01, Meta loss averaged over last 500 steps = 3.7158e-01, PNorm = 79.7380, GNorm = 0.2559
Meta loss on this task batch = 3.6754e-01, Meta loss averaged over last 500 steps = 3.7159e-01, PNorm = 79.7483, GNorm = 0.2922
Meta loss on this task batch = 3.5785e-01, Meta loss averaged over last 500 steps = 3.7158e-01, PNorm = 79.7572, GNorm = 0.3912
Meta loss on this task batch = 3.3579e-01, Meta loss averaged over last 500 steps = 3.7154e-01, PNorm = 79.7667, GNorm = 0.2418
Took 131.66598105430603 seconds to complete one epoch of meta training
Took 140.15776944160461 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503411
Epoch 139
Meta loss on this task batch = 3.4364e-01, Meta loss averaged over last 500 steps = 3.7157e-01, PNorm = 79.7768, GNorm = 0.2277
Meta loss on this task batch = 3.4403e-01, Meta loss averaged over last 500 steps = 3.7149e-01, PNorm = 79.7883, GNorm = 0.2208
Meta loss on this task batch = 3.8605e-01, Meta loss averaged over last 500 steps = 3.7143e-01, PNorm = 79.8007, GNorm = 0.2575
Meta loss on this task batch = 3.6444e-01, Meta loss averaged over last 500 steps = 3.7138e-01, PNorm = 79.8142, GNorm = 0.2465
Meta loss on this task batch = 3.8011e-01, Meta loss averaged over last 500 steps = 3.7147e-01, PNorm = 79.8306, GNorm = 0.2555
Meta loss on this task batch = 3.9151e-01, Meta loss averaged over last 500 steps = 3.7150e-01, PNorm = 79.8459, GNorm = 0.3842
Meta loss on this task batch = 2.7652e-01, Meta loss averaged over last 500 steps = 3.7136e-01, PNorm = 79.8639, GNorm = 0.2450
Meta loss on this task batch = 3.9642e-01, Meta loss averaged over last 500 steps = 3.7131e-01, PNorm = 79.8809, GNorm = 0.2727
Meta loss on this task batch = 3.9134e-01, Meta loss averaged over last 500 steps = 3.7139e-01, PNorm = 79.8945, GNorm = 0.2781
Meta loss on this task batch = 3.8649e-01, Meta loss averaged over last 500 steps = 3.7142e-01, PNorm = 79.9047, GNorm = 0.3419
Meta loss on this task batch = 3.5147e-01, Meta loss averaged over last 500 steps = 3.7131e-01, PNorm = 79.9135, GNorm = 0.2800
Meta loss on this task batch = 3.7185e-01, Meta loss averaged over last 500 steps = 3.7121e-01, PNorm = 79.9208, GNorm = 0.3272
Meta loss on this task batch = 3.3450e-01, Meta loss averaged over last 500 steps = 3.7115e-01, PNorm = 79.9279, GNorm = 0.2591
Meta loss on this task batch = 3.4366e-01, Meta loss averaged over last 500 steps = 3.7101e-01, PNorm = 79.9354, GNorm = 0.2406
Meta loss on this task batch = 3.5755e-01, Meta loss averaged over last 500 steps = 3.7090e-01, PNorm = 79.9431, GNorm = 0.2534
Meta loss on this task batch = 4.4832e-01, Meta loss averaged over last 500 steps = 3.7091e-01, PNorm = 79.9526, GNorm = 0.2865
Meta loss on this task batch = 3.5751e-01, Meta loss averaged over last 500 steps = 3.7073e-01, PNorm = 79.9629, GNorm = 0.2242
Meta loss on this task batch = 3.3039e-01, Meta loss averaged over last 500 steps = 3.7074e-01, PNorm = 79.9767, GNorm = 0.2566
Meta loss on this task batch = 3.3253e-01, Meta loss averaged over last 500 steps = 3.7057e-01, PNorm = 79.9918, GNorm = 0.3009
Took 130.6582281589508 seconds to complete one epoch of meta training
Took 138.1732370853424 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468124
Epoch 140
Meta loss on this task batch = 2.9852e-01, Meta loss averaged over last 500 steps = 3.7048e-01, PNorm = 80.0086, GNorm = 0.2033
Meta loss on this task batch = 3.8113e-01, Meta loss averaged over last 500 steps = 3.7049e-01, PNorm = 80.0236, GNorm = 0.2938
Meta loss on this task batch = 3.3946e-01, Meta loss averaged over last 500 steps = 3.7054e-01, PNorm = 80.0384, GNorm = 0.2494
Meta loss on this task batch = 3.4375e-01, Meta loss averaged over last 500 steps = 3.7042e-01, PNorm = 80.0494, GNorm = 0.2801
Meta loss on this task batch = 2.9723e-01, Meta loss averaged over last 500 steps = 3.7018e-01, PNorm = 80.0590, GNorm = 0.2569
Meta loss on this task batch = 3.6971e-01, Meta loss averaged over last 500 steps = 3.7032e-01, PNorm = 80.0671, GNorm = 0.2744
Meta loss on this task batch = 3.8928e-01, Meta loss averaged over last 500 steps = 3.7044e-01, PNorm = 80.0704, GNorm = 0.3182
Meta loss on this task batch = 3.8404e-01, Meta loss averaged over last 500 steps = 3.7043e-01, PNorm = 80.0751, GNorm = 0.2491
Meta loss on this task batch = 4.2783e-01, Meta loss averaged over last 500 steps = 3.7051e-01, PNorm = 80.0815, GNorm = 0.2670
Meta loss on this task batch = 4.0425e-01, Meta loss averaged over last 500 steps = 3.7062e-01, PNorm = 80.0889, GNorm = 0.2522
Meta loss on this task batch = 3.6314e-01, Meta loss averaged over last 500 steps = 3.7060e-01, PNorm = 80.0995, GNorm = 0.2678
Meta loss on this task batch = 3.8452e-01, Meta loss averaged over last 500 steps = 3.7057e-01, PNorm = 80.1107, GNorm = 0.2727
Meta loss on this task batch = 3.4136e-01, Meta loss averaged over last 500 steps = 3.7058e-01, PNorm = 80.1231, GNorm = 0.2314
Meta loss on this task batch = 3.6766e-01, Meta loss averaged over last 500 steps = 3.7067e-01, PNorm = 80.1355, GNorm = 0.2157
Meta loss on this task batch = 3.1924e-01, Meta loss averaged over last 500 steps = 3.7051e-01, PNorm = 80.1482, GNorm = 0.2374
Meta loss on this task batch = 3.8445e-01, Meta loss averaged over last 500 steps = 3.7049e-01, PNorm = 80.1586, GNorm = 0.2585
Meta loss on this task batch = 4.0556e-01, Meta loss averaged over last 500 steps = 3.7050e-01, PNorm = 80.1682, GNorm = 0.3594
Meta loss on this task batch = 3.3381e-01, Meta loss averaged over last 500 steps = 3.7045e-01, PNorm = 80.1798, GNorm = 0.2653
Meta loss on this task batch = 4.4767e-01, Meta loss averaged over last 500 steps = 3.7054e-01, PNorm = 80.1898, GNorm = 0.3065
Took 130.38369178771973 seconds to complete one epoch of meta training
Took 138.7337987422943 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463869
Epoch 141
Meta loss on this task batch = 3.8795e-01, Meta loss averaged over last 500 steps = 3.7053e-01, PNorm = 80.2007, GNorm = 0.2770
Meta loss on this task batch = 4.0242e-01, Meta loss averaged over last 500 steps = 3.7051e-01, PNorm = 80.2121, GNorm = 0.2560
Meta loss on this task batch = 3.6300e-01, Meta loss averaged over last 500 steps = 3.7042e-01, PNorm = 80.2231, GNorm = 0.2505
Meta loss on this task batch = 3.6559e-01, Meta loss averaged over last 500 steps = 3.7028e-01, PNorm = 80.2347, GNorm = 0.2812
Meta loss on this task batch = 2.9862e-01, Meta loss averaged over last 500 steps = 3.7021e-01, PNorm = 80.2486, GNorm = 0.2402
Meta loss on this task batch = 3.7222e-01, Meta loss averaged over last 500 steps = 3.7036e-01, PNorm = 80.2618, GNorm = 0.2426
Meta loss on this task batch = 3.5199e-01, Meta loss averaged over last 500 steps = 3.7033e-01, PNorm = 80.2758, GNorm = 0.2638
Meta loss on this task batch = 3.8572e-01, Meta loss averaged over last 500 steps = 3.7033e-01, PNorm = 80.2908, GNorm = 0.2934
Meta loss on this task batch = 3.3175e-01, Meta loss averaged over last 500 steps = 3.7021e-01, PNorm = 80.3040, GNorm = 0.2490
Meta loss on this task batch = 3.8093e-01, Meta loss averaged over last 500 steps = 3.7017e-01, PNorm = 80.3142, GNorm = 0.2871
Meta loss on this task batch = 3.9296e-01, Meta loss averaged over last 500 steps = 3.7019e-01, PNorm = 80.3228, GNorm = 0.2824
Meta loss on this task batch = 3.7078e-01, Meta loss averaged over last 500 steps = 3.7018e-01, PNorm = 80.3285, GNorm = 0.3528
Meta loss on this task batch = 3.5066e-01, Meta loss averaged over last 500 steps = 3.7010e-01, PNorm = 80.3325, GNorm = 0.2596
Meta loss on this task batch = 3.9489e-01, Meta loss averaged over last 500 steps = 3.7017e-01, PNorm = 80.3362, GNorm = 0.2557
Meta loss on this task batch = 3.2469e-01, Meta loss averaged over last 500 steps = 3.7005e-01, PNorm = 80.3421, GNorm = 0.2440
Meta loss on this task batch = 3.3645e-01, Meta loss averaged over last 500 steps = 3.6983e-01, PNorm = 80.3482, GNorm = 0.2480
Meta loss on this task batch = 3.8492e-01, Meta loss averaged over last 500 steps = 3.6986e-01, PNorm = 80.3569, GNorm = 0.2620
Meta loss on this task batch = 3.2694e-01, Meta loss averaged over last 500 steps = 3.6988e-01, PNorm = 80.3663, GNorm = 0.2157
Meta loss on this task batch = 3.9615e-01, Meta loss averaged over last 500 steps = 3.7002e-01, PNorm = 80.3753, GNorm = 0.2939
Took 131.47691798210144 seconds to complete one epoch of meta training
Took 140.3710663318634 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474781
Epoch 142
Meta loss on this task batch = 3.1031e-01, Meta loss averaged over last 500 steps = 3.6995e-01, PNorm = 80.3848, GNorm = 0.2162
Meta loss on this task batch = 3.8726e-01, Meta loss averaged over last 500 steps = 3.6994e-01, PNorm = 80.3941, GNorm = 0.2777
Meta loss on this task batch = 3.9109e-01, Meta loss averaged over last 500 steps = 3.6991e-01, PNorm = 80.4006, GNorm = 0.3390
Meta loss on this task batch = 3.5840e-01, Meta loss averaged over last 500 steps = 3.6980e-01, PNorm = 80.4083, GNorm = 0.2396
Meta loss on this task batch = 2.9234e-01, Meta loss averaged over last 500 steps = 3.6965e-01, PNorm = 80.4179, GNorm = 0.2028
Meta loss on this task batch = 3.0961e-01, Meta loss averaged over last 500 steps = 3.6943e-01, PNorm = 80.4281, GNorm = 0.2105
Meta loss on this task batch = 4.0290e-01, Meta loss averaged over last 500 steps = 3.6943e-01, PNorm = 80.4393, GNorm = 0.2539
Meta loss on this task batch = 3.9788e-01, Meta loss averaged over last 500 steps = 3.6955e-01, PNorm = 80.4506, GNorm = 0.3027
Meta loss on this task batch = 3.5531e-01, Meta loss averaged over last 500 steps = 3.6941e-01, PNorm = 80.4616, GNorm = 0.2538
Meta loss on this task batch = 3.9237e-01, Meta loss averaged over last 500 steps = 3.6941e-01, PNorm = 80.4728, GNorm = 0.2563
Meta loss on this task batch = 3.9003e-01, Meta loss averaged over last 500 steps = 3.6932e-01, PNorm = 80.4817, GNorm = 0.2695
Meta loss on this task batch = 3.1492e-01, Meta loss averaged over last 500 steps = 3.6921e-01, PNorm = 80.4909, GNorm = 0.2447
Meta loss on this task batch = 3.6846e-01, Meta loss averaged over last 500 steps = 3.6922e-01, PNorm = 80.4996, GNorm = 0.3033
Meta loss on this task batch = 4.3179e-01, Meta loss averaged over last 500 steps = 3.6939e-01, PNorm = 80.5057, GNorm = 0.3184
Meta loss on this task batch = 3.4243e-01, Meta loss averaged over last 500 steps = 3.6923e-01, PNorm = 80.5136, GNorm = 0.2440
Meta loss on this task batch = 3.5868e-01, Meta loss averaged over last 500 steps = 3.6927e-01, PNorm = 80.5219, GNorm = 0.2734
Meta loss on this task batch = 3.6233e-01, Meta loss averaged over last 500 steps = 3.6925e-01, PNorm = 80.5298, GNorm = 0.2239
Meta loss on this task batch = 3.1855e-01, Meta loss averaged over last 500 steps = 3.6918e-01, PNorm = 80.5386, GNorm = 0.1996
Meta loss on this task batch = 3.7970e-01, Meta loss averaged over last 500 steps = 3.6918e-01, PNorm = 80.5486, GNorm = 0.3017
Took 132.930344581604 seconds to complete one epoch of meta training
Took 141.46289253234863 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491691
Epoch 143
Meta loss on this task batch = 3.9645e-01, Meta loss averaged over last 500 steps = 3.6923e-01, PNorm = 80.5584, GNorm = 0.2518
Meta loss on this task batch = 3.5789e-01, Meta loss averaged over last 500 steps = 3.6918e-01, PNorm = 80.5688, GNorm = 0.2749
Meta loss on this task batch = 3.2327e-01, Meta loss averaged over last 500 steps = 3.6901e-01, PNorm = 80.5789, GNorm = 0.2105
Meta loss on this task batch = 4.0691e-01, Meta loss averaged over last 500 steps = 3.6915e-01, PNorm = 80.5897, GNorm = 0.2555
Meta loss on this task batch = 3.5998e-01, Meta loss averaged over last 500 steps = 3.6917e-01, PNorm = 80.6004, GNorm = 0.2337
Meta loss on this task batch = 3.9529e-01, Meta loss averaged over last 500 steps = 3.6912e-01, PNorm = 80.6106, GNorm = 0.3495
Meta loss on this task batch = 3.8353e-01, Meta loss averaged over last 500 steps = 3.6898e-01, PNorm = 80.6192, GNorm = 0.3108
Meta loss on this task batch = 3.7268e-01, Meta loss averaged over last 500 steps = 3.6897e-01, PNorm = 80.6263, GNorm = 0.2754
Meta loss on this task batch = 3.6022e-01, Meta loss averaged over last 500 steps = 3.6881e-01, PNorm = 80.6345, GNorm = 0.2530
Meta loss on this task batch = 3.9271e-01, Meta loss averaged over last 500 steps = 3.6883e-01, PNorm = 80.6407, GNorm = 0.3238
Meta loss on this task batch = 3.5608e-01, Meta loss averaged over last 500 steps = 3.6876e-01, PNorm = 80.6504, GNorm = 0.2971
Meta loss on this task batch = 3.4648e-01, Meta loss averaged over last 500 steps = 3.6872e-01, PNorm = 80.6623, GNorm = 0.2816
Meta loss on this task batch = 3.0896e-01, Meta loss averaged over last 500 steps = 3.6864e-01, PNorm = 80.6775, GNorm = 0.2905
Meta loss on this task batch = 3.2335e-01, Meta loss averaged over last 500 steps = 3.6859e-01, PNorm = 80.6904, GNorm = 0.2592
Meta loss on this task batch = 3.6443e-01, Meta loss averaged over last 500 steps = 3.6857e-01, PNorm = 80.7016, GNorm = 0.2879
Meta loss on this task batch = 3.5555e-01, Meta loss averaged over last 500 steps = 3.6851e-01, PNorm = 80.7102, GNorm = 0.3007
Meta loss on this task batch = 4.1590e-01, Meta loss averaged over last 500 steps = 3.6868e-01, PNorm = 80.7177, GNorm = 0.2638
Meta loss on this task batch = 4.1475e-01, Meta loss averaged over last 500 steps = 3.6889e-01, PNorm = 80.7256, GNorm = 0.3048
Meta loss on this task batch = 3.9445e-01, Meta loss averaged over last 500 steps = 3.6898e-01, PNorm = 80.7324, GNorm = 0.2891
Took 134.12451314926147 seconds to complete one epoch of meta training
Took 141.7003674507141 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473177
Epoch 144
Meta loss on this task batch = 3.7927e-01, Meta loss averaged over last 500 steps = 3.6900e-01, PNorm = 80.7400, GNorm = 0.2247
Meta loss on this task batch = 3.7417e-01, Meta loss averaged over last 500 steps = 3.6904e-01, PNorm = 80.7478, GNorm = 0.2137
Meta loss on this task batch = 3.2433e-01, Meta loss averaged over last 500 steps = 3.6877e-01, PNorm = 80.7567, GNorm = 0.2253
Meta loss on this task batch = 4.1101e-01, Meta loss averaged over last 500 steps = 3.6885e-01, PNorm = 80.7657, GNorm = 0.2312
Meta loss on this task batch = 3.6070e-01, Meta loss averaged over last 500 steps = 3.6883e-01, PNorm = 80.7770, GNorm = 0.2580
Meta loss on this task batch = 3.0779e-01, Meta loss averaged over last 500 steps = 3.6875e-01, PNorm = 80.7898, GNorm = 0.2276
Meta loss on this task batch = 3.5757e-01, Meta loss averaged over last 500 steps = 3.6874e-01, PNorm = 80.8027, GNorm = 0.2273
Meta loss on this task batch = 3.5011e-01, Meta loss averaged over last 500 steps = 3.6877e-01, PNorm = 80.8167, GNorm = 0.2656
Meta loss on this task batch = 3.8513e-01, Meta loss averaged over last 500 steps = 3.6886e-01, PNorm = 80.8298, GNorm = 0.2762
Meta loss on this task batch = 3.3540e-01, Meta loss averaged over last 500 steps = 3.6874e-01, PNorm = 80.8422, GNorm = 0.2357
Meta loss on this task batch = 4.2259e-01, Meta loss averaged over last 500 steps = 3.6878e-01, PNorm = 80.8544, GNorm = 0.3067
Meta loss on this task batch = 3.2058e-01, Meta loss averaged over last 500 steps = 3.6861e-01, PNorm = 80.8649, GNorm = 0.2704
Meta loss on this task batch = 3.8072e-01, Meta loss averaged over last 500 steps = 3.6874e-01, PNorm = 80.8735, GNorm = 0.3245
Meta loss on this task batch = 3.6133e-01, Meta loss averaged over last 500 steps = 3.6881e-01, PNorm = 80.8812, GNorm = 0.2593
Meta loss on this task batch = 3.7956e-01, Meta loss averaged over last 500 steps = 3.6861e-01, PNorm = 80.8897, GNorm = 0.2937
Meta loss on this task batch = 3.7278e-01, Meta loss averaged over last 500 steps = 3.6871e-01, PNorm = 80.8989, GNorm = 0.2456
Meta loss on this task batch = 3.6006e-01, Meta loss averaged over last 500 steps = 3.6863e-01, PNorm = 80.9091, GNorm = 0.2402
Meta loss on this task batch = 3.8642e-01, Meta loss averaged over last 500 steps = 3.6861e-01, PNorm = 80.9211, GNorm = 0.2582
Meta loss on this task batch = 3.5857e-01, Meta loss averaged over last 500 steps = 3.6849e-01, PNorm = 80.9325, GNorm = 0.2644
Took 134.100102186203 seconds to complete one epoch of meta training
Took 141.95752596855164 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458518
Epoch 145
Meta loss on this task batch = 4.0146e-01, Meta loss averaged over last 500 steps = 3.6846e-01, PNorm = 80.9442, GNorm = 0.2788
Meta loss on this task batch = 4.2895e-01, Meta loss averaged over last 500 steps = 3.6856e-01, PNorm = 80.9527, GNorm = 0.2949
Meta loss on this task batch = 3.1943e-01, Meta loss averaged over last 500 steps = 3.6851e-01, PNorm = 80.9617, GNorm = 0.2597
Meta loss on this task batch = 3.3999e-01, Meta loss averaged over last 500 steps = 3.6848e-01, PNorm = 80.9690, GNorm = 0.2973
Meta loss on this task batch = 3.2806e-01, Meta loss averaged over last 500 steps = 3.6833e-01, PNorm = 80.9760, GNorm = 0.2462
Meta loss on this task batch = 3.6921e-01, Meta loss averaged over last 500 steps = 3.6821e-01, PNorm = 80.9831, GNorm = 0.2548
Meta loss on this task batch = 3.4992e-01, Meta loss averaged over last 500 steps = 3.6809e-01, PNorm = 80.9902, GNorm = 0.2605
Meta loss on this task batch = 2.8963e-01, Meta loss averaged over last 500 steps = 3.6798e-01, PNorm = 80.9981, GNorm = 0.2114
Meta loss on this task batch = 3.8883e-01, Meta loss averaged over last 500 steps = 3.6807e-01, PNorm = 81.0043, GNorm = 0.2990
Meta loss on this task batch = 3.7420e-01, Meta loss averaged over last 500 steps = 3.6808e-01, PNorm = 81.0095, GNorm = 0.2609
Meta loss on this task batch = 3.1717e-01, Meta loss averaged over last 500 steps = 3.6797e-01, PNorm = 81.0159, GNorm = 0.2335
Meta loss on this task batch = 3.4880e-01, Meta loss averaged over last 500 steps = 3.6777e-01, PNorm = 81.0217, GNorm = 0.2982
Meta loss on this task batch = 3.9889e-01, Meta loss averaged over last 500 steps = 3.6781e-01, PNorm = 81.0279, GNorm = 0.2697
Meta loss on this task batch = 3.9337e-01, Meta loss averaged over last 500 steps = 3.6784e-01, PNorm = 81.0332, GNorm = 0.3409
Meta loss on this task batch = 3.6135e-01, Meta loss averaged over last 500 steps = 3.6782e-01, PNorm = 81.0393, GNorm = 0.2723
Meta loss on this task batch = 3.5714e-01, Meta loss averaged over last 500 steps = 3.6784e-01, PNorm = 81.0474, GNorm = 0.2862
Meta loss on this task batch = 3.3199e-01, Meta loss averaged over last 500 steps = 3.6779e-01, PNorm = 81.0567, GNorm = 0.2512
Meta loss on this task batch = 3.0052e-01, Meta loss averaged over last 500 steps = 3.6758e-01, PNorm = 81.0672, GNorm = 0.2597
Meta loss on this task batch = 2.9537e-01, Meta loss averaged over last 500 steps = 3.6731e-01, PNorm = 81.0791, GNorm = 0.3024
Took 133.78805661201477 seconds to complete one epoch of meta training
Took 141.48679876327515 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479607
Epoch 146
Meta loss on this task batch = 3.5827e-01, Meta loss averaged over last 500 steps = 3.6737e-01, PNorm = 81.0904, GNorm = 0.2438
Meta loss on this task batch = 4.2581e-01, Meta loss averaged over last 500 steps = 3.6736e-01, PNorm = 81.1005, GNorm = 0.2514
Meta loss on this task batch = 3.1395e-01, Meta loss averaged over last 500 steps = 3.6728e-01, PNorm = 81.1115, GNorm = 0.2399
Meta loss on this task batch = 3.8694e-01, Meta loss averaged over last 500 steps = 3.6737e-01, PNorm = 81.1219, GNorm = 0.2699
Meta loss on this task batch = 3.1265e-01, Meta loss averaged over last 500 steps = 3.6740e-01, PNorm = 81.1327, GNorm = 0.2320
Meta loss on this task batch = 3.5282e-01, Meta loss averaged over last 500 steps = 3.6735e-01, PNorm = 81.1408, GNorm = 0.2367
Meta loss on this task batch = 4.0055e-01, Meta loss averaged over last 500 steps = 3.6737e-01, PNorm = 81.1481, GNorm = 0.2739
Meta loss on this task batch = 3.4410e-01, Meta loss averaged over last 500 steps = 3.6733e-01, PNorm = 81.1566, GNorm = 0.2837
Meta loss on this task batch = 3.8025e-01, Meta loss averaged over last 500 steps = 3.6741e-01, PNorm = 81.1649, GNorm = 0.2480
Meta loss on this task batch = 2.9671e-01, Meta loss averaged over last 500 steps = 3.6737e-01, PNorm = 81.1728, GNorm = 0.2272
Meta loss on this task batch = 3.6764e-01, Meta loss averaged over last 500 steps = 3.6733e-01, PNorm = 81.1821, GNorm = 0.2356
Meta loss on this task batch = 3.6707e-01, Meta loss averaged over last 500 steps = 3.6730e-01, PNorm = 81.1909, GNorm = 0.2653
Meta loss on this task batch = 3.4066e-01, Meta loss averaged over last 500 steps = 3.6721e-01, PNorm = 81.2010, GNorm = 0.2382
Meta loss on this task batch = 3.5790e-01, Meta loss averaged over last 500 steps = 3.6710e-01, PNorm = 81.2118, GNorm = 0.2445
Meta loss on this task batch = 3.6513e-01, Meta loss averaged over last 500 steps = 3.6699e-01, PNorm = 81.2226, GNorm = 0.2682
Meta loss on this task batch = 4.3004e-01, Meta loss averaged over last 500 steps = 3.6697e-01, PNorm = 81.2321, GNorm = 0.3222
Meta loss on this task batch = 3.3977e-01, Meta loss averaged over last 500 steps = 3.6686e-01, PNorm = 81.2421, GNorm = 0.3739
Meta loss on this task batch = 3.6834e-01, Meta loss averaged over last 500 steps = 3.6697e-01, PNorm = 81.2517, GNorm = 0.2769
Meta loss on this task batch = 4.1107e-01, Meta loss averaged over last 500 steps = 3.6713e-01, PNorm = 81.2623, GNorm = 0.3081
Took 130.44376921653748 seconds to complete one epoch of meta training
Took 139.5082414150238 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485701
Epoch 147
Meta loss on this task batch = 3.5397e-01, Meta loss averaged over last 500 steps = 3.6708e-01, PNorm = 81.2730, GNorm = 0.2468
Meta loss on this task batch = 3.3773e-01, Meta loss averaged over last 500 steps = 3.6703e-01, PNorm = 81.2847, GNorm = 0.2271
Meta loss on this task batch = 3.7578e-01, Meta loss averaged over last 500 steps = 3.6697e-01, PNorm = 81.2964, GNorm = 0.2498
Meta loss on this task batch = 3.2734e-01, Meta loss averaged over last 500 steps = 3.6693e-01, PNorm = 81.3070, GNorm = 0.2472
Meta loss on this task batch = 3.7071e-01, Meta loss averaged over last 500 steps = 3.6682e-01, PNorm = 81.3166, GNorm = 0.2609
Meta loss on this task batch = 3.5444e-01, Meta loss averaged over last 500 steps = 3.6677e-01, PNorm = 81.3250, GNorm = 0.2622
Meta loss on this task batch = 4.2900e-01, Meta loss averaged over last 500 steps = 3.6685e-01, PNorm = 81.3312, GNorm = 0.2780
Meta loss on this task batch = 3.8880e-01, Meta loss averaged over last 500 steps = 3.6681e-01, PNorm = 81.3374, GNorm = 0.3230
Meta loss on this task batch = 3.8642e-01, Meta loss averaged over last 500 steps = 3.6680e-01, PNorm = 81.3422, GNorm = 0.2935
Meta loss on this task batch = 3.4212e-01, Meta loss averaged over last 500 steps = 3.6676e-01, PNorm = 81.3470, GNorm = 0.2732
Meta loss on this task batch = 3.2996e-01, Meta loss averaged over last 500 steps = 3.6658e-01, PNorm = 81.3533, GNorm = 0.2603
Meta loss on this task batch = 3.1966e-01, Meta loss averaged over last 500 steps = 3.6648e-01, PNorm = 81.3607, GNorm = 0.2238
Meta loss on this task batch = 3.0992e-01, Meta loss averaged over last 500 steps = 3.6655e-01, PNorm = 81.3701, GNorm = 0.2225
Meta loss on this task batch = 3.7984e-01, Meta loss averaged over last 500 steps = 3.6665e-01, PNorm = 81.3807, GNorm = 0.2878
Meta loss on this task batch = 3.3024e-01, Meta loss averaged over last 500 steps = 3.6660e-01, PNorm = 81.3910, GNorm = 0.2273
Meta loss on this task batch = 3.6163e-01, Meta loss averaged over last 500 steps = 3.6656e-01, PNorm = 81.4006, GNorm = 0.2888
Meta loss on this task batch = 4.1157e-01, Meta loss averaged over last 500 steps = 3.6670e-01, PNorm = 81.4089, GNorm = 0.3019
Meta loss on this task batch = 3.6065e-01, Meta loss averaged over last 500 steps = 3.6656e-01, PNorm = 81.4179, GNorm = 0.3075
Meta loss on this task batch = 3.1137e-01, Meta loss averaged over last 500 steps = 3.6637e-01, PNorm = 81.4282, GNorm = 0.2646
Took 135.75515961647034 seconds to complete one epoch of meta training
Took 143.5550377368927 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481404
Epoch 148
Meta loss on this task batch = 3.9988e-01, Meta loss averaged over last 500 steps = 3.6635e-01, PNorm = 81.4398, GNorm = 0.2701
Meta loss on this task batch = 3.1476e-01, Meta loss averaged over last 500 steps = 3.6626e-01, PNorm = 81.4535, GNorm = 0.2363
Meta loss on this task batch = 3.7955e-01, Meta loss averaged over last 500 steps = 3.6617e-01, PNorm = 81.4669, GNorm = 0.2380
Meta loss on this task batch = 3.3814e-01, Meta loss averaged over last 500 steps = 3.6604e-01, PNorm = 81.4806, GNorm = 0.2411
Meta loss on this task batch = 3.2969e-01, Meta loss averaged over last 500 steps = 3.6611e-01, PNorm = 81.4927, GNorm = 0.2802
Meta loss on this task batch = 3.1641e-01, Meta loss averaged over last 500 steps = 3.6601e-01, PNorm = 81.5056, GNorm = 0.2413
Meta loss on this task batch = 3.8667e-01, Meta loss averaged over last 500 steps = 3.6603e-01, PNorm = 81.5178, GNorm = 0.2707
Meta loss on this task batch = 4.5164e-01, Meta loss averaged over last 500 steps = 3.6610e-01, PNorm = 81.5260, GNorm = 0.3668
Meta loss on this task batch = 3.6806e-01, Meta loss averaged over last 500 steps = 3.6615e-01, PNorm = 81.5329, GNorm = 0.2502
Meta loss on this task batch = 3.5301e-01, Meta loss averaged over last 500 steps = 3.6616e-01, PNorm = 81.5383, GNorm = 0.2501
Meta loss on this task batch = 3.8861e-01, Meta loss averaged over last 500 steps = 3.6625e-01, PNorm = 81.5437, GNorm = 0.2926
Meta loss on this task batch = 3.1867e-01, Meta loss averaged over last 500 steps = 3.6611e-01, PNorm = 81.5501, GNorm = 0.2347
Meta loss on this task batch = 3.3518e-01, Meta loss averaged over last 500 steps = 3.6592e-01, PNorm = 81.5582, GNorm = 0.2211
Meta loss on this task batch = 3.7599e-01, Meta loss averaged over last 500 steps = 3.6588e-01, PNorm = 81.5678, GNorm = 0.2823
Meta loss on this task batch = 3.5875e-01, Meta loss averaged over last 500 steps = 3.6584e-01, PNorm = 81.5776, GNorm = 0.2537
Meta loss on this task batch = 3.8006e-01, Meta loss averaged over last 500 steps = 3.6585e-01, PNorm = 81.5882, GNorm = 0.2593
Meta loss on this task batch = 3.0647e-01, Meta loss averaged over last 500 steps = 3.6569e-01, PNorm = 81.6000, GNorm = 0.2402
Meta loss on this task batch = 3.4582e-01, Meta loss averaged over last 500 steps = 3.6569e-01, PNorm = 81.6121, GNorm = 0.2289
Meta loss on this task batch = 4.1748e-01, Meta loss averaged over last 500 steps = 3.6574e-01, PNorm = 81.6233, GNorm = 0.3351
Took 132.36680126190186 seconds to complete one epoch of meta training
Took 140.96178674697876 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474842
Epoch 149
Meta loss on this task batch = 3.6500e-01, Meta loss averaged over last 500 steps = 3.6584e-01, PNorm = 81.6336, GNorm = 0.2928
Meta loss on this task batch = 3.5754e-01, Meta loss averaged over last 500 steps = 3.6569e-01, PNorm = 81.6447, GNorm = 0.2984
Meta loss on this task batch = 3.7873e-01, Meta loss averaged over last 500 steps = 3.6571e-01, PNorm = 81.6543, GNorm = 0.2677
Meta loss on this task batch = 3.6077e-01, Meta loss averaged over last 500 steps = 3.6560e-01, PNorm = 81.6646, GNorm = 0.2513
Meta loss on this task batch = 4.0783e-01, Meta loss averaged over last 500 steps = 3.6572e-01, PNorm = 81.6742, GNorm = 0.2730
Meta loss on this task batch = 3.6403e-01, Meta loss averaged over last 500 steps = 3.6562e-01, PNorm = 81.6846, GNorm = 0.2436
Meta loss on this task batch = 3.2315e-01, Meta loss averaged over last 500 steps = 3.6549e-01, PNorm = 81.6944, GNorm = 0.2512
Meta loss on this task batch = 3.6078e-01, Meta loss averaged over last 500 steps = 3.6556e-01, PNorm = 81.7030, GNorm = 0.2785
Meta loss on this task batch = 3.5532e-01, Meta loss averaged over last 500 steps = 3.6546e-01, PNorm = 81.7120, GNorm = 0.2529
Meta loss on this task batch = 3.6167e-01, Meta loss averaged over last 500 steps = 3.6536e-01, PNorm = 81.7201, GNorm = 0.2496
Meta loss on this task batch = 3.3503e-01, Meta loss averaged over last 500 steps = 3.6538e-01, PNorm = 81.7279, GNorm = 0.2464
Meta loss on this task batch = 3.7315e-01, Meta loss averaged over last 500 steps = 3.6530e-01, PNorm = 81.7360, GNorm = 0.2729
Meta loss on this task batch = 3.2506e-01, Meta loss averaged over last 500 steps = 3.6518e-01, PNorm = 81.7453, GNorm = 0.2596
Meta loss on this task batch = 3.2720e-01, Meta loss averaged over last 500 steps = 3.6512e-01, PNorm = 81.7542, GNorm = 0.2076
Meta loss on this task batch = 4.1117e-01, Meta loss averaged over last 500 steps = 3.6517e-01, PNorm = 81.7622, GNorm = 0.2825
Meta loss on this task batch = 2.8507e-01, Meta loss averaged over last 500 steps = 3.6497e-01, PNorm = 81.7722, GNorm = 0.2541
Meta loss on this task batch = 3.0876e-01, Meta loss averaged over last 500 steps = 3.6501e-01, PNorm = 81.7823, GNorm = 0.2380
Meta loss on this task batch = 3.8506e-01, Meta loss averaged over last 500 steps = 3.6507e-01, PNorm = 81.7911, GNorm = 0.2741
Meta loss on this task batch = 3.6391e-01, Meta loss averaged over last 500 steps = 3.6517e-01, PNorm = 81.7983, GNorm = 0.3119
Took 138.20859098434448 seconds to complete one epoch of meta training
Took 145.7027509212494 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486839
Epoch 150
Meta loss on this task batch = 3.6713e-01, Meta loss averaged over last 500 steps = 3.6505e-01, PNorm = 81.8063, GNorm = 0.2529
Meta loss on this task batch = 3.5893e-01, Meta loss averaged over last 500 steps = 3.6495e-01, PNorm = 81.8131, GNorm = 0.2509
Meta loss on this task batch = 3.8397e-01, Meta loss averaged over last 500 steps = 3.6496e-01, PNorm = 81.8193, GNorm = 0.2461
Meta loss on this task batch = 4.0456e-01, Meta loss averaged over last 500 steps = 3.6503e-01, PNorm = 81.8241, GNorm = 0.2961
Meta loss on this task batch = 2.8099e-01, Meta loss averaged over last 500 steps = 3.6476e-01, PNorm = 81.8308, GNorm = 0.2382
Meta loss on this task batch = 4.0067e-01, Meta loss averaged over last 500 steps = 3.6476e-01, PNorm = 81.8376, GNorm = 0.2545
Meta loss on this task batch = 3.0691e-01, Meta loss averaged over last 500 steps = 3.6463e-01, PNorm = 81.8444, GNorm = 0.2445
Meta loss on this task batch = 4.0439e-01, Meta loss averaged over last 500 steps = 3.6476e-01, PNorm = 81.8499, GNorm = 0.3584
Meta loss on this task batch = 3.8631e-01, Meta loss averaged over last 500 steps = 3.6480e-01, PNorm = 81.8573, GNorm = 0.2669
Meta loss on this task batch = 4.1159e-01, Meta loss averaged over last 500 steps = 3.6482e-01, PNorm = 81.8677, GNorm = 0.2734
Meta loss on this task batch = 3.7782e-01, Meta loss averaged over last 500 steps = 3.6480e-01, PNorm = 81.8810, GNorm = 0.2696
Meta loss on this task batch = 3.2820e-01, Meta loss averaged over last 500 steps = 3.6470e-01, PNorm = 81.8963, GNorm = 0.2654
Meta loss on this task batch = 3.9059e-01, Meta loss averaged over last 500 steps = 3.6479e-01, PNorm = 81.9119, GNorm = 0.2930
Meta loss on this task batch = 4.0297e-01, Meta loss averaged over last 500 steps = 3.6485e-01, PNorm = 81.9282, GNorm = 0.2625
Meta loss on this task batch = 3.0417e-01, Meta loss averaged over last 500 steps = 3.6483e-01, PNorm = 81.9461, GNorm = 0.2409
Meta loss on this task batch = 3.4511e-01, Meta loss averaged over last 500 steps = 3.6468e-01, PNorm = 81.9609, GNorm = 0.2837
Meta loss on this task batch = 3.7163e-01, Meta loss averaged over last 500 steps = 3.6477e-01, PNorm = 81.9717, GNorm = 0.2953
Meta loss on this task batch = 3.5730e-01, Meta loss averaged over last 500 steps = 3.6480e-01, PNorm = 81.9804, GNorm = 0.3078
Meta loss on this task batch = 3.9196e-01, Meta loss averaged over last 500 steps = 3.6471e-01, PNorm = 81.9858, GNorm = 0.3589
Took 140.0884711742401 seconds to complete one epoch of meta training
Took 148.75618433952332 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504699
Epoch 151
Meta loss on this task batch = 3.6202e-01, Meta loss averaged over last 500 steps = 3.6465e-01, PNorm = 81.9900, GNorm = 0.2555
Meta loss on this task batch = 3.8956e-01, Meta loss averaged over last 500 steps = 3.6473e-01, PNorm = 81.9940, GNorm = 0.2592
Meta loss on this task batch = 3.8977e-01, Meta loss averaged over last 500 steps = 3.6473e-01, PNorm = 81.9991, GNorm = 0.2457
Meta loss on this task batch = 3.5927e-01, Meta loss averaged over last 500 steps = 3.6471e-01, PNorm = 82.0036, GNorm = 0.2921
Meta loss on this task batch = 3.1862e-01, Meta loss averaged over last 500 steps = 3.6456e-01, PNorm = 82.0107, GNorm = 0.2443
Meta loss on this task batch = 3.0107e-01, Meta loss averaged over last 500 steps = 3.6435e-01, PNorm = 82.0207, GNorm = 0.2527
Meta loss on this task batch = 3.7507e-01, Meta loss averaged over last 500 steps = 3.6424e-01, PNorm = 82.0313, GNorm = 0.2751
Meta loss on this task batch = 3.2934e-01, Meta loss averaged over last 500 steps = 3.6406e-01, PNorm = 82.0410, GNorm = 0.2577
Meta loss on this task batch = 3.4964e-01, Meta loss averaged over last 500 steps = 3.6406e-01, PNorm = 82.0501, GNorm = 0.2538
Meta loss on this task batch = 3.5359e-01, Meta loss averaged over last 500 steps = 3.6399e-01, PNorm = 82.0588, GNorm = 0.3169
Meta loss on this task batch = 3.5918e-01, Meta loss averaged over last 500 steps = 3.6377e-01, PNorm = 82.0689, GNorm = 0.3691
Meta loss on this task batch = 3.4804e-01, Meta loss averaged over last 500 steps = 3.6384e-01, PNorm = 82.0799, GNorm = 0.2846
Meta loss on this task batch = 3.4451e-01, Meta loss averaged over last 500 steps = 3.6375e-01, PNorm = 82.0926, GNorm = 0.2764
Meta loss on this task batch = 3.7334e-01, Meta loss averaged over last 500 steps = 3.6373e-01, PNorm = 82.1053, GNorm = 0.2831
Meta loss on this task batch = 3.4335e-01, Meta loss averaged over last 500 steps = 3.6370e-01, PNorm = 82.1190, GNorm = 0.2482
Meta loss on this task batch = 4.2033e-01, Meta loss averaged over last 500 steps = 3.6371e-01, PNorm = 82.1338, GNorm = 0.2733
Meta loss on this task batch = 3.5431e-01, Meta loss averaged over last 500 steps = 3.6372e-01, PNorm = 82.1488, GNorm = 0.2457
Meta loss on this task batch = 3.3777e-01, Meta loss averaged over last 500 steps = 3.6368e-01, PNorm = 82.1629, GNorm = 0.2289
Meta loss on this task batch = 3.3167e-01, Meta loss averaged over last 500 steps = 3.6367e-01, PNorm = 82.1781, GNorm = 0.2985
Took 140.12830567359924 seconds to complete one epoch of meta training
Took 148.7838032245636 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447145
Epoch 152
Meta loss on this task batch = 3.1956e-01, Meta loss averaged over last 500 steps = 3.6357e-01, PNorm = 82.1937, GNorm = 0.2134
Meta loss on this task batch = 3.8971e-01, Meta loss averaged over last 500 steps = 3.6372e-01, PNorm = 82.2086, GNorm = 0.2524
Meta loss on this task batch = 3.5451e-01, Meta loss averaged over last 500 steps = 3.6378e-01, PNorm = 82.2216, GNorm = 0.2673
Meta loss on this task batch = 4.4150e-01, Meta loss averaged over last 500 steps = 3.6399e-01, PNorm = 82.2312, GNorm = 0.3526
Meta loss on this task batch = 3.3231e-01, Meta loss averaged over last 500 steps = 3.6388e-01, PNorm = 82.2398, GNorm = 0.2665
Meta loss on this task batch = 3.8994e-01, Meta loss averaged over last 500 steps = 3.6385e-01, PNorm = 82.2461, GNorm = 0.3392
Meta loss on this task batch = 3.7917e-01, Meta loss averaged over last 500 steps = 3.6388e-01, PNorm = 82.2506, GNorm = 0.3153
Meta loss on this task batch = 3.1933e-01, Meta loss averaged over last 500 steps = 3.6384e-01, PNorm = 82.2571, GNorm = 0.2561
Meta loss on this task batch = 3.5273e-01, Meta loss averaged over last 500 steps = 3.6382e-01, PNorm = 82.2647, GNorm = 0.2499
Meta loss on this task batch = 3.8906e-01, Meta loss averaged over last 500 steps = 3.6386e-01, PNorm = 82.2724, GNorm = 0.2820
Meta loss on this task batch = 2.8825e-01, Meta loss averaged over last 500 steps = 3.6360e-01, PNorm = 82.2809, GNorm = 0.2145
Meta loss on this task batch = 4.0965e-01, Meta loss averaged over last 500 steps = 3.6361e-01, PNorm = 82.2880, GNorm = 0.3124
Meta loss on this task batch = 3.7800e-01, Meta loss averaged over last 500 steps = 3.6357e-01, PNorm = 82.2967, GNorm = 0.2697
Meta loss on this task batch = 3.6920e-01, Meta loss averaged over last 500 steps = 3.6353e-01, PNorm = 82.3065, GNorm = 0.2609
Meta loss on this task batch = 3.7745e-01, Meta loss averaged over last 500 steps = 3.6362e-01, PNorm = 82.3160, GNorm = 0.2795
Meta loss on this task batch = 3.2870e-01, Meta loss averaged over last 500 steps = 3.6361e-01, PNorm = 82.3256, GNorm = 0.2633
Meta loss on this task batch = 4.0650e-01, Meta loss averaged over last 500 steps = 3.6373e-01, PNorm = 82.3360, GNorm = 0.2748
Meta loss on this task batch = 2.9117e-01, Meta loss averaged over last 500 steps = 3.6357e-01, PNorm = 82.3475, GNorm = 0.2187
Meta loss on this task batch = 3.6877e-01, Meta loss averaged over last 500 steps = 3.6366e-01, PNorm = 82.3574, GNorm = 0.3064
Took 139.14290380477905 seconds to complete one epoch of meta training
Took 147.61578702926636 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486273
Epoch 153
Meta loss on this task batch = 3.3323e-01, Meta loss averaged over last 500 steps = 3.6348e-01, PNorm = 82.3682, GNorm = 0.2201
Meta loss on this task batch = 3.3050e-01, Meta loss averaged over last 500 steps = 3.6342e-01, PNorm = 82.3798, GNorm = 0.2390
Meta loss on this task batch = 3.4184e-01, Meta loss averaged over last 500 steps = 3.6332e-01, PNorm = 82.3909, GNorm = 0.2258
Meta loss on this task batch = 3.8180e-01, Meta loss averaged over last 500 steps = 3.6334e-01, PNorm = 82.4008, GNorm = 0.2760
Meta loss on this task batch = 3.4680e-01, Meta loss averaged over last 500 steps = 3.6320e-01, PNorm = 82.4119, GNorm = 0.2252
Meta loss on this task batch = 3.4486e-01, Meta loss averaged over last 500 steps = 3.6318e-01, PNorm = 82.4237, GNorm = 0.2524
Meta loss on this task batch = 3.6025e-01, Meta loss averaged over last 500 steps = 3.6321e-01, PNorm = 82.4366, GNorm = 0.2316
Meta loss on this task batch = 3.7555e-01, Meta loss averaged over last 500 steps = 3.6311e-01, PNorm = 82.4489, GNorm = 0.2595
Meta loss on this task batch = 3.3525e-01, Meta loss averaged over last 500 steps = 3.6295e-01, PNorm = 82.4601, GNorm = 0.2616
Meta loss on this task batch = 3.4833e-01, Meta loss averaged over last 500 steps = 3.6290e-01, PNorm = 82.4697, GNorm = 0.2437
Meta loss on this task batch = 4.0858e-01, Meta loss averaged over last 500 steps = 3.6297e-01, PNorm = 82.4790, GNorm = 0.2651
Meta loss on this task batch = 3.2780e-01, Meta loss averaged over last 500 steps = 3.6279e-01, PNorm = 82.4865, GNorm = 0.2415
Meta loss on this task batch = 3.4951e-01, Meta loss averaged over last 500 steps = 3.6282e-01, PNorm = 82.4948, GNorm = 0.2555
Meta loss on this task batch = 3.7387e-01, Meta loss averaged over last 500 steps = 3.6270e-01, PNorm = 82.5033, GNorm = 0.2717
Meta loss on this task batch = 3.6382e-01, Meta loss averaged over last 500 steps = 3.6272e-01, PNorm = 82.5120, GNorm = 0.2892
Meta loss on this task batch = 3.9153e-01, Meta loss averaged over last 500 steps = 3.6273e-01, PNorm = 82.5196, GNorm = 0.2987
Meta loss on this task batch = 3.0726e-01, Meta loss averaged over last 500 steps = 3.6268e-01, PNorm = 82.5272, GNorm = 0.2283
Meta loss on this task batch = 3.7256e-01, Meta loss averaged over last 500 steps = 3.6269e-01, PNorm = 82.5329, GNorm = 0.2790
Meta loss on this task batch = 4.0676e-01, Meta loss averaged over last 500 steps = 3.6282e-01, PNorm = 82.5381, GNorm = 0.3339
Took 136.22727274894714 seconds to complete one epoch of meta training
Took 144.72378396987915 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491867
Epoch 154
Meta loss on this task batch = 3.1280e-01, Meta loss averaged over last 500 steps = 3.6274e-01, PNorm = 82.5449, GNorm = 0.2492
Meta loss on this task batch = 3.8080e-01, Meta loss averaged over last 500 steps = 3.6269e-01, PNorm = 82.5530, GNorm = 0.2619
Meta loss on this task batch = 3.8249e-01, Meta loss averaged over last 500 steps = 3.6268e-01, PNorm = 82.5600, GNorm = 0.2774
Meta loss on this task batch = 3.5724e-01, Meta loss averaged over last 500 steps = 3.6287e-01, PNorm = 82.5665, GNorm = 0.2778
Meta loss on this task batch = 3.5768e-01, Meta loss averaged over last 500 steps = 3.6287e-01, PNorm = 82.5751, GNorm = 0.2306
Meta loss on this task batch = 3.7003e-01, Meta loss averaged over last 500 steps = 3.6293e-01, PNorm = 82.5858, GNorm = 0.2803
Meta loss on this task batch = 3.3199e-01, Meta loss averaged over last 500 steps = 3.6278e-01, PNorm = 82.5968, GNorm = 0.2388
Meta loss on this task batch = 3.9621e-01, Meta loss averaged over last 500 steps = 3.6292e-01, PNorm = 82.6060, GNorm = 0.2713
Meta loss on this task batch = 4.1356e-01, Meta loss averaged over last 500 steps = 3.6302e-01, PNorm = 82.6143, GNorm = 0.2638
Meta loss on this task batch = 3.4931e-01, Meta loss averaged over last 500 steps = 3.6294e-01, PNorm = 82.6229, GNorm = 0.2482
Meta loss on this task batch = 3.1505e-01, Meta loss averaged over last 500 steps = 3.6287e-01, PNorm = 82.6325, GNorm = 0.2146
Meta loss on this task batch = 3.9443e-01, Meta loss averaged over last 500 steps = 3.6301e-01, PNorm = 82.6401, GNorm = 0.2681
Meta loss on this task batch = 3.1434e-01, Meta loss averaged over last 500 steps = 3.6291e-01, PNorm = 82.6500, GNorm = 0.2428
Meta loss on this task batch = 3.3347e-01, Meta loss averaged over last 500 steps = 3.6273e-01, PNorm = 82.6605, GNorm = 0.2403
Meta loss on this task batch = 2.5641e-01, Meta loss averaged over last 500 steps = 3.6244e-01, PNorm = 82.6732, GNorm = 0.2178
Meta loss on this task batch = 4.1356e-01, Meta loss averaged over last 500 steps = 3.6254e-01, PNorm = 82.6840, GNorm = 0.2788
Meta loss on this task batch = 4.0229e-01, Meta loss averaged over last 500 steps = 3.6262e-01, PNorm = 82.6933, GNorm = 0.3242
Meta loss on this task batch = 4.0473e-01, Meta loss averaged over last 500 steps = 3.6267e-01, PNorm = 82.7005, GNorm = 0.3300
Meta loss on this task batch = 3.7723e-01, Meta loss averaged over last 500 steps = 3.6266e-01, PNorm = 82.7059, GNorm = 0.2946
Took 130.67976236343384 seconds to complete one epoch of meta training
Took 139.05658388137817 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481237
Epoch 155
Meta loss on this task batch = 3.9061e-01, Meta loss averaged over last 500 steps = 3.6269e-01, PNorm = 82.7104, GNorm = 0.2693
Meta loss on this task batch = 3.2844e-01, Meta loss averaged over last 500 steps = 3.6263e-01, PNorm = 82.7169, GNorm = 0.2341
Meta loss on this task batch = 3.5856e-01, Meta loss averaged over last 500 steps = 3.6269e-01, PNorm = 82.7255, GNorm = 0.2704
Meta loss on this task batch = 3.7453e-01, Meta loss averaged over last 500 steps = 3.6265e-01, PNorm = 82.7351, GNorm = 0.2797
Meta loss on this task batch = 3.2229e-01, Meta loss averaged over last 500 steps = 3.6253e-01, PNorm = 82.7466, GNorm = 0.2508
Meta loss on this task batch = 3.4860e-01, Meta loss averaged over last 500 steps = 3.6251e-01, PNorm = 82.7588, GNorm = 0.2930
Meta loss on this task batch = 3.4114e-01, Meta loss averaged over last 500 steps = 3.6240e-01, PNorm = 82.7707, GNorm = 0.2600
Meta loss on this task batch = 3.8001e-01, Meta loss averaged over last 500 steps = 3.6245e-01, PNorm = 82.7830, GNorm = 0.2873
Meta loss on this task batch = 3.2462e-01, Meta loss averaged over last 500 steps = 3.6233e-01, PNorm = 82.7950, GNorm = 0.2565
Meta loss on this task batch = 3.0452e-01, Meta loss averaged over last 500 steps = 3.6227e-01, PNorm = 82.8069, GNorm = 0.2404
Meta loss on this task batch = 3.4494e-01, Meta loss averaged over last 500 steps = 3.6232e-01, PNorm = 82.8162, GNorm = 0.2655
Meta loss on this task batch = 3.7539e-01, Meta loss averaged over last 500 steps = 3.6232e-01, PNorm = 82.8236, GNorm = 0.2608
Meta loss on this task batch = 3.3213e-01, Meta loss averaged over last 500 steps = 3.6226e-01, PNorm = 82.8293, GNorm = 0.2555
Meta loss on this task batch = 4.0879e-01, Meta loss averaged over last 500 steps = 3.6231e-01, PNorm = 82.8351, GNorm = 0.2439
Meta loss on this task batch = 3.8781e-01, Meta loss averaged over last 500 steps = 3.6232e-01, PNorm = 82.8413, GNorm = 0.2397
Meta loss on this task batch = 3.8478e-01, Meta loss averaged over last 500 steps = 3.6235e-01, PNorm = 82.8453, GNorm = 0.2576
Meta loss on this task batch = 3.5633e-01, Meta loss averaged over last 500 steps = 3.6232e-01, PNorm = 82.8498, GNorm = 0.2771
Meta loss on this task batch = 3.4648e-01, Meta loss averaged over last 500 steps = 3.6242e-01, PNorm = 82.8556, GNorm = 0.2311
Meta loss on this task batch = 3.1702e-01, Meta loss averaged over last 500 steps = 3.6240e-01, PNorm = 82.8619, GNorm = 0.2615
Took 139.5553638935089 seconds to complete one epoch of meta training
Took 147.60322546958923 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490869
Epoch 156
Meta loss on this task batch = 3.9820e-01, Meta loss averaged over last 500 steps = 3.6250e-01, PNorm = 82.8682, GNorm = 0.2599
Meta loss on this task batch = 3.4538e-01, Meta loss averaged over last 500 steps = 3.6247e-01, PNorm = 82.8770, GNorm = 0.2560
Meta loss on this task batch = 2.9657e-01, Meta loss averaged over last 500 steps = 3.6235e-01, PNorm = 82.8870, GNorm = 0.2383
Meta loss on this task batch = 3.0872e-01, Meta loss averaged over last 500 steps = 3.6214e-01, PNorm = 82.8986, GNorm = 0.2368
Meta loss on this task batch = 3.3786e-01, Meta loss averaged over last 500 steps = 3.6207e-01, PNorm = 82.9103, GNorm = 0.2359
Meta loss on this task batch = 3.9692e-01, Meta loss averaged over last 500 steps = 3.6228e-01, PNorm = 82.9187, GNorm = 0.3249
Meta loss on this task batch = 3.5824e-01, Meta loss averaged over last 500 steps = 3.6220e-01, PNorm = 82.9274, GNorm = 0.3620
Meta loss on this task batch = 3.6909e-01, Meta loss averaged over last 500 steps = 3.6208e-01, PNorm = 82.9357, GNorm = 0.2825
Meta loss on this task batch = 3.5721e-01, Meta loss averaged over last 500 steps = 3.6211e-01, PNorm = 82.9429, GNorm = 0.3085
Meta loss on this task batch = 3.5600e-01, Meta loss averaged over last 500 steps = 3.6221e-01, PNorm = 82.9506, GNorm = 0.3004
Meta loss on this task batch = 3.8004e-01, Meta loss averaged over last 500 steps = 3.6227e-01, PNorm = 82.9584, GNorm = 0.2698
Meta loss on this task batch = 4.1174e-01, Meta loss averaged over last 500 steps = 3.6241e-01, PNorm = 82.9667, GNorm = 0.2741
Meta loss on this task batch = 3.1795e-01, Meta loss averaged over last 500 steps = 3.6237e-01, PNorm = 82.9758, GNorm = 0.2341
Meta loss on this task batch = 4.0400e-01, Meta loss averaged over last 500 steps = 3.6251e-01, PNorm = 82.9847, GNorm = 0.2376
Meta loss on this task batch = 2.9945e-01, Meta loss averaged over last 500 steps = 3.6233e-01, PNorm = 82.9953, GNorm = 0.2214
Meta loss on this task batch = 3.2928e-01, Meta loss averaged over last 500 steps = 3.6225e-01, PNorm = 83.0061, GNorm = 0.2289
Meta loss on this task batch = 3.9860e-01, Meta loss averaged over last 500 steps = 3.6234e-01, PNorm = 83.0179, GNorm = 0.2757
Meta loss on this task batch = 3.7742e-01, Meta loss averaged over last 500 steps = 3.6239e-01, PNorm = 83.0285, GNorm = 0.2639
Meta loss on this task batch = 3.5399e-01, Meta loss averaged over last 500 steps = 3.6225e-01, PNorm = 83.0392, GNorm = 0.2964
Took 132.45398592948914 seconds to complete one epoch of meta training
Took 140.6498258113861 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502753
Epoch 157
Meta loss on this task batch = 3.3210e-01, Meta loss averaged over last 500 steps = 3.6209e-01, PNorm = 83.0492, GNorm = 0.2827
Meta loss on this task batch = 3.0508e-01, Meta loss averaged over last 500 steps = 3.6204e-01, PNorm = 83.0571, GNorm = 0.2566
Meta loss on this task batch = 4.3862e-01, Meta loss averaged over last 500 steps = 3.6216e-01, PNorm = 83.0609, GNorm = 0.3270
Meta loss on this task batch = 3.6516e-01, Meta loss averaged over last 500 steps = 3.6219e-01, PNorm = 83.0619, GNorm = 0.3164
Meta loss on this task batch = 4.1115e-01, Meta loss averaged over last 500 steps = 3.6219e-01, PNorm = 83.0596, GNorm = 0.3565
Meta loss on this task batch = 3.5041e-01, Meta loss averaged over last 500 steps = 3.6220e-01, PNorm = 83.0586, GNorm = 0.2801
Meta loss on this task batch = 4.1067e-01, Meta loss averaged over last 500 steps = 3.6226e-01, PNorm = 83.0562, GNorm = 0.3199
Meta loss on this task batch = 3.4357e-01, Meta loss averaged over last 500 steps = 3.6226e-01, PNorm = 83.0557, GNorm = 0.2322
Meta loss on this task batch = 3.0845e-01, Meta loss averaged over last 500 steps = 3.6216e-01, PNorm = 83.0588, GNorm = 0.2585
Meta loss on this task batch = 3.3476e-01, Meta loss averaged over last 500 steps = 3.6207e-01, PNorm = 83.0637, GNorm = 0.2251
Meta loss on this task batch = 3.8639e-01, Meta loss averaged over last 500 steps = 3.6205e-01, PNorm = 83.0695, GNorm = 0.2416
Meta loss on this task batch = 3.1966e-01, Meta loss averaged over last 500 steps = 3.6195e-01, PNorm = 83.0777, GNorm = 0.2202
Meta loss on this task batch = 3.1793e-01, Meta loss averaged over last 500 steps = 3.6190e-01, PNorm = 83.0866, GNorm = 0.2128
Meta loss on this task batch = 3.3656e-01, Meta loss averaged over last 500 steps = 3.6195e-01, PNorm = 83.0967, GNorm = 0.2137
Meta loss on this task batch = 3.8325e-01, Meta loss averaged over last 500 steps = 3.6199e-01, PNorm = 83.1077, GNorm = 0.2347
Meta loss on this task batch = 3.9332e-01, Meta loss averaged over last 500 steps = 3.6204e-01, PNorm = 83.1178, GNorm = 0.2957
Meta loss on this task batch = 3.2348e-01, Meta loss averaged over last 500 steps = 3.6192e-01, PNorm = 83.1278, GNorm = 0.2872
Meta loss on this task batch = 3.4765e-01, Meta loss averaged over last 500 steps = 3.6194e-01, PNorm = 83.1384, GNorm = 0.2446
Meta loss on this task batch = 3.5480e-01, Meta loss averaged over last 500 steps = 3.6196e-01, PNorm = 83.1480, GNorm = 0.2899
Took 135.25084042549133 seconds to complete one epoch of meta training
Took 144.2774064540863 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501975
Epoch 158
Meta loss on this task batch = 3.9387e-01, Meta loss averaged over last 500 steps = 3.6195e-01, PNorm = 83.1581, GNorm = 0.2563
Meta loss on this task batch = 3.7952e-01, Meta loss averaged over last 500 steps = 3.6183e-01, PNorm = 83.1697, GNorm = 0.2759
Meta loss on this task batch = 3.5044e-01, Meta loss averaged over last 500 steps = 3.6184e-01, PNorm = 83.1820, GNorm = 0.2825
Meta loss on this task batch = 3.2578e-01, Meta loss averaged over last 500 steps = 3.6176e-01, PNorm = 83.1952, GNorm = 0.2562
Meta loss on this task batch = 3.3405e-01, Meta loss averaged over last 500 steps = 3.6158e-01, PNorm = 83.2069, GNorm = 0.2456
Meta loss on this task batch = 3.1918e-01, Meta loss averaged over last 500 steps = 3.6144e-01, PNorm = 83.2172, GNorm = 0.2554
Meta loss on this task batch = 3.4212e-01, Meta loss averaged over last 500 steps = 3.6148e-01, PNorm = 83.2261, GNorm = 0.2680
Meta loss on this task batch = 3.2884e-01, Meta loss averaged over last 500 steps = 3.6139e-01, PNorm = 83.2332, GNorm = 0.2826
Meta loss on this task batch = 4.3379e-01, Meta loss averaged over last 500 steps = 3.6154e-01, PNorm = 83.2393, GNorm = 0.3209
Meta loss on this task batch = 3.6112e-01, Meta loss averaged over last 500 steps = 3.6156e-01, PNorm = 83.2457, GNorm = 0.2750
Meta loss on this task batch = 3.5711e-01, Meta loss averaged over last 500 steps = 3.6154e-01, PNorm = 83.2502, GNorm = 0.2954
Meta loss on this task batch = 3.3528e-01, Meta loss averaged over last 500 steps = 3.6136e-01, PNorm = 83.2547, GNorm = 0.2641
Meta loss on this task batch = 3.7534e-01, Meta loss averaged over last 500 steps = 3.6143e-01, PNorm = 83.2608, GNorm = 0.3235
Meta loss on this task batch = 3.2861e-01, Meta loss averaged over last 500 steps = 3.6130e-01, PNorm = 83.2701, GNorm = 0.2680
Meta loss on this task batch = 3.7787e-01, Meta loss averaged over last 500 steps = 3.6125e-01, PNorm = 83.2814, GNorm = 0.2804
Meta loss on this task batch = 3.6437e-01, Meta loss averaged over last 500 steps = 3.6134e-01, PNorm = 83.2933, GNorm = 0.2390
Meta loss on this task batch = 3.8495e-01, Meta loss averaged over last 500 steps = 3.6147e-01, PNorm = 83.3066, GNorm = 0.2406
Meta loss on this task batch = 3.5653e-01, Meta loss averaged over last 500 steps = 3.6145e-01, PNorm = 83.3218, GNorm = 0.2472
Meta loss on this task batch = 3.5210e-01, Meta loss averaged over last 500 steps = 3.6137e-01, PNorm = 83.3373, GNorm = 0.2595
Took 139.47334814071655 seconds to complete one epoch of meta training
Took 148.0413520336151 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500014
Epoch 159
Meta loss on this task batch = 3.1132e-01, Meta loss averaged over last 500 steps = 3.6125e-01, PNorm = 83.3526, GNorm = 0.2747
Meta loss on this task batch = 3.3185e-01, Meta loss averaged over last 500 steps = 3.6119e-01, PNorm = 83.3680, GNorm = 0.2290
Meta loss on this task batch = 3.5249e-01, Meta loss averaged over last 500 steps = 3.6120e-01, PNorm = 83.3810, GNorm = 0.2247
Meta loss on this task batch = 2.8617e-01, Meta loss averaged over last 500 steps = 3.6094e-01, PNorm = 83.3932, GNorm = 0.1994
Meta loss on this task batch = 3.4518e-01, Meta loss averaged over last 500 steps = 3.6088e-01, PNorm = 83.4055, GNorm = 0.2746
Meta loss on this task batch = 3.9973e-01, Meta loss averaged over last 500 steps = 3.6094e-01, PNorm = 83.4150, GNorm = 0.2942
Meta loss on this task batch = 3.2577e-01, Meta loss averaged over last 500 steps = 3.6080e-01, PNorm = 83.4227, GNorm = 0.2593
Meta loss on this task batch = 3.6212e-01, Meta loss averaged over last 500 steps = 3.6063e-01, PNorm = 83.4290, GNorm = 0.3006
Meta loss on this task batch = 3.2514e-01, Meta loss averaged over last 500 steps = 3.6059e-01, PNorm = 83.4355, GNorm = 0.2414
Meta loss on this task batch = 3.3347e-01, Meta loss averaged over last 500 steps = 3.6047e-01, PNorm = 83.4429, GNorm = 0.2601
Meta loss on this task batch = 3.6916e-01, Meta loss averaged over last 500 steps = 3.6054e-01, PNorm = 83.4499, GNorm = 0.3168
Meta loss on this task batch = 3.7277e-01, Meta loss averaged over last 500 steps = 3.6044e-01, PNorm = 83.4576, GNorm = 0.2461
Meta loss on this task batch = 3.8425e-01, Meta loss averaged over last 500 steps = 3.6050e-01, PNorm = 83.4671, GNorm = 0.2706
Meta loss on this task batch = 3.2338e-01, Meta loss averaged over last 500 steps = 3.6040e-01, PNorm = 83.4785, GNorm = 0.2548
Meta loss on this task batch = 3.7149e-01, Meta loss averaged over last 500 steps = 3.6047e-01, PNorm = 83.4901, GNorm = 0.2725
Meta loss on this task batch = 3.2370e-01, Meta loss averaged over last 500 steps = 3.6050e-01, PNorm = 83.5042, GNorm = 0.2559
Meta loss on this task batch = 3.1815e-01, Meta loss averaged over last 500 steps = 3.6047e-01, PNorm = 83.5202, GNorm = 0.2517
Meta loss on this task batch = 3.5584e-01, Meta loss averaged over last 500 steps = 3.6048e-01, PNorm = 83.5345, GNorm = 0.2890
Meta loss on this task batch = 3.3011e-01, Meta loss averaged over last 500 steps = 3.6037e-01, PNorm = 83.5445, GNorm = 0.3283
Took 132.35196566581726 seconds to complete one epoch of meta training
Took 140.94550037384033 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.519844
Epoch 160
Meta loss on this task batch = 3.7650e-01, Meta loss averaged over last 500 steps = 3.6054e-01, PNorm = 83.5528, GNorm = 0.3146
Meta loss on this task batch = 3.3393e-01, Meta loss averaged over last 500 steps = 3.6050e-01, PNorm = 83.5604, GNorm = 0.2691
Meta loss on this task batch = 3.6731e-01, Meta loss averaged over last 500 steps = 3.6053e-01, PNorm = 83.5630, GNorm = 0.3372
Meta loss on this task batch = 3.7210e-01, Meta loss averaged over last 500 steps = 3.6053e-01, PNorm = 83.5656, GNorm = 0.3201
Meta loss on this task batch = 3.3709e-01, Meta loss averaged over last 500 steps = 3.6048e-01, PNorm = 83.5682, GNorm = 0.2833
Meta loss on this task batch = 3.2651e-01, Meta loss averaged over last 500 steps = 3.6038e-01, PNorm = 83.5719, GNorm = 0.2387
Meta loss on this task batch = 3.0051e-01, Meta loss averaged over last 500 steps = 3.6019e-01, PNorm = 83.5790, GNorm = 0.2685
Meta loss on this task batch = 3.3960e-01, Meta loss averaged over last 500 steps = 3.6027e-01, PNorm = 83.5875, GNorm = 0.2423
Meta loss on this task batch = 3.1382e-01, Meta loss averaged over last 500 steps = 3.6013e-01, PNorm = 83.5990, GNorm = 0.3002
Meta loss on this task batch = 3.8082e-01, Meta loss averaged over last 500 steps = 3.6003e-01, PNorm = 83.6104, GNorm = 0.2370
Meta loss on this task batch = 4.2495e-01, Meta loss averaged over last 500 steps = 3.6025e-01, PNorm = 83.6220, GNorm = 0.2859
Meta loss on this task batch = 3.4471e-01, Meta loss averaged over last 500 steps = 3.6024e-01, PNorm = 83.6351, GNorm = 0.2339
Meta loss on this task batch = 3.5159e-01, Meta loss averaged over last 500 steps = 3.6031e-01, PNorm = 83.6486, GNorm = 0.2798
Meta loss on this task batch = 3.7676e-01, Meta loss averaged over last 500 steps = 3.6027e-01, PNorm = 83.6605, GNorm = 0.3050
Meta loss on this task batch = 3.5420e-01, Meta loss averaged over last 500 steps = 3.6023e-01, PNorm = 83.6709, GNorm = 0.2662
Meta loss on this task batch = 3.6451e-01, Meta loss averaged over last 500 steps = 3.6031e-01, PNorm = 83.6809, GNorm = 0.2988
Meta loss on this task batch = 3.9977e-01, Meta loss averaged over last 500 steps = 3.6042e-01, PNorm = 83.6889, GNorm = 0.2793
Meta loss on this task batch = 3.2296e-01, Meta loss averaged over last 500 steps = 3.6031e-01, PNorm = 83.6972, GNorm = 0.2417
Meta loss on this task batch = 3.4812e-01, Meta loss averaged over last 500 steps = 3.6028e-01, PNorm = 83.7064, GNorm = 0.2990
Took 134.4104323387146 seconds to complete one epoch of meta training
Took 143.04666185379028 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505548
Epoch 161
Meta loss on this task batch = 3.1866e-01, Meta loss averaged over last 500 steps = 3.6010e-01, PNorm = 83.7157, GNorm = 0.2857
Meta loss on this task batch = 3.4567e-01, Meta loss averaged over last 500 steps = 3.6008e-01, PNorm = 83.7264, GNorm = 0.2177
Meta loss on this task batch = 3.5197e-01, Meta loss averaged over last 500 steps = 3.6001e-01, PNorm = 83.7379, GNorm = 0.2449
Meta loss on this task batch = 3.4598e-01, Meta loss averaged over last 500 steps = 3.5982e-01, PNorm = 83.7509, GNorm = 0.2394
Meta loss on this task batch = 2.8213e-01, Meta loss averaged over last 500 steps = 3.5962e-01, PNorm = 83.7638, GNorm = 0.2177
Meta loss on this task batch = 3.6671e-01, Meta loss averaged over last 500 steps = 3.5968e-01, PNorm = 83.7757, GNorm = 0.3003
Meta loss on this task batch = 3.6845e-01, Meta loss averaged over last 500 steps = 3.5967e-01, PNorm = 83.7884, GNorm = 0.3185
Meta loss on this task batch = 3.5210e-01, Meta loss averaged over last 500 steps = 3.5966e-01, PNorm = 83.7984, GNorm = 0.3116
Meta loss on this task batch = 3.8548e-01, Meta loss averaged over last 500 steps = 3.5954e-01, PNorm = 83.8071, GNorm = 0.2859
Meta loss on this task batch = 3.2582e-01, Meta loss averaged over last 500 steps = 3.5942e-01, PNorm = 83.8144, GNorm = 0.2885
Meta loss on this task batch = 3.6510e-01, Meta loss averaged over last 500 steps = 3.5948e-01, PNorm = 83.8206, GNorm = 0.2729
Meta loss on this task batch = 3.3519e-01, Meta loss averaged over last 500 steps = 3.5931e-01, PNorm = 83.8269, GNorm = 0.2421
Meta loss on this task batch = 3.5911e-01, Meta loss averaged over last 500 steps = 3.5921e-01, PNorm = 83.8347, GNorm = 0.2654
Meta loss on this task batch = 3.7745e-01, Meta loss averaged over last 500 steps = 3.5932e-01, PNorm = 83.8451, GNorm = 0.2772
Meta loss on this task batch = 3.2876e-01, Meta loss averaged over last 500 steps = 3.5939e-01, PNorm = 83.8563, GNorm = 0.2621
Meta loss on this task batch = 3.8369e-01, Meta loss averaged over last 500 steps = 3.5947e-01, PNorm = 83.8668, GNorm = 0.2721
Meta loss on this task batch = 3.4894e-01, Meta loss averaged over last 500 steps = 3.5948e-01, PNorm = 83.8761, GNorm = 0.2871
Meta loss on this task batch = 4.4272e-01, Meta loss averaged over last 500 steps = 3.5977e-01, PNorm = 83.8837, GNorm = 0.3111
Meta loss on this task batch = 3.5925e-01, Meta loss averaged over last 500 steps = 3.5977e-01, PNorm = 83.8902, GNorm = 0.3228
Took 138.4034948348999 seconds to complete one epoch of meta training
Took 147.17375898361206 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502760
Epoch 162
Meta loss on this task batch = 3.9139e-01, Meta loss averaged over last 500 steps = 3.5978e-01, PNorm = 83.8969, GNorm = 0.2665
Meta loss on this task batch = 3.4581e-01, Meta loss averaged over last 500 steps = 3.5975e-01, PNorm = 83.9047, GNorm = 0.2476
Meta loss on this task batch = 3.3928e-01, Meta loss averaged over last 500 steps = 3.5977e-01, PNorm = 83.9134, GNorm = 0.2285
Meta loss on this task batch = 3.9358e-01, Meta loss averaged over last 500 steps = 3.5982e-01, PNorm = 83.9214, GNorm = 0.2609
Meta loss on this task batch = 2.9502e-01, Meta loss averaged over last 500 steps = 3.5967e-01, PNorm = 83.9309, GNorm = 0.2437
Meta loss on this task batch = 3.9574e-01, Meta loss averaged over last 500 steps = 3.5979e-01, PNorm = 83.9404, GNorm = 0.3920
Meta loss on this task batch = 3.8748e-01, Meta loss averaged over last 500 steps = 3.5979e-01, PNorm = 83.9514, GNorm = 0.2746
Meta loss on this task batch = 3.5995e-01, Meta loss averaged over last 500 steps = 3.5990e-01, PNorm = 83.9605, GNorm = 0.2693
Meta loss on this task batch = 3.2364e-01, Meta loss averaged over last 500 steps = 3.5970e-01, PNorm = 83.9705, GNorm = 0.2708
Meta loss on this task batch = 3.5368e-01, Meta loss averaged over last 500 steps = 3.5973e-01, PNorm = 83.9800, GNorm = 0.2478
Meta loss on this task batch = 3.8086e-01, Meta loss averaged over last 500 steps = 3.5974e-01, PNorm = 83.9898, GNorm = 0.2676
Meta loss on this task batch = 2.9322e-01, Meta loss averaged over last 500 steps = 3.5965e-01, PNorm = 84.0003, GNorm = 0.2433
Meta loss on this task batch = 3.6708e-01, Meta loss averaged over last 500 steps = 3.5974e-01, PNorm = 84.0104, GNorm = 0.2670
Meta loss on this task batch = 3.2630e-01, Meta loss averaged over last 500 steps = 3.5971e-01, PNorm = 84.0183, GNorm = 0.2452
Meta loss on this task batch = 3.2432e-01, Meta loss averaged over last 500 steps = 3.5953e-01, PNorm = 84.0267, GNorm = 0.2541
Meta loss on this task batch = 3.5501e-01, Meta loss averaged over last 500 steps = 3.5948e-01, PNorm = 84.0359, GNorm = 0.2461
Meta loss on this task batch = 3.4055e-01, Meta loss averaged over last 500 steps = 3.5938e-01, PNorm = 84.0438, GNorm = 0.2551
Meta loss on this task batch = 3.7565e-01, Meta loss averaged over last 500 steps = 3.5941e-01, PNorm = 84.0513, GNorm = 0.2766
Meta loss on this task batch = 3.7100e-01, Meta loss averaged over last 500 steps = 3.5940e-01, PNorm = 84.0586, GNorm = 0.3327
Took 136.87905597686768 seconds to complete one epoch of meta training
Took 145.65206718444824 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477106
Epoch 163
Meta loss on this task batch = 2.8843e-01, Meta loss averaged over last 500 steps = 3.5931e-01, PNorm = 84.0671, GNorm = 0.2172
Meta loss on this task batch = 3.1370e-01, Meta loss averaged over last 500 steps = 3.5925e-01, PNorm = 84.0775, GNorm = 0.2250
Meta loss on this task batch = 3.6975e-01, Meta loss averaged over last 500 steps = 3.5929e-01, PNorm = 84.0877, GNorm = 0.2766
Meta loss on this task batch = 4.0218e-01, Meta loss averaged over last 500 steps = 3.5941e-01, PNorm = 84.0969, GNorm = 0.2889
Meta loss on this task batch = 3.4428e-01, Meta loss averaged over last 500 steps = 3.5937e-01, PNorm = 84.1059, GNorm = 0.2567
Meta loss on this task batch = 3.4544e-01, Meta loss averaged over last 500 steps = 3.5928e-01, PNorm = 84.1167, GNorm = 0.2596
Meta loss on this task batch = 3.9029e-01, Meta loss averaged over last 500 steps = 3.5935e-01, PNorm = 84.1238, GNorm = 0.3292
Meta loss on this task batch = 2.8147e-01, Meta loss averaged over last 500 steps = 3.5911e-01, PNorm = 84.1308, GNorm = 0.2172
Meta loss on this task batch = 3.9075e-01, Meta loss averaged over last 500 steps = 3.5924e-01, PNorm = 84.1361, GNorm = 0.3156
Meta loss on this task batch = 2.9872e-01, Meta loss averaged over last 500 steps = 3.5904e-01, PNorm = 84.1424, GNorm = 0.2534
Meta loss on this task batch = 3.2407e-01, Meta loss averaged over last 500 steps = 3.5905e-01, PNorm = 84.1498, GNorm = 0.2467
Meta loss on this task batch = 3.6267e-01, Meta loss averaged over last 500 steps = 3.5901e-01, PNorm = 84.1570, GNorm = 0.2696
Meta loss on this task batch = 3.0510e-01, Meta loss averaged over last 500 steps = 3.5893e-01, PNorm = 84.1643, GNorm = 0.2493
Meta loss on this task batch = 3.4608e-01, Meta loss averaged over last 500 steps = 3.5889e-01, PNorm = 84.1707, GNorm = 0.2575
Meta loss on this task batch = 3.5709e-01, Meta loss averaged over last 500 steps = 3.5883e-01, PNorm = 84.1758, GNorm = 0.2434
Meta loss on this task batch = 3.7276e-01, Meta loss averaged over last 500 steps = 3.5890e-01, PNorm = 84.1816, GNorm = 0.2713
Meta loss on this task batch = 4.1047e-01, Meta loss averaged over last 500 steps = 3.5907e-01, PNorm = 84.1856, GNorm = 0.2798
Meta loss on this task batch = 3.6190e-01, Meta loss averaged over last 500 steps = 3.5915e-01, PNorm = 84.1906, GNorm = 0.2472
Meta loss on this task batch = 3.9350e-01, Meta loss averaged over last 500 steps = 3.5921e-01, PNorm = 84.1924, GNorm = 0.3604
Took 130.3955044746399 seconds to complete one epoch of meta training
Took 138.72209405899048 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495454
Epoch 164
Meta loss on this task batch = 3.9023e-01, Meta loss averaged over last 500 steps = 3.5913e-01, PNorm = 84.1933, GNorm = 0.2791
Meta loss on this task batch = 3.4482e-01, Meta loss averaged over last 500 steps = 3.5905e-01, PNorm = 84.1944, GNorm = 0.2495
Meta loss on this task batch = 2.9721e-01, Meta loss averaged over last 500 steps = 3.5891e-01, PNorm = 84.1974, GNorm = 0.2355
Meta loss on this task batch = 3.2953e-01, Meta loss averaged over last 500 steps = 3.5880e-01, PNorm = 84.2020, GNorm = 0.2227
Meta loss on this task batch = 3.4998e-01, Meta loss averaged over last 500 steps = 3.5877e-01, PNorm = 84.2097, GNorm = 0.2677
Meta loss on this task batch = 3.3890e-01, Meta loss averaged over last 500 steps = 3.5867e-01, PNorm = 84.2178, GNorm = 0.2430
Meta loss on this task batch = 2.9650e-01, Meta loss averaged over last 500 steps = 3.5847e-01, PNorm = 84.2275, GNorm = 0.2319
Meta loss on this task batch = 3.5195e-01, Meta loss averaged over last 500 steps = 3.5843e-01, PNorm = 84.2363, GNorm = 0.2942
Meta loss on this task batch = 4.0090e-01, Meta loss averaged over last 500 steps = 3.5843e-01, PNorm = 84.2437, GNorm = 0.2920
Meta loss on this task batch = 4.0751e-01, Meta loss averaged over last 500 steps = 3.5844e-01, PNorm = 84.2499, GNorm = 0.3095
Meta loss on this task batch = 3.3576e-01, Meta loss averaged over last 500 steps = 3.5834e-01, PNorm = 84.2583, GNorm = 0.2584
Meta loss on this task batch = 3.8255e-01, Meta loss averaged over last 500 steps = 3.5840e-01, PNorm = 84.2664, GNorm = 0.2564
Meta loss on this task batch = 3.1788e-01, Meta loss averaged over last 500 steps = 3.5824e-01, PNorm = 84.2750, GNorm = 0.2271
Meta loss on this task batch = 3.3380e-01, Meta loss averaged over last 500 steps = 3.5822e-01, PNorm = 84.2834, GNorm = 0.2726
Meta loss on this task batch = 3.5672e-01, Meta loss averaged over last 500 steps = 3.5827e-01, PNorm = 84.2926, GNorm = 0.2422
Meta loss on this task batch = 3.4706e-01, Meta loss averaged over last 500 steps = 3.5824e-01, PNorm = 84.3024, GNorm = 0.2845
Meta loss on this task batch = 4.1370e-01, Meta loss averaged over last 500 steps = 3.5835e-01, PNorm = 84.3111, GNorm = 0.3399
Meta loss on this task batch = 3.3420e-01, Meta loss averaged over last 500 steps = 3.5831e-01, PNorm = 84.3209, GNorm = 0.2642
Meta loss on this task batch = 4.0249e-01, Meta loss averaged over last 500 steps = 3.5835e-01, PNorm = 84.3301, GNorm = 0.3285
Took 135.12195539474487 seconds to complete one epoch of meta training
Took 143.48436522483826 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473756
Epoch 165
Meta loss on this task batch = 3.0593e-01, Meta loss averaged over last 500 steps = 3.5826e-01, PNorm = 84.3399, GNorm = 0.2691
Meta loss on this task batch = 3.9947e-01, Meta loss averaged over last 500 steps = 3.5845e-01, PNorm = 84.3493, GNorm = 0.2868
Meta loss on this task batch = 3.6018e-01, Meta loss averaged over last 500 steps = 3.5842e-01, PNorm = 84.3596, GNorm = 0.2635
Meta loss on this task batch = 4.0400e-01, Meta loss averaged over last 500 steps = 3.5849e-01, PNorm = 84.3664, GNorm = 0.3217
Meta loss on this task batch = 3.4610e-01, Meta loss averaged over last 500 steps = 3.5847e-01, PNorm = 84.3709, GNorm = 0.2723
Meta loss on this task batch = 3.6729e-01, Meta loss averaged over last 500 steps = 3.5853e-01, PNorm = 84.3762, GNorm = 0.2424
Meta loss on this task batch = 3.2886e-01, Meta loss averaged over last 500 steps = 3.5850e-01, PNorm = 84.3823, GNorm = 0.2291
Meta loss on this task batch = 3.3116e-01, Meta loss averaged over last 500 steps = 3.5848e-01, PNorm = 84.3882, GNorm = 0.2550
Meta loss on this task batch = 3.5439e-01, Meta loss averaged over last 500 steps = 3.5841e-01, PNorm = 84.3945, GNorm = 0.2464
Meta loss on this task batch = 3.5777e-01, Meta loss averaged over last 500 steps = 3.5840e-01, PNorm = 84.4007, GNorm = 0.2426
Meta loss on this task batch = 3.3238e-01, Meta loss averaged over last 500 steps = 3.5831e-01, PNorm = 84.4090, GNorm = 0.2336
Meta loss on this task batch = 3.4942e-01, Meta loss averaged over last 500 steps = 3.5822e-01, PNorm = 84.4191, GNorm = 0.2533
Meta loss on this task batch = 3.4806e-01, Meta loss averaged over last 500 steps = 3.5836e-01, PNorm = 84.4276, GNorm = 0.2679
Meta loss on this task batch = 3.5275e-01, Meta loss averaged over last 500 steps = 3.5828e-01, PNorm = 84.4377, GNorm = 0.2392
Meta loss on this task batch = 3.4156e-01, Meta loss averaged over last 500 steps = 3.5818e-01, PNorm = 84.4475, GNorm = 0.2632
Meta loss on this task batch = 3.4257e-01, Meta loss averaged over last 500 steps = 3.5809e-01, PNorm = 84.4578, GNorm = 0.2367
Meta loss on this task batch = 3.7424e-01, Meta loss averaged over last 500 steps = 3.5813e-01, PNorm = 84.4670, GNorm = 0.2571
Meta loss on this task batch = 3.1924e-01, Meta loss averaged over last 500 steps = 3.5803e-01, PNorm = 84.4773, GNorm = 0.2641
Meta loss on this task batch = 4.0926e-01, Meta loss averaged over last 500 steps = 3.5818e-01, PNorm = 84.4843, GNorm = 0.3616
Took 132.6674702167511 seconds to complete one epoch of meta training
Took 141.01244711875916 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494681
Epoch 166
Meta loss on this task batch = 3.3476e-01, Meta loss averaged over last 500 steps = 3.5816e-01, PNorm = 84.4922, GNorm = 0.2368
Meta loss on this task batch = 3.5415e-01, Meta loss averaged over last 500 steps = 3.5815e-01, PNorm = 84.4993, GNorm = 0.2564
Meta loss on this task batch = 3.9554e-01, Meta loss averaged over last 500 steps = 3.5805e-01, PNorm = 84.5056, GNorm = 0.2457
Meta loss on this task batch = 3.8183e-01, Meta loss averaged over last 500 steps = 3.5810e-01, PNorm = 84.5099, GNorm = 0.2808
Meta loss on this task batch = 3.7715e-01, Meta loss averaged over last 500 steps = 3.5819e-01, PNorm = 84.5156, GNorm = 0.2678
Meta loss on this task batch = 3.8761e-01, Meta loss averaged over last 500 steps = 3.5830e-01, PNorm = 84.5222, GNorm = 0.2765
Meta loss on this task batch = 3.1170e-01, Meta loss averaged over last 500 steps = 3.5833e-01, PNorm = 84.5279, GNorm = 0.2280
Meta loss on this task batch = 3.2323e-01, Meta loss averaged over last 500 steps = 3.5821e-01, PNorm = 84.5346, GNorm = 0.2525
Meta loss on this task batch = 3.3212e-01, Meta loss averaged over last 500 steps = 3.5820e-01, PNorm = 84.5440, GNorm = 0.2551
Meta loss on this task batch = 3.1123e-01, Meta loss averaged over last 500 steps = 3.5813e-01, PNorm = 84.5515, GNorm = 0.2740
Meta loss on this task batch = 3.7624e-01, Meta loss averaged over last 500 steps = 3.5829e-01, PNorm = 84.5583, GNorm = 0.2899
Meta loss on this task batch = 3.6790e-01, Meta loss averaged over last 500 steps = 3.5829e-01, PNorm = 84.5632, GNorm = 0.2806
Meta loss on this task batch = 3.6358e-01, Meta loss averaged over last 500 steps = 3.5824e-01, PNorm = 84.5684, GNorm = 0.2757
Meta loss on this task batch = 3.0776e-01, Meta loss averaged over last 500 steps = 3.5808e-01, PNorm = 84.5733, GNorm = 0.2234
Meta loss on this task batch = 3.2548e-01, Meta loss averaged over last 500 steps = 3.5788e-01, PNorm = 84.5789, GNorm = 0.2085
Meta loss on this task batch = 3.4513e-01, Meta loss averaged over last 500 steps = 3.5776e-01, PNorm = 84.5857, GNorm = 0.2335
Meta loss on this task batch = 3.6894e-01, Meta loss averaged over last 500 steps = 3.5777e-01, PNorm = 84.5938, GNorm = 0.2991
Meta loss on this task batch = 4.2509e-01, Meta loss averaged over last 500 steps = 3.5785e-01, PNorm = 84.6014, GNorm = 0.2811
Meta loss on this task batch = 2.7391e-01, Meta loss averaged over last 500 steps = 3.5772e-01, PNorm = 84.6123, GNorm = 0.2570
Took 135.9492223262787 seconds to complete one epoch of meta training
Took 144.78392052650452 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507503
Epoch 167
Meta loss on this task batch = 3.6360e-01, Meta loss averaged over last 500 steps = 3.5771e-01, PNorm = 84.6227, GNorm = 0.2336
Meta loss on this task batch = 3.7278e-01, Meta loss averaged over last 500 steps = 3.5782e-01, PNorm = 84.6315, GNorm = 0.2765
Meta loss on this task batch = 3.9049e-01, Meta loss averaged over last 500 steps = 3.5783e-01, PNorm = 84.6391, GNorm = 0.3181
Meta loss on this task batch = 3.6267e-01, Meta loss averaged over last 500 steps = 3.5774e-01, PNorm = 84.6445, GNorm = 0.2905
Meta loss on this task batch = 3.2815e-01, Meta loss averaged over last 500 steps = 3.5773e-01, PNorm = 84.6512, GNorm = 0.2368
Meta loss on this task batch = 3.9797e-01, Meta loss averaged over last 500 steps = 3.5763e-01, PNorm = 84.6551, GNorm = 0.2630
Meta loss on this task batch = 2.4384e-01, Meta loss averaged over last 500 steps = 3.5734e-01, PNorm = 84.6608, GNorm = 0.1995
Meta loss on this task batch = 3.3377e-01, Meta loss averaged over last 500 steps = 3.5721e-01, PNorm = 84.6661, GNorm = 0.2439
Meta loss on this task batch = 3.5661e-01, Meta loss averaged over last 500 steps = 3.5719e-01, PNorm = 84.6717, GNorm = 0.2633
Meta loss on this task batch = 3.6002e-01, Meta loss averaged over last 500 steps = 3.5718e-01, PNorm = 84.6780, GNorm = 0.2960
Meta loss on this task batch = 3.4188e-01, Meta loss averaged over last 500 steps = 3.5727e-01, PNorm = 84.6846, GNorm = 0.2552
Meta loss on this task batch = 3.1509e-01, Meta loss averaged over last 500 steps = 3.5715e-01, PNorm = 84.6931, GNorm = 0.2554
Meta loss on this task batch = 3.5211e-01, Meta loss averaged over last 500 steps = 3.5716e-01, PNorm = 84.7019, GNorm = 0.2528
Meta loss on this task batch = 3.7458e-01, Meta loss averaged over last 500 steps = 3.5713e-01, PNorm = 84.7095, GNorm = 0.2729
Meta loss on this task batch = 3.4218e-01, Meta loss averaged over last 500 steps = 3.5715e-01, PNorm = 84.7176, GNorm = 0.2558
Meta loss on this task batch = 3.6943e-01, Meta loss averaged over last 500 steps = 3.5713e-01, PNorm = 84.7235, GNorm = 0.3222
Meta loss on this task batch = 3.2896e-01, Meta loss averaged over last 500 steps = 3.5700e-01, PNorm = 84.7288, GNorm = 0.2568
Meta loss on this task batch = 3.7068e-01, Meta loss averaged over last 500 steps = 3.5700e-01, PNorm = 84.7342, GNorm = 0.2700
Meta loss on this task batch = 3.6718e-01, Meta loss averaged over last 500 steps = 3.5704e-01, PNorm = 84.7427, GNorm = 0.3783
Took 137.04053854942322 seconds to complete one epoch of meta training
Took 145.57879114151 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471323
Epoch 168
Meta loss on this task batch = 3.8560e-01, Meta loss averaged over last 500 steps = 3.5702e-01, PNorm = 84.7528, GNorm = 0.2745
Meta loss on this task batch = 3.1009e-01, Meta loss averaged over last 500 steps = 3.5699e-01, PNorm = 84.7660, GNorm = 0.2669
Meta loss on this task batch = 3.6477e-01, Meta loss averaged over last 500 steps = 3.5704e-01, PNorm = 84.7801, GNorm = 0.2661
Meta loss on this task batch = 3.1489e-01, Meta loss averaged over last 500 steps = 3.5690e-01, PNorm = 84.7952, GNorm = 0.3171
Meta loss on this task batch = 3.1353e-01, Meta loss averaged over last 500 steps = 3.5688e-01, PNorm = 84.8105, GNorm = 0.2491
Meta loss on this task batch = 4.0227e-01, Meta loss averaged over last 500 steps = 3.5689e-01, PNorm = 84.8244, GNorm = 0.2730
Meta loss on this task batch = 3.4962e-01, Meta loss averaged over last 500 steps = 3.5697e-01, PNorm = 84.8386, GNorm = 0.2718
Meta loss on this task batch = 3.8565e-01, Meta loss averaged over last 500 steps = 3.5697e-01, PNorm = 84.8504, GNorm = 0.2753
Meta loss on this task batch = 3.0607e-01, Meta loss averaged over last 500 steps = 3.5680e-01, PNorm = 84.8614, GNorm = 0.2680
Meta loss on this task batch = 3.4160e-01, Meta loss averaged over last 500 steps = 3.5676e-01, PNorm = 84.8711, GNorm = 0.2790
Meta loss on this task batch = 3.5652e-01, Meta loss averaged over last 500 steps = 3.5689e-01, PNorm = 84.8790, GNorm = 0.2861
Meta loss on this task batch = 3.7450e-01, Meta loss averaged over last 500 steps = 3.5702e-01, PNorm = 84.8830, GNorm = 0.3179
Meta loss on this task batch = 3.3761e-01, Meta loss averaged over last 500 steps = 3.5689e-01, PNorm = 84.8865, GNorm = 0.2940
Meta loss on this task batch = 3.2602e-01, Meta loss averaged over last 500 steps = 3.5675e-01, PNorm = 84.8886, GNorm = 0.2431
Meta loss on this task batch = 3.4950e-01, Meta loss averaged over last 500 steps = 3.5673e-01, PNorm = 84.8902, GNorm = 0.2710
Meta loss on this task batch = 3.3363e-01, Meta loss averaged over last 500 steps = 3.5662e-01, PNorm = 84.8947, GNorm = 0.2705
Meta loss on this task batch = 3.4884e-01, Meta loss averaged over last 500 steps = 3.5653e-01, PNorm = 84.8998, GNorm = 0.2704
Meta loss on this task batch = 3.0432e-01, Meta loss averaged over last 500 steps = 3.5651e-01, PNorm = 84.9053, GNorm = 0.2148
Meta loss on this task batch = 3.9051e-01, Meta loss averaged over last 500 steps = 3.5656e-01, PNorm = 84.9111, GNorm = 0.3407
Took 134.20883083343506 seconds to complete one epoch of meta training
Took 141.69036960601807 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461222
Epoch 169
Meta loss on this task batch = 3.8592e-01, Meta loss averaged over last 500 steps = 3.5647e-01, PNorm = 84.9186, GNorm = 0.3101
Meta loss on this task batch = 3.1845e-01, Meta loss averaged over last 500 steps = 3.5642e-01, PNorm = 84.9289, GNorm = 0.2884
Meta loss on this task batch = 3.5661e-01, Meta loss averaged over last 500 steps = 3.5641e-01, PNorm = 84.9403, GNorm = 0.2761
Meta loss on this task batch = 3.5695e-01, Meta loss averaged over last 500 steps = 3.5640e-01, PNorm = 84.9518, GNorm = 0.2820
Meta loss on this task batch = 2.9379e-01, Meta loss averaged over last 500 steps = 3.5635e-01, PNorm = 84.9629, GNorm = 0.2384
Meta loss on this task batch = 3.7570e-01, Meta loss averaged over last 500 steps = 3.5634e-01, PNorm = 84.9720, GNorm = 0.3023
Meta loss on this task batch = 3.4487e-01, Meta loss averaged over last 500 steps = 3.5624e-01, PNorm = 84.9820, GNorm = 0.3058
Meta loss on this task batch = 3.6886e-01, Meta loss averaged over last 500 steps = 3.5626e-01, PNorm = 84.9885, GNorm = 0.2701
Meta loss on this task batch = 3.6615e-01, Meta loss averaged over last 500 steps = 3.5635e-01, PNorm = 84.9933, GNorm = 0.2998
Meta loss on this task batch = 3.9854e-01, Meta loss averaged over last 500 steps = 3.5633e-01, PNorm = 84.9965, GNorm = 0.3233
Meta loss on this task batch = 3.3585e-01, Meta loss averaged over last 500 steps = 3.5628e-01, PNorm = 84.9991, GNorm = 0.2952
Meta loss on this task batch = 3.6717e-01, Meta loss averaged over last 500 steps = 3.5623e-01, PNorm = 85.0037, GNorm = 0.2690
Meta loss on this task batch = 3.9389e-01, Meta loss averaged over last 500 steps = 3.5625e-01, PNorm = 85.0079, GNorm = 0.2963
Meta loss on this task batch = 3.1536e-01, Meta loss averaged over last 500 steps = 3.5613e-01, PNorm = 85.0148, GNorm = 0.2591
Meta loss on this task batch = 2.9499e-01, Meta loss averaged over last 500 steps = 3.5600e-01, PNorm = 85.0229, GNorm = 0.2327
Meta loss on this task batch = 3.6137e-01, Meta loss averaged over last 500 steps = 3.5594e-01, PNorm = 85.0315, GNorm = 0.2930
Meta loss on this task batch = 3.6167e-01, Meta loss averaged over last 500 steps = 3.5595e-01, PNorm = 85.0396, GNorm = 0.2545
Meta loss on this task batch = 3.4170e-01, Meta loss averaged over last 500 steps = 3.5594e-01, PNorm = 85.0499, GNorm = 0.2667
Meta loss on this task batch = 4.6390e-01, Meta loss averaged over last 500 steps = 3.5625e-01, PNorm = 85.0580, GNorm = 0.3374
Took 135.94349670410156 seconds to complete one epoch of meta training
Took 144.6328730583191 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456744
Epoch 170
Meta loss on this task batch = 3.5000e-01, Meta loss averaged over last 500 steps = 3.5631e-01, PNorm = 85.0661, GNorm = 0.2896
Meta loss on this task batch = 3.1192e-01, Meta loss averaged over last 500 steps = 3.5620e-01, PNorm = 85.0736, GNorm = 0.2325
Meta loss on this task batch = 3.4199e-01, Meta loss averaged over last 500 steps = 3.5617e-01, PNorm = 85.0812, GNorm = 0.2329
Meta loss on this task batch = 3.4172e-01, Meta loss averaged over last 500 steps = 3.5603e-01, PNorm = 85.0902, GNorm = 0.2746
Meta loss on this task batch = 3.7334e-01, Meta loss averaged over last 500 steps = 3.5594e-01, PNorm = 85.0983, GNorm = 0.2987
Meta loss on this task batch = 3.4039e-01, Meta loss averaged over last 500 steps = 3.5583e-01, PNorm = 85.1050, GNorm = 0.2916
Meta loss on this task batch = 3.4563e-01, Meta loss averaged over last 500 steps = 3.5577e-01, PNorm = 85.1135, GNorm = 0.3088
Meta loss on this task batch = 3.4972e-01, Meta loss averaged over last 500 steps = 3.5572e-01, PNorm = 85.1215, GNorm = 0.2698
Meta loss on this task batch = 3.6608e-01, Meta loss averaged over last 500 steps = 3.5580e-01, PNorm = 85.1309, GNorm = 0.2501
Meta loss on this task batch = 3.5445e-01, Meta loss averaged over last 500 steps = 3.5569e-01, PNorm = 85.1426, GNorm = 0.2516
Meta loss on this task batch = 3.6757e-01, Meta loss averaged over last 500 steps = 3.5570e-01, PNorm = 85.1569, GNorm = 0.2722
Meta loss on this task batch = 3.4274e-01, Meta loss averaged over last 500 steps = 3.5577e-01, PNorm = 85.1724, GNorm = 0.2547
Meta loss on this task batch = 3.8004e-01, Meta loss averaged over last 500 steps = 3.5582e-01, PNorm = 85.1884, GNorm = 0.2956
Meta loss on this task batch = 3.4695e-01, Meta loss averaged over last 500 steps = 3.5581e-01, PNorm = 85.2029, GNorm = 0.2515
Meta loss on this task batch = 3.4101e-01, Meta loss averaged over last 500 steps = 3.5572e-01, PNorm = 85.2158, GNorm = 0.2627
Meta loss on this task batch = 3.8374e-01, Meta loss averaged over last 500 steps = 3.5582e-01, PNorm = 85.2254, GNorm = 0.3143
Meta loss on this task batch = 3.6025e-01, Meta loss averaged over last 500 steps = 3.5569e-01, PNorm = 85.2335, GNorm = 0.2681
Meta loss on this task batch = 3.1850e-01, Meta loss averaged over last 500 steps = 3.5569e-01, PNorm = 85.2415, GNorm = 0.2795
Meta loss on this task batch = 4.1788e-01, Meta loss averaged over last 500 steps = 3.5576e-01, PNorm = 85.2485, GNorm = 0.3227
Took 136.14900255203247 seconds to complete one epoch of meta training
Took 144.63444089889526 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496839
Epoch 171
Meta loss on this task batch = 3.1511e-01, Meta loss averaged over last 500 steps = 3.5567e-01, PNorm = 85.2565, GNorm = 0.2548
Meta loss on this task batch = 3.8264e-01, Meta loss averaged over last 500 steps = 3.5568e-01, PNorm = 85.2629, GNorm = 0.2816
Meta loss on this task batch = 3.9380e-01, Meta loss averaged over last 500 steps = 3.5572e-01, PNorm = 85.2734, GNorm = 0.2863
Meta loss on this task batch = 3.7113e-01, Meta loss averaged over last 500 steps = 3.5574e-01, PNorm = 85.2891, GNorm = 0.3151
Meta loss on this task batch = 3.7978e-01, Meta loss averaged over last 500 steps = 3.5573e-01, PNorm = 85.3075, GNorm = 0.3193
Meta loss on this task batch = 3.0782e-01, Meta loss averaged over last 500 steps = 3.5563e-01, PNorm = 85.3280, GNorm = 0.2502
Meta loss on this task batch = 3.5422e-01, Meta loss averaged over last 500 steps = 3.5553e-01, PNorm = 85.3474, GNorm = 0.2554
Meta loss on this task batch = 3.2265e-01, Meta loss averaged over last 500 steps = 3.5532e-01, PNorm = 85.3670, GNorm = 0.3031
Meta loss on this task batch = 3.8167e-01, Meta loss averaged over last 500 steps = 3.5545e-01, PNorm = 85.3839, GNorm = 0.3165
Meta loss on this task batch = 3.3014e-01, Meta loss averaged over last 500 steps = 3.5543e-01, PNorm = 85.3982, GNorm = 0.2708
Meta loss on this task batch = 3.4757e-01, Meta loss averaged over last 500 steps = 3.5546e-01, PNorm = 85.4100, GNorm = 0.2794
Meta loss on this task batch = 2.7349e-01, Meta loss averaged over last 500 steps = 3.5527e-01, PNorm = 85.4201, GNorm = 0.2887
Meta loss on this task batch = 3.3569e-01, Meta loss averaged over last 500 steps = 3.5524e-01, PNorm = 85.4263, GNorm = 0.3257
Meta loss on this task batch = 3.8310e-01, Meta loss averaged over last 500 steps = 3.5543e-01, PNorm = 85.4290, GNorm = 0.3385
Meta loss on this task batch = 4.0632e-01, Meta loss averaged over last 500 steps = 3.5547e-01, PNorm = 85.4296, GNorm = 0.3508
Meta loss on this task batch = 3.2396e-01, Meta loss averaged over last 500 steps = 3.5537e-01, PNorm = 85.4319, GNorm = 0.2479
Meta loss on this task batch = 4.2505e-01, Meta loss averaged over last 500 steps = 3.5558e-01, PNorm = 85.4339, GNorm = 0.2709
Meta loss on this task batch = 3.3185e-01, Meta loss averaged over last 500 steps = 3.5555e-01, PNorm = 85.4362, GNorm = 0.2430
Meta loss on this task batch = 3.5006e-01, Meta loss averaged over last 500 steps = 3.5545e-01, PNorm = 85.4390, GNorm = 0.3173
Took 137.0488030910492 seconds to complete one epoch of meta training
Took 146.3790202140808 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473267
Epoch 172
Meta loss on this task batch = 3.2845e-01, Meta loss averaged over last 500 steps = 3.5532e-01, PNorm = 85.4439, GNorm = 0.2467
Meta loss on this task batch = 3.5910e-01, Meta loss averaged over last 500 steps = 3.5532e-01, PNorm = 85.4496, GNorm = 0.2781
Meta loss on this task batch = 3.3457e-01, Meta loss averaged over last 500 steps = 3.5527e-01, PNorm = 85.4571, GNorm = 0.2553
Meta loss on this task batch = 3.7691e-01, Meta loss averaged over last 500 steps = 3.5536e-01, PNorm = 85.4657, GNorm = 0.2733
Meta loss on this task batch = 3.6273e-01, Meta loss averaged over last 500 steps = 3.5548e-01, PNorm = 85.4767, GNorm = 0.2597
Meta loss on this task batch = 3.3441e-01, Meta loss averaged over last 500 steps = 3.5556e-01, PNorm = 85.4878, GNorm = 0.2729
Meta loss on this task batch = 3.3617e-01, Meta loss averaged over last 500 steps = 3.5552e-01, PNorm = 85.5003, GNorm = 0.2708
Meta loss on this task batch = 3.2262e-01, Meta loss averaged over last 500 steps = 3.5531e-01, PNorm = 85.5123, GNorm = 0.2430
Meta loss on this task batch = 3.5124e-01, Meta loss averaged over last 500 steps = 3.5539e-01, PNorm = 85.5225, GNorm = 0.3139
Meta loss on this task batch = 3.9900e-01, Meta loss averaged over last 500 steps = 3.5541e-01, PNorm = 85.5292, GNorm = 0.3618
Meta loss on this task batch = 4.0192e-01, Meta loss averaged over last 500 steps = 3.5559e-01, PNorm = 85.5337, GNorm = 0.3427
Meta loss on this task batch = 3.3590e-01, Meta loss averaged over last 500 steps = 3.5556e-01, PNorm = 85.5389, GNorm = 0.2547
Meta loss on this task batch = 3.8518e-01, Meta loss averaged over last 500 steps = 3.5553e-01, PNorm = 85.5430, GNorm = 0.2466
Meta loss on this task batch = 4.3174e-01, Meta loss averaged over last 500 steps = 3.5570e-01, PNorm = 85.5495, GNorm = 0.3031
Meta loss on this task batch = 3.2120e-01, Meta loss averaged over last 500 steps = 3.5558e-01, PNorm = 85.5597, GNorm = 0.2799
Meta loss on this task batch = 3.4770e-01, Meta loss averaged over last 500 steps = 3.5568e-01, PNorm = 85.5698, GNorm = 0.2518
Meta loss on this task batch = 3.8835e-01, Meta loss averaged over last 500 steps = 3.5573e-01, PNorm = 85.5805, GNorm = 0.2490
Meta loss on this task batch = 3.1685e-01, Meta loss averaged over last 500 steps = 3.5563e-01, PNorm = 85.5928, GNorm = 0.2336
Meta loss on this task batch = 3.4346e-01, Meta loss averaged over last 500 steps = 3.5563e-01, PNorm = 85.6065, GNorm = 0.3295
Took 133.09633946418762 seconds to complete one epoch of meta training
Took 142.29448127746582 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.520408
Epoch 173
Meta loss on this task batch = 4.0816e-01, Meta loss averaged over last 500 steps = 3.5573e-01, PNorm = 85.6198, GNorm = 0.2861
Meta loss on this task batch = 2.9557e-01, Meta loss averaged over last 500 steps = 3.5559e-01, PNorm = 85.6324, GNorm = 0.2190
Meta loss on this task batch = 3.5185e-01, Meta loss averaged over last 500 steps = 3.5544e-01, PNorm = 85.6442, GNorm = 0.2631
Meta loss on this task batch = 3.7288e-01, Meta loss averaged over last 500 steps = 3.5550e-01, PNorm = 85.6543, GNorm = 0.2815
Meta loss on this task batch = 3.5261e-01, Meta loss averaged over last 500 steps = 3.5547e-01, PNorm = 85.6634, GNorm = 0.2619
Meta loss on this task batch = 3.4734e-01, Meta loss averaged over last 500 steps = 3.5534e-01, PNorm = 85.6700, GNorm = 0.2834
Meta loss on this task batch = 3.2016e-01, Meta loss averaged over last 500 steps = 3.5528e-01, PNorm = 85.6744, GNorm = 0.3041
Meta loss on this task batch = 3.1946e-01, Meta loss averaged over last 500 steps = 3.5524e-01, PNorm = 85.6790, GNorm = 0.2839
Meta loss on this task batch = 3.7115e-01, Meta loss averaged over last 500 steps = 3.5523e-01, PNorm = 85.6827, GNorm = 0.2938
Meta loss on this task batch = 3.2507e-01, Meta loss averaged over last 500 steps = 3.5523e-01, PNorm = 85.6896, GNorm = 0.2570
Meta loss on this task batch = 3.1623e-01, Meta loss averaged over last 500 steps = 3.5512e-01, PNorm = 85.6972, GNorm = 0.2421
Meta loss on this task batch = 3.5033e-01, Meta loss averaged over last 500 steps = 3.5511e-01, PNorm = 85.7078, GNorm = 0.2889
Meta loss on this task batch = 3.6906e-01, Meta loss averaged over last 500 steps = 3.5499e-01, PNorm = 85.7199, GNorm = 0.2878
Meta loss on this task batch = 3.3382e-01, Meta loss averaged over last 500 steps = 3.5488e-01, PNorm = 85.7320, GNorm = 0.2795
Meta loss on this task batch = 4.3935e-01, Meta loss averaged over last 500 steps = 3.5498e-01, PNorm = 85.7432, GNorm = 0.2684
Meta loss on this task batch = 3.1041e-01, Meta loss averaged over last 500 steps = 3.5492e-01, PNorm = 85.7544, GNorm = 0.2355
Meta loss on this task batch = 3.3416e-01, Meta loss averaged over last 500 steps = 3.5493e-01, PNorm = 85.7649, GNorm = 0.2489
Meta loss on this task batch = 3.4238e-01, Meta loss averaged over last 500 steps = 3.5497e-01, PNorm = 85.7755, GNorm = 0.2585
Meta loss on this task batch = 3.7108e-01, Meta loss averaged over last 500 steps = 3.5510e-01, PNorm = 85.7830, GNorm = 0.3318
Took 135.1772439479828 seconds to complete one epoch of meta training
Took 144.41623997688293 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481712
Epoch 174
Meta loss on this task batch = 3.3212e-01, Meta loss averaged over last 500 steps = 3.5500e-01, PNorm = 85.7903, GNorm = 0.2619
Meta loss on this task batch = 3.7985e-01, Meta loss averaged over last 500 steps = 3.5510e-01, PNorm = 85.7940, GNorm = 0.3427
Meta loss on this task batch = 3.6012e-01, Meta loss averaged over last 500 steps = 3.5510e-01, PNorm = 85.7976, GNorm = 0.2955
Meta loss on this task batch = 3.3259e-01, Meta loss averaged over last 500 steps = 3.5494e-01, PNorm = 85.8033, GNorm = 0.2682
Meta loss on this task batch = 3.5097e-01, Meta loss averaged over last 500 steps = 3.5492e-01, PNorm = 85.8088, GNorm = 0.2414
Meta loss on this task batch = 3.8293e-01, Meta loss averaged over last 500 steps = 3.5506e-01, PNorm = 85.8132, GNorm = 0.2623
Meta loss on this task batch = 3.8100e-01, Meta loss averaged over last 500 steps = 3.5503e-01, PNorm = 85.8178, GNorm = 0.2763
Meta loss on this task batch = 3.3621e-01, Meta loss averaged over last 500 steps = 3.5507e-01, PNorm = 85.8237, GNorm = 0.2312
Meta loss on this task batch = 3.4034e-01, Meta loss averaged over last 500 steps = 3.5499e-01, PNorm = 85.8327, GNorm = 0.2302
Meta loss on this task batch = 3.3340e-01, Meta loss averaged over last 500 steps = 3.5498e-01, PNorm = 85.8436, GNorm = 0.2438
Meta loss on this task batch = 3.5531e-01, Meta loss averaged over last 500 steps = 3.5503e-01, PNorm = 85.8561, GNorm = 0.2903
Meta loss on this task batch = 3.9405e-01, Meta loss averaged over last 500 steps = 3.5519e-01, PNorm = 85.8686, GNorm = 0.2632
Meta loss on this task batch = 3.6066e-01, Meta loss averaged over last 500 steps = 3.5514e-01, PNorm = 85.8822, GNorm = 0.2698
Meta loss on this task batch = 3.8044e-01, Meta loss averaged over last 500 steps = 3.5499e-01, PNorm = 85.8934, GNorm = 0.2671
Meta loss on this task batch = 3.4697e-01, Meta loss averaged over last 500 steps = 3.5495e-01, PNorm = 85.9028, GNorm = 0.2552
Meta loss on this task batch = 3.5109e-01, Meta loss averaged over last 500 steps = 3.5495e-01, PNorm = 85.9084, GNorm = 0.3127
Meta loss on this task batch = 3.1240e-01, Meta loss averaged over last 500 steps = 3.5479e-01, PNorm = 85.9125, GNorm = 0.2764
Meta loss on this task batch = 3.2467e-01, Meta loss averaged over last 500 steps = 3.5481e-01, PNorm = 85.9143, GNorm = 0.2898
Meta loss on this task batch = 3.2076e-01, Meta loss averaged over last 500 steps = 3.5478e-01, PNorm = 85.9147, GNorm = 0.3127
Took 136.0027642250061 seconds to complete one epoch of meta training
Took 145.04393410682678 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501692
Epoch 175
Meta loss on this task batch = 3.4515e-01, Meta loss averaged over last 500 steps = 3.5472e-01, PNorm = 85.9161, GNorm = 0.2520
Meta loss on this task batch = 3.9884e-01, Meta loss averaged over last 500 steps = 3.5480e-01, PNorm = 85.9183, GNorm = 0.2665
Meta loss on this task batch = 3.2229e-01, Meta loss averaged over last 500 steps = 3.5468e-01, PNorm = 85.9258, GNorm = 0.2788
Meta loss on this task batch = 3.7129e-01, Meta loss averaged over last 500 steps = 3.5481e-01, PNorm = 85.9361, GNorm = 0.2882
Meta loss on this task batch = 4.0837e-01, Meta loss averaged over last 500 steps = 3.5494e-01, PNorm = 85.9483, GNorm = 0.2917
Meta loss on this task batch = 3.5191e-01, Meta loss averaged over last 500 steps = 3.5480e-01, PNorm = 85.9629, GNorm = 0.2525
Meta loss on this task batch = 3.0548e-01, Meta loss averaged over last 500 steps = 3.5468e-01, PNorm = 85.9778, GNorm = 0.2493
Meta loss on this task batch = 3.0014e-01, Meta loss averaged over last 500 steps = 3.5457e-01, PNorm = 85.9914, GNorm = 0.2786
Meta loss on this task batch = 2.9562e-01, Meta loss averaged over last 500 steps = 3.5440e-01, PNorm = 86.0027, GNorm = 0.3127
Meta loss on this task batch = 3.3927e-01, Meta loss averaged over last 500 steps = 3.5436e-01, PNorm = 86.0138, GNorm = 0.3064
Meta loss on this task batch = 3.7891e-01, Meta loss averaged over last 500 steps = 3.5430e-01, PNorm = 86.0222, GNorm = 0.3224
Meta loss on this task batch = 3.1319e-01, Meta loss averaged over last 500 steps = 3.5420e-01, PNorm = 86.0279, GNorm = 0.2805
Meta loss on this task batch = 3.2858e-01, Meta loss averaged over last 500 steps = 3.5421e-01, PNorm = 86.0341, GNorm = 0.2755
Meta loss on this task batch = 3.9309e-01, Meta loss averaged over last 500 steps = 3.5428e-01, PNorm = 86.0379, GNorm = 0.3774
Meta loss on this task batch = 3.6703e-01, Meta loss averaged over last 500 steps = 3.5430e-01, PNorm = 86.0394, GNorm = 0.2886
Meta loss on this task batch = 3.4810e-01, Meta loss averaged over last 500 steps = 3.5427e-01, PNorm = 86.0426, GNorm = 0.2412
Meta loss on this task batch = 3.4765e-01, Meta loss averaged over last 500 steps = 3.5430e-01, PNorm = 86.0448, GNorm = 0.2491
Meta loss on this task batch = 3.0928e-01, Meta loss averaged over last 500 steps = 3.5417e-01, PNorm = 86.0479, GNorm = 0.2269
Meta loss on this task batch = 4.2076e-01, Meta loss averaged over last 500 steps = 3.5436e-01, PNorm = 86.0511, GNorm = 0.3936
Took 134.99117469787598 seconds to complete one epoch of meta training
Took 143.7751226425171 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468214
Epoch 176
Meta loss on this task batch = 3.0981e-01, Meta loss averaged over last 500 steps = 3.5433e-01, PNorm = 86.0572, GNorm = 0.2544
Meta loss on this task batch = 3.5120e-01, Meta loss averaged over last 500 steps = 3.5421e-01, PNorm = 86.0642, GNorm = 0.2326
Meta loss on this task batch = 3.8246e-01, Meta loss averaged over last 500 steps = 3.5440e-01, PNorm = 86.0742, GNorm = 0.2687
Meta loss on this task batch = 3.6998e-01, Meta loss averaged over last 500 steps = 3.5452e-01, PNorm = 86.0862, GNorm = 0.2494
Meta loss on this task batch = 3.4088e-01, Meta loss averaged over last 500 steps = 3.5444e-01, PNorm = 86.1006, GNorm = 0.2356
Meta loss on this task batch = 3.2783e-01, Meta loss averaged over last 500 steps = 3.5436e-01, PNorm = 86.1170, GNorm = 0.2640
Meta loss on this task batch = 3.5777e-01, Meta loss averaged over last 500 steps = 3.5435e-01, PNorm = 86.1329, GNorm = 0.2404
Meta loss on this task batch = 3.8327e-01, Meta loss averaged over last 500 steps = 3.5439e-01, PNorm = 86.1487, GNorm = 0.2555
Meta loss on this task batch = 2.9752e-01, Meta loss averaged over last 500 steps = 3.5422e-01, PNorm = 86.1634, GNorm = 0.2435
Meta loss on this task batch = 3.6011e-01, Meta loss averaged over last 500 steps = 3.5413e-01, PNorm = 86.1765, GNorm = 0.2826
Meta loss on this task batch = 3.4294e-01, Meta loss averaged over last 500 steps = 3.5426e-01, PNorm = 86.1875, GNorm = 0.3266
Meta loss on this task batch = 3.1456e-01, Meta loss averaged over last 500 steps = 3.5408e-01, PNorm = 86.1966, GNorm = 0.2542
Meta loss on this task batch = 3.5401e-01, Meta loss averaged over last 500 steps = 3.5418e-01, PNorm = 86.2036, GNorm = 0.2537
Meta loss on this task batch = 3.2492e-01, Meta loss averaged over last 500 steps = 3.5402e-01, PNorm = 86.2104, GNorm = 0.2910
Meta loss on this task batch = 3.5846e-01, Meta loss averaged over last 500 steps = 3.5396e-01, PNorm = 86.2178, GNorm = 0.2996
Meta loss on this task batch = 3.1154e-01, Meta loss averaged over last 500 steps = 3.5376e-01, PNorm = 86.2268, GNorm = 0.2245
Meta loss on this task batch = 3.0994e-01, Meta loss averaged over last 500 steps = 3.5363e-01, PNorm = 86.2363, GNorm = 0.2780
Meta loss on this task batch = 3.4802e-01, Meta loss averaged over last 500 steps = 3.5367e-01, PNorm = 86.2452, GNorm = 0.3101
Meta loss on this task batch = 4.2134e-01, Meta loss averaged over last 500 steps = 3.5373e-01, PNorm = 86.2521, GNorm = 0.3843
Took 135.9786660671234 seconds to complete one epoch of meta training
Took 144.57618379592896 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489818
Epoch 177
Meta loss on this task batch = 3.2182e-01, Meta loss averaged over last 500 steps = 3.5357e-01, PNorm = 86.2587, GNorm = 0.2524
Meta loss on this task batch = 3.5094e-01, Meta loss averaged over last 500 steps = 3.5366e-01, PNorm = 86.2656, GNorm = 0.3008
Meta loss on this task batch = 3.4021e-01, Meta loss averaged over last 500 steps = 3.5365e-01, PNorm = 86.2714, GNorm = 0.2977
Meta loss on this task batch = 3.6545e-01, Meta loss averaged over last 500 steps = 3.5364e-01, PNorm = 86.2783, GNorm = 0.2843
Meta loss on this task batch = 3.8827e-01, Meta loss averaged over last 500 steps = 3.5370e-01, PNorm = 86.2871, GNorm = 0.3279
Meta loss on this task batch = 3.3348e-01, Meta loss averaged over last 500 steps = 3.5358e-01, PNorm = 86.2977, GNorm = 0.2556
Meta loss on this task batch = 2.9846e-01, Meta loss averaged over last 500 steps = 3.5346e-01, PNorm = 86.3099, GNorm = 0.2523
Meta loss on this task batch = 4.5179e-01, Meta loss averaged over last 500 steps = 3.5358e-01, PNorm = 86.3204, GNorm = 0.3481
Meta loss on this task batch = 3.5472e-01, Meta loss averaged over last 500 steps = 3.5351e-01, PNorm = 86.3297, GNorm = 0.3040
Meta loss on this task batch = 3.1714e-01, Meta loss averaged over last 500 steps = 3.5343e-01, PNorm = 86.3387, GNorm = 0.2412
Meta loss on this task batch = 3.8718e-01, Meta loss averaged over last 500 steps = 3.5356e-01, PNorm = 86.3462, GNorm = 0.2794
Meta loss on this task batch = 3.2998e-01, Meta loss averaged over last 500 steps = 3.5362e-01, PNorm = 86.3528, GNorm = 0.2609
Meta loss on this task batch = 3.5026e-01, Meta loss averaged over last 500 steps = 3.5357e-01, PNorm = 86.3582, GNorm = 0.2553
Meta loss on this task batch = 3.1177e-01, Meta loss averaged over last 500 steps = 3.5354e-01, PNorm = 86.3624, GNorm = 0.2482
Meta loss on this task batch = 3.3300e-01, Meta loss averaged over last 500 steps = 3.5350e-01, PNorm = 86.3669, GNorm = 0.2703
Meta loss on this task batch = 3.1790e-01, Meta loss averaged over last 500 steps = 3.5343e-01, PNorm = 86.3699, GNorm = 0.2553
Meta loss on this task batch = 3.4715e-01, Meta loss averaged over last 500 steps = 3.5341e-01, PNorm = 86.3734, GNorm = 0.2553
Meta loss on this task batch = 3.3581e-01, Meta loss averaged over last 500 steps = 3.5338e-01, PNorm = 86.3795, GNorm = 0.3000
Meta loss on this task batch = 3.5748e-01, Meta loss averaged over last 500 steps = 3.5341e-01, PNorm = 86.3867, GNorm = 0.3031
Took 135.6152367591858 seconds to complete one epoch of meta training
Took 144.09924721717834 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489712
Epoch 178
Meta loss on this task batch = 3.3293e-01, Meta loss averaged over last 500 steps = 3.5333e-01, PNorm = 86.3948, GNorm = 0.2386
Meta loss on this task batch = 3.5416e-01, Meta loss averaged over last 500 steps = 3.5335e-01, PNorm = 86.4048, GNorm = 0.2646
Meta loss on this task batch = 3.5810e-01, Meta loss averaged over last 500 steps = 3.5322e-01, PNorm = 86.4149, GNorm = 0.2421
Meta loss on this task batch = 4.2474e-01, Meta loss averaged over last 500 steps = 3.5337e-01, PNorm = 86.4225, GNorm = 0.3227
Meta loss on this task batch = 2.9530e-01, Meta loss averaged over last 500 steps = 3.5328e-01, PNorm = 86.4308, GNorm = 0.2545
Meta loss on this task batch = 3.5143e-01, Meta loss averaged over last 500 steps = 3.5332e-01, PNorm = 86.4411, GNorm = 0.3121
Meta loss on this task batch = 3.2461e-01, Meta loss averaged over last 500 steps = 3.5333e-01, PNorm = 86.4509, GNorm = 0.2401
Meta loss on this task batch = 2.5100e-01, Meta loss averaged over last 500 steps = 3.5305e-01, PNorm = 86.4613, GNorm = 0.2064
Meta loss on this task batch = 3.8846e-01, Meta loss averaged over last 500 steps = 3.5312e-01, PNorm = 86.4684, GNorm = 0.3883
Meta loss on this task batch = 3.8394e-01, Meta loss averaged over last 500 steps = 3.5301e-01, PNorm = 86.4761, GNorm = 0.2903
Meta loss on this task batch = 3.1901e-01, Meta loss averaged over last 500 steps = 3.5298e-01, PNorm = 86.4841, GNorm = 0.2605
Meta loss on this task batch = 3.4509e-01, Meta loss averaged over last 500 steps = 3.5289e-01, PNorm = 86.4928, GNorm = 0.2905
Meta loss on this task batch = 3.1693e-01, Meta loss averaged over last 500 steps = 3.5277e-01, PNorm = 86.5005, GNorm = 0.2531
Meta loss on this task batch = 3.3413e-01, Meta loss averaged over last 500 steps = 3.5279e-01, PNorm = 86.5091, GNorm = 0.2606
Meta loss on this task batch = 3.3631e-01, Meta loss averaged over last 500 steps = 3.5276e-01, PNorm = 86.5154, GNorm = 0.3005
Meta loss on this task batch = 3.7400e-01, Meta loss averaged over last 500 steps = 3.5273e-01, PNorm = 86.5206, GNorm = 0.3435
Meta loss on this task batch = 3.5051e-01, Meta loss averaged over last 500 steps = 3.5286e-01, PNorm = 86.5269, GNorm = 0.2677
Meta loss on this task batch = 3.3666e-01, Meta loss averaged over last 500 steps = 3.5271e-01, PNorm = 86.5317, GNorm = 0.2699
Meta loss on this task batch = 3.7842e-01, Meta loss averaged over last 500 steps = 3.5271e-01, PNorm = 86.5372, GNorm = 0.3070
Took 151.08028888702393 seconds to complete one epoch of meta training
Took 159.820734500885 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471947
Epoch 179
Meta loss on this task batch = 3.0360e-01, Meta loss averaged over last 500 steps = 3.5258e-01, PNorm = 86.5442, GNorm = 0.2357
Meta loss on this task batch = 4.3180e-01, Meta loss averaged over last 500 steps = 3.5269e-01, PNorm = 86.5476, GNorm = 0.3222
Meta loss on this task batch = 3.7429e-01, Meta loss averaged over last 500 steps = 3.5278e-01, PNorm = 86.5512, GNorm = 0.2577
Meta loss on this task batch = 3.3910e-01, Meta loss averaged over last 500 steps = 3.5264e-01, PNorm = 86.5550, GNorm = 0.2525
Meta loss on this task batch = 3.3536e-01, Meta loss averaged over last 500 steps = 3.5273e-01, PNorm = 86.5603, GNorm = 0.2519
Meta loss on this task batch = 3.7136e-01, Meta loss averaged over last 500 steps = 3.5274e-01, PNorm = 86.5677, GNorm = 0.2956
Meta loss on this task batch = 3.1575e-01, Meta loss averaged over last 500 steps = 3.5270e-01, PNorm = 86.5747, GNorm = 0.2639
Meta loss on this task batch = 3.7876e-01, Meta loss averaged over last 500 steps = 3.5280e-01, PNorm = 86.5821, GNorm = 0.2509
Meta loss on this task batch = 3.1130e-01, Meta loss averaged over last 500 steps = 3.5274e-01, PNorm = 86.5912, GNorm = 0.2398
Meta loss on this task batch = 3.5156e-01, Meta loss averaged over last 500 steps = 3.5268e-01, PNorm = 86.5973, GNorm = 0.2989
Meta loss on this task batch = 3.0909e-01, Meta loss averaged over last 500 steps = 3.5260e-01, PNorm = 86.6040, GNorm = 0.2603
Meta loss on this task batch = 3.3709e-01, Meta loss averaged over last 500 steps = 3.5259e-01, PNorm = 86.6115, GNorm = 0.2663
Meta loss on this task batch = 3.4853e-01, Meta loss averaged over last 500 steps = 3.5256e-01, PNorm = 86.6190, GNorm = 0.2536
Meta loss on this task batch = 3.3216e-01, Meta loss averaged over last 500 steps = 3.5248e-01, PNorm = 86.6264, GNorm = 0.2567
Meta loss on this task batch = 3.1525e-01, Meta loss averaged over last 500 steps = 3.5244e-01, PNorm = 86.6329, GNorm = 0.2669
Meta loss on this task batch = 3.5420e-01, Meta loss averaged over last 500 steps = 3.5245e-01, PNorm = 86.6397, GNorm = 0.2865
Meta loss on this task batch = 3.9296e-01, Meta loss averaged over last 500 steps = 3.5242e-01, PNorm = 86.6473, GNorm = 0.2971
Meta loss on this task batch = 3.2166e-01, Meta loss averaged over last 500 steps = 3.5241e-01, PNorm = 86.6560, GNorm = 0.2572
Meta loss on this task batch = 4.0583e-01, Meta loss averaged over last 500 steps = 3.5252e-01, PNorm = 86.6635, GNorm = 0.3716
Took 134.15529823303223 seconds to complete one epoch of meta training
Took 142.794757604599 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479657
Epoch 180
Meta loss on this task batch = 3.5547e-01, Meta loss averaged over last 500 steps = 3.5248e-01, PNorm = 86.6706, GNorm = 0.2649
Meta loss on this task batch = 3.4023e-01, Meta loss averaged over last 500 steps = 3.5243e-01, PNorm = 86.6791, GNorm = 0.2427
Meta loss on this task batch = 3.4885e-01, Meta loss averaged over last 500 steps = 3.5235e-01, PNorm = 86.6874, GNorm = 0.3192
Meta loss on this task batch = 3.3638e-01, Meta loss averaged over last 500 steps = 3.5241e-01, PNorm = 86.6964, GNorm = 0.2492
Meta loss on this task batch = 3.3623e-01, Meta loss averaged over last 500 steps = 3.5233e-01, PNorm = 86.7057, GNorm = 0.2539
Meta loss on this task batch = 3.2827e-01, Meta loss averaged over last 500 steps = 3.5218e-01, PNorm = 86.7145, GNorm = 0.2366
Meta loss on this task batch = 3.0253e-01, Meta loss averaged over last 500 steps = 3.5216e-01, PNorm = 86.7227, GNorm = 0.2287
Meta loss on this task batch = 4.2360e-01, Meta loss averaged over last 500 steps = 3.5224e-01, PNorm = 86.7270, GNorm = 0.3312
Meta loss on this task batch = 2.9564e-01, Meta loss averaged over last 500 steps = 3.5207e-01, PNorm = 86.7315, GNorm = 0.2199
Meta loss on this task batch = 3.8407e-01, Meta loss averaged over last 500 steps = 3.5212e-01, PNorm = 86.7359, GNorm = 0.2661
Meta loss on this task batch = 3.9586e-01, Meta loss averaged over last 500 steps = 3.5220e-01, PNorm = 86.7401, GNorm = 0.2746
Meta loss on this task batch = 3.0600e-01, Meta loss averaged over last 500 steps = 3.5207e-01, PNorm = 86.7460, GNorm = 0.2539
Meta loss on this task batch = 3.5285e-01, Meta loss averaged over last 500 steps = 3.5211e-01, PNorm = 86.7550, GNorm = 0.2697
Meta loss on this task batch = 3.2537e-01, Meta loss averaged over last 500 steps = 3.5197e-01, PNorm = 86.7649, GNorm = 0.2599
Meta loss on this task batch = 3.8847e-01, Meta loss averaged over last 500 steps = 3.5192e-01, PNorm = 86.7771, GNorm = 0.2839
Meta loss on this task batch = 3.2682e-01, Meta loss averaged over last 500 steps = 3.5188e-01, PNorm = 86.7884, GNorm = 0.2773
Meta loss on this task batch = 3.4672e-01, Meta loss averaged over last 500 steps = 3.5194e-01, PNorm = 86.7995, GNorm = 0.2587
Meta loss on this task batch = 3.6837e-01, Meta loss averaged over last 500 steps = 3.5189e-01, PNorm = 86.8096, GNorm = 0.3456
Meta loss on this task batch = 3.4789e-01, Meta loss averaged over last 500 steps = 3.5195e-01, PNorm = 86.8150, GNorm = 0.3355
Took 132.40678024291992 seconds to complete one epoch of meta training
Took 141.41203594207764 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494817
Epoch 181
Meta loss on this task batch = 3.5604e-01, Meta loss averaged over last 500 steps = 3.5200e-01, PNorm = 86.8201, GNorm = 0.3021
Meta loss on this task batch = 2.9835e-01, Meta loss averaged over last 500 steps = 3.5208e-01, PNorm = 86.8265, GNorm = 0.2325
Meta loss on this task batch = 3.5210e-01, Meta loss averaged over last 500 steps = 3.5196e-01, PNorm = 86.8341, GNorm = 0.2196
Meta loss on this task batch = 3.2053e-01, Meta loss averaged over last 500 steps = 3.5180e-01, PNorm = 86.8410, GNorm = 0.2732
Meta loss on this task batch = 2.9667e-01, Meta loss averaged over last 500 steps = 3.5158e-01, PNorm = 86.8479, GNorm = 0.2557
Meta loss on this task batch = 3.6869e-01, Meta loss averaged over last 500 steps = 3.5156e-01, PNorm = 86.8555, GNorm = 0.2800
Meta loss on this task batch = 3.8081e-01, Meta loss averaged over last 500 steps = 3.5154e-01, PNorm = 86.8657, GNorm = 0.2849
Meta loss on this task batch = 3.1665e-01, Meta loss averaged over last 500 steps = 3.5152e-01, PNorm = 86.8772, GNorm = 0.2216
Meta loss on this task batch = 3.0242e-01, Meta loss averaged over last 500 steps = 3.5141e-01, PNorm = 86.8893, GNorm = 0.2730
Meta loss on this task batch = 3.2264e-01, Meta loss averaged over last 500 steps = 3.5130e-01, PNorm = 86.9014, GNorm = 0.2459
Meta loss on this task batch = 3.2982e-01, Meta loss averaged over last 500 steps = 3.5132e-01, PNorm = 86.9125, GNorm = 0.2667
Meta loss on this task batch = 3.8962e-01, Meta loss averaged over last 500 steps = 3.5140e-01, PNorm = 86.9224, GNorm = 0.2755
Meta loss on this task batch = 3.5311e-01, Meta loss averaged over last 500 steps = 3.5143e-01, PNorm = 86.9330, GNorm = 0.2497
Meta loss on this task batch = 3.6770e-01, Meta loss averaged over last 500 steps = 3.5140e-01, PNorm = 86.9407, GNorm = 0.3090
Meta loss on this task batch = 3.5423e-01, Meta loss averaged over last 500 steps = 3.5146e-01, PNorm = 86.9468, GNorm = 0.2972
Meta loss on this task batch = 4.0574e-01, Meta loss averaged over last 500 steps = 3.5166e-01, PNorm = 86.9512, GNorm = 0.3313
Meta loss on this task batch = 3.0053e-01, Meta loss averaged over last 500 steps = 3.5157e-01, PNorm = 86.9562, GNorm = 0.3054
Meta loss on this task batch = 3.6421e-01, Meta loss averaged over last 500 steps = 3.5155e-01, PNorm = 86.9618, GNorm = 0.2741
Meta loss on this task batch = 3.4581e-01, Meta loss averaged over last 500 steps = 3.5158e-01, PNorm = 86.9683, GNorm = 0.3060
Took 134.3322148323059 seconds to complete one epoch of meta training
Took 142.5596342086792 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498373
Epoch 182
Meta loss on this task batch = 3.3475e-01, Meta loss averaged over last 500 steps = 3.5143e-01, PNorm = 86.9762, GNorm = 0.2764
Meta loss on this task batch = 3.6195e-01, Meta loss averaged over last 500 steps = 3.5138e-01, PNorm = 86.9835, GNorm = 0.2794
Meta loss on this task batch = 3.1138e-01, Meta loss averaged over last 500 steps = 3.5123e-01, PNorm = 86.9919, GNorm = 0.2601
Meta loss on this task batch = 2.7594e-01, Meta loss averaged over last 500 steps = 3.5107e-01, PNorm = 87.0013, GNorm = 0.2318
Meta loss on this task batch = 3.2061e-01, Meta loss averaged over last 500 steps = 3.5102e-01, PNorm = 87.0121, GNorm = 0.2332
Meta loss on this task batch = 3.2769e-01, Meta loss averaged over last 500 steps = 3.5104e-01, PNorm = 87.0235, GNorm = 0.2902
Meta loss on this task batch = 3.7155e-01, Meta loss averaged over last 500 steps = 3.5099e-01, PNorm = 87.0341, GNorm = 0.2528
Meta loss on this task batch = 3.8576e-01, Meta loss averaged over last 500 steps = 3.5107e-01, PNorm = 87.0434, GNorm = 0.2731
Meta loss on this task batch = 3.7024e-01, Meta loss averaged over last 500 steps = 3.5122e-01, PNorm = 87.0509, GNorm = 0.3172
Meta loss on this task batch = 3.1799e-01, Meta loss averaged over last 500 steps = 3.5123e-01, PNorm = 87.0598, GNorm = 0.2573
Meta loss on this task batch = 4.2989e-01, Meta loss averaged over last 500 steps = 3.5142e-01, PNorm = 87.0679, GNorm = 0.3016
Meta loss on this task batch = 3.0275e-01, Meta loss averaged over last 500 steps = 3.5123e-01, PNorm = 87.0754, GNorm = 0.2538
Meta loss on this task batch = 3.9626e-01, Meta loss averaged over last 500 steps = 3.5131e-01, PNorm = 87.0845, GNorm = 0.2998
Meta loss on this task batch = 3.7351e-01, Meta loss averaged over last 500 steps = 3.5131e-01, PNorm = 87.0942, GNorm = 0.3022
Meta loss on this task batch = 3.5456e-01, Meta loss averaged over last 500 steps = 3.5131e-01, PNorm = 87.1034, GNorm = 0.2746
Meta loss on this task batch = 3.4360e-01, Meta loss averaged over last 500 steps = 3.5128e-01, PNorm = 87.1133, GNorm = 0.2550
Meta loss on this task batch = 3.6924e-01, Meta loss averaged over last 500 steps = 3.5126e-01, PNorm = 87.1237, GNorm = 0.3080
Meta loss on this task batch = 3.4447e-01, Meta loss averaged over last 500 steps = 3.5113e-01, PNorm = 87.1339, GNorm = 0.2475
Meta loss on this task batch = 3.5322e-01, Meta loss averaged over last 500 steps = 3.5120e-01, PNorm = 87.1427, GNorm = 0.2781
Took 131.8162796497345 seconds to complete one epoch of meta training
Took 140.597021818161 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.534703
Found better MAML checkpoint after meta validation, saving now
Epoch 183
Meta loss on this task batch = 3.5525e-01, Meta loss averaged over last 500 steps = 3.5110e-01, PNorm = 87.1524, GNorm = 0.2452
Meta loss on this task batch = 3.9648e-01, Meta loss averaged over last 500 steps = 3.5130e-01, PNorm = 87.1621, GNorm = 0.3046
Meta loss on this task batch = 3.1783e-01, Meta loss averaged over last 500 steps = 3.5127e-01, PNorm = 87.1711, GNorm = 0.2462
Meta loss on this task batch = 3.5732e-01, Meta loss averaged over last 500 steps = 3.5119e-01, PNorm = 87.1795, GNorm = 0.2548
Meta loss on this task batch = 3.4198e-01, Meta loss averaged over last 500 steps = 3.5112e-01, PNorm = 87.1894, GNorm = 0.3001
Meta loss on this task batch = 3.4879e-01, Meta loss averaged over last 500 steps = 3.5111e-01, PNorm = 87.1978, GNorm = 0.2946
Meta loss on this task batch = 3.7599e-01, Meta loss averaged over last 500 steps = 3.5120e-01, PNorm = 87.2043, GNorm = 0.2913
Meta loss on this task batch = 3.0737e-01, Meta loss averaged over last 500 steps = 3.5120e-01, PNorm = 87.2127, GNorm = 0.2283
Meta loss on this task batch = 3.6899e-01, Meta loss averaged over last 500 steps = 3.5106e-01, PNorm = 87.2202, GNorm = 0.2727
Meta loss on this task batch = 3.6532e-01, Meta loss averaged over last 500 steps = 3.5106e-01, PNorm = 87.2285, GNorm = 0.2352
Meta loss on this task batch = 3.7551e-01, Meta loss averaged over last 500 steps = 3.5099e-01, PNorm = 87.2383, GNorm = 0.2350
Meta loss on this task batch = 3.7143e-01, Meta loss averaged over last 500 steps = 3.5103e-01, PNorm = 87.2500, GNorm = 0.2981
Meta loss on this task batch = 3.5392e-01, Meta loss averaged over last 500 steps = 3.5092e-01, PNorm = 87.2625, GNorm = 0.2769
Meta loss on this task batch = 2.6088e-01, Meta loss averaged over last 500 steps = 3.5075e-01, PNorm = 87.2770, GNorm = 0.2215
Meta loss on this task batch = 3.6872e-01, Meta loss averaged over last 500 steps = 3.5087e-01, PNorm = 87.2928, GNorm = 0.2550
Meta loss on this task batch = 3.4459e-01, Meta loss averaged over last 500 steps = 3.5089e-01, PNorm = 87.3075, GNorm = 0.2560
Meta loss on this task batch = 3.6383e-01, Meta loss averaged over last 500 steps = 3.5085e-01, PNorm = 87.3223, GNorm = 0.3051
Meta loss on this task batch = 2.9244e-01, Meta loss averaged over last 500 steps = 3.5079e-01, PNorm = 87.3349, GNorm = 0.2463
Meta loss on this task batch = 3.0852e-01, Meta loss averaged over last 500 steps = 3.5078e-01, PNorm = 87.3452, GNorm = 0.3091
Took 137.94083094596863 seconds to complete one epoch of meta training
Took 145.67337656021118 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502411
Epoch 184
Meta loss on this task batch = 3.1545e-01, Meta loss averaged over last 500 steps = 3.5073e-01, PNorm = 87.3539, GNorm = 0.2543
Meta loss on this task batch = 3.6471e-01, Meta loss averaged over last 500 steps = 3.5070e-01, PNorm = 87.3607, GNorm = 0.2740
Meta loss on this task batch = 2.4264e-01, Meta loss averaged over last 500 steps = 3.5040e-01, PNorm = 87.3689, GNorm = 0.2116
Meta loss on this task batch = 3.9439e-01, Meta loss averaged over last 500 steps = 3.5054e-01, PNorm = 87.3763, GNorm = 0.3119
Meta loss on this task batch = 3.2025e-01, Meta loss averaged over last 500 steps = 3.5048e-01, PNorm = 87.3836, GNorm = 0.2200
Meta loss on this task batch = 3.3293e-01, Meta loss averaged over last 500 steps = 3.5044e-01, PNorm = 87.3896, GNorm = 0.3175
Meta loss on this task batch = 2.9163e-01, Meta loss averaged over last 500 steps = 3.5023e-01, PNorm = 87.3966, GNorm = 0.2299
Meta loss on this task batch = 3.8815e-01, Meta loss averaged over last 500 steps = 3.5025e-01, PNorm = 87.4043, GNorm = 0.2522
Meta loss on this task batch = 3.4720e-01, Meta loss averaged over last 500 steps = 3.5024e-01, PNorm = 87.4135, GNorm = 0.2709
Meta loss on this task batch = 3.9622e-01, Meta loss averaged over last 500 steps = 3.5039e-01, PNorm = 87.4226, GNorm = 0.2574
Meta loss on this task batch = 3.6751e-01, Meta loss averaged over last 500 steps = 3.5045e-01, PNorm = 87.4314, GNorm = 0.2620
Meta loss on this task batch = 3.6593e-01, Meta loss averaged over last 500 steps = 3.5055e-01, PNorm = 87.4396, GNorm = 0.2522
Meta loss on this task batch = 3.3897e-01, Meta loss averaged over last 500 steps = 3.5054e-01, PNorm = 87.4477, GNorm = 0.2249
Meta loss on this task batch = 3.6663e-01, Meta loss averaged over last 500 steps = 3.5062e-01, PNorm = 87.4565, GNorm = 0.2783
Meta loss on this task batch = 3.4255e-01, Meta loss averaged over last 500 steps = 3.5043e-01, PNorm = 87.4637, GNorm = 0.3367
Meta loss on this task batch = 3.7356e-01, Meta loss averaged over last 500 steps = 3.5046e-01, PNorm = 87.4694, GNorm = 0.3263
Meta loss on this task batch = 3.8471e-01, Meta loss averaged over last 500 steps = 3.5051e-01, PNorm = 87.4731, GNorm = 0.3118
Meta loss on this task batch = 3.2251e-01, Meta loss averaged over last 500 steps = 3.5049e-01, PNorm = 87.4768, GNorm = 0.2943
Meta loss on this task batch = 3.6247e-01, Meta loss averaged over last 500 steps = 3.5046e-01, PNorm = 87.4791, GNorm = 0.3443
Took 131.0373260974884 seconds to complete one epoch of meta training
Took 138.61758065223694 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500967
Epoch 185
Meta loss on this task batch = 3.6797e-01, Meta loss averaged over last 500 steps = 3.5054e-01, PNorm = 87.4825, GNorm = 0.2902
Meta loss on this task batch = 3.3246e-01, Meta loss averaged over last 500 steps = 3.5045e-01, PNorm = 87.4871, GNorm = 0.2701
Meta loss on this task batch = 3.1315e-01, Meta loss averaged over last 500 steps = 3.5035e-01, PNorm = 87.4922, GNorm = 0.2755
Meta loss on this task batch = 3.6025e-01, Meta loss averaged over last 500 steps = 3.5030e-01, PNorm = 87.4963, GNorm = 0.2642
Meta loss on this task batch = 2.9731e-01, Meta loss averaged over last 500 steps = 3.5018e-01, PNorm = 87.5025, GNorm = 0.2376
Meta loss on this task batch = 3.1196e-01, Meta loss averaged over last 500 steps = 3.5010e-01, PNorm = 87.5080, GNorm = 0.2698
Meta loss on this task batch = 3.5346e-01, Meta loss averaged over last 500 steps = 3.5018e-01, PNorm = 87.5139, GNorm = 0.2788
Meta loss on this task batch = 3.1402e-01, Meta loss averaged over last 500 steps = 3.5015e-01, PNorm = 87.5215, GNorm = 0.2759
Meta loss on this task batch = 3.2982e-01, Meta loss averaged over last 500 steps = 3.5010e-01, PNorm = 87.5287, GNorm = 0.2763
Meta loss on this task batch = 3.6670e-01, Meta loss averaged over last 500 steps = 3.5026e-01, PNorm = 87.5359, GNorm = 0.3094
Meta loss on this task batch = 3.9321e-01, Meta loss averaged over last 500 steps = 3.5036e-01, PNorm = 87.5425, GNorm = 0.2836
Meta loss on this task batch = 3.8647e-01, Meta loss averaged over last 500 steps = 3.5033e-01, PNorm = 87.5494, GNorm = 0.3141
Meta loss on this task batch = 3.6136e-01, Meta loss averaged over last 500 steps = 3.5040e-01, PNorm = 87.5570, GNorm = 0.2267
Meta loss on this task batch = 3.5190e-01, Meta loss averaged over last 500 steps = 3.5038e-01, PNorm = 87.5654, GNorm = 0.2508
Meta loss on this task batch = 3.4247e-01, Meta loss averaged over last 500 steps = 3.5042e-01, PNorm = 87.5738, GNorm = 0.2520
Meta loss on this task batch = 3.1013e-01, Meta loss averaged over last 500 steps = 3.5037e-01, PNorm = 87.5816, GNorm = 0.2525
Meta loss on this task batch = 3.2055e-01, Meta loss averaged over last 500 steps = 3.5027e-01, PNorm = 87.5908, GNorm = 0.2765
Meta loss on this task batch = 3.9214e-01, Meta loss averaged over last 500 steps = 3.5031e-01, PNorm = 87.6000, GNorm = 0.2872
Meta loss on this task batch = 3.6254e-01, Meta loss averaged over last 500 steps = 3.5027e-01, PNorm = 87.6080, GNorm = 0.3435
Took 134.024418592453 seconds to complete one epoch of meta training
Took 141.76677703857422 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488798
Epoch 186
Meta loss on this task batch = 3.3135e-01, Meta loss averaged over last 500 steps = 3.5029e-01, PNorm = 87.6146, GNorm = 0.2559
Meta loss on this task batch = 3.2905e-01, Meta loss averaged over last 500 steps = 3.5020e-01, PNorm = 87.6196, GNorm = 0.2982
Meta loss on this task batch = 3.2921e-01, Meta loss averaged over last 500 steps = 3.5021e-01, PNorm = 87.6225, GNorm = 0.2528
Meta loss on this task batch = 3.1091e-01, Meta loss averaged over last 500 steps = 3.5020e-01, PNorm = 87.6264, GNorm = 0.2773
Meta loss on this task batch = 3.6587e-01, Meta loss averaged over last 500 steps = 3.5022e-01, PNorm = 87.6313, GNorm = 0.2901
Meta loss on this task batch = 4.0046e-01, Meta loss averaged over last 500 steps = 3.5036e-01, PNorm = 87.6356, GNorm = 0.3425
Meta loss on this task batch = 3.2713e-01, Meta loss averaged over last 500 steps = 3.5026e-01, PNorm = 87.6397, GNorm = 0.2455
Meta loss on this task batch = 2.8345e-01, Meta loss averaged over last 500 steps = 3.5016e-01, PNorm = 87.6452, GNorm = 0.2063
Meta loss on this task batch = 3.1022e-01, Meta loss averaged over last 500 steps = 3.5004e-01, PNorm = 87.6540, GNorm = 0.2595
Meta loss on this task batch = 4.3313e-01, Meta loss averaged over last 500 steps = 3.5017e-01, PNorm = 87.6636, GNorm = 0.3043
Meta loss on this task batch = 4.1188e-01, Meta loss averaged over last 500 steps = 3.5032e-01, PNorm = 87.6765, GNorm = 0.3153
Meta loss on this task batch = 3.4609e-01, Meta loss averaged over last 500 steps = 3.5035e-01, PNorm = 87.6901, GNorm = 0.2513
Meta loss on this task batch = 3.3103e-01, Meta loss averaged over last 500 steps = 3.5042e-01, PNorm = 87.7042, GNorm = 0.2744
Meta loss on this task batch = 3.2286e-01, Meta loss averaged over last 500 steps = 3.5038e-01, PNorm = 87.7141, GNorm = 0.2489
Meta loss on this task batch = 3.3597e-01, Meta loss averaged over last 500 steps = 3.5043e-01, PNorm = 87.7221, GNorm = 0.2821
Meta loss on this task batch = 2.8267e-01, Meta loss averaged over last 500 steps = 3.5023e-01, PNorm = 87.7300, GNorm = 0.2617
Meta loss on this task batch = 4.0811e-01, Meta loss averaged over last 500 steps = 3.5020e-01, PNorm = 87.7356, GNorm = 0.3436
Meta loss on this task batch = 3.2962e-01, Meta loss averaged over last 500 steps = 3.5017e-01, PNorm = 87.7386, GNorm = 0.2903
Meta loss on this task batch = 3.9419e-01, Meta loss averaged over last 500 steps = 3.5025e-01, PNorm = 87.7379, GNorm = 0.3617
Took 134.5722599029541 seconds to complete one epoch of meta training
Took 143.02701807022095 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489926
Epoch 187
Meta loss on this task batch = 4.0986e-01, Meta loss averaged over last 500 steps = 3.5032e-01, PNorm = 87.7347, GNorm = 0.3088
Meta loss on this task batch = 3.6024e-01, Meta loss averaged over last 500 steps = 3.5033e-01, PNorm = 87.7337, GNorm = 0.2346
Meta loss on this task batch = 3.6697e-01, Meta loss averaged over last 500 steps = 3.5033e-01, PNorm = 87.7344, GNorm = 0.3126
Meta loss on this task batch = 2.9656e-01, Meta loss averaged over last 500 steps = 3.5013e-01, PNorm = 87.7372, GNorm = 0.2371
Meta loss on this task batch = 3.7462e-01, Meta loss averaged over last 500 steps = 3.5023e-01, PNorm = 87.7442, GNorm = 0.2845
Meta loss on this task batch = 3.2669e-01, Meta loss averaged over last 500 steps = 3.5019e-01, PNorm = 87.7562, GNorm = 0.2902
Meta loss on this task batch = 3.4802e-01, Meta loss averaged over last 500 steps = 3.5025e-01, PNorm = 87.7690, GNorm = 0.2657
Meta loss on this task batch = 2.9204e-01, Meta loss averaged over last 500 steps = 3.5014e-01, PNorm = 87.7832, GNorm = 0.2402
Meta loss on this task batch = 3.2675e-01, Meta loss averaged over last 500 steps = 3.5009e-01, PNorm = 87.7987, GNorm = 0.2576
Meta loss on this task batch = 2.9842e-01, Meta loss averaged over last 500 steps = 3.4999e-01, PNorm = 87.8139, GNorm = 0.2245
Meta loss on this task batch = 3.6786e-01, Meta loss averaged over last 500 steps = 3.5017e-01, PNorm = 87.8301, GNorm = 0.2890
Meta loss on this task batch = 3.3935e-01, Meta loss averaged over last 500 steps = 3.5011e-01, PNorm = 87.8456, GNorm = 0.2776
Meta loss on this task batch = 3.6843e-01, Meta loss averaged over last 500 steps = 3.5011e-01, PNorm = 87.8605, GNorm = 0.2941
Meta loss on this task batch = 3.9876e-01, Meta loss averaged over last 500 steps = 3.5020e-01, PNorm = 87.8721, GNorm = 0.3197
Meta loss on this task batch = 3.3374e-01, Meta loss averaged over last 500 steps = 3.5010e-01, PNorm = 87.8824, GNorm = 0.2454
Meta loss on this task batch = 3.2945e-01, Meta loss averaged over last 500 steps = 3.5011e-01, PNorm = 87.8921, GNorm = 0.2909
Meta loss on this task batch = 3.5853e-01, Meta loss averaged over last 500 steps = 3.5010e-01, PNorm = 87.9016, GNorm = 0.2671
Meta loss on this task batch = 3.5987e-01, Meta loss averaged over last 500 steps = 3.5014e-01, PNorm = 87.9107, GNorm = 0.3248
Meta loss on this task batch = 3.0663e-01, Meta loss averaged over last 500 steps = 3.5004e-01, PNorm = 87.9211, GNorm = 0.3137
Took 129.28975987434387 seconds to complete one epoch of meta training
Took 137.80288004875183 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482616
Epoch 188
Meta loss on this task batch = 3.4048e-01, Meta loss averaged over last 500 steps = 3.4997e-01, PNorm = 87.9270, GNorm = 0.2885
Meta loss on this task batch = 3.6239e-01, Meta loss averaged over last 500 steps = 3.5003e-01, PNorm = 87.9314, GNorm = 0.2727
Meta loss on this task batch = 3.6167e-01, Meta loss averaged over last 500 steps = 3.4999e-01, PNorm = 87.9342, GNorm = 0.2541
Meta loss on this task batch = 3.5081e-01, Meta loss averaged over last 500 steps = 3.4999e-01, PNorm = 87.9382, GNorm = 0.2617
Meta loss on this task batch = 3.2744e-01, Meta loss averaged over last 500 steps = 3.4976e-01, PNorm = 87.9414, GNorm = 0.2794
Meta loss on this task batch = 3.4014e-01, Meta loss averaged over last 500 steps = 3.4972e-01, PNorm = 87.9435, GNorm = 0.2627
Meta loss on this task batch = 3.4313e-01, Meta loss averaged over last 500 steps = 3.4963e-01, PNorm = 87.9434, GNorm = 0.2851
Meta loss on this task batch = 3.3577e-01, Meta loss averaged over last 500 steps = 3.4961e-01, PNorm = 87.9455, GNorm = 0.2950
Meta loss on this task batch = 3.3955e-01, Meta loss averaged over last 500 steps = 3.4961e-01, PNorm = 87.9500, GNorm = 0.3290
Meta loss on this task batch = 3.4380e-01, Meta loss averaged over last 500 steps = 3.4951e-01, PNorm = 87.9547, GNorm = 0.2630
Meta loss on this task batch = 3.5387e-01, Meta loss averaged over last 500 steps = 3.4963e-01, PNorm = 87.9562, GNorm = 0.2461
Meta loss on this task batch = 3.0666e-01, Meta loss averaged over last 500 steps = 3.4945e-01, PNorm = 87.9594, GNorm = 0.2565
Meta loss on this task batch = 3.5068e-01, Meta loss averaged over last 500 steps = 3.4937e-01, PNorm = 87.9641, GNorm = 0.2724
Meta loss on this task batch = 3.5611e-01, Meta loss averaged over last 500 steps = 3.4937e-01, PNorm = 87.9707, GNorm = 0.2833
Meta loss on this task batch = 3.7842e-01, Meta loss averaged over last 500 steps = 3.4948e-01, PNorm = 87.9777, GNorm = 0.2754
Meta loss on this task batch = 3.3019e-01, Meta loss averaged over last 500 steps = 3.4943e-01, PNorm = 87.9856, GNorm = 0.2694
Meta loss on this task batch = 3.5879e-01, Meta loss averaged over last 500 steps = 3.4939e-01, PNorm = 87.9932, GNorm = 0.2707
Meta loss on this task batch = 3.6889e-01, Meta loss averaged over last 500 steps = 3.4954e-01, PNorm = 88.0005, GNorm = 0.2752
Meta loss on this task batch = 2.9884e-01, Meta loss averaged over last 500 steps = 3.4940e-01, PNorm = 88.0092, GNorm = 0.2679
Took 131.15152740478516 seconds to complete one epoch of meta training
Took 139.63229060173035 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508987
Epoch 189
Meta loss on this task batch = 2.1965e-01, Meta loss averaged over last 500 steps = 3.4919e-01, PNorm = 88.0199, GNorm = 0.2202
Meta loss on this task batch = 3.3337e-01, Meta loss averaged over last 500 steps = 3.4921e-01, PNorm = 88.0308, GNorm = 0.2783
Meta loss on this task batch = 3.5595e-01, Meta loss averaged over last 500 steps = 3.4921e-01, PNorm = 88.0416, GNorm = 0.2954
Meta loss on this task batch = 2.8507e-01, Meta loss averaged over last 500 steps = 3.4910e-01, PNorm = 88.0503, GNorm = 0.2706
Meta loss on this task batch = 3.3673e-01, Meta loss averaged over last 500 steps = 3.4902e-01, PNorm = 88.0585, GNorm = 0.2935
Meta loss on this task batch = 3.4592e-01, Meta loss averaged over last 500 steps = 3.4897e-01, PNorm = 88.0640, GNorm = 0.3411
Meta loss on this task batch = 4.0911e-01, Meta loss averaged over last 500 steps = 3.4921e-01, PNorm = 88.0695, GNorm = 0.2978
Meta loss on this task batch = 3.0846e-01, Meta loss averaged over last 500 steps = 3.4920e-01, PNorm = 88.0758, GNorm = 0.2494
Meta loss on this task batch = 3.2263e-01, Meta loss averaged over last 500 steps = 3.4910e-01, PNorm = 88.0836, GNorm = 0.2373
Meta loss on this task batch = 3.6109e-01, Meta loss averaged over last 500 steps = 3.4902e-01, PNorm = 88.0916, GNorm = 0.2641
Meta loss on this task batch = 3.9118e-01, Meta loss averaged over last 500 steps = 3.4912e-01, PNorm = 88.1006, GNorm = 0.2505
Meta loss on this task batch = 3.4536e-01, Meta loss averaged over last 500 steps = 3.4912e-01, PNorm = 88.1102, GNorm = 0.2615
Meta loss on this task batch = 3.6133e-01, Meta loss averaged over last 500 steps = 3.4906e-01, PNorm = 88.1190, GNorm = 0.2588
Meta loss on this task batch = 3.5825e-01, Meta loss averaged over last 500 steps = 3.4921e-01, PNorm = 88.1272, GNorm = 0.2604
Meta loss on this task batch = 3.8812e-01, Meta loss averaged over last 500 steps = 3.4921e-01, PNorm = 88.1341, GNorm = 0.2825
Meta loss on this task batch = 3.5341e-01, Meta loss averaged over last 500 steps = 3.4932e-01, PNorm = 88.1413, GNorm = 0.2335
Meta loss on this task batch = 3.2596e-01, Meta loss averaged over last 500 steps = 3.4932e-01, PNorm = 88.1502, GNorm = 0.2227
Meta loss on this task batch = 3.6533e-01, Meta loss averaged over last 500 steps = 3.4932e-01, PNorm = 88.1588, GNorm = 0.2503
Meta loss on this task batch = 3.7696e-01, Meta loss averaged over last 500 steps = 3.4947e-01, PNorm = 88.1691, GNorm = 0.3060
Took 130.7919511795044 seconds to complete one epoch of meta training
Took 139.5783040523529 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500111
Epoch 190
Meta loss on this task batch = 3.4816e-01, Meta loss averaged over last 500 steps = 3.4947e-01, PNorm = 88.1796, GNorm = 0.2560
Meta loss on this task batch = 3.2526e-01, Meta loss averaged over last 500 steps = 3.4941e-01, PNorm = 88.1894, GNorm = 0.2328
Meta loss on this task batch = 3.8310e-01, Meta loss averaged over last 500 steps = 3.4943e-01, PNorm = 88.1983, GNorm = 0.2745
Meta loss on this task batch = 3.7815e-01, Meta loss averaged over last 500 steps = 3.4937e-01, PNorm = 88.2075, GNorm = 0.2812
Meta loss on this task batch = 3.8378e-01, Meta loss averaged over last 500 steps = 3.4941e-01, PNorm = 88.2167, GNorm = 0.2806
Meta loss on this task batch = 3.1250e-01, Meta loss averaged over last 500 steps = 3.4925e-01, PNorm = 88.2271, GNorm = 0.2115
Meta loss on this task batch = 3.3938e-01, Meta loss averaged over last 500 steps = 3.4915e-01, PNorm = 88.2393, GNorm = 0.2575
Meta loss on this task batch = 3.0389e-01, Meta loss averaged over last 500 steps = 3.4906e-01, PNorm = 88.2502, GNorm = 0.3172
Meta loss on this task batch = 3.0072e-01, Meta loss averaged over last 500 steps = 3.4907e-01, PNorm = 88.2582, GNorm = 0.2596
Meta loss on this task batch = 3.1698e-01, Meta loss averaged over last 500 steps = 3.4905e-01, PNorm = 88.2643, GNorm = 0.2787
Meta loss on this task batch = 3.4153e-01, Meta loss averaged over last 500 steps = 3.4903e-01, PNorm = 88.2674, GNorm = 0.3018
Meta loss on this task batch = 3.7047e-01, Meta loss averaged over last 500 steps = 3.4909e-01, PNorm = 88.2698, GNorm = 0.3105
Meta loss on this task batch = 3.1672e-01, Meta loss averaged over last 500 steps = 3.4913e-01, PNorm = 88.2717, GNorm = 0.2840
Meta loss on this task batch = 3.1768e-01, Meta loss averaged over last 500 steps = 3.4906e-01, PNorm = 88.2742, GNorm = 0.2370
Meta loss on this task batch = 3.6535e-01, Meta loss averaged over last 500 steps = 3.4899e-01, PNorm = 88.2779, GNorm = 0.2989
Meta loss on this task batch = 3.5386e-01, Meta loss averaged over last 500 steps = 3.4889e-01, PNorm = 88.2826, GNorm = 0.2757
Meta loss on this task batch = 3.0215e-01, Meta loss averaged over last 500 steps = 3.4882e-01, PNorm = 88.2891, GNorm = 0.2321
Meta loss on this task batch = 3.7352e-01, Meta loss averaged over last 500 steps = 3.4880e-01, PNorm = 88.2961, GNorm = 0.2891
Meta loss on this task batch = 3.2105e-01, Meta loss averaged over last 500 steps = 3.4881e-01, PNorm = 88.3066, GNorm = 0.3496
Took 134.77306485176086 seconds to complete one epoch of meta training
Took 142.14718890190125 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509889
Epoch 191
Meta loss on this task batch = 3.4209e-01, Meta loss averaged over last 500 steps = 3.4882e-01, PNorm = 88.3162, GNorm = 0.2775
Meta loss on this task batch = 3.8586e-01, Meta loss averaged over last 500 steps = 3.4888e-01, PNorm = 88.3254, GNorm = 0.2542
Meta loss on this task batch = 3.5378e-01, Meta loss averaged over last 500 steps = 3.4889e-01, PNorm = 88.3343, GNorm = 0.2578
Meta loss on this task batch = 3.4447e-01, Meta loss averaged over last 500 steps = 3.4876e-01, PNorm = 88.3395, GNorm = 0.2675
Meta loss on this task batch = 3.9399e-01, Meta loss averaged over last 500 steps = 3.4888e-01, PNorm = 88.3445, GNorm = 0.3347
Meta loss on this task batch = 3.3312e-01, Meta loss averaged over last 500 steps = 3.4874e-01, PNorm = 88.3489, GNorm = 0.3081
Meta loss on this task batch = 3.3262e-01, Meta loss averaged over last 500 steps = 3.4879e-01, PNorm = 88.3550, GNorm = 0.2573
Meta loss on this task batch = 3.0161e-01, Meta loss averaged over last 500 steps = 3.4859e-01, PNorm = 88.3630, GNorm = 0.2380
Meta loss on this task batch = 3.1725e-01, Meta loss averaged over last 500 steps = 3.4851e-01, PNorm = 88.3725, GNorm = 0.2439
Meta loss on this task batch = 3.3594e-01, Meta loss averaged over last 500 steps = 3.4837e-01, PNorm = 88.3820, GNorm = 0.2595
Meta loss on this task batch = 3.2097e-01, Meta loss averaged over last 500 steps = 3.4832e-01, PNorm = 88.3924, GNorm = 0.2859
Meta loss on this task batch = 3.6365e-01, Meta loss averaged over last 500 steps = 3.4832e-01, PNorm = 88.4018, GNorm = 0.2709
Meta loss on this task batch = 3.0641e-01, Meta loss averaged over last 500 steps = 3.4827e-01, PNorm = 88.4120, GNorm = 0.2514
Meta loss on this task batch = 3.1564e-01, Meta loss averaged over last 500 steps = 3.4824e-01, PNorm = 88.4211, GNorm = 0.2738
Meta loss on this task batch = 4.0579e-01, Meta loss averaged over last 500 steps = 3.4834e-01, PNorm = 88.4294, GNorm = 0.3428
Meta loss on this task batch = 2.8957e-01, Meta loss averaged over last 500 steps = 3.4821e-01, PNorm = 88.4391, GNorm = 0.2382
Meta loss on this task batch = 3.7735e-01, Meta loss averaged over last 500 steps = 3.4830e-01, PNorm = 88.4481, GNorm = 0.3078
Meta loss on this task batch = 3.5258e-01, Meta loss averaged over last 500 steps = 3.4830e-01, PNorm = 88.4577, GNorm = 0.2775
Meta loss on this task batch = 3.3409e-01, Meta loss averaged over last 500 steps = 3.4827e-01, PNorm = 88.4715, GNorm = 0.3725
Took 132.66710138320923 seconds to complete one epoch of meta training
Took 141.00562071800232 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494443
Epoch 192
Meta loss on this task batch = 3.3288e-01, Meta loss averaged over last 500 steps = 3.4823e-01, PNorm = 88.4862, GNorm = 0.2472
Meta loss on this task batch = 3.6819e-01, Meta loss averaged over last 500 steps = 3.4829e-01, PNorm = 88.4983, GNorm = 0.2923
Meta loss on this task batch = 3.9324e-01, Meta loss averaged over last 500 steps = 3.4839e-01, PNorm = 88.5086, GNorm = 0.3203
Meta loss on this task batch = 3.1548e-01, Meta loss averaged over last 500 steps = 3.4827e-01, PNorm = 88.5172, GNorm = 0.3198
Meta loss on this task batch = 3.2521e-01, Meta loss averaged over last 500 steps = 3.4828e-01, PNorm = 88.5224, GNorm = 0.3462
Meta loss on this task batch = 3.4953e-01, Meta loss averaged over last 500 steps = 3.4816e-01, PNorm = 88.5257, GNorm = 0.2919
Meta loss on this task batch = 3.9993e-01, Meta loss averaged over last 500 steps = 3.4829e-01, PNorm = 88.5286, GNorm = 0.3538
Meta loss on this task batch = 3.6040e-01, Meta loss averaged over last 500 steps = 3.4831e-01, PNorm = 88.5291, GNorm = 0.3321
Meta loss on this task batch = 3.3946e-01, Meta loss averaged over last 500 steps = 3.4819e-01, PNorm = 88.5297, GNorm = 0.2725
Meta loss on this task batch = 3.0330e-01, Meta loss averaged over last 500 steps = 3.4804e-01, PNorm = 88.5329, GNorm = 0.3074
Meta loss on this task batch = 3.1775e-01, Meta loss averaged over last 500 steps = 3.4792e-01, PNorm = 88.5367, GNorm = 0.2251
Meta loss on this task batch = 3.3470e-01, Meta loss averaged over last 500 steps = 3.4781e-01, PNorm = 88.5416, GNorm = 0.2318
Meta loss on this task batch = 3.3547e-01, Meta loss averaged over last 500 steps = 3.4786e-01, PNorm = 88.5491, GNorm = 0.2950
Meta loss on this task batch = 3.9537e-01, Meta loss averaged over last 500 steps = 3.4800e-01, PNorm = 88.5576, GNorm = 0.2673
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 3.4791e-01, PNorm = 88.5685, GNorm = 0.2388
Meta loss on this task batch = 3.3618e-01, Meta loss averaged over last 500 steps = 3.4796e-01, PNorm = 88.5793, GNorm = 0.2435
Meta loss on this task batch = 3.0950e-01, Meta loss averaged over last 500 steps = 3.4783e-01, PNorm = 88.5887, GNorm = 0.2383
Meta loss on this task batch = 2.9325e-01, Meta loss averaged over last 500 steps = 3.4768e-01, PNorm = 88.5988, GNorm = 0.2263
Meta loss on this task batch = 2.4552e-01, Meta loss averaged over last 500 steps = 3.4744e-01, PNorm = 88.6076, GNorm = 0.2827
Took 135.1541726589203 seconds to complete one epoch of meta training
Took 143.84610557556152 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495074
Epoch 193
Meta loss on this task batch = 3.9424e-01, Meta loss averaged over last 500 steps = 3.4762e-01, PNorm = 88.6145, GNorm = 0.3064
Meta loss on this task batch = 3.7032e-01, Meta loss averaged over last 500 steps = 3.4771e-01, PNorm = 88.6222, GNorm = 0.2961
Meta loss on this task batch = 3.3141e-01, Meta loss averaged over last 500 steps = 3.4768e-01, PNorm = 88.6322, GNorm = 0.2660
Meta loss on this task batch = 3.2748e-01, Meta loss averaged over last 500 steps = 3.4760e-01, PNorm = 88.6413, GNorm = 0.2730
Meta loss on this task batch = 3.4501e-01, Meta loss averaged over last 500 steps = 3.4744e-01, PNorm = 88.6508, GNorm = 0.2704
Meta loss on this task batch = 4.1735e-01, Meta loss averaged over last 500 steps = 3.4772e-01, PNorm = 88.6603, GNorm = 0.3452
Meta loss on this task batch = 4.0987e-01, Meta loss averaged over last 500 steps = 3.4781e-01, PNorm = 88.6690, GNorm = 0.2860
Meta loss on this task batch = 2.8630e-01, Meta loss averaged over last 500 steps = 3.4764e-01, PNorm = 88.6794, GNorm = 0.2499
Meta loss on this task batch = 3.6367e-01, Meta loss averaged over last 500 steps = 3.4759e-01, PNorm = 88.6911, GNorm = 0.3150
Meta loss on this task batch = 3.1670e-01, Meta loss averaged over last 500 steps = 3.4750e-01, PNorm = 88.7053, GNorm = 0.2991
Meta loss on this task batch = 3.2195e-01, Meta loss averaged over last 500 steps = 3.4748e-01, PNorm = 88.7179, GNorm = 0.2759
Meta loss on this task batch = 2.8703e-01, Meta loss averaged over last 500 steps = 3.4726e-01, PNorm = 88.7298, GNorm = 0.2601
Meta loss on this task batch = 3.4548e-01, Meta loss averaged over last 500 steps = 3.4747e-01, PNorm = 88.7404, GNorm = 0.2774
Meta loss on this task batch = 3.4807e-01, Meta loss averaged over last 500 steps = 3.4749e-01, PNorm = 88.7494, GNorm = 0.2668
Meta loss on this task batch = 3.8817e-01, Meta loss averaged over last 500 steps = 3.4756e-01, PNorm = 88.7559, GNorm = 0.3258
Meta loss on this task batch = 3.4226e-01, Meta loss averaged over last 500 steps = 3.4752e-01, PNorm = 88.7626, GNorm = 0.3015
Meta loss on this task batch = 3.5784e-01, Meta loss averaged over last 500 steps = 3.4755e-01, PNorm = 88.7675, GNorm = 0.3090
Meta loss on this task batch = 3.0741e-01, Meta loss averaged over last 500 steps = 3.4754e-01, PNorm = 88.7736, GNorm = 0.2579
Meta loss on this task batch = 3.4412e-01, Meta loss averaged over last 500 steps = 3.4752e-01, PNorm = 88.7800, GNorm = 0.3153
Took 132.7143428325653 seconds to complete one epoch of meta training
Took 141.64637756347656 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489566
Epoch 194
Meta loss on this task batch = 3.5466e-01, Meta loss averaged over last 500 steps = 3.4748e-01, PNorm = 88.7871, GNorm = 0.2685
Meta loss on this task batch = 3.1702e-01, Meta loss averaged over last 500 steps = 3.4743e-01, PNorm = 88.7954, GNorm = 0.2413
Meta loss on this task batch = 3.7574e-01, Meta loss averaged over last 500 steps = 3.4744e-01, PNorm = 88.8048, GNorm = 0.2614
Meta loss on this task batch = 3.4259e-01, Meta loss averaged over last 500 steps = 3.4747e-01, PNorm = 88.8150, GNorm = 0.2649
Meta loss on this task batch = 3.7586e-01, Meta loss averaged over last 500 steps = 3.4748e-01, PNorm = 88.8250, GNorm = 0.2557
Meta loss on this task batch = 4.0174e-01, Meta loss averaged over last 500 steps = 3.4755e-01, PNorm = 88.8341, GNorm = 0.3459
Meta loss on this task batch = 3.5196e-01, Meta loss averaged over last 500 steps = 3.4748e-01, PNorm = 88.8447, GNorm = 0.2871
Meta loss on this task batch = 3.5057e-01, Meta loss averaged over last 500 steps = 3.4757e-01, PNorm = 88.8554, GNorm = 0.2633
Meta loss on this task batch = 3.2066e-01, Meta loss averaged over last 500 steps = 3.4748e-01, PNorm = 88.8650, GNorm = 0.2354
Meta loss on this task batch = 2.9295e-01, Meta loss averaged over last 500 steps = 3.4743e-01, PNorm = 88.8760, GNorm = 0.2529
Meta loss on this task batch = 3.6777e-01, Meta loss averaged over last 500 steps = 3.4754e-01, PNorm = 88.8848, GNorm = 0.3532
Meta loss on this task batch = 3.0435e-01, Meta loss averaged over last 500 steps = 3.4735e-01, PNorm = 88.8924, GNorm = 0.2566
Meta loss on this task batch = 3.3870e-01, Meta loss averaged over last 500 steps = 3.4732e-01, PNorm = 88.9017, GNorm = 0.2713
Meta loss on this task batch = 3.5109e-01, Meta loss averaged over last 500 steps = 3.4725e-01, PNorm = 88.9111, GNorm = 0.2535
Meta loss on this task batch = 3.3666e-01, Meta loss averaged over last 500 steps = 3.4732e-01, PNorm = 88.9201, GNorm = 0.2919
Meta loss on this task batch = 3.2741e-01, Meta loss averaged over last 500 steps = 3.4729e-01, PNorm = 88.9284, GNorm = 0.2791
Meta loss on this task batch = 3.2839e-01, Meta loss averaged over last 500 steps = 3.4723e-01, PNorm = 88.9375, GNorm = 0.2802
Meta loss on this task batch = 2.9693e-01, Meta loss averaged over last 500 steps = 3.4708e-01, PNorm = 88.9475, GNorm = 0.2848
Meta loss on this task batch = 3.0437e-01, Meta loss averaged over last 500 steps = 3.4701e-01, PNorm = 88.9582, GNorm = 0.4157
Took 132.40218257904053 seconds to complete one epoch of meta training
Took 141.0578408241272 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490401
Epoch 195
Meta loss on this task batch = 2.8209e-01, Meta loss averaged over last 500 steps = 3.4692e-01, PNorm = 88.9686, GNorm = 0.2746
Meta loss on this task batch = 3.7899e-01, Meta loss averaged over last 500 steps = 3.4698e-01, PNorm = 88.9779, GNorm = 0.3253
Meta loss on this task batch = 2.8595e-01, Meta loss averaged over last 500 steps = 3.4689e-01, PNorm = 88.9857, GNorm = 0.2672
Meta loss on this task batch = 3.9183e-01, Meta loss averaged over last 500 steps = 3.4697e-01, PNorm = 88.9900, GNorm = 0.3322
Meta loss on this task batch = 3.3833e-01, Meta loss averaged over last 500 steps = 3.4704e-01, PNorm = 88.9933, GNorm = 0.3177
Meta loss on this task batch = 3.5307e-01, Meta loss averaged over last 500 steps = 3.4696e-01, PNorm = 88.9967, GNorm = 0.2853
Meta loss on this task batch = 3.4150e-01, Meta loss averaged over last 500 steps = 3.4688e-01, PNorm = 88.9961, GNorm = 0.3347
Meta loss on this task batch = 2.9704e-01, Meta loss averaged over last 500 steps = 3.4683e-01, PNorm = 88.9932, GNorm = 0.3213
Meta loss on this task batch = 3.2862e-01, Meta loss averaged over last 500 steps = 3.4678e-01, PNorm = 88.9938, GNorm = 0.3571
Meta loss on this task batch = 3.3870e-01, Meta loss averaged over last 500 steps = 3.4674e-01, PNorm = 88.9952, GNorm = 0.2541
Meta loss on this task batch = 3.0800e-01, Meta loss averaged over last 500 steps = 3.4677e-01, PNorm = 88.9989, GNorm = 0.2634
Meta loss on this task batch = 3.9206e-01, Meta loss averaged over last 500 steps = 3.4680e-01, PNorm = 89.0039, GNorm = 0.2831
Meta loss on this task batch = 3.2781e-01, Meta loss averaged over last 500 steps = 3.4677e-01, PNorm = 89.0083, GNorm = 0.2567
Meta loss on this task batch = 3.8138e-01, Meta loss averaged over last 500 steps = 3.4679e-01, PNorm = 89.0148, GNorm = 0.2933
Meta loss on this task batch = 3.1475e-01, Meta loss averaged over last 500 steps = 3.4669e-01, PNorm = 89.0218, GNorm = 0.2364
Meta loss on this task batch = 3.5097e-01, Meta loss averaged over last 500 steps = 3.4659e-01, PNorm = 89.0269, GNorm = 0.2952
Meta loss on this task batch = 3.8902e-01, Meta loss averaged over last 500 steps = 3.4670e-01, PNorm = 89.0308, GNorm = 0.3231
Meta loss on this task batch = 3.6520e-01, Meta loss averaged over last 500 steps = 3.4670e-01, PNorm = 89.0353, GNorm = 0.2887
Meta loss on this task batch = 3.6661e-01, Meta loss averaged over last 500 steps = 3.4664e-01, PNorm = 89.0396, GNorm = 0.3019
Took 137.0927300453186 seconds to complete one epoch of meta training
Took 145.62721467018127 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479771
Epoch 196
Meta loss on this task batch = 3.2473e-01, Meta loss averaged over last 500 steps = 3.4666e-01, PNorm = 89.0461, GNorm = 0.2835
Meta loss on this task batch = 3.6599e-01, Meta loss averaged over last 500 steps = 3.4680e-01, PNorm = 89.0522, GNorm = 0.2859
Meta loss on this task batch = 2.7998e-01, Meta loss averaged over last 500 steps = 3.4664e-01, PNorm = 89.0591, GNorm = 0.2215
Meta loss on this task batch = 4.1163e-01, Meta loss averaged over last 500 steps = 3.4674e-01, PNorm = 89.0661, GNorm = 0.2945
Meta loss on this task batch = 3.3520e-01, Meta loss averaged over last 500 steps = 3.4673e-01, PNorm = 89.0740, GNorm = 0.2579
Meta loss on this task batch = 3.6506e-01, Meta loss averaged over last 500 steps = 3.4653e-01, PNorm = 89.0829, GNorm = 0.3024
Meta loss on this task batch = 3.8017e-01, Meta loss averaged over last 500 steps = 3.4659e-01, PNorm = 89.0939, GNorm = 0.2731
Meta loss on this task batch = 2.8743e-01, Meta loss averaged over last 500 steps = 3.4654e-01, PNorm = 89.1070, GNorm = 0.2641
Meta loss on this task batch = 3.0816e-01, Meta loss averaged over last 500 steps = 3.4647e-01, PNorm = 89.1207, GNorm = 0.2779
Meta loss on this task batch = 3.6285e-01, Meta loss averaged over last 500 steps = 3.4652e-01, PNorm = 89.1333, GNorm = 0.2906
Meta loss on this task batch = 3.3822e-01, Meta loss averaged over last 500 steps = 3.4645e-01, PNorm = 89.1446, GNorm = 0.2831
Meta loss on this task batch = 3.1429e-01, Meta loss averaged over last 500 steps = 3.4639e-01, PNorm = 89.1545, GNorm = 0.3262
Meta loss on this task batch = 2.9861e-01, Meta loss averaged over last 500 steps = 3.4630e-01, PNorm = 89.1621, GNorm = 0.2830
Meta loss on this task batch = 3.6557e-01, Meta loss averaged over last 500 steps = 3.4633e-01, PNorm = 89.1668, GNorm = 0.3478
Meta loss on this task batch = 3.7013e-01, Meta loss averaged over last 500 steps = 3.4634e-01, PNorm = 89.1713, GNorm = 0.3005
Meta loss on this task batch = 3.5701e-01, Meta loss averaged over last 500 steps = 3.4634e-01, PNorm = 89.1754, GNorm = 0.3106
Meta loss on this task batch = 2.7331e-01, Meta loss averaged over last 500 steps = 3.4616e-01, PNorm = 89.1805, GNorm = 0.2526
Meta loss on this task batch = 3.4375e-01, Meta loss averaged over last 500 steps = 3.4616e-01, PNorm = 89.1862, GNorm = 0.3126
Meta loss on this task batch = 3.0723e-01, Meta loss averaged over last 500 steps = 3.4601e-01, PNorm = 89.1931, GNorm = 0.3074
Took 132.27344250679016 seconds to complete one epoch of meta training
Took 141.0273540019989 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476380
Epoch 197
Meta loss on this task batch = 3.4759e-01, Meta loss averaged over last 500 steps = 3.4601e-01, PNorm = 89.2011, GNorm = 0.2622
Meta loss on this task batch = 3.3239e-01, Meta loss averaged over last 500 steps = 3.4600e-01, PNorm = 89.2111, GNorm = 0.2590
Meta loss on this task batch = 3.2946e-01, Meta loss averaged over last 500 steps = 3.4589e-01, PNorm = 89.2213, GNorm = 0.2456
Meta loss on this task batch = 3.3848e-01, Meta loss averaged over last 500 steps = 3.4584e-01, PNorm = 89.2301, GNorm = 0.2676
Meta loss on this task batch = 3.7908e-01, Meta loss averaged over last 500 steps = 3.4596e-01, PNorm = 89.2381, GNorm = 0.2926
Meta loss on this task batch = 2.8261e-01, Meta loss averaged over last 500 steps = 3.4569e-01, PNorm = 89.2471, GNorm = 0.2427
Meta loss on this task batch = 3.5074e-01, Meta loss averaged over last 500 steps = 3.4577e-01, PNorm = 89.2547, GNorm = 0.2839
Meta loss on this task batch = 2.7162e-01, Meta loss averaged over last 500 steps = 3.4554e-01, PNorm = 89.2638, GNorm = 0.2256
Meta loss on this task batch = 3.4213e-01, Meta loss averaged over last 500 steps = 3.4544e-01, PNorm = 89.2723, GNorm = 0.2768
Meta loss on this task batch = 3.3429e-01, Meta loss averaged over last 500 steps = 3.4537e-01, PNorm = 89.2791, GNorm = 0.3131
Meta loss on this task batch = 3.1257e-01, Meta loss averaged over last 500 steps = 3.4523e-01, PNorm = 89.2886, GNorm = 0.2568
Meta loss on this task batch = 3.3557e-01, Meta loss averaged over last 500 steps = 3.4529e-01, PNorm = 89.2993, GNorm = 0.2582
Meta loss on this task batch = 3.0690e-01, Meta loss averaged over last 500 steps = 3.4519e-01, PNorm = 89.3105, GNorm = 0.2774
Meta loss on this task batch = 3.7304e-01, Meta loss averaged over last 500 steps = 3.4529e-01, PNorm = 89.3226, GNorm = 0.3162
Meta loss on this task batch = 3.0719e-01, Meta loss averaged over last 500 steps = 3.4514e-01, PNorm = 89.3339, GNorm = 0.2562
Meta loss on this task batch = 3.4560e-01, Meta loss averaged over last 500 steps = 3.4518e-01, PNorm = 89.3453, GNorm = 0.2647
Meta loss on this task batch = 3.4721e-01, Meta loss averaged over last 500 steps = 3.4517e-01, PNorm = 89.3546, GNorm = 0.2795
Meta loss on this task batch = 3.9198e-01, Meta loss averaged over last 500 steps = 3.4541e-01, PNorm = 89.3641, GNorm = 0.3209
Meta loss on this task batch = 3.3647e-01, Meta loss averaged over last 500 steps = 3.4541e-01, PNorm = 89.3739, GNorm = 0.3037
Took 134.55282592773438 seconds to complete one epoch of meta training
Took 143.3781180381775 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500821
Epoch 198
Meta loss on this task batch = 3.4010e-01, Meta loss averaged over last 500 steps = 3.4533e-01, PNorm = 89.3825, GNorm = 0.2584
Meta loss on this task batch = 3.4384e-01, Meta loss averaged over last 500 steps = 3.4520e-01, PNorm = 89.3916, GNorm = 0.2865
Meta loss on this task batch = 3.8678e-01, Meta loss averaged over last 500 steps = 3.4533e-01, PNorm = 89.4003, GNorm = 0.3038
Meta loss on this task batch = 3.6391e-01, Meta loss averaged over last 500 steps = 3.4521e-01, PNorm = 89.4072, GNorm = 0.2532
Meta loss on this task batch = 3.1340e-01, Meta loss averaged over last 500 steps = 3.4517e-01, PNorm = 89.4146, GNorm = 0.2868
Meta loss on this task batch = 2.8004e-01, Meta loss averaged over last 500 steps = 3.4503e-01, PNorm = 89.4225, GNorm = 0.2509
Meta loss on this task batch = 3.6676e-01, Meta loss averaged over last 500 steps = 3.4511e-01, PNorm = 89.4285, GNorm = 0.2951
Meta loss on this task batch = 2.8185e-01, Meta loss averaged over last 500 steps = 3.4495e-01, PNorm = 89.4341, GNorm = 0.2653
Meta loss on this task batch = 3.2276e-01, Meta loss averaged over last 500 steps = 3.4493e-01, PNorm = 89.4373, GNorm = 0.2721
Meta loss on this task batch = 3.5907e-01, Meta loss averaged over last 500 steps = 3.4489e-01, PNorm = 89.4402, GNorm = 0.2915
Meta loss on this task batch = 3.5924e-01, Meta loss averaged over last 500 steps = 3.4488e-01, PNorm = 89.4418, GNorm = 0.2990
Meta loss on this task batch = 2.8903e-01, Meta loss averaged over last 500 steps = 3.4479e-01, PNorm = 89.4443, GNorm = 0.3017
Meta loss on this task batch = 3.2826e-01, Meta loss averaged over last 500 steps = 3.4478e-01, PNorm = 89.4502, GNorm = 0.3178
Meta loss on this task batch = 3.3354e-01, Meta loss averaged over last 500 steps = 3.4480e-01, PNorm = 89.4564, GNorm = 0.2808
Meta loss on this task batch = 3.3788e-01, Meta loss averaged over last 500 steps = 3.4477e-01, PNorm = 89.4609, GNorm = 0.2874
Meta loss on this task batch = 3.9903e-01, Meta loss averaged over last 500 steps = 3.4477e-01, PNorm = 89.4653, GNorm = 0.2912
Meta loss on this task batch = 3.8042e-01, Meta loss averaged over last 500 steps = 3.4473e-01, PNorm = 89.4703, GNorm = 0.2975
Meta loss on this task batch = 3.6535e-01, Meta loss averaged over last 500 steps = 3.4479e-01, PNorm = 89.4749, GNorm = 0.3137
Meta loss on this task batch = 3.2629e-01, Meta loss averaged over last 500 steps = 3.4467e-01, PNorm = 89.4822, GNorm = 0.2999
Took 130.87495708465576 seconds to complete one epoch of meta training
Took 139.22824954986572 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465635
Epoch 199
Meta loss on this task batch = 3.1537e-01, Meta loss averaged over last 500 steps = 3.4444e-01, PNorm = 89.4931, GNorm = 0.2517
Meta loss on this task batch = 3.6736e-01, Meta loss averaged over last 500 steps = 3.4453e-01, PNorm = 89.5051, GNorm = 0.2569
Meta loss on this task batch = 3.3998e-01, Meta loss averaged over last 500 steps = 3.4452e-01, PNorm = 89.5175, GNorm = 0.2973
Meta loss on this task batch = 3.7960e-01, Meta loss averaged over last 500 steps = 3.4450e-01, PNorm = 89.5282, GNorm = 0.2719
Meta loss on this task batch = 3.9664e-01, Meta loss averaged over last 500 steps = 3.4466e-01, PNorm = 89.5374, GNorm = 0.3324
Meta loss on this task batch = 3.0832e-01, Meta loss averaged over last 500 steps = 3.4459e-01, PNorm = 89.5435, GNorm = 0.2744
Meta loss on this task batch = 3.7707e-01, Meta loss averaged over last 500 steps = 3.4453e-01, PNorm = 89.5469, GNorm = 0.3348
Meta loss on this task batch = 3.5198e-01, Meta loss averaged over last 500 steps = 3.4464e-01, PNorm = 89.5471, GNorm = 0.3637
Meta loss on this task batch = 3.4068e-01, Meta loss averaged over last 500 steps = 3.4462e-01, PNorm = 89.5475, GNorm = 0.2697
Meta loss on this task batch = 3.3466e-01, Meta loss averaged over last 500 steps = 3.4454e-01, PNorm = 89.5462, GNorm = 0.2784
Meta loss on this task batch = 3.6036e-01, Meta loss averaged over last 500 steps = 3.4455e-01, PNorm = 89.5470, GNorm = 0.2987
Meta loss on this task batch = 3.3032e-01, Meta loss averaged over last 500 steps = 3.4452e-01, PNorm = 89.5477, GNorm = 0.3062
Meta loss on this task batch = 2.5040e-01, Meta loss averaged over last 500 steps = 3.4438e-01, PNorm = 89.5519, GNorm = 0.2461
Meta loss on this task batch = 3.5584e-01, Meta loss averaged over last 500 steps = 3.4445e-01, PNorm = 89.5570, GNorm = 0.3276
Meta loss on this task batch = 3.3254e-01, Meta loss averaged over last 500 steps = 3.4438e-01, PNorm = 89.5654, GNorm = 0.2657
Meta loss on this task batch = 3.8645e-01, Meta loss averaged over last 500 steps = 3.4450e-01, PNorm = 89.5735, GNorm = 0.2836
Meta loss on this task batch = 3.4210e-01, Meta loss averaged over last 500 steps = 3.4455e-01, PNorm = 89.5819, GNorm = 0.3065
Meta loss on this task batch = 3.4550e-01, Meta loss averaged over last 500 steps = 3.4454e-01, PNorm = 89.5908, GNorm = 0.2643
Meta loss on this task batch = 2.6720e-01, Meta loss averaged over last 500 steps = 3.4434e-01, PNorm = 89.6006, GNorm = 0.2582
Took 136.7897548675537 seconds to complete one epoch of meta training
Took 145.10120105743408 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499289
Epoch 200
Meta loss on this task batch = 3.2399e-01, Meta loss averaged over last 500 steps = 3.4432e-01, PNorm = 89.6109, GNorm = 0.2605
Meta loss on this task batch = 3.4156e-01, Meta loss averaged over last 500 steps = 3.4412e-01, PNorm = 89.6204, GNorm = 0.2629
Meta loss on this task batch = 3.7479e-01, Meta loss averaged over last 500 steps = 3.4425e-01, PNorm = 89.6292, GNorm = 0.2903
Meta loss on this task batch = 3.6212e-01, Meta loss averaged over last 500 steps = 3.4431e-01, PNorm = 89.6369, GNorm = 0.3292
Meta loss on this task batch = 3.6533e-01, Meta loss averaged over last 500 steps = 3.4435e-01, PNorm = 89.6446, GNorm = 0.3197
Meta loss on this task batch = 3.5738e-01, Meta loss averaged over last 500 steps = 3.4433e-01, PNorm = 89.6520, GNorm = 0.2801
Meta loss on this task batch = 3.0028e-01, Meta loss averaged over last 500 steps = 3.4426e-01, PNorm = 89.6608, GNorm = 0.2587
Meta loss on this task batch = 3.8076e-01, Meta loss averaged over last 500 steps = 3.4426e-01, PNorm = 89.6679, GNorm = 0.2971
Meta loss on this task batch = 3.5119e-01, Meta loss averaged over last 500 steps = 3.4425e-01, PNorm = 89.6739, GNorm = 0.2704
Meta loss on this task batch = 2.8075e-01, Meta loss averaged over last 500 steps = 3.4414e-01, PNorm = 89.6789, GNorm = 0.2630
Meta loss on this task batch = 3.3423e-01, Meta loss averaged over last 500 steps = 3.4411e-01, PNorm = 89.6822, GNorm = 0.3088
Meta loss on this task batch = 3.6750e-01, Meta loss averaged over last 500 steps = 3.4408e-01, PNorm = 89.6838, GNorm = 0.2966
Meta loss on this task batch = 2.9195e-01, Meta loss averaged over last 500 steps = 3.4390e-01, PNorm = 89.6871, GNorm = 0.2522
Meta loss on this task batch = 3.2446e-01, Meta loss averaged over last 500 steps = 3.4388e-01, PNorm = 89.6925, GNorm = 0.2915
Meta loss on this task batch = 3.4856e-01, Meta loss averaged over last 500 steps = 3.4389e-01, PNorm = 89.6970, GNorm = 0.3365
Meta loss on this task batch = 3.6439e-01, Meta loss averaged over last 500 steps = 3.4395e-01, PNorm = 89.6986, GNorm = 0.3502
Meta loss on this task batch = 3.4035e-01, Meta loss averaged over last 500 steps = 3.4393e-01, PNorm = 89.7007, GNorm = 0.2730
Meta loss on this task batch = 3.3794e-01, Meta loss averaged over last 500 steps = 3.4381e-01, PNorm = 89.7037, GNorm = 0.2483
Meta loss on this task batch = 3.2386e-01, Meta loss averaged over last 500 steps = 3.4374e-01, PNorm = 89.7077, GNorm = 0.3117
Took 135.42352271080017 seconds to complete one epoch of meta training
Took 144.16465950012207 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485830
Epoch 201
Meta loss on this task batch = 3.6994e-01, Meta loss averaged over last 500 steps = 3.4372e-01, PNorm = 89.7136, GNorm = 0.2612
Meta loss on this task batch = 2.9552e-01, Meta loss averaged over last 500 steps = 3.4362e-01, PNorm = 89.7211, GNorm = 0.2443
Meta loss on this task batch = 3.9069e-01, Meta loss averaged over last 500 steps = 3.4369e-01, PNorm = 89.7289, GNorm = 0.3130
Meta loss on this task batch = 3.1542e-01, Meta loss averaged over last 500 steps = 3.4370e-01, PNorm = 89.7372, GNorm = 0.2560
Meta loss on this task batch = 3.4051e-01, Meta loss averaged over last 500 steps = 3.4373e-01, PNorm = 89.7461, GNorm = 0.2508
Meta loss on this task batch = 3.4383e-01, Meta loss averaged over last 500 steps = 3.4378e-01, PNorm = 89.7559, GNorm = 0.2478
Meta loss on this task batch = 3.0171e-01, Meta loss averaged over last 500 steps = 3.4369e-01, PNorm = 89.7663, GNorm = 0.2211
Meta loss on this task batch = 3.5280e-01, Meta loss averaged over last 500 steps = 3.4360e-01, PNorm = 89.7753, GNorm = 0.2990
Meta loss on this task batch = 2.8731e-01, Meta loss averaged over last 500 steps = 3.4353e-01, PNorm = 89.7846, GNorm = 0.2436
Meta loss on this task batch = 3.2258e-01, Meta loss averaged over last 500 steps = 3.4343e-01, PNorm = 89.7937, GNorm = 0.2330
Meta loss on this task batch = 3.1073e-01, Meta loss averaged over last 500 steps = 3.4324e-01, PNorm = 89.8035, GNorm = 0.2292
Meta loss on this task batch = 3.7267e-01, Meta loss averaged over last 500 steps = 3.4328e-01, PNorm = 89.8136, GNorm = 0.2614
Meta loss on this task batch = 3.3603e-01, Meta loss averaged over last 500 steps = 3.4334e-01, PNorm = 89.8236, GNorm = 0.2373
Meta loss on this task batch = 3.4681e-01, Meta loss averaged over last 500 steps = 3.4343e-01, PNorm = 89.8333, GNorm = 0.2432
Meta loss on this task batch = 3.2820e-01, Meta loss averaged over last 500 steps = 3.4350e-01, PNorm = 89.8414, GNorm = 0.2527
Meta loss on this task batch = 3.7070e-01, Meta loss averaged over last 500 steps = 3.4356e-01, PNorm = 89.8491, GNorm = 0.3418
Meta loss on this task batch = 3.6376e-01, Meta loss averaged over last 500 steps = 3.4353e-01, PNorm = 89.8551, GNorm = 0.3005
Meta loss on this task batch = 3.4672e-01, Meta loss averaged over last 500 steps = 3.4360e-01, PNorm = 89.8609, GNorm = 0.2810
Meta loss on this task batch = 3.8707e-01, Meta loss averaged over last 500 steps = 3.4371e-01, PNorm = 89.8665, GNorm = 0.3083
Took 134.45565748214722 seconds to complete one epoch of meta training
Took 143.02111315727234 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491659
Epoch 202
Meta loss on this task batch = 3.6391e-01, Meta loss averaged over last 500 steps = 3.4366e-01, PNorm = 89.8719, GNorm = 0.2751
Meta loss on this task batch = 2.8654e-01, Meta loss averaged over last 500 steps = 3.4350e-01, PNorm = 89.8780, GNorm = 0.2659
Meta loss on this task batch = 3.2953e-01, Meta loss averaged over last 500 steps = 3.4346e-01, PNorm = 89.8840, GNorm = 0.2499
Meta loss on this task batch = 3.3550e-01, Meta loss averaged over last 500 steps = 3.4343e-01, PNorm = 89.8900, GNorm = 0.2785
Meta loss on this task batch = 3.7399e-01, Meta loss averaged over last 500 steps = 3.4356e-01, PNorm = 89.8943, GNorm = 0.3653
Meta loss on this task batch = 3.6473e-01, Meta loss averaged over last 500 steps = 3.4345e-01, PNorm = 89.9014, GNorm = 0.2573
Meta loss on this task batch = 3.1624e-01, Meta loss averaged over last 500 steps = 3.4346e-01, PNorm = 89.9105, GNorm = 0.2882
Meta loss on this task batch = 3.4477e-01, Meta loss averaged over last 500 steps = 3.4345e-01, PNorm = 89.9199, GNorm = 0.3267
Meta loss on this task batch = 3.2259e-01, Meta loss averaged over last 500 steps = 3.4333e-01, PNorm = 89.9300, GNorm = 0.2912
Meta loss on this task batch = 3.7366e-01, Meta loss averaged over last 500 steps = 3.4334e-01, PNorm = 89.9401, GNorm = 0.2800
Meta loss on this task batch = 3.1496e-01, Meta loss averaged over last 500 steps = 3.4329e-01, PNorm = 89.9512, GNorm = 0.2656
Meta loss on this task batch = 3.2845e-01, Meta loss averaged over last 500 steps = 3.4329e-01, PNorm = 89.9616, GNorm = 0.2937
Meta loss on this task batch = 2.9286e-01, Meta loss averaged over last 500 steps = 3.4316e-01, PNorm = 89.9725, GNorm = 0.2619
Meta loss on this task batch = 2.9855e-01, Meta loss averaged over last 500 steps = 3.4299e-01, PNorm = 89.9822, GNorm = 0.3100
Meta loss on this task batch = 3.3560e-01, Meta loss averaged over last 500 steps = 3.4307e-01, PNorm = 89.9897, GNorm = 0.2528
Meta loss on this task batch = 3.2476e-01, Meta loss averaged over last 500 steps = 3.4299e-01, PNorm = 89.9971, GNorm = 0.2475
Meta loss on this task batch = 3.3053e-01, Meta loss averaged over last 500 steps = 3.4297e-01, PNorm = 90.0070, GNorm = 0.2819
Meta loss on this task batch = 4.1872e-01, Meta loss averaged over last 500 steps = 3.4318e-01, PNorm = 90.0128, GNorm = 0.3430
Meta loss on this task batch = 3.4728e-01, Meta loss averaged over last 500 steps = 3.4316e-01, PNorm = 90.0156, GNorm = 0.3158
Took 135.80120611190796 seconds to complete one epoch of meta training
Took 144.474684715271 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497573
Epoch 203
Meta loss on this task batch = 3.6592e-01, Meta loss averaged over last 500 steps = 3.4325e-01, PNorm = 90.0162, GNorm = 0.2951
Meta loss on this task batch = 3.2485e-01, Meta loss averaged over last 500 steps = 3.4318e-01, PNorm = 90.0165, GNorm = 0.2539
Meta loss on this task batch = 2.7946e-01, Meta loss averaged over last 500 steps = 3.4312e-01, PNorm = 90.0163, GNorm = 0.2442
Meta loss on this task batch = 3.6567e-01, Meta loss averaged over last 500 steps = 3.4323e-01, PNorm = 90.0160, GNorm = 0.2659
Meta loss on this task batch = 3.2623e-01, Meta loss averaged over last 500 steps = 3.4318e-01, PNorm = 90.0168, GNorm = 0.2629
Meta loss on this task batch = 3.4558e-01, Meta loss averaged over last 500 steps = 3.4303e-01, PNorm = 90.0181, GNorm = 0.2866
Meta loss on this task batch = 3.0007e-01, Meta loss averaged over last 500 steps = 3.4299e-01, PNorm = 90.0204, GNorm = 0.2640
Meta loss on this task batch = 3.9482e-01, Meta loss averaged over last 500 steps = 3.4308e-01, PNorm = 90.0247, GNorm = 0.2617
Meta loss on this task batch = 3.2165e-01, Meta loss averaged over last 500 steps = 3.4304e-01, PNorm = 90.0316, GNorm = 0.2463
Meta loss on this task batch = 3.7248e-01, Meta loss averaged over last 500 steps = 3.4305e-01, PNorm = 90.0401, GNorm = 0.2768
Meta loss on this task batch = 3.2274e-01, Meta loss averaged over last 500 steps = 3.4292e-01, PNorm = 90.0516, GNorm = 0.2797
Meta loss on this task batch = 3.2468e-01, Meta loss averaged over last 500 steps = 3.4290e-01, PNorm = 90.0648, GNorm = 0.2702
Meta loss on this task batch = 3.1868e-01, Meta loss averaged over last 500 steps = 3.4294e-01, PNorm = 90.0783, GNorm = 0.2389
Meta loss on this task batch = 3.1878e-01, Meta loss averaged over last 500 steps = 3.4268e-01, PNorm = 90.0932, GNorm = 0.2694
Meta loss on this task batch = 3.7417e-01, Meta loss averaged over last 500 steps = 3.4272e-01, PNorm = 90.1084, GNorm = 0.3020
Meta loss on this task batch = 2.7615e-01, Meta loss averaged over last 500 steps = 3.4264e-01, PNorm = 90.1214, GNorm = 0.3048
Meta loss on this task batch = 3.1305e-01, Meta loss averaged over last 500 steps = 3.4249e-01, PNorm = 90.1322, GNorm = 0.2775
Meta loss on this task batch = 3.9015e-01, Meta loss averaged over last 500 steps = 3.4261e-01, PNorm = 90.1432, GNorm = 0.3039
Meta loss on this task batch = 3.5432e-01, Meta loss averaged over last 500 steps = 3.4262e-01, PNorm = 90.1526, GNorm = 0.3776
Took 132.80979013442993 seconds to complete one epoch of meta training
Took 141.21962785720825 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.516737
Epoch 204
Meta loss on this task batch = 3.7021e-01, Meta loss averaged over last 500 steps = 3.4273e-01, PNorm = 90.1585, GNorm = 0.3103
Meta loss on this task batch = 3.5442e-01, Meta loss averaged over last 500 steps = 3.4278e-01, PNorm = 90.1657, GNorm = 0.2998
Meta loss on this task batch = 3.6765e-01, Meta loss averaged over last 500 steps = 3.4287e-01, PNorm = 90.1712, GNorm = 0.2819
Meta loss on this task batch = 3.7747e-01, Meta loss averaged over last 500 steps = 3.4294e-01, PNorm = 90.1768, GNorm = 0.2821
Meta loss on this task batch = 3.2188e-01, Meta loss averaged over last 500 steps = 3.4291e-01, PNorm = 90.1837, GNorm = 0.2428
Meta loss on this task batch = 3.5181e-01, Meta loss averaged over last 500 steps = 3.4290e-01, PNorm = 90.1923, GNorm = 0.2734
Meta loss on this task batch = 2.8260e-01, Meta loss averaged over last 500 steps = 3.4280e-01, PNorm = 90.2020, GNorm = 0.2866
Meta loss on this task batch = 2.9149e-01, Meta loss averaged over last 500 steps = 3.4267e-01, PNorm = 90.2140, GNorm = 0.2550
Meta loss on this task batch = 3.2096e-01, Meta loss averaged over last 500 steps = 3.4260e-01, PNorm = 90.2268, GNorm = 0.2732
Meta loss on this task batch = 3.3119e-01, Meta loss averaged over last 500 steps = 3.4241e-01, PNorm = 90.2377, GNorm = 0.3181
Meta loss on this task batch = 3.1868e-01, Meta loss averaged over last 500 steps = 3.4246e-01, PNorm = 90.2474, GNorm = 0.2855
Meta loss on this task batch = 3.5682e-01, Meta loss averaged over last 500 steps = 3.4247e-01, PNorm = 90.2558, GNorm = 0.3350
Meta loss on this task batch = 3.2598e-01, Meta loss averaged over last 500 steps = 3.4247e-01, PNorm = 90.2635, GNorm = 0.2592
Meta loss on this task batch = 3.1397e-01, Meta loss averaged over last 500 steps = 3.4260e-01, PNorm = 90.2710, GNorm = 0.2678
Meta loss on this task batch = 3.5583e-01, Meta loss averaged over last 500 steps = 3.4253e-01, PNorm = 90.2775, GNorm = 0.3199
Meta loss on this task batch = 3.2088e-01, Meta loss averaged over last 500 steps = 3.4240e-01, PNorm = 90.2828, GNorm = 0.3159
Meta loss on this task batch = 3.0226e-01, Meta loss averaged over last 500 steps = 3.4237e-01, PNorm = 90.2876, GNorm = 0.2766
Meta loss on this task batch = 3.9006e-01, Meta loss averaged over last 500 steps = 3.4246e-01, PNorm = 90.2927, GNorm = 0.3028
Meta loss on this task batch = 3.2179e-01, Meta loss averaged over last 500 steps = 3.4247e-01, PNorm = 90.2986, GNorm = 0.3748
Took 133.19234442710876 seconds to complete one epoch of meta training
Took 142.0082221031189 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497209
Epoch 205
Meta loss on this task batch = 3.0845e-01, Meta loss averaged over last 500 steps = 3.4242e-01, PNorm = 90.3050, GNorm = 0.2325
Meta loss on this task batch = 3.2182e-01, Meta loss averaged over last 500 steps = 3.4239e-01, PNorm = 90.3126, GNorm = 0.2421
Meta loss on this task batch = 3.6236e-01, Meta loss averaged over last 500 steps = 3.4237e-01, PNorm = 90.3218, GNorm = 0.2733
Meta loss on this task batch = 4.0287e-01, Meta loss averaged over last 500 steps = 3.4247e-01, PNorm = 90.3310, GNorm = 0.2978
Meta loss on this task batch = 3.3975e-01, Meta loss averaged over last 500 steps = 3.4248e-01, PNorm = 90.3397, GNorm = 0.2620
Meta loss on this task batch = 3.8532e-01, Meta loss averaged over last 500 steps = 3.4249e-01, PNorm = 90.3477, GNorm = 0.2872
Meta loss on this task batch = 3.6075e-01, Meta loss averaged over last 500 steps = 3.4261e-01, PNorm = 90.3543, GNorm = 0.2636
Meta loss on this task batch = 2.9277e-01, Meta loss averaged over last 500 steps = 3.4233e-01, PNorm = 90.3613, GNorm = 0.2374
Meta loss on this task batch = 3.2914e-01, Meta loss averaged over last 500 steps = 3.4224e-01, PNorm = 90.3677, GNorm = 0.2496
Meta loss on this task batch = 3.1473e-01, Meta loss averaged over last 500 steps = 3.4219e-01, PNorm = 90.3751, GNorm = 0.2432
Meta loss on this task batch = 2.6793e-01, Meta loss averaged over last 500 steps = 3.4205e-01, PNorm = 90.3841, GNorm = 0.2181
Meta loss on this task batch = 3.3906e-01, Meta loss averaged over last 500 steps = 3.4199e-01, PNorm = 90.3921, GNorm = 0.2805
Meta loss on this task batch = 3.1538e-01, Meta loss averaged over last 500 steps = 3.4199e-01, PNorm = 90.3997, GNorm = 0.2679
Meta loss on this task batch = 2.8733e-01, Meta loss averaged over last 500 steps = 3.4181e-01, PNorm = 90.4069, GNorm = 0.2856
Meta loss on this task batch = 3.6875e-01, Meta loss averaged over last 500 steps = 3.4192e-01, PNorm = 90.4093, GNorm = 0.4378
Meta loss on this task batch = 3.2160e-01, Meta loss averaged over last 500 steps = 3.4186e-01, PNorm = 90.4105, GNorm = 0.2884
Meta loss on this task batch = 3.6420e-01, Meta loss averaged over last 500 steps = 3.4197e-01, PNorm = 90.4127, GNorm = 0.3467
Meta loss on this task batch = 3.8343e-01, Meta loss averaged over last 500 steps = 3.4206e-01, PNorm = 90.4178, GNorm = 0.3172
Meta loss on this task batch = 3.6121e-01, Meta loss averaged over last 500 steps = 3.4209e-01, PNorm = 90.4247, GNorm = 0.3347
Took 134.561017036438 seconds to complete one epoch of meta training
Took 142.8992965221405 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482688
Epoch 206
Meta loss on this task batch = 3.6463e-01, Meta loss averaged over last 500 steps = 3.4215e-01, PNorm = 90.4329, GNorm = 0.3106
Meta loss on this task batch = 3.2122e-01, Meta loss averaged over last 500 steps = 3.4217e-01, PNorm = 90.4428, GNorm = 0.2744
Meta loss on this task batch = 3.3878e-01, Meta loss averaged over last 500 steps = 3.4213e-01, PNorm = 90.4541, GNorm = 0.2683
Meta loss on this task batch = 3.5392e-01, Meta loss averaged over last 500 steps = 3.4206e-01, PNorm = 90.4675, GNorm = 0.2750
Meta loss on this task batch = 3.1696e-01, Meta loss averaged over last 500 steps = 3.4205e-01, PNorm = 90.4816, GNorm = 0.2829
Meta loss on this task batch = 3.0764e-01, Meta loss averaged over last 500 steps = 3.4185e-01, PNorm = 90.4945, GNorm = 0.2550
Meta loss on this task batch = 2.8057e-01, Meta loss averaged over last 500 steps = 3.4170e-01, PNorm = 90.5074, GNorm = 0.2483
Meta loss on this task batch = 3.8992e-01, Meta loss averaged over last 500 steps = 3.4180e-01, PNorm = 90.5168, GNorm = 0.2912
Meta loss on this task batch = 3.5165e-01, Meta loss averaged over last 500 steps = 3.4181e-01, PNorm = 90.5239, GNorm = 0.3081
Meta loss on this task batch = 3.7575e-01, Meta loss averaged over last 500 steps = 3.4188e-01, PNorm = 90.5274, GNorm = 0.3110
Meta loss on this task batch = 2.5897e-01, Meta loss averaged over last 500 steps = 3.4173e-01, PNorm = 90.5332, GNorm = 0.2348
Meta loss on this task batch = 2.9951e-01, Meta loss averaged over last 500 steps = 3.4167e-01, PNorm = 90.5385, GNorm = 0.2178
Meta loss on this task batch = 3.4579e-01, Meta loss averaged over last 500 steps = 3.4176e-01, PNorm = 90.5445, GNorm = 0.2529
Meta loss on this task batch = 4.0843e-01, Meta loss averaged over last 500 steps = 3.4173e-01, PNorm = 90.5465, GNorm = 0.2827
Meta loss on this task batch = 3.2849e-01, Meta loss averaged over last 500 steps = 3.4179e-01, PNorm = 90.5492, GNorm = 0.2646
Meta loss on this task batch = 3.4948e-01, Meta loss averaged over last 500 steps = 3.4173e-01, PNorm = 90.5528, GNorm = 0.2636
Meta loss on this task batch = 3.2209e-01, Meta loss averaged over last 500 steps = 3.4158e-01, PNorm = 90.5587, GNorm = 0.2509
Meta loss on this task batch = 3.9531e-01, Meta loss averaged over last 500 steps = 3.4176e-01, PNorm = 90.5668, GNorm = 0.2714
Meta loss on this task batch = 4.1668e-01, Meta loss averaged over last 500 steps = 3.4188e-01, PNorm = 90.5756, GNorm = 0.3210
Took 132.30176877975464 seconds to complete one epoch of meta training
Took 140.5890371799469 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487592
Epoch 207
Meta loss on this task batch = 3.7251e-01, Meta loss averaged over last 500 steps = 3.4198e-01, PNorm = 90.5845, GNorm = 0.2400
Meta loss on this task batch = 2.9057e-01, Meta loss averaged over last 500 steps = 3.4178e-01, PNorm = 90.5939, GNorm = 0.2254
Meta loss on this task batch = 3.3120e-01, Meta loss averaged over last 500 steps = 3.4179e-01, PNorm = 90.6034, GNorm = 0.2678
Meta loss on this task batch = 3.1392e-01, Meta loss averaged over last 500 steps = 3.4173e-01, PNorm = 90.6124, GNorm = 0.2527
Meta loss on this task batch = 3.0088e-01, Meta loss averaged over last 500 steps = 3.4159e-01, PNorm = 90.6213, GNorm = 0.2570
Meta loss on this task batch = 3.0376e-01, Meta loss averaged over last 500 steps = 3.4150e-01, PNorm = 90.6304, GNorm = 0.2801
Meta loss on this task batch = 3.5639e-01, Meta loss averaged over last 500 steps = 3.4150e-01, PNorm = 90.6371, GNorm = 0.3026
Meta loss on this task batch = 3.4298e-01, Meta loss averaged over last 500 steps = 3.4159e-01, PNorm = 90.6425, GNorm = 0.2870
Meta loss on this task batch = 3.3826e-01, Meta loss averaged over last 500 steps = 3.4156e-01, PNorm = 90.6470, GNorm = 0.2989
Meta loss on this task batch = 3.6795e-01, Meta loss averaged over last 500 steps = 3.4166e-01, PNorm = 90.6504, GNorm = 0.2857
Meta loss on this task batch = 3.1642e-01, Meta loss averaged over last 500 steps = 3.4170e-01, PNorm = 90.6538, GNorm = 0.2532
Meta loss on this task batch = 3.2086e-01, Meta loss averaged over last 500 steps = 3.4160e-01, PNorm = 90.6565, GNorm = 0.2599
Meta loss on this task batch = 3.2172e-01, Meta loss averaged over last 500 steps = 3.4148e-01, PNorm = 90.6597, GNorm = 0.2646
Meta loss on this task batch = 3.6320e-01, Meta loss averaged over last 500 steps = 3.4158e-01, PNorm = 90.6636, GNorm = 0.2693
Meta loss on this task batch = 3.5897e-01, Meta loss averaged over last 500 steps = 3.4169e-01, PNorm = 90.6697, GNorm = 0.3114
Meta loss on this task batch = 3.1026e-01, Meta loss averaged over last 500 steps = 3.4167e-01, PNorm = 90.6766, GNorm = 0.2307
Meta loss on this task batch = 3.3513e-01, Meta loss averaged over last 500 steps = 3.4168e-01, PNorm = 90.6830, GNorm = 0.2731
Meta loss on this task batch = 3.1293e-01, Meta loss averaged over last 500 steps = 3.4152e-01, PNorm = 90.6907, GNorm = 0.2500
Meta loss on this task batch = 4.0871e-01, Meta loss averaged over last 500 steps = 3.4163e-01, PNorm = 90.6972, GNorm = 0.3495
Took 134.8707354068756 seconds to complete one epoch of meta training
Took 143.34542679786682 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485007
Epoch 208
Meta loss on this task batch = 3.9850e-01, Meta loss averaged over last 500 steps = 3.4170e-01, PNorm = 90.6981, GNorm = 0.3469
Meta loss on this task batch = 3.8575e-01, Meta loss averaged over last 500 steps = 3.4176e-01, PNorm = 90.6991, GNorm = 0.2847
Meta loss on this task batch = 3.0469e-01, Meta loss averaged over last 500 steps = 3.4156e-01, PNorm = 90.7016, GNorm = 0.2268
Meta loss on this task batch = 3.4307e-01, Meta loss averaged over last 500 steps = 3.4164e-01, PNorm = 90.7063, GNorm = 0.2755
Meta loss on this task batch = 3.0021e-01, Meta loss averaged over last 500 steps = 3.4151e-01, PNorm = 90.7130, GNorm = 0.2594
Meta loss on this task batch = 3.5187e-01, Meta loss averaged over last 500 steps = 3.4153e-01, PNorm = 90.7221, GNorm = 0.2729
Meta loss on this task batch = 2.6023e-01, Meta loss averaged over last 500 steps = 3.4138e-01, PNorm = 90.7345, GNorm = 0.2475
Meta loss on this task batch = 4.0305e-01, Meta loss averaged over last 500 steps = 3.4146e-01, PNorm = 90.7438, GNorm = 0.3658
Meta loss on this task batch = 2.7139e-01, Meta loss averaged over last 500 steps = 3.4138e-01, PNorm = 90.7532, GNorm = 0.2847
Meta loss on this task batch = 2.8140e-01, Meta loss averaged over last 500 steps = 3.4139e-01, PNorm = 90.7629, GNorm = 0.2678
Meta loss on this task batch = 2.9237e-01, Meta loss averaged over last 500 steps = 3.4133e-01, PNorm = 90.7722, GNorm = 0.2489
Meta loss on this task batch = 3.5994e-01, Meta loss averaged over last 500 steps = 3.4140e-01, PNorm = 90.7785, GNorm = 0.3231
Meta loss on this task batch = 3.2672e-01, Meta loss averaged over last 500 steps = 3.4131e-01, PNorm = 90.7847, GNorm = 0.2954
Meta loss on this task batch = 3.2840e-01, Meta loss averaged over last 500 steps = 3.4119e-01, PNorm = 90.7920, GNorm = 0.2779
Meta loss on this task batch = 4.0571e-01, Meta loss averaged over last 500 steps = 3.4127e-01, PNorm = 90.7999, GNorm = 0.3190
Meta loss on this task batch = 3.2648e-01, Meta loss averaged over last 500 steps = 3.4128e-01, PNorm = 90.8100, GNorm = 0.2760
Meta loss on this task batch = 3.6822e-01, Meta loss averaged over last 500 steps = 3.4116e-01, PNorm = 90.8228, GNorm = 0.3215
Meta loss on this task batch = 3.6063e-01, Meta loss averaged over last 500 steps = 3.4127e-01, PNorm = 90.8354, GNorm = 0.2701
Meta loss on this task batch = 3.6842e-01, Meta loss averaged over last 500 steps = 3.4122e-01, PNorm = 90.8475, GNorm = 0.3294
Took 132.99103164672852 seconds to complete one epoch of meta training
Took 141.42938947677612 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511427
Epoch 209
Meta loss on this task batch = 3.2823e-01, Meta loss averaged over last 500 steps = 3.4113e-01, PNorm = 90.8578, GNorm = 0.2452
Meta loss on this task batch = 3.3941e-01, Meta loss averaged over last 500 steps = 3.4110e-01, PNorm = 90.8681, GNorm = 0.2463
Meta loss on this task batch = 3.4712e-01, Meta loss averaged over last 500 steps = 3.4111e-01, PNorm = 90.8771, GNorm = 0.3127
Meta loss on this task batch = 3.5784e-01, Meta loss averaged over last 500 steps = 3.4108e-01, PNorm = 90.8831, GNorm = 0.2924
Meta loss on this task batch = 3.6157e-01, Meta loss averaged over last 500 steps = 3.4112e-01, PNorm = 90.8854, GNorm = 0.2882
Meta loss on this task batch = 3.4677e-01, Meta loss averaged over last 500 steps = 3.4110e-01, PNorm = 90.8885, GNorm = 0.2626
Meta loss on this task batch = 3.4223e-01, Meta loss averaged over last 500 steps = 3.4108e-01, PNorm = 90.8923, GNorm = 0.2706
Meta loss on this task batch = 2.7005e-01, Meta loss averaged over last 500 steps = 3.4082e-01, PNorm = 90.8978, GNorm = 0.2453
Meta loss on this task batch = 3.6970e-01, Meta loss averaged over last 500 steps = 3.4093e-01, PNorm = 90.9039, GNorm = 0.2895
Meta loss on this task batch = 2.8349e-01, Meta loss averaged over last 500 steps = 3.4078e-01, PNorm = 90.9103, GNorm = 0.2158
Meta loss on this task batch = 3.8323e-01, Meta loss averaged over last 500 steps = 3.4086e-01, PNorm = 90.9157, GNorm = 0.2997
Meta loss on this task batch = 3.3887e-01, Meta loss averaged over last 500 steps = 3.4084e-01, PNorm = 90.9234, GNorm = 0.2826
Meta loss on this task batch = 2.9797e-01, Meta loss averaged over last 500 steps = 3.4069e-01, PNorm = 90.9302, GNorm = 0.2580
Meta loss on this task batch = 3.3280e-01, Meta loss averaged over last 500 steps = 3.4074e-01, PNorm = 90.9389, GNorm = 0.2847
Meta loss on this task batch = 3.4199e-01, Meta loss averaged over last 500 steps = 3.4068e-01, PNorm = 90.9495, GNorm = 0.2614
Meta loss on this task batch = 2.7457e-01, Meta loss averaged over last 500 steps = 3.4050e-01, PNorm = 90.9597, GNorm = 0.2074
Meta loss on this task batch = 3.5353e-01, Meta loss averaged over last 500 steps = 3.4046e-01, PNorm = 90.9687, GNorm = 0.2815
Meta loss on this task batch = 3.5997e-01, Meta loss averaged over last 500 steps = 3.4044e-01, PNorm = 90.9764, GNorm = 0.2534
Meta loss on this task batch = 4.2122e-01, Meta loss averaged over last 500 steps = 3.4057e-01, PNorm = 90.9725, GNorm = 1.0000
Took 131.28560853004456 seconds to complete one epoch of meta training
Took 139.70103192329407 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485824
Epoch 210
Meta loss on this task batch = 3.0682e-01, Meta loss averaged over last 500 steps = 3.4066e-01, PNorm = 90.9713, GNorm = 0.2373
Meta loss on this task batch = 3.7726e-01, Meta loss averaged over last 500 steps = 3.4068e-01, PNorm = 90.9713, GNorm = 0.2793
Meta loss on this task batch = 3.3609e-01, Meta loss averaged over last 500 steps = 3.4066e-01, PNorm = 90.9738, GNorm = 0.2644
Meta loss on this task batch = 3.6290e-01, Meta loss averaged over last 500 steps = 3.4066e-01, PNorm = 90.9758, GNorm = 0.2788
Meta loss on this task batch = 2.8565e-01, Meta loss averaged over last 500 steps = 3.4065e-01, PNorm = 90.9780, GNorm = 0.2709
Meta loss on this task batch = 3.5977e-01, Meta loss averaged over last 500 steps = 3.4075e-01, PNorm = 90.9770, GNorm = 0.2789
Meta loss on this task batch = 3.3774e-01, Meta loss averaged over last 500 steps = 3.4079e-01, PNorm = 90.9768, GNorm = 0.2670
Meta loss on this task batch = 3.8577e-01, Meta loss averaged over last 500 steps = 3.4084e-01, PNorm = 90.9754, GNorm = 0.2965
Meta loss on this task batch = 3.8057e-01, Meta loss averaged over last 500 steps = 3.4111e-01, PNorm = 90.9741, GNorm = 0.3192
Meta loss on this task batch = 3.1983e-01, Meta loss averaged over last 500 steps = 3.4096e-01, PNorm = 90.9764, GNorm = 0.2889
Meta loss on this task batch = 3.1214e-01, Meta loss averaged over last 500 steps = 3.4095e-01, PNorm = 90.9811, GNorm = 0.2401
Meta loss on this task batch = 3.4692e-01, Meta loss averaged over last 500 steps = 3.4097e-01, PNorm = 90.9872, GNorm = 0.3269
Meta loss on this task batch = 3.4565e-01, Meta loss averaged over last 500 steps = 3.4108e-01, PNorm = 90.9936, GNorm = 0.2712
Meta loss on this task batch = 3.4390e-01, Meta loss averaged over last 500 steps = 3.4099e-01, PNorm = 90.9932, GNorm = 0.3838
Meta loss on this task batch = 3.6255e-01, Meta loss averaged over last 500 steps = 3.4102e-01, PNorm = 90.9924, GNorm = 0.3065
Meta loss on this task batch = 3.1026e-01, Meta loss averaged over last 500 steps = 3.4085e-01, PNorm = 90.9927, GNorm = 0.2358
Meta loss on this task batch = 3.2395e-01, Meta loss averaged over last 500 steps = 3.4077e-01, PNorm = 90.9957, GNorm = 0.2365
Meta loss on this task batch = 3.2400e-01, Meta loss averaged over last 500 steps = 3.4068e-01, PNorm = 91.0007, GNorm = 0.2572
Meta loss on this task batch = 3.8027e-01, Meta loss averaged over last 500 steps = 3.4076e-01, PNorm = 91.0060, GNorm = 0.3366
Took 134.5232493877411 seconds to complete one epoch of meta training
Took 142.75829195976257 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499404
Epoch 211
Meta loss on this task batch = 3.3117e-01, Meta loss averaged over last 500 steps = 3.4069e-01, PNorm = 91.0130, GNorm = 0.2488
Meta loss on this task batch = 3.4598e-01, Meta loss averaged over last 500 steps = 3.4070e-01, PNorm = 91.0185, GNorm = 0.3381
Meta loss on this task batch = 3.4895e-01, Meta loss averaged over last 500 steps = 3.4065e-01, PNorm = 91.0233, GNorm = 0.2952
Meta loss on this task batch = 3.5244e-01, Meta loss averaged over last 500 steps = 3.4059e-01, PNorm = 91.0300, GNorm = 0.2661
Meta loss on this task batch = 3.6096e-01, Meta loss averaged over last 500 steps = 3.4066e-01, PNorm = 91.0384, GNorm = 0.2568
Meta loss on this task batch = 3.4674e-01, Meta loss averaged over last 500 steps = 3.4063e-01, PNorm = 91.0474, GNorm = 0.2348
Meta loss on this task batch = 3.2316e-01, Meta loss averaged over last 500 steps = 3.4054e-01, PNorm = 91.0575, GNorm = 0.2223
Meta loss on this task batch = 3.7619e-01, Meta loss averaged over last 500 steps = 3.4063e-01, PNorm = 91.0685, GNorm = 0.2710
Meta loss on this task batch = 3.4687e-01, Meta loss averaged over last 500 steps = 3.4070e-01, PNorm = 91.0791, GNorm = 0.2672
Meta loss on this task batch = 3.0775e-01, Meta loss averaged over last 500 steps = 3.4059e-01, PNorm = 91.0918, GNorm = 0.2768
Meta loss on this task batch = 3.6677e-01, Meta loss averaged over last 500 steps = 3.4073e-01, PNorm = 91.1028, GNorm = 0.2770
Meta loss on this task batch = 3.3562e-01, Meta loss averaged over last 500 steps = 3.4078e-01, PNorm = 91.1127, GNorm = 0.2686
Meta loss on this task batch = 3.9466e-01, Meta loss averaged over last 500 steps = 3.4086e-01, PNorm = 91.1222, GNorm = 0.3097
Meta loss on this task batch = 2.6759e-01, Meta loss averaged over last 500 steps = 3.4077e-01, PNorm = 91.1328, GNorm = 0.2115
Meta loss on this task batch = 3.1606e-01, Meta loss averaged over last 500 steps = 3.4074e-01, PNorm = 91.1416, GNorm = 0.2821
Meta loss on this task batch = 3.4579e-01, Meta loss averaged over last 500 steps = 3.4070e-01, PNorm = 91.1502, GNorm = 0.3044
Meta loss on this task batch = 2.6161e-01, Meta loss averaged over last 500 steps = 3.4044e-01, PNorm = 91.1592, GNorm = 0.2617
Meta loss on this task batch = 3.0831e-01, Meta loss averaged over last 500 steps = 3.4028e-01, PNorm = 91.1666, GNorm = 0.2649
Meta loss on this task batch = 3.1815e-01, Meta loss averaged over last 500 steps = 3.4019e-01, PNorm = 91.1725, GNorm = 0.3802
Took 132.7473382949829 seconds to complete one epoch of meta training
Took 141.0959255695343 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502175
Epoch 212
Meta loss on this task batch = 3.4618e-01, Meta loss averaged over last 500 steps = 3.4018e-01, PNorm = 91.1768, GNorm = 0.2729
Meta loss on this task batch = 3.2064e-01, Meta loss averaged over last 500 steps = 3.4014e-01, PNorm = 91.1812, GNorm = 0.2628
Meta loss on this task batch = 3.1178e-01, Meta loss averaged over last 500 steps = 3.4014e-01, PNorm = 91.1865, GNorm = 0.3009
Meta loss on this task batch = 3.8111e-01, Meta loss averaged over last 500 steps = 3.4026e-01, PNorm = 91.1920, GNorm = 0.3054
Meta loss on this task batch = 2.9420e-01, Meta loss averaged over last 500 steps = 3.4007e-01, PNorm = 91.1965, GNorm = 0.2562
Meta loss on this task batch = 3.6210e-01, Meta loss averaged over last 500 steps = 3.4007e-01, PNorm = 91.2007, GNorm = 0.2963
Meta loss on this task batch = 3.3669e-01, Meta loss averaged over last 500 steps = 3.4008e-01, PNorm = 91.2048, GNorm = 0.2934
Meta loss on this task batch = 3.8812e-01, Meta loss averaged over last 500 steps = 3.4019e-01, PNorm = 91.2075, GNorm = 0.3310
Meta loss on this task batch = 3.4254e-01, Meta loss averaged over last 500 steps = 3.4022e-01, PNorm = 91.2096, GNorm = 0.2764
Meta loss on this task batch = 3.2399e-01, Meta loss averaged over last 500 steps = 3.4025e-01, PNorm = 91.2136, GNorm = 0.2767
Meta loss on this task batch = 3.1655e-01, Meta loss averaged over last 500 steps = 3.4015e-01, PNorm = 91.2204, GNorm = 0.2600
Meta loss on this task batch = 3.0676e-01, Meta loss averaged over last 500 steps = 3.3996e-01, PNorm = 91.2272, GNorm = 0.2701
Meta loss on this task batch = 2.9531e-01, Meta loss averaged over last 500 steps = 3.3990e-01, PNorm = 91.2309, GNorm = 0.2532
Meta loss on this task batch = 3.1978e-01, Meta loss averaged over last 500 steps = 3.3997e-01, PNorm = 91.2341, GNorm = 0.2620
Meta loss on this task batch = 3.3881e-01, Meta loss averaged over last 500 steps = 3.4003e-01, PNorm = 91.2378, GNorm = 0.2723
Meta loss on this task batch = 3.7817e-01, Meta loss averaged over last 500 steps = 3.3992e-01, PNorm = 91.2433, GNorm = 0.3169
Meta loss on this task batch = 3.4024e-01, Meta loss averaged over last 500 steps = 3.3977e-01, PNorm = 91.2496, GNorm = 0.2754
Meta loss on this task batch = 3.2039e-01, Meta loss averaged over last 500 steps = 3.3972e-01, PNorm = 91.2575, GNorm = 0.3005
Meta loss on this task batch = 3.4362e-01, Meta loss averaged over last 500 steps = 3.3975e-01, PNorm = 91.2636, GNorm = 0.3508
Took 135.9442105293274 seconds to complete one epoch of meta training
Took 145.12626266479492 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484557
Epoch 213
Meta loss on this task batch = 3.2504e-01, Meta loss averaged over last 500 steps = 3.3975e-01, PNorm = 91.2691, GNorm = 0.2628
Meta loss on this task batch = 3.1439e-01, Meta loss averaged over last 500 steps = 3.3971e-01, PNorm = 91.2748, GNorm = 0.2711
Meta loss on this task batch = 3.5974e-01, Meta loss averaged over last 500 steps = 3.3986e-01, PNorm = 91.2797, GNorm = 0.2655
Meta loss on this task batch = 2.9415e-01, Meta loss averaged over last 500 steps = 3.3964e-01, PNorm = 91.2848, GNorm = 0.2673
Meta loss on this task batch = 3.5017e-01, Meta loss averaged over last 500 steps = 3.3968e-01, PNorm = 91.2891, GNorm = 0.2785
Meta loss on this task batch = 3.8765e-01, Meta loss averaged over last 500 steps = 3.3966e-01, PNorm = 91.2936, GNorm = 0.2963
Meta loss on this task batch = 3.1038e-01, Meta loss averaged over last 500 steps = 3.3946e-01, PNorm = 91.3009, GNorm = 0.2816
Meta loss on this task batch = 2.9699e-01, Meta loss averaged over last 500 steps = 3.3934e-01, PNorm = 91.3100, GNorm = 0.2287
Meta loss on this task batch = 3.0010e-01, Meta loss averaged over last 500 steps = 3.3920e-01, PNorm = 91.3206, GNorm = 0.2335
Meta loss on this task batch = 3.4307e-01, Meta loss averaged over last 500 steps = 3.3930e-01, PNorm = 91.3324, GNorm = 0.2585
Meta loss on this task batch = 3.3303e-01, Meta loss averaged over last 500 steps = 3.3921e-01, PNorm = 91.3448, GNorm = 0.2836
Meta loss on this task batch = 3.4774e-01, Meta loss averaged over last 500 steps = 3.3926e-01, PNorm = 91.3576, GNorm = 0.2668
Meta loss on this task batch = 3.2889e-01, Meta loss averaged over last 500 steps = 3.3922e-01, PNorm = 91.3710, GNorm = 0.2721
Meta loss on this task batch = 3.6137e-01, Meta loss averaged over last 500 steps = 3.3936e-01, PNorm = 91.3857, GNorm = 0.2915
Meta loss on this task batch = 4.1737e-01, Meta loss averaged over last 500 steps = 3.3954e-01, PNorm = 91.3967, GNorm = 0.2923
Meta loss on this task batch = 2.7306e-01, Meta loss averaged over last 500 steps = 3.3949e-01, PNorm = 91.4068, GNorm = 0.2710
Meta loss on this task batch = 3.9542e-01, Meta loss averaged over last 500 steps = 3.3954e-01, PNorm = 91.4136, GNorm = 0.3930
Meta loss on this task batch = 3.2037e-01, Meta loss averaged over last 500 steps = 3.3950e-01, PNorm = 91.4178, GNorm = 0.2852
Meta loss on this task batch = 3.6458e-01, Meta loss averaged over last 500 steps = 3.3950e-01, PNorm = 91.4213, GNorm = 0.3421
Took 131.45218515396118 seconds to complete one epoch of meta training
Took 139.77791333198547 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501921
Epoch 214
Meta loss on this task batch = 2.9783e-01, Meta loss averaged over last 500 steps = 3.3929e-01, PNorm = 91.4249, GNorm = 0.2475
Meta loss on this task batch = 3.3390e-01, Meta loss averaged over last 500 steps = 3.3930e-01, PNorm = 91.4288, GNorm = 0.2919
Meta loss on this task batch = 3.5652e-01, Meta loss averaged over last 500 steps = 3.3935e-01, PNorm = 91.4324, GNorm = 0.2847
Meta loss on this task batch = 2.8344e-01, Meta loss averaged over last 500 steps = 3.3920e-01, PNorm = 91.4377, GNorm = 0.2475
Meta loss on this task batch = 3.0215e-01, Meta loss averaged over last 500 steps = 3.3908e-01, PNorm = 91.4449, GNorm = 0.2699
Meta loss on this task batch = 3.4806e-01, Meta loss averaged over last 500 steps = 3.3917e-01, PNorm = 91.4534, GNorm = 0.2871
Meta loss on this task batch = 3.4445e-01, Meta loss averaged over last 500 steps = 3.3917e-01, PNorm = 91.4629, GNorm = 0.2934
Meta loss on this task batch = 3.4699e-01, Meta loss averaged over last 500 steps = 3.3914e-01, PNorm = 91.4748, GNorm = 0.2969
Meta loss on this task batch = 3.3471e-01, Meta loss averaged over last 500 steps = 3.3909e-01, PNorm = 91.4870, GNorm = 0.3157
Meta loss on this task batch = 3.4664e-01, Meta loss averaged over last 500 steps = 3.3908e-01, PNorm = 91.4996, GNorm = 0.2935
Meta loss on this task batch = 3.8910e-01, Meta loss averaged over last 500 steps = 3.3920e-01, PNorm = 91.5095, GNorm = 0.3122
Meta loss on this task batch = 2.9009e-01, Meta loss averaged over last 500 steps = 3.3910e-01, PNorm = 91.5198, GNorm = 0.2727
Meta loss on this task batch = 3.2649e-01, Meta loss averaged over last 500 steps = 3.3907e-01, PNorm = 91.5258, GNorm = 0.3731
Meta loss on this task batch = 3.6566e-01, Meta loss averaged over last 500 steps = 3.3913e-01, PNorm = 91.5270, GNorm = 0.3542
Meta loss on this task batch = 3.6328e-01, Meta loss averaged over last 500 steps = 3.3918e-01, PNorm = 91.5271, GNorm = 0.3026
Meta loss on this task batch = 3.6582e-01, Meta loss averaged over last 500 steps = 3.3922e-01, PNorm = 91.5292, GNorm = 0.2957
Meta loss on this task batch = 3.2720e-01, Meta loss averaged over last 500 steps = 3.3917e-01, PNorm = 91.5323, GNorm = 0.2822
Meta loss on this task batch = 3.3252e-01, Meta loss averaged over last 500 steps = 3.3922e-01, PNorm = 91.5353, GNorm = 0.2548
Meta loss on this task batch = 2.8077e-01, Meta loss averaged over last 500 steps = 3.3908e-01, PNorm = 91.5387, GNorm = 0.2711
Took 134.37885236740112 seconds to complete one epoch of meta training
Took 143.1880567073822 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485007
Epoch 215
Meta loss on this task batch = 3.5096e-01, Meta loss averaged over last 500 steps = 3.3907e-01, PNorm = 91.5430, GNorm = 0.2688
Meta loss on this task batch = 3.7555e-01, Meta loss averaged over last 500 steps = 3.3907e-01, PNorm = 91.5461, GNorm = 0.3070
Meta loss on this task batch = 3.4486e-01, Meta loss averaged over last 500 steps = 3.3909e-01, PNorm = 91.5494, GNorm = 0.2515
Meta loss on this task batch = 3.7834e-01, Meta loss averaged over last 500 steps = 3.3913e-01, PNorm = 91.5547, GNorm = 0.2616
Meta loss on this task batch = 3.4594e-01, Meta loss averaged over last 500 steps = 3.3909e-01, PNorm = 91.5611, GNorm = 0.2582
Meta loss on this task batch = 3.4067e-01, Meta loss averaged over last 500 steps = 3.3917e-01, PNorm = 91.5689, GNorm = 0.2608
Meta loss on this task batch = 3.4629e-01, Meta loss averaged over last 500 steps = 3.3942e-01, PNorm = 91.5786, GNorm = 0.2791
Meta loss on this task batch = 3.6155e-01, Meta loss averaged over last 500 steps = 3.3948e-01, PNorm = 91.5873, GNorm = 0.2779
Meta loss on this task batch = 3.0315e-01, Meta loss averaged over last 500 steps = 3.3938e-01, PNorm = 91.5968, GNorm = 0.2604
Meta loss on this task batch = 3.1850e-01, Meta loss averaged over last 500 steps = 3.3944e-01, PNorm = 91.6064, GNorm = 0.2541
Meta loss on this task batch = 3.3959e-01, Meta loss averaged over last 500 steps = 3.3945e-01, PNorm = 91.6145, GNorm = 0.2694
Meta loss on this task batch = 3.5352e-01, Meta loss averaged over last 500 steps = 3.3946e-01, PNorm = 91.6202, GNorm = 0.2874
Meta loss on this task batch = 3.0757e-01, Meta loss averaged over last 500 steps = 3.3926e-01, PNorm = 91.6265, GNorm = 0.2395
Meta loss on this task batch = 3.1298e-01, Meta loss averaged over last 500 steps = 3.3927e-01, PNorm = 91.6321, GNorm = 0.2445
Meta loss on this task batch = 3.3885e-01, Meta loss averaged over last 500 steps = 3.3930e-01, PNorm = 91.6365, GNorm = 0.3097
Meta loss on this task batch = 3.5339e-01, Meta loss averaged over last 500 steps = 3.3929e-01, PNorm = 91.6424, GNorm = 0.2735
Meta loss on this task batch = 3.7435e-01, Meta loss averaged over last 500 steps = 3.3925e-01, PNorm = 91.6503, GNorm = 0.2563
Meta loss on this task batch = 3.0623e-01, Meta loss averaged over last 500 steps = 3.3917e-01, PNorm = 91.6590, GNorm = 0.2460
Meta loss on this task batch = 3.1394e-01, Meta loss averaged over last 500 steps = 3.3908e-01, PNorm = 91.6692, GNorm = 0.2583
Took 133.3053686618805 seconds to complete one epoch of meta training
Took 141.68741178512573 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501419
Epoch 216
Meta loss on this task batch = 3.4604e-01, Meta loss averaged over last 500 steps = 3.3906e-01, PNorm = 91.6805, GNorm = 0.2584
Meta loss on this task batch = 3.2498e-01, Meta loss averaged over last 500 steps = 3.3893e-01, PNorm = 91.6927, GNorm = 0.2490
Meta loss on this task batch = 3.7791e-01, Meta loss averaged over last 500 steps = 3.3898e-01, PNorm = 91.7048, GNorm = 0.2651
Meta loss on this task batch = 3.4274e-01, Meta loss averaged over last 500 steps = 3.3901e-01, PNorm = 91.7149, GNorm = 0.2826
Meta loss on this task batch = 3.5397e-01, Meta loss averaged over last 500 steps = 3.3899e-01, PNorm = 91.7234, GNorm = 0.2663
Meta loss on this task batch = 3.0276e-01, Meta loss averaged over last 500 steps = 3.3884e-01, PNorm = 91.7329, GNorm = 0.2362
Meta loss on this task batch = 3.4218e-01, Meta loss averaged over last 500 steps = 3.3883e-01, PNorm = 91.7394, GNorm = 0.3041
Meta loss on this task batch = 3.4281e-01, Meta loss averaged over last 500 steps = 3.3886e-01, PNorm = 91.7442, GNorm = 0.2643
Meta loss on this task batch = 3.2882e-01, Meta loss averaged over last 500 steps = 3.3875e-01, PNorm = 91.7482, GNorm = 0.2742
Meta loss on this task batch = 3.2243e-01, Meta loss averaged over last 500 steps = 3.3864e-01, PNorm = 91.7539, GNorm = 0.2738
Meta loss on this task batch = 3.4193e-01, Meta loss averaged over last 500 steps = 3.3856e-01, PNorm = 91.7617, GNorm = 0.2732
Meta loss on this task batch = 3.2371e-01, Meta loss averaged over last 500 steps = 3.3858e-01, PNorm = 91.7700, GNorm = 0.2861
Meta loss on this task batch = 3.2365e-01, Meta loss averaged over last 500 steps = 3.3855e-01, PNorm = 91.7788, GNorm = 0.2677
Meta loss on this task batch = 3.9562e-01, Meta loss averaged over last 500 steps = 3.3873e-01, PNorm = 91.7891, GNorm = 0.3318
Meta loss on this task batch = 3.2190e-01, Meta loss averaged over last 500 steps = 3.3878e-01, PNorm = 91.7993, GNorm = 0.2339
Meta loss on this task batch = 3.2993e-01, Meta loss averaged over last 500 steps = 3.3880e-01, PNorm = 91.8071, GNorm = 0.3065
Meta loss on this task batch = 3.7151e-01, Meta loss averaged over last 500 steps = 3.3886e-01, PNorm = 91.8120, GNorm = 0.2720
Meta loss on this task batch = 3.2031e-01, Meta loss averaged over last 500 steps = 3.3876e-01, PNorm = 91.8179, GNorm = 0.2678
Meta loss on this task batch = 3.1246e-01, Meta loss averaged over last 500 steps = 3.3875e-01, PNorm = 91.8251, GNorm = 0.2690
Took 132.20179343223572 seconds to complete one epoch of meta training
Took 141.1287660598755 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515288
Epoch 217
Meta loss on this task batch = 3.6143e-01, Meta loss averaged over last 500 steps = 3.3884e-01, PNorm = 91.8287, GNorm = 0.2961
Meta loss on this task batch = 3.0242e-01, Meta loss averaged over last 500 steps = 3.3872e-01, PNorm = 91.8333, GNorm = 0.2429
Meta loss on this task batch = 3.1793e-01, Meta loss averaged over last 500 steps = 3.3864e-01, PNorm = 91.8401, GNorm = 0.2471
Meta loss on this task batch = 3.1634e-01, Meta loss averaged over last 500 steps = 3.3867e-01, PNorm = 91.8487, GNorm = 0.2435
Meta loss on this task batch = 3.7124e-01, Meta loss averaged over last 500 steps = 3.3867e-01, PNorm = 91.8606, GNorm = 0.2818
Meta loss on this task batch = 3.9394e-01, Meta loss averaged over last 500 steps = 3.3881e-01, PNorm = 91.8741, GNorm = 0.3339
Meta loss on this task batch = 2.9799e-01, Meta loss averaged over last 500 steps = 3.3872e-01, PNorm = 91.8883, GNorm = 0.2534
Meta loss on this task batch = 3.1312e-01, Meta loss averaged over last 500 steps = 3.3858e-01, PNorm = 91.9001, GNorm = 0.2598
Meta loss on this task batch = 2.8300e-01, Meta loss averaged over last 500 steps = 3.3844e-01, PNorm = 91.9116, GNorm = 0.2894
Meta loss on this task batch = 3.5433e-01, Meta loss averaged over last 500 steps = 3.3846e-01, PNorm = 91.9210, GNorm = 0.2668
Meta loss on this task batch = 3.6183e-01, Meta loss averaged over last 500 steps = 3.3839e-01, PNorm = 91.9263, GNorm = 0.3393
Meta loss on this task batch = 3.7562e-01, Meta loss averaged over last 500 steps = 3.3848e-01, PNorm = 91.9314, GNorm = 0.3383
Meta loss on this task batch = 3.3582e-01, Meta loss averaged over last 500 steps = 3.3848e-01, PNorm = 91.9359, GNorm = 0.2636
Meta loss on this task batch = 2.8190e-01, Meta loss averaged over last 500 steps = 3.3844e-01, PNorm = 91.9401, GNorm = 0.2667
Meta loss on this task batch = 2.6589e-01, Meta loss averaged over last 500 steps = 3.3834e-01, PNorm = 91.9454, GNorm = 0.2690
Meta loss on this task batch = 3.6978e-01, Meta loss averaged over last 500 steps = 3.3841e-01, PNorm = 91.9490, GNorm = 0.3029
Meta loss on this task batch = 3.2487e-01, Meta loss averaged over last 500 steps = 3.3842e-01, PNorm = 91.9525, GNorm = 0.2546
Meta loss on this task batch = 3.5856e-01, Meta loss averaged over last 500 steps = 3.3841e-01, PNorm = 91.9564, GNorm = 0.3048
Meta loss on this task batch = 3.4924e-01, Meta loss averaged over last 500 steps = 3.3849e-01, PNorm = 91.9619, GNorm = 0.3357
Took 134.5746340751648 seconds to complete one epoch of meta training
Took 142.9859118461609 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495159
Epoch 218
Meta loss on this task batch = 3.2677e-01, Meta loss averaged over last 500 steps = 3.3852e-01, PNorm = 91.9676, GNorm = 0.2515
Meta loss on this task batch = 3.9644e-01, Meta loss averaged over last 500 steps = 3.3850e-01, PNorm = 91.9736, GNorm = 0.3266
Meta loss on this task batch = 3.0141e-01, Meta loss averaged over last 500 steps = 3.3852e-01, PNorm = 91.9833, GNorm = 0.2631
Meta loss on this task batch = 2.8611e-01, Meta loss averaged over last 500 steps = 3.3834e-01, PNorm = 91.9944, GNorm = 0.2553
Meta loss on this task batch = 3.4781e-01, Meta loss averaged over last 500 steps = 3.3833e-01, PNorm = 92.0049, GNorm = 0.2696
Meta loss on this task batch = 3.7376e-01, Meta loss averaged over last 500 steps = 3.3841e-01, PNorm = 92.0150, GNorm = 0.2925
Meta loss on this task batch = 3.0835e-01, Meta loss averaged over last 500 steps = 3.3836e-01, PNorm = 92.0254, GNorm = 0.2715
Meta loss on this task batch = 3.5431e-01, Meta loss averaged over last 500 steps = 3.3833e-01, PNorm = 92.0344, GNorm = 0.3253
Meta loss on this task batch = 3.1020e-01, Meta loss averaged over last 500 steps = 3.3816e-01, PNorm = 92.0441, GNorm = 0.2508
Meta loss on this task batch = 3.3629e-01, Meta loss averaged over last 500 steps = 3.3821e-01, PNorm = 92.0527, GNorm = 0.2542
Meta loss on this task batch = 3.9848e-01, Meta loss averaged over last 500 steps = 3.3835e-01, PNorm = 92.0589, GNorm = 0.2784
Meta loss on this task batch = 3.0573e-01, Meta loss averaged over last 500 steps = 3.3827e-01, PNorm = 92.0651, GNorm = 0.3026
Meta loss on this task batch = 3.3644e-01, Meta loss averaged over last 500 steps = 3.3814e-01, PNorm = 92.0697, GNorm = 0.2947
Meta loss on this task batch = 3.6044e-01, Meta loss averaged over last 500 steps = 3.3814e-01, PNorm = 92.0734, GNorm = 0.2685
Meta loss on this task batch = 3.1975e-01, Meta loss averaged over last 500 steps = 3.3810e-01, PNorm = 92.0791, GNorm = 0.2386
Meta loss on this task batch = 3.2317e-01, Meta loss averaged over last 500 steps = 3.3814e-01, PNorm = 92.0840, GNorm = 0.2715
Meta loss on this task batch = 3.1162e-01, Meta loss averaged over last 500 steps = 3.3813e-01, PNorm = 92.0883, GNorm = 0.2687
Meta loss on this task batch = 3.3560e-01, Meta loss averaged over last 500 steps = 3.3813e-01, PNorm = 92.0922, GNorm = 0.2920
Meta loss on this task batch = 3.0140e-01, Meta loss averaged over last 500 steps = 3.3806e-01, PNorm = 92.0969, GNorm = 0.2630
Took 132.57737803459167 seconds to complete one epoch of meta training
Took 141.01460719108582 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503468
Epoch 219
Meta loss on this task batch = 3.2440e-01, Meta loss averaged over last 500 steps = 3.3792e-01, PNorm = 92.1025, GNorm = 0.2623
Meta loss on this task batch = 2.9777e-01, Meta loss averaged over last 500 steps = 3.3794e-01, PNorm = 92.1085, GNorm = 0.2972
Meta loss on this task batch = 3.4373e-01, Meta loss averaged over last 500 steps = 3.3796e-01, PNorm = 92.1127, GNorm = 0.2967
Meta loss on this task batch = 3.1278e-01, Meta loss averaged over last 500 steps = 3.3796e-01, PNorm = 92.1182, GNorm = 0.3031
Meta loss on this task batch = 3.1742e-01, Meta loss averaged over last 500 steps = 3.3801e-01, PNorm = 92.1236, GNorm = 0.2606
Meta loss on this task batch = 3.1871e-01, Meta loss averaged over last 500 steps = 3.3816e-01, PNorm = 92.1310, GNorm = 0.2782
Meta loss on this task batch = 3.0901e-01, Meta loss averaged over last 500 steps = 3.3799e-01, PNorm = 92.1363, GNorm = 0.2929
Meta loss on this task batch = 3.8805e-01, Meta loss averaged over last 500 steps = 3.3802e-01, PNorm = 92.1418, GNorm = 0.3318
Meta loss on this task batch = 2.5648e-01, Meta loss averaged over last 500 steps = 3.3787e-01, PNorm = 92.1483, GNorm = 0.2484
Meta loss on this task batch = 3.8688e-01, Meta loss averaged over last 500 steps = 3.3799e-01, PNorm = 92.1553, GNorm = 0.2869
Meta loss on this task batch = 3.6490e-01, Meta loss averaged over last 500 steps = 3.3803e-01, PNorm = 92.1619, GNorm = 0.2915
Meta loss on this task batch = 3.1681e-01, Meta loss averaged over last 500 steps = 3.3783e-01, PNorm = 92.1692, GNorm = 0.2740
Meta loss on this task batch = 3.3145e-01, Meta loss averaged over last 500 steps = 3.3767e-01, PNorm = 92.1778, GNorm = 0.3382
Meta loss on this task batch = 3.7655e-01, Meta loss averaged over last 500 steps = 3.3785e-01, PNorm = 92.1870, GNorm = 0.3811
Meta loss on this task batch = 3.4961e-01, Meta loss averaged over last 500 steps = 3.3783e-01, PNorm = 92.1956, GNorm = 0.2681
Meta loss on this task batch = 3.0959e-01, Meta loss averaged over last 500 steps = 3.3781e-01, PNorm = 92.2043, GNorm = 0.2634
Meta loss on this task batch = 3.0896e-01, Meta loss averaged over last 500 steps = 3.3779e-01, PNorm = 92.2134, GNorm = 0.2529
Meta loss on this task batch = 3.6807e-01, Meta loss averaged over last 500 steps = 3.3795e-01, PNorm = 92.2218, GNorm = 0.3565
Meta loss on this task batch = 3.2886e-01, Meta loss averaged over last 500 steps = 3.3791e-01, PNorm = 92.2297, GNorm = 0.3361
Took 132.51901865005493 seconds to complete one epoch of meta training
Took 141.39403200149536 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507653
Epoch 220
Meta loss on this task batch = 3.2458e-01, Meta loss averaged over last 500 steps = 3.3787e-01, PNorm = 92.2382, GNorm = 0.2798
Meta loss on this task batch = 3.2999e-01, Meta loss averaged over last 500 steps = 3.3775e-01, PNorm = 92.2459, GNorm = 0.2991
Meta loss on this task batch = 3.2909e-01, Meta loss averaged over last 500 steps = 3.3773e-01, PNorm = 92.2530, GNorm = 0.2542
Meta loss on this task batch = 3.0939e-01, Meta loss averaged over last 500 steps = 3.3763e-01, PNorm = 92.2606, GNorm = 0.2700
Meta loss on this task batch = 3.8686e-01, Meta loss averaged over last 500 steps = 3.3779e-01, PNorm = 92.2688, GNorm = 0.2906
Meta loss on this task batch = 3.1369e-01, Meta loss averaged over last 500 steps = 3.3773e-01, PNorm = 92.2773, GNorm = 0.2760
Meta loss on this task batch = 3.1850e-01, Meta loss averaged over last 500 steps = 3.3765e-01, PNorm = 92.2857, GNorm = 0.2883
Meta loss on this task batch = 3.4833e-01, Meta loss averaged over last 500 steps = 3.3772e-01, PNorm = 92.2958, GNorm = 0.2690
Meta loss on this task batch = 3.7594e-01, Meta loss averaged over last 500 steps = 3.3772e-01, PNorm = 92.3060, GNorm = 0.3124
Meta loss on this task batch = 3.6941e-01, Meta loss averaged over last 500 steps = 3.3777e-01, PNorm = 92.3164, GNorm = 0.3005
Meta loss on this task batch = 2.8869e-01, Meta loss averaged over last 500 steps = 3.3760e-01, PNorm = 92.3293, GNorm = 0.2499
Meta loss on this task batch = 3.3690e-01, Meta loss averaged over last 500 steps = 3.3747e-01, PNorm = 92.3436, GNorm = 0.2742
Meta loss on this task batch = 3.0300e-01, Meta loss averaged over last 500 steps = 3.3737e-01, PNorm = 92.3587, GNorm = 0.2354
Meta loss on this task batch = 3.2304e-01, Meta loss averaged over last 500 steps = 3.3731e-01, PNorm = 92.3733, GNorm = 0.2710
Meta loss on this task batch = 3.6208e-01, Meta loss averaged over last 500 steps = 3.3740e-01, PNorm = 92.3859, GNorm = 0.3303
Meta loss on this task batch = 3.7034e-01, Meta loss averaged over last 500 steps = 3.3755e-01, PNorm = 92.3968, GNorm = 0.3286
Meta loss on this task batch = 3.5119e-01, Meta loss averaged over last 500 steps = 3.3752e-01, PNorm = 92.4055, GNorm = 0.3098
Meta loss on this task batch = 2.6502e-01, Meta loss averaged over last 500 steps = 3.3744e-01, PNorm = 92.4147, GNorm = 0.2379
Meta loss on this task batch = 3.9735e-01, Meta loss averaged over last 500 steps = 3.3756e-01, PNorm = 92.4212, GNorm = 0.4218
Took 131.24568462371826 seconds to complete one epoch of meta training
Took 139.58693099021912 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511988
Epoch 221
Meta loss on this task batch = 3.1126e-01, Meta loss averaged over last 500 steps = 3.3748e-01, PNorm = 92.4282, GNorm = 0.2638
Meta loss on this task batch = 3.4192e-01, Meta loss averaged over last 500 steps = 3.3749e-01, PNorm = 92.4353, GNorm = 0.2805
Meta loss on this task batch = 3.1664e-01, Meta loss averaged over last 500 steps = 3.3747e-01, PNorm = 92.4428, GNorm = 0.2606
Meta loss on this task batch = 3.5032e-01, Meta loss averaged over last 500 steps = 3.3751e-01, PNorm = 92.4502, GNorm = 0.2758
Meta loss on this task batch = 2.8806e-01, Meta loss averaged over last 500 steps = 3.3749e-01, PNorm = 92.4572, GNorm = 0.2486
Meta loss on this task batch = 3.3107e-01, Meta loss averaged over last 500 steps = 3.3755e-01, PNorm = 92.4667, GNorm = 0.2744
Meta loss on this task batch = 3.1143e-01, Meta loss averaged over last 500 steps = 3.3760e-01, PNorm = 92.4753, GNorm = 0.2546
Meta loss on this task batch = 3.1267e-01, Meta loss averaged over last 500 steps = 3.3747e-01, PNorm = 92.4844, GNorm = 0.2895
Meta loss on this task batch = 3.7045e-01, Meta loss averaged over last 500 steps = 3.3764e-01, PNorm = 92.4896, GNorm = 0.3318
Meta loss on this task batch = 3.3736e-01, Meta loss averaged over last 500 steps = 3.3753e-01, PNorm = 92.4903, GNorm = 0.3369
Meta loss on this task batch = 3.8069e-01, Meta loss averaged over last 500 steps = 3.3762e-01, PNorm = 92.4895, GNorm = 0.3331
Meta loss on this task batch = 3.3510e-01, Meta loss averaged over last 500 steps = 3.3758e-01, PNorm = 92.4879, GNorm = 0.2612
Meta loss on this task batch = 3.7169e-01, Meta loss averaged over last 500 steps = 3.3764e-01, PNorm = 92.4831, GNorm = 0.3640
Meta loss on this task batch = 3.1767e-01, Meta loss averaged over last 500 steps = 3.3768e-01, PNorm = 92.4787, GNorm = 0.2996
Meta loss on this task batch = 3.8831e-01, Meta loss averaged over last 500 steps = 3.3780e-01, PNorm = 92.4723, GNorm = 0.3230
Meta loss on this task batch = 3.3130e-01, Meta loss averaged over last 500 steps = 3.3779e-01, PNorm = 92.4682, GNorm = 0.2628
Meta loss on this task batch = 3.3196e-01, Meta loss averaged over last 500 steps = 3.3783e-01, PNorm = 92.4681, GNorm = 0.2643
Meta loss on this task batch = 3.4923e-01, Meta loss averaged over last 500 steps = 3.3775e-01, PNorm = 92.4717, GNorm = 0.2748
Meta loss on this task batch = 2.8134e-01, Meta loss averaged over last 500 steps = 3.3766e-01, PNorm = 92.4807, GNorm = 0.2910
Took 129.62985610961914 seconds to complete one epoch of meta training
Took 138.9532368183136 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499186
Epoch 222
Meta loss on this task batch = 3.4312e-01, Meta loss averaged over last 500 steps = 3.3758e-01, PNorm = 92.4917, GNorm = 0.2743
Meta loss on this task batch = 2.8814e-01, Meta loss averaged over last 500 steps = 3.3753e-01, PNorm = 92.5059, GNorm = 0.2659
Meta loss on this task batch = 3.2259e-01, Meta loss averaged over last 500 steps = 3.3747e-01, PNorm = 92.5200, GNorm = 0.2707
Meta loss on this task batch = 3.3044e-01, Meta loss averaged over last 500 steps = 3.3735e-01, PNorm = 92.5292, GNorm = 0.3555
Meta loss on this task batch = 4.0215e-01, Meta loss averaged over last 500 steps = 3.3743e-01, PNorm = 92.5386, GNorm = 0.3603
Meta loss on this task batch = 3.4000e-01, Meta loss averaged over last 500 steps = 3.3737e-01, PNorm = 92.5455, GNorm = 0.2849
Meta loss on this task batch = 3.7771e-01, Meta loss averaged over last 500 steps = 3.3748e-01, PNorm = 92.5514, GNorm = 0.3736
Meta loss on this task batch = 3.2429e-01, Meta loss averaged over last 500 steps = 3.3740e-01, PNorm = 92.5566, GNorm = 0.2238
Meta loss on this task batch = 3.1829e-01, Meta loss averaged over last 500 steps = 3.3747e-01, PNorm = 92.5610, GNorm = 0.2532
Meta loss on this task batch = 3.5872e-01, Meta loss averaged over last 500 steps = 3.3737e-01, PNorm = 92.5651, GNorm = 0.2702
Meta loss on this task batch = 2.9656e-01, Meta loss averaged over last 500 steps = 3.3729e-01, PNorm = 92.5710, GNorm = 0.2845
Meta loss on this task batch = 2.9954e-01, Meta loss averaged over last 500 steps = 3.3716e-01, PNorm = 92.5794, GNorm = 0.2608
Meta loss on this task batch = 3.1031e-01, Meta loss averaged over last 500 steps = 3.3702e-01, PNorm = 92.5885, GNorm = 0.2743
Meta loss on this task batch = 3.1652e-01, Meta loss averaged over last 500 steps = 3.3708e-01, PNorm = 92.5979, GNorm = 0.2541
Meta loss on this task batch = 3.3646e-01, Meta loss averaged over last 500 steps = 3.3713e-01, PNorm = 92.6074, GNorm = 0.2618
Meta loss on this task batch = 3.3828e-01, Meta loss averaged over last 500 steps = 3.3708e-01, PNorm = 92.6172, GNorm = 0.2766
Meta loss on this task batch = 3.2698e-01, Meta loss averaged over last 500 steps = 3.3706e-01, PNorm = 92.6270, GNorm = 0.2662
Meta loss on this task batch = 3.4488e-01, Meta loss averaged over last 500 steps = 3.3712e-01, PNorm = 92.6355, GNorm = 0.3269
Meta loss on this task batch = 3.4226e-01, Meta loss averaged over last 500 steps = 3.3721e-01, PNorm = 92.6440, GNorm = 0.3102
Took 132.75509357452393 seconds to complete one epoch of meta training
Took 141.35986185073853 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515891
Epoch 223
Meta loss on this task batch = 3.7265e-01, Meta loss averaged over last 500 steps = 3.3722e-01, PNorm = 92.6506, GNorm = 0.2907
Meta loss on this task batch = 3.7611e-01, Meta loss averaged over last 500 steps = 3.3724e-01, PNorm = 92.6566, GNorm = 0.2958
Meta loss on this task batch = 2.9143e-01, Meta loss averaged over last 500 steps = 3.3710e-01, PNorm = 92.6628, GNorm = 0.2753
Meta loss on this task batch = 3.5295e-01, Meta loss averaged over last 500 steps = 3.3726e-01, PNorm = 92.6666, GNorm = 0.3296
Meta loss on this task batch = 2.8979e-01, Meta loss averaged over last 500 steps = 3.3716e-01, PNorm = 92.6702, GNorm = 0.3223
Meta loss on this task batch = 3.2113e-01, Meta loss averaged over last 500 steps = 3.3718e-01, PNorm = 92.6753, GNorm = 0.2777
Meta loss on this task batch = 3.0536e-01, Meta loss averaged over last 500 steps = 3.3710e-01, PNorm = 92.6820, GNorm = 0.2579
Meta loss on this task batch = 3.1949e-01, Meta loss averaged over last 500 steps = 3.3707e-01, PNorm = 92.6892, GNorm = 0.2647
Meta loss on this task batch = 2.7222e-01, Meta loss averaged over last 500 steps = 3.3696e-01, PNorm = 92.6974, GNorm = 0.2577
Meta loss on this task batch = 3.3013e-01, Meta loss averaged over last 500 steps = 3.3694e-01, PNorm = 92.7094, GNorm = 0.3076
Meta loss on this task batch = 3.8767e-01, Meta loss averaged over last 500 steps = 3.3696e-01, PNorm = 92.7214, GNorm = 0.2907
Meta loss on this task batch = 3.4372e-01, Meta loss averaged over last 500 steps = 3.3708e-01, PNorm = 92.7323, GNorm = 0.2773
Meta loss on this task batch = 3.1449e-01, Meta loss averaged over last 500 steps = 3.3701e-01, PNorm = 92.7431, GNorm = 0.2669
Meta loss on this task batch = 3.0435e-01, Meta loss averaged over last 500 steps = 3.3707e-01, PNorm = 92.7495, GNorm = 0.2701
Meta loss on this task batch = 3.1939e-01, Meta loss averaged over last 500 steps = 3.3703e-01, PNorm = 92.7548, GNorm = 0.2673
Meta loss on this task batch = 3.6073e-01, Meta loss averaged over last 500 steps = 3.3708e-01, PNorm = 92.7561, GNorm = 0.3663
Meta loss on this task batch = 3.8887e-01, Meta loss averaged over last 500 steps = 3.3723e-01, PNorm = 92.7564, GNorm = 0.3039
Meta loss on this task batch = 3.1560e-01, Meta loss averaged over last 500 steps = 3.3720e-01, PNorm = 92.7575, GNorm = 0.2412
Meta loss on this task batch = 3.7632e-01, Meta loss averaged over last 500 steps = 3.3733e-01, PNorm = 92.7598, GNorm = 0.3287
Took 132.92856740951538 seconds to complete one epoch of meta training
Took 142.0661551952362 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509500
Epoch 224
Meta loss on this task batch = 3.2965e-01, Meta loss averaged over last 500 steps = 3.3725e-01, PNorm = 92.7633, GNorm = 0.2428
Meta loss on this task batch = 2.7292e-01, Meta loss averaged over last 500 steps = 3.3718e-01, PNorm = 92.7673, GNorm = 0.2538
Meta loss on this task batch = 3.0261e-01, Meta loss averaged over last 500 steps = 3.3709e-01, PNorm = 92.7727, GNorm = 0.2494
Meta loss on this task batch = 3.6880e-01, Meta loss averaged over last 500 steps = 3.3714e-01, PNorm = 92.7777, GNorm = 0.2810
Meta loss on this task batch = 3.3702e-01, Meta loss averaged over last 500 steps = 3.3703e-01, PNorm = 92.7832, GNorm = 0.2762
Meta loss on this task batch = 3.3192e-01, Meta loss averaged over last 500 steps = 3.3702e-01, PNorm = 92.7897, GNorm = 0.2817
Meta loss on this task batch = 2.9558e-01, Meta loss averaged over last 500 steps = 3.3693e-01, PNorm = 92.7977, GNorm = 0.2616
Meta loss on this task batch = 3.0402e-01, Meta loss averaged over last 500 steps = 3.3685e-01, PNorm = 92.8081, GNorm = 0.2748
Meta loss on this task batch = 4.0638e-01, Meta loss averaged over last 500 steps = 3.3689e-01, PNorm = 92.8182, GNorm = 0.3372
Meta loss on this task batch = 3.6844e-01, Meta loss averaged over last 500 steps = 3.3690e-01, PNorm = 92.8260, GNorm = 0.3096
Meta loss on this task batch = 2.5719e-01, Meta loss averaged over last 500 steps = 3.3678e-01, PNorm = 92.8340, GNorm = 0.2571
Meta loss on this task batch = 2.8403e-01, Meta loss averaged over last 500 steps = 3.3679e-01, PNorm = 92.8425, GNorm = 0.2719
Meta loss on this task batch = 3.5378e-01, Meta loss averaged over last 500 steps = 3.3677e-01, PNorm = 92.8492, GNorm = 0.3145
Meta loss on this task batch = 3.4161e-01, Meta loss averaged over last 500 steps = 3.3689e-01, PNorm = 92.8545, GNorm = 0.3105
Meta loss on this task batch = 3.3106e-01, Meta loss averaged over last 500 steps = 3.3690e-01, PNorm = 92.8573, GNorm = 0.3150
Meta loss on this task batch = 3.5410e-01, Meta loss averaged over last 500 steps = 3.3689e-01, PNorm = 92.8600, GNorm = 0.3076
Meta loss on this task batch = 3.5553e-01, Meta loss averaged over last 500 steps = 3.3688e-01, PNorm = 92.8619, GNorm = 0.2945
Meta loss on this task batch = 3.3565e-01, Meta loss averaged over last 500 steps = 3.3698e-01, PNorm = 92.8627, GNorm = 0.3096
Meta loss on this task batch = 3.4266e-01, Meta loss averaged over last 500 steps = 3.3701e-01, PNorm = 92.8627, GNorm = 0.3100
Took 137.0791187286377 seconds to complete one epoch of meta training
Took 145.82307529449463 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.539620
Found better MAML checkpoint after meta validation, saving now
Epoch 225
Meta loss on this task batch = 3.4029e-01, Meta loss averaged over last 500 steps = 3.3702e-01, PNorm = 92.8639, GNorm = 0.2873
Meta loss on this task batch = 3.2815e-01, Meta loss averaged over last 500 steps = 3.3700e-01, PNorm = 92.8654, GNorm = 0.2915
Meta loss on this task batch = 3.0308e-01, Meta loss averaged over last 500 steps = 3.3681e-01, PNorm = 92.8688, GNorm = 0.2467
Meta loss on this task batch = 3.1811e-01, Meta loss averaged over last 500 steps = 3.3668e-01, PNorm = 92.8741, GNorm = 0.2551
Meta loss on this task batch = 3.0673e-01, Meta loss averaged over last 500 steps = 3.3657e-01, PNorm = 92.8786, GNorm = 0.2545
Meta loss on this task batch = 3.4342e-01, Meta loss averaged over last 500 steps = 3.3660e-01, PNorm = 92.8822, GNorm = 0.2563
Meta loss on this task batch = 3.4450e-01, Meta loss averaged over last 500 steps = 3.3666e-01, PNorm = 92.8853, GNorm = 0.2609
Meta loss on this task batch = 2.4596e-01, Meta loss averaged over last 500 steps = 3.3642e-01, PNorm = 92.8910, GNorm = 0.2440
Meta loss on this task batch = 3.5184e-01, Meta loss averaged over last 500 steps = 3.3644e-01, PNorm = 92.8964, GNorm = 0.3322
Meta loss on this task batch = 3.2453e-01, Meta loss averaged over last 500 steps = 3.3633e-01, PNorm = 92.9019, GNorm = 0.2479
Meta loss on this task batch = 3.4037e-01, Meta loss averaged over last 500 steps = 3.3622e-01, PNorm = 92.9058, GNorm = 0.2642
Meta loss on this task batch = 3.5852e-01, Meta loss averaged over last 500 steps = 3.3632e-01, PNorm = 92.9113, GNorm = 0.2920
Meta loss on this task batch = 3.0414e-01, Meta loss averaged over last 500 steps = 3.3617e-01, PNorm = 92.9170, GNorm = 0.2442
Meta loss on this task batch = 3.3007e-01, Meta loss averaged over last 500 steps = 3.3613e-01, PNorm = 92.9225, GNorm = 0.2592
Meta loss on this task batch = 3.3576e-01, Meta loss averaged over last 500 steps = 3.3612e-01, PNorm = 92.9276, GNorm = 0.2852
Meta loss on this task batch = 3.5977e-01, Meta loss averaged over last 500 steps = 3.3617e-01, PNorm = 92.9320, GNorm = 0.2847
Meta loss on this task batch = 3.2393e-01, Meta loss averaged over last 500 steps = 3.3610e-01, PNorm = 92.9375, GNorm = 0.2753
Meta loss on this task batch = 3.1666e-01, Meta loss averaged over last 500 steps = 3.3607e-01, PNorm = 92.9424, GNorm = 0.3083
Meta loss on this task batch = 3.6921e-01, Meta loss averaged over last 500 steps = 3.3631e-01, PNorm = 92.9497, GNorm = 0.4121
Took 132.1377296447754 seconds to complete one epoch of meta training
Took 140.70369029045105 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.519065
Epoch 226
Meta loss on this task batch = 3.6483e-01, Meta loss averaged over last 500 steps = 3.3632e-01, PNorm = 92.9585, GNorm = 0.2829
Meta loss on this task batch = 3.2515e-01, Meta loss averaged over last 500 steps = 3.3631e-01, PNorm = 92.9669, GNorm = 0.2646
Meta loss on this task batch = 3.1445e-01, Meta loss averaged over last 500 steps = 3.3617e-01, PNorm = 92.9771, GNorm = 0.2703
Meta loss on this task batch = 3.0474e-01, Meta loss averaged over last 500 steps = 3.3609e-01, PNorm = 92.9859, GNorm = 0.2614
Meta loss on this task batch = 3.3614e-01, Meta loss averaged over last 500 steps = 3.3607e-01, PNorm = 92.9950, GNorm = 0.2711
Meta loss on this task batch = 3.1592e-01, Meta loss averaged over last 500 steps = 3.3617e-01, PNorm = 93.0030, GNorm = 0.2465
Meta loss on this task batch = 3.5995e-01, Meta loss averaged over last 500 steps = 3.3624e-01, PNorm = 93.0105, GNorm = 0.2926
Meta loss on this task batch = 4.0096e-01, Meta loss averaged over last 500 steps = 3.3636e-01, PNorm = 93.0145, GNorm = 0.3410
Meta loss on this task batch = 3.5728e-01, Meta loss averaged over last 500 steps = 3.3633e-01, PNorm = 93.0179, GNorm = 0.2553
Meta loss on this task batch = 3.1025e-01, Meta loss averaged over last 500 steps = 3.3622e-01, PNorm = 93.0204, GNorm = 0.2424
Meta loss on this task batch = 2.9554e-01, Meta loss averaged over last 500 steps = 3.3608e-01, PNorm = 93.0245, GNorm = 0.2662
Meta loss on this task batch = 2.7562e-01, Meta loss averaged over last 500 steps = 3.3592e-01, PNorm = 93.0289, GNorm = 0.2514
Meta loss on this task batch = 3.4671e-01, Meta loss averaged over last 500 steps = 3.3601e-01, PNorm = 93.0318, GNorm = 0.2661
Meta loss on this task batch = 3.5215e-01, Meta loss averaged over last 500 steps = 3.3595e-01, PNorm = 93.0350, GNorm = 0.3135
Meta loss on this task batch = 3.2324e-01, Meta loss averaged over last 500 steps = 3.3590e-01, PNorm = 93.0378, GNorm = 0.2575
Meta loss on this task batch = 3.3240e-01, Meta loss averaged over last 500 steps = 3.3600e-01, PNorm = 93.0425, GNorm = 0.2493
Meta loss on this task batch = 2.9054e-01, Meta loss averaged over last 500 steps = 3.3591e-01, PNorm = 93.0508, GNorm = 0.2652
Meta loss on this task batch = 2.8814e-01, Meta loss averaged over last 500 steps = 3.3576e-01, PNorm = 93.0607, GNorm = 0.2691
Meta loss on this task batch = 3.6400e-01, Meta loss averaged over last 500 steps = 3.3590e-01, PNorm = 93.0696, GNorm = 0.3695
Took 132.75028324127197 seconds to complete one epoch of meta training
Took 142.15122652053833 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480522
Epoch 227
Meta loss on this task batch = 3.8612e-01, Meta loss averaged over last 500 steps = 3.3602e-01, PNorm = 93.0799, GNorm = 0.3066
Meta loss on this task batch = 2.9557e-01, Meta loss averaged over last 500 steps = 3.3592e-01, PNorm = 93.0903, GNorm = 0.2804
Meta loss on this task batch = 3.1137e-01, Meta loss averaged over last 500 steps = 3.3581e-01, PNorm = 93.0989, GNorm = 0.2947
Meta loss on this task batch = 2.3341e-01, Meta loss averaged over last 500 steps = 3.3560e-01, PNorm = 93.1059, GNorm = 0.2341
Meta loss on this task batch = 2.8685e-01, Meta loss averaged over last 500 steps = 3.3549e-01, PNorm = 93.1094, GNorm = 0.2707
Meta loss on this task batch = 3.2166e-01, Meta loss averaged over last 500 steps = 3.3549e-01, PNorm = 93.1127, GNorm = 0.2670
Meta loss on this task batch = 3.7990e-01, Meta loss averaged over last 500 steps = 3.3551e-01, PNorm = 93.1162, GNorm = 0.3193
Meta loss on this task batch = 3.0878e-01, Meta loss averaged over last 500 steps = 3.3554e-01, PNorm = 93.1205, GNorm = 0.2472
Meta loss on this task batch = 3.1790e-01, Meta loss averaged over last 500 steps = 3.3539e-01, PNorm = 93.1257, GNorm = 0.2926
Meta loss on this task batch = 3.5823e-01, Meta loss averaged over last 500 steps = 3.3548e-01, PNorm = 93.1312, GNorm = 0.2934
Meta loss on this task batch = 3.1971e-01, Meta loss averaged over last 500 steps = 3.3544e-01, PNorm = 93.1374, GNorm = 0.2641
Meta loss on this task batch = 3.6817e-01, Meta loss averaged over last 500 steps = 3.3548e-01, PNorm = 93.1465, GNorm = 0.3057
Meta loss on this task batch = 3.8294e-01, Meta loss averaged over last 500 steps = 3.3565e-01, PNorm = 93.1524, GNorm = 0.3164
Meta loss on this task batch = 2.9857e-01, Meta loss averaged over last 500 steps = 3.3554e-01, PNorm = 93.1588, GNorm = 0.2756
Meta loss on this task batch = 2.9960e-01, Meta loss averaged over last 500 steps = 3.3556e-01, PNorm = 93.1663, GNorm = 0.2735
Meta loss on this task batch = 3.2360e-01, Meta loss averaged over last 500 steps = 3.3556e-01, PNorm = 93.1736, GNorm = 0.2861
Meta loss on this task batch = 3.4620e-01, Meta loss averaged over last 500 steps = 3.3564e-01, PNorm = 93.1801, GNorm = 0.2795
Meta loss on this task batch = 3.1791e-01, Meta loss averaged over last 500 steps = 3.3553e-01, PNorm = 93.1858, GNorm = 0.2704
Meta loss on this task batch = 2.9124e-01, Meta loss averaged over last 500 steps = 3.3544e-01, PNorm = 93.1936, GNorm = 0.2962
Took 133.99081230163574 seconds to complete one epoch of meta training
Took 142.6826274394989 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461054
Epoch 228
Meta loss on this task batch = 3.4575e-01, Meta loss averaged over last 500 steps = 3.3543e-01, PNorm = 93.2038, GNorm = 0.2669
Meta loss on this task batch = 3.2187e-01, Meta loss averaged over last 500 steps = 3.3542e-01, PNorm = 93.2111, GNorm = 0.2852
Meta loss on this task batch = 3.2313e-01, Meta loss averaged over last 500 steps = 3.3533e-01, PNorm = 93.2167, GNorm = 0.2426
Meta loss on this task batch = 3.3947e-01, Meta loss averaged over last 500 steps = 3.3528e-01, PNorm = 93.2227, GNorm = 0.2617
Meta loss on this task batch = 3.7892e-01, Meta loss averaged over last 500 steps = 3.3534e-01, PNorm = 93.2282, GNorm = 0.3662
Meta loss on this task batch = 2.8676e-01, Meta loss averaged over last 500 steps = 3.3514e-01, PNorm = 93.2352, GNorm = 0.2447
Meta loss on this task batch = 2.7132e-01, Meta loss averaged over last 500 steps = 3.3496e-01, PNorm = 93.2421, GNorm = 0.2367
Meta loss on this task batch = 3.0957e-01, Meta loss averaged over last 500 steps = 3.3500e-01, PNorm = 93.2474, GNorm = 0.3079
Meta loss on this task batch = 3.3355e-01, Meta loss averaged over last 500 steps = 3.3501e-01, PNorm = 93.2536, GNorm = 0.2963
Meta loss on this task batch = 3.4025e-01, Meta loss averaged over last 500 steps = 3.3502e-01, PNorm = 93.2619, GNorm = 0.2734
Meta loss on this task batch = 3.6208e-01, Meta loss averaged over last 500 steps = 3.3500e-01, PNorm = 93.2694, GNorm = 0.3134
Meta loss on this task batch = 3.5955e-01, Meta loss averaged over last 500 steps = 3.3499e-01, PNorm = 93.2758, GNorm = 0.3245
Meta loss on this task batch = 3.2267e-01, Meta loss averaged over last 500 steps = 3.3500e-01, PNorm = 93.2835, GNorm = 0.3051
Meta loss on this task batch = 3.4563e-01, Meta loss averaged over last 500 steps = 3.3500e-01, PNorm = 93.2912, GNorm = 0.3001
Meta loss on this task batch = 3.4886e-01, Meta loss averaged over last 500 steps = 3.3505e-01, PNorm = 93.2987, GNorm = 0.2614
Meta loss on this task batch = 3.0250e-01, Meta loss averaged over last 500 steps = 3.3491e-01, PNorm = 93.3054, GNorm = 0.2596
Meta loss on this task batch = 3.6613e-01, Meta loss averaged over last 500 steps = 3.3501e-01, PNorm = 93.3138, GNorm = 0.2964
Meta loss on this task batch = 3.2941e-01, Meta loss averaged over last 500 steps = 3.3501e-01, PNorm = 93.3210, GNorm = 0.2757
Meta loss on this task batch = 2.4451e-01, Meta loss averaged over last 500 steps = 3.3492e-01, PNorm = 93.3299, GNorm = 0.2984
Took 130.44481253623962 seconds to complete one epoch of meta training
Took 138.94609785079956 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500280
Epoch 229
Meta loss on this task batch = 2.7987e-01, Meta loss averaged over last 500 steps = 3.3488e-01, PNorm = 93.3385, GNorm = 0.2523
Meta loss on this task batch = 3.1393e-01, Meta loss averaged over last 500 steps = 3.3484e-01, PNorm = 93.3473, GNorm = 0.2742
Meta loss on this task batch = 2.9941e-01, Meta loss averaged over last 500 steps = 3.3479e-01, PNorm = 93.3566, GNorm = 0.2885
Meta loss on this task batch = 3.5181e-01, Meta loss averaged over last 500 steps = 3.3483e-01, PNorm = 93.3640, GNorm = 0.3154
Meta loss on this task batch = 3.5372e-01, Meta loss averaged over last 500 steps = 3.3470e-01, PNorm = 93.3707, GNorm = 0.2741
Meta loss on this task batch = 3.0858e-01, Meta loss averaged over last 500 steps = 3.3462e-01, PNorm = 93.3772, GNorm = 0.3030
Meta loss on this task batch = 3.2156e-01, Meta loss averaged over last 500 steps = 3.3453e-01, PNorm = 93.3845, GNorm = 0.2815
Meta loss on this task batch = 3.5453e-01, Meta loss averaged over last 500 steps = 3.3459e-01, PNorm = 93.3919, GNorm = 0.3049
Meta loss on this task batch = 2.9883e-01, Meta loss averaged over last 500 steps = 3.3463e-01, PNorm = 93.3992, GNorm = 0.3200
Meta loss on this task batch = 2.9882e-01, Meta loss averaged over last 500 steps = 3.3450e-01, PNorm = 93.4049, GNorm = 0.2832
Meta loss on this task batch = 2.7356e-01, Meta loss averaged over last 500 steps = 3.3439e-01, PNorm = 93.4103, GNorm = 0.2503
Meta loss on this task batch = 3.4561e-01, Meta loss averaged over last 500 steps = 3.3439e-01, PNorm = 93.4182, GNorm = 0.3046
Meta loss on this task batch = 2.4989e-01, Meta loss averaged over last 500 steps = 3.3429e-01, PNorm = 93.4275, GNorm = 0.2414
Meta loss on this task batch = 3.5664e-01, Meta loss averaged over last 500 steps = 3.3422e-01, PNorm = 93.4362, GNorm = 0.2977
Meta loss on this task batch = 3.3452e-01, Meta loss averaged over last 500 steps = 3.3424e-01, PNorm = 93.4444, GNorm = 0.2734
Meta loss on this task batch = 3.6425e-01, Meta loss averaged over last 500 steps = 3.3422e-01, PNorm = 93.4522, GNorm = 0.2793
Meta loss on this task batch = 3.3307e-01, Meta loss averaged over last 500 steps = 3.3425e-01, PNorm = 93.4591, GNorm = 0.3016
Meta loss on this task batch = 3.3950e-01, Meta loss averaged over last 500 steps = 3.3428e-01, PNorm = 93.4668, GNorm = 0.2569
Meta loss on this task batch = 3.4879e-01, Meta loss averaged over last 500 steps = 3.3434e-01, PNorm = 93.4744, GNorm = 0.3543
Took 135.77863216400146 seconds to complete one epoch of meta training
Took 144.26668643951416 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.533086
Epoch 230
Meta loss on this task batch = 2.7812e-01, Meta loss averaged over last 500 steps = 3.3425e-01, PNorm = 93.4819, GNorm = 0.2720
Meta loss on this task batch = 3.2286e-01, Meta loss averaged over last 500 steps = 3.3415e-01, PNorm = 93.4882, GNorm = 0.3049
Meta loss on this task batch = 3.2116e-01, Meta loss averaged over last 500 steps = 3.3424e-01, PNorm = 93.4972, GNorm = 0.2972
Meta loss on this task batch = 3.6926e-01, Meta loss averaged over last 500 steps = 3.3435e-01, PNorm = 93.5040, GNorm = 0.2882
Meta loss on this task batch = 3.5280e-01, Meta loss averaged over last 500 steps = 3.3428e-01, PNorm = 93.5118, GNorm = 0.2605
Meta loss on this task batch = 2.8129e-01, Meta loss averaged over last 500 steps = 3.3413e-01, PNorm = 93.5207, GNorm = 0.2570
Meta loss on this task batch = 2.9671e-01, Meta loss averaged over last 500 steps = 3.3399e-01, PNorm = 93.5314, GNorm = 0.2808
Meta loss on this task batch = 3.7443e-01, Meta loss averaged over last 500 steps = 3.3403e-01, PNorm = 93.5394, GNorm = 0.2780
Meta loss on this task batch = 3.1934e-01, Meta loss averaged over last 500 steps = 3.3393e-01, PNorm = 93.5473, GNorm = 0.2634
Meta loss on this task batch = 3.7486e-01, Meta loss averaged over last 500 steps = 3.3392e-01, PNorm = 93.5545, GNorm = 0.3171
Meta loss on this task batch = 3.4327e-01, Meta loss averaged over last 500 steps = 3.3397e-01, PNorm = 93.5621, GNorm = 0.2901
Meta loss on this task batch = 3.4206e-01, Meta loss averaged over last 500 steps = 3.3395e-01, PNorm = 93.5701, GNorm = 0.3233
Meta loss on this task batch = 2.9541e-01, Meta loss averaged over last 500 steps = 3.3397e-01, PNorm = 93.5791, GNorm = 0.2472
Meta loss on this task batch = 3.5543e-01, Meta loss averaged over last 500 steps = 3.3410e-01, PNorm = 93.5859, GNorm = 0.2885
Meta loss on this task batch = 3.9554e-01, Meta loss averaged over last 500 steps = 3.3425e-01, PNorm = 93.5935, GNorm = 0.3055
Meta loss on this task batch = 3.2342e-01, Meta loss averaged over last 500 steps = 3.3423e-01, PNorm = 93.5995, GNorm = 0.2684
Meta loss on this task batch = 3.0680e-01, Meta loss averaged over last 500 steps = 3.3421e-01, PNorm = 93.6070, GNorm = 0.2886
Meta loss on this task batch = 3.0405e-01, Meta loss averaged over last 500 steps = 3.3411e-01, PNorm = 93.6136, GNorm = 0.2712
Meta loss on this task batch = 3.5671e-01, Meta loss averaged over last 500 steps = 3.3417e-01, PNorm = 93.6202, GNorm = 0.3580
Took 133.55313420295715 seconds to complete one epoch of meta training
Took 142.26040744781494 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500225
Epoch 231
Meta loss on this task batch = 4.3893e-01, Meta loss averaged over last 500 steps = 3.3442e-01, PNorm = 93.6241, GNorm = 0.4226
Meta loss on this task batch = 3.1795e-01, Meta loss averaged over last 500 steps = 3.3434e-01, PNorm = 93.6278, GNorm = 0.2847
Meta loss on this task batch = 3.5330e-01, Meta loss averaged over last 500 steps = 3.3441e-01, PNorm = 93.6303, GNorm = 0.3239
Meta loss on this task batch = 3.2059e-01, Meta loss averaged over last 500 steps = 3.3444e-01, PNorm = 93.6322, GNorm = 0.2675
Meta loss on this task batch = 3.3448e-01, Meta loss averaged over last 500 steps = 3.3433e-01, PNorm = 93.6361, GNorm = 0.2409
Meta loss on this task batch = 3.2360e-01, Meta loss averaged over last 500 steps = 3.3433e-01, PNorm = 93.6397, GNorm = 0.2429
Meta loss on this task batch = 2.8279e-01, Meta loss averaged over last 500 steps = 3.3428e-01, PNorm = 93.6454, GNorm = 0.2276
Meta loss on this task batch = 3.4187e-01, Meta loss averaged over last 500 steps = 3.3432e-01, PNorm = 93.6504, GNorm = 0.2883
Meta loss on this task batch = 3.5578e-01, Meta loss averaged over last 500 steps = 3.3431e-01, PNorm = 93.6561, GNorm = 0.2826
Meta loss on this task batch = 2.7954e-01, Meta loss averaged over last 500 steps = 3.3406e-01, PNorm = 93.6634, GNorm = 0.2552
Meta loss on this task batch = 3.6435e-01, Meta loss averaged over last 500 steps = 3.3411e-01, PNorm = 93.6707, GNorm = 0.2782
Meta loss on this task batch = 3.0193e-01, Meta loss averaged over last 500 steps = 3.3395e-01, PNorm = 93.6793, GNorm = 0.2619
Meta loss on this task batch = 3.0395e-01, Meta loss averaged over last 500 steps = 3.3383e-01, PNorm = 93.6882, GNorm = 0.2733
Meta loss on this task batch = 2.8440e-01, Meta loss averaged over last 500 steps = 3.3382e-01, PNorm = 93.6970, GNorm = 0.2754
Meta loss on this task batch = 3.6201e-01, Meta loss averaged over last 500 steps = 3.3388e-01, PNorm = 93.7033, GNorm = 0.3587
Meta loss on this task batch = 3.3701e-01, Meta loss averaged over last 500 steps = 3.3393e-01, PNorm = 93.7095, GNorm = 0.3066
Meta loss on this task batch = 3.6647e-01, Meta loss averaged over last 500 steps = 3.3412e-01, PNorm = 93.7148, GNorm = 0.2858
Meta loss on this task batch = 3.2812e-01, Meta loss averaged over last 500 steps = 3.3410e-01, PNorm = 93.7208, GNorm = 0.2455
Meta loss on this task batch = 2.6242e-01, Meta loss averaged over last 500 steps = 3.3400e-01, PNorm = 93.7297, GNorm = 0.3059
Took 135.64289546012878 seconds to complete one epoch of meta training
Took 144.3655469417572 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495648
Epoch 232
Meta loss on this task batch = 3.0042e-01, Meta loss averaged over last 500 steps = 3.3402e-01, PNorm = 93.7398, GNorm = 0.2482
Meta loss on this task batch = 2.8998e-01, Meta loss averaged over last 500 steps = 3.3386e-01, PNorm = 93.7500, GNorm = 0.2898
Meta loss on this task batch = 3.3425e-01, Meta loss averaged over last 500 steps = 3.3389e-01, PNorm = 93.7609, GNorm = 0.3645
Meta loss on this task batch = 2.9657e-01, Meta loss averaged over last 500 steps = 3.3375e-01, PNorm = 93.7725, GNorm = 0.2599
Meta loss on this task batch = 2.9416e-01, Meta loss averaged over last 500 steps = 3.3358e-01, PNorm = 93.7816, GNorm = 0.2877
Meta loss on this task batch = 3.8954e-01, Meta loss averaged over last 500 steps = 3.3363e-01, PNorm = 93.7848, GNorm = 0.4330
Meta loss on this task batch = 3.2731e-01, Meta loss averaged over last 500 steps = 3.3356e-01, PNorm = 93.7882, GNorm = 0.2785
Meta loss on this task batch = 3.7842e-01, Meta loss averaged over last 500 steps = 3.3367e-01, PNorm = 93.7944, GNorm = 0.3478
Meta loss on this task batch = 3.2723e-01, Meta loss averaged over last 500 steps = 3.3365e-01, PNorm = 93.8031, GNorm = 0.2746
Meta loss on this task batch = 3.6863e-01, Meta loss averaged over last 500 steps = 3.3368e-01, PNorm = 93.8144, GNorm = 0.2936
Meta loss on this task batch = 3.5689e-01, Meta loss averaged over last 500 steps = 3.3376e-01, PNorm = 93.8271, GNorm = 0.3290
Meta loss on this task batch = 3.6007e-01, Meta loss averaged over last 500 steps = 3.3386e-01, PNorm = 93.8397, GNorm = 0.3066
Meta loss on this task batch = 3.4529e-01, Meta loss averaged over last 500 steps = 3.3399e-01, PNorm = 93.8533, GNorm = 0.2755
Meta loss on this task batch = 3.5807e-01, Meta loss averaged over last 500 steps = 3.3393e-01, PNorm = 93.8671, GNorm = 0.3003
Meta loss on this task batch = 3.2404e-01, Meta loss averaged over last 500 steps = 3.3387e-01, PNorm = 93.8789, GNorm = 0.3536
Meta loss on this task batch = 3.5362e-01, Meta loss averaged over last 500 steps = 3.3383e-01, PNorm = 93.8881, GNorm = 0.2917
Meta loss on this task batch = 3.4157e-01, Meta loss averaged over last 500 steps = 3.3399e-01, PNorm = 93.8954, GNorm = 0.2898
Meta loss on this task batch = 3.2495e-01, Meta loss averaged over last 500 steps = 3.3405e-01, PNorm = 93.9002, GNorm = 0.2809
Meta loss on this task batch = 2.9451e-01, Meta loss averaged over last 500 steps = 3.3394e-01, PNorm = 93.9042, GNorm = 0.3102
Took 133.76174068450928 seconds to complete one epoch of meta training
Took 142.3092610836029 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493098
Epoch 233
Meta loss on this task batch = 2.7794e-01, Meta loss averaged over last 500 steps = 3.3368e-01, PNorm = 93.9092, GNorm = 0.2617
Meta loss on this task batch = 3.3157e-01, Meta loss averaged over last 500 steps = 3.3369e-01, PNorm = 93.9138, GNorm = 0.3479
Meta loss on this task batch = 2.8583e-01, Meta loss averaged over last 500 steps = 3.3356e-01, PNorm = 93.9205, GNorm = 0.2782
Meta loss on this task batch = 3.5895e-01, Meta loss averaged over last 500 steps = 3.3363e-01, PNorm = 93.9256, GNorm = 0.2923
Meta loss on this task batch = 3.0941e-01, Meta loss averaged over last 500 steps = 3.3346e-01, PNorm = 93.9314, GNorm = 0.2438
Meta loss on this task batch = 2.8447e-01, Meta loss averaged over last 500 steps = 3.3320e-01, PNorm = 93.9378, GNorm = 0.2656
Meta loss on this task batch = 3.5906e-01, Meta loss averaged over last 500 steps = 3.3317e-01, PNorm = 93.9429, GNorm = 0.2946
Meta loss on this task batch = 3.2429e-01, Meta loss averaged over last 500 steps = 3.3324e-01, PNorm = 93.9526, GNorm = 0.2501
Meta loss on this task batch = 3.4920e-01, Meta loss averaged over last 500 steps = 3.3327e-01, PNorm = 93.9631, GNorm = 0.2586
Meta loss on this task batch = 3.5874e-01, Meta loss averaged over last 500 steps = 3.3336e-01, PNorm = 93.9736, GNorm = 0.2979
Meta loss on this task batch = 3.2736e-01, Meta loss averaged over last 500 steps = 3.3342e-01, PNorm = 93.9839, GNorm = 0.2807
Meta loss on this task batch = 3.6152e-01, Meta loss averaged over last 500 steps = 3.3353e-01, PNorm = 93.9942, GNorm = 0.2971
Meta loss on this task batch = 3.4105e-01, Meta loss averaged over last 500 steps = 3.3350e-01, PNorm = 94.0021, GNorm = 0.3033
Meta loss on this task batch = 3.3698e-01, Meta loss averaged over last 500 steps = 3.3349e-01, PNorm = 94.0078, GNorm = 0.3112
Meta loss on this task batch = 3.3294e-01, Meta loss averaged over last 500 steps = 3.3348e-01, PNorm = 94.0147, GNorm = 0.2450
Meta loss on this task batch = 3.1811e-01, Meta loss averaged over last 500 steps = 3.3338e-01, PNorm = 94.0220, GNorm = 0.2572
Meta loss on this task batch = 3.2509e-01, Meta loss averaged over last 500 steps = 3.3340e-01, PNorm = 94.0287, GNorm = 0.2800
Meta loss on this task batch = 3.6899e-01, Meta loss averaged over last 500 steps = 3.3349e-01, PNorm = 94.0366, GNorm = 0.2610
Meta loss on this task batch = 3.3312e-01, Meta loss averaged over last 500 steps = 3.3352e-01, PNorm = 94.0471, GNorm = 0.3179
Took 131.8182396888733 seconds to complete one epoch of meta training
Took 140.47357392311096 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493040
Epoch 234
Meta loss on this task batch = 3.2724e-01, Meta loss averaged over last 500 steps = 3.3344e-01, PNorm = 94.0582, GNorm = 0.2471
Meta loss on this task batch = 3.3557e-01, Meta loss averaged over last 500 steps = 3.3340e-01, PNorm = 94.0692, GNorm = 0.2656
Meta loss on this task batch = 3.4650e-01, Meta loss averaged over last 500 steps = 3.3347e-01, PNorm = 94.0815, GNorm = 0.3164
Meta loss on this task batch = 2.4807e-01, Meta loss averaged over last 500 steps = 3.3330e-01, PNorm = 94.0958, GNorm = 0.2496
Meta loss on this task batch = 3.2507e-01, Meta loss averaged over last 500 steps = 3.3332e-01, PNorm = 94.1085, GNorm = 0.2990
Meta loss on this task batch = 3.4136e-01, Meta loss averaged over last 500 steps = 3.3319e-01, PNorm = 94.1209, GNorm = 0.2782
Meta loss on this task batch = 3.3170e-01, Meta loss averaged over last 500 steps = 3.3305e-01, PNorm = 94.1322, GNorm = 0.3054
Meta loss on this task batch = 3.1480e-01, Meta loss averaged over last 500 steps = 3.3291e-01, PNorm = 94.1398, GNorm = 0.3279
Meta loss on this task batch = 3.5204e-01, Meta loss averaged over last 500 steps = 3.3300e-01, PNorm = 94.1444, GNorm = 0.3493
Meta loss on this task batch = 3.4382e-01, Meta loss averaged over last 500 steps = 3.3301e-01, PNorm = 94.1453, GNorm = 0.3253
Meta loss on this task batch = 3.7214e-01, Meta loss averaged over last 500 steps = 3.3315e-01, PNorm = 94.1465, GNorm = 0.3359
Meta loss on this task batch = 3.7010e-01, Meta loss averaged over last 500 steps = 3.3319e-01, PNorm = 94.1484, GNorm = 0.3082
Meta loss on this task batch = 3.8563e-01, Meta loss averaged over last 500 steps = 3.3344e-01, PNorm = 94.1491, GNorm = 0.3391
Meta loss on this task batch = 2.6928e-01, Meta loss averaged over last 500 steps = 3.3317e-01, PNorm = 94.1522, GNorm = 0.2431
Meta loss on this task batch = 3.1757e-01, Meta loss averaged over last 500 steps = 3.3326e-01, PNorm = 94.1563, GNorm = 0.2737
Meta loss on this task batch = 3.4792e-01, Meta loss averaged over last 500 steps = 3.3340e-01, PNorm = 94.1619, GNorm = 0.2775
Meta loss on this task batch = 3.3407e-01, Meta loss averaged over last 500 steps = 3.3348e-01, PNorm = 94.1693, GNorm = 0.2871
Meta loss on this task batch = 3.5788e-01, Meta loss averaged over last 500 steps = 3.3347e-01, PNorm = 94.1762, GNorm = 0.2774
Meta loss on this task batch = 3.0266e-01, Meta loss averaged over last 500 steps = 3.3343e-01, PNorm = 94.1836, GNorm = 0.2961
Took 133.81131505966187 seconds to complete one epoch of meta training
Took 142.44872522354126 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476779
Epoch 235
Meta loss on this task batch = 3.7015e-01, Meta loss averaged over last 500 steps = 3.3351e-01, PNorm = 94.1900, GNorm = 0.3075
Meta loss on this task batch = 3.2047e-01, Meta loss averaged over last 500 steps = 3.3334e-01, PNorm = 94.1965, GNorm = 0.2367
Meta loss on this task batch = 3.4706e-01, Meta loss averaged over last 500 steps = 3.3338e-01, PNorm = 94.2029, GNorm = 0.2906
Meta loss on this task batch = 3.4040e-01, Meta loss averaged over last 500 steps = 3.3332e-01, PNorm = 94.2088, GNorm = 0.2955
Meta loss on this task batch = 3.3292e-01, Meta loss averaged over last 500 steps = 3.3327e-01, PNorm = 94.2133, GNorm = 0.2730
Meta loss on this task batch = 2.8736e-01, Meta loss averaged over last 500 steps = 3.3311e-01, PNorm = 94.2177, GNorm = 0.2872
Meta loss on this task batch = 2.8687e-01, Meta loss averaged over last 500 steps = 3.3302e-01, PNorm = 94.2244, GNorm = 0.2490
Meta loss on this task batch = 3.1615e-01, Meta loss averaged over last 500 steps = 3.3298e-01, PNorm = 94.2322, GNorm = 0.2470
Meta loss on this task batch = 3.3411e-01, Meta loss averaged over last 500 steps = 3.3295e-01, PNorm = 94.2389, GNorm = 0.2736
Meta loss on this task batch = 3.7525e-01, Meta loss averaged over last 500 steps = 3.3299e-01, PNorm = 94.2457, GNorm = 0.2676
Meta loss on this task batch = 3.7599e-01, Meta loss averaged over last 500 steps = 3.3302e-01, PNorm = 94.2516, GNorm = 0.2919
Meta loss on this task batch = 3.4823e-01, Meta loss averaged over last 500 steps = 3.3302e-01, PNorm = 94.2596, GNorm = 0.3036
Meta loss on this task batch = 2.9622e-01, Meta loss averaged over last 500 steps = 3.3293e-01, PNorm = 94.2686, GNorm = 0.2475
Meta loss on this task batch = 3.6071e-01, Meta loss averaged over last 500 steps = 3.3311e-01, PNorm = 94.2753, GNorm = 0.2772
Meta loss on this task batch = 3.2672e-01, Meta loss averaged over last 500 steps = 3.3302e-01, PNorm = 94.2806, GNorm = 0.2836
Meta loss on this task batch = 3.4503e-01, Meta loss averaged over last 500 steps = 3.3315e-01, PNorm = 94.2875, GNorm = 0.2767
Meta loss on this task batch = 2.7705e-01, Meta loss averaged over last 500 steps = 3.3293e-01, PNorm = 94.2964, GNorm = 0.2857
Meta loss on this task batch = 3.3028e-01, Meta loss averaged over last 500 steps = 3.3292e-01, PNorm = 94.3067, GNorm = 0.2819
Meta loss on this task batch = 3.1164e-01, Meta loss averaged over last 500 steps = 3.3294e-01, PNorm = 94.3173, GNorm = 0.2931
Took 132.13700103759766 seconds to complete one epoch of meta training
Took 141.18614745140076 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498292
Epoch 236
Meta loss on this task batch = 2.9972e-01, Meta loss averaged over last 500 steps = 3.3288e-01, PNorm = 94.3274, GNorm = 0.2599
Meta loss on this task batch = 3.8250e-01, Meta loss averaged over last 500 steps = 3.3296e-01, PNorm = 94.3352, GNorm = 0.3587
Meta loss on this task batch = 3.4453e-01, Meta loss averaged over last 500 steps = 3.3310e-01, PNorm = 94.3405, GNorm = 0.3037
Meta loss on this task batch = 3.2032e-01, Meta loss averaged over last 500 steps = 3.3303e-01, PNorm = 94.3444, GNorm = 0.2737
Meta loss on this task batch = 3.4456e-01, Meta loss averaged over last 500 steps = 3.3300e-01, PNorm = 94.3472, GNorm = 0.2889
Meta loss on this task batch = 3.4459e-01, Meta loss averaged over last 500 steps = 3.3285e-01, PNorm = 94.3498, GNorm = 0.3213
Meta loss on this task batch = 3.7284e-01, Meta loss averaged over last 500 steps = 3.3298e-01, PNorm = 94.3524, GNorm = 0.2880
Meta loss on this task batch = 3.2287e-01, Meta loss averaged over last 500 steps = 3.3287e-01, PNorm = 94.3569, GNorm = 0.2787
Meta loss on this task batch = 3.7595e-01, Meta loss averaged over last 500 steps = 3.3295e-01, PNorm = 94.3645, GNorm = 0.3037
Meta loss on this task batch = 3.7338e-01, Meta loss averaged over last 500 steps = 3.3297e-01, PNorm = 94.3723, GNorm = 0.2778
Meta loss on this task batch = 3.3031e-01, Meta loss averaged over last 500 steps = 3.3306e-01, PNorm = 94.3824, GNorm = 0.3058
Meta loss on this task batch = 3.0217e-01, Meta loss averaged over last 500 steps = 3.3295e-01, PNorm = 94.3920, GNorm = 0.2356
Meta loss on this task batch = 3.0945e-01, Meta loss averaged over last 500 steps = 3.3289e-01, PNorm = 94.4005, GNorm = 0.2566
Meta loss on this task batch = 2.6123e-01, Meta loss averaged over last 500 steps = 3.3264e-01, PNorm = 94.4102, GNorm = 0.2308
Meta loss on this task batch = 3.4850e-01, Meta loss averaged over last 500 steps = 3.3258e-01, PNorm = 94.4189, GNorm = 0.2961
Meta loss on this task batch = 3.1597e-01, Meta loss averaged over last 500 steps = 3.3257e-01, PNorm = 94.4248, GNorm = 0.3018
Meta loss on this task batch = 3.1842e-01, Meta loss averaged over last 500 steps = 3.3258e-01, PNorm = 94.4285, GNorm = 0.2688
Meta loss on this task batch = 3.5029e-01, Meta loss averaged over last 500 steps = 3.3259e-01, PNorm = 94.4285, GNorm = 0.3432
Meta loss on this task batch = 3.0425e-01, Meta loss averaged over last 500 steps = 3.3250e-01, PNorm = 94.4276, GNorm = 0.3362
Took 131.83461451530457 seconds to complete one epoch of meta training
Took 139.3344316482544 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501291
Epoch 237
Meta loss on this task batch = 3.4051e-01, Meta loss averaged over last 500 steps = 3.3250e-01, PNorm = 94.4269, GNorm = 0.2739
Meta loss on this task batch = 3.2158e-01, Meta loss averaged over last 500 steps = 3.3242e-01, PNorm = 94.4291, GNorm = 0.2928
Meta loss on this task batch = 3.4307e-01, Meta loss averaged over last 500 steps = 3.3248e-01, PNorm = 94.4340, GNorm = 0.3021
Meta loss on this task batch = 2.8153e-01, Meta loss averaged over last 500 steps = 3.3240e-01, PNorm = 94.4424, GNorm = 0.2907
Meta loss on this task batch = 2.9791e-01, Meta loss averaged over last 500 steps = 3.3234e-01, PNorm = 94.4534, GNorm = 0.2704
Meta loss on this task batch = 3.4743e-01, Meta loss averaged over last 500 steps = 3.3228e-01, PNorm = 94.4667, GNorm = 0.2743
Meta loss on this task batch = 2.9781e-01, Meta loss averaged over last 500 steps = 3.3221e-01, PNorm = 94.4811, GNorm = 0.2787
Meta loss on this task batch = 3.0393e-01, Meta loss averaged over last 500 steps = 3.3213e-01, PNorm = 94.4959, GNorm = 0.2878
Meta loss on this task batch = 3.2699e-01, Meta loss averaged over last 500 steps = 3.3208e-01, PNorm = 94.5079, GNorm = 0.2846
Meta loss on this task batch = 3.0516e-01, Meta loss averaged over last 500 steps = 3.3199e-01, PNorm = 94.5176, GNorm = 0.2979
Meta loss on this task batch = 3.2157e-01, Meta loss averaged over last 500 steps = 3.3191e-01, PNorm = 94.5252, GNorm = 0.3262
Meta loss on this task batch = 3.0655e-01, Meta loss averaged over last 500 steps = 3.3183e-01, PNorm = 94.5315, GNorm = 0.3118
Meta loss on this task batch = 3.2693e-01, Meta loss averaged over last 500 steps = 3.3184e-01, PNorm = 94.5367, GNorm = 0.2867
Meta loss on this task batch = 3.1445e-01, Meta loss averaged over last 500 steps = 3.3171e-01, PNorm = 94.5404, GNorm = 0.3442
Meta loss on this task batch = 2.7387e-01, Meta loss averaged over last 500 steps = 3.3157e-01, PNorm = 94.5435, GNorm = 0.2585
Meta loss on this task batch = 3.5033e-01, Meta loss averaged over last 500 steps = 3.3165e-01, PNorm = 94.5436, GNorm = 0.2953
Meta loss on this task batch = 3.9514e-01, Meta loss averaged over last 500 steps = 3.3171e-01, PNorm = 94.5423, GNorm = 0.3344
Meta loss on this task batch = 3.3415e-01, Meta loss averaged over last 500 steps = 3.3171e-01, PNorm = 94.5407, GNorm = 0.3010
Meta loss on this task batch = 3.1600e-01, Meta loss averaged over last 500 steps = 3.3155e-01, PNorm = 94.5417, GNorm = 0.3391
Took 133.9132800102234 seconds to complete one epoch of meta training
Took 142.1203694343567 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503700
Epoch 238
Meta loss on this task batch = 3.0842e-01, Meta loss averaged over last 500 steps = 3.3163e-01, PNorm = 94.5446, GNorm = 0.2378
Meta loss on this task batch = 3.4085e-01, Meta loss averaged over last 500 steps = 3.3168e-01, PNorm = 94.5502, GNorm = 0.2780
Meta loss on this task batch = 3.7196e-01, Meta loss averaged over last 500 steps = 3.3173e-01, PNorm = 94.5567, GNorm = 0.2733
Meta loss on this task batch = 2.9268e-01, Meta loss averaged over last 500 steps = 3.3180e-01, PNorm = 94.5647, GNorm = 0.2706
Meta loss on this task batch = 3.4154e-01, Meta loss averaged over last 500 steps = 3.3186e-01, PNorm = 94.5747, GNorm = 0.2781
Meta loss on this task batch = 3.3087e-01, Meta loss averaged over last 500 steps = 3.3189e-01, PNorm = 94.5849, GNorm = 0.2777
Meta loss on this task batch = 3.3145e-01, Meta loss averaged over last 500 steps = 3.3186e-01, PNorm = 94.5959, GNorm = 0.2942
Meta loss on this task batch = 3.2030e-01, Meta loss averaged over last 500 steps = 3.3186e-01, PNorm = 94.6068, GNorm = 0.2849
Meta loss on this task batch = 3.1140e-01, Meta loss averaged over last 500 steps = 3.3186e-01, PNorm = 94.6178, GNorm = 0.2530
Meta loss on this task batch = 3.3264e-01, Meta loss averaged over last 500 steps = 3.3176e-01, PNorm = 94.6257, GNorm = 0.2629
Meta loss on this task batch = 3.2916e-01, Meta loss averaged over last 500 steps = 3.3183e-01, PNorm = 94.6328, GNorm = 0.2703
Meta loss on this task batch = 3.1851e-01, Meta loss averaged over last 500 steps = 3.3174e-01, PNorm = 94.6377, GNorm = 0.2772
Meta loss on this task batch = 3.1904e-01, Meta loss averaged over last 500 steps = 3.3171e-01, PNorm = 94.6423, GNorm = 0.2881
Meta loss on this task batch = 3.9043e-01, Meta loss averaged over last 500 steps = 3.3171e-01, PNorm = 94.6472, GNorm = 0.2626
Meta loss on this task batch = 3.2876e-01, Meta loss averaged over last 500 steps = 3.3168e-01, PNorm = 94.6527, GNorm = 0.3098
Meta loss on this task batch = 2.4990e-01, Meta loss averaged over last 500 steps = 3.3154e-01, PNorm = 94.6602, GNorm = 0.2401
Meta loss on this task batch = 3.5057e-01, Meta loss averaged over last 500 steps = 3.3160e-01, PNorm = 94.6694, GNorm = 0.2933
Meta loss on this task batch = 3.6223e-01, Meta loss averaged over last 500 steps = 3.3171e-01, PNorm = 94.6771, GNorm = 0.2875
Meta loss on this task batch = 3.4443e-01, Meta loss averaged over last 500 steps = 3.3181e-01, PNorm = 94.6818, GNorm = 0.4125
Took 132.23447799682617 seconds to complete one epoch of meta training
Took 140.77473831176758 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.514014
Epoch 239
Meta loss on this task batch = 2.9169e-01, Meta loss averaged over last 500 steps = 3.3176e-01, PNorm = 94.6862, GNorm = 0.2734
Meta loss on this task batch = 3.4953e-01, Meta loss averaged over last 500 steps = 3.3178e-01, PNorm = 94.6912, GNorm = 0.3199
Meta loss on this task batch = 3.4678e-01, Meta loss averaged over last 500 steps = 3.3172e-01, PNorm = 94.6977, GNorm = 0.2861
Meta loss on this task batch = 3.1896e-01, Meta loss averaged over last 500 steps = 3.3167e-01, PNorm = 94.7065, GNorm = 0.2760
Meta loss on this task batch = 3.0833e-01, Meta loss averaged over last 500 steps = 3.3165e-01, PNorm = 94.7176, GNorm = 0.2751
Meta loss on this task batch = 3.5322e-01, Meta loss averaged over last 500 steps = 3.3167e-01, PNorm = 94.7303, GNorm = 0.3179
Meta loss on this task batch = 3.3989e-01, Meta loss averaged over last 500 steps = 3.3170e-01, PNorm = 94.7436, GNorm = 0.2827
Meta loss on this task batch = 3.1541e-01, Meta loss averaged over last 500 steps = 3.3170e-01, PNorm = 94.7549, GNorm = 0.2866
Meta loss on this task batch = 3.7855e-01, Meta loss averaged over last 500 steps = 3.3174e-01, PNorm = 94.7637, GNorm = 0.3446
Meta loss on this task batch = 2.7750e-01, Meta loss averaged over last 500 steps = 3.3170e-01, PNorm = 94.7724, GNorm = 0.2799
Meta loss on this task batch = 3.3859e-01, Meta loss averaged over last 500 steps = 3.3168e-01, PNorm = 94.7816, GNorm = 0.3147
Meta loss on this task batch = 3.0846e-01, Meta loss averaged over last 500 steps = 3.3152e-01, PNorm = 94.7909, GNorm = 0.2877
Meta loss on this task batch = 3.5239e-01, Meta loss averaged over last 500 steps = 3.3161e-01, PNorm = 94.7971, GNorm = 0.3996
Meta loss on this task batch = 3.3240e-01, Meta loss averaged over last 500 steps = 3.3168e-01, PNorm = 94.8029, GNorm = 0.2928
Meta loss on this task batch = 3.2204e-01, Meta loss averaged over last 500 steps = 3.3172e-01, PNorm = 94.8069, GNorm = 0.3206
Meta loss on this task batch = 2.7236e-01, Meta loss averaged over last 500 steps = 3.3158e-01, PNorm = 94.8121, GNorm = 0.3265
Meta loss on this task batch = 3.9083e-01, Meta loss averaged over last 500 steps = 3.3170e-01, PNorm = 94.8175, GNorm = 0.3101
Meta loss on this task batch = 3.1712e-01, Meta loss averaged over last 500 steps = 3.3163e-01, PNorm = 94.8246, GNorm = 0.2572
Meta loss on this task batch = 3.2027e-01, Meta loss averaged over last 500 steps = 3.3162e-01, PNorm = 94.8322, GNorm = 0.3141
Took 134.94996428489685 seconds to complete one epoch of meta training
Took 142.62672328948975 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492820
Epoch 240
Meta loss on this task batch = 3.1223e-01, Meta loss averaged over last 500 steps = 3.3152e-01, PNorm = 94.8422, GNorm = 0.2868
Meta loss on this task batch = 3.4250e-01, Meta loss averaged over last 500 steps = 3.3137e-01, PNorm = 94.8509, GNorm = 0.2664
Meta loss on this task batch = 3.8152e-01, Meta loss averaged over last 500 steps = 3.3159e-01, PNorm = 94.8594, GNorm = 0.3050
Meta loss on this task batch = 3.1756e-01, Meta loss averaged over last 500 steps = 3.3143e-01, PNorm = 94.8660, GNorm = 0.3015
Meta loss on this task batch = 3.4211e-01, Meta loss averaged over last 500 steps = 3.3147e-01, PNorm = 94.8719, GNorm = 0.2742
Meta loss on this task batch = 3.4348e-01, Meta loss averaged over last 500 steps = 3.3143e-01, PNorm = 94.8779, GNorm = 0.2983
Meta loss on this task batch = 3.2352e-01, Meta loss averaged over last 500 steps = 3.3148e-01, PNorm = 94.8836, GNorm = 0.3021
Meta loss on this task batch = 3.5577e-01, Meta loss averaged over last 500 steps = 3.3153e-01, PNorm = 94.8889, GNorm = 0.3309
Meta loss on this task batch = 3.5292e-01, Meta loss averaged over last 500 steps = 3.3152e-01, PNorm = 94.8950, GNorm = 0.2757
Meta loss on this task batch = 3.5118e-01, Meta loss averaged over last 500 steps = 3.3165e-01, PNorm = 94.9013, GNorm = 0.2811
Meta loss on this task batch = 3.0644e-01, Meta loss averaged over last 500 steps = 3.3166e-01, PNorm = 94.9080, GNorm = 0.2656
Meta loss on this task batch = 3.0331e-01, Meta loss averaged over last 500 steps = 3.3157e-01, PNorm = 94.9161, GNorm = 0.2746
Meta loss on this task batch = 3.5539e-01, Meta loss averaged over last 500 steps = 3.3160e-01, PNorm = 94.9241, GNorm = 0.2934
Meta loss on this task batch = 4.1766e-01, Meta loss averaged over last 500 steps = 3.3174e-01, PNorm = 94.9299, GNorm = 0.3549
Meta loss on this task batch = 2.7585e-01, Meta loss averaged over last 500 steps = 3.3162e-01, PNorm = 94.9349, GNorm = 0.3021
Meta loss on this task batch = 3.1806e-01, Meta loss averaged over last 500 steps = 3.3156e-01, PNorm = 94.9426, GNorm = 0.2957
Meta loss on this task batch = 2.9283e-01, Meta loss averaged over last 500 steps = 3.3137e-01, PNorm = 94.9506, GNorm = 0.2360
Meta loss on this task batch = 3.2915e-01, Meta loss averaged over last 500 steps = 3.3145e-01, PNorm = 94.9586, GNorm = 0.2655
Meta loss on this task batch = 3.5271e-01, Meta loss averaged over last 500 steps = 3.3150e-01, PNorm = 94.9668, GNorm = 0.3145
Took 134.80620503425598 seconds to complete one epoch of meta training
Took 143.52021622657776 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498933
Epoch 241
Meta loss on this task batch = 2.9776e-01, Meta loss averaged over last 500 steps = 3.3136e-01, PNorm = 94.9745, GNorm = 0.2531
Meta loss on this task batch = 3.1966e-01, Meta loss averaged over last 500 steps = 3.3128e-01, PNorm = 94.9798, GNorm = 0.2862
Meta loss on this task batch = 2.7743e-01, Meta loss averaged over last 500 steps = 3.3110e-01, PNorm = 94.9853, GNorm = 0.2506
Meta loss on this task batch = 3.8421e-01, Meta loss averaged over last 500 steps = 3.3121e-01, PNorm = 94.9892, GNorm = 0.2817
Meta loss on this task batch = 3.4918e-01, Meta loss averaged over last 500 steps = 3.3125e-01, PNorm = 94.9934, GNorm = 0.2561
Meta loss on this task batch = 3.6515e-01, Meta loss averaged over last 500 steps = 3.3142e-01, PNorm = 94.9987, GNorm = 0.2835
Meta loss on this task batch = 3.4601e-01, Meta loss averaged over last 500 steps = 3.3141e-01, PNorm = 95.0057, GNorm = 0.2939
Meta loss on this task batch = 3.1484e-01, Meta loss averaged over last 500 steps = 3.3129e-01, PNorm = 95.0116, GNorm = 0.2756
Meta loss on this task batch = 3.2771e-01, Meta loss averaged over last 500 steps = 3.3125e-01, PNorm = 95.0153, GNorm = 0.3009
Meta loss on this task batch = 3.1760e-01, Meta loss averaged over last 500 steps = 3.3113e-01, PNorm = 95.0170, GNorm = 0.2772
Meta loss on this task batch = 3.4971e-01, Meta loss averaged over last 500 steps = 3.3114e-01, PNorm = 95.0163, GNorm = 0.3166
Meta loss on this task batch = 3.1775e-01, Meta loss averaged over last 500 steps = 3.3109e-01, PNorm = 95.0178, GNorm = 0.2863
Meta loss on this task batch = 3.8329e-01, Meta loss averaged over last 500 steps = 3.3117e-01, PNorm = 95.0205, GNorm = 0.3185
Meta loss on this task batch = 3.2096e-01, Meta loss averaged over last 500 steps = 3.3108e-01, PNorm = 95.0246, GNorm = 0.2680
Meta loss on this task batch = 3.6098e-01, Meta loss averaged over last 500 steps = 3.3120e-01, PNorm = 95.0293, GNorm = 0.3217
Meta loss on this task batch = 2.8394e-01, Meta loss averaged over last 500 steps = 3.3113e-01, PNorm = 95.0370, GNorm = 0.2713
Meta loss on this task batch = 3.0016e-01, Meta loss averaged over last 500 steps = 3.3105e-01, PNorm = 95.0455, GNorm = 0.2700
Meta loss on this task batch = 3.1501e-01, Meta loss averaged over last 500 steps = 3.3097e-01, PNorm = 95.0561, GNorm = 0.3080
Meta loss on this task batch = 2.7853e-01, Meta loss averaged over last 500 steps = 3.3092e-01, PNorm = 95.0669, GNorm = 0.3359
Took 132.55144119262695 seconds to complete one epoch of meta training
Took 140.84412574768066 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482654
Epoch 242
Meta loss on this task batch = 3.3253e-01, Meta loss averaged over last 500 steps = 3.3096e-01, PNorm = 95.0754, GNorm = 0.3082
Meta loss on this task batch = 3.1544e-01, Meta loss averaged over last 500 steps = 3.3091e-01, PNorm = 95.0852, GNorm = 0.2695
Meta loss on this task batch = 2.9208e-01, Meta loss averaged over last 500 steps = 3.3079e-01, PNorm = 95.0939, GNorm = 0.2890
Meta loss on this task batch = 3.6001e-01, Meta loss averaged over last 500 steps = 3.3076e-01, PNorm = 95.1017, GNorm = 0.2870
Meta loss on this task batch = 2.4555e-01, Meta loss averaged over last 500 steps = 3.3064e-01, PNorm = 95.1110, GNorm = 0.2436
Meta loss on this task batch = 3.2055e-01, Meta loss averaged over last 500 steps = 3.3065e-01, PNorm = 95.1201, GNorm = 0.2834
Meta loss on this task batch = 3.9980e-01, Meta loss averaged over last 500 steps = 3.3076e-01, PNorm = 95.1277, GNorm = 0.3778
Meta loss on this task batch = 2.7163e-01, Meta loss averaged over last 500 steps = 3.3065e-01, PNorm = 95.1362, GNorm = 0.2522
Meta loss on this task batch = 3.2536e-01, Meta loss averaged over last 500 steps = 3.3055e-01, PNorm = 95.1454, GNorm = 0.2714
Meta loss on this task batch = 3.5558e-01, Meta loss averaged over last 500 steps = 3.3057e-01, PNorm = 95.1556, GNorm = 0.2893
Meta loss on this task batch = 3.5024e-01, Meta loss averaged over last 500 steps = 3.3056e-01, PNorm = 95.1670, GNorm = 0.2758
Meta loss on this task batch = 3.6655e-01, Meta loss averaged over last 500 steps = 3.3069e-01, PNorm = 95.1777, GNorm = 0.2862
Meta loss on this task batch = 3.1489e-01, Meta loss averaged over last 500 steps = 3.3064e-01, PNorm = 95.1882, GNorm = 0.2577
Meta loss on this task batch = 3.6043e-01, Meta loss averaged over last 500 steps = 3.3067e-01, PNorm = 95.1966, GNorm = 0.3127
Meta loss on this task batch = 3.0276e-01, Meta loss averaged over last 500 steps = 3.3062e-01, PNorm = 95.2043, GNorm = 0.2453
Meta loss on this task batch = 2.8770e-01, Meta loss averaged over last 500 steps = 3.3055e-01, PNorm = 95.2105, GNorm = 0.2546
Meta loss on this task batch = 3.7825e-01, Meta loss averaged over last 500 steps = 3.3062e-01, PNorm = 95.2149, GNorm = 0.3093
Meta loss on this task batch = 3.4982e-01, Meta loss averaged over last 500 steps = 3.3067e-01, PNorm = 95.2194, GNorm = 0.3194
Meta loss on this task batch = 3.2980e-01, Meta loss averaged over last 500 steps = 3.3069e-01, PNorm = 95.2242, GNorm = 0.3561
Took 133.29476833343506 seconds to complete one epoch of meta training
Took 140.9439661502838 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.529715
Epoch 243
Meta loss on this task batch = 3.4740e-01, Meta loss averaged over last 500 steps = 3.3059e-01, PNorm = 95.2292, GNorm = 0.2979
Meta loss on this task batch = 3.2881e-01, Meta loss averaged over last 500 steps = 3.3060e-01, PNorm = 95.2343, GNorm = 0.3106
Meta loss on this task batch = 3.0323e-01, Meta loss averaged over last 500 steps = 3.3055e-01, PNorm = 95.2404, GNorm = 0.2767
Meta loss on this task batch = 3.4439e-01, Meta loss averaged over last 500 steps = 3.3050e-01, PNorm = 95.2443, GNorm = 0.3832
Meta loss on this task batch = 3.2699e-01, Meta loss averaged over last 500 steps = 3.3051e-01, PNorm = 95.2493, GNorm = 0.2826
Meta loss on this task batch = 3.2538e-01, Meta loss averaged over last 500 steps = 3.3054e-01, PNorm = 95.2548, GNorm = 0.2909
Meta loss on this task batch = 2.7979e-01, Meta loss averaged over last 500 steps = 3.3037e-01, PNorm = 95.2620, GNorm = 0.2393
Meta loss on this task batch = 3.3403e-01, Meta loss averaged over last 500 steps = 3.3044e-01, PNorm = 95.2708, GNorm = 0.2788
Meta loss on this task batch = 2.9947e-01, Meta loss averaged over last 500 steps = 3.3040e-01, PNorm = 95.2805, GNorm = 0.2540
Meta loss on this task batch = 3.0419e-01, Meta loss averaged over last 500 steps = 3.3037e-01, PNorm = 95.2904, GNorm = 0.2314
Meta loss on this task batch = 3.2015e-01, Meta loss averaged over last 500 steps = 3.3027e-01, PNorm = 95.3008, GNorm = 0.2589
Meta loss on this task batch = 3.1925e-01, Meta loss averaged over last 500 steps = 3.3012e-01, PNorm = 95.3119, GNorm = 0.2884
Meta loss on this task batch = 3.3628e-01, Meta loss averaged over last 500 steps = 3.3020e-01, PNorm = 95.3232, GNorm = 0.2578
Meta loss on this task batch = 3.6348e-01, Meta loss averaged over last 500 steps = 3.3030e-01, PNorm = 95.3320, GNorm = 0.2992
Meta loss on this task batch = 3.1801e-01, Meta loss averaged over last 500 steps = 3.3037e-01, PNorm = 95.3395, GNorm = 0.2947
Meta loss on this task batch = 3.3962e-01, Meta loss averaged over last 500 steps = 3.3034e-01, PNorm = 95.3450, GNorm = 0.2664
Meta loss on this task batch = 3.4163e-01, Meta loss averaged over last 500 steps = 3.3030e-01, PNorm = 95.3493, GNorm = 0.3201
Meta loss on this task batch = 3.0559e-01, Meta loss averaged over last 500 steps = 3.3016e-01, PNorm = 95.3539, GNorm = 0.2663
Meta loss on this task batch = 3.2127e-01, Meta loss averaged over last 500 steps = 3.3013e-01, PNorm = 95.3585, GNorm = 0.2872
Took 135.12358021736145 seconds to complete one epoch of meta training
Took 143.5888421535492 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490016
Epoch 244
Meta loss on this task batch = 3.5542e-01, Meta loss averaged over last 500 steps = 3.3028e-01, PNorm = 95.3607, GNorm = 0.3100
Meta loss on this task batch = 3.4051e-01, Meta loss averaged over last 500 steps = 3.3043e-01, PNorm = 95.3595, GNorm = 0.3324
Meta loss on this task batch = 3.3854e-01, Meta loss averaged over last 500 steps = 3.3037e-01, PNorm = 95.3582, GNorm = 0.2851
Meta loss on this task batch = 3.2804e-01, Meta loss averaged over last 500 steps = 3.3037e-01, PNorm = 95.3553, GNorm = 0.2656
Meta loss on this task batch = 3.1248e-01, Meta loss averaged over last 500 steps = 3.3028e-01, PNorm = 95.3543, GNorm = 0.2752
Meta loss on this task batch = 3.1888e-01, Meta loss averaged over last 500 steps = 3.3022e-01, PNorm = 95.3555, GNorm = 0.2781
Meta loss on this task batch = 2.9404e-01, Meta loss averaged over last 500 steps = 3.3015e-01, PNorm = 95.3620, GNorm = 0.3028
Meta loss on this task batch = 2.8249e-01, Meta loss averaged over last 500 steps = 3.2993e-01, PNorm = 95.3730, GNorm = 0.2779
Meta loss on this task batch = 3.1563e-01, Meta loss averaged over last 500 steps = 3.2995e-01, PNorm = 95.3851, GNorm = 0.2921
Meta loss on this task batch = 3.6681e-01, Meta loss averaged over last 500 steps = 3.3012e-01, PNorm = 95.3975, GNorm = 0.2652
Meta loss on this task batch = 3.2056e-01, Meta loss averaged over last 500 steps = 3.3006e-01, PNorm = 95.4116, GNorm = 0.2519
Meta loss on this task batch = 2.9451e-01, Meta loss averaged over last 500 steps = 3.2990e-01, PNorm = 95.4222, GNorm = 0.2780
Meta loss on this task batch = 3.5442e-01, Meta loss averaged over last 500 steps = 3.2999e-01, PNorm = 95.4282, GNorm = 0.3203
Meta loss on this task batch = 3.2405e-01, Meta loss averaged over last 500 steps = 3.2993e-01, PNorm = 95.4317, GNorm = 0.2849
Meta loss on this task batch = 3.2773e-01, Meta loss averaged over last 500 steps = 3.2997e-01, PNorm = 95.4300, GNorm = 0.3663
Meta loss on this task batch = 3.9821e-01, Meta loss averaged over last 500 steps = 3.3009e-01, PNorm = 95.4276, GNorm = 0.3447
Meta loss on this task batch = 3.1790e-01, Meta loss averaged over last 500 steps = 3.2993e-01, PNorm = 95.4259, GNorm = 0.2742
Meta loss on this task batch = 3.2339e-01, Meta loss averaged over last 500 steps = 3.2997e-01, PNorm = 95.4251, GNorm = 0.2773
Meta loss on this task batch = 3.2021e-01, Meta loss averaged over last 500 steps = 3.2993e-01, PNorm = 95.4275, GNorm = 0.3394
Took 136.89723944664001 seconds to complete one epoch of meta training
Took 146.0842583179474 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505606
Epoch 245
Meta loss on this task batch = 3.5246e-01, Meta loss averaged over last 500 steps = 3.2992e-01, PNorm = 95.4332, GNorm = 0.3135
Meta loss on this task batch = 3.0825e-01, Meta loss averaged over last 500 steps = 3.2990e-01, PNorm = 95.4421, GNorm = 0.2755
Meta loss on this task batch = 3.0077e-01, Meta loss averaged over last 500 steps = 3.2985e-01, PNorm = 95.4533, GNorm = 0.2722
Meta loss on this task batch = 3.3700e-01, Meta loss averaged over last 500 steps = 3.2990e-01, PNorm = 95.4641, GNorm = 0.2941
Meta loss on this task batch = 3.0002e-01, Meta loss averaged over last 500 steps = 3.2983e-01, PNorm = 95.4744, GNorm = 0.2814
Meta loss on this task batch = 3.0048e-01, Meta loss averaged over last 500 steps = 3.2983e-01, PNorm = 95.4847, GNorm = 0.2916
Meta loss on this task batch = 3.7619e-01, Meta loss averaged over last 500 steps = 3.2993e-01, PNorm = 95.4916, GNorm = 0.3736
Meta loss on this task batch = 4.0177e-01, Meta loss averaged over last 500 steps = 3.3014e-01, PNorm = 95.4971, GNorm = 0.3856
Meta loss on this task batch = 3.0088e-01, Meta loss averaged over last 500 steps = 3.3005e-01, PNorm = 95.5044, GNorm = 0.2837
Meta loss on this task batch = 3.2656e-01, Meta loss averaged over last 500 steps = 3.3008e-01, PNorm = 95.5130, GNorm = 0.2857
Meta loss on this task batch = 2.7892e-01, Meta loss averaged over last 500 steps = 3.3000e-01, PNorm = 95.5225, GNorm = 0.2540
Meta loss on this task batch = 3.4586e-01, Meta loss averaged over last 500 steps = 3.3006e-01, PNorm = 95.5315, GNorm = 0.3184
Meta loss on this task batch = 3.1485e-01, Meta loss averaged over last 500 steps = 3.3007e-01, PNorm = 95.5413, GNorm = 0.2646
Meta loss on this task batch = 3.5678e-01, Meta loss averaged over last 500 steps = 3.3001e-01, PNorm = 95.5502, GNorm = 0.2679
Meta loss on this task batch = 3.6277e-01, Meta loss averaged over last 500 steps = 3.3022e-01, PNorm = 95.5580, GNorm = 0.2784
Meta loss on this task batch = 3.2163e-01, Meta loss averaged over last 500 steps = 3.3009e-01, PNorm = 95.5667, GNorm = 0.3116
Meta loss on this task batch = 3.5371e-01, Meta loss averaged over last 500 steps = 3.3007e-01, PNorm = 95.5757, GNorm = 0.2924
Meta loss on this task batch = 3.5068e-01, Meta loss averaged over last 500 steps = 3.3014e-01, PNorm = 95.5856, GNorm = 0.3452
Meta loss on this task batch = 2.7068e-01, Meta loss averaged over last 500 steps = 3.3001e-01, PNorm = 95.5957, GNorm = 0.2724
Took 163.84229135513306 seconds to complete one epoch of meta training
Took 172.1771423816681 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492121
Epoch 246
Meta loss on this task batch = 3.0376e-01, Meta loss averaged over last 500 steps = 3.2987e-01, PNorm = 95.6058, GNorm = 0.2526
Meta loss on this task batch = 3.0209e-01, Meta loss averaged over last 500 steps = 3.2977e-01, PNorm = 95.6139, GNorm = 0.2542
Meta loss on this task batch = 3.2160e-01, Meta loss averaged over last 500 steps = 3.2980e-01, PNorm = 95.6235, GNorm = 0.2839
Meta loss on this task batch = 2.7901e-01, Meta loss averaged over last 500 steps = 3.2974e-01, PNorm = 95.6348, GNorm = 0.2708
Meta loss on this task batch = 3.5152e-01, Meta loss averaged over last 500 steps = 3.2970e-01, PNorm = 95.6440, GNorm = 0.2820
Meta loss on this task batch = 3.5800e-01, Meta loss averaged over last 500 steps = 3.2976e-01, PNorm = 95.6504, GNorm = 0.3981
Meta loss on this task batch = 3.3048e-01, Meta loss averaged over last 500 steps = 3.2977e-01, PNorm = 95.6567, GNorm = 0.3088
Meta loss on this task batch = 2.9825e-01, Meta loss averaged over last 500 steps = 3.2971e-01, PNorm = 95.6623, GNorm = 0.2952
Meta loss on this task batch = 3.1392e-01, Meta loss averaged over last 500 steps = 3.2968e-01, PNorm = 95.6691, GNorm = 0.3172
Meta loss on this task batch = 3.1205e-01, Meta loss averaged over last 500 steps = 3.2969e-01, PNorm = 95.6773, GNorm = 0.2336
Meta loss on this task batch = 2.8041e-01, Meta loss averaged over last 500 steps = 3.2947e-01, PNorm = 95.6863, GNorm = 0.2312
Meta loss on this task batch = 3.1456e-01, Meta loss averaged over last 500 steps = 3.2948e-01, PNorm = 95.6956, GNorm = 0.2792
Meta loss on this task batch = 3.2496e-01, Meta loss averaged over last 500 steps = 3.2949e-01, PNorm = 95.7040, GNorm = 0.2653
Meta loss on this task batch = 3.2567e-01, Meta loss averaged over last 500 steps = 3.2944e-01, PNorm = 95.7124, GNorm = 0.2808
Meta loss on this task batch = 3.4355e-01, Meta loss averaged over last 500 steps = 3.2938e-01, PNorm = 95.7190, GNorm = 0.2580
Meta loss on this task batch = 3.7013e-01, Meta loss averaged over last 500 steps = 3.2938e-01, PNorm = 95.7261, GNorm = 0.2953
Meta loss on this task batch = 3.5041e-01, Meta loss averaged over last 500 steps = 3.2950e-01, PNorm = 95.7330, GNorm = 0.2868
Meta loss on this task batch = 3.1608e-01, Meta loss averaged over last 500 steps = 3.2946e-01, PNorm = 95.7403, GNorm = 0.2693
Meta loss on this task batch = 3.7610e-01, Meta loss averaged over last 500 steps = 3.2961e-01, PNorm = 95.7473, GNorm = 0.3364
Took 136.71027636528015 seconds to complete one epoch of meta training
Took 145.3371765613556 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490986
Epoch 247
Meta loss on this task batch = 3.2221e-01, Meta loss averaged over last 500 steps = 3.2961e-01, PNorm = 95.7525, GNorm = 0.2861
Meta loss on this task batch = 3.3252e-01, Meta loss averaged over last 500 steps = 3.2955e-01, PNorm = 95.7565, GNorm = 0.2703
Meta loss on this task batch = 3.5695e-01, Meta loss averaged over last 500 steps = 3.2952e-01, PNorm = 95.7573, GNorm = 0.3019
Meta loss on this task batch = 3.7108e-01, Meta loss averaged over last 500 steps = 3.2956e-01, PNorm = 95.7573, GNorm = 0.3103
Meta loss on this task batch = 2.4700e-01, Meta loss averaged over last 500 steps = 3.2952e-01, PNorm = 95.7584, GNorm = 0.2475
Meta loss on this task batch = 3.1064e-01, Meta loss averaged over last 500 steps = 3.2935e-01, PNorm = 95.7612, GNorm = 0.2835
Meta loss on this task batch = 3.3940e-01, Meta loss averaged over last 500 steps = 3.2941e-01, PNorm = 95.7651, GNorm = 0.2745
Meta loss on this task batch = 3.3200e-01, Meta loss averaged over last 500 steps = 3.2939e-01, PNorm = 95.7704, GNorm = 0.2862
Meta loss on this task batch = 3.3890e-01, Meta loss averaged over last 500 steps = 3.2943e-01, PNorm = 95.7752, GNorm = 0.2958
Meta loss on this task batch = 2.8579e-01, Meta loss averaged over last 500 steps = 3.2930e-01, PNorm = 95.7821, GNorm = 0.2686
Meta loss on this task batch = 3.5898e-01, Meta loss averaged over last 500 steps = 3.2944e-01, PNorm = 95.7890, GNorm = 0.3199
Meta loss on this task batch = 3.1059e-01, Meta loss averaged over last 500 steps = 3.2940e-01, PNorm = 95.7993, GNorm = 0.2447
Meta loss on this task batch = 3.5287e-01, Meta loss averaged over last 500 steps = 3.2949e-01, PNorm = 95.8101, GNorm = 0.2717
Meta loss on this task batch = 2.9311e-01, Meta loss averaged over last 500 steps = 3.2945e-01, PNorm = 95.8205, GNorm = 0.2685
Meta loss on this task batch = 3.2712e-01, Meta loss averaged over last 500 steps = 3.2936e-01, PNorm = 95.8286, GNorm = 0.2961
Meta loss on this task batch = 3.5091e-01, Meta loss averaged over last 500 steps = 3.2939e-01, PNorm = 95.8345, GNorm = 0.2541
Meta loss on this task batch = 2.8656e-01, Meta loss averaged over last 500 steps = 3.2920e-01, PNorm = 95.8391, GNorm = 0.2731
Meta loss on this task batch = 3.5902e-01, Meta loss averaged over last 500 steps = 3.2925e-01, PNorm = 95.8408, GNorm = 0.3287
Meta loss on this task batch = 3.5671e-01, Meta loss averaged over last 500 steps = 3.2922e-01, PNorm = 95.8435, GNorm = 0.4910
Took 146.88878107070923 seconds to complete one epoch of meta training
Took 155.17151284217834 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478313
Epoch 248
Meta loss on this task batch = 3.3337e-01, Meta loss averaged over last 500 steps = 3.2925e-01, PNorm = 95.8476, GNorm = 0.2813
Meta loss on this task batch = 3.7389e-01, Meta loss averaged over last 500 steps = 3.2922e-01, PNorm = 95.8498, GNorm = 0.3008
Meta loss on this task batch = 3.2421e-01, Meta loss averaged over last 500 steps = 3.2921e-01, PNorm = 95.8541, GNorm = 0.2900
Meta loss on this task batch = 3.0346e-01, Meta loss averaged over last 500 steps = 3.2915e-01, PNorm = 95.8590, GNorm = 0.2413
Meta loss on this task batch = 3.4131e-01, Meta loss averaged over last 500 steps = 3.2913e-01, PNorm = 95.8643, GNorm = 0.2842
Meta loss on this task batch = 3.3636e-01, Meta loss averaged over last 500 steps = 3.2924e-01, PNorm = 95.8705, GNorm = 0.2656
Meta loss on this task batch = 3.2996e-01, Meta loss averaged over last 500 steps = 3.2922e-01, PNorm = 95.8777, GNorm = 0.2683
Meta loss on this task batch = 3.2696e-01, Meta loss averaged over last 500 steps = 3.2929e-01, PNorm = 95.8876, GNorm = 0.2864
Meta loss on this task batch = 3.4943e-01, Meta loss averaged over last 500 steps = 3.2935e-01, PNorm = 95.8980, GNorm = 0.2471
Meta loss on this task batch = 3.0381e-01, Meta loss averaged over last 500 steps = 3.2929e-01, PNorm = 95.9093, GNorm = 0.2484
Meta loss on this task batch = 3.3401e-01, Meta loss averaged over last 500 steps = 3.2916e-01, PNorm = 95.9195, GNorm = 0.2891
Meta loss on this task batch = 3.1079e-01, Meta loss averaged over last 500 steps = 3.2910e-01, PNorm = 95.9292, GNorm = 0.2450
Meta loss on this task batch = 3.3800e-01, Meta loss averaged over last 500 steps = 3.2902e-01, PNorm = 95.9377, GNorm = 0.2902
Meta loss on this task batch = 2.7742e-01, Meta loss averaged over last 500 steps = 3.2893e-01, PNorm = 95.9443, GNorm = 0.2506
Meta loss on this task batch = 2.9587e-01, Meta loss averaged over last 500 steps = 3.2888e-01, PNorm = 95.9510, GNorm = 0.2851
Meta loss on this task batch = 3.6327e-01, Meta loss averaged over last 500 steps = 3.2889e-01, PNorm = 95.9546, GNorm = 0.3248
Meta loss on this task batch = 3.2382e-01, Meta loss averaged over last 500 steps = 3.2894e-01, PNorm = 95.9571, GNorm = 0.2926
Meta loss on this task batch = 3.3901e-01, Meta loss averaged over last 500 steps = 3.2902e-01, PNorm = 95.9604, GNorm = 0.2991
Meta loss on this task batch = 3.3179e-01, Meta loss averaged over last 500 steps = 3.2907e-01, PNorm = 95.9624, GNorm = 0.3648
Took 163.51798367500305 seconds to complete one epoch of meta training
Took 173.17806100845337 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483783
Epoch 249
Meta loss on this task batch = 3.2503e-01, Meta loss averaged over last 500 steps = 3.2908e-01, PNorm = 95.9650, GNorm = 0.3000
Meta loss on this task batch = 3.8080e-01, Meta loss averaged over last 500 steps = 3.2917e-01, PNorm = 95.9671, GNorm = 0.3034
Meta loss on this task batch = 2.8892e-01, Meta loss averaged over last 500 steps = 3.2907e-01, PNorm = 95.9720, GNorm = 0.2812
Meta loss on this task batch = 2.9883e-01, Meta loss averaged over last 500 steps = 3.2902e-01, PNorm = 95.9773, GNorm = 0.2744
Meta loss on this task batch = 3.2577e-01, Meta loss averaged over last 500 steps = 3.2898e-01, PNorm = 95.9848, GNorm = 0.2811
Meta loss on this task batch = 3.6378e-01, Meta loss averaged over last 500 steps = 3.2902e-01, PNorm = 95.9931, GNorm = 0.3084
Meta loss on this task batch = 3.0244e-01, Meta loss averaged over last 500 steps = 3.2888e-01, PNorm = 96.0022, GNorm = 0.2652
Meta loss on this task batch = 3.1468e-01, Meta loss averaged over last 500 steps = 3.2876e-01, PNorm = 96.0112, GNorm = 0.3083
Meta loss on this task batch = 3.4878e-01, Meta loss averaged over last 500 steps = 3.2887e-01, PNorm = 96.0196, GNorm = 0.2818
Meta loss on this task batch = 2.7588e-01, Meta loss averaged over last 500 steps = 3.2872e-01, PNorm = 96.0267, GNorm = 0.2819
Meta loss on this task batch = 3.4445e-01, Meta loss averaged over last 500 steps = 3.2883e-01, PNorm = 96.0315, GNorm = 0.3133
Meta loss on this task batch = 3.5808e-01, Meta loss averaged over last 500 steps = 3.2890e-01, PNorm = 96.0377, GNorm = 0.2888
Meta loss on this task batch = 3.0839e-01, Meta loss averaged over last 500 steps = 3.2891e-01, PNorm = 96.0433, GNorm = 0.3342
Meta loss on this task batch = 3.3165e-01, Meta loss averaged over last 500 steps = 3.2893e-01, PNorm = 96.0480, GNorm = 0.3028
Meta loss on this task batch = 3.0168e-01, Meta loss averaged over last 500 steps = 3.2899e-01, PNorm = 96.0513, GNorm = 0.2753
Meta loss on this task batch = 3.2804e-01, Meta loss averaged over last 500 steps = 3.2899e-01, PNorm = 96.0555, GNorm = 0.2667
Meta loss on this task batch = 3.3956e-01, Meta loss averaged over last 500 steps = 3.2889e-01, PNorm = 96.0606, GNorm = 0.2780
Meta loss on this task batch = 3.5540e-01, Meta loss averaged over last 500 steps = 3.2892e-01, PNorm = 96.0680, GNorm = 0.2817
Meta loss on this task batch = 3.6891e-01, Meta loss averaged over last 500 steps = 3.2902e-01, PNorm = 96.0762, GNorm = 0.3476
Took 135.90707564353943 seconds to complete one epoch of meta training
Took 143.62388825416565 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475924
Epoch 250
Meta loss on this task batch = 3.4195e-01, Meta loss averaged over last 500 steps = 3.2910e-01, PNorm = 96.0842, GNorm = 0.2711
Meta loss on this task batch = 3.1628e-01, Meta loss averaged over last 500 steps = 3.2909e-01, PNorm = 96.0907, GNorm = 0.2888
Meta loss on this task batch = 3.3645e-01, Meta loss averaged over last 500 steps = 3.2904e-01, PNorm = 96.0969, GNorm = 0.2958
Meta loss on this task batch = 3.7650e-01, Meta loss averaged over last 500 steps = 3.2902e-01, PNorm = 96.1029, GNorm = 0.2964
Meta loss on this task batch = 2.8000e-01, Meta loss averaged over last 500 steps = 3.2895e-01, PNorm = 96.1064, GNorm = 0.2624
Meta loss on this task batch = 2.9327e-01, Meta loss averaged over last 500 steps = 3.2878e-01, PNorm = 96.1110, GNorm = 0.2523
Meta loss on this task batch = 3.3857e-01, Meta loss averaged over last 500 steps = 3.2880e-01, PNorm = 96.1171, GNorm = 0.2804
Meta loss on this task batch = 3.8653e-01, Meta loss averaged over last 500 steps = 3.2903e-01, PNorm = 96.1233, GNorm = 0.3835
Meta loss on this task batch = 3.1094e-01, Meta loss averaged over last 500 steps = 3.2904e-01, PNorm = 96.1301, GNorm = 0.2643
Meta loss on this task batch = 2.7439e-01, Meta loss averaged over last 500 steps = 3.2886e-01, PNorm = 96.1379, GNorm = 0.2551
Meta loss on this task batch = 2.9130e-01, Meta loss averaged over last 500 steps = 3.2876e-01, PNorm = 96.1478, GNorm = 0.2517
Meta loss on this task batch = 2.9302e-01, Meta loss averaged over last 500 steps = 3.2869e-01, PNorm = 96.1589, GNorm = 0.2633
Meta loss on this task batch = 2.9627e-01, Meta loss averaged over last 500 steps = 3.2869e-01, PNorm = 96.1707, GNorm = 0.3046
Meta loss on this task batch = 3.5324e-01, Meta loss averaged over last 500 steps = 3.2879e-01, PNorm = 96.1808, GNorm = 0.2911
Meta loss on this task batch = 3.1610e-01, Meta loss averaged over last 500 steps = 3.2861e-01, PNorm = 96.1907, GNorm = 0.2516
Meta loss on this task batch = 3.3713e-01, Meta loss averaged over last 500 steps = 3.2854e-01, PNorm = 96.1996, GNorm = 0.3045
Meta loss on this task batch = 3.3310e-01, Meta loss averaged over last 500 steps = 3.2869e-01, PNorm = 96.2065, GNorm = 0.3002
Meta loss on this task batch = 3.5218e-01, Meta loss averaged over last 500 steps = 3.2883e-01, PNorm = 96.2137, GNorm = 0.2875
Meta loss on this task batch = 3.0968e-01, Meta loss averaged over last 500 steps = 3.2874e-01, PNorm = 96.2202, GNorm = 0.3629
Took 135.80642342567444 seconds to complete one epoch of meta training
Took 144.2807273864746 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468608
Epoch 251
Meta loss on this task batch = 2.9433e-01, Meta loss averaged over last 500 steps = 3.2865e-01, PNorm = 96.2254, GNorm = 0.2625
Meta loss on this task batch = 2.9240e-01, Meta loss averaged over last 500 steps = 3.2857e-01, PNorm = 96.2310, GNorm = 0.2660
Meta loss on this task batch = 3.2872e-01, Meta loss averaged over last 500 steps = 3.2852e-01, PNorm = 96.2347, GNorm = 0.3259
Meta loss on this task batch = 3.1469e-01, Meta loss averaged over last 500 steps = 3.2844e-01, PNorm = 96.2384, GNorm = 0.3010
Meta loss on this task batch = 3.2232e-01, Meta loss averaged over last 500 steps = 3.2841e-01, PNorm = 96.2407, GNorm = 0.3128
Meta loss on this task batch = 2.9062e-01, Meta loss averaged over last 500 steps = 3.2831e-01, PNorm = 96.2442, GNorm = 0.2384
Meta loss on this task batch = 3.3307e-01, Meta loss averaged over last 500 steps = 3.2829e-01, PNorm = 96.2483, GNorm = 0.3008
Meta loss on this task batch = 3.2758e-01, Meta loss averaged over last 500 steps = 3.2829e-01, PNorm = 96.2514, GNorm = 0.2784
Meta loss on this task batch = 3.1685e-01, Meta loss averaged over last 500 steps = 3.2832e-01, PNorm = 96.2581, GNorm = 0.2935
Meta loss on this task batch = 3.8238e-01, Meta loss averaged over last 500 steps = 3.2845e-01, PNorm = 96.2645, GNorm = 0.3049
Meta loss on this task batch = 3.5058e-01, Meta loss averaged over last 500 steps = 3.2854e-01, PNorm = 96.2732, GNorm = 0.2860
Meta loss on this task batch = 2.8449e-01, Meta loss averaged over last 500 steps = 3.2842e-01, PNorm = 96.2861, GNorm = 0.2919
Meta loss on this task batch = 3.4849e-01, Meta loss averaged over last 500 steps = 3.2843e-01, PNorm = 96.3001, GNorm = 0.2697
Meta loss on this task batch = 3.1437e-01, Meta loss averaged over last 500 steps = 3.2856e-01, PNorm = 96.3125, GNorm = 0.2815
Meta loss on this task batch = 3.3771e-01, Meta loss averaged over last 500 steps = 3.2853e-01, PNorm = 96.3249, GNorm = 0.2966
Meta loss on this task batch = 3.4364e-01, Meta loss averaged over last 500 steps = 3.2857e-01, PNorm = 96.3352, GNorm = 0.2774
Meta loss on this task batch = 3.3624e-01, Meta loss averaged over last 500 steps = 3.2856e-01, PNorm = 96.3431, GNorm = 0.3234
Meta loss on this task batch = 3.3500e-01, Meta loss averaged over last 500 steps = 3.2852e-01, PNorm = 96.3508, GNorm = 0.2747
Meta loss on this task batch = 3.7826e-01, Meta loss averaged over last 500 steps = 3.2867e-01, PNorm = 96.3566, GNorm = 0.3403
Took 133.81292176246643 seconds to complete one epoch of meta training
Took 142.45485854148865 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471650
Epoch 252
Meta loss on this task batch = 3.3254e-01, Meta loss averaged over last 500 steps = 3.2867e-01, PNorm = 96.3610, GNorm = 0.3005
Meta loss on this task batch = 3.4524e-01, Meta loss averaged over last 500 steps = 3.2869e-01, PNorm = 96.3654, GNorm = 0.2791
Meta loss on this task batch = 2.8380e-01, Meta loss averaged over last 500 steps = 3.2854e-01, PNorm = 96.3712, GNorm = 0.2524
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 3.2848e-01, PNorm = 96.3778, GNorm = 0.2541
Meta loss on this task batch = 3.4675e-01, Meta loss averaged over last 500 steps = 3.2854e-01, PNorm = 96.3852, GNorm = 0.2795
Meta loss on this task batch = 3.0497e-01, Meta loss averaged over last 500 steps = 3.2842e-01, PNorm = 96.3944, GNorm = 0.2475
Meta loss on this task batch = 3.4675e-01, Meta loss averaged over last 500 steps = 3.2838e-01, PNorm = 96.4047, GNorm = 0.2658
Meta loss on this task batch = 2.5527e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 96.4176, GNorm = 0.2463
Meta loss on this task batch = 3.6175e-01, Meta loss averaged over last 500 steps = 3.2833e-01, PNorm = 96.4313, GNorm = 0.2933
Meta loss on this task batch = 2.9908e-01, Meta loss averaged over last 500 steps = 3.2832e-01, PNorm = 96.4442, GNorm = 0.2733
Meta loss on this task batch = 2.8138e-01, Meta loss averaged over last 500 steps = 3.2821e-01, PNorm = 96.4533, GNorm = 0.3203
Meta loss on this task batch = 3.5676e-01, Meta loss averaged over last 500 steps = 3.2830e-01, PNorm = 96.4592, GNorm = 0.3146
Meta loss on this task batch = 3.1427e-01, Meta loss averaged over last 500 steps = 3.2820e-01, PNorm = 96.4624, GNorm = 0.2866
Meta loss on this task batch = 3.0640e-01, Meta loss averaged over last 500 steps = 3.2802e-01, PNorm = 96.4645, GNorm = 0.2954
Meta loss on this task batch = 3.1279e-01, Meta loss averaged over last 500 steps = 3.2793e-01, PNorm = 96.4659, GNorm = 0.2929
Meta loss on this task batch = 3.8401e-01, Meta loss averaged over last 500 steps = 3.2807e-01, PNorm = 96.4669, GNorm = 0.4127
Meta loss on this task batch = 3.5255e-01, Meta loss averaged over last 500 steps = 3.2819e-01, PNorm = 96.4691, GNorm = 0.2984
Meta loss on this task batch = 3.2536e-01, Meta loss averaged over last 500 steps = 3.2829e-01, PNorm = 96.4712, GNorm = 0.3502
Meta loss on this task batch = 3.0587e-01, Meta loss averaged over last 500 steps = 3.2821e-01, PNorm = 96.4737, GNorm = 0.3287
Took 133.41830110549927 seconds to complete one epoch of meta training
Took 141.7469310760498 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490100
Epoch 253
Meta loss on this task batch = 3.0769e-01, Meta loss averaged over last 500 steps = 3.2812e-01, PNorm = 96.4781, GNorm = 0.2944
Meta loss on this task batch = 3.3406e-01, Meta loss averaged over last 500 steps = 3.2814e-01, PNorm = 96.4819, GNorm = 0.2644
Meta loss on this task batch = 2.5531e-01, Meta loss averaged over last 500 steps = 3.2798e-01, PNorm = 96.4885, GNorm = 0.2500
Meta loss on this task batch = 3.0485e-01, Meta loss averaged over last 500 steps = 3.2801e-01, PNorm = 96.4961, GNorm = 0.3052
Meta loss on this task batch = 3.2940e-01, Meta loss averaged over last 500 steps = 3.2810e-01, PNorm = 96.5053, GNorm = 0.2943
Meta loss on this task batch = 3.5818e-01, Meta loss averaged over last 500 steps = 3.2808e-01, PNorm = 96.5156, GNorm = 0.2746
Meta loss on this task batch = 3.2785e-01, Meta loss averaged over last 500 steps = 3.2797e-01, PNorm = 96.5259, GNorm = 0.3029
Meta loss on this task batch = 3.5983e-01, Meta loss averaged over last 500 steps = 3.2810e-01, PNorm = 96.5354, GNorm = 0.3087
Meta loss on this task batch = 3.9244e-01, Meta loss averaged over last 500 steps = 3.2826e-01, PNorm = 96.5417, GNorm = 0.3258
Meta loss on this task batch = 3.0422e-01, Meta loss averaged over last 500 steps = 3.2840e-01, PNorm = 96.5457, GNorm = 0.3005
Meta loss on this task batch = 3.4986e-01, Meta loss averaged over last 500 steps = 3.2853e-01, PNorm = 96.5481, GNorm = 0.3343
Meta loss on this task batch = 3.3736e-01, Meta loss averaged over last 500 steps = 3.2856e-01, PNorm = 96.5498, GNorm = 0.3189
Meta loss on this task batch = 3.3231e-01, Meta loss averaged over last 500 steps = 3.2846e-01, PNorm = 96.5500, GNorm = 0.3016
Meta loss on this task batch = 3.6275e-01, Meta loss averaged over last 500 steps = 3.2857e-01, PNorm = 96.5496, GNorm = 0.3130
Meta loss on this task batch = 3.2336e-01, Meta loss averaged over last 500 steps = 3.2858e-01, PNorm = 96.5505, GNorm = 0.2906
Meta loss on this task batch = 3.1332e-01, Meta loss averaged over last 500 steps = 3.2849e-01, PNorm = 96.5546, GNorm = 0.2752
Meta loss on this task batch = 3.0519e-01, Meta loss averaged over last 500 steps = 3.2846e-01, PNorm = 96.5598, GNorm = 0.2525
Meta loss on this task batch = 3.0165e-01, Meta loss averaged over last 500 steps = 3.2833e-01, PNorm = 96.5677, GNorm = 0.2840
Meta loss on this task batch = 3.2416e-01, Meta loss averaged over last 500 steps = 3.2821e-01, PNorm = 96.5770, GNorm = 0.3337
Took 132.635479927063 seconds to complete one epoch of meta training
Took 141.07134342193604 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491163
Epoch 254
Meta loss on this task batch = 2.5777e-01, Meta loss averaged over last 500 steps = 3.2813e-01, PNorm = 96.5869, GNorm = 0.2840
Meta loss on this task batch = 3.4061e-01, Meta loss averaged over last 500 steps = 3.2821e-01, PNorm = 96.5979, GNorm = 0.2884
Meta loss on this task batch = 3.3762e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 96.6086, GNorm = 0.2739
Meta loss on this task batch = 2.8753e-01, Meta loss averaged over last 500 steps = 3.2812e-01, PNorm = 96.6196, GNorm = 0.2428
Meta loss on this task batch = 3.2509e-01, Meta loss averaged over last 500 steps = 3.2814e-01, PNorm = 96.6271, GNorm = 0.3701
Meta loss on this task batch = 2.8319e-01, Meta loss averaged over last 500 steps = 3.2812e-01, PNorm = 96.6361, GNorm = 0.2707
Meta loss on this task batch = 3.0331e-01, Meta loss averaged over last 500 steps = 3.2804e-01, PNorm = 96.6437, GNorm = 0.2830
Meta loss on this task batch = 3.1506e-01, Meta loss averaged over last 500 steps = 3.2802e-01, PNorm = 96.6519, GNorm = 0.2735
Meta loss on this task batch = 3.4648e-01, Meta loss averaged over last 500 steps = 3.2807e-01, PNorm = 96.6604, GNorm = 0.3080
Meta loss on this task batch = 3.2495e-01, Meta loss averaged over last 500 steps = 3.2804e-01, PNorm = 96.6701, GNorm = 0.3009
Meta loss on this task batch = 3.7891e-01, Meta loss averaged over last 500 steps = 3.2804e-01, PNorm = 96.6806, GNorm = 0.2847
Meta loss on this task batch = 2.7887e-01, Meta loss averaged over last 500 steps = 3.2802e-01, PNorm = 96.6921, GNorm = 0.2587
Meta loss on this task batch = 2.7276e-01, Meta loss averaged over last 500 steps = 3.2803e-01, PNorm = 96.7045, GNorm = 0.2604
Meta loss on this task batch = 3.2134e-01, Meta loss averaged over last 500 steps = 3.2805e-01, PNorm = 96.7175, GNorm = 0.2723
Meta loss on this task batch = 3.2901e-01, Meta loss averaged over last 500 steps = 3.2804e-01, PNorm = 96.7269, GNorm = 0.3688
Meta loss on this task batch = 3.1698e-01, Meta loss averaged over last 500 steps = 3.2799e-01, PNorm = 96.7357, GNorm = 0.2891
Meta loss on this task batch = 3.3022e-01, Meta loss averaged over last 500 steps = 3.2793e-01, PNorm = 96.7417, GNorm = 0.3551
Meta loss on this task batch = 3.6105e-01, Meta loss averaged over last 500 steps = 3.2793e-01, PNorm = 96.7475, GNorm = 0.3906
Meta loss on this task batch = 3.1218e-01, Meta loss averaged over last 500 steps = 3.2791e-01, PNorm = 96.7530, GNorm = 0.3495
Took 132.34152817726135 seconds to complete one epoch of meta training
Took 140.9422891139984 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501426
Epoch 255
Meta loss on this task batch = 3.3876e-01, Meta loss averaged over last 500 steps = 3.2790e-01, PNorm = 96.7582, GNorm = 0.3174
Meta loss on this task batch = 3.2409e-01, Meta loss averaged over last 500 steps = 3.2785e-01, PNorm = 96.7630, GNorm = 0.2731
Meta loss on this task batch = 4.0005e-01, Meta loss averaged over last 500 steps = 3.2804e-01, PNorm = 96.7648, GNorm = 0.3613
Meta loss on this task batch = 3.5644e-01, Meta loss averaged over last 500 steps = 3.2803e-01, PNorm = 96.7660, GNorm = 0.2804
Meta loss on this task batch = 3.3629e-01, Meta loss averaged over last 500 steps = 3.2804e-01, PNorm = 96.7680, GNorm = 0.2563
Meta loss on this task batch = 3.0322e-01, Meta loss averaged over last 500 steps = 3.2816e-01, PNorm = 96.7704, GNorm = 0.3237
Meta loss on this task batch = 3.4459e-01, Meta loss averaged over last 500 steps = 3.2829e-01, PNorm = 96.7747, GNorm = 0.2622
Meta loss on this task batch = 2.9997e-01, Meta loss averaged over last 500 steps = 3.2826e-01, PNorm = 96.7816, GNorm = 0.2664
Meta loss on this task batch = 3.2502e-01, Meta loss averaged over last 500 steps = 3.2831e-01, PNorm = 96.7892, GNorm = 0.2689
Meta loss on this task batch = 3.3348e-01, Meta loss averaged over last 500 steps = 3.2827e-01, PNorm = 96.7954, GNorm = 0.2701
Meta loss on this task batch = 3.3770e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 96.8026, GNorm = 0.2935
Meta loss on this task batch = 2.9158e-01, Meta loss averaged over last 500 steps = 3.2821e-01, PNorm = 96.8087, GNorm = 0.3293
Meta loss on this task batch = 3.1446e-01, Meta loss averaged over last 500 steps = 3.2819e-01, PNorm = 96.8159, GNorm = 0.2840
Meta loss on this task batch = 3.4821e-01, Meta loss averaged over last 500 steps = 3.2818e-01, PNorm = 96.8217, GNorm = 0.2764
Meta loss on this task batch = 3.2001e-01, Meta loss averaged over last 500 steps = 3.2822e-01, PNorm = 96.8291, GNorm = 0.2671
Meta loss on this task batch = 2.9572e-01, Meta loss averaged over last 500 steps = 3.2822e-01, PNorm = 96.8373, GNorm = 0.2550
Meta loss on this task batch = 3.1056e-01, Meta loss averaged over last 500 steps = 3.2829e-01, PNorm = 96.8450, GNorm = 0.2896
Meta loss on this task batch = 2.8130e-01, Meta loss averaged over last 500 steps = 3.2816e-01, PNorm = 96.8527, GNorm = 0.2476
Meta loss on this task batch = 2.3942e-01, Meta loss averaged over last 500 steps = 3.2814e-01, PNorm = 96.8603, GNorm = 0.2969
Took 132.47240495681763 seconds to complete one epoch of meta training
Took 141.2013874053955 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508353
Epoch 256
Meta loss on this task batch = 3.4674e-01, Meta loss averaged over last 500 steps = 3.2812e-01, PNorm = 96.8668, GNorm = 0.3004
Meta loss on this task batch = 2.9422e-01, Meta loss averaged over last 500 steps = 3.2804e-01, PNorm = 96.8736, GNorm = 0.2602
Meta loss on this task batch = 2.8755e-01, Meta loss averaged over last 500 steps = 3.2789e-01, PNorm = 96.8805, GNorm = 0.3023
Meta loss on this task batch = 2.9882e-01, Meta loss averaged over last 500 steps = 3.2782e-01, PNorm = 96.8869, GNorm = 0.3147
Meta loss on this task batch = 2.7179e-01, Meta loss averaged over last 500 steps = 3.2768e-01, PNorm = 96.8917, GNorm = 0.2795
Meta loss on this task batch = 2.8607e-01, Meta loss averaged over last 500 steps = 3.2756e-01, PNorm = 96.8990, GNorm = 0.2962
Meta loss on this task batch = 3.2182e-01, Meta loss averaged over last 500 steps = 3.2764e-01, PNorm = 96.9028, GNorm = 0.3576
Meta loss on this task batch = 3.7278e-01, Meta loss averaged over last 500 steps = 3.2774e-01, PNorm = 96.9035, GNorm = 0.3529
Meta loss on this task batch = 3.7807e-01, Meta loss averaged over last 500 steps = 3.2786e-01, PNorm = 96.9029, GNorm = 0.3721
Meta loss on this task batch = 3.5442e-01, Meta loss averaged over last 500 steps = 3.2783e-01, PNorm = 96.9015, GNorm = 0.3361
Meta loss on this task batch = 3.3513e-01, Meta loss averaged over last 500 steps = 3.2779e-01, PNorm = 96.9013, GNorm = 0.3608
Meta loss on this task batch = 2.8873e-01, Meta loss averaged over last 500 steps = 3.2781e-01, PNorm = 96.9051, GNorm = 0.2761
Meta loss on this task batch = 3.0021e-01, Meta loss averaged over last 500 steps = 3.2781e-01, PNorm = 96.9123, GNorm = 0.2783
Meta loss on this task batch = 3.4390e-01, Meta loss averaged over last 500 steps = 3.2775e-01, PNorm = 96.9227, GNorm = 0.3046
Meta loss on this task batch = 3.4683e-01, Meta loss averaged over last 500 steps = 3.2781e-01, PNorm = 96.9336, GNorm = 0.2897
Meta loss on this task batch = 3.5550e-01, Meta loss averaged over last 500 steps = 3.2777e-01, PNorm = 96.9433, GNorm = 0.2575
Meta loss on this task batch = 3.1882e-01, Meta loss averaged over last 500 steps = 3.2772e-01, PNorm = 96.9517, GNorm = 0.2675
Meta loss on this task batch = 3.4926e-01, Meta loss averaged over last 500 steps = 3.2774e-01, PNorm = 96.9612, GNorm = 0.2845
Meta loss on this task batch = 2.9909e-01, Meta loss averaged over last 500 steps = 3.2774e-01, PNorm = 96.9695, GNorm = 0.3110
Took 133.238618850708 seconds to complete one epoch of meta training
Took 141.34795093536377 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.522027
Epoch 257
Meta loss on this task batch = 2.9466e-01, Meta loss averaged over last 500 steps = 3.2762e-01, PNorm = 96.9771, GNorm = 0.2577
Meta loss on this task batch = 2.9450e-01, Meta loss averaged over last 500 steps = 3.2742e-01, PNorm = 96.9844, GNorm = 0.2767
Meta loss on this task batch = 3.0682e-01, Meta loss averaged over last 500 steps = 3.2739e-01, PNorm = 96.9891, GNorm = 0.2674
Meta loss on this task batch = 3.0038e-01, Meta loss averaged over last 500 steps = 3.2737e-01, PNorm = 96.9929, GNorm = 0.2842
Meta loss on this task batch = 2.9970e-01, Meta loss averaged over last 500 steps = 3.2736e-01, PNorm = 96.9986, GNorm = 0.2858
Meta loss on this task batch = 3.0898e-01, Meta loss averaged over last 500 steps = 3.2727e-01, PNorm = 97.0050, GNorm = 0.2784
Meta loss on this task batch = 3.7328e-01, Meta loss averaged over last 500 steps = 3.2714e-01, PNorm = 97.0137, GNorm = 0.2916
Meta loss on this task batch = 2.7828e-01, Meta loss averaged over last 500 steps = 3.2706e-01, PNorm = 97.0220, GNorm = 0.2533
Meta loss on this task batch = 3.3508e-01, Meta loss averaged over last 500 steps = 3.2702e-01, PNorm = 97.0281, GNorm = 0.3336
Meta loss on this task batch = 2.5736e-01, Meta loss averaged over last 500 steps = 3.2690e-01, PNorm = 97.0374, GNorm = 0.2575
Meta loss on this task batch = 3.4591e-01, Meta loss averaged over last 500 steps = 3.2692e-01, PNorm = 97.0464, GNorm = 0.3091
Meta loss on this task batch = 3.8695e-01, Meta loss averaged over last 500 steps = 3.2705e-01, PNorm = 97.0558, GNorm = 0.3179
Meta loss on this task batch = 3.9074e-01, Meta loss averaged over last 500 steps = 3.2726e-01, PNorm = 97.0646, GNorm = 0.3272
Meta loss on this task batch = 3.3212e-01, Meta loss averaged over last 500 steps = 3.2724e-01, PNorm = 97.0712, GNorm = 0.3102
Meta loss on this task batch = 3.4481e-01, Meta loss averaged over last 500 steps = 3.2722e-01, PNorm = 97.0774, GNorm = 0.3000
Meta loss on this task batch = 3.3967e-01, Meta loss averaged over last 500 steps = 3.2734e-01, PNorm = 97.0843, GNorm = 0.3129
Meta loss on this task batch = 2.9540e-01, Meta loss averaged over last 500 steps = 3.2720e-01, PNorm = 97.0910, GNorm = 0.2782
Meta loss on this task batch = 3.6119e-01, Meta loss averaged over last 500 steps = 3.2732e-01, PNorm = 97.0993, GNorm = 0.3447
Meta loss on this task batch = 3.0346e-01, Meta loss averaged over last 500 steps = 3.2732e-01, PNorm = 97.1077, GNorm = 0.2936
Took 131.5641508102417 seconds to complete one epoch of meta training
Took 140.27486109733582 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498689
Epoch 258
Meta loss on this task batch = 3.0024e-01, Meta loss averaged over last 500 steps = 3.2735e-01, PNorm = 97.1158, GNorm = 0.2727
Meta loss on this task batch = 3.2710e-01, Meta loss averaged over last 500 steps = 3.2728e-01, PNorm = 97.1245, GNorm = 0.2962
Meta loss on this task batch = 3.5004e-01, Meta loss averaged over last 500 steps = 3.2731e-01, PNorm = 97.1339, GNorm = 0.3258
Meta loss on this task batch = 3.5977e-01, Meta loss averaged over last 500 steps = 3.2729e-01, PNorm = 97.1414, GNorm = 0.2794
Meta loss on this task batch = 2.7639e-01, Meta loss averaged over last 500 steps = 3.2719e-01, PNorm = 97.1485, GNorm = 0.2743
Meta loss on this task batch = 3.2104e-01, Meta loss averaged over last 500 steps = 3.2731e-01, PNorm = 97.1561, GNorm = 0.2775
Meta loss on this task batch = 3.6007e-01, Meta loss averaged over last 500 steps = 3.2743e-01, PNorm = 97.1634, GNorm = 0.2675
Meta loss on this task batch = 2.9812e-01, Meta loss averaged over last 500 steps = 3.2744e-01, PNorm = 97.1720, GNorm = 0.2790
Meta loss on this task batch = 3.2052e-01, Meta loss averaged over last 500 steps = 3.2742e-01, PNorm = 97.1797, GNorm = 0.3142
Meta loss on this task batch = 3.8432e-01, Meta loss averaged over last 500 steps = 3.2759e-01, PNorm = 97.1876, GNorm = 0.3264
Meta loss on this task batch = 3.3301e-01, Meta loss averaged over last 500 steps = 3.2767e-01, PNorm = 97.1960, GNorm = 0.2904
Meta loss on this task batch = 2.7531e-01, Meta loss averaged over last 500 steps = 3.2744e-01, PNorm = 97.2052, GNorm = 0.2652
Meta loss on this task batch = 3.4175e-01, Meta loss averaged over last 500 steps = 3.2747e-01, PNorm = 97.2148, GNorm = 0.3255
Meta loss on this task batch = 3.9004e-01, Meta loss averaged over last 500 steps = 3.2749e-01, PNorm = 97.2189, GNorm = 0.3966
Meta loss on this task batch = 3.3800e-01, Meta loss averaged over last 500 steps = 3.2751e-01, PNorm = 97.2237, GNorm = 0.3136
Meta loss on this task batch = 3.3307e-01, Meta loss averaged over last 500 steps = 3.2744e-01, PNorm = 97.2303, GNorm = 0.2760
Meta loss on this task batch = 3.4930e-01, Meta loss averaged over last 500 steps = 3.2743e-01, PNorm = 97.2352, GNorm = 0.3140
Meta loss on this task batch = 2.6074e-01, Meta loss averaged over last 500 steps = 3.2723e-01, PNorm = 97.2413, GNorm = 0.2265
Meta loss on this task batch = 3.0636e-01, Meta loss averaged over last 500 steps = 3.2715e-01, PNorm = 97.2480, GNorm = 0.3208
Took 134.31680917739868 seconds to complete one epoch of meta training
Took 142.93890047073364 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484217
Epoch 259
Meta loss on this task batch = 2.9933e-01, Meta loss averaged over last 500 steps = 3.2703e-01, PNorm = 97.2553, GNorm = 0.2579
Meta loss on this task batch = 3.1561e-01, Meta loss averaged over last 500 steps = 3.2702e-01, PNorm = 97.2623, GNorm = 0.2712
Meta loss on this task batch = 3.0640e-01, Meta loss averaged over last 500 steps = 3.2692e-01, PNorm = 97.2680, GNorm = 0.2856
Meta loss on this task batch = 3.5705e-01, Meta loss averaged over last 500 steps = 3.2695e-01, PNorm = 97.2723, GNorm = 0.2826
Meta loss on this task batch = 4.0601e-01, Meta loss averaged over last 500 steps = 3.2712e-01, PNorm = 97.2733, GNorm = 0.3534
Meta loss on this task batch = 3.6587e-01, Meta loss averaged over last 500 steps = 3.2726e-01, PNorm = 97.2725, GNorm = 0.3140
Meta loss on this task batch = 2.6447e-01, Meta loss averaged over last 500 steps = 3.2723e-01, PNorm = 97.2737, GNorm = 0.2978
Meta loss on this task batch = 3.1791e-01, Meta loss averaged over last 500 steps = 3.2720e-01, PNorm = 97.2747, GNorm = 0.2967
Meta loss on this task batch = 3.1258e-01, Meta loss averaged over last 500 steps = 3.2726e-01, PNorm = 97.2791, GNorm = 0.2918
Meta loss on this task batch = 3.7659e-01, Meta loss averaged over last 500 steps = 3.2729e-01, PNorm = 97.2878, GNorm = 0.3213
Meta loss on this task batch = 3.3554e-01, Meta loss averaged over last 500 steps = 3.2735e-01, PNorm = 97.2948, GNorm = 0.2631
Meta loss on this task batch = 3.1316e-01, Meta loss averaged over last 500 steps = 3.2740e-01, PNorm = 97.3035, GNorm = 0.2476
Meta loss on this task batch = 2.9727e-01, Meta loss averaged over last 500 steps = 3.2728e-01, PNorm = 97.3107, GNorm = 0.2623
Meta loss on this task batch = 3.3014e-01, Meta loss averaged over last 500 steps = 3.2729e-01, PNorm = 97.3177, GNorm = 0.2799
Meta loss on this task batch = 2.9981e-01, Meta loss averaged over last 500 steps = 3.2719e-01, PNorm = 97.3212, GNorm = 0.2886
Meta loss on this task batch = 2.7846e-01, Meta loss averaged over last 500 steps = 3.2703e-01, PNorm = 97.3251, GNorm = 0.2431
Meta loss on this task batch = 3.5635e-01, Meta loss averaged over last 500 steps = 3.2709e-01, PNorm = 97.3272, GNorm = 0.3484
Meta loss on this task batch = 3.0416e-01, Meta loss averaged over last 500 steps = 3.2697e-01, PNorm = 97.3286, GNorm = 0.2734
Meta loss on this task batch = 3.1801e-01, Meta loss averaged over last 500 steps = 3.2693e-01, PNorm = 97.3290, GNorm = 0.2928
Took 133.1070201396942 seconds to complete one epoch of meta training
Took 142.27877354621887 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478831
Epoch 260
Meta loss on this task batch = 2.7838e-01, Meta loss averaged over last 500 steps = 3.2681e-01, PNorm = 97.3298, GNorm = 0.2660
Meta loss on this task batch = 3.7082e-01, Meta loss averaged over last 500 steps = 3.2689e-01, PNorm = 97.3311, GNorm = 0.3041
Meta loss on this task batch = 2.8644e-01, Meta loss averaged over last 500 steps = 3.2682e-01, PNorm = 97.3315, GNorm = 0.2800
Meta loss on this task batch = 3.5768e-01, Meta loss averaged over last 500 steps = 3.2689e-01, PNorm = 97.3333, GNorm = 0.2914
Meta loss on this task batch = 3.1984e-01, Meta loss averaged over last 500 steps = 3.2679e-01, PNorm = 97.3351, GNorm = 0.2620
Meta loss on this task batch = 2.9527e-01, Meta loss averaged over last 500 steps = 3.2672e-01, PNorm = 97.3385, GNorm = 0.2665
Meta loss on this task batch = 3.5938e-01, Meta loss averaged over last 500 steps = 3.2678e-01, PNorm = 97.3421, GNorm = 0.2884
Meta loss on this task batch = 3.1613e-01, Meta loss averaged over last 500 steps = 3.2674e-01, PNorm = 97.3477, GNorm = 0.2671
Meta loss on this task batch = 3.2159e-01, Meta loss averaged over last 500 steps = 3.2669e-01, PNorm = 97.3549, GNorm = 0.2664
Meta loss on this task batch = 3.7038e-01, Meta loss averaged over last 500 steps = 3.2694e-01, PNorm = 97.3628, GNorm = 0.2910
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 3.2685e-01, PNorm = 97.3729, GNorm = 0.2773
Meta loss on this task batch = 3.7056e-01, Meta loss averaged over last 500 steps = 3.2691e-01, PNorm = 97.3812, GNorm = 0.3444
Meta loss on this task batch = 2.9240e-01, Meta loss averaged over last 500 steps = 3.2683e-01, PNorm = 97.3901, GNorm = 0.2593
Meta loss on this task batch = 3.2079e-01, Meta loss averaged over last 500 steps = 3.2685e-01, PNorm = 97.3997, GNorm = 0.3071
Meta loss on this task batch = 3.3948e-01, Meta loss averaged over last 500 steps = 3.2682e-01, PNorm = 97.4091, GNorm = 0.3038
Meta loss on this task batch = 2.8389e-01, Meta loss averaged over last 500 steps = 3.2670e-01, PNorm = 97.4179, GNorm = 0.2853
Meta loss on this task batch = 2.9378e-01, Meta loss averaged over last 500 steps = 3.2654e-01, PNorm = 97.4276, GNorm = 0.2724
Meta loss on this task batch = 2.9261e-01, Meta loss averaged over last 500 steps = 3.2639e-01, PNorm = 97.4392, GNorm = 0.2502
Meta loss on this task batch = 3.2916e-01, Meta loss averaged over last 500 steps = 3.2628e-01, PNorm = 97.4511, GNorm = 0.3162
Took 132.39737462997437 seconds to complete one epoch of meta training
Took 141.00354552268982 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508920
Epoch 261
Meta loss on this task batch = 3.0371e-01, Meta loss averaged over last 500 steps = 3.2635e-01, PNorm = 97.4644, GNorm = 0.3029
Meta loss on this task batch = 2.7839e-01, Meta loss averaged over last 500 steps = 3.2627e-01, PNorm = 97.4783, GNorm = 0.2719
Meta loss on this task batch = 3.0354e-01, Meta loss averaged over last 500 steps = 3.2618e-01, PNorm = 97.4888, GNorm = 0.2832
Meta loss on this task batch = 3.4719e-01, Meta loss averaged over last 500 steps = 3.2620e-01, PNorm = 97.4957, GNorm = 0.3431
Meta loss on this task batch = 3.8755e-01, Meta loss averaged over last 500 steps = 3.2626e-01, PNorm = 97.4998, GNorm = 0.3863
Meta loss on this task batch = 3.2806e-01, Meta loss averaged over last 500 steps = 3.2631e-01, PNorm = 97.5033, GNorm = 0.3021
Meta loss on this task batch = 3.3735e-01, Meta loss averaged over last 500 steps = 3.2625e-01, PNorm = 97.5074, GNorm = 0.2609
Meta loss on this task batch = 3.3017e-01, Meta loss averaged over last 500 steps = 3.2627e-01, PNorm = 97.5134, GNorm = 0.2708
Meta loss on this task batch = 2.9952e-01, Meta loss averaged over last 500 steps = 3.2617e-01, PNorm = 97.5190, GNorm = 0.2483
Meta loss on this task batch = 3.2819e-01, Meta loss averaged over last 500 steps = 3.2615e-01, PNorm = 97.5228, GNorm = 0.2946
Meta loss on this task batch = 3.4089e-01, Meta loss averaged over last 500 steps = 3.2617e-01, PNorm = 97.5256, GNorm = 0.3127
Meta loss on this task batch = 3.0431e-01, Meta loss averaged over last 500 steps = 3.2620e-01, PNorm = 97.5312, GNorm = 0.2651
Meta loss on this task batch = 3.4375e-01, Meta loss averaged over last 500 steps = 3.2631e-01, PNorm = 97.5381, GNorm = 0.2725
Meta loss on this task batch = 3.3150e-01, Meta loss averaged over last 500 steps = 3.2634e-01, PNorm = 97.5465, GNorm = 0.2736
Meta loss on this task batch = 2.9390e-01, Meta loss averaged over last 500 steps = 3.2626e-01, PNorm = 97.5546, GNorm = 0.2942
Meta loss on this task batch = 3.0591e-01, Meta loss averaged over last 500 steps = 3.2612e-01, PNorm = 97.5648, GNorm = 0.3140
Meta loss on this task batch = 3.5487e-01, Meta loss averaged over last 500 steps = 3.2608e-01, PNorm = 97.5735, GNorm = 0.2860
Meta loss on this task batch = 3.3176e-01, Meta loss averaged over last 500 steps = 3.2605e-01, PNorm = 97.5799, GNorm = 0.2715
Meta loss on this task batch = 3.5078e-01, Meta loss averaged over last 500 steps = 3.2616e-01, PNorm = 97.5870, GNorm = 0.3471
Took 132.63206553459167 seconds to complete one epoch of meta training
Took 141.39336609840393 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487349
Epoch 262
Meta loss on this task batch = 2.9571e-01, Meta loss averaged over last 500 steps = 3.2603e-01, PNorm = 97.5933, GNorm = 0.2490
Meta loss on this task batch = 3.7004e-01, Meta loss averaged over last 500 steps = 3.2611e-01, PNorm = 97.5993, GNorm = 0.3233
Meta loss on this task batch = 3.1152e-01, Meta loss averaged over last 500 steps = 3.2605e-01, PNorm = 97.6055, GNorm = 0.2913
Meta loss on this task batch = 3.4312e-01, Meta loss averaged over last 500 steps = 3.2618e-01, PNorm = 97.6131, GNorm = 0.2746
Meta loss on this task batch = 3.3424e-01, Meta loss averaged over last 500 steps = 3.2619e-01, PNorm = 97.6212, GNorm = 0.2610
Meta loss on this task batch = 3.0733e-01, Meta loss averaged over last 500 steps = 3.2618e-01, PNorm = 97.6314, GNorm = 0.2549
Meta loss on this task batch = 3.0975e-01, Meta loss averaged over last 500 steps = 3.2620e-01, PNorm = 97.6427, GNorm = 0.2729
Meta loss on this task batch = 3.3029e-01, Meta loss averaged over last 500 steps = 3.2610e-01, PNorm = 97.6550, GNorm = 0.2607
Meta loss on this task batch = 3.3348e-01, Meta loss averaged over last 500 steps = 3.2607e-01, PNorm = 97.6678, GNorm = 0.2678
Meta loss on this task batch = 3.2214e-01, Meta loss averaged over last 500 steps = 3.2608e-01, PNorm = 97.6794, GNorm = 0.2832
Meta loss on this task batch = 2.9880e-01, Meta loss averaged over last 500 steps = 3.2599e-01, PNorm = 97.6909, GNorm = 0.2831
Meta loss on this task batch = 3.6598e-01, Meta loss averaged over last 500 steps = 3.2603e-01, PNorm = 97.6990, GNorm = 0.3130
Meta loss on this task batch = 3.1445e-01, Meta loss averaged over last 500 steps = 3.2591e-01, PNorm = 97.7049, GNorm = 0.2980
Meta loss on this task batch = 3.0423e-01, Meta loss averaged over last 500 steps = 3.2587e-01, PNorm = 97.7092, GNorm = 0.2698
Meta loss on this task batch = 3.3883e-01, Meta loss averaged over last 500 steps = 3.2580e-01, PNorm = 97.7121, GNorm = 0.2901
Meta loss on this task batch = 3.5733e-01, Meta loss averaged over last 500 steps = 3.2577e-01, PNorm = 97.7155, GNorm = 0.3283
Meta loss on this task batch = 3.3498e-01, Meta loss averaged over last 500 steps = 3.2578e-01, PNorm = 97.7242, GNorm = 0.3718
Meta loss on this task batch = 3.5918e-01, Meta loss averaged over last 500 steps = 3.2589e-01, PNorm = 97.7349, GNorm = 0.3260
Meta loss on this task batch = 2.6597e-01, Meta loss averaged over last 500 steps = 3.2580e-01, PNorm = 97.7490, GNorm = 0.3064
Took 135.80799889564514 seconds to complete one epoch of meta training
Took 143.26041078567505 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508700
Epoch 263
Meta loss on this task batch = 3.8832e-01, Meta loss averaged over last 500 steps = 3.2606e-01, PNorm = 97.7628, GNorm = 0.2913
Meta loss on this task batch = 3.2664e-01, Meta loss averaged over last 500 steps = 3.2601e-01, PNorm = 97.7777, GNorm = 0.2642
Meta loss on this task batch = 3.4426e-01, Meta loss averaged over last 500 steps = 3.2607e-01, PNorm = 97.7904, GNorm = 0.3126
Meta loss on this task batch = 3.3631e-01, Meta loss averaged over last 500 steps = 3.2611e-01, PNorm = 97.8025, GNorm = 0.2941
Meta loss on this task batch = 2.9437e-01, Meta loss averaged over last 500 steps = 3.2599e-01, PNorm = 97.8143, GNorm = 0.2395
Meta loss on this task batch = 3.5181e-01, Meta loss averaged over last 500 steps = 3.2609e-01, PNorm = 97.8235, GNorm = 0.2938
Meta loss on this task batch = 2.6893e-01, Meta loss averaged over last 500 steps = 3.2595e-01, PNorm = 97.8302, GNorm = 0.2432
Meta loss on this task batch = 2.8624e-01, Meta loss averaged over last 500 steps = 3.2588e-01, PNorm = 97.8359, GNorm = 0.2438
Meta loss on this task batch = 3.2522e-01, Meta loss averaged over last 500 steps = 3.2584e-01, PNorm = 97.8394, GNorm = 0.2958
Meta loss on this task batch = 3.1379e-01, Meta loss averaged over last 500 steps = 3.2590e-01, PNorm = 97.8422, GNorm = 0.2925
Meta loss on this task batch = 3.3904e-01, Meta loss averaged over last 500 steps = 3.2599e-01, PNorm = 97.8440, GNorm = 0.2776
Meta loss on this task batch = 3.8296e-01, Meta loss averaged over last 500 steps = 3.2606e-01, PNorm = 97.8461, GNorm = 0.3039
Meta loss on this task batch = 2.9281e-01, Meta loss averaged over last 500 steps = 3.2605e-01, PNorm = 97.8490, GNorm = 0.2636
Meta loss on this task batch = 2.8582e-01, Meta loss averaged over last 500 steps = 3.2601e-01, PNorm = 97.8569, GNorm = 0.3726
Meta loss on this task batch = 2.8687e-01, Meta loss averaged over last 500 steps = 3.2593e-01, PNorm = 97.8651, GNorm = 0.2802
Meta loss on this task batch = 3.2542e-01, Meta loss averaged over last 500 steps = 3.2597e-01, PNorm = 97.8735, GNorm = 0.3059
Meta loss on this task batch = 3.2703e-01, Meta loss averaged over last 500 steps = 3.2598e-01, PNorm = 97.8830, GNorm = 0.2751
Meta loss on this task batch = 3.3758e-01, Meta loss averaged over last 500 steps = 3.2605e-01, PNorm = 97.8915, GNorm = 0.2984
Meta loss on this task batch = 3.5149e-01, Meta loss averaged over last 500 steps = 3.2609e-01, PNorm = 97.8980, GNorm = 0.3410
Took 134.15886974334717 seconds to complete one epoch of meta training
Took 142.81202101707458 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481354
Epoch 264
Meta loss on this task batch = 3.0646e-01, Meta loss averaged over last 500 steps = 3.2608e-01, PNorm = 97.9044, GNorm = 0.2970
Meta loss on this task batch = 3.1539e-01, Meta loss averaged over last 500 steps = 3.2616e-01, PNorm = 97.9085, GNorm = 0.2943
Meta loss on this task batch = 3.0455e-01, Meta loss averaged over last 500 steps = 3.2607e-01, PNorm = 97.9143, GNorm = 0.3147
Meta loss on this task batch = 3.3483e-01, Meta loss averaged over last 500 steps = 3.2595e-01, PNorm = 97.9195, GNorm = 0.3483
Meta loss on this task batch = 3.2783e-01, Meta loss averaged over last 500 steps = 3.2594e-01, PNorm = 97.9264, GNorm = 0.2837
Meta loss on this task batch = 3.0673e-01, Meta loss averaged over last 500 steps = 3.2592e-01, PNorm = 97.9331, GNorm = 0.2957
Meta loss on this task batch = 3.3368e-01, Meta loss averaged over last 500 steps = 3.2597e-01, PNorm = 97.9400, GNorm = 0.3138
Meta loss on this task batch = 3.2362e-01, Meta loss averaged over last 500 steps = 3.2593e-01, PNorm = 97.9453, GNorm = 0.2950
Meta loss on this task batch = 3.5515e-01, Meta loss averaged over last 500 steps = 3.2590e-01, PNorm = 97.9502, GNorm = 0.2983
Meta loss on this task batch = 2.8717e-01, Meta loss averaged over last 500 steps = 3.2589e-01, PNorm = 97.9534, GNorm = 0.2812
Meta loss on this task batch = 3.1723e-01, Meta loss averaged over last 500 steps = 3.2584e-01, PNorm = 97.9561, GNorm = 0.2964
Meta loss on this task batch = 3.5203e-01, Meta loss averaged over last 500 steps = 3.2588e-01, PNorm = 97.9594, GNorm = 0.2796
Meta loss on this task batch = 3.6348e-01, Meta loss averaged over last 500 steps = 3.2595e-01, PNorm = 97.9611, GNorm = 0.3018
Meta loss on this task batch = 3.0506e-01, Meta loss averaged over last 500 steps = 3.2592e-01, PNorm = 97.9637, GNorm = 0.2573
Meta loss on this task batch = 3.0508e-01, Meta loss averaged over last 500 steps = 3.2590e-01, PNorm = 97.9651, GNorm = 0.3079
Meta loss on this task batch = 3.3456e-01, Meta loss averaged over last 500 steps = 3.2591e-01, PNorm = 97.9664, GNorm = 0.3038
Meta loss on this task batch = 3.2591e-01, Meta loss averaged over last 500 steps = 3.2590e-01, PNorm = 97.9672, GNorm = 0.2740
Meta loss on this task batch = 2.9744e-01, Meta loss averaged over last 500 steps = 3.2586e-01, PNorm = 97.9679, GNorm = 0.2734
Meta loss on this task batch = 3.1847e-01, Meta loss averaged over last 500 steps = 3.2586e-01, PNorm = 97.9686, GNorm = 0.3671
Took 133.9970726966858 seconds to complete one epoch of meta training
Took 142.54549980163574 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479541
Epoch 265
Meta loss on this task batch = 2.7473e-01, Meta loss averaged over last 500 steps = 3.2563e-01, PNorm = 97.9734, GNorm = 0.2700
Meta loss on this task batch = 3.1461e-01, Meta loss averaged over last 500 steps = 3.2560e-01, PNorm = 97.9786, GNorm = 0.2661
Meta loss on this task batch = 3.2650e-01, Meta loss averaged over last 500 steps = 3.2575e-01, PNorm = 97.9837, GNorm = 0.2617
Meta loss on this task batch = 3.5087e-01, Meta loss averaged over last 500 steps = 3.2575e-01, PNorm = 97.9905, GNorm = 0.2781
Meta loss on this task batch = 3.3013e-01, Meta loss averaged over last 500 steps = 3.2569e-01, PNorm = 97.9986, GNorm = 0.2598
Meta loss on this task batch = 3.1891e-01, Meta loss averaged over last 500 steps = 3.2564e-01, PNorm = 98.0070, GNorm = 0.2649
Meta loss on this task batch = 2.9198e-01, Meta loss averaged over last 500 steps = 3.2564e-01, PNorm = 98.0117, GNorm = 0.2959
Meta loss on this task batch = 3.4666e-01, Meta loss averaged over last 500 steps = 3.2563e-01, PNorm = 98.0166, GNorm = 0.3105
Meta loss on this task batch = 3.4495e-01, Meta loss averaged over last 500 steps = 3.2563e-01, PNorm = 98.0209, GNorm = 0.2894
Meta loss on this task batch = 2.8446e-01, Meta loss averaged over last 500 steps = 3.2556e-01, PNorm = 98.0271, GNorm = 0.2900
Meta loss on this task batch = 3.2097e-01, Meta loss averaged over last 500 steps = 3.2558e-01, PNorm = 98.0285, GNorm = 0.2960
Meta loss on this task batch = 3.5420e-01, Meta loss averaged over last 500 steps = 3.2559e-01, PNorm = 98.0285, GNorm = 0.3522
Meta loss on this task batch = 2.9569e-01, Meta loss averaged over last 500 steps = 3.2550e-01, PNorm = 98.0278, GNorm = 0.2694
Meta loss on this task batch = 2.7378e-01, Meta loss averaged over last 500 steps = 3.2541e-01, PNorm = 98.0300, GNorm = 0.2985
Meta loss on this task batch = 2.7865e-01, Meta loss averaged over last 500 steps = 3.2521e-01, PNorm = 98.0350, GNorm = 0.2999
Meta loss on this task batch = 3.1920e-01, Meta loss averaged over last 500 steps = 3.2530e-01, PNorm = 98.0400, GNorm = 0.3103
Meta loss on this task batch = 3.5507e-01, Meta loss averaged over last 500 steps = 3.2533e-01, PNorm = 98.0437, GNorm = 0.3161
Meta loss on this task batch = 2.6915e-01, Meta loss averaged over last 500 steps = 3.2525e-01, PNorm = 98.0474, GNorm = 0.2920
Meta loss on this task batch = 3.6321e-01, Meta loss averaged over last 500 steps = 3.2527e-01, PNorm = 98.0539, GNorm = 0.4462
Took 136.71850872039795 seconds to complete one epoch of meta training
Took 145.5668716430664 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506589
Epoch 266
Meta loss on this task batch = 3.3872e-01, Meta loss averaged over last 500 steps = 3.2529e-01, PNorm = 98.0620, GNorm = 0.2716
Meta loss on this task batch = 3.2045e-01, Meta loss averaged over last 500 steps = 3.2528e-01, PNorm = 98.0707, GNorm = 0.2938
Meta loss on this task batch = 3.8898e-01, Meta loss averaged over last 500 steps = 3.2552e-01, PNorm = 98.0789, GNorm = 0.3592
Meta loss on this task batch = 3.1801e-01, Meta loss averaged over last 500 steps = 3.2537e-01, PNorm = 98.0849, GNorm = 0.2805
Meta loss on this task batch = 3.6780e-01, Meta loss averaged over last 500 steps = 3.2547e-01, PNorm = 98.0893, GNorm = 0.2827
Meta loss on this task batch = 2.7739e-01, Meta loss averaged over last 500 steps = 3.2539e-01, PNorm = 98.0943, GNorm = 0.2462
Meta loss on this task batch = 2.7949e-01, Meta loss averaged over last 500 steps = 3.2532e-01, PNorm = 98.1003, GNorm = 0.2487
Meta loss on this task batch = 3.1300e-01, Meta loss averaged over last 500 steps = 3.2526e-01, PNorm = 98.1073, GNorm = 0.2913
Meta loss on this task batch = 3.0286e-01, Meta loss averaged over last 500 steps = 3.2511e-01, PNorm = 98.1143, GNorm = 0.2852
Meta loss on this task batch = 3.3661e-01, Meta loss averaged over last 500 steps = 3.2514e-01, PNorm = 98.1200, GNorm = 0.2962
Meta loss on this task batch = 3.1950e-01, Meta loss averaged over last 500 steps = 3.2510e-01, PNorm = 98.1236, GNorm = 0.3023
Meta loss on this task batch = 2.5297e-01, Meta loss averaged over last 500 steps = 3.2492e-01, PNorm = 98.1268, GNorm = 0.2896
Meta loss on this task batch = 3.0471e-01, Meta loss averaged over last 500 steps = 3.2488e-01, PNorm = 98.1301, GNorm = 0.2725
Meta loss on this task batch = 3.4103e-01, Meta loss averaged over last 500 steps = 3.2485e-01, PNorm = 98.1337, GNorm = 0.2807
Meta loss on this task batch = 3.2426e-01, Meta loss averaged over last 500 steps = 3.2479e-01, PNorm = 98.1373, GNorm = 0.2546
Meta loss on this task batch = 3.4422e-01, Meta loss averaged over last 500 steps = 3.2478e-01, PNorm = 98.1430, GNorm = 0.2679
Meta loss on this task batch = 3.1747e-01, Meta loss averaged over last 500 steps = 3.2480e-01, PNorm = 98.1514, GNorm = 0.3010
Meta loss on this task batch = 2.7791e-01, Meta loss averaged over last 500 steps = 3.2475e-01, PNorm = 98.1611, GNorm = 0.3025
Meta loss on this task batch = 3.1299e-01, Meta loss averaged over last 500 steps = 3.2467e-01, PNorm = 98.1711, GNorm = 0.3637
Took 137.4154703617096 seconds to complete one epoch of meta training
Took 146.17272925376892 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496743
Epoch 267
Meta loss on this task batch = 3.5964e-01, Meta loss averaged over last 500 steps = 3.2455e-01, PNorm = 98.1795, GNorm = 0.3109
Meta loss on this task batch = 3.2562e-01, Meta loss averaged over last 500 steps = 3.2465e-01, PNorm = 98.1894, GNorm = 0.2712
Meta loss on this task batch = 3.1403e-01, Meta loss averaged over last 500 steps = 3.2464e-01, PNorm = 98.1998, GNorm = 0.2766
Meta loss on this task batch = 3.1010e-01, Meta loss averaged over last 500 steps = 3.2468e-01, PNorm = 98.2100, GNorm = 0.2777
Meta loss on this task batch = 3.2141e-01, Meta loss averaged over last 500 steps = 3.2466e-01, PNorm = 98.2201, GNorm = 0.3155
Meta loss on this task batch = 3.4811e-01, Meta loss averaged over last 500 steps = 3.2465e-01, PNorm = 98.2274, GNorm = 0.3255
Meta loss on this task batch = 3.1728e-01, Meta loss averaged over last 500 steps = 3.2469e-01, PNorm = 98.2343, GNorm = 0.2696
Meta loss on this task batch = 3.0492e-01, Meta loss averaged over last 500 steps = 3.2466e-01, PNorm = 98.2388, GNorm = 0.3009
Meta loss on this task batch = 3.5062e-01, Meta loss averaged over last 500 steps = 3.2481e-01, PNorm = 98.2430, GNorm = 0.2975
Meta loss on this task batch = 2.7865e-01, Meta loss averaged over last 500 steps = 3.2460e-01, PNorm = 98.2456, GNorm = 0.2634
Meta loss on this task batch = 2.8337e-01, Meta loss averaged over last 500 steps = 3.2446e-01, PNorm = 98.2485, GNorm = 0.2509
Meta loss on this task batch = 3.3265e-01, Meta loss averaged over last 500 steps = 3.2440e-01, PNorm = 98.2491, GNorm = 0.2845
Meta loss on this task batch = 2.8337e-01, Meta loss averaged over last 500 steps = 3.2427e-01, PNorm = 98.2518, GNorm = 0.2760
Meta loss on this task batch = 3.3650e-01, Meta loss averaged over last 500 steps = 3.2432e-01, PNorm = 98.2555, GNorm = 0.2998
Meta loss on this task batch = 3.5707e-01, Meta loss averaged over last 500 steps = 3.2438e-01, PNorm = 98.2594, GNorm = 0.3301
Meta loss on this task batch = 3.7600e-01, Meta loss averaged over last 500 steps = 3.2449e-01, PNorm = 98.2649, GNorm = 0.3083
Meta loss on this task batch = 2.9858e-01, Meta loss averaged over last 500 steps = 3.2439e-01, PNorm = 98.2706, GNorm = 0.2735
Meta loss on this task batch = 2.9461e-01, Meta loss averaged over last 500 steps = 3.2434e-01, PNorm = 98.2803, GNorm = 0.3115
Meta loss on this task batch = 2.7984e-01, Meta loss averaged over last 500 steps = 3.2414e-01, PNorm = 98.2905, GNorm = 0.3136
Took 133.25544548034668 seconds to complete one epoch of meta training
Took 141.8017692565918 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488984
Epoch 268
Meta loss on this task batch = 2.9541e-01, Meta loss averaged over last 500 steps = 3.2409e-01, PNorm = 98.3009, GNorm = 0.2705
Meta loss on this task batch = 3.5060e-01, Meta loss averaged over last 500 steps = 3.2406e-01, PNorm = 98.3125, GNorm = 0.2659
Meta loss on this task batch = 2.8045e-01, Meta loss averaged over last 500 steps = 3.2406e-01, PNorm = 98.3231, GNorm = 0.2405
Meta loss on this task batch = 3.1551e-01, Meta loss averaged over last 500 steps = 3.2409e-01, PNorm = 98.3311, GNorm = 0.2673
Meta loss on this task batch = 3.2509e-01, Meta loss averaged over last 500 steps = 3.2411e-01, PNorm = 98.3374, GNorm = 0.3436
Meta loss on this task batch = 2.8857e-01, Meta loss averaged over last 500 steps = 3.2413e-01, PNorm = 98.3432, GNorm = 0.2771
Meta loss on this task batch = 2.5985e-01, Meta loss averaged over last 500 steps = 3.2398e-01, PNorm = 98.3471, GNorm = 0.2652
Meta loss on this task batch = 3.3319e-01, Meta loss averaged over last 500 steps = 3.2402e-01, PNorm = 98.3480, GNorm = 0.3344
Meta loss on this task batch = 3.6558e-01, Meta loss averaged over last 500 steps = 3.2417e-01, PNorm = 98.3474, GNorm = 0.3460
Meta loss on this task batch = 3.3769e-01, Meta loss averaged over last 500 steps = 3.2412e-01, PNorm = 98.3447, GNorm = 0.3282
Meta loss on this task batch = 2.9553e-01, Meta loss averaged over last 500 steps = 3.2422e-01, PNorm = 98.3430, GNorm = 0.2963
Meta loss on this task batch = 3.2619e-01, Meta loss averaged over last 500 steps = 3.2423e-01, PNorm = 98.3430, GNorm = 0.2667
Meta loss on this task batch = 3.1811e-01, Meta loss averaged over last 500 steps = 3.2407e-01, PNorm = 98.3459, GNorm = 0.2812
Meta loss on this task batch = 3.1429e-01, Meta loss averaged over last 500 steps = 3.2415e-01, PNorm = 98.3507, GNorm = 0.2853
Meta loss on this task batch = 3.3398e-01, Meta loss averaged over last 500 steps = 3.2417e-01, PNorm = 98.3584, GNorm = 0.3129
Meta loss on this task batch = 3.6755e-01, Meta loss averaged over last 500 steps = 3.2420e-01, PNorm = 98.3673, GNorm = 0.3097
Meta loss on this task batch = 2.6207e-01, Meta loss averaged over last 500 steps = 3.2402e-01, PNorm = 98.3756, GNorm = 0.2456
Meta loss on this task batch = 3.5053e-01, Meta loss averaged over last 500 steps = 3.2399e-01, PNorm = 98.3833, GNorm = 0.3064
Meta loss on this task batch = 3.8855e-01, Meta loss averaged over last 500 steps = 3.2413e-01, PNorm = 98.3896, GNorm = 0.3374
Took 130.24561834335327 seconds to complete one epoch of meta training
Took 138.77998399734497 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472782
Epoch 269
Meta loss on this task batch = 2.8706e-01, Meta loss averaged over last 500 steps = 3.2399e-01, PNorm = 98.3987, GNorm = 0.2510
Meta loss on this task batch = 4.0105e-01, Meta loss averaged over last 500 steps = 3.2418e-01, PNorm = 98.4078, GNorm = 0.3464
Meta loss on this task batch = 3.5937e-01, Meta loss averaged over last 500 steps = 3.2433e-01, PNorm = 98.4163, GNorm = 0.2844
Meta loss on this task batch = 2.7925e-01, Meta loss averaged over last 500 steps = 3.2413e-01, PNorm = 98.4248, GNorm = 0.2485
Meta loss on this task batch = 2.9824e-01, Meta loss averaged over last 500 steps = 3.2403e-01, PNorm = 98.4333, GNorm = 0.2351
Meta loss on this task batch = 2.9492e-01, Meta loss averaged over last 500 steps = 3.2396e-01, PNorm = 98.4440, GNorm = 0.3041
Meta loss on this task batch = 3.5691e-01, Meta loss averaged over last 500 steps = 3.2398e-01, PNorm = 98.4545, GNorm = 0.3310
Meta loss on this task batch = 3.4368e-01, Meta loss averaged over last 500 steps = 3.2401e-01, PNorm = 98.4625, GNorm = 0.2617
Meta loss on this task batch = 3.6358e-01, Meta loss averaged over last 500 steps = 3.2413e-01, PNorm = 98.4684, GNorm = 0.3105
Meta loss on this task batch = 2.9267e-01, Meta loss averaged over last 500 steps = 3.2402e-01, PNorm = 98.4729, GNorm = 0.3060
Meta loss on this task batch = 3.6771e-01, Meta loss averaged over last 500 steps = 3.2410e-01, PNorm = 98.4757, GNorm = 0.3235
Meta loss on this task batch = 2.9738e-01, Meta loss averaged over last 500 steps = 3.2405e-01, PNorm = 98.4736, GNorm = 0.3629
Meta loss on this task batch = 2.4901e-01, Meta loss averaged over last 500 steps = 3.2399e-01, PNorm = 98.4707, GNorm = 0.2469
Meta loss on this task batch = 3.1986e-01, Meta loss averaged over last 500 steps = 3.2396e-01, PNorm = 98.4676, GNorm = 0.3048
Meta loss on this task batch = 3.4846e-01, Meta loss averaged over last 500 steps = 3.2406e-01, PNorm = 98.4666, GNorm = 0.2789
Meta loss on this task batch = 3.4899e-01, Meta loss averaged over last 500 steps = 3.2415e-01, PNorm = 98.4682, GNorm = 0.2501
Meta loss on this task batch = 2.7879e-01, Meta loss averaged over last 500 steps = 3.2406e-01, PNorm = 98.4739, GNorm = 0.2848
Meta loss on this task batch = 3.3467e-01, Meta loss averaged over last 500 steps = 3.2409e-01, PNorm = 98.4836, GNorm = 0.2757
Meta loss on this task batch = 3.3650e-01, Meta loss averaged over last 500 steps = 3.2409e-01, PNorm = 98.4936, GNorm = 0.3349
Took 130.81389355659485 seconds to complete one epoch of meta training
Took 139.01585364341736 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476050
Epoch 270
Meta loss on this task batch = 2.9659e-01, Meta loss averaged over last 500 steps = 3.2396e-01, PNorm = 98.5049, GNorm = 0.2531
Meta loss on this task batch = 3.3487e-01, Meta loss averaged over last 500 steps = 3.2399e-01, PNorm = 98.5169, GNorm = 0.2589
Meta loss on this task batch = 3.4049e-01, Meta loss averaged over last 500 steps = 3.2400e-01, PNorm = 98.5248, GNorm = 0.3146
Meta loss on this task batch = 3.2756e-01, Meta loss averaged over last 500 steps = 3.2397e-01, PNorm = 98.5287, GNorm = 0.3036
Meta loss on this task batch = 2.7164e-01, Meta loss averaged over last 500 steps = 3.2390e-01, PNorm = 98.5337, GNorm = 0.2569
Meta loss on this task batch = 3.5327e-01, Meta loss averaged over last 500 steps = 3.2396e-01, PNorm = 98.5360, GNorm = 0.2881
Meta loss on this task batch = 3.1625e-01, Meta loss averaged over last 500 steps = 3.2389e-01, PNorm = 98.5384, GNorm = 0.2477
Meta loss on this task batch = 3.0457e-01, Meta loss averaged over last 500 steps = 3.2381e-01, PNorm = 98.5421, GNorm = 0.2634
Meta loss on this task batch = 3.0439e-01, Meta loss averaged over last 500 steps = 3.2375e-01, PNorm = 98.5480, GNorm = 0.2584
Meta loss on this task batch = 3.0240e-01, Meta loss averaged over last 500 steps = 3.2369e-01, PNorm = 98.5538, GNorm = 0.2838
Meta loss on this task batch = 3.4208e-01, Meta loss averaged over last 500 steps = 3.2375e-01, PNorm = 98.5604, GNorm = 0.2786
Meta loss on this task batch = 3.0235e-01, Meta loss averaged over last 500 steps = 3.2372e-01, PNorm = 98.5666, GNorm = 0.2541
Meta loss on this task batch = 3.4425e-01, Meta loss averaged over last 500 steps = 3.2382e-01, PNorm = 98.5717, GNorm = 0.2916
Meta loss on this task batch = 2.6936e-01, Meta loss averaged over last 500 steps = 3.2379e-01, PNorm = 98.5770, GNorm = 0.2542
Meta loss on this task batch = 2.9061e-01, Meta loss averaged over last 500 steps = 3.2374e-01, PNorm = 98.5823, GNorm = 0.2780
Meta loss on this task batch = 3.5469e-01, Meta loss averaged over last 500 steps = 3.2372e-01, PNorm = 98.5862, GNorm = 0.2995
Meta loss on this task batch = 3.1146e-01, Meta loss averaged over last 500 steps = 3.2370e-01, PNorm = 98.5906, GNorm = 0.2748
Meta loss on this task batch = 3.2979e-01, Meta loss averaged over last 500 steps = 3.2377e-01, PNorm = 98.5970, GNorm = 0.2948
Meta loss on this task batch = 2.9745e-01, Meta loss averaged over last 500 steps = 3.2366e-01, PNorm = 98.6050, GNorm = 0.3371
Took 132.85139894485474 seconds to complete one epoch of meta training
Took 141.64575457572937 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498355
Epoch 271
Meta loss on this task batch = 2.9668e-01, Meta loss averaged over last 500 steps = 3.2360e-01, PNorm = 98.6133, GNorm = 0.2878
Meta loss on this task batch = 3.1617e-01, Meta loss averaged over last 500 steps = 3.2358e-01, PNorm = 98.6225, GNorm = 0.2692
Meta loss on this task batch = 2.6246e-01, Meta loss averaged over last 500 steps = 3.2331e-01, PNorm = 98.6333, GNorm = 0.2771
Meta loss on this task batch = 3.0718e-01, Meta loss averaged over last 500 steps = 3.2329e-01, PNorm = 98.6426, GNorm = 0.3061
Meta loss on this task batch = 3.4753e-01, Meta loss averaged over last 500 steps = 3.2334e-01, PNorm = 98.6516, GNorm = 0.3219
Meta loss on this task batch = 2.6748e-01, Meta loss averaged over last 500 steps = 3.2323e-01, PNorm = 98.6600, GNorm = 0.3039
Meta loss on this task batch = 3.7102e-01, Meta loss averaged over last 500 steps = 3.2327e-01, PNorm = 98.6662, GNorm = 0.2920
Meta loss on this task batch = 3.2571e-01, Meta loss averaged over last 500 steps = 3.2330e-01, PNorm = 98.6700, GNorm = 0.3520
Meta loss on this task batch = 3.4266e-01, Meta loss averaged over last 500 steps = 3.2339e-01, PNorm = 98.6713, GNorm = 0.3504
Meta loss on this task batch = 3.3468e-01, Meta loss averaged over last 500 steps = 3.2338e-01, PNorm = 98.6722, GNorm = 0.3056
Meta loss on this task batch = 3.6065e-01, Meta loss averaged over last 500 steps = 3.2350e-01, PNorm = 98.6730, GNorm = 0.3164
Meta loss on this task batch = 3.3963e-01, Meta loss averaged over last 500 steps = 3.2358e-01, PNorm = 98.6740, GNorm = 0.3308
Meta loss on this task batch = 3.0577e-01, Meta loss averaged over last 500 steps = 3.2344e-01, PNorm = 98.6748, GNorm = 0.2919
Meta loss on this task batch = 3.2059e-01, Meta loss averaged over last 500 steps = 3.2328e-01, PNorm = 98.6755, GNorm = 0.2766
Meta loss on this task batch = 3.1176e-01, Meta loss averaged over last 500 steps = 3.2330e-01, PNorm = 98.6778, GNorm = 0.2475
Meta loss on this task batch = 3.2023e-01, Meta loss averaged over last 500 steps = 3.2329e-01, PNorm = 98.6809, GNorm = 0.2823
Meta loss on this task batch = 3.3016e-01, Meta loss averaged over last 500 steps = 3.2339e-01, PNorm = 98.6842, GNorm = 0.2760
Meta loss on this task batch = 2.9603e-01, Meta loss averaged over last 500 steps = 3.2329e-01, PNorm = 98.6892, GNorm = 0.2362
Meta loss on this task batch = 2.9992e-01, Meta loss averaged over last 500 steps = 3.2326e-01, PNorm = 98.6970, GNorm = 0.3159
Took 132.50999474525452 seconds to complete one epoch of meta training
Took 140.82341861724854 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487748
Epoch 272
Meta loss on this task batch = 3.0625e-01, Meta loss averaged over last 500 steps = 3.2316e-01, PNorm = 98.7048, GNorm = 0.2651
Meta loss on this task batch = 3.0493e-01, Meta loss averaged over last 500 steps = 3.2304e-01, PNorm = 98.7144, GNorm = 0.2794
Meta loss on this task batch = 2.9968e-01, Meta loss averaged over last 500 steps = 3.2300e-01, PNorm = 98.7256, GNorm = 0.2911
Meta loss on this task batch = 3.4741e-01, Meta loss averaged over last 500 steps = 3.2299e-01, PNorm = 98.7350, GNorm = 0.2823
Meta loss on this task batch = 3.2790e-01, Meta loss averaged over last 500 steps = 3.2294e-01, PNorm = 98.7433, GNorm = 0.2589
Meta loss on this task batch = 2.7923e-01, Meta loss averaged over last 500 steps = 3.2296e-01, PNorm = 98.7510, GNorm = 0.2279
Meta loss on this task batch = 3.7419e-01, Meta loss averaged over last 500 steps = 3.2310e-01, PNorm = 98.7580, GNorm = 0.3011
Meta loss on this task batch = 3.3050e-01, Meta loss averaged over last 500 steps = 3.2316e-01, PNorm = 98.7627, GNorm = 0.2738
Meta loss on this task batch = 2.7005e-01, Meta loss averaged over last 500 steps = 3.2305e-01, PNorm = 98.7676, GNorm = 0.2501
Meta loss on this task batch = 2.8459e-01, Meta loss averaged over last 500 steps = 3.2306e-01, PNorm = 98.7717, GNorm = 0.2846
Meta loss on this task batch = 3.3457e-01, Meta loss averaged over last 500 steps = 3.2303e-01, PNorm = 98.7779, GNorm = 0.3081
Meta loss on this task batch = 3.0301e-01, Meta loss averaged over last 500 steps = 3.2292e-01, PNorm = 98.7851, GNorm = 0.3231
Meta loss on this task batch = 4.0164e-01, Meta loss averaged over last 500 steps = 3.2306e-01, PNorm = 98.7921, GNorm = 0.3365
Meta loss on this task batch = 3.1308e-01, Meta loss averaged over last 500 steps = 3.2309e-01, PNorm = 98.8009, GNorm = 0.2720
Meta loss on this task batch = 3.4986e-01, Meta loss averaged over last 500 steps = 3.2316e-01, PNorm = 98.8078, GNorm = 0.3184
Meta loss on this task batch = 2.7370e-01, Meta loss averaged over last 500 steps = 3.2309e-01, PNorm = 98.8148, GNorm = 0.2519
Meta loss on this task batch = 3.0220e-01, Meta loss averaged over last 500 steps = 3.2313e-01, PNorm = 98.8206, GNorm = 0.2462
Meta loss on this task batch = 2.8911e-01, Meta loss averaged over last 500 steps = 3.2308e-01, PNorm = 98.8266, GNorm = 0.2547
Meta loss on this task batch = 2.8628e-01, Meta loss averaged over last 500 steps = 3.2300e-01, PNorm = 98.8356, GNorm = 0.3283
Took 132.99674153327942 seconds to complete one epoch of meta training
Took 141.71654391288757 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481803
Epoch 273
Meta loss on this task batch = 2.7298e-01, Meta loss averaged over last 500 steps = 3.2290e-01, PNorm = 98.8454, GNorm = 0.2512
Meta loss on this task batch = 2.8466e-01, Meta loss averaged over last 500 steps = 3.2278e-01, PNorm = 98.8533, GNorm = 0.2554
Meta loss on this task batch = 3.5595e-01, Meta loss averaged over last 500 steps = 3.2275e-01, PNorm = 98.8580, GNorm = 0.3422
Meta loss on this task batch = 2.9265e-01, Meta loss averaged over last 500 steps = 3.2264e-01, PNorm = 98.8605, GNorm = 0.2724
Meta loss on this task batch = 3.1108e-01, Meta loss averaged over last 500 steps = 3.2263e-01, PNorm = 98.8627, GNorm = 0.2526
Meta loss on this task batch = 2.9580e-01, Meta loss averaged over last 500 steps = 3.2247e-01, PNorm = 98.8658, GNorm = 0.2671
Meta loss on this task batch = 2.9976e-01, Meta loss averaged over last 500 steps = 3.2242e-01, PNorm = 98.8681, GNorm = 0.2590
Meta loss on this task batch = 3.5428e-01, Meta loss averaged over last 500 steps = 3.2246e-01, PNorm = 98.8688, GNorm = 0.3036
Meta loss on this task batch = 3.2045e-01, Meta loss averaged over last 500 steps = 3.2239e-01, PNorm = 98.8711, GNorm = 0.2540
Meta loss on this task batch = 3.3328e-01, Meta loss averaged over last 500 steps = 3.2232e-01, PNorm = 98.8748, GNorm = 0.3087
Meta loss on this task batch = 3.7156e-01, Meta loss averaged over last 500 steps = 3.2256e-01, PNorm = 98.8777, GNorm = 0.3096
Meta loss on this task batch = 3.2533e-01, Meta loss averaged over last 500 steps = 3.2259e-01, PNorm = 98.8819, GNorm = 0.2943
Meta loss on this task batch = 2.6144e-01, Meta loss averaged over last 500 steps = 3.2244e-01, PNorm = 98.8900, GNorm = 0.3091
Meta loss on this task batch = 3.4472e-01, Meta loss averaged over last 500 steps = 3.2246e-01, PNorm = 98.8988, GNorm = 0.3319
Meta loss on this task batch = 3.0377e-01, Meta loss averaged over last 500 steps = 3.2239e-01, PNorm = 98.9086, GNorm = 0.2852
Meta loss on this task batch = 3.4559e-01, Meta loss averaged over last 500 steps = 3.2251e-01, PNorm = 98.9164, GNorm = 0.3693
Meta loss on this task batch = 3.5616e-01, Meta loss averaged over last 500 steps = 3.2251e-01, PNorm = 98.9225, GNorm = 0.3615
Meta loss on this task batch = 3.0790e-01, Meta loss averaged over last 500 steps = 3.2250e-01, PNorm = 98.9291, GNorm = 0.3134
Meta loss on this task batch = 3.3574e-01, Meta loss averaged over last 500 steps = 3.2247e-01, PNorm = 98.9353, GNorm = 0.3930
Took 135.7052080631256 seconds to complete one epoch of meta training
Took 143.87580180168152 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475210
Epoch 274
Meta loss on this task batch = 2.8825e-01, Meta loss averaged over last 500 steps = 3.2246e-01, PNorm = 98.9419, GNorm = 0.2701
Meta loss on this task batch = 3.1052e-01, Meta loss averaged over last 500 steps = 3.2242e-01, PNorm = 98.9510, GNorm = 0.3150
Meta loss on this task batch = 3.3921e-01, Meta loss averaged over last 500 steps = 3.2240e-01, PNorm = 98.9584, GNorm = 0.2892
Meta loss on this task batch = 3.5050e-01, Meta loss averaged over last 500 steps = 3.2253e-01, PNorm = 98.9659, GNorm = 0.2766
Meta loss on this task batch = 2.9827e-01, Meta loss averaged over last 500 steps = 3.2241e-01, PNorm = 98.9738, GNorm = 0.3020
Meta loss on this task batch = 3.1114e-01, Meta loss averaged over last 500 steps = 3.2232e-01, PNorm = 98.9819, GNorm = 0.2719
Meta loss on this task batch = 3.2793e-01, Meta loss averaged over last 500 steps = 3.2231e-01, PNorm = 98.9915, GNorm = 0.2683
Meta loss on this task batch = 3.6376e-01, Meta loss averaged over last 500 steps = 3.2229e-01, PNorm = 99.0014, GNorm = 0.2998
Meta loss on this task batch = 3.3053e-01, Meta loss averaged over last 500 steps = 3.2230e-01, PNorm = 99.0125, GNorm = 0.2940
Meta loss on this task batch = 3.0570e-01, Meta loss averaged over last 500 steps = 3.2230e-01, PNorm = 99.0227, GNorm = 0.2913
Meta loss on this task batch = 2.9060e-01, Meta loss averaged over last 500 steps = 3.2220e-01, PNorm = 99.0298, GNorm = 0.2860
Meta loss on this task batch = 3.6506e-01, Meta loss averaged over last 500 steps = 3.2226e-01, PNorm = 99.0359, GNorm = 0.3376
Meta loss on this task batch = 2.8859e-01, Meta loss averaged over last 500 steps = 3.2218e-01, PNorm = 99.0406, GNorm = 0.2630
Meta loss on this task batch = 3.0037e-01, Meta loss averaged over last 500 steps = 3.2212e-01, PNorm = 99.0450, GNorm = 0.2960
Meta loss on this task batch = 2.8433e-01, Meta loss averaged over last 500 steps = 3.2199e-01, PNorm = 99.0482, GNorm = 0.2585
Meta loss on this task batch = 3.1998e-01, Meta loss averaged over last 500 steps = 3.2202e-01, PNorm = 99.0495, GNorm = 0.3157
Meta loss on this task batch = 3.0200e-01, Meta loss averaged over last 500 steps = 3.2196e-01, PNorm = 99.0498, GNorm = 0.2783
Meta loss on this task batch = 3.5195e-01, Meta loss averaged over last 500 steps = 3.2204e-01, PNorm = 99.0517, GNorm = 0.3151
Meta loss on this task batch = 3.1205e-01, Meta loss averaged over last 500 steps = 3.2199e-01, PNorm = 99.0549, GNorm = 0.3217
Took 133.06151866912842 seconds to complete one epoch of meta training
Took 142.04855012893677 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492109
Epoch 275
Meta loss on this task batch = 3.0244e-01, Meta loss averaged over last 500 steps = 3.2204e-01, PNorm = 99.0608, GNorm = 0.2521
Meta loss on this task batch = 3.4237e-01, Meta loss averaged over last 500 steps = 3.2213e-01, PNorm = 99.0692, GNorm = 0.2814
Meta loss on this task batch = 3.1075e-01, Meta loss averaged over last 500 steps = 3.2203e-01, PNorm = 99.0794, GNorm = 0.2757
Meta loss on this task batch = 3.3273e-01, Meta loss averaged over last 500 steps = 3.2205e-01, PNorm = 99.0921, GNorm = 0.2846
Meta loss on this task batch = 3.3295e-01, Meta loss averaged over last 500 steps = 3.2203e-01, PNorm = 99.1049, GNorm = 0.2906
Meta loss on this task batch = 3.0662e-01, Meta loss averaged over last 500 steps = 3.2198e-01, PNorm = 99.1177, GNorm = 0.2962
Meta loss on this task batch = 3.5010e-01, Meta loss averaged over last 500 steps = 3.2203e-01, PNorm = 99.1288, GNorm = 0.3334
Meta loss on this task batch = 2.5281e-01, Meta loss averaged over last 500 steps = 3.2178e-01, PNorm = 99.1381, GNorm = 0.2831
Meta loss on this task batch = 3.1307e-01, Meta loss averaged over last 500 steps = 3.2183e-01, PNorm = 99.1448, GNorm = 0.3261
Meta loss on this task batch = 3.3508e-01, Meta loss averaged over last 500 steps = 3.2190e-01, PNorm = 99.1507, GNorm = 0.3307
Meta loss on this task batch = 2.9281e-01, Meta loss averaged over last 500 steps = 3.2183e-01, PNorm = 99.1552, GNorm = 0.2904
Meta loss on this task batch = 3.2988e-01, Meta loss averaged over last 500 steps = 3.2177e-01, PNorm = 99.1587, GNorm = 0.3260
Meta loss on this task batch = 2.7824e-01, Meta loss averaged over last 500 steps = 3.2172e-01, PNorm = 99.1615, GNorm = 0.2786
Meta loss on this task batch = 3.6564e-01, Meta loss averaged over last 500 steps = 3.2182e-01, PNorm = 99.1655, GNorm = 0.3261
Meta loss on this task batch = 3.4755e-01, Meta loss averaged over last 500 steps = 3.2182e-01, PNorm = 99.1674, GNorm = 0.3189
Meta loss on this task batch = 2.8783e-01, Meta loss averaged over last 500 steps = 3.2184e-01, PNorm = 99.1707, GNorm = 0.2480
Meta loss on this task batch = 3.3411e-01, Meta loss averaged over last 500 steps = 3.2182e-01, PNorm = 99.1742, GNorm = 0.2855
Meta loss on this task batch = 2.8558e-01, Meta loss averaged over last 500 steps = 3.2168e-01, PNorm = 99.1788, GNorm = 0.3237
Meta loss on this task batch = 2.9030e-01, Meta loss averaged over last 500 steps = 3.2164e-01, PNorm = 99.1863, GNorm = 0.3374
Took 132.62269639968872 seconds to complete one epoch of meta training
Took 140.90533447265625 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510923
Epoch 276
Meta loss on this task batch = 3.2340e-01, Meta loss averaged over last 500 steps = 3.2162e-01, PNorm = 99.1967, GNorm = 0.2437
Meta loss on this task batch = 2.9514e-01, Meta loss averaged over last 500 steps = 3.2161e-01, PNorm = 99.2089, GNorm = 0.2603
Meta loss on this task batch = 3.2827e-01, Meta loss averaged over last 500 steps = 3.2161e-01, PNorm = 99.2210, GNorm = 0.2681
Meta loss on this task batch = 3.1457e-01, Meta loss averaged over last 500 steps = 3.2156e-01, PNorm = 99.2308, GNorm = 0.2813
Meta loss on this task batch = 3.4404e-01, Meta loss averaged over last 500 steps = 3.2154e-01, PNorm = 99.2399, GNorm = 0.2728
Meta loss on this task batch = 3.2894e-01, Meta loss averaged over last 500 steps = 3.2146e-01, PNorm = 99.2474, GNorm = 0.2768
Meta loss on this task batch = 3.5419e-01, Meta loss averaged over last 500 steps = 3.2148e-01, PNorm = 99.2539, GNorm = 0.3031
Meta loss on this task batch = 2.5242e-01, Meta loss averaged over last 500 steps = 3.2135e-01, PNorm = 99.2608, GNorm = 0.2297
Meta loss on this task batch = 2.9973e-01, Meta loss averaged over last 500 steps = 3.2128e-01, PNorm = 99.2650, GNorm = 0.2580
Meta loss on this task batch = 3.0070e-01, Meta loss averaged over last 500 steps = 3.2113e-01, PNorm = 99.2684, GNorm = 0.2766
Meta loss on this task batch = 3.4710e-01, Meta loss averaged over last 500 steps = 3.2126e-01, PNorm = 99.2716, GNorm = 0.2953
Meta loss on this task batch = 3.4785e-01, Meta loss averaged over last 500 steps = 3.2137e-01, PNorm = 99.2756, GNorm = 0.2678
Meta loss on this task batch = 3.2378e-01, Meta loss averaged over last 500 steps = 3.2134e-01, PNorm = 99.2796, GNorm = 0.2992
Meta loss on this task batch = 3.4000e-01, Meta loss averaged over last 500 steps = 3.2125e-01, PNorm = 99.2858, GNorm = 0.2788
Meta loss on this task batch = 3.3432e-01, Meta loss averaged over last 500 steps = 3.2130e-01, PNorm = 99.2943, GNorm = 0.2931
Meta loss on this task batch = 3.0793e-01, Meta loss averaged over last 500 steps = 3.2136e-01, PNorm = 99.3030, GNorm = 0.2926
Meta loss on this task batch = 3.1565e-01, Meta loss averaged over last 500 steps = 3.2141e-01, PNorm = 99.3120, GNorm = 0.2468
Meta loss on this task batch = 3.2166e-01, Meta loss averaged over last 500 steps = 3.2147e-01, PNorm = 99.3208, GNorm = 0.2981
Meta loss on this task batch = 2.5213e-01, Meta loss averaged over last 500 steps = 3.2138e-01, PNorm = 99.3310, GNorm = 0.3170
Took 129.53703093528748 seconds to complete one epoch of meta training
Took 136.8768389225006 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490344
Epoch 277
Meta loss on this task batch = 3.4283e-01, Meta loss averaged over last 500 steps = 3.2136e-01, PNorm = 99.3416, GNorm = 0.3063
Meta loss on this task batch = 3.2051e-01, Meta loss averaged over last 500 steps = 3.2137e-01, PNorm = 99.3515, GNorm = 0.2936
Meta loss on this task batch = 2.6029e-01, Meta loss averaged over last 500 steps = 3.2122e-01, PNorm = 99.3610, GNorm = 0.2718
Meta loss on this task batch = 3.1680e-01, Meta loss averaged over last 500 steps = 3.2118e-01, PNorm = 99.3689, GNorm = 0.2752
Meta loss on this task batch = 3.0722e-01, Meta loss averaged over last 500 steps = 3.2109e-01, PNorm = 99.3770, GNorm = 0.3087
Meta loss on this task batch = 3.2784e-01, Meta loss averaged over last 500 steps = 3.2113e-01, PNorm = 99.3850, GNorm = 0.3452
Meta loss on this task batch = 3.4100e-01, Meta loss averaged over last 500 steps = 3.2122e-01, PNorm = 99.3926, GNorm = 0.2985
Meta loss on this task batch = 3.0572e-01, Meta loss averaged over last 500 steps = 3.2125e-01, PNorm = 99.4025, GNorm = 0.2832
Meta loss on this task batch = 2.9822e-01, Meta loss averaged over last 500 steps = 3.2119e-01, PNorm = 99.4120, GNorm = 0.3007
Meta loss on this task batch = 3.2783e-01, Meta loss averaged over last 500 steps = 3.2121e-01, PNorm = 99.4201, GNorm = 0.2721
Meta loss on this task batch = 2.9389e-01, Meta loss averaged over last 500 steps = 3.2116e-01, PNorm = 99.4273, GNorm = 0.2689
Meta loss on this task batch = 3.3234e-01, Meta loss averaged over last 500 steps = 3.2124e-01, PNorm = 99.4308, GNorm = 0.3275
Meta loss on this task batch = 4.0162e-01, Meta loss averaged over last 500 steps = 3.2138e-01, PNorm = 99.4334, GNorm = 0.3183
Meta loss on this task batch = 3.2657e-01, Meta loss averaged over last 500 steps = 3.2138e-01, PNorm = 99.4367, GNorm = 0.2839
Meta loss on this task batch = 3.2780e-01, Meta loss averaged over last 500 steps = 3.2140e-01, PNorm = 99.4397, GNorm = 0.2765
Meta loss on this task batch = 3.5502e-01, Meta loss averaged over last 500 steps = 3.2134e-01, PNorm = 99.4378, GNorm = 0.3172
Meta loss on this task batch = 2.8842e-01, Meta loss averaged over last 500 steps = 3.2122e-01, PNorm = 99.4341, GNorm = 0.2707
Meta loss on this task batch = 3.0370e-01, Meta loss averaged over last 500 steps = 3.2126e-01, PNorm = 99.4314, GNorm = 0.2712
Meta loss on this task batch = 2.4428e-01, Meta loss averaged over last 500 steps = 3.2105e-01, PNorm = 99.4322, GNorm = 0.2801
Took 134.34282040596008 seconds to complete one epoch of meta training
Took 142.5656476020813 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473134
Epoch 278
Meta loss on this task batch = 3.0568e-01, Meta loss averaged over last 500 steps = 3.2103e-01, PNorm = 99.4320, GNorm = 0.2848
Meta loss on this task batch = 2.7610e-01, Meta loss averaged over last 500 steps = 3.2091e-01, PNorm = 99.4341, GNorm = 0.2407
Meta loss on this task batch = 3.2122e-01, Meta loss averaged over last 500 steps = 3.2086e-01, PNorm = 99.4401, GNorm = 0.2877
Meta loss on this task batch = 3.0929e-01, Meta loss averaged over last 500 steps = 3.2081e-01, PNorm = 99.4488, GNorm = 0.2866
Meta loss on this task batch = 3.9310e-01, Meta loss averaged over last 500 steps = 3.2093e-01, PNorm = 99.4574, GNorm = 0.3174
Meta loss on this task batch = 2.9452e-01, Meta loss averaged over last 500 steps = 3.2076e-01, PNorm = 99.4671, GNorm = 0.2605
Meta loss on this task batch = 2.9353e-01, Meta loss averaged over last 500 steps = 3.2068e-01, PNorm = 99.4788, GNorm = 0.2562
Meta loss on this task batch = 3.0613e-01, Meta loss averaged over last 500 steps = 3.2060e-01, PNorm = 99.4930, GNorm = 0.2870
Meta loss on this task batch = 3.2671e-01, Meta loss averaged over last 500 steps = 3.2069e-01, PNorm = 99.5048, GNorm = 0.2981
Meta loss on this task batch = 3.3734e-01, Meta loss averaged over last 500 steps = 3.2077e-01, PNorm = 99.5159, GNorm = 0.3032
Meta loss on this task batch = 3.2945e-01, Meta loss averaged over last 500 steps = 3.2073e-01, PNorm = 99.5266, GNorm = 0.3103
Meta loss on this task batch = 2.6773e-01, Meta loss averaged over last 500 steps = 3.2066e-01, PNorm = 99.5373, GNorm = 0.2444
Meta loss on this task batch = 3.0269e-01, Meta loss averaged over last 500 steps = 3.2057e-01, PNorm = 99.5448, GNorm = 0.2800
Meta loss on this task batch = 3.0159e-01, Meta loss averaged over last 500 steps = 3.2066e-01, PNorm = 99.5526, GNorm = 0.2688
Meta loss on this task batch = 3.7474e-01, Meta loss averaged over last 500 steps = 3.2069e-01, PNorm = 99.5563, GNorm = 0.3405
Meta loss on this task batch = 3.3664e-01, Meta loss averaged over last 500 steps = 3.2076e-01, PNorm = 99.5617, GNorm = 0.3413
Meta loss on this task batch = 3.3382e-01, Meta loss averaged over last 500 steps = 3.2087e-01, PNorm = 99.5666, GNorm = 0.2872
Meta loss on this task batch = 4.0853e-01, Meta loss averaged over last 500 steps = 3.2097e-01, PNorm = 99.5725, GNorm = 0.4501
Meta loss on this task batch = 3.1598e-01, Meta loss averaged over last 500 steps = 3.2098e-01, PNorm = 99.5795, GNorm = 0.3535
Took 130.02707076072693 seconds to complete one epoch of meta training
Took 138.9433195590973 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489385
Epoch 279
Meta loss on this task batch = 3.0402e-01, Meta loss averaged over last 500 steps = 3.2097e-01, PNorm = 99.5863, GNorm = 0.2863
Meta loss on this task batch = 2.5586e-01, Meta loss averaged over last 500 steps = 3.2086e-01, PNorm = 99.5947, GNorm = 0.2567
Meta loss on this task batch = 2.9689e-01, Meta loss averaged over last 500 steps = 3.2068e-01, PNorm = 99.6048, GNorm = 0.2834
Meta loss on this task batch = 3.0258e-01, Meta loss averaged over last 500 steps = 3.2058e-01, PNorm = 99.6152, GNorm = 0.2627
Meta loss on this task batch = 3.2580e-01, Meta loss averaged over last 500 steps = 3.2058e-01, PNorm = 99.6238, GNorm = 0.2997
Meta loss on this task batch = 3.5578e-01, Meta loss averaged over last 500 steps = 3.2068e-01, PNorm = 99.6315, GNorm = 0.3199
Meta loss on this task batch = 3.6773e-01, Meta loss averaged over last 500 steps = 3.2080e-01, PNorm = 99.6380, GNorm = 0.2836
Meta loss on this task batch = 3.0580e-01, Meta loss averaged over last 500 steps = 3.2075e-01, PNorm = 99.6406, GNorm = 0.2790
Meta loss on this task batch = 3.0145e-01, Meta loss averaged over last 500 steps = 3.2084e-01, PNorm = 99.6429, GNorm = 0.2635
Meta loss on this task batch = 3.1072e-01, Meta loss averaged over last 500 steps = 3.2085e-01, PNorm = 99.6458, GNorm = 0.2575
Meta loss on this task batch = 2.7442e-01, Meta loss averaged over last 500 steps = 3.2074e-01, PNorm = 99.6504, GNorm = 0.2564
Meta loss on this task batch = 3.6242e-01, Meta loss averaged over last 500 steps = 3.2075e-01, PNorm = 99.6557, GNorm = 0.2821
Meta loss on this task batch = 3.2481e-01, Meta loss averaged over last 500 steps = 3.2074e-01, PNorm = 99.6631, GNorm = 0.2946
Meta loss on this task batch = 3.2827e-01, Meta loss averaged over last 500 steps = 3.2068e-01, PNorm = 99.6696, GNorm = 0.3053
Meta loss on this task batch = 2.6480e-01, Meta loss averaged over last 500 steps = 3.2043e-01, PNorm = 99.6783, GNorm = 0.2607
Meta loss on this task batch = 3.5258e-01, Meta loss averaged over last 500 steps = 3.2052e-01, PNorm = 99.6842, GNorm = 0.3522
Meta loss on this task batch = 2.5515e-01, Meta loss averaged over last 500 steps = 3.2033e-01, PNorm = 99.6906, GNorm = 0.2664
Meta loss on this task batch = 3.1774e-01, Meta loss averaged over last 500 steps = 3.2029e-01, PNorm = 99.6962, GNorm = 0.3086
Meta loss on this task batch = 3.4948e-01, Meta loss averaged over last 500 steps = 3.2033e-01, PNorm = 99.7017, GNorm = 0.3387
Took 132.90073370933533 seconds to complete one epoch of meta training
Took 140.68326497077942 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485838
Epoch 280
Meta loss on this task batch = 3.0093e-01, Meta loss averaged over last 500 steps = 3.2020e-01, PNorm = 99.7063, GNorm = 0.3083
Meta loss on this task batch = 2.8591e-01, Meta loss averaged over last 500 steps = 3.2013e-01, PNorm = 99.7105, GNorm = 0.2532
Meta loss on this task batch = 2.9749e-01, Meta loss averaged over last 500 steps = 3.2010e-01, PNorm = 99.7164, GNorm = 0.2781
Meta loss on this task batch = 3.1427e-01, Meta loss averaged over last 500 steps = 3.2012e-01, PNorm = 99.7211, GNorm = 0.2763
Meta loss on this task batch = 2.8046e-01, Meta loss averaged over last 500 steps = 3.2007e-01, PNorm = 99.7255, GNorm = 0.3228
Meta loss on this task batch = 3.1717e-01, Meta loss averaged over last 500 steps = 3.2006e-01, PNorm = 99.7296, GNorm = 0.2833
Meta loss on this task batch = 2.7519e-01, Meta loss averaged over last 500 steps = 3.2009e-01, PNorm = 99.7344, GNorm = 0.2921
Meta loss on this task batch = 3.2941e-01, Meta loss averaged over last 500 steps = 3.2007e-01, PNorm = 99.7396, GNorm = 0.3168
Meta loss on this task batch = 3.0183e-01, Meta loss averaged over last 500 steps = 3.2000e-01, PNorm = 99.7453, GNorm = 0.2961
Meta loss on this task batch = 3.7790e-01, Meta loss averaged over last 500 steps = 3.2018e-01, PNorm = 99.7481, GNorm = 0.3559
Meta loss on this task batch = 3.5290e-01, Meta loss averaged over last 500 steps = 3.2024e-01, PNorm = 99.7528, GNorm = 0.2634
Meta loss on this task batch = 3.6591e-01, Meta loss averaged over last 500 steps = 3.2040e-01, PNorm = 99.7579, GNorm = 0.2933
Meta loss on this task batch = 3.1093e-01, Meta loss averaged over last 500 steps = 3.2042e-01, PNorm = 99.7669, GNorm = 0.2815
Meta loss on this task batch = 3.6596e-01, Meta loss averaged over last 500 steps = 3.2052e-01, PNorm = 99.7745, GNorm = 0.3254
Meta loss on this task batch = 3.3917e-01, Meta loss averaged over last 500 steps = 3.2050e-01, PNorm = 99.7826, GNorm = 0.2672
Meta loss on this task batch = 3.2408e-01, Meta loss averaged over last 500 steps = 3.2050e-01, PNorm = 99.7929, GNorm = 0.2722
Meta loss on this task batch = 3.0837e-01, Meta loss averaged over last 500 steps = 3.2036e-01, PNorm = 99.8021, GNorm = 0.2338
Meta loss on this task batch = 3.3582e-01, Meta loss averaged over last 500 steps = 3.2048e-01, PNorm = 99.8093, GNorm = 0.2414
Meta loss on this task batch = 3.1703e-01, Meta loss averaged over last 500 steps = 3.2056e-01, PNorm = 99.8141, GNorm = 0.3902
Took 130.98914337158203 seconds to complete one epoch of meta training
Took 139.59508752822876 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475795
Epoch 281
Meta loss on this task batch = 3.7472e-01, Meta loss averaged over last 500 steps = 3.2067e-01, PNorm = 99.8143, GNorm = 0.3173
Meta loss on this task batch = 2.6551e-01, Meta loss averaged over last 500 steps = 3.2054e-01, PNorm = 99.8145, GNorm = 0.2562
Meta loss on this task batch = 2.4597e-01, Meta loss averaged over last 500 steps = 3.2040e-01, PNorm = 99.8145, GNorm = 0.2604
Meta loss on this task batch = 3.4399e-01, Meta loss averaged over last 500 steps = 3.2043e-01, PNorm = 99.8130, GNorm = 0.3221
Meta loss on this task batch = 2.8949e-01, Meta loss averaged over last 500 steps = 3.2029e-01, PNorm = 99.8128, GNorm = 0.2447
Meta loss on this task batch = 3.2181e-01, Meta loss averaged over last 500 steps = 3.2031e-01, PNorm = 99.8112, GNorm = 0.2981
Meta loss on this task batch = 3.6401e-01, Meta loss averaged over last 500 steps = 3.2036e-01, PNorm = 99.8131, GNorm = 0.3213
Meta loss on this task batch = 2.7284e-01, Meta loss averaged over last 500 steps = 3.2025e-01, PNorm = 99.8156, GNorm = 0.2394
Meta loss on this task batch = 2.6495e-01, Meta loss averaged over last 500 steps = 3.1998e-01, PNorm = 99.8218, GNorm = 0.2573
Meta loss on this task batch = 2.9480e-01, Meta loss averaged over last 500 steps = 3.1986e-01, PNorm = 99.8319, GNorm = 0.2908
Meta loss on this task batch = 3.3520e-01, Meta loss averaged over last 500 steps = 3.1986e-01, PNorm = 99.8421, GNorm = 0.3283
Meta loss on this task batch = 3.0000e-01, Meta loss averaged over last 500 steps = 3.1985e-01, PNorm = 99.8529, GNorm = 0.2854
Meta loss on this task batch = 3.0733e-01, Meta loss averaged over last 500 steps = 3.1978e-01, PNorm = 99.8633, GNorm = 0.3123
Meta loss on this task batch = 3.2774e-01, Meta loss averaged over last 500 steps = 3.1983e-01, PNorm = 99.8739, GNorm = 0.2922
Meta loss on this task batch = 3.3962e-01, Meta loss averaged over last 500 steps = 3.1986e-01, PNorm = 99.8824, GNorm = 0.3076
Meta loss on this task batch = 2.9194e-01, Meta loss averaged over last 500 steps = 3.1978e-01, PNorm = 99.8916, GNorm = 0.2841
Meta loss on this task batch = 3.5027e-01, Meta loss averaged over last 500 steps = 3.1980e-01, PNorm = 99.8996, GNorm = 0.3140
Meta loss on this task batch = 3.4460e-01, Meta loss averaged over last 500 steps = 3.1991e-01, PNorm = 99.9058, GNorm = 0.2988
Meta loss on this task batch = 3.5616e-01, Meta loss averaged over last 500 steps = 3.1999e-01, PNorm = 99.9110, GNorm = 0.3698
Took 128.79286885261536 seconds to complete one epoch of meta training
Took 136.9986822605133 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493436
Epoch 282
Meta loss on this task batch = 3.0631e-01, Meta loss averaged over last 500 steps = 3.1991e-01, PNorm = 99.9156, GNorm = 0.2769
Meta loss on this task batch = 3.3248e-01, Meta loss averaged over last 500 steps = 3.1993e-01, PNorm = 99.9204, GNorm = 0.2542
Meta loss on this task batch = 2.7608e-01, Meta loss averaged over last 500 steps = 3.1990e-01, PNorm = 99.9252, GNorm = 0.2725
Meta loss on this task batch = 3.0111e-01, Meta loss averaged over last 500 steps = 3.1988e-01, PNorm = 99.9321, GNorm = 0.2774
Meta loss on this task batch = 3.1108e-01, Meta loss averaged over last 500 steps = 3.1994e-01, PNorm = 99.9369, GNorm = 0.3294
Meta loss on this task batch = 3.3004e-01, Meta loss averaged over last 500 steps = 3.2012e-01, PNorm = 99.9392, GNorm = 0.3005
Meta loss on this task batch = 3.1005e-01, Meta loss averaged over last 500 steps = 3.2004e-01, PNorm = 99.9431, GNorm = 0.2921
Meta loss on this task batch = 3.4036e-01, Meta loss averaged over last 500 steps = 3.2014e-01, PNorm = 99.9447, GNorm = 0.3110
Meta loss on this task batch = 2.6880e-01, Meta loss averaged over last 500 steps = 3.2010e-01, PNorm = 99.9474, GNorm = 0.3067
Meta loss on this task batch = 3.5156e-01, Meta loss averaged over last 500 steps = 3.2020e-01, PNorm = 99.9500, GNorm = 0.2987
Meta loss on this task batch = 3.2189e-01, Meta loss averaged over last 500 steps = 3.2030e-01, PNorm = 99.9544, GNorm = 0.3415
Meta loss on this task batch = 3.0926e-01, Meta loss averaged over last 500 steps = 3.2035e-01, PNorm = 99.9601, GNorm = 0.2793
Meta loss on this task batch = 2.8001e-01, Meta loss averaged over last 500 steps = 3.2027e-01, PNorm = 99.9686, GNorm = 0.2684
Meta loss on this task batch = 3.2465e-01, Meta loss averaged over last 500 steps = 3.2017e-01, PNorm = 99.9789, GNorm = 0.3002
Meta loss on this task batch = 3.0465e-01, Meta loss averaged over last 500 steps = 3.2002e-01, PNorm = 99.9897, GNorm = 0.3018
Meta loss on this task batch = 3.2882e-01, Meta loss averaged over last 500 steps = 3.1997e-01, PNorm = 99.9985, GNorm = 0.3014
Meta loss on this task batch = 3.1993e-01, Meta loss averaged over last 500 steps = 3.1994e-01, PNorm = 100.0073, GNorm = 0.2695
Meta loss on this task batch = 3.4135e-01, Meta loss averaged over last 500 steps = 3.2005e-01, PNorm = 100.0151, GNorm = 0.3166
Meta loss on this task batch = 3.1879e-01, Meta loss averaged over last 500 steps = 3.2008e-01, PNorm = 100.0205, GNorm = 0.3060
Took 133.40745496749878 seconds to complete one epoch of meta training
Took 142.0153214931488 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493517
Epoch 283
Meta loss on this task batch = 3.6054e-01, Meta loss averaged over last 500 steps = 3.2012e-01, PNorm = 100.0236, GNorm = 0.3201
Meta loss on this task batch = 3.0802e-01, Meta loss averaged over last 500 steps = 3.2004e-01, PNorm = 100.0260, GNorm = 0.2845
Meta loss on this task batch = 3.1048e-01, Meta loss averaged over last 500 steps = 3.1995e-01, PNorm = 100.0293, GNorm = 0.2441
Meta loss on this task batch = 3.1093e-01, Meta loss averaged over last 500 steps = 3.1993e-01, PNorm = 100.0316, GNorm = 0.3152
Meta loss on this task batch = 2.7297e-01, Meta loss averaged over last 500 steps = 3.1978e-01, PNorm = 100.0305, GNorm = 0.2934
Meta loss on this task batch = 3.4599e-01, Meta loss averaged over last 500 steps = 3.1988e-01, PNorm = 100.0301, GNorm = 0.3161
Meta loss on this task batch = 3.2571e-01, Meta loss averaged over last 500 steps = 3.1994e-01, PNorm = 100.0310, GNorm = 0.2802
Meta loss on this task batch = 3.5693e-01, Meta loss averaged over last 500 steps = 3.2006e-01, PNorm = 100.0332, GNorm = 0.3124
Meta loss on this task batch = 2.8601e-01, Meta loss averaged over last 500 steps = 3.2002e-01, PNorm = 100.0386, GNorm = 0.2638
Meta loss on this task batch = 3.3271e-01, Meta loss averaged over last 500 steps = 3.2009e-01, PNorm = 100.0459, GNorm = 0.3299
Meta loss on this task batch = 2.7519e-01, Meta loss averaged over last 500 steps = 3.2004e-01, PNorm = 100.0560, GNorm = 0.2574
Meta loss on this task batch = 3.2753e-01, Meta loss averaged over last 500 steps = 3.2007e-01, PNorm = 100.0666, GNorm = 0.3338
Meta loss on this task batch = 3.3879e-01, Meta loss averaged over last 500 steps = 3.2001e-01, PNorm = 100.0770, GNorm = 0.2986
Meta loss on this task batch = 3.3391e-01, Meta loss averaged over last 500 steps = 3.2012e-01, PNorm = 100.0873, GNorm = 0.2990
Meta loss on this task batch = 3.1423e-01, Meta loss averaged over last 500 steps = 3.2007e-01, PNorm = 100.0967, GNorm = 0.2836
Meta loss on this task batch = 2.7123e-01, Meta loss averaged over last 500 steps = 3.2010e-01, PNorm = 100.1085, GNorm = 0.2568
Meta loss on this task batch = 3.0093e-01, Meta loss averaged over last 500 steps = 3.2001e-01, PNorm = 100.1196, GNorm = 0.2902
Meta loss on this task batch = 3.4927e-01, Meta loss averaged over last 500 steps = 3.1994e-01, PNorm = 100.1314, GNorm = 0.3031
Meta loss on this task batch = 2.2910e-01, Meta loss averaged over last 500 steps = 3.1961e-01, PNorm = 100.1438, GNorm = 0.2587
Took 130.1426215171814 seconds to complete one epoch of meta training
Took 138.89194416999817 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484366
Epoch 284
Meta loss on this task batch = 3.1731e-01, Meta loss averaged over last 500 steps = 3.1958e-01, PNorm = 100.1542, GNorm = 0.3124
Meta loss on this task batch = 3.4648e-01, Meta loss averaged over last 500 steps = 3.1959e-01, PNorm = 100.1617, GNorm = 0.3275
Meta loss on this task batch = 3.6969e-01, Meta loss averaged over last 500 steps = 3.1965e-01, PNorm = 100.1642, GNorm = 0.3299
Meta loss on this task batch = 2.8674e-01, Meta loss averaged over last 500 steps = 3.1963e-01, PNorm = 100.1662, GNorm = 0.2512
Meta loss on this task batch = 3.6204e-01, Meta loss averaged over last 500 steps = 3.1963e-01, PNorm = 100.1667, GNorm = 0.3027
Meta loss on this task batch = 2.9490e-01, Meta loss averaged over last 500 steps = 3.1961e-01, PNorm = 100.1657, GNorm = 0.2686
Meta loss on this task batch = 2.8227e-01, Meta loss averaged over last 500 steps = 3.1958e-01, PNorm = 100.1628, GNorm = 0.3033
Meta loss on this task batch = 3.3077e-01, Meta loss averaged over last 500 steps = 3.1959e-01, PNorm = 100.1607, GNorm = 0.2570
Meta loss on this task batch = 3.6057e-01, Meta loss averaged over last 500 steps = 3.1961e-01, PNorm = 100.1606, GNorm = 0.3500
Meta loss on this task batch = 3.8565e-01, Meta loss averaged over last 500 steps = 3.1966e-01, PNorm = 100.1611, GNorm = 0.3341
Meta loss on this task batch = 3.3864e-01, Meta loss averaged over last 500 steps = 3.1978e-01, PNorm = 100.1644, GNorm = 0.3026
Meta loss on this task batch = 3.5244e-01, Meta loss averaged over last 500 steps = 3.1985e-01, PNorm = 100.1713, GNorm = 0.3034
Meta loss on this task batch = 3.3194e-01, Meta loss averaged over last 500 steps = 3.1979e-01, PNorm = 100.1799, GNorm = 0.2705
Meta loss on this task batch = 3.1704e-01, Meta loss averaged over last 500 steps = 3.1983e-01, PNorm = 100.1889, GNorm = 0.2444
Meta loss on this task batch = 2.8408e-01, Meta loss averaged over last 500 steps = 3.1976e-01, PNorm = 100.1967, GNorm = 0.2779
Meta loss on this task batch = 2.3939e-01, Meta loss averaged over last 500 steps = 3.1947e-01, PNorm = 100.2066, GNorm = 0.2660
Meta loss on this task batch = 2.8056e-01, Meta loss averaged over last 500 steps = 3.1936e-01, PNorm = 100.2153, GNorm = 0.2870
Meta loss on this task batch = 2.7101e-01, Meta loss averaged over last 500 steps = 3.1935e-01, PNorm = 100.2239, GNorm = 0.2738
Meta loss on this task batch = 2.8425e-01, Meta loss averaged over last 500 steps = 3.1924e-01, PNorm = 100.2320, GNorm = 0.3343
Took 130.26235699653625 seconds to complete one epoch of meta training
Took 137.513174533844 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500848
Epoch 285
Meta loss on this task batch = 3.0795e-01, Meta loss averaged over last 500 steps = 3.1907e-01, PNorm = 100.2381, GNorm = 0.3112
Meta loss on this task batch = 2.7664e-01, Meta loss averaged over last 500 steps = 3.1895e-01, PNorm = 100.2444, GNorm = 0.2691
Meta loss on this task batch = 3.1823e-01, Meta loss averaged over last 500 steps = 3.1892e-01, PNorm = 100.2490, GNorm = 0.3079
Meta loss on this task batch = 3.3666e-01, Meta loss averaged over last 500 steps = 3.1889e-01, PNorm = 100.2540, GNorm = 0.3084
Meta loss on this task batch = 3.4732e-01, Meta loss averaged over last 500 steps = 3.1907e-01, PNorm = 100.2572, GNorm = 0.3153
Meta loss on this task batch = 3.3824e-01, Meta loss averaged over last 500 steps = 3.1913e-01, PNorm = 100.2611, GNorm = 0.3070
Meta loss on this task batch = 3.2040e-01, Meta loss averaged over last 500 steps = 3.1917e-01, PNorm = 100.2648, GNorm = 0.3118
Meta loss on this task batch = 3.0296e-01, Meta loss averaged over last 500 steps = 3.1915e-01, PNorm = 100.2681, GNorm = 0.2961
Meta loss on this task batch = 3.4954e-01, Meta loss averaged over last 500 steps = 3.1923e-01, PNorm = 100.2717, GNorm = 0.3292
Meta loss on this task batch = 3.4633e-01, Meta loss averaged over last 500 steps = 3.1921e-01, PNorm = 100.2751, GNorm = 0.2957
Meta loss on this task batch = 2.8914e-01, Meta loss averaged over last 500 steps = 3.1898e-01, PNorm = 100.2800, GNorm = 0.2402
Meta loss on this task batch = 3.4592e-01, Meta loss averaged over last 500 steps = 3.1894e-01, PNorm = 100.2832, GNorm = 0.2954
Meta loss on this task batch = 3.2518e-01, Meta loss averaged over last 500 steps = 3.1906e-01, PNorm = 100.2868, GNorm = 0.2735
Meta loss on this task batch = 3.3935e-01, Meta loss averaged over last 500 steps = 3.1910e-01, PNorm = 100.2924, GNorm = 0.2690
Meta loss on this task batch = 3.4877e-01, Meta loss averaged over last 500 steps = 3.1918e-01, PNorm = 100.2982, GNorm = 0.3385
Meta loss on this task batch = 2.7865e-01, Meta loss averaged over last 500 steps = 3.1898e-01, PNorm = 100.3073, GNorm = 0.2490
Meta loss on this task batch = 3.4145e-01, Meta loss averaged over last 500 steps = 3.1899e-01, PNorm = 100.3164, GNorm = 0.2674
Meta loss on this task batch = 2.9638e-01, Meta loss averaged over last 500 steps = 3.1896e-01, PNorm = 100.3245, GNorm = 0.2671
Meta loss on this task batch = 3.0883e-01, Meta loss averaged over last 500 steps = 3.1898e-01, PNorm = 100.3325, GNorm = 0.3000
Took 131.7309274673462 seconds to complete one epoch of meta training
Took 140.20198941230774 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487636
Epoch 286
Meta loss on this task batch = 3.3163e-01, Meta loss averaged over last 500 steps = 3.1899e-01, PNorm = 100.3401, GNorm = 0.2559
Meta loss on this task batch = 2.9154e-01, Meta loss averaged over last 500 steps = 3.1897e-01, PNorm = 100.3475, GNorm = 0.2553
Meta loss on this task batch = 3.3142e-01, Meta loss averaged over last 500 steps = 3.1907e-01, PNorm = 100.3551, GNorm = 0.2728
Meta loss on this task batch = 3.6774e-01, Meta loss averaged over last 500 steps = 3.1910e-01, PNorm = 100.3618, GNorm = 0.3175
Meta loss on this task batch = 3.4041e-01, Meta loss averaged over last 500 steps = 3.1917e-01, PNorm = 100.3686, GNorm = 0.2681
Meta loss on this task batch = 2.4068e-01, Meta loss averaged over last 500 steps = 3.1902e-01, PNorm = 100.3762, GNorm = 0.2216
Meta loss on this task batch = 3.8945e-01, Meta loss averaged over last 500 steps = 3.1924e-01, PNorm = 100.3830, GNorm = 0.3233
Meta loss on this task batch = 2.5367e-01, Meta loss averaged over last 500 steps = 3.1900e-01, PNorm = 100.3885, GNorm = 0.2557
Meta loss on this task batch = 3.0925e-01, Meta loss averaged over last 500 steps = 3.1905e-01, PNorm = 100.3953, GNorm = 0.2768
Meta loss on this task batch = 3.0825e-01, Meta loss averaged over last 500 steps = 3.1895e-01, PNorm = 100.4032, GNorm = 0.2880
Meta loss on this task batch = 2.9397e-01, Meta loss averaged over last 500 steps = 3.1890e-01, PNorm = 100.4114, GNorm = 0.2666
Meta loss on this task batch = 3.3427e-01, Meta loss averaged over last 500 steps = 3.1898e-01, PNorm = 100.4208, GNorm = 0.3081
Meta loss on this task batch = 3.2873e-01, Meta loss averaged over last 500 steps = 3.1891e-01, PNorm = 100.4293, GNorm = 0.3023
Meta loss on this task batch = 3.8041e-01, Meta loss averaged over last 500 steps = 3.1904e-01, PNorm = 100.4363, GNorm = 0.3271
Meta loss on this task batch = 3.1877e-01, Meta loss averaged over last 500 steps = 3.1904e-01, PNorm = 100.4443, GNorm = 0.2840
Meta loss on this task batch = 3.0251e-01, Meta loss averaged over last 500 steps = 3.1890e-01, PNorm = 100.4512, GNorm = 0.2850
Meta loss on this task batch = 3.1886e-01, Meta loss averaged over last 500 steps = 3.1897e-01, PNorm = 100.4576, GNorm = 0.3174
Meta loss on this task batch = 2.5642e-01, Meta loss averaged over last 500 steps = 3.1874e-01, PNorm = 100.4626, GNorm = 0.2998
Meta loss on this task batch = 2.8758e-01, Meta loss averaged over last 500 steps = 3.1873e-01, PNorm = 100.4659, GNorm = 0.3487
Took 131.84512400627136 seconds to complete one epoch of meta training
Took 140.09745168685913 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481679
Epoch 287
Meta loss on this task batch = 3.2466e-01, Meta loss averaged over last 500 steps = 3.1874e-01, PNorm = 100.4670, GNorm = 0.3158
Meta loss on this task batch = 3.5631e-01, Meta loss averaged over last 500 steps = 3.1877e-01, PNorm = 100.4687, GNorm = 0.3129
Meta loss on this task batch = 3.0775e-01, Meta loss averaged over last 500 steps = 3.1882e-01, PNorm = 100.4725, GNorm = 0.2737
Meta loss on this task batch = 3.4217e-01, Meta loss averaged over last 500 steps = 3.1892e-01, PNorm = 100.4771, GNorm = 0.2904
Meta loss on this task batch = 2.8374e-01, Meta loss averaged over last 500 steps = 3.1890e-01, PNorm = 100.4819, GNorm = 0.2933
Meta loss on this task batch = 2.7251e-01, Meta loss averaged over last 500 steps = 3.1879e-01, PNorm = 100.4860, GNorm = 0.2887
Meta loss on this task batch = 2.7649e-01, Meta loss averaged over last 500 steps = 3.1873e-01, PNorm = 100.4915, GNorm = 0.2943
Meta loss on this task batch = 2.7544e-01, Meta loss averaged over last 500 steps = 3.1873e-01, PNorm = 100.4983, GNorm = 0.2586
Meta loss on this task batch = 2.9642e-01, Meta loss averaged over last 500 steps = 3.1871e-01, PNorm = 100.5051, GNorm = 0.2836
Meta loss on this task batch = 3.3266e-01, Meta loss averaged over last 500 steps = 3.1868e-01, PNorm = 100.5122, GNorm = 0.3186
Meta loss on this task batch = 3.0766e-01, Meta loss averaged over last 500 steps = 3.1852e-01, PNorm = 100.5187, GNorm = 0.2989
Meta loss on this task batch = 3.0800e-01, Meta loss averaged over last 500 steps = 3.1848e-01, PNorm = 100.5237, GNorm = 0.3085
Meta loss on this task batch = 3.3986e-01, Meta loss averaged over last 500 steps = 3.1849e-01, PNorm = 100.5276, GNorm = 0.2991
Meta loss on this task batch = 3.1007e-01, Meta loss averaged over last 500 steps = 3.1845e-01, PNorm = 100.5324, GNorm = 0.2911
Meta loss on this task batch = 3.0776e-01, Meta loss averaged over last 500 steps = 3.1846e-01, PNorm = 100.5365, GNorm = 0.3122
Meta loss on this task batch = 3.0986e-01, Meta loss averaged over last 500 steps = 3.1843e-01, PNorm = 100.5430, GNorm = 0.2814
Meta loss on this task batch = 3.1541e-01, Meta loss averaged over last 500 steps = 3.1838e-01, PNorm = 100.5488, GNorm = 0.2816
Meta loss on this task batch = 3.2969e-01, Meta loss averaged over last 500 steps = 3.1843e-01, PNorm = 100.5530, GNorm = 0.2965
Meta loss on this task batch = 3.5467e-01, Meta loss averaged over last 500 steps = 3.1845e-01, PNorm = 100.5598, GNorm = 0.3991
Took 128.7282156944275 seconds to complete one epoch of meta training
Took 137.43463134765625 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481111
Epoch 288
Meta loss on this task batch = 3.3592e-01, Meta loss averaged over last 500 steps = 3.1846e-01, PNorm = 100.5678, GNorm = 0.3049
Meta loss on this task batch = 2.6662e-01, Meta loss averaged over last 500 steps = 3.1840e-01, PNorm = 100.5773, GNorm = 0.2612
Meta loss on this task batch = 3.4057e-01, Meta loss averaged over last 500 steps = 3.1847e-01, PNorm = 100.5863, GNorm = 0.2763
Meta loss on this task batch = 3.1218e-01, Meta loss averaged over last 500 steps = 3.1839e-01, PNorm = 100.5944, GNorm = 0.2749
Meta loss on this task batch = 3.0525e-01, Meta loss averaged over last 500 steps = 3.1833e-01, PNorm = 100.6032, GNorm = 0.3085
Meta loss on this task batch = 2.7310e-01, Meta loss averaged over last 500 steps = 3.1818e-01, PNorm = 100.6118, GNorm = 0.2673
Meta loss on this task batch = 3.4410e-01, Meta loss averaged over last 500 steps = 3.1828e-01, PNorm = 100.6177, GNorm = 0.3500
Meta loss on this task batch = 2.6189e-01, Meta loss averaged over last 500 steps = 3.1806e-01, PNorm = 100.6236, GNorm = 0.2631
Meta loss on this task batch = 3.0626e-01, Meta loss averaged over last 500 steps = 3.1805e-01, PNorm = 100.6268, GNorm = 0.2872
Meta loss on this task batch = 2.7210e-01, Meta loss averaged over last 500 steps = 3.1791e-01, PNorm = 100.6274, GNorm = 0.2991
Meta loss on this task batch = 3.1439e-01, Meta loss averaged over last 500 steps = 3.1787e-01, PNorm = 100.6261, GNorm = 0.3682
Meta loss on this task batch = 2.9225e-01, Meta loss averaged over last 500 steps = 3.1784e-01, PNorm = 100.6247, GNorm = 0.2728
Meta loss on this task batch = 3.6667e-01, Meta loss averaged over last 500 steps = 3.1795e-01, PNorm = 100.6227, GNorm = 0.3347
Meta loss on this task batch = 3.1924e-01, Meta loss averaged over last 500 steps = 3.1793e-01, PNorm = 100.6225, GNorm = 0.3103
Meta loss on this task batch = 3.6158e-01, Meta loss averaged over last 500 steps = 3.1799e-01, PNorm = 100.6217, GNorm = 0.3171
Meta loss on this task batch = 3.4173e-01, Meta loss averaged over last 500 steps = 3.1802e-01, PNorm = 100.6226, GNorm = 0.2831
Meta loss on this task batch = 3.0513e-01, Meta loss averaged over last 500 steps = 3.1804e-01, PNorm = 100.6271, GNorm = 0.2965
Meta loss on this task batch = 3.3800e-01, Meta loss averaged over last 500 steps = 3.1798e-01, PNorm = 100.6335, GNorm = 0.2969
Meta loss on this task batch = 3.0966e-01, Meta loss averaged over last 500 steps = 3.1797e-01, PNorm = 100.6424, GNorm = 0.3333
Took 130.75000143051147 seconds to complete one epoch of meta training
Took 138.92484092712402 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510278
Epoch 289
Meta loss on this task batch = 3.5285e-01, Meta loss averaged over last 500 steps = 3.1807e-01, PNorm = 100.6517, GNorm = 0.3543
Meta loss on this task batch = 3.7035e-01, Meta loss averaged over last 500 steps = 3.1813e-01, PNorm = 100.6624, GNorm = 0.3177
Meta loss on this task batch = 2.8553e-01, Meta loss averaged over last 500 steps = 3.1799e-01, PNorm = 100.6742, GNorm = 0.2769
Meta loss on this task batch = 2.9542e-01, Meta loss averaged over last 500 steps = 3.1791e-01, PNorm = 100.6863, GNorm = 0.2918
Meta loss on this task batch = 3.3163e-01, Meta loss averaged over last 500 steps = 3.1785e-01, PNorm = 100.6983, GNorm = 0.2602
Meta loss on this task batch = 2.5486e-01, Meta loss averaged over last 500 steps = 3.1783e-01, PNorm = 100.7084, GNorm = 0.2483
Meta loss on this task batch = 3.0728e-01, Meta loss averaged over last 500 steps = 3.1767e-01, PNorm = 100.7161, GNorm = 0.2996
Meta loss on this task batch = 3.6486e-01, Meta loss averaged over last 500 steps = 3.1775e-01, PNorm = 100.7199, GNorm = 0.3364
Meta loss on this task batch = 2.9111e-01, Meta loss averaged over last 500 steps = 3.1764e-01, PNorm = 100.7234, GNorm = 0.2780
Meta loss on this task batch = 3.2006e-01, Meta loss averaged over last 500 steps = 3.1761e-01, PNorm = 100.7246, GNorm = 0.2984
Meta loss on this task batch = 3.1479e-01, Meta loss averaged over last 500 steps = 3.1765e-01, PNorm = 100.7263, GNorm = 0.2909
Meta loss on this task batch = 3.7872e-01, Meta loss averaged over last 500 steps = 3.1770e-01, PNorm = 100.7264, GNorm = 0.3568
Meta loss on this task batch = 2.8553e-01, Meta loss averaged over last 500 steps = 3.1774e-01, PNorm = 100.7288, GNorm = 0.2463
Meta loss on this task batch = 3.4348e-01, Meta loss averaged over last 500 steps = 3.1785e-01, PNorm = 100.7317, GNorm = 0.3065
Meta loss on this task batch = 3.2133e-01, Meta loss averaged over last 500 steps = 3.1784e-01, PNorm = 100.7365, GNorm = 0.2915
Meta loss on this task batch = 2.7959e-01, Meta loss averaged over last 500 steps = 3.1777e-01, PNorm = 100.7438, GNorm = 0.2771
Meta loss on this task batch = 3.1801e-01, Meta loss averaged over last 500 steps = 3.1773e-01, PNorm = 100.7537, GNorm = 0.3071
Meta loss on this task batch = 3.2329e-01, Meta loss averaged over last 500 steps = 3.1761e-01, PNorm = 100.7677, GNorm = 0.2863
Meta loss on this task batch = 2.7187e-01, Meta loss averaged over last 500 steps = 3.1757e-01, PNorm = 100.7807, GNorm = 0.3883
Took 131.88673162460327 seconds to complete one epoch of meta training
Took 140.83746457099915 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484439
Epoch 290
Meta loss on this task batch = 3.3486e-01, Meta loss averaged over last 500 steps = 3.1767e-01, PNorm = 100.7918, GNorm = 0.2779
Meta loss on this task batch = 2.9873e-01, Meta loss averaged over last 500 steps = 3.1769e-01, PNorm = 100.8002, GNorm = 0.2584
Meta loss on this task batch = 2.9298e-01, Meta loss averaged over last 500 steps = 3.1763e-01, PNorm = 100.8098, GNorm = 0.2764
Meta loss on this task batch = 2.8248e-01, Meta loss averaged over last 500 steps = 3.1754e-01, PNorm = 100.8198, GNorm = 0.2437
Meta loss on this task batch = 3.0838e-01, Meta loss averaged over last 500 steps = 3.1748e-01, PNorm = 100.8270, GNorm = 0.3258
Meta loss on this task batch = 2.9970e-01, Meta loss averaged over last 500 steps = 3.1738e-01, PNorm = 100.8340, GNorm = 0.2775
Meta loss on this task batch = 3.0979e-01, Meta loss averaged over last 500 steps = 3.1738e-01, PNorm = 100.8386, GNorm = 0.2828
Meta loss on this task batch = 3.8373e-01, Meta loss averaged over last 500 steps = 3.1752e-01, PNorm = 100.8403, GNorm = 0.3372
Meta loss on this task batch = 3.0433e-01, Meta loss averaged over last 500 steps = 3.1752e-01, PNorm = 100.8415, GNorm = 0.3178
Meta loss on this task batch = 3.0732e-01, Meta loss averaged over last 500 steps = 3.1746e-01, PNorm = 100.8413, GNorm = 0.2938
Meta loss on this task batch = 3.3025e-01, Meta loss averaged over last 500 steps = 3.1747e-01, PNorm = 100.8391, GNorm = 0.3066
Meta loss on this task batch = 2.5030e-01, Meta loss averaged over last 500 steps = 3.1736e-01, PNorm = 100.8370, GNorm = 0.2961
Meta loss on this task batch = 3.5530e-01, Meta loss averaged over last 500 steps = 3.1740e-01, PNorm = 100.8372, GNorm = 0.3450
Meta loss on this task batch = 3.8165e-01, Meta loss averaged over last 500 steps = 3.1752e-01, PNorm = 100.8379, GNorm = 0.3502
Meta loss on this task batch = 3.2400e-01, Meta loss averaged over last 500 steps = 3.1745e-01, PNorm = 100.8382, GNorm = 0.3144
Meta loss on this task batch = 2.9697e-01, Meta loss averaged over last 500 steps = 3.1747e-01, PNorm = 100.8389, GNorm = 0.3122
Meta loss on this task batch = 3.0469e-01, Meta loss averaged over last 500 steps = 3.1745e-01, PNorm = 100.8429, GNorm = 0.2976
Meta loss on this task batch = 3.2734e-01, Meta loss averaged over last 500 steps = 3.1740e-01, PNorm = 100.8480, GNorm = 0.2707
Meta loss on this task batch = 3.3639e-01, Meta loss averaged over last 500 steps = 3.1734e-01, PNorm = 100.8532, GNorm = 0.3768
Took 131.7449414730072 seconds to complete one epoch of meta training
Took 139.86222076416016 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485710
Epoch 291
Meta loss on this task batch = 3.3605e-01, Meta loss averaged over last 500 steps = 3.1741e-01, PNorm = 100.8588, GNorm = 0.3020
Meta loss on this task batch = 3.1967e-01, Meta loss averaged over last 500 steps = 3.1744e-01, PNorm = 100.8648, GNorm = 0.2707
Meta loss on this task batch = 2.9404e-01, Meta loss averaged over last 500 steps = 3.1735e-01, PNorm = 100.8720, GNorm = 0.2942
Meta loss on this task batch = 3.0386e-01, Meta loss averaged over last 500 steps = 3.1731e-01, PNorm = 100.8801, GNorm = 0.2473
Meta loss on this task batch = 3.1724e-01, Meta loss averaged over last 500 steps = 3.1735e-01, PNorm = 100.8849, GNorm = 0.2790
Meta loss on this task batch = 3.2400e-01, Meta loss averaged over last 500 steps = 3.1736e-01, PNorm = 100.8912, GNorm = 0.2781
Meta loss on this task batch = 3.5078e-01, Meta loss averaged over last 500 steps = 3.1751e-01, PNorm = 100.8932, GNorm = 0.3294
Meta loss on this task batch = 3.1067e-01, Meta loss averaged over last 500 steps = 3.1750e-01, PNorm = 100.8945, GNorm = 0.2577
Meta loss on this task batch = 2.8441e-01, Meta loss averaged over last 500 steps = 3.1742e-01, PNorm = 100.8950, GNorm = 0.2719
Meta loss on this task batch = 2.6195e-01, Meta loss averaged over last 500 steps = 3.1724e-01, PNorm = 100.8968, GNorm = 0.2510
Meta loss on this task batch = 3.3133e-01, Meta loss averaged over last 500 steps = 3.1725e-01, PNorm = 100.8969, GNorm = 0.2634
Meta loss on this task batch = 3.1160e-01, Meta loss averaged over last 500 steps = 3.1723e-01, PNorm = 100.8965, GNorm = 0.2769
Meta loss on this task batch = 3.0570e-01, Meta loss averaged over last 500 steps = 3.1726e-01, PNorm = 100.9005, GNorm = 0.2967
Meta loss on this task batch = 2.9886e-01, Meta loss averaged over last 500 steps = 3.1716e-01, PNorm = 100.9052, GNorm = 0.2663
Meta loss on this task batch = 3.1917e-01, Meta loss averaged over last 500 steps = 3.1711e-01, PNorm = 100.9114, GNorm = 0.2759
Meta loss on this task batch = 2.9739e-01, Meta loss averaged over last 500 steps = 3.1714e-01, PNorm = 100.9166, GNorm = 0.2560
Meta loss on this task batch = 3.0652e-01, Meta loss averaged over last 500 steps = 3.1711e-01, PNorm = 100.9236, GNorm = 0.2661
Meta loss on this task batch = 3.3634e-01, Meta loss averaged over last 500 steps = 3.1707e-01, PNorm = 100.9324, GNorm = 0.3284
Meta loss on this task batch = 3.5411e-01, Meta loss averaged over last 500 steps = 3.1719e-01, PNorm = 100.9431, GNorm = 0.3262
Took 130.3413896560669 seconds to complete one epoch of meta training
Took 138.97476387023926 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499893
Epoch 292
Meta loss on this task batch = 2.7873e-01, Meta loss averaged over last 500 steps = 3.1720e-01, PNorm = 100.9557, GNorm = 0.2671
Meta loss on this task batch = 3.0950e-01, Meta loss averaged over last 500 steps = 3.1726e-01, PNorm = 100.9641, GNorm = 0.3178
Meta loss on this task batch = 3.2862e-01, Meta loss averaged over last 500 steps = 3.1728e-01, PNorm = 100.9705, GNorm = 0.2838
Meta loss on this task batch = 3.1376e-01, Meta loss averaged over last 500 steps = 3.1720e-01, PNorm = 100.9765, GNorm = 0.3321
Meta loss on this task batch = 3.0400e-01, Meta loss averaged over last 500 steps = 3.1727e-01, PNorm = 100.9814, GNorm = 0.3089
Meta loss on this task batch = 2.9185e-01, Meta loss averaged over last 500 steps = 3.1712e-01, PNorm = 100.9850, GNorm = 0.2848
Meta loss on this task batch = 3.3734e-01, Meta loss averaged over last 500 steps = 3.1712e-01, PNorm = 100.9866, GNorm = 0.3431
Meta loss on this task batch = 3.5584e-01, Meta loss averaged over last 500 steps = 3.1719e-01, PNorm = 100.9849, GNorm = 0.3684
Meta loss on this task batch = 3.0404e-01, Meta loss averaged over last 500 steps = 3.1702e-01, PNorm = 100.9864, GNorm = 0.3967
Meta loss on this task batch = 3.2896e-01, Meta loss averaged over last 500 steps = 3.1704e-01, PNorm = 100.9903, GNorm = 0.2990
Meta loss on this task batch = 3.6743e-01, Meta loss averaged over last 500 steps = 3.1704e-01, PNorm = 100.9961, GNorm = 0.2935
Meta loss on this task batch = 3.1741e-01, Meta loss averaged over last 500 steps = 3.1712e-01, PNorm = 101.0067, GNorm = 0.3044
Meta loss on this task batch = 2.9837e-01, Meta loss averaged over last 500 steps = 3.1716e-01, PNorm = 101.0206, GNorm = 0.2738
Meta loss on this task batch = 3.4295e-01, Meta loss averaged over last 500 steps = 3.1722e-01, PNorm = 101.0366, GNorm = 0.2803
Meta loss on this task batch = 3.3338e-01, Meta loss averaged over last 500 steps = 3.1728e-01, PNorm = 101.0544, GNorm = 0.2627
Meta loss on this task batch = 3.0257e-01, Meta loss averaged over last 500 steps = 3.1721e-01, PNorm = 101.0735, GNorm = 0.2815
Meta loss on this task batch = 3.1271e-01, Meta loss averaged over last 500 steps = 3.1720e-01, PNorm = 101.0918, GNorm = 0.2786
Meta loss on this task batch = 2.9769e-01, Meta loss averaged over last 500 steps = 3.1729e-01, PNorm = 101.1074, GNorm = 0.2957
Meta loss on this task batch = 3.7624e-01, Meta loss averaged over last 500 steps = 3.1743e-01, PNorm = 101.1185, GNorm = 0.3929
Took 134.07523560523987 seconds to complete one epoch of meta training
Took 142.75079464912415 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494904
Epoch 293
Meta loss on this task batch = 3.3176e-01, Meta loss averaged over last 500 steps = 3.1741e-01, PNorm = 101.1272, GNorm = 0.3012
Meta loss on this task batch = 3.0053e-01, Meta loss averaged over last 500 steps = 3.1737e-01, PNorm = 101.1334, GNorm = 0.3064
Meta loss on this task batch = 3.2760e-01, Meta loss averaged over last 500 steps = 3.1733e-01, PNorm = 101.1377, GNorm = 0.3052
Meta loss on this task batch = 2.5975e-01, Meta loss averaged over last 500 steps = 3.1722e-01, PNorm = 101.1432, GNorm = 0.2588
Meta loss on this task batch = 3.3370e-01, Meta loss averaged over last 500 steps = 3.1733e-01, PNorm = 101.1488, GNorm = 0.2830
Meta loss on this task batch = 3.3969e-01, Meta loss averaged over last 500 steps = 3.1738e-01, PNorm = 101.1555, GNorm = 0.3029
Meta loss on this task batch = 2.9028e-01, Meta loss averaged over last 500 steps = 3.1724e-01, PNorm = 101.1646, GNorm = 0.2616
Meta loss on this task batch = 2.4055e-01, Meta loss averaged over last 500 steps = 3.1707e-01, PNorm = 101.1750, GNorm = 0.2419
Meta loss on this task batch = 2.9070e-01, Meta loss averaged over last 500 steps = 3.1703e-01, PNorm = 101.1848, GNorm = 0.2831
Meta loss on this task batch = 3.9185e-01, Meta loss averaged over last 500 steps = 3.1719e-01, PNorm = 101.1904, GNorm = 0.3627
Meta loss on this task batch = 3.3519e-01, Meta loss averaged over last 500 steps = 3.1722e-01, PNorm = 101.1962, GNorm = 0.3584
Meta loss on this task batch = 3.2300e-01, Meta loss averaged over last 500 steps = 3.1717e-01, PNorm = 101.2034, GNorm = 0.3039
Meta loss on this task batch = 3.4517e-01, Meta loss averaged over last 500 steps = 3.1722e-01, PNorm = 101.2103, GNorm = 0.3188
Meta loss on this task batch = 3.3193e-01, Meta loss averaged over last 500 steps = 3.1728e-01, PNorm = 101.2178, GNorm = 0.3474
Meta loss on this task batch = 2.3321e-01, Meta loss averaged over last 500 steps = 3.1704e-01, PNorm = 101.2279, GNorm = 0.2424
Meta loss on this task batch = 3.7722e-01, Meta loss averaged over last 500 steps = 3.1724e-01, PNorm = 101.2363, GNorm = 0.3311
Meta loss on this task batch = 2.9883e-01, Meta loss averaged over last 500 steps = 3.1727e-01, PNorm = 101.2432, GNorm = 0.2785
Meta loss on this task batch = 3.5445e-01, Meta loss averaged over last 500 steps = 3.1732e-01, PNorm = 101.2494, GNorm = 0.2685
Meta loss on this task batch = 3.2454e-01, Meta loss averaged over last 500 steps = 3.1740e-01, PNorm = 101.2573, GNorm = 0.3688
Took 130.65953397750854 seconds to complete one epoch of meta training
Took 139.0751543045044 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.521405
Epoch 294
Meta loss on this task batch = 3.1882e-01, Meta loss averaged over last 500 steps = 3.1736e-01, PNorm = 101.2642, GNorm = 0.2753
Meta loss on this task batch = 2.9438e-01, Meta loss averaged over last 500 steps = 3.1724e-01, PNorm = 101.2699, GNorm = 0.2651
Meta loss on this task batch = 3.0564e-01, Meta loss averaged over last 500 steps = 3.1710e-01, PNorm = 101.2767, GNorm = 0.2613
Meta loss on this task batch = 2.9353e-01, Meta loss averaged over last 500 steps = 3.1709e-01, PNorm = 101.2827, GNorm = 0.2808
Meta loss on this task batch = 3.2373e-01, Meta loss averaged over last 500 steps = 3.1714e-01, PNorm = 101.2837, GNorm = 0.3369
Meta loss on this task batch = 3.2533e-01, Meta loss averaged over last 500 steps = 3.1724e-01, PNorm = 101.2825, GNorm = 0.3055
Meta loss on this task batch = 3.3653e-01, Meta loss averaged over last 500 steps = 3.1732e-01, PNorm = 101.2781, GNorm = 0.2713
Meta loss on this task batch = 3.1450e-01, Meta loss averaged over last 500 steps = 3.1725e-01, PNorm = 101.2722, GNorm = 0.3297
Meta loss on this task batch = 3.1947e-01, Meta loss averaged over last 500 steps = 3.1732e-01, PNorm = 101.2690, GNorm = 0.3551
Meta loss on this task batch = 3.3700e-01, Meta loss averaged over last 500 steps = 3.1737e-01, PNorm = 101.2673, GNorm = 0.2893
Meta loss on this task batch = 3.5505e-01, Meta loss averaged over last 500 steps = 3.1743e-01, PNorm = 101.2676, GNorm = 0.3240
Meta loss on this task batch = 3.1538e-01, Meta loss averaged over last 500 steps = 3.1748e-01, PNorm = 101.2682, GNorm = 0.2737
Meta loss on this task batch = 3.3089e-01, Meta loss averaged over last 500 steps = 3.1762e-01, PNorm = 101.2725, GNorm = 0.2948
Meta loss on this task batch = 3.0061e-01, Meta loss averaged over last 500 steps = 3.1756e-01, PNorm = 101.2809, GNorm = 0.2842
Meta loss on this task batch = 3.1370e-01, Meta loss averaged over last 500 steps = 3.1745e-01, PNorm = 101.2929, GNorm = 0.2847
Meta loss on this task batch = 2.9651e-01, Meta loss averaged over last 500 steps = 3.1737e-01, PNorm = 101.3054, GNorm = 0.2795
Meta loss on this task batch = 3.3494e-01, Meta loss averaged over last 500 steps = 3.1745e-01, PNorm = 101.3161, GNorm = 0.2798
Meta loss on this task batch = 3.4105e-01, Meta loss averaged over last 500 steps = 3.1748e-01, PNorm = 101.3257, GNorm = 0.2810
Meta loss on this task batch = 3.3424e-01, Meta loss averaged over last 500 steps = 3.1751e-01, PNorm = 101.3341, GNorm = 0.3857
Took 133.45503497123718 seconds to complete one epoch of meta training
Took 141.82983708381653 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490802
Epoch 295
Meta loss on this task batch = 2.7317e-01, Meta loss averaged over last 500 steps = 3.1743e-01, PNorm = 101.3410, GNorm = 0.2802
Meta loss on this task batch = 2.7589e-01, Meta loss averaged over last 500 steps = 3.1731e-01, PNorm = 101.3476, GNorm = 0.2569
Meta loss on this task batch = 3.9997e-01, Meta loss averaged over last 500 steps = 3.1738e-01, PNorm = 101.3525, GNorm = 0.3395
Meta loss on this task batch = 2.8020e-01, Meta loss averaged over last 500 steps = 3.1741e-01, PNorm = 101.3569, GNorm = 0.2520
Meta loss on this task batch = 3.0339e-01, Meta loss averaged over last 500 steps = 3.1732e-01, PNorm = 101.3627, GNorm = 0.2606
Meta loss on this task batch = 3.8109e-01, Meta loss averaged over last 500 steps = 3.1730e-01, PNorm = 101.3703, GNorm = 0.3236
Meta loss on this task batch = 3.5571e-01, Meta loss averaged over last 500 steps = 3.1744e-01, PNorm = 101.3804, GNorm = 0.3255
Meta loss on this task batch = 2.9031e-01, Meta loss averaged over last 500 steps = 3.1722e-01, PNorm = 101.3927, GNorm = 0.3066
Meta loss on this task batch = 3.7527e-01, Meta loss averaged over last 500 steps = 3.1725e-01, PNorm = 101.4072, GNorm = 0.3452
Meta loss on this task batch = 3.1593e-01, Meta loss averaged over last 500 steps = 3.1733e-01, PNorm = 101.4254, GNorm = 0.2851
Meta loss on this task batch = 2.7120e-01, Meta loss averaged over last 500 steps = 3.1727e-01, PNorm = 101.4463, GNorm = 0.3019
Meta loss on this task batch = 2.9950e-01, Meta loss averaged over last 500 steps = 3.1728e-01, PNorm = 101.4671, GNorm = 0.3019
Meta loss on this task batch = 2.5290e-01, Meta loss averaged over last 500 steps = 3.1707e-01, PNorm = 101.4893, GNorm = 0.2765
Meta loss on this task batch = 3.4244e-01, Meta loss averaged over last 500 steps = 3.1707e-01, PNorm = 101.5094, GNorm = 0.3742
Meta loss on this task batch = 3.3107e-01, Meta loss averaged over last 500 steps = 3.1701e-01, PNorm = 101.5257, GNorm = 0.3359
Meta loss on this task batch = 3.4555e-01, Meta loss averaged over last 500 steps = 3.1711e-01, PNorm = 101.5373, GNorm = 0.3510
Meta loss on this task batch = 3.0373e-01, Meta loss averaged over last 500 steps = 3.1698e-01, PNorm = 101.5473, GNorm = 0.2876
Meta loss on this task batch = 3.1411e-01, Meta loss averaged over last 500 steps = 3.1702e-01, PNorm = 101.5541, GNorm = 0.2967
Meta loss on this task batch = 2.6992e-01, Meta loss averaged over last 500 steps = 3.1706e-01, PNorm = 101.5565, GNorm = 0.3738
Took 131.75689578056335 seconds to complete one epoch of meta training
Took 140.4833950996399 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489392
Epoch 296
Meta loss on this task batch = 3.2109e-01, Meta loss averaged over last 500 steps = 3.1706e-01, PNorm = 101.5574, GNorm = 0.3275
Meta loss on this task batch = 2.2982e-01, Meta loss averaged over last 500 steps = 3.1682e-01, PNorm = 101.5566, GNorm = 0.2498
Meta loss on this task batch = 3.0773e-01, Meta loss averaged over last 500 steps = 3.1674e-01, PNorm = 101.5553, GNorm = 0.3025
Meta loss on this task batch = 3.2772e-01, Meta loss averaged over last 500 steps = 3.1684e-01, PNorm = 101.5540, GNorm = 0.3217
Meta loss on this task batch = 3.8918e-01, Meta loss averaged over last 500 steps = 3.1695e-01, PNorm = 101.5513, GNorm = 0.3296
Meta loss on this task batch = 3.4451e-01, Meta loss averaged over last 500 steps = 3.1696e-01, PNorm = 101.5462, GNorm = 0.3237
Meta loss on this task batch = 3.0831e-01, Meta loss averaged over last 500 steps = 3.1699e-01, PNorm = 101.5429, GNorm = 0.2652
Meta loss on this task batch = 3.4345e-01, Meta loss averaged over last 500 steps = 3.1700e-01, PNorm = 101.5408, GNorm = 0.2716
Meta loss on this task batch = 3.2853e-01, Meta loss averaged over last 500 steps = 3.1698e-01, PNorm = 101.5399, GNorm = 0.2918
Meta loss on this task batch = 3.3413e-01, Meta loss averaged over last 500 steps = 3.1699e-01, PNorm = 101.5416, GNorm = 0.2768
Meta loss on this task batch = 3.3953e-01, Meta loss averaged over last 500 steps = 3.1713e-01, PNorm = 101.5474, GNorm = 0.3074
Meta loss on this task batch = 3.1997e-01, Meta loss averaged over last 500 steps = 3.1706e-01, PNorm = 101.5550, GNorm = 0.3099
Meta loss on this task batch = 3.2789e-01, Meta loss averaged over last 500 steps = 3.1709e-01, PNorm = 101.5619, GNorm = 0.3299
Meta loss on this task batch = 2.9259e-01, Meta loss averaged over last 500 steps = 3.1706e-01, PNorm = 101.5705, GNorm = 0.2919
Meta loss on this task batch = 3.7852e-01, Meta loss averaged over last 500 steps = 3.1721e-01, PNorm = 101.5782, GNorm = 0.3138
Meta loss on this task batch = 2.5740e-01, Meta loss averaged over last 500 steps = 3.1712e-01, PNorm = 101.5872, GNorm = 0.2440
Meta loss on this task batch = 3.0714e-01, Meta loss averaged over last 500 steps = 3.1705e-01, PNorm = 101.5980, GNorm = 0.2847
Meta loss on this task batch = 2.6173e-01, Meta loss averaged over last 500 steps = 3.1697e-01, PNorm = 101.6093, GNorm = 0.2533
Meta loss on this task batch = 3.6640e-01, Meta loss averaged over last 500 steps = 3.1701e-01, PNorm = 101.6179, GNorm = 0.3640
Took 131.12194561958313 seconds to complete one epoch of meta training
Took 139.420672416687 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502432
Epoch 297
Meta loss on this task batch = 2.7782e-01, Meta loss averaged over last 500 steps = 3.1703e-01, PNorm = 101.6269, GNorm = 0.2360
Meta loss on this task batch = 3.2265e-01, Meta loss averaged over last 500 steps = 3.1709e-01, PNorm = 101.6332, GNorm = 0.2625
Meta loss on this task batch = 2.6263e-01, Meta loss averaged over last 500 steps = 3.1691e-01, PNorm = 101.6396, GNorm = 0.2647
Meta loss on this task batch = 3.6769e-01, Meta loss averaged over last 500 steps = 3.1702e-01, PNorm = 101.6459, GNorm = 0.3320
Meta loss on this task batch = 3.0864e-01, Meta loss averaged over last 500 steps = 3.1698e-01, PNorm = 101.6488, GNorm = 0.3351
Meta loss on this task batch = 3.5320e-01, Meta loss averaged over last 500 steps = 3.1709e-01, PNorm = 101.6523, GNorm = 0.2954
Meta loss on this task batch = 3.1695e-01, Meta loss averaged over last 500 steps = 3.1713e-01, PNorm = 101.6557, GNorm = 0.2965
Meta loss on this task batch = 3.1363e-01, Meta loss averaged over last 500 steps = 3.1713e-01, PNorm = 101.6599, GNorm = 0.2652
Meta loss on this task batch = 3.6864e-01, Meta loss averaged over last 500 steps = 3.1734e-01, PNorm = 101.6663, GNorm = 0.2913
Meta loss on this task batch = 3.2042e-01, Meta loss averaged over last 500 steps = 3.1737e-01, PNorm = 101.6733, GNorm = 0.2482
Meta loss on this task batch = 3.3046e-01, Meta loss averaged over last 500 steps = 3.1733e-01, PNorm = 101.6812, GNorm = 0.2385
Meta loss on this task batch = 2.8300e-01, Meta loss averaged over last 500 steps = 3.1736e-01, PNorm = 101.6906, GNorm = 0.2615
Meta loss on this task batch = 3.3606e-01, Meta loss averaged over last 500 steps = 3.1729e-01, PNorm = 101.7000, GNorm = 0.2881
Meta loss on this task batch = 2.9730e-01, Meta loss averaged over last 500 steps = 3.1724e-01, PNorm = 101.7075, GNorm = 0.3269
Meta loss on this task batch = 3.1629e-01, Meta loss averaged over last 500 steps = 3.1718e-01, PNorm = 101.7145, GNorm = 0.2735
Meta loss on this task batch = 3.0086e-01, Meta loss averaged over last 500 steps = 3.1712e-01, PNorm = 101.7231, GNorm = 0.3163
Meta loss on this task batch = 2.7606e-01, Meta loss averaged over last 500 steps = 3.1695e-01, PNorm = 101.7310, GNorm = 0.2815
Meta loss on this task batch = 3.2453e-01, Meta loss averaged over last 500 steps = 3.1692e-01, PNorm = 101.7384, GNorm = 0.3071
Meta loss on this task batch = 2.9732e-01, Meta loss averaged over last 500 steps = 3.1690e-01, PNorm = 101.7461, GNorm = 0.3402
Took 133.4986228942871 seconds to complete one epoch of meta training
Took 142.0305118560791 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511714
Epoch 298
Meta loss on this task batch = 3.7965e-01, Meta loss averaged over last 500 steps = 3.1702e-01, PNorm = 101.7537, GNorm = 0.2755
Meta loss on this task batch = 2.7155e-01, Meta loss averaged over last 500 steps = 3.1694e-01, PNorm = 101.7629, GNorm = 0.2529
Meta loss on this task batch = 2.9909e-01, Meta loss averaged over last 500 steps = 3.1690e-01, PNorm = 101.7718, GNorm = 0.2670
Meta loss on this task batch = 2.9766e-01, Meta loss averaged over last 500 steps = 3.1683e-01, PNorm = 101.7794, GNorm = 0.2769
Meta loss on this task batch = 3.2557e-01, Meta loss averaged over last 500 steps = 3.1689e-01, PNorm = 101.7847, GNorm = 0.2948
Meta loss on this task batch = 2.9823e-01, Meta loss averaged over last 500 steps = 3.1689e-01, PNorm = 101.7880, GNorm = 0.3162
Meta loss on this task batch = 3.1887e-01, Meta loss averaged over last 500 steps = 3.1691e-01, PNorm = 101.7881, GNorm = 0.3041
Meta loss on this task batch = 3.2047e-01, Meta loss averaged over last 500 steps = 3.1694e-01, PNorm = 101.7897, GNorm = 0.3089
Meta loss on this task batch = 2.8752e-01, Meta loss averaged over last 500 steps = 3.1692e-01, PNorm = 101.7900, GNorm = 0.3049
Meta loss on this task batch = 2.8842e-01, Meta loss averaged over last 500 steps = 3.1680e-01, PNorm = 101.7883, GNorm = 0.2888
Meta loss on this task batch = 3.0237e-01, Meta loss averaged over last 500 steps = 3.1675e-01, PNorm = 101.7889, GNorm = 0.2841
Meta loss on this task batch = 2.9031e-01, Meta loss averaged over last 500 steps = 3.1677e-01, PNorm = 101.7883, GNorm = 0.2897
Meta loss on this task batch = 2.6237e-01, Meta loss averaged over last 500 steps = 3.1655e-01, PNorm = 101.7885, GNorm = 0.2967
Meta loss on this task batch = 3.7147e-01, Meta loss averaged over last 500 steps = 3.1663e-01, PNorm = 101.7862, GNorm = 0.3565
Meta loss on this task batch = 3.4933e-01, Meta loss averaged over last 500 steps = 3.1679e-01, PNorm = 101.7853, GNorm = 0.4153
Meta loss on this task batch = 2.7867e-01, Meta loss averaged over last 500 steps = 3.1678e-01, PNorm = 101.7885, GNorm = 0.3047
Meta loss on this task batch = 2.6310e-01, Meta loss averaged over last 500 steps = 3.1663e-01, PNorm = 101.7960, GNorm = 0.2786
Meta loss on this task batch = 3.0556e-01, Meta loss averaged over last 500 steps = 3.1664e-01, PNorm = 101.8045, GNorm = 0.2701
Meta loss on this task batch = 3.1358e-01, Meta loss averaged over last 500 steps = 3.1646e-01, PNorm = 101.8131, GNorm = 0.3231
Took 133.04121589660645 seconds to complete one epoch of meta training
Took 140.31087851524353 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480856
Epoch 299
Meta loss on this task batch = 2.7896e-01, Meta loss averaged over last 500 steps = 3.1639e-01, PNorm = 101.8235, GNorm = 0.3229
Meta loss on this task batch = 2.7100e-01, Meta loss averaged over last 500 steps = 3.1624e-01, PNorm = 101.8369, GNorm = 0.3184
Meta loss on this task batch = 3.2663e-01, Meta loss averaged over last 500 steps = 3.1634e-01, PNorm = 101.8502, GNorm = 0.2625
Meta loss on this task batch = 2.8000e-01, Meta loss averaged over last 500 steps = 3.1630e-01, PNorm = 101.8621, GNorm = 0.2597
Meta loss on this task batch = 3.2727e-01, Meta loss averaged over last 500 steps = 3.1637e-01, PNorm = 101.8714, GNorm = 0.2988
Meta loss on this task batch = 3.1822e-01, Meta loss averaged over last 500 steps = 3.1644e-01, PNorm = 101.8784, GNorm = 0.3568
Meta loss on this task batch = 3.0276e-01, Meta loss averaged over last 500 steps = 3.1650e-01, PNorm = 101.8846, GNorm = 0.2748
Meta loss on this task batch = 3.0022e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 101.8877, GNorm = 0.2919
Meta loss on this task batch = 3.4655e-01, Meta loss averaged over last 500 steps = 3.1651e-01, PNorm = 101.8894, GNorm = 0.2889
Meta loss on this task batch = 3.2593e-01, Meta loss averaged over last 500 steps = 3.1658e-01, PNorm = 101.8903, GNorm = 0.2957
Meta loss on this task batch = 3.2983e-01, Meta loss averaged over last 500 steps = 3.1661e-01, PNorm = 101.8939, GNorm = 0.3488
Meta loss on this task batch = 3.1117e-01, Meta loss averaged over last 500 steps = 3.1664e-01, PNorm = 101.8973, GNorm = 0.2995
Meta loss on this task batch = 3.1675e-01, Meta loss averaged over last 500 steps = 3.1668e-01, PNorm = 101.9007, GNorm = 0.2733
Meta loss on this task batch = 3.2922e-01, Meta loss averaged over last 500 steps = 3.1663e-01, PNorm = 101.9068, GNorm = 0.3945
Meta loss on this task batch = 3.5866e-01, Meta loss averaged over last 500 steps = 3.1670e-01, PNorm = 101.9155, GNorm = 0.2796
Meta loss on this task batch = 2.9831e-01, Meta loss averaged over last 500 steps = 3.1663e-01, PNorm = 101.9254, GNorm = 0.2390
Meta loss on this task batch = 3.6896e-01, Meta loss averaged over last 500 steps = 3.1663e-01, PNorm = 101.9344, GNorm = 0.3392
Meta loss on this task batch = 3.2382e-01, Meta loss averaged over last 500 steps = 3.1663e-01, PNorm = 101.9442, GNorm = 0.3365
Meta loss on this task batch = 2.7452e-01, Meta loss averaged over last 500 steps = 3.1665e-01, PNorm = 101.9527, GNorm = 0.3074
Took 134.8829402923584 seconds to complete one epoch of meta training
Took 143.42238998413086 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510105
Epoch 300
Meta loss on this task batch = 3.4014e-01, Meta loss averaged over last 500 steps = 3.1664e-01, PNorm = 101.9566, GNorm = 0.2883
Meta loss on this task batch = 2.8814e-01, Meta loss averaged over last 500 steps = 3.1661e-01, PNorm = 101.9613, GNorm = 0.2464
Meta loss on this task batch = 2.7954e-01, Meta loss averaged over last 500 steps = 3.1648e-01, PNorm = 101.9678, GNorm = 0.2443
Meta loss on this task batch = 3.5331e-01, Meta loss averaged over last 500 steps = 3.1647e-01, PNorm = 101.9716, GNorm = 0.2795
Meta loss on this task batch = 3.7334e-01, Meta loss averaged over last 500 steps = 3.1661e-01, PNorm = 101.9749, GNorm = 0.3356
Meta loss on this task batch = 3.2366e-01, Meta loss averaged over last 500 steps = 3.1658e-01, PNorm = 101.9810, GNorm = 0.2743
Meta loss on this task batch = 2.9902e-01, Meta loss averaged over last 500 steps = 3.1660e-01, PNorm = 101.9884, GNorm = 0.2607
Meta loss on this task batch = 3.6133e-01, Meta loss averaged over last 500 steps = 3.1670e-01, PNorm = 101.9940, GNorm = 0.3202
Meta loss on this task batch = 2.3779e-01, Meta loss averaged over last 500 steps = 3.1650e-01, PNorm = 102.0010, GNorm = 0.2391
Meta loss on this task batch = 2.4286e-01, Meta loss averaged over last 500 steps = 3.1629e-01, PNorm = 102.0099, GNorm = 0.2415
Meta loss on this task batch = 3.7143e-01, Meta loss averaged over last 500 steps = 3.1643e-01, PNorm = 102.0173, GNorm = 0.3469
Meta loss on this task batch = 3.3881e-01, Meta loss averaged over last 500 steps = 3.1649e-01, PNorm = 102.0234, GNorm = 0.3009
Meta loss on this task batch = 3.1320e-01, Meta loss averaged over last 500 steps = 3.1646e-01, PNorm = 102.0294, GNorm = 0.2667
Meta loss on this task batch = 3.0423e-01, Meta loss averaged over last 500 steps = 3.1634e-01, PNorm = 102.0331, GNorm = 0.2700
Meta loss on this task batch = 2.8604e-01, Meta loss averaged over last 500 steps = 3.1625e-01, PNorm = 102.0388, GNorm = 0.2782
Meta loss on this task batch = 3.2788e-01, Meta loss averaged over last 500 steps = 3.1629e-01, PNorm = 102.0451, GNorm = 0.2914
Meta loss on this task batch = 3.3274e-01, Meta loss averaged over last 500 steps = 3.1638e-01, PNorm = 102.0509, GNorm = 0.2877
Meta loss on this task batch = 2.6166e-01, Meta loss averaged over last 500 steps = 3.1617e-01, PNorm = 102.0573, GNorm = 0.2543
Meta loss on this task batch = 3.8437e-01, Meta loss averaged over last 500 steps = 3.1636e-01, PNorm = 102.0640, GNorm = 0.3811
Took 130.28149604797363 seconds to complete one epoch of meta training
Took 138.99223709106445 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496877
Epoch 301
Meta loss on this task batch = 3.3558e-01, Meta loss averaged over last 500 steps = 3.1643e-01, PNorm = 102.0684, GNorm = 0.4046
Meta loss on this task batch = 2.9788e-01, Meta loss averaged over last 500 steps = 3.1646e-01, PNorm = 102.0706, GNorm = 0.3135
Meta loss on this task batch = 3.3452e-01, Meta loss averaged over last 500 steps = 3.1649e-01, PNorm = 102.0722, GNorm = 0.3610
Meta loss on this task batch = 3.5232e-01, Meta loss averaged over last 500 steps = 3.1659e-01, PNorm = 102.0743, GNorm = 0.2962
Meta loss on this task batch = 2.9958e-01, Meta loss averaged over last 500 steps = 3.1649e-01, PNorm = 102.0790, GNorm = 0.2560
Meta loss on this task batch = 2.8566e-01, Meta loss averaged over last 500 steps = 3.1643e-01, PNorm = 102.0879, GNorm = 0.3259
Meta loss on this task batch = 2.5981e-01, Meta loss averaged over last 500 steps = 3.1635e-01, PNorm = 102.0994, GNorm = 0.2792
Meta loss on this task batch = 2.8249e-01, Meta loss averaged over last 500 steps = 3.1623e-01, PNorm = 102.1132, GNorm = 0.2747
Meta loss on this task batch = 3.2964e-01, Meta loss averaged over last 500 steps = 3.1627e-01, PNorm = 102.1301, GNorm = 0.3369
Meta loss on this task batch = 2.8152e-01, Meta loss averaged over last 500 steps = 3.1616e-01, PNorm = 102.1471, GNorm = 0.2763
Meta loss on this task batch = 3.3290e-01, Meta loss averaged over last 500 steps = 3.1616e-01, PNorm = 102.1604, GNorm = 0.3172
Meta loss on this task batch = 3.2249e-01, Meta loss averaged over last 500 steps = 3.1620e-01, PNorm = 102.1682, GNorm = 0.3962
Meta loss on this task batch = 3.5716e-01, Meta loss averaged over last 500 steps = 3.1621e-01, PNorm = 102.1732, GNorm = 0.3295
Meta loss on this task batch = 3.3816e-01, Meta loss averaged over last 500 steps = 3.1638e-01, PNorm = 102.1785, GNorm = 0.3166
Meta loss on this task batch = 3.4668e-01, Meta loss averaged over last 500 steps = 3.1645e-01, PNorm = 102.1825, GNorm = 0.3666
Meta loss on this task batch = 3.0874e-01, Meta loss averaged over last 500 steps = 3.1640e-01, PNorm = 102.1887, GNorm = 0.4018
Meta loss on this task batch = 3.6560e-01, Meta loss averaged over last 500 steps = 3.1654e-01, PNorm = 102.1947, GNorm = 0.2964
Meta loss on this task batch = 3.1010e-01, Meta loss averaged over last 500 steps = 3.1650e-01, PNorm = 102.1999, GNorm = 0.2870
Meta loss on this task batch = 2.9473e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 102.2079, GNorm = 0.3285
Took 130.97461199760437 seconds to complete one epoch of meta training
Took 138.55201029777527 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483499
Epoch 302
Meta loss on this task batch = 3.3171e-01, Meta loss averaged over last 500 steps = 3.1647e-01, PNorm = 102.2193, GNorm = 0.2887
Meta loss on this task batch = 2.9658e-01, Meta loss averaged over last 500 steps = 3.1636e-01, PNorm = 102.2344, GNorm = 0.2775
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 3.1636e-01, PNorm = 102.2514, GNorm = 0.2522
Meta loss on this task batch = 2.7506e-01, Meta loss averaged over last 500 steps = 3.1625e-01, PNorm = 102.2688, GNorm = 0.2538
Meta loss on this task batch = 3.0538e-01, Meta loss averaged over last 500 steps = 3.1628e-01, PNorm = 102.2876, GNorm = 0.2981
Meta loss on this task batch = 3.9077e-01, Meta loss averaged over last 500 steps = 3.1649e-01, PNorm = 102.3048, GNorm = 0.3502
Meta loss on this task batch = 3.4950e-01, Meta loss averaged over last 500 steps = 3.1654e-01, PNorm = 102.3218, GNorm = 0.2726
Meta loss on this task batch = 2.6389e-01, Meta loss averaged over last 500 steps = 3.1648e-01, PNorm = 102.3386, GNorm = 0.2426
Meta loss on this task batch = 2.9477e-01, Meta loss averaged over last 500 steps = 3.1641e-01, PNorm = 102.3538, GNorm = 0.2953
Meta loss on this task batch = 3.0807e-01, Meta loss averaged over last 500 steps = 3.1640e-01, PNorm = 102.3647, GNorm = 0.3496
Meta loss on this task batch = 3.1830e-01, Meta loss averaged over last 500 steps = 3.1634e-01, PNorm = 102.3731, GNorm = 0.3077
Meta loss on this task batch = 3.0348e-01, Meta loss averaged over last 500 steps = 3.1629e-01, PNorm = 102.3752, GNorm = 0.3290
Meta loss on this task batch = 3.5485e-01, Meta loss averaged over last 500 steps = 3.1629e-01, PNorm = 102.3718, GNorm = 0.3966
Meta loss on this task batch = 3.1131e-01, Meta loss averaged over last 500 steps = 3.1641e-01, PNorm = 102.3680, GNorm = 0.3253
Meta loss on this task batch = 3.6244e-01, Meta loss averaged over last 500 steps = 3.1654e-01, PNorm = 102.3655, GNorm = 0.2763
Meta loss on this task batch = 2.9888e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 102.3636, GNorm = 0.2675
Meta loss on this task batch = 3.2863e-01, Meta loss averaged over last 500 steps = 3.1650e-01, PNorm = 102.3642, GNorm = 0.3287
Meta loss on this task batch = 3.2281e-01, Meta loss averaged over last 500 steps = 3.1645e-01, PNorm = 102.3668, GNorm = 0.2796
Meta loss on this task batch = 2.7636e-01, Meta loss averaged over last 500 steps = 3.1635e-01, PNorm = 102.3718, GNorm = 0.3080
Took 130.6101770401001 seconds to complete one epoch of meta training
Took 138.78117609024048 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478332
Epoch 303
Meta loss on this task batch = 2.4736e-01, Meta loss averaged over last 500 steps = 3.1617e-01, PNorm = 102.3827, GNorm = 0.2911
Meta loss on this task batch = 3.2881e-01, Meta loss averaged over last 500 steps = 3.1616e-01, PNorm = 102.3965, GNorm = 0.3384
Meta loss on this task batch = 3.3244e-01, Meta loss averaged over last 500 steps = 3.1620e-01, PNorm = 102.4116, GNorm = 0.3013
Meta loss on this task batch = 3.0685e-01, Meta loss averaged over last 500 steps = 3.1619e-01, PNorm = 102.4292, GNorm = 0.3016
Meta loss on this task batch = 3.6528e-01, Meta loss averaged over last 500 steps = 3.1627e-01, PNorm = 102.4443, GNorm = 0.3519
Meta loss on this task batch = 3.6853e-01, Meta loss averaged over last 500 steps = 3.1651e-01, PNorm = 102.4569, GNorm = 0.3654
Meta loss on this task batch = 3.4123e-01, Meta loss averaged over last 500 steps = 3.1650e-01, PNorm = 102.4645, GNorm = 0.3853
Meta loss on this task batch = 2.5917e-01, Meta loss averaged over last 500 steps = 3.1638e-01, PNorm = 102.4690, GNorm = 0.3256
Meta loss on this task batch = 3.0474e-01, Meta loss averaged over last 500 steps = 3.1647e-01, PNorm = 102.4727, GNorm = 0.3122
Meta loss on this task batch = 3.3523e-01, Meta loss averaged over last 500 steps = 3.1651e-01, PNorm = 102.4761, GNorm = 0.3433
Meta loss on this task batch = 3.5198e-01, Meta loss averaged over last 500 steps = 3.1660e-01, PNorm = 102.4805, GNorm = 0.3004
Meta loss on this task batch = 2.5392e-01, Meta loss averaged over last 500 steps = 3.1645e-01, PNorm = 102.4868, GNorm = 0.2571
Meta loss on this task batch = 3.2288e-01, Meta loss averaged over last 500 steps = 3.1641e-01, PNorm = 102.4946, GNorm = 0.2602
Meta loss on this task batch = 2.8734e-01, Meta loss averaged over last 500 steps = 3.1638e-01, PNorm = 102.5025, GNorm = 0.2423
Meta loss on this task batch = 3.2244e-01, Meta loss averaged over last 500 steps = 3.1642e-01, PNorm = 102.5108, GNorm = 0.2726
Meta loss on this task batch = 3.3821e-01, Meta loss averaged over last 500 steps = 3.1644e-01, PNorm = 102.5193, GNorm = 0.2778
Meta loss on this task batch = 3.4808e-01, Meta loss averaged over last 500 steps = 3.1655e-01, PNorm = 102.5286, GNorm = 0.2767
Meta loss on this task batch = 3.2897e-01, Meta loss averaged over last 500 steps = 3.1655e-01, PNorm = 102.5359, GNorm = 0.3142
Meta loss on this task batch = 3.1963e-01, Meta loss averaged over last 500 steps = 3.1638e-01, PNorm = 102.5437, GNorm = 0.3199
Took 131.66097283363342 seconds to complete one epoch of meta training
Took 139.92267537117004 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499506
Epoch 304
Meta loss on this task batch = 2.5791e-01, Meta loss averaged over last 500 steps = 3.1625e-01, PNorm = 102.5501, GNorm = 0.2729
Meta loss on this task batch = 3.7652e-01, Meta loss averaged over last 500 steps = 3.1634e-01, PNorm = 102.5527, GNorm = 0.3307
Meta loss on this task batch = 2.9458e-01, Meta loss averaged over last 500 steps = 3.1622e-01, PNorm = 102.5525, GNorm = 0.2727
Meta loss on this task batch = 3.0046e-01, Meta loss averaged over last 500 steps = 3.1625e-01, PNorm = 102.5509, GNorm = 0.2694
Meta loss on this task batch = 3.2414e-01, Meta loss averaged over last 500 steps = 3.1629e-01, PNorm = 102.5470, GNorm = 0.2980
Meta loss on this task batch = 3.2650e-01, Meta loss averaged over last 500 steps = 3.1645e-01, PNorm = 102.5417, GNorm = 0.3065
Meta loss on this task batch = 3.2654e-01, Meta loss averaged over last 500 steps = 3.1649e-01, PNorm = 102.5394, GNorm = 0.2810
Meta loss on this task batch = 2.6318e-01, Meta loss averaged over last 500 steps = 3.1647e-01, PNorm = 102.5384, GNorm = 0.2711
Meta loss on this task batch = 3.3933e-01, Meta loss averaged over last 500 steps = 3.1650e-01, PNorm = 102.5401, GNorm = 0.3243
Meta loss on this task batch = 3.1946e-01, Meta loss averaged over last 500 steps = 3.1652e-01, PNorm = 102.5433, GNorm = 0.3030
Meta loss on this task batch = 3.4207e-01, Meta loss averaged over last 500 steps = 3.1642e-01, PNorm = 102.5494, GNorm = 0.2818
Meta loss on this task batch = 3.0172e-01, Meta loss averaged over last 500 steps = 3.1644e-01, PNorm = 102.5564, GNorm = 0.2761
Meta loss on this task batch = 2.7864e-01, Meta loss averaged over last 500 steps = 3.1641e-01, PNorm = 102.5653, GNorm = 0.2617
Meta loss on this task batch = 3.1021e-01, Meta loss averaged over last 500 steps = 3.1641e-01, PNorm = 102.5740, GNorm = 0.2752
Meta loss on this task batch = 3.2079e-01, Meta loss averaged over last 500 steps = 3.1640e-01, PNorm = 102.5817, GNorm = 0.2521
Meta loss on this task batch = 2.9458e-01, Meta loss averaged over last 500 steps = 3.1632e-01, PNorm = 102.5898, GNorm = 0.2564
Meta loss on this task batch = 3.2322e-01, Meta loss averaged over last 500 steps = 3.1630e-01, PNorm = 102.5966, GNorm = 0.2963
Meta loss on this task batch = 3.1558e-01, Meta loss averaged over last 500 steps = 3.1640e-01, PNorm = 102.6034, GNorm = 0.3272
Meta loss on this task batch = 2.7959e-01, Meta loss averaged over last 500 steps = 3.1635e-01, PNorm = 102.6111, GNorm = 0.3108
Took 133.09307169914246 seconds to complete one epoch of meta training
Took 141.6174898147583 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497401
Epoch 305
Meta loss on this task batch = 2.7043e-01, Meta loss averaged over last 500 steps = 3.1629e-01, PNorm = 102.6193, GNorm = 0.2885
Meta loss on this task batch = 3.2611e-01, Meta loss averaged over last 500 steps = 3.1619e-01, PNorm = 102.6234, GNorm = 0.3221
Meta loss on this task batch = 3.0760e-01, Meta loss averaged over last 500 steps = 3.1614e-01, PNorm = 102.6257, GNorm = 0.2995
Meta loss on this task batch = 2.8197e-01, Meta loss averaged over last 500 steps = 3.1603e-01, PNorm = 102.6281, GNorm = 0.2515
Meta loss on this task batch = 3.9523e-01, Meta loss averaged over last 500 steps = 3.1601e-01, PNorm = 102.6278, GNorm = 0.3488
Meta loss on this task batch = 2.8963e-01, Meta loss averaged over last 500 steps = 3.1595e-01, PNorm = 102.6293, GNorm = 0.2813
Meta loss on this task batch = 3.1475e-01, Meta loss averaged over last 500 steps = 3.1597e-01, PNorm = 102.6316, GNorm = 0.3091
Meta loss on this task batch = 3.3954e-01, Meta loss averaged over last 500 steps = 3.1614e-01, PNorm = 102.6332, GNorm = 0.2721
Meta loss on this task batch = 3.2569e-01, Meta loss averaged over last 500 steps = 3.1620e-01, PNorm = 102.6361, GNorm = 0.3125
Meta loss on this task batch = 3.4178e-01, Meta loss averaged over last 500 steps = 3.1628e-01, PNorm = 102.6410, GNorm = 0.2933
Meta loss on this task batch = 3.1797e-01, Meta loss averaged over last 500 steps = 3.1626e-01, PNorm = 102.6472, GNorm = 0.3286
Meta loss on this task batch = 3.6205e-01, Meta loss averaged over last 500 steps = 3.1627e-01, PNorm = 102.6569, GNorm = 0.3505
Meta loss on this task batch = 3.6948e-01, Meta loss averaged over last 500 steps = 3.1628e-01, PNorm = 102.6670, GNorm = 0.3413
Meta loss on this task batch = 3.2579e-01, Meta loss averaged over last 500 steps = 3.1632e-01, PNorm = 102.6802, GNorm = 0.2784
Meta loss on this task batch = 2.4035e-01, Meta loss averaged over last 500 steps = 3.1620e-01, PNorm = 102.6935, GNorm = 0.2739
Meta loss on this task batch = 2.7884e-01, Meta loss averaged over last 500 steps = 3.1613e-01, PNorm = 102.7041, GNorm = 0.2755
Meta loss on this task batch = 3.1820e-01, Meta loss averaged over last 500 steps = 3.1622e-01, PNorm = 102.7127, GNorm = 0.2670
Meta loss on this task batch = 3.3657e-01, Meta loss averaged over last 500 steps = 3.1617e-01, PNorm = 102.7174, GNorm = 0.3750
Meta loss on this task batch = 3.3010e-01, Meta loss averaged over last 500 steps = 3.1618e-01, PNorm = 102.7198, GNorm = 0.3267
Took 132.45558977127075 seconds to complete one epoch of meta training
Took 140.63831663131714 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470634
Epoch 306
Meta loss on this task batch = 3.4703e-01, Meta loss averaged over last 500 steps = 3.1622e-01, PNorm = 102.7214, GNorm = 0.3234
Meta loss on this task batch = 3.4490e-01, Meta loss averaged over last 500 steps = 3.1638e-01, PNorm = 102.7229, GNorm = 0.2921
Meta loss on this task batch = 2.8527e-01, Meta loss averaged over last 500 steps = 3.1624e-01, PNorm = 102.7246, GNorm = 0.2640
Meta loss on this task batch = 2.7695e-01, Meta loss averaged over last 500 steps = 3.1629e-01, PNorm = 102.7253, GNorm = 0.2867
Meta loss on this task batch = 2.5262e-01, Meta loss averaged over last 500 steps = 3.1616e-01, PNorm = 102.7265, GNorm = 0.2629
Meta loss on this task batch = 3.0878e-01, Meta loss averaged over last 500 steps = 3.1607e-01, PNorm = 102.7297, GNorm = 0.2716
Meta loss on this task batch = 3.2771e-01, Meta loss averaged over last 500 steps = 3.1613e-01, PNorm = 102.7336, GNorm = 0.2717
Meta loss on this task batch = 3.3691e-01, Meta loss averaged over last 500 steps = 3.1623e-01, PNorm = 102.7391, GNorm = 0.3076
Meta loss on this task batch = 3.2155e-01, Meta loss averaged over last 500 steps = 3.1628e-01, PNorm = 102.7469, GNorm = 0.2605
Meta loss on this task batch = 3.4361e-01, Meta loss averaged over last 500 steps = 3.1634e-01, PNorm = 102.7566, GNorm = 0.2948
Meta loss on this task batch = 3.2952e-01, Meta loss averaged over last 500 steps = 3.1643e-01, PNorm = 102.7666, GNorm = 0.2699
Meta loss on this task batch = 3.0366e-01, Meta loss averaged over last 500 steps = 3.1641e-01, PNorm = 102.7780, GNorm = 0.2873
Meta loss on this task batch = 3.3051e-01, Meta loss averaged over last 500 steps = 3.1652e-01, PNorm = 102.7887, GNorm = 0.2685
Meta loss on this task batch = 3.2040e-01, Meta loss averaged over last 500 steps = 3.1650e-01, PNorm = 102.7996, GNorm = 0.3017
Meta loss on this task batch = 3.2183e-01, Meta loss averaged over last 500 steps = 3.1654e-01, PNorm = 102.8129, GNorm = 0.3119
Meta loss on this task batch = 2.8882e-01, Meta loss averaged over last 500 steps = 3.1636e-01, PNorm = 102.8267, GNorm = 0.2820
Meta loss on this task batch = 3.3708e-01, Meta loss averaged over last 500 steps = 3.1633e-01, PNorm = 102.8383, GNorm = 0.3031
Meta loss on this task batch = 2.4938e-01, Meta loss averaged over last 500 steps = 3.1610e-01, PNorm = 102.8490, GNorm = 0.2499
Meta loss on this task batch = 3.5312e-01, Meta loss averaged over last 500 steps = 3.1618e-01, PNorm = 102.8582, GNorm = 0.3516
Took 130.74923872947693 seconds to complete one epoch of meta training
Took 139.3990216255188 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479594
Epoch 307
Meta loss on this task batch = 2.7935e-01, Meta loss averaged over last 500 steps = 3.1601e-01, PNorm = 102.8632, GNorm = 0.2548
Meta loss on this task batch = 3.0966e-01, Meta loss averaged over last 500 steps = 3.1595e-01, PNorm = 102.8628, GNorm = 0.3549
Meta loss on this task batch = 2.7897e-01, Meta loss averaged over last 500 steps = 3.1586e-01, PNorm = 102.8631, GNorm = 0.2632
Meta loss on this task batch = 3.4468e-01, Meta loss averaged over last 500 steps = 3.1593e-01, PNorm = 102.8631, GNorm = 0.3075
Meta loss on this task batch = 3.1441e-01, Meta loss averaged over last 500 steps = 3.1589e-01, PNorm = 102.8653, GNorm = 0.2885
Meta loss on this task batch = 3.1733e-01, Meta loss averaged over last 500 steps = 3.1589e-01, PNorm = 102.8663, GNorm = 0.2924
Meta loss on this task batch = 3.2619e-01, Meta loss averaged over last 500 steps = 3.1579e-01, PNorm = 102.8641, GNorm = 0.3095
Meta loss on this task batch = 3.5070e-01, Meta loss averaged over last 500 steps = 3.1596e-01, PNorm = 102.8627, GNorm = 0.3395
Meta loss on this task batch = 2.9754e-01, Meta loss averaged over last 500 steps = 3.1607e-01, PNorm = 102.8636, GNorm = 0.2766
Meta loss on this task batch = 3.5584e-01, Meta loss averaged over last 500 steps = 3.1609e-01, PNorm = 102.8665, GNorm = 0.2828
Meta loss on this task batch = 2.9921e-01, Meta loss averaged over last 500 steps = 3.1611e-01, PNorm = 102.8703, GNorm = 0.2631
Meta loss on this task batch = 2.6735e-01, Meta loss averaged over last 500 steps = 3.1600e-01, PNorm = 102.8762, GNorm = 0.2691
Meta loss on this task batch = 3.1343e-01, Meta loss averaged over last 500 steps = 3.1590e-01, PNorm = 102.8836, GNorm = 0.2844
Meta loss on this task batch = 3.4773e-01, Meta loss averaged over last 500 steps = 3.1605e-01, PNorm = 102.8903, GNorm = 0.3039
Meta loss on this task batch = 3.1015e-01, Meta loss averaged over last 500 steps = 3.1614e-01, PNorm = 102.8963, GNorm = 0.2830
Meta loss on this task batch = 2.6275e-01, Meta loss averaged over last 500 steps = 3.1608e-01, PNorm = 102.9024, GNorm = 0.2561
Meta loss on this task batch = 3.0895e-01, Meta loss averaged over last 500 steps = 3.1602e-01, PNorm = 102.9078, GNorm = 0.3009
Meta loss on this task batch = 2.8747e-01, Meta loss averaged over last 500 steps = 3.1600e-01, PNorm = 102.9157, GNorm = 0.2707
Meta loss on this task batch = 2.8631e-01, Meta loss averaged over last 500 steps = 3.1596e-01, PNorm = 102.9253, GNorm = 0.3972
Took 135.10588908195496 seconds to complete one epoch of meta training
Took 143.87223172187805 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490887
Epoch 308
Meta loss on this task batch = 3.3578e-01, Meta loss averaged over last 500 steps = 3.1597e-01, PNorm = 102.9317, GNorm = 0.2664
Meta loss on this task batch = 3.1975e-01, Meta loss averaged over last 500 steps = 3.1593e-01, PNorm = 102.9378, GNorm = 0.2929
Meta loss on this task batch = 2.5599e-01, Meta loss averaged over last 500 steps = 3.1586e-01, PNorm = 102.9445, GNorm = 0.2789
Meta loss on this task batch = 2.7997e-01, Meta loss averaged over last 500 steps = 3.1572e-01, PNorm = 102.9468, GNorm = 0.3145
Meta loss on this task batch = 3.1705e-01, Meta loss averaged over last 500 steps = 3.1566e-01, PNorm = 102.9459, GNorm = 0.3347
Meta loss on this task batch = 2.9266e-01, Meta loss averaged over last 500 steps = 3.1554e-01, PNorm = 102.9442, GNorm = 0.2935
Meta loss on this task batch = 3.6493e-01, Meta loss averaged over last 500 steps = 3.1565e-01, PNorm = 102.9410, GNorm = 0.2970
Meta loss on this task batch = 3.3482e-01, Meta loss averaged over last 500 steps = 3.1566e-01, PNorm = 102.9367, GNorm = 0.3074
Meta loss on this task batch = 3.3117e-01, Meta loss averaged over last 500 steps = 3.1577e-01, PNorm = 102.9356, GNorm = 0.2967
Meta loss on this task batch = 3.0837e-01, Meta loss averaged over last 500 steps = 3.1578e-01, PNorm = 102.9362, GNorm = 0.2727
Meta loss on this task batch = 2.9977e-01, Meta loss averaged over last 500 steps = 3.1576e-01, PNorm = 102.9406, GNorm = 0.3036
Meta loss on this task batch = 3.0151e-01, Meta loss averaged over last 500 steps = 3.1570e-01, PNorm = 102.9483, GNorm = 0.2937
Meta loss on this task batch = 3.1153e-01, Meta loss averaged over last 500 steps = 3.1571e-01, PNorm = 102.9580, GNorm = 0.2941
Meta loss on this task batch = 2.9966e-01, Meta loss averaged over last 500 steps = 3.1563e-01, PNorm = 102.9675, GNorm = 0.2635
Meta loss on this task batch = 3.3724e-01, Meta loss averaged over last 500 steps = 3.1576e-01, PNorm = 102.9765, GNorm = 0.4168
Meta loss on this task batch = 3.4367e-01, Meta loss averaged over last 500 steps = 3.1575e-01, PNorm = 102.9835, GNorm = 0.3097
Meta loss on this task batch = 3.4586e-01, Meta loss averaged over last 500 steps = 3.1579e-01, PNorm = 102.9885, GNorm = 0.2987
Meta loss on this task batch = 2.6220e-01, Meta loss averaged over last 500 steps = 3.1570e-01, PNorm = 102.9945, GNorm = 0.2963
Meta loss on this task batch = 3.5900e-01, Meta loss averaged over last 500 steps = 3.1586e-01, PNorm = 102.9982, GNorm = 0.3950
Took 134.2282199859619 seconds to complete one epoch of meta training
Took 141.8746213912964 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465344
Epoch 309
Meta loss on this task batch = 3.6640e-01, Meta loss averaged over last 500 steps = 3.1594e-01, PNorm = 103.0021, GNorm = 0.2841
Meta loss on this task batch = 2.6348e-01, Meta loss averaged over last 500 steps = 3.1586e-01, PNorm = 103.0080, GNorm = 0.2416
Meta loss on this task batch = 2.6322e-01, Meta loss averaged over last 500 steps = 3.1573e-01, PNorm = 103.0175, GNorm = 0.3042
Meta loss on this task batch = 3.2754e-01, Meta loss averaged over last 500 steps = 3.1574e-01, PNorm = 103.0260, GNorm = 0.3000
Meta loss on this task batch = 3.5285e-01, Meta loss averaged over last 500 steps = 3.1577e-01, PNorm = 103.0344, GNorm = 0.3077
Meta loss on this task batch = 3.5678e-01, Meta loss averaged over last 500 steps = 3.1584e-01, PNorm = 103.0405, GNorm = 0.3118
Meta loss on this task batch = 3.0290e-01, Meta loss averaged over last 500 steps = 3.1573e-01, PNorm = 103.0488, GNorm = 0.2678
Meta loss on this task batch = 2.8501e-01, Meta loss averaged over last 500 steps = 3.1568e-01, PNorm = 103.0593, GNorm = 0.2457
Meta loss on this task batch = 2.8669e-01, Meta loss averaged over last 500 steps = 3.1563e-01, PNorm = 103.0698, GNorm = 0.2338
Meta loss on this task batch = 3.6389e-01, Meta loss averaged over last 500 steps = 3.1574e-01, PNorm = 103.0772, GNorm = 0.3075
Meta loss on this task batch = 3.4065e-01, Meta loss averaged over last 500 steps = 3.1588e-01, PNorm = 103.0829, GNorm = 0.3020
Meta loss on this task batch = 3.0396e-01, Meta loss averaged over last 500 steps = 3.1579e-01, PNorm = 103.0896, GNorm = 0.2717
Meta loss on this task batch = 3.7027e-01, Meta loss averaged over last 500 steps = 3.1588e-01, PNorm = 103.0957, GNorm = 0.2688
Meta loss on this task batch = 3.2058e-01, Meta loss averaged over last 500 steps = 3.1581e-01, PNorm = 103.1021, GNorm = 0.2501
Meta loss on this task batch = 3.0903e-01, Meta loss averaged over last 500 steps = 3.1585e-01, PNorm = 103.1109, GNorm = 0.2776
Meta loss on this task batch = 3.2949e-01, Meta loss averaged over last 500 steps = 3.1585e-01, PNorm = 103.1214, GNorm = 0.2834
Meta loss on this task batch = 2.9774e-01, Meta loss averaged over last 500 steps = 3.1589e-01, PNorm = 103.1328, GNorm = 0.2725
Meta loss on this task batch = 3.1984e-01, Meta loss averaged over last 500 steps = 3.1588e-01, PNorm = 103.1431, GNorm = 0.2592
Meta loss on this task batch = 2.9853e-01, Meta loss averaged over last 500 steps = 3.1580e-01, PNorm = 103.1530, GNorm = 0.3169
Took 136.9024841785431 seconds to complete one epoch of meta training
Took 145.45507502555847 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467133
Epoch 310
Meta loss on this task batch = 2.9462e-01, Meta loss averaged over last 500 steps = 3.1572e-01, PNorm = 103.1598, GNorm = 0.2849
Meta loss on this task batch = 2.9597e-01, Meta loss averaged over last 500 steps = 3.1568e-01, PNorm = 103.1668, GNorm = 0.2400
Meta loss on this task batch = 3.4852e-01, Meta loss averaged over last 500 steps = 3.1584e-01, PNorm = 103.1726, GNorm = 0.2800
Meta loss on this task batch = 3.2551e-01, Meta loss averaged over last 500 steps = 3.1589e-01, PNorm = 103.1794, GNorm = 0.2753
Meta loss on this task batch = 3.0875e-01, Meta loss averaged over last 500 steps = 3.1580e-01, PNorm = 103.1857, GNorm = 0.2672
Meta loss on this task batch = 2.8067e-01, Meta loss averaged over last 500 steps = 3.1591e-01, PNorm = 103.1915, GNorm = 0.2857
Meta loss on this task batch = 2.6381e-01, Meta loss averaged over last 500 steps = 3.1580e-01, PNorm = 103.1980, GNorm = 0.2468
Meta loss on this task batch = 3.5941e-01, Meta loss averaged over last 500 steps = 3.1583e-01, PNorm = 103.2045, GNorm = 0.3463
Meta loss on this task batch = 2.9812e-01, Meta loss averaged over last 500 steps = 3.1568e-01, PNorm = 103.2107, GNorm = 0.2906
Meta loss on this task batch = 3.4602e-01, Meta loss averaged over last 500 steps = 3.1580e-01, PNorm = 103.2159, GNorm = 0.3323
Meta loss on this task batch = 2.7811e-01, Meta loss averaged over last 500 steps = 3.1563e-01, PNorm = 103.2204, GNorm = 0.2418
Meta loss on this task batch = 3.5449e-01, Meta loss averaged over last 500 steps = 3.1575e-01, PNorm = 103.2259, GNorm = 0.2971
Meta loss on this task batch = 2.5370e-01, Meta loss averaged over last 500 steps = 3.1570e-01, PNorm = 103.2307, GNorm = 0.2790
Meta loss on this task batch = 2.7709e-01, Meta loss averaged over last 500 steps = 3.1559e-01, PNorm = 103.2368, GNorm = 0.2818
Meta loss on this task batch = 2.6671e-01, Meta loss averaged over last 500 steps = 3.1540e-01, PNorm = 103.2429, GNorm = 0.2847
Meta loss on this task batch = 3.7389e-01, Meta loss averaged over last 500 steps = 3.1538e-01, PNorm = 103.2471, GNorm = 0.3216
Meta loss on this task batch = 3.4061e-01, Meta loss averaged over last 500 steps = 3.1538e-01, PNorm = 103.2490, GNorm = 0.3300
Meta loss on this task batch = 3.1292e-01, Meta loss averaged over last 500 steps = 3.1530e-01, PNorm = 103.2514, GNorm = 0.3059
Meta loss on this task batch = 3.3336e-01, Meta loss averaged over last 500 steps = 3.1530e-01, PNorm = 103.2533, GNorm = 0.3740
Took 133.91804909706116 seconds to complete one epoch of meta training
Took 142.31133103370667 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477171
Epoch 311
Meta loss on this task batch = 2.4250e-01, Meta loss averaged over last 500 steps = 3.1516e-01, PNorm = 103.2554, GNorm = 0.2685
Meta loss on this task batch = 2.8553e-01, Meta loss averaged over last 500 steps = 3.1516e-01, PNorm = 103.2578, GNorm = 0.2448
Meta loss on this task batch = 2.9956e-01, Meta loss averaged over last 500 steps = 3.1528e-01, PNorm = 103.2625, GNorm = 0.2924
Meta loss on this task batch = 3.3970e-01, Meta loss averaged over last 500 steps = 3.1540e-01, PNorm = 103.2665, GNorm = 0.3022
Meta loss on this task batch = 3.5952e-01, Meta loss averaged over last 500 steps = 3.1557e-01, PNorm = 103.2714, GNorm = 0.3262
Meta loss on this task batch = 3.2580e-01, Meta loss averaged over last 500 steps = 3.1566e-01, PNorm = 103.2773, GNorm = 0.2759
Meta loss on this task batch = 3.7102e-01, Meta loss averaged over last 500 steps = 3.1578e-01, PNorm = 103.2846, GNorm = 0.3500
Meta loss on this task batch = 3.3757e-01, Meta loss averaged over last 500 steps = 3.1591e-01, PNorm = 103.2926, GNorm = 0.2589
Meta loss on this task batch = 3.1393e-01, Meta loss averaged over last 500 steps = 3.1590e-01, PNorm = 103.3037, GNorm = 0.2967
Meta loss on this task batch = 3.2734e-01, Meta loss averaged over last 500 steps = 3.1588e-01, PNorm = 103.3151, GNorm = 0.2654
Meta loss on this task batch = 2.8521e-01, Meta loss averaged over last 500 steps = 3.1575e-01, PNorm = 103.3251, GNorm = 0.2345
Meta loss on this task batch = 2.5290e-01, Meta loss averaged over last 500 steps = 3.1558e-01, PNorm = 103.3348, GNorm = 0.2427
Meta loss on this task batch = 2.9755e-01, Meta loss averaged over last 500 steps = 3.1554e-01, PNorm = 103.3417, GNorm = 0.2938
Meta loss on this task batch = 3.5507e-01, Meta loss averaged over last 500 steps = 3.1564e-01, PNorm = 103.3454, GNorm = 0.3331
Meta loss on this task batch = 3.2810e-01, Meta loss averaged over last 500 steps = 3.1560e-01, PNorm = 103.3490, GNorm = 0.2787
Meta loss on this task batch = 2.9818e-01, Meta loss averaged over last 500 steps = 3.1550e-01, PNorm = 103.3527, GNorm = 0.2599
Meta loss on this task batch = 2.7208e-01, Meta loss averaged over last 500 steps = 3.1547e-01, PNorm = 103.3575, GNorm = 0.2849
Meta loss on this task batch = 3.1482e-01, Meta loss averaged over last 500 steps = 3.1541e-01, PNorm = 103.3627, GNorm = 0.2875
Meta loss on this task batch = 3.0027e-01, Meta loss averaged over last 500 steps = 3.1536e-01, PNorm = 103.3673, GNorm = 0.3428
Took 132.71786093711853 seconds to complete one epoch of meta training
Took 141.19122052192688 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481517
Epoch 312
Meta loss on this task batch = 3.3009e-01, Meta loss averaged over last 500 steps = 3.1534e-01, PNorm = 103.3708, GNorm = 0.3038
Meta loss on this task batch = 2.9229e-01, Meta loss averaged over last 500 steps = 3.1522e-01, PNorm = 103.3744, GNorm = 0.3009
Meta loss on this task batch = 2.6641e-01, Meta loss averaged over last 500 steps = 3.1520e-01, PNorm = 103.3774, GNorm = 0.2674
Meta loss on this task batch = 3.5109e-01, Meta loss averaged over last 500 steps = 3.1522e-01, PNorm = 103.3799, GNorm = 0.3588
Meta loss on this task batch = 3.2252e-01, Meta loss averaged over last 500 steps = 3.1527e-01, PNorm = 103.3803, GNorm = 0.3003
Meta loss on this task batch = 2.5120e-01, Meta loss averaged over last 500 steps = 3.1516e-01, PNorm = 103.3814, GNorm = 0.2772
Meta loss on this task batch = 3.3045e-01, Meta loss averaged over last 500 steps = 3.1515e-01, PNorm = 103.3808, GNorm = 0.4221
Meta loss on this task batch = 3.1309e-01, Meta loss averaged over last 500 steps = 3.1520e-01, PNorm = 103.3815, GNorm = 0.3212
Meta loss on this task batch = 2.8999e-01, Meta loss averaged over last 500 steps = 3.1511e-01, PNorm = 103.3824, GNorm = 0.2746
Meta loss on this task batch = 3.5194e-01, Meta loss averaged over last 500 steps = 3.1508e-01, PNorm = 103.3837, GNorm = 0.3360
Meta loss on this task batch = 3.1114e-01, Meta loss averaged over last 500 steps = 3.1502e-01, PNorm = 103.3869, GNorm = 0.3167
Meta loss on this task batch = 3.5188e-01, Meta loss averaged over last 500 steps = 3.1525e-01, PNorm = 103.3926, GNorm = 0.3573
Meta loss on this task batch = 3.6656e-01, Meta loss averaged over last 500 steps = 3.1520e-01, PNorm = 103.3984, GNorm = 0.3473
Meta loss on this task batch = 2.8730e-01, Meta loss averaged over last 500 steps = 3.1527e-01, PNorm = 103.4060, GNorm = 0.2588
Meta loss on this task batch = 3.1982e-01, Meta loss averaged over last 500 steps = 3.1529e-01, PNorm = 103.4154, GNorm = 0.2849
Meta loss on this task batch = 2.9142e-01, Meta loss averaged over last 500 steps = 3.1526e-01, PNorm = 103.4261, GNorm = 0.2702
Meta loss on this task batch = 2.7725e-01, Meta loss averaged over last 500 steps = 3.1522e-01, PNorm = 103.4399, GNorm = 0.3101
Meta loss on this task batch = 3.4771e-01, Meta loss averaged over last 500 steps = 3.1525e-01, PNorm = 103.4503, GNorm = 0.3343
Meta loss on this task batch = 2.2228e-01, Meta loss averaged over last 500 steps = 3.1504e-01, PNorm = 103.4617, GNorm = 0.2820
Took 134.36238360404968 seconds to complete one epoch of meta training
Took 143.18865633010864 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457240
Epoch 313
Meta loss on this task batch = 3.1419e-01, Meta loss averaged over last 500 steps = 3.1490e-01, PNorm = 103.4726, GNorm = 0.2911
Meta loss on this task batch = 3.0257e-01, Meta loss averaged over last 500 steps = 3.1487e-01, PNorm = 103.4808, GNorm = 0.3153
Meta loss on this task batch = 3.3159e-01, Meta loss averaged over last 500 steps = 3.1493e-01, PNorm = 103.4847, GNorm = 0.3389
Meta loss on this task batch = 3.1973e-01, Meta loss averaged over last 500 steps = 3.1493e-01, PNorm = 103.4866, GNorm = 0.2857
Meta loss on this task batch = 3.0454e-01, Meta loss averaged over last 500 steps = 3.1503e-01, PNorm = 103.4863, GNorm = 0.3259
Meta loss on this task batch = 3.3682e-01, Meta loss averaged over last 500 steps = 3.1513e-01, PNorm = 103.4867, GNorm = 0.2854
Meta loss on this task batch = 3.8707e-01, Meta loss averaged over last 500 steps = 3.1525e-01, PNorm = 103.4867, GNorm = 0.3687
Meta loss on this task batch = 3.0126e-01, Meta loss averaged over last 500 steps = 3.1514e-01, PNorm = 103.4883, GNorm = 0.2735
Meta loss on this task batch = 3.1883e-01, Meta loss averaged over last 500 steps = 3.1516e-01, PNorm = 103.4912, GNorm = 0.3027
Meta loss on this task batch = 3.4808e-01, Meta loss averaged over last 500 steps = 3.1517e-01, PNorm = 103.4950, GNorm = 0.3131
Meta loss on this task batch = 2.7131e-01, Meta loss averaged over last 500 steps = 3.1515e-01, PNorm = 103.5025, GNorm = 0.2883
Meta loss on this task batch = 2.9276e-01, Meta loss averaged over last 500 steps = 3.1519e-01, PNorm = 103.5133, GNorm = 0.3367
Meta loss on this task batch = 2.7081e-01, Meta loss averaged over last 500 steps = 3.1518e-01, PNorm = 103.5237, GNorm = 0.2321
Meta loss on this task batch = 2.3110e-01, Meta loss averaged over last 500 steps = 3.1509e-01, PNorm = 103.5343, GNorm = 0.2178
Meta loss on this task batch = 3.5449e-01, Meta loss averaged over last 500 steps = 3.1521e-01, PNorm = 103.5437, GNorm = 0.3036
Meta loss on this task batch = 3.0001e-01, Meta loss averaged over last 500 steps = 3.1514e-01, PNorm = 103.5519, GNorm = 0.3209
Meta loss on this task batch = 3.6535e-01, Meta loss averaged over last 500 steps = 3.1526e-01, PNorm = 103.5564, GNorm = 0.3712
Meta loss on this task batch = 2.7223e-01, Meta loss averaged over last 500 steps = 3.1519e-01, PNorm = 103.5613, GNorm = 0.2800
Meta loss on this task batch = 2.7752e-01, Meta loss averaged over last 500 steps = 3.1506e-01, PNorm = 103.5663, GNorm = 0.3263
Took 136.12630224227905 seconds to complete one epoch of meta training
Took 143.68433022499084 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474698
Epoch 314
Meta loss on this task batch = 3.1104e-01, Meta loss averaged over last 500 steps = 3.1506e-01, PNorm = 103.5696, GNorm = 0.3570
Meta loss on this task batch = 2.5342e-01, Meta loss averaged over last 500 steps = 3.1495e-01, PNorm = 103.5754, GNorm = 0.2861
Meta loss on this task batch = 3.1393e-01, Meta loss averaged over last 500 steps = 3.1496e-01, PNorm = 103.5811, GNorm = 0.2900
Meta loss on this task batch = 2.9079e-01, Meta loss averaged over last 500 steps = 3.1491e-01, PNorm = 103.5887, GNorm = 0.2985
Meta loss on this task batch = 3.2971e-01, Meta loss averaged over last 500 steps = 3.1491e-01, PNorm = 103.5991, GNorm = 0.3059
Meta loss on this task batch = 3.2349e-01, Meta loss averaged over last 500 steps = 3.1485e-01, PNorm = 103.6085, GNorm = 0.2836
Meta loss on this task batch = 2.7145e-01, Meta loss averaged over last 500 steps = 3.1472e-01, PNorm = 103.6204, GNorm = 0.2323
Meta loss on this task batch = 3.5188e-01, Meta loss averaged over last 500 steps = 3.1489e-01, PNorm = 103.6304, GNorm = 0.2778
Meta loss on this task batch = 3.3013e-01, Meta loss averaged over last 500 steps = 3.1487e-01, PNorm = 103.6405, GNorm = 0.2874
Meta loss on this task batch = 3.1628e-01, Meta loss averaged over last 500 steps = 3.1488e-01, PNorm = 103.6491, GNorm = 0.2930
Meta loss on this task batch = 3.2445e-01, Meta loss averaged over last 500 steps = 3.1492e-01, PNorm = 103.6586, GNorm = 0.3494
Meta loss on this task batch = 3.2822e-01, Meta loss averaged over last 500 steps = 3.1503e-01, PNorm = 103.6691, GNorm = 0.2854
Meta loss on this task batch = 3.1924e-01, Meta loss averaged over last 500 steps = 3.1498e-01, PNorm = 103.6766, GNorm = 0.2927
Meta loss on this task batch = 2.8827e-01, Meta loss averaged over last 500 steps = 3.1503e-01, PNorm = 103.6812, GNorm = 0.2645
Meta loss on this task batch = 3.0470e-01, Meta loss averaged over last 500 steps = 3.1503e-01, PNorm = 103.6849, GNorm = 0.3269
Meta loss on this task batch = 3.7581e-01, Meta loss averaged over last 500 steps = 3.1524e-01, PNorm = 103.6862, GNorm = 0.3353
Meta loss on this task batch = 3.2357e-01, Meta loss averaged over last 500 steps = 3.1525e-01, PNorm = 103.6875, GNorm = 0.2960
Meta loss on this task batch = 2.8677e-01, Meta loss averaged over last 500 steps = 3.1524e-01, PNorm = 103.6901, GNorm = 0.2581
Meta loss on this task batch = 3.2889e-01, Meta loss averaged over last 500 steps = 3.1517e-01, PNorm = 103.6912, GNorm = 0.3685
Took 135.48103547096252 seconds to complete one epoch of meta training
Took 144.19944882392883 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472172
Epoch 315
Meta loss on this task batch = 3.4149e-01, Meta loss averaged over last 500 steps = 3.1521e-01, PNorm = 103.6943, GNorm = 0.2907
Meta loss on this task batch = 3.1956e-01, Meta loss averaged over last 500 steps = 3.1513e-01, PNorm = 103.6996, GNorm = 0.3374
Meta loss on this task batch = 3.0379e-01, Meta loss averaged over last 500 steps = 3.1505e-01, PNorm = 103.7027, GNorm = 0.3143
Meta loss on this task batch = 3.2398e-01, Meta loss averaged over last 500 steps = 3.1509e-01, PNorm = 103.7089, GNorm = 0.2949
Meta loss on this task batch = 2.6396e-01, Meta loss averaged over last 500 steps = 3.1494e-01, PNorm = 103.7172, GNorm = 0.2448
Meta loss on this task batch = 3.2815e-01, Meta loss averaged over last 500 steps = 3.1498e-01, PNorm = 103.7266, GNorm = 0.2676
Meta loss on this task batch = 3.4404e-01, Meta loss averaged over last 500 steps = 3.1496e-01, PNorm = 103.7378, GNorm = 0.3039
Meta loss on this task batch = 2.5070e-01, Meta loss averaged over last 500 steps = 3.1472e-01, PNorm = 103.7512, GNorm = 0.2613
Meta loss on this task batch = 3.2581e-01, Meta loss averaged over last 500 steps = 3.1480e-01, PNorm = 103.7619, GNorm = 0.3171
Meta loss on this task batch = 3.1160e-01, Meta loss averaged over last 500 steps = 3.1483e-01, PNorm = 103.7713, GNorm = 0.3292
Meta loss on this task batch = 3.5115e-01, Meta loss averaged over last 500 steps = 3.1487e-01, PNorm = 103.7778, GNorm = 0.3124
Meta loss on this task batch = 2.9826e-01, Meta loss averaged over last 500 steps = 3.1496e-01, PNorm = 103.7812, GNorm = 0.2925
Meta loss on this task batch = 3.5181e-01, Meta loss averaged over last 500 steps = 3.1505e-01, PNorm = 103.7821, GNorm = 0.3284
Meta loss on this task batch = 3.2672e-01, Meta loss averaged over last 500 steps = 3.1497e-01, PNorm = 103.7811, GNorm = 0.2938
Meta loss on this task batch = 2.9154e-01, Meta loss averaged over last 500 steps = 3.1497e-01, PNorm = 103.7786, GNorm = 0.3057
Meta loss on this task batch = 3.4200e-01, Meta loss averaged over last 500 steps = 3.1502e-01, PNorm = 103.7788, GNorm = 0.3099
Meta loss on this task batch = 3.3559e-01, Meta loss averaged over last 500 steps = 3.1506e-01, PNorm = 103.7786, GNorm = 0.3139
Meta loss on this task batch = 2.9975e-01, Meta loss averaged over last 500 steps = 3.1490e-01, PNorm = 103.7779, GNorm = 0.2825
Meta loss on this task batch = 2.8913e-01, Meta loss averaged over last 500 steps = 3.1491e-01, PNorm = 103.7778, GNorm = 0.3245
Took 130.73029613494873 seconds to complete one epoch of meta training
Took 139.38912558555603 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467582
Epoch 316
Meta loss on this task batch = 2.5446e-01, Meta loss averaged over last 500 steps = 3.1473e-01, PNorm = 103.7793, GNorm = 0.2842
Meta loss on this task batch = 2.6618e-01, Meta loss averaged over last 500 steps = 3.1462e-01, PNorm = 103.7833, GNorm = 0.2365
Meta loss on this task batch = 3.7640e-01, Meta loss averaged over last 500 steps = 3.1481e-01, PNorm = 103.7868, GNorm = 0.2883
Meta loss on this task batch = 3.4105e-01, Meta loss averaged over last 500 steps = 3.1486e-01, PNorm = 103.7914, GNorm = 0.3021
Meta loss on this task batch = 3.1086e-01, Meta loss averaged over last 500 steps = 3.1483e-01, PNorm = 103.7972, GNorm = 0.3024
Meta loss on this task batch = 3.9170e-01, Meta loss averaged over last 500 steps = 3.1507e-01, PNorm = 103.8026, GNorm = 0.3431
Meta loss on this task batch = 3.0663e-01, Meta loss averaged over last 500 steps = 3.1502e-01, PNorm = 103.8078, GNorm = 0.2609
Meta loss on this task batch = 3.6380e-01, Meta loss averaged over last 500 steps = 3.1515e-01, PNorm = 103.8123, GNorm = 0.3070
Meta loss on this task batch = 3.2937e-01, Meta loss averaged over last 500 steps = 3.1522e-01, PNorm = 103.8179, GNorm = 0.3281
Meta loss on this task batch = 3.4632e-01, Meta loss averaged over last 500 steps = 3.1535e-01, PNorm = 103.8242, GNorm = 0.2681
Meta loss on this task batch = 3.2127e-01, Meta loss averaged over last 500 steps = 3.1537e-01, PNorm = 103.8308, GNorm = 0.2716
Meta loss on this task batch = 2.6185e-01, Meta loss averaged over last 500 steps = 3.1530e-01, PNorm = 103.8390, GNorm = 0.2859
Meta loss on this task batch = 2.8278e-01, Meta loss averaged over last 500 steps = 3.1524e-01, PNorm = 103.8479, GNorm = 0.2491
Meta loss on this task batch = 2.9306e-01, Meta loss averaged over last 500 steps = 3.1506e-01, PNorm = 103.8562, GNorm = 0.2940
Meta loss on this task batch = 2.9556e-01, Meta loss averaged over last 500 steps = 3.1505e-01, PNorm = 103.8631, GNorm = 0.3563
Meta loss on this task batch = 2.7608e-01, Meta loss averaged over last 500 steps = 3.1498e-01, PNorm = 103.8674, GNorm = 0.2542
Meta loss on this task batch = 2.9340e-01, Meta loss averaged over last 500 steps = 3.1491e-01, PNorm = 103.8707, GNorm = 0.2703
Meta loss on this task batch = 2.7085e-01, Meta loss averaged over last 500 steps = 3.1495e-01, PNorm = 103.8715, GNorm = 0.2928
Meta loss on this task batch = 3.6238e-01, Meta loss averaged over last 500 steps = 3.1497e-01, PNorm = 103.8717, GNorm = 0.3566
Took 138.47294855117798 seconds to complete one epoch of meta training
Took 147.0051498413086 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466851
Epoch 317
Meta loss on this task batch = 3.2567e-01, Meta loss averaged over last 500 steps = 3.1485e-01, PNorm = 103.8735, GNorm = 0.2856
Meta loss on this task batch = 2.8237e-01, Meta loss averaged over last 500 steps = 3.1477e-01, PNorm = 103.8762, GNorm = 0.2657
Meta loss on this task batch = 2.7118e-01, Meta loss averaged over last 500 steps = 3.1472e-01, PNorm = 103.8808, GNorm = 0.2465
Meta loss on this task batch = 3.0762e-01, Meta loss averaged over last 500 steps = 3.1472e-01, PNorm = 103.8857, GNorm = 0.2759
Meta loss on this task batch = 3.0296e-01, Meta loss averaged over last 500 steps = 3.1468e-01, PNorm = 103.8925, GNorm = 0.3076
Meta loss on this task batch = 3.2983e-01, Meta loss averaged over last 500 steps = 3.1466e-01, PNorm = 103.8992, GNorm = 0.3020
Meta loss on this task batch = 2.8501e-01, Meta loss averaged over last 500 steps = 3.1456e-01, PNorm = 103.9090, GNorm = 0.2884
Meta loss on this task batch = 2.9014e-01, Meta loss averaged over last 500 steps = 3.1450e-01, PNorm = 103.9216, GNorm = 0.2680
Meta loss on this task batch = 2.8072e-01, Meta loss averaged over last 500 steps = 3.1447e-01, PNorm = 103.9384, GNorm = 0.2860
Meta loss on this task batch = 3.1647e-01, Meta loss averaged over last 500 steps = 3.1450e-01, PNorm = 103.9549, GNorm = 0.2745
Meta loss on this task batch = 3.3604e-01, Meta loss averaged over last 500 steps = 3.1454e-01, PNorm = 103.9699, GNorm = 0.3124
Meta loss on this task batch = 3.1197e-01, Meta loss averaged over last 500 steps = 3.1451e-01, PNorm = 103.9806, GNorm = 0.2957
Meta loss on this task batch = 2.9556e-01, Meta loss averaged over last 500 steps = 3.1440e-01, PNorm = 103.9888, GNorm = 0.3444
Meta loss on this task batch = 3.6691e-01, Meta loss averaged over last 500 steps = 3.1452e-01, PNorm = 103.9953, GNorm = 0.3237
Meta loss on this task batch = 2.7195e-01, Meta loss averaged over last 500 steps = 3.1449e-01, PNorm = 103.9999, GNorm = 0.3112
Meta loss on this task batch = 3.7425e-01, Meta loss averaged over last 500 steps = 3.1471e-01, PNorm = 104.0054, GNorm = 0.3489
Meta loss on this task batch = 3.0405e-01, Meta loss averaged over last 500 steps = 3.1466e-01, PNorm = 104.0109, GNorm = 0.2836
Meta loss on this task batch = 2.7668e-01, Meta loss averaged over last 500 steps = 3.1459e-01, PNorm = 104.0165, GNorm = 0.2732
Meta loss on this task batch = 3.4196e-01, Meta loss averaged over last 500 steps = 3.1466e-01, PNorm = 104.0227, GNorm = 0.3692
Took 134.26517486572266 seconds to complete one epoch of meta training
Took 142.94966888427734 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476556
Epoch 318
Meta loss on this task batch = 3.4179e-01, Meta loss averaged over last 500 steps = 3.1475e-01, PNorm = 104.0296, GNorm = 0.3016
Meta loss on this task batch = 3.4091e-01, Meta loss averaged over last 500 steps = 3.1479e-01, PNorm = 104.0384, GNorm = 0.2734
Meta loss on this task batch = 2.6544e-01, Meta loss averaged over last 500 steps = 3.1473e-01, PNorm = 104.0490, GNorm = 0.2607
Meta loss on this task batch = 2.9587e-01, Meta loss averaged over last 500 steps = 3.1471e-01, PNorm = 104.0610, GNorm = 0.2344
Meta loss on this task batch = 2.3092e-01, Meta loss averaged over last 500 steps = 3.1450e-01, PNorm = 104.0735, GNorm = 0.2220
Meta loss on this task batch = 3.0731e-01, Meta loss averaged over last 500 steps = 3.1440e-01, PNorm = 104.0837, GNorm = 0.2922
Meta loss on this task batch = 3.0806e-01, Meta loss averaged over last 500 steps = 3.1446e-01, PNorm = 104.0914, GNorm = 0.2952
Meta loss on this task batch = 3.4297e-01, Meta loss averaged over last 500 steps = 3.1453e-01, PNorm = 104.0952, GNorm = 0.3877
Meta loss on this task batch = 3.4632e-01, Meta loss averaged over last 500 steps = 3.1456e-01, PNorm = 104.0958, GNorm = 0.3172
Meta loss on this task batch = 2.6497e-01, Meta loss averaged over last 500 steps = 3.1447e-01, PNorm = 104.0961, GNorm = 0.2761
Meta loss on this task batch = 2.9847e-01, Meta loss averaged over last 500 steps = 3.1446e-01, PNorm = 104.0960, GNorm = 0.3321
Meta loss on this task batch = 2.9782e-01, Meta loss averaged over last 500 steps = 3.1447e-01, PNorm = 104.0992, GNorm = 0.3690
Meta loss on this task batch = 3.4233e-01, Meta loss averaged over last 500 steps = 3.1448e-01, PNorm = 104.1030, GNorm = 0.3043
Meta loss on this task batch = 3.4524e-01, Meta loss averaged over last 500 steps = 3.1446e-01, PNorm = 104.1092, GNorm = 0.2813
Meta loss on this task batch = 3.2261e-01, Meta loss averaged over last 500 steps = 3.1449e-01, PNorm = 104.1170, GNorm = 0.2790
Meta loss on this task batch = 3.5752e-01, Meta loss averaged over last 500 steps = 3.1455e-01, PNorm = 104.1248, GNorm = 0.3228
Meta loss on this task batch = 2.7787e-01, Meta loss averaged over last 500 steps = 3.1437e-01, PNorm = 104.1312, GNorm = 0.2690
Meta loss on this task batch = 3.1039e-01, Meta loss averaged over last 500 steps = 3.1436e-01, PNorm = 104.1382, GNorm = 0.2665
Meta loss on this task batch = 2.8283e-01, Meta loss averaged over last 500 steps = 3.1433e-01, PNorm = 104.1432, GNorm = 0.3295
Took 133.9528305530548 seconds to complete one epoch of meta training
Took 142.48903059959412 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486505
Epoch 319
Meta loss on this task batch = 2.7648e-01, Meta loss averaged over last 500 steps = 3.1419e-01, PNorm = 104.1480, GNorm = 0.2710
Meta loss on this task batch = 3.6789e-01, Meta loss averaged over last 500 steps = 3.1426e-01, PNorm = 104.1502, GNorm = 0.3468
Meta loss on this task batch = 2.8571e-01, Meta loss averaged over last 500 steps = 3.1423e-01, PNorm = 104.1524, GNorm = 0.2221
Meta loss on this task batch = 3.1190e-01, Meta loss averaged over last 500 steps = 3.1423e-01, PNorm = 104.1534, GNorm = 0.2833
Meta loss on this task batch = 2.7708e-01, Meta loss averaged over last 500 steps = 3.1419e-01, PNorm = 104.1563, GNorm = 0.2475
Meta loss on this task batch = 3.2205e-01, Meta loss averaged over last 500 steps = 3.1408e-01, PNorm = 104.1586, GNorm = 0.2987
Meta loss on this task batch = 3.1155e-01, Meta loss averaged over last 500 steps = 3.1404e-01, PNorm = 104.1628, GNorm = 0.2940
Meta loss on this task batch = 3.2170e-01, Meta loss averaged over last 500 steps = 3.1408e-01, PNorm = 104.1666, GNorm = 0.2712
Meta loss on this task batch = 3.0261e-01, Meta loss averaged over last 500 steps = 3.1403e-01, PNorm = 104.1728, GNorm = 0.2618
Meta loss on this task batch = 2.6739e-01, Meta loss averaged over last 500 steps = 3.1404e-01, PNorm = 104.1793, GNorm = 0.2716
Meta loss on this task batch = 2.4054e-01, Meta loss averaged over last 500 steps = 3.1386e-01, PNorm = 104.1864, GNorm = 0.2592
Meta loss on this task batch = 2.8991e-01, Meta loss averaged over last 500 steps = 3.1376e-01, PNorm = 104.1935, GNorm = 0.2703
Meta loss on this task batch = 3.5358e-01, Meta loss averaged over last 500 steps = 3.1388e-01, PNorm = 104.1985, GNorm = 0.3447
Meta loss on this task batch = 2.9895e-01, Meta loss averaged over last 500 steps = 3.1400e-01, PNorm = 104.2011, GNorm = 0.3176
Meta loss on this task batch = 2.6412e-01, Meta loss averaged over last 500 steps = 3.1395e-01, PNorm = 104.2044, GNorm = 0.3030
Meta loss on this task batch = 3.4594e-01, Meta loss averaged over last 500 steps = 3.1386e-01, PNorm = 104.2062, GNorm = 0.3120
Meta loss on this task batch = 4.0278e-01, Meta loss averaged over last 500 steps = 3.1399e-01, PNorm = 104.2071, GNorm = 0.3637
Meta loss on this task batch = 3.5124e-01, Meta loss averaged over last 500 steps = 3.1405e-01, PNorm = 104.2064, GNorm = 0.3595
Meta loss on this task batch = 3.1956e-01, Meta loss averaged over last 500 steps = 3.1400e-01, PNorm = 104.2059, GNorm = 0.3464
Took 134.1180112361908 seconds to complete one epoch of meta training
Took 142.82593870162964 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506078
Epoch 320
Meta loss on this task batch = 2.5641e-01, Meta loss averaged over last 500 steps = 3.1385e-01, PNorm = 104.2056, GNorm = 0.2766
Meta loss on this task batch = 3.1077e-01, Meta loss averaged over last 500 steps = 3.1400e-01, PNorm = 104.2082, GNorm = 0.2885
Meta loss on this task batch = 3.0912e-01, Meta loss averaged over last 500 steps = 3.1387e-01, PNorm = 104.2127, GNorm = 0.2747
Meta loss on this task batch = 3.1335e-01, Meta loss averaged over last 500 steps = 3.1389e-01, PNorm = 104.2170, GNorm = 0.2983
Meta loss on this task batch = 2.4212e-01, Meta loss averaged over last 500 steps = 3.1367e-01, PNorm = 104.2221, GNorm = 0.2359
Meta loss on this task batch = 3.1396e-01, Meta loss averaged over last 500 steps = 3.1365e-01, PNorm = 104.2263, GNorm = 0.2959
Meta loss on this task batch = 2.5552e-01, Meta loss averaged over last 500 steps = 3.1352e-01, PNorm = 104.2316, GNorm = 0.2636
Meta loss on this task batch = 2.7286e-01, Meta loss averaged over last 500 steps = 3.1348e-01, PNorm = 104.2371, GNorm = 0.2667
Meta loss on this task batch = 3.7104e-01, Meta loss averaged over last 500 steps = 3.1361e-01, PNorm = 104.2410, GNorm = 0.3187
Meta loss on this task batch = 3.0323e-01, Meta loss averaged over last 500 steps = 3.1363e-01, PNorm = 104.2453, GNorm = 0.2618
Meta loss on this task batch = 2.6442e-01, Meta loss averaged over last 500 steps = 3.1351e-01, PNorm = 104.2503, GNorm = 0.2542
Meta loss on this task batch = 3.8521e-01, Meta loss averaged over last 500 steps = 3.1363e-01, PNorm = 104.2544, GNorm = 0.3284
Meta loss on this task batch = 2.9173e-01, Meta loss averaged over last 500 steps = 3.1354e-01, PNorm = 104.2593, GNorm = 0.2994
Meta loss on this task batch = 2.9097e-01, Meta loss averaged over last 500 steps = 3.1349e-01, PNorm = 104.2625, GNorm = 0.2510
Meta loss on this task batch = 2.5229e-01, Meta loss averaged over last 500 steps = 3.1336e-01, PNorm = 104.2640, GNorm = 0.2864
Meta loss on this task batch = 2.9683e-01, Meta loss averaged over last 500 steps = 3.1328e-01, PNorm = 104.2641, GNorm = 0.3089
Meta loss on this task batch = 3.2508e-01, Meta loss averaged over last 500 steps = 3.1322e-01, PNorm = 104.2651, GNorm = 0.2746
Meta loss on this task batch = 3.2171e-01, Meta loss averaged over last 500 steps = 3.1323e-01, PNorm = 104.2672, GNorm = 0.3028
Meta loss on this task batch = 3.8784e-01, Meta loss averaged over last 500 steps = 3.1335e-01, PNorm = 104.2680, GNorm = 0.3779
Took 134.95560789108276 seconds to complete one epoch of meta training
Took 143.24814462661743 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479081
Epoch 321
Meta loss on this task batch = 2.5913e-01, Meta loss averaged over last 500 steps = 3.1326e-01, PNorm = 104.2708, GNorm = 0.2483
Meta loss on this task batch = 2.6546e-01, Meta loss averaged over last 500 steps = 3.1317e-01, PNorm = 104.2755, GNorm = 0.2545
Meta loss on this task batch = 3.2881e-01, Meta loss averaged over last 500 steps = 3.1323e-01, PNorm = 104.2840, GNorm = 0.3295
Meta loss on this task batch = 3.1523e-01, Meta loss averaged over last 500 steps = 3.1319e-01, PNorm = 104.2947, GNorm = 0.2723
Meta loss on this task batch = 2.9467e-01, Meta loss averaged over last 500 steps = 3.1310e-01, PNorm = 104.3070, GNorm = 0.2596
Meta loss on this task batch = 2.9835e-01, Meta loss averaged over last 500 steps = 3.1303e-01, PNorm = 104.3173, GNorm = 0.2545
Meta loss on this task batch = 2.9285e-01, Meta loss averaged over last 500 steps = 3.1307e-01, PNorm = 104.3277, GNorm = 0.2956
Meta loss on this task batch = 3.5490e-01, Meta loss averaged over last 500 steps = 3.1322e-01, PNorm = 104.3386, GNorm = 0.2959
Meta loss on this task batch = 3.9731e-01, Meta loss averaged over last 500 steps = 3.1322e-01, PNorm = 104.3475, GNorm = 0.3468
Meta loss on this task batch = 3.2999e-01, Meta loss averaged over last 500 steps = 3.1332e-01, PNorm = 104.3570, GNorm = 0.2914
Meta loss on this task batch = 3.2236e-01, Meta loss averaged over last 500 steps = 3.1336e-01, PNorm = 104.3659, GNorm = 0.3039
Meta loss on this task batch = 3.0884e-01, Meta loss averaged over last 500 steps = 3.1321e-01, PNorm = 104.3749, GNorm = 0.2675
Meta loss on this task batch = 3.8449e-01, Meta loss averaged over last 500 steps = 3.1327e-01, PNorm = 104.3795, GNorm = 0.3656
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 3.1326e-01, PNorm = 104.3807, GNorm = 0.2995
Meta loss on this task batch = 2.6092e-01, Meta loss averaged over last 500 steps = 3.1303e-01, PNorm = 104.3841, GNorm = 0.3096
Meta loss on this task batch = 3.5543e-01, Meta loss averaged over last 500 steps = 3.1311e-01, PNorm = 104.3860, GNorm = 0.2949
Meta loss on this task batch = 3.1548e-01, Meta loss averaged over last 500 steps = 3.1320e-01, PNorm = 104.3895, GNorm = 0.3051
Meta loss on this task batch = 3.4214e-01, Meta loss averaged over last 500 steps = 3.1329e-01, PNorm = 104.3953, GNorm = 0.2721
Meta loss on this task batch = 3.0048e-01, Meta loss averaged over last 500 steps = 3.1338e-01, PNorm = 104.4032, GNorm = 0.3207
Took 134.18373227119446 seconds to complete one epoch of meta training
Took 143.19274425506592 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.525220
Epoch 322
Meta loss on this task batch = 3.6544e-01, Meta loss averaged over last 500 steps = 3.1343e-01, PNorm = 104.4127, GNorm = 0.3248
Meta loss on this task batch = 2.7343e-01, Meta loss averaged over last 500 steps = 3.1331e-01, PNorm = 104.4243, GNorm = 0.2709
Meta loss on this task batch = 3.2765e-01, Meta loss averaged over last 500 steps = 3.1328e-01, PNorm = 104.4360, GNorm = 0.2623
Meta loss on this task batch = 3.3997e-01, Meta loss averaged over last 500 steps = 3.1335e-01, PNorm = 104.4470, GNorm = 0.3015
Meta loss on this task batch = 2.5559e-01, Meta loss averaged over last 500 steps = 3.1323e-01, PNorm = 104.4603, GNorm = 0.3180
Meta loss on this task batch = 3.3393e-01, Meta loss averaged over last 500 steps = 3.1336e-01, PNorm = 104.4712, GNorm = 0.3483
Meta loss on this task batch = 3.0487e-01, Meta loss averaged over last 500 steps = 3.1333e-01, PNorm = 104.4785, GNorm = 0.2861
Meta loss on this task batch = 2.8506e-01, Meta loss averaged over last 500 steps = 3.1344e-01, PNorm = 104.4837, GNorm = 0.2890
Meta loss on this task batch = 2.5708e-01, Meta loss averaged over last 500 steps = 3.1334e-01, PNorm = 104.4864, GNorm = 0.2554
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 3.1320e-01, PNorm = 104.4903, GNorm = 0.2730
Meta loss on this task batch = 3.4955e-01, Meta loss averaged over last 500 steps = 3.1312e-01, PNorm = 104.4925, GNorm = 0.3334
Meta loss on this task batch = 3.3816e-01, Meta loss averaged over last 500 steps = 3.1311e-01, PNorm = 104.4904, GNorm = 0.3602
Meta loss on this task batch = 3.1492e-01, Meta loss averaged over last 500 steps = 3.1312e-01, PNorm = 104.4892, GNorm = 0.2832
Meta loss on this task batch = 2.7735e-01, Meta loss averaged over last 500 steps = 3.1299e-01, PNorm = 104.4888, GNorm = 0.3079
Meta loss on this task batch = 2.7167e-01, Meta loss averaged over last 500 steps = 3.1288e-01, PNorm = 104.4887, GNorm = 0.3138
Meta loss on this task batch = 2.3507e-01, Meta loss averaged over last 500 steps = 3.1268e-01, PNorm = 104.4906, GNorm = 0.2476
Meta loss on this task batch = 3.5257e-01, Meta loss averaged over last 500 steps = 3.1270e-01, PNorm = 104.4946, GNorm = 0.3219
Meta loss on this task batch = 3.0261e-01, Meta loss averaged over last 500 steps = 3.1267e-01, PNorm = 104.5008, GNorm = 0.2946
Meta loss on this task batch = 3.2011e-01, Meta loss averaged over last 500 steps = 3.1265e-01, PNorm = 104.5068, GNorm = 0.3394
Took 133.23159956932068 seconds to complete one epoch of meta training
Took 141.93848156929016 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510284
Epoch 323
Meta loss on this task batch = 3.4169e-01, Meta loss averaged over last 500 steps = 3.1275e-01, PNorm = 104.5143, GNorm = 0.3364
Meta loss on this task batch = 3.1835e-01, Meta loss averaged over last 500 steps = 3.1263e-01, PNorm = 104.5231, GNorm = 0.2877
Meta loss on this task batch = 3.1684e-01, Meta loss averaged over last 500 steps = 3.1275e-01, PNorm = 104.5333, GNorm = 0.3016
Meta loss on this task batch = 2.7247e-01, Meta loss averaged over last 500 steps = 3.1268e-01, PNorm = 104.5438, GNorm = 0.2406
Meta loss on this task batch = 3.0235e-01, Meta loss averaged over last 500 steps = 3.1276e-01, PNorm = 104.5560, GNorm = 0.2908
Meta loss on this task batch = 3.2703e-01, Meta loss averaged over last 500 steps = 3.1268e-01, PNorm = 104.5666, GNorm = 0.2976
Meta loss on this task batch = 3.3327e-01, Meta loss averaged over last 500 steps = 3.1279e-01, PNorm = 104.5786, GNorm = 0.3252
Meta loss on this task batch = 2.9185e-01, Meta loss averaged over last 500 steps = 3.1273e-01, PNorm = 104.5866, GNorm = 0.2981
Meta loss on this task batch = 3.0072e-01, Meta loss averaged over last 500 steps = 3.1281e-01, PNorm = 104.5931, GNorm = 0.2935
Meta loss on this task batch = 2.7282e-01, Meta loss averaged over last 500 steps = 3.1262e-01, PNorm = 104.5969, GNorm = 0.2977
Meta loss on this task batch = 1.8886e-01, Meta loss averaged over last 500 steps = 3.1238e-01, PNorm = 104.6017, GNorm = 0.2386
Meta loss on this task batch = 3.0628e-01, Meta loss averaged over last 500 steps = 3.1229e-01, PNorm = 104.6057, GNorm = 0.2819
Meta loss on this task batch = 2.6863e-01, Meta loss averaged over last 500 steps = 3.1219e-01, PNorm = 104.6070, GNorm = 0.3097
Meta loss on this task batch = 2.8889e-01, Meta loss averaged over last 500 steps = 3.1214e-01, PNorm = 104.6075, GNorm = 0.3084
Meta loss on this task batch = 2.8092e-01, Meta loss averaged over last 500 steps = 3.1196e-01, PNorm = 104.6090, GNorm = 0.3057
Meta loss on this task batch = 3.2261e-01, Meta loss averaged over last 500 steps = 3.1197e-01, PNorm = 104.6086, GNorm = 0.3259
Meta loss on this task batch = 3.3141e-01, Meta loss averaged over last 500 steps = 3.1197e-01, PNorm = 104.6089, GNorm = 0.2769
Meta loss on this task batch = 3.9357e-01, Meta loss averaged over last 500 steps = 3.1219e-01, PNorm = 104.6081, GNorm = 0.3411
Meta loss on this task batch = 3.2434e-01, Meta loss averaged over last 500 steps = 3.1217e-01, PNorm = 104.6102, GNorm = 0.3732
Took 177.20849919319153 seconds to complete one epoch of meta training
Took 186.4500629901886 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479191
Epoch 324
Meta loss on this task batch = 2.8609e-01, Meta loss averaged over last 500 steps = 3.1215e-01, PNorm = 104.6143, GNorm = 0.3013
Meta loss on this task batch = 2.3183e-01, Meta loss averaged over last 500 steps = 3.1198e-01, PNorm = 104.6219, GNorm = 0.2567
Meta loss on this task batch = 3.1811e-01, Meta loss averaged over last 500 steps = 3.1201e-01, PNorm = 104.6318, GNorm = 0.3286
Meta loss on this task batch = 3.1241e-01, Meta loss averaged over last 500 steps = 3.1208e-01, PNorm = 104.6420, GNorm = 0.2938
Meta loss on this task batch = 3.4856e-01, Meta loss averaged over last 500 steps = 3.1213e-01, PNorm = 104.6519, GNorm = 0.3264
Meta loss on this task batch = 3.0312e-01, Meta loss averaged over last 500 steps = 3.1214e-01, PNorm = 104.6617, GNorm = 0.2938
Meta loss on this task batch = 3.5436e-01, Meta loss averaged over last 500 steps = 3.1209e-01, PNorm = 104.6692, GNorm = 0.2773
Meta loss on this task batch = 2.9172e-01, Meta loss averaged over last 500 steps = 3.1213e-01, PNorm = 104.6744, GNorm = 0.2910
Meta loss on this task batch = 2.6463e-01, Meta loss averaged over last 500 steps = 3.1206e-01, PNorm = 104.6776, GNorm = 0.2641
Meta loss on this task batch = 3.3256e-01, Meta loss averaged over last 500 steps = 3.1213e-01, PNorm = 104.6801, GNorm = 0.2926
Meta loss on this task batch = 3.6845e-01, Meta loss averaged over last 500 steps = 3.1222e-01, PNorm = 104.6786, GNorm = 0.3708
Meta loss on this task batch = 3.2760e-01, Meta loss averaged over last 500 steps = 3.1228e-01, PNorm = 104.6765, GNorm = 0.3355
Meta loss on this task batch = 3.4470e-01, Meta loss averaged over last 500 steps = 3.1233e-01, PNorm = 104.6707, GNorm = 0.3447
Meta loss on this task batch = 3.7204e-01, Meta loss averaged over last 500 steps = 3.1243e-01, PNorm = 104.6614, GNorm = 0.3738
Meta loss on this task batch = 2.8800e-01, Meta loss averaged over last 500 steps = 3.1243e-01, PNorm = 104.6540, GNorm = 0.2787
Meta loss on this task batch = 2.8441e-01, Meta loss averaged over last 500 steps = 3.1243e-01, PNorm = 104.6504, GNorm = 0.2882
Meta loss on this task batch = 3.4165e-01, Meta loss averaged over last 500 steps = 3.1250e-01, PNorm = 104.6493, GNorm = 0.2899
Meta loss on this task batch = 3.1191e-01, Meta loss averaged over last 500 steps = 3.1255e-01, PNorm = 104.6525, GNorm = 0.2598
Meta loss on this task batch = 3.0124e-01, Meta loss averaged over last 500 steps = 3.1263e-01, PNorm = 104.6579, GNorm = 0.3302
Took 175.3005769252777 seconds to complete one epoch of meta training
Took 183.72113490104675 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486319
Epoch 325
Meta loss on this task batch = 3.3897e-01, Meta loss averaged over last 500 steps = 3.1256e-01, PNorm = 104.6649, GNorm = 0.2956
Meta loss on this task batch = 3.0430e-01, Meta loss averaged over last 500 steps = 3.1247e-01, PNorm = 104.6742, GNorm = 0.2674
Meta loss on this task batch = 3.0338e-01, Meta loss averaged over last 500 steps = 3.1252e-01, PNorm = 104.6828, GNorm = 0.2853
Meta loss on this task batch = 3.1751e-01, Meta loss averaged over last 500 steps = 3.1263e-01, PNorm = 104.6931, GNorm = 0.2965
Meta loss on this task batch = 3.2794e-01, Meta loss averaged over last 500 steps = 3.1267e-01, PNorm = 104.7021, GNorm = 0.2989
Meta loss on this task batch = 3.1507e-01, Meta loss averaged over last 500 steps = 3.1268e-01, PNorm = 104.7099, GNorm = 0.2550
Meta loss on this task batch = 2.7881e-01, Meta loss averaged over last 500 steps = 3.1268e-01, PNorm = 104.7160, GNorm = 0.3041
Meta loss on this task batch = 3.2874e-01, Meta loss averaged over last 500 steps = 3.1279e-01, PNorm = 104.7212, GNorm = 0.2686
Meta loss on this task batch = 3.5445e-01, Meta loss averaged over last 500 steps = 3.1285e-01, PNorm = 104.7240, GNorm = 0.3634
Meta loss on this task batch = 3.3249e-01, Meta loss averaged over last 500 steps = 3.1295e-01, PNorm = 104.7253, GNorm = 0.3202
Meta loss on this task batch = 3.0159e-01, Meta loss averaged over last 500 steps = 3.1290e-01, PNorm = 104.7270, GNorm = 0.2571
Meta loss on this task batch = 3.3140e-01, Meta loss averaged over last 500 steps = 3.1293e-01, PNorm = 104.7292, GNorm = 0.2873
Meta loss on this task batch = 3.0271e-01, Meta loss averaged over last 500 steps = 3.1293e-01, PNorm = 104.7329, GNorm = 0.2704
Meta loss on this task batch = 2.9865e-01, Meta loss averaged over last 500 steps = 3.1292e-01, PNorm = 104.7386, GNorm = 0.2718
Meta loss on this task batch = 2.7275e-01, Meta loss averaged over last 500 steps = 3.1278e-01, PNorm = 104.7473, GNorm = 0.2600
Meta loss on this task batch = 3.2411e-01, Meta loss averaged over last 500 steps = 3.1277e-01, PNorm = 104.7572, GNorm = 0.2869
Meta loss on this task batch = 3.1842e-01, Meta loss averaged over last 500 steps = 3.1275e-01, PNorm = 104.7696, GNorm = 0.2768
Meta loss on this task batch = 2.9977e-01, Meta loss averaged over last 500 steps = 3.1273e-01, PNorm = 104.7825, GNorm = 0.2879
Meta loss on this task batch = 3.0861e-01, Meta loss averaged over last 500 steps = 3.1271e-01, PNorm = 104.7955, GNorm = 0.3430
Took 134.38931393623352 seconds to complete one epoch of meta training
Took 142.98270750045776 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482029
Epoch 326
Meta loss on this task batch = 2.4402e-01, Meta loss averaged over last 500 steps = 3.1254e-01, PNorm = 104.8067, GNorm = 0.2649
Meta loss on this task batch = 3.1827e-01, Meta loss averaged over last 500 steps = 3.1246e-01, PNorm = 104.8160, GNorm = 0.3042
Meta loss on this task batch = 2.8174e-01, Meta loss averaged over last 500 steps = 3.1243e-01, PNorm = 104.8235, GNorm = 0.2725
Meta loss on this task batch = 3.4590e-01, Meta loss averaged over last 500 steps = 3.1238e-01, PNorm = 104.8281, GNorm = 0.3740
Meta loss on this task batch = 3.3736e-01, Meta loss averaged over last 500 steps = 3.1241e-01, PNorm = 104.8297, GNorm = 0.3071
Meta loss on this task batch = 2.7482e-01, Meta loss averaged over last 500 steps = 3.1241e-01, PNorm = 104.8296, GNorm = 0.2845
Meta loss on this task batch = 3.2896e-01, Meta loss averaged over last 500 steps = 3.1239e-01, PNorm = 104.8287, GNorm = 0.2795
Meta loss on this task batch = 3.5256e-01, Meta loss averaged over last 500 steps = 3.1251e-01, PNorm = 104.8270, GNorm = 0.3364
Meta loss on this task batch = 3.3445e-01, Meta loss averaged over last 500 steps = 3.1262e-01, PNorm = 104.8266, GNorm = 0.2941
Meta loss on this task batch = 2.7336e-01, Meta loss averaged over last 500 steps = 3.1246e-01, PNorm = 104.8289, GNorm = 0.2746
Meta loss on this task batch = 3.1546e-01, Meta loss averaged over last 500 steps = 3.1235e-01, PNorm = 104.8326, GNorm = 0.3247
Meta loss on this task batch = 3.2280e-01, Meta loss averaged over last 500 steps = 3.1235e-01, PNorm = 104.8366, GNorm = 0.2959
Meta loss on this task batch = 3.2162e-01, Meta loss averaged over last 500 steps = 3.1239e-01, PNorm = 104.8419, GNorm = 0.3058
Meta loss on this task batch = 2.6742e-01, Meta loss averaged over last 500 steps = 3.1220e-01, PNorm = 104.8499, GNorm = 0.2715
Meta loss on this task batch = 2.9940e-01, Meta loss averaged over last 500 steps = 3.1233e-01, PNorm = 104.8582, GNorm = 0.2863
Meta loss on this task batch = 2.9294e-01, Meta loss averaged over last 500 steps = 3.1243e-01, PNorm = 104.8672, GNorm = 0.2945
Meta loss on this task batch = 2.8672e-01, Meta loss averaged over last 500 steps = 3.1226e-01, PNorm = 104.8771, GNorm = 0.3242
Meta loss on this task batch = 3.5330e-01, Meta loss averaged over last 500 steps = 3.1229e-01, PNorm = 104.8840, GNorm = 0.3130
Meta loss on this task batch = 2.5573e-01, Meta loss averaged over last 500 steps = 3.1217e-01, PNorm = 104.8896, GNorm = 0.3095
Took 135.02253437042236 seconds to complete one epoch of meta training
Took 143.45104098320007 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496623
Epoch 327
Meta loss on this task batch = 3.0796e-01, Meta loss averaged over last 500 steps = 3.1218e-01, PNorm = 104.8938, GNorm = 0.2993
Meta loss on this task batch = 3.0708e-01, Meta loss averaged over last 500 steps = 3.1222e-01, PNorm = 104.8968, GNorm = 0.3675
Meta loss on this task batch = 3.0929e-01, Meta loss averaged over last 500 steps = 3.1219e-01, PNorm = 104.9034, GNorm = 0.2955
Meta loss on this task batch = 3.1785e-01, Meta loss averaged over last 500 steps = 3.1216e-01, PNorm = 104.9109, GNorm = 0.3039
Meta loss on this task batch = 3.3954e-01, Meta loss averaged over last 500 steps = 3.1231e-01, PNorm = 104.9205, GNorm = 0.3250
Meta loss on this task batch = 2.7593e-01, Meta loss averaged over last 500 steps = 3.1209e-01, PNorm = 104.9334, GNorm = 0.3060
Meta loss on this task batch = 2.8833e-01, Meta loss averaged over last 500 steps = 3.1200e-01, PNorm = 104.9462, GNorm = 0.3414
Meta loss on this task batch = 3.2705e-01, Meta loss averaged over last 500 steps = 3.1206e-01, PNorm = 104.9582, GNorm = 0.2743
Meta loss on this task batch = 2.4219e-01, Meta loss averaged over last 500 steps = 3.1187e-01, PNorm = 104.9683, GNorm = 0.2841
Meta loss on this task batch = 2.9142e-01, Meta loss averaged over last 500 steps = 3.1175e-01, PNorm = 104.9792, GNorm = 0.2919
Meta loss on this task batch = 3.0409e-01, Meta loss averaged over last 500 steps = 3.1176e-01, PNorm = 104.9895, GNorm = 0.2811
Meta loss on this task batch = 3.5291e-01, Meta loss averaged over last 500 steps = 3.1189e-01, PNorm = 104.9969, GNorm = 0.3415
Meta loss on this task batch = 2.8931e-01, Meta loss averaged over last 500 steps = 3.1195e-01, PNorm = 105.0063, GNorm = 0.3158
Meta loss on this task batch = 3.1512e-01, Meta loss averaged over last 500 steps = 3.1202e-01, PNorm = 105.0119, GNorm = 0.3731
Meta loss on this task batch = 3.3596e-01, Meta loss averaged over last 500 steps = 3.1203e-01, PNorm = 105.0159, GNorm = 0.3251
Meta loss on this task batch = 3.5894e-01, Meta loss averaged over last 500 steps = 3.1219e-01, PNorm = 105.0202, GNorm = 0.3263
Meta loss on this task batch = 2.9701e-01, Meta loss averaged over last 500 steps = 3.1211e-01, PNorm = 105.0268, GNorm = 0.2533
Meta loss on this task batch = 3.2116e-01, Meta loss averaged over last 500 steps = 3.1211e-01, PNorm = 105.0320, GNorm = 0.3179
Meta loss on this task batch = 2.9639e-01, Meta loss averaged over last 500 steps = 3.1199e-01, PNorm = 105.0397, GNorm = 0.3238
Took 160.6842906475067 seconds to complete one epoch of meta training
Took 169.20867586135864 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488353
Epoch 328
Meta loss on this task batch = 3.0823e-01, Meta loss averaged over last 500 steps = 3.1193e-01, PNorm = 105.0468, GNorm = 0.2768
Meta loss on this task batch = 3.3725e-01, Meta loss averaged over last 500 steps = 3.1191e-01, PNorm = 105.0535, GNorm = 0.2602
Meta loss on this task batch = 3.4422e-01, Meta loss averaged over last 500 steps = 3.1198e-01, PNorm = 105.0605, GNorm = 0.3250
Meta loss on this task batch = 3.1232e-01, Meta loss averaged over last 500 steps = 3.1188e-01, PNorm = 105.0683, GNorm = 0.2867
Meta loss on this task batch = 3.7424e-01, Meta loss averaged over last 500 steps = 3.1200e-01, PNorm = 105.0751, GNorm = 0.3027
Meta loss on this task batch = 3.0286e-01, Meta loss averaged over last 500 steps = 3.1202e-01, PNorm = 105.0847, GNorm = 0.2836
Meta loss on this task batch = 2.9881e-01, Meta loss averaged over last 500 steps = 3.1196e-01, PNorm = 105.0971, GNorm = 0.2982
Meta loss on this task batch = 2.6660e-01, Meta loss averaged over last 500 steps = 3.1190e-01, PNorm = 105.1087, GNorm = 0.2331
Meta loss on this task batch = 3.0552e-01, Meta loss averaged over last 500 steps = 3.1193e-01, PNorm = 105.1186, GNorm = 0.2627
Meta loss on this task batch = 2.7766e-01, Meta loss averaged over last 500 steps = 3.1194e-01, PNorm = 105.1286, GNorm = 0.2526
Meta loss on this task batch = 2.9032e-01, Meta loss averaged over last 500 steps = 3.1191e-01, PNorm = 105.1372, GNorm = 0.2945
Meta loss on this task batch = 3.3209e-01, Meta loss averaged over last 500 steps = 3.1179e-01, PNorm = 105.1421, GNorm = 0.3119
Meta loss on this task batch = 2.9976e-01, Meta loss averaged over last 500 steps = 3.1169e-01, PNorm = 105.1450, GNorm = 0.3123
Meta loss on this task batch = 2.7924e-01, Meta loss averaged over last 500 steps = 3.1172e-01, PNorm = 105.1452, GNorm = 0.2821
Meta loss on this task batch = 3.2158e-01, Meta loss averaged over last 500 steps = 3.1177e-01, PNorm = 105.1435, GNorm = 0.3150
Meta loss on this task batch = 3.2737e-01, Meta loss averaged over last 500 steps = 3.1181e-01, PNorm = 105.1406, GNorm = 0.3155
Meta loss on this task batch = 3.1186e-01, Meta loss averaged over last 500 steps = 3.1180e-01, PNorm = 105.1376, GNorm = 0.3739
Meta loss on this task batch = 2.8863e-01, Meta loss averaged over last 500 steps = 3.1177e-01, PNorm = 105.1357, GNorm = 0.2592
Meta loss on this task batch = 2.2684e-01, Meta loss averaged over last 500 steps = 3.1151e-01, PNorm = 105.1365, GNorm = 0.2999
Took 135.04875540733337 seconds to complete one epoch of meta training
Took 143.67380046844482 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492533
Epoch 329
Meta loss on this task batch = 3.4284e-01, Meta loss averaged over last 500 steps = 3.1158e-01, PNorm = 105.1379, GNorm = 0.2718
Meta loss on this task batch = 2.8102e-01, Meta loss averaged over last 500 steps = 3.1141e-01, PNorm = 105.1413, GNorm = 0.3072
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 3.1140e-01, PNorm = 105.1445, GNorm = 0.2859
Meta loss on this task batch = 3.4441e-01, Meta loss averaged over last 500 steps = 3.1143e-01, PNorm = 105.1484, GNorm = 0.3118
Meta loss on this task batch = 2.4877e-01, Meta loss averaged over last 500 steps = 3.1128e-01, PNorm = 105.1543, GNorm = 0.2667
Meta loss on this task batch = 2.9701e-01, Meta loss averaged over last 500 steps = 3.1132e-01, PNorm = 105.1596, GNorm = 0.2896
Meta loss on this task batch = 2.7441e-01, Meta loss averaged over last 500 steps = 3.1138e-01, PNorm = 105.1642, GNorm = 0.2559
Meta loss on this task batch = 3.4061e-01, Meta loss averaged over last 500 steps = 3.1140e-01, PNorm = 105.1665, GNorm = 0.3002
Meta loss on this task batch = 2.7430e-01, Meta loss averaged over last 500 steps = 3.1129e-01, PNorm = 105.1659, GNorm = 0.2552
Meta loss on this task batch = 2.7230e-01, Meta loss averaged over last 500 steps = 3.1122e-01, PNorm = 105.1663, GNorm = 0.2669
Meta loss on this task batch = 3.8463e-01, Meta loss averaged over last 500 steps = 3.1126e-01, PNorm = 105.1627, GNorm = 0.3706
Meta loss on this task batch = 3.3643e-01, Meta loss averaged over last 500 steps = 3.1119e-01, PNorm = 105.1632, GNorm = 0.3175
Meta loss on this task batch = 3.1404e-01, Meta loss averaged over last 500 steps = 3.1114e-01, PNorm = 105.1655, GNorm = 0.2703
Meta loss on this task batch = 2.7817e-01, Meta loss averaged over last 500 steps = 3.1118e-01, PNorm = 105.1712, GNorm = 0.2647
Meta loss on this task batch = 3.1730e-01, Meta loss averaged over last 500 steps = 3.1120e-01, PNorm = 105.1810, GNorm = 0.3006
Meta loss on this task batch = 3.1372e-01, Meta loss averaged over last 500 steps = 3.1116e-01, PNorm = 105.1928, GNorm = 0.3427
Meta loss on this task batch = 2.6458e-01, Meta loss averaged over last 500 steps = 3.1098e-01, PNorm = 105.2056, GNorm = 0.2812
Meta loss on this task batch = 3.2016e-01, Meta loss averaged over last 500 steps = 3.1112e-01, PNorm = 105.2174, GNorm = 0.2557
Meta loss on this task batch = 3.5679e-01, Meta loss averaged over last 500 steps = 3.1118e-01, PNorm = 105.2301, GNorm = 0.3677
Took 133.56427836418152 seconds to complete one epoch of meta training
Took 142.33070516586304 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478413
Epoch 330
Meta loss on this task batch = 3.1229e-01, Meta loss averaged over last 500 steps = 3.1123e-01, PNorm = 105.2402, GNorm = 0.3220
Meta loss on this task batch = 3.4477e-01, Meta loss averaged over last 500 steps = 3.1128e-01, PNorm = 105.2462, GNorm = 0.3436
Meta loss on this task batch = 2.5658e-01, Meta loss averaged over last 500 steps = 3.1111e-01, PNorm = 105.2513, GNorm = 0.2404
Meta loss on this task batch = 2.8988e-01, Meta loss averaged over last 500 steps = 3.1100e-01, PNorm = 105.2560, GNorm = 0.2927
Meta loss on this task batch = 3.1178e-01, Meta loss averaged over last 500 steps = 3.1096e-01, PNorm = 105.2603, GNorm = 0.2742
Meta loss on this task batch = 2.9900e-01, Meta loss averaged over last 500 steps = 3.1092e-01, PNorm = 105.2671, GNorm = 0.2791
Meta loss on this task batch = 2.7032e-01, Meta loss averaged over last 500 steps = 3.1095e-01, PNorm = 105.2731, GNorm = 0.2423
Meta loss on this task batch = 2.6699e-01, Meta loss averaged over last 500 steps = 3.1073e-01, PNorm = 105.2796, GNorm = 0.2435
Meta loss on this task batch = 3.6822e-01, Meta loss averaged over last 500 steps = 3.1088e-01, PNorm = 105.2875, GNorm = 0.3040
Meta loss on this task batch = 3.4151e-01, Meta loss averaged over last 500 steps = 3.1096e-01, PNorm = 105.2960, GNorm = 0.3099
Meta loss on this task batch = 2.9001e-01, Meta loss averaged over last 500 steps = 3.1089e-01, PNorm = 105.3024, GNorm = 0.3013
Meta loss on this task batch = 3.0093e-01, Meta loss averaged over last 500 steps = 3.1084e-01, PNorm = 105.3064, GNorm = 0.2828
Meta loss on this task batch = 3.0698e-01, Meta loss averaged over last 500 steps = 3.1080e-01, PNorm = 105.3124, GNorm = 0.2886
Meta loss on this task batch = 3.1321e-01, Meta loss averaged over last 500 steps = 3.1090e-01, PNorm = 105.3178, GNorm = 0.3278
Meta loss on this task batch = 2.8033e-01, Meta loss averaged over last 500 steps = 3.1078e-01, PNorm = 105.3234, GNorm = 0.2693
Meta loss on this task batch = 3.5684e-01, Meta loss averaged over last 500 steps = 3.1086e-01, PNorm = 105.3285, GNorm = 0.2932
Meta loss on this task batch = 2.9484e-01, Meta loss averaged over last 500 steps = 3.1076e-01, PNorm = 105.3333, GNorm = 0.2934
Meta loss on this task batch = 3.3759e-01, Meta loss averaged over last 500 steps = 3.1083e-01, PNorm = 105.3375, GNorm = 0.2924
Meta loss on this task batch = 2.7098e-01, Meta loss averaged over last 500 steps = 3.1082e-01, PNorm = 105.3414, GNorm = 0.2957
Took 134.75736141204834 seconds to complete one epoch of meta training
Took 141.99542260169983 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479389
Epoch 331
Meta loss on this task batch = 2.9904e-01, Meta loss averaged over last 500 steps = 3.1080e-01, PNorm = 105.3435, GNorm = 0.2725
Meta loss on this task batch = 3.5457e-01, Meta loss averaged over last 500 steps = 3.1086e-01, PNorm = 105.3437, GNorm = 0.3599
Meta loss on this task batch = 3.3112e-01, Meta loss averaged over last 500 steps = 3.1094e-01, PNorm = 105.3433, GNorm = 0.2897
Meta loss on this task batch = 2.6872e-01, Meta loss averaged over last 500 steps = 3.1083e-01, PNorm = 105.3426, GNorm = 0.2643
Meta loss on this task batch = 3.2455e-01, Meta loss averaged over last 500 steps = 3.1084e-01, PNorm = 105.3442, GNorm = 0.2711
Meta loss on this task batch = 2.5789e-01, Meta loss averaged over last 500 steps = 3.1080e-01, PNorm = 105.3478, GNorm = 0.2909
Meta loss on this task batch = 2.9531e-01, Meta loss averaged over last 500 steps = 3.1085e-01, PNorm = 105.3516, GNorm = 0.2768
Meta loss on this task batch = 3.0305e-01, Meta loss averaged over last 500 steps = 3.1081e-01, PNorm = 105.3561, GNorm = 0.2690
Meta loss on this task batch = 2.7548e-01, Meta loss averaged over last 500 steps = 3.1074e-01, PNorm = 105.3622, GNorm = 0.2705
Meta loss on this task batch = 3.2419e-01, Meta loss averaged over last 500 steps = 3.1083e-01, PNorm = 105.3657, GNorm = 0.3259
Meta loss on this task batch = 3.4199e-01, Meta loss averaged over last 500 steps = 3.1072e-01, PNorm = 105.3674, GNorm = 0.3569
Meta loss on this task batch = 3.4766e-01, Meta loss averaged over last 500 steps = 3.1083e-01, PNorm = 105.3708, GNorm = 0.3309
Meta loss on this task batch = 2.6576e-01, Meta loss averaged over last 500 steps = 3.1074e-01, PNorm = 105.3745, GNorm = 0.2598
Meta loss on this task batch = 3.3574e-01, Meta loss averaged over last 500 steps = 3.1073e-01, PNorm = 105.3805, GNorm = 0.3809
Meta loss on this task batch = 2.8393e-01, Meta loss averaged over last 500 steps = 3.1065e-01, PNorm = 105.3846, GNorm = 0.2965
Meta loss on this task batch = 3.1251e-01, Meta loss averaged over last 500 steps = 3.1059e-01, PNorm = 105.3901, GNorm = 0.3345
Meta loss on this task batch = 3.4518e-01, Meta loss averaged over last 500 steps = 3.1064e-01, PNorm = 105.3949, GNorm = 0.3044
Meta loss on this task batch = 2.9655e-01, Meta loss averaged over last 500 steps = 3.1051e-01, PNorm = 105.4009, GNorm = 0.3003
Meta loss on this task batch = 3.7291e-01, Meta loss averaged over last 500 steps = 3.1052e-01, PNorm = 105.4067, GNorm = 0.3504
Took 136.3596522808075 seconds to complete one epoch of meta training
Took 144.10096788406372 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455596
Epoch 332
Meta loss on this task batch = 3.6039e-01, Meta loss averaged over last 500 steps = 3.1059e-01, PNorm = 105.4098, GNorm = 0.3052
Meta loss on this task batch = 2.9178e-01, Meta loss averaged over last 500 steps = 3.1069e-01, PNorm = 105.4139, GNorm = 0.2931
Meta loss on this task batch = 2.9901e-01, Meta loss averaged over last 500 steps = 3.1073e-01, PNorm = 105.4214, GNorm = 0.2729
Meta loss on this task batch = 2.6880e-01, Meta loss averaged over last 500 steps = 3.1063e-01, PNorm = 105.4307, GNorm = 0.2659
Meta loss on this task batch = 3.3534e-01, Meta loss averaged over last 500 steps = 3.1063e-01, PNorm = 105.4399, GNorm = 0.2818
Meta loss on this task batch = 2.8086e-01, Meta loss averaged over last 500 steps = 3.1053e-01, PNorm = 105.4492, GNorm = 0.2693
Meta loss on this task batch = 3.9257e-01, Meta loss averaged over last 500 steps = 3.1062e-01, PNorm = 105.4565, GNorm = 0.3369
Meta loss on this task batch = 3.9776e-01, Meta loss averaged over last 500 steps = 3.1073e-01, PNorm = 105.4633, GNorm = 0.3427
Meta loss on this task batch = 3.0517e-01, Meta loss averaged over last 500 steps = 3.1077e-01, PNorm = 105.4670, GNorm = 0.3476
Meta loss on this task batch = 2.9617e-01, Meta loss averaged over last 500 steps = 3.1081e-01, PNorm = 105.4699, GNorm = 0.3011
Meta loss on this task batch = 3.1842e-01, Meta loss averaged over last 500 steps = 3.1094e-01, PNorm = 105.4704, GNorm = 0.3304
Meta loss on this task batch = 2.9320e-01, Meta loss averaged over last 500 steps = 3.1091e-01, PNorm = 105.4706, GNorm = 0.2740
Meta loss on this task batch = 2.6500e-01, Meta loss averaged over last 500 steps = 3.1078e-01, PNorm = 105.4693, GNorm = 0.3022
Meta loss on this task batch = 2.7682e-01, Meta loss averaged over last 500 steps = 3.1066e-01, PNorm = 105.4706, GNorm = 0.2555
Meta loss on this task batch = 3.2331e-01, Meta loss averaged over last 500 steps = 3.1066e-01, PNorm = 105.4703, GNorm = 0.2952
Meta loss on this task batch = 3.2940e-01, Meta loss averaged over last 500 steps = 3.1064e-01, PNorm = 105.4727, GNorm = 0.3197
Meta loss on this task batch = 2.7969e-01, Meta loss averaged over last 500 steps = 3.1054e-01, PNorm = 105.4769, GNorm = 0.2684
Meta loss on this task batch = 3.1136e-01, Meta loss averaged over last 500 steps = 3.1055e-01, PNorm = 105.4842, GNorm = 0.3090
Meta loss on this task batch = 2.8299e-01, Meta loss averaged over last 500 steps = 3.1046e-01, PNorm = 105.4921, GNorm = 0.3298
Took 134.5213484764099 seconds to complete one epoch of meta training
Took 141.94248032569885 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478160
Epoch 333
Meta loss on this task batch = 2.5976e-01, Meta loss averaged over last 500 steps = 3.1033e-01, PNorm = 105.4999, GNorm = 0.2530
Meta loss on this task batch = 3.2429e-01, Meta loss averaged over last 500 steps = 3.1034e-01, PNorm = 105.5085, GNorm = 0.3101
Meta loss on this task batch = 3.4896e-01, Meta loss averaged over last 500 steps = 3.1046e-01, PNorm = 105.5141, GNorm = 0.3029
Meta loss on this task batch = 3.0067e-01, Meta loss averaged over last 500 steps = 3.1039e-01, PNorm = 105.5208, GNorm = 0.3015
Meta loss on this task batch = 3.7578e-01, Meta loss averaged over last 500 steps = 3.1064e-01, PNorm = 105.5254, GNorm = 0.3118
Meta loss on this task batch = 2.8578e-01, Meta loss averaged over last 500 steps = 3.1051e-01, PNorm = 105.5300, GNorm = 0.2705
Meta loss on this task batch = 2.4599e-01, Meta loss averaged over last 500 steps = 3.1044e-01, PNorm = 105.5343, GNorm = 0.2540
Meta loss on this task batch = 3.1178e-01, Meta loss averaged over last 500 steps = 3.1044e-01, PNorm = 105.5390, GNorm = 0.2911
Meta loss on this task batch = 3.0247e-01, Meta loss averaged over last 500 steps = 3.1049e-01, PNorm = 105.5428, GNorm = 0.2648
Meta loss on this task batch = 3.0666e-01, Meta loss averaged over last 500 steps = 3.1041e-01, PNorm = 105.5459, GNorm = 0.3090
Meta loss on this task batch = 3.3803e-01, Meta loss averaged over last 500 steps = 3.1046e-01, PNorm = 105.5477, GNorm = 0.3572
Meta loss on this task batch = 3.1696e-01, Meta loss averaged over last 500 steps = 3.1046e-01, PNorm = 105.5499, GNorm = 0.3081
Meta loss on this task batch = 2.8666e-01, Meta loss averaged over last 500 steps = 3.1038e-01, PNorm = 105.5530, GNorm = 0.2887
Meta loss on this task batch = 3.0623e-01, Meta loss averaged over last 500 steps = 3.1029e-01, PNorm = 105.5557, GNorm = 0.2861
Meta loss on this task batch = 3.0345e-01, Meta loss averaged over last 500 steps = 3.1030e-01, PNorm = 105.5598, GNorm = 0.2604
Meta loss on this task batch = 2.8925e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 105.5680, GNorm = 0.3610
Meta loss on this task batch = 2.5626e-01, Meta loss averaged over last 500 steps = 3.1008e-01, PNorm = 105.5786, GNorm = 0.2596
Meta loss on this task batch = 3.0863e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 105.5885, GNorm = 0.2978
Meta loss on this task batch = 3.2112e-01, Meta loss averaged over last 500 steps = 3.1018e-01, PNorm = 105.5956, GNorm = 0.3500
Took 137.39075565338135 seconds to complete one epoch of meta training
Took 145.75104117393494 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480122
Epoch 334
Meta loss on this task batch = 3.1090e-01, Meta loss averaged over last 500 steps = 3.1011e-01, PNorm = 105.6037, GNorm = 0.2890
Meta loss on this task batch = 3.2240e-01, Meta loss averaged over last 500 steps = 3.1013e-01, PNorm = 105.6113, GNorm = 0.2738
Meta loss on this task batch = 3.0207e-01, Meta loss averaged over last 500 steps = 3.1021e-01, PNorm = 105.6166, GNorm = 0.2944
Meta loss on this task batch = 2.5135e-01, Meta loss averaged over last 500 steps = 3.1010e-01, PNorm = 105.6237, GNorm = 0.2639
Meta loss on this task batch = 2.6363e-01, Meta loss averaged over last 500 steps = 3.1005e-01, PNorm = 105.6321, GNorm = 0.2853
Meta loss on this task batch = 2.6380e-01, Meta loss averaged over last 500 steps = 3.1000e-01, PNorm = 105.6400, GNorm = 0.2487
Meta loss on this task batch = 3.9504e-01, Meta loss averaged over last 500 steps = 3.1012e-01, PNorm = 105.6465, GNorm = 0.3329
Meta loss on this task batch = 3.1894e-01, Meta loss averaged over last 500 steps = 3.1012e-01, PNorm = 105.6500, GNorm = 0.3210
Meta loss on this task batch = 3.4349e-01, Meta loss averaged over last 500 steps = 3.1030e-01, PNorm = 105.6515, GNorm = 0.3066
Meta loss on this task batch = 3.0012e-01, Meta loss averaged over last 500 steps = 3.1034e-01, PNorm = 105.6546, GNorm = 0.2591
Meta loss on this task batch = 3.0559e-01, Meta loss averaged over last 500 steps = 3.1031e-01, PNorm = 105.6571, GNorm = 0.2933
Meta loss on this task batch = 2.6853e-01, Meta loss averaged over last 500 steps = 3.1027e-01, PNorm = 105.6630, GNorm = 0.2720
Meta loss on this task batch = 3.4859e-01, Meta loss averaged over last 500 steps = 3.1023e-01, PNorm = 105.6692, GNorm = 0.3094
Meta loss on this task batch = 2.8952e-01, Meta loss averaged over last 500 steps = 3.1014e-01, PNorm = 105.6747, GNorm = 0.3061
Meta loss on this task batch = 3.4931e-01, Meta loss averaged over last 500 steps = 3.1018e-01, PNorm = 105.6800, GNorm = 0.3130
Meta loss on this task batch = 3.1422e-01, Meta loss averaged over last 500 steps = 3.1019e-01, PNorm = 105.6842, GNorm = 0.2882
Meta loss on this task batch = 3.2501e-01, Meta loss averaged over last 500 steps = 3.1024e-01, PNorm = 105.6885, GNorm = 0.3033
Meta loss on this task batch = 3.5831e-01, Meta loss averaged over last 500 steps = 3.1035e-01, PNorm = 105.6918, GNorm = 0.2928
Meta loss on this task batch = 2.5474e-01, Meta loss averaged over last 500 steps = 3.1024e-01, PNorm = 105.6963, GNorm = 0.2785
Took 309.14801049232483 seconds to complete one epoch of meta training
Took 318.1282424926758 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487034
Epoch 335
Meta loss on this task batch = 3.0365e-01, Meta loss averaged over last 500 steps = 3.1025e-01, PNorm = 105.7026, GNorm = 0.2691
Meta loss on this task batch = 3.6642e-01, Meta loss averaged over last 500 steps = 3.1031e-01, PNorm = 105.7100, GNorm = 0.2803
Meta loss on this task batch = 3.4090e-01, Meta loss averaged over last 500 steps = 3.1030e-01, PNorm = 105.7197, GNorm = 0.2647
Meta loss on this task batch = 3.2418e-01, Meta loss averaged over last 500 steps = 3.1026e-01, PNorm = 105.7302, GNorm = 0.3149
Meta loss on this task batch = 2.7991e-01, Meta loss averaged over last 500 steps = 3.1029e-01, PNorm = 105.7421, GNorm = 0.2862
Meta loss on this task batch = 2.8087e-01, Meta loss averaged over last 500 steps = 3.1014e-01, PNorm = 105.7521, GNorm = 0.2558
Meta loss on this task batch = 2.9071e-01, Meta loss averaged over last 500 steps = 3.0999e-01, PNorm = 105.7625, GNorm = 0.2641
Meta loss on this task batch = 2.9393e-01, Meta loss averaged over last 500 steps = 3.1005e-01, PNorm = 105.7709, GNorm = 0.2622
Meta loss on this task batch = 2.9598e-01, Meta loss averaged over last 500 steps = 3.1011e-01, PNorm = 105.7781, GNorm = 0.2520
Meta loss on this task batch = 3.0674e-01, Meta loss averaged over last 500 steps = 3.1007e-01, PNorm = 105.7829, GNorm = 0.3064
Meta loss on this task batch = 3.1343e-01, Meta loss averaged over last 500 steps = 3.0999e-01, PNorm = 105.7876, GNorm = 0.2930
Meta loss on this task batch = 3.2288e-01, Meta loss averaged over last 500 steps = 3.0992e-01, PNorm = 105.7913, GNorm = 0.3615
Meta loss on this task batch = 3.1613e-01, Meta loss averaged over last 500 steps = 3.0995e-01, PNorm = 105.7933, GNorm = 0.2783
Meta loss on this task batch = 2.5173e-01, Meta loss averaged over last 500 steps = 3.0988e-01, PNorm = 105.7973, GNorm = 0.2595
Meta loss on this task batch = 2.9788e-01, Meta loss averaged over last 500 steps = 3.0991e-01, PNorm = 105.8018, GNorm = 0.2640
Meta loss on this task batch = 2.8972e-01, Meta loss averaged over last 500 steps = 3.0976e-01, PNorm = 105.8081, GNorm = 0.2663
Meta loss on this task batch = 2.6167e-01, Meta loss averaged over last 500 steps = 3.0960e-01, PNorm = 105.8146, GNorm = 0.2618
Meta loss on this task batch = 2.8311e-01, Meta loss averaged over last 500 steps = 3.0956e-01, PNorm = 105.8206, GNorm = 0.2736
Meta loss on this task batch = 2.7998e-01, Meta loss averaged over last 500 steps = 3.0938e-01, PNorm = 105.8275, GNorm = 0.2723
Took 286.4042065143585 seconds to complete one epoch of meta training
Took 294.73504424095154 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493251
Epoch 336
Meta loss on this task batch = 2.8562e-01, Meta loss averaged over last 500 steps = 3.0931e-01, PNorm = 105.8333, GNorm = 0.2729
Meta loss on this task batch = 2.9669e-01, Meta loss averaged over last 500 steps = 3.0928e-01, PNorm = 105.8390, GNorm = 0.2815
Meta loss on this task batch = 2.9041e-01, Meta loss averaged over last 500 steps = 3.0920e-01, PNorm = 105.8452, GNorm = 0.2692
Meta loss on this task batch = 2.9649e-01, Meta loss averaged over last 500 steps = 3.0920e-01, PNorm = 105.8506, GNorm = 0.2873
Meta loss on this task batch = 3.2636e-01, Meta loss averaged over last 500 steps = 3.0922e-01, PNorm = 105.8550, GNorm = 0.3005
Meta loss on this task batch = 3.1632e-01, Meta loss averaged over last 500 steps = 3.0925e-01, PNorm = 105.8602, GNorm = 0.2873
Meta loss on this task batch = 3.5686e-01, Meta loss averaged over last 500 steps = 3.0938e-01, PNorm = 105.8670, GNorm = 0.3088
Meta loss on this task batch = 2.8490e-01, Meta loss averaged over last 500 steps = 3.0935e-01, PNorm = 105.8770, GNorm = 0.3077
Meta loss on this task batch = 2.4454e-01, Meta loss averaged over last 500 steps = 3.0915e-01, PNorm = 105.8888, GNorm = 0.2755
Meta loss on this task batch = 3.3284e-01, Meta loss averaged over last 500 steps = 3.0916e-01, PNorm = 105.8992, GNorm = 0.2955
Meta loss on this task batch = 3.1541e-01, Meta loss averaged over last 500 steps = 3.0917e-01, PNorm = 105.9089, GNorm = 0.3398
Meta loss on this task batch = 3.4136e-01, Meta loss averaged over last 500 steps = 3.0929e-01, PNorm = 105.9164, GNorm = 0.2827
Meta loss on this task batch = 3.3585e-01, Meta loss averaged over last 500 steps = 3.0944e-01, PNorm = 105.9227, GNorm = 0.3129
Meta loss on this task batch = 2.9997e-01, Meta loss averaged over last 500 steps = 3.0932e-01, PNorm = 105.9276, GNorm = 0.2760
Meta loss on this task batch = 3.2061e-01, Meta loss averaged over last 500 steps = 3.0936e-01, PNorm = 105.9318, GNorm = 0.2963
Meta loss on this task batch = 3.1915e-01, Meta loss averaged over last 500 steps = 3.0931e-01, PNorm = 105.9329, GNorm = 0.3103
Meta loss on this task batch = 3.0090e-01, Meta loss averaged over last 500 steps = 3.0936e-01, PNorm = 105.9369, GNorm = 0.2743
Meta loss on this task batch = 3.1868e-01, Meta loss averaged over last 500 steps = 3.0929e-01, PNorm = 105.9413, GNorm = 0.2820
Meta loss on this task batch = 2.9341e-01, Meta loss averaged over last 500 steps = 3.0936e-01, PNorm = 105.9452, GNorm = 0.3047
Took 131.17894005775452 seconds to complete one epoch of meta training
Took 139.95853281021118 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495047
Epoch 337
Meta loss on this task batch = 2.9567e-01, Meta loss averaged over last 500 steps = 3.0940e-01, PNorm = 105.9494, GNorm = 0.2652
Meta loss on this task batch = 3.0462e-01, Meta loss averaged over last 500 steps = 3.0948e-01, PNorm = 105.9543, GNorm = 0.2826
Meta loss on this task batch = 3.4477e-01, Meta loss averaged over last 500 steps = 3.0942e-01, PNorm = 105.9601, GNorm = 0.2610
Meta loss on this task batch = 3.4208e-01, Meta loss averaged over last 500 steps = 3.0942e-01, PNorm = 105.9665, GNorm = 0.2740
Meta loss on this task batch = 2.7861e-01, Meta loss averaged over last 500 steps = 3.0935e-01, PNorm = 105.9737, GNorm = 0.2725
Meta loss on this task batch = 3.2856e-01, Meta loss averaged over last 500 steps = 3.0934e-01, PNorm = 105.9792, GNorm = 0.2702
Meta loss on this task batch = 2.8248e-01, Meta loss averaged over last 500 steps = 3.0942e-01, PNorm = 105.9851, GNorm = 0.3103
Meta loss on this task batch = 3.0467e-01, Meta loss averaged over last 500 steps = 3.0946e-01, PNorm = 105.9920, GNorm = 0.2853
Meta loss on this task batch = 2.9622e-01, Meta loss averaged over last 500 steps = 3.0946e-01, PNorm = 105.9988, GNorm = 0.2557
Meta loss on this task batch = 3.2917e-01, Meta loss averaged over last 500 steps = 3.0943e-01, PNorm = 106.0064, GNorm = 0.3133
Meta loss on this task batch = 2.6303e-01, Meta loss averaged over last 500 steps = 3.0924e-01, PNorm = 106.0149, GNorm = 0.2605
Meta loss on this task batch = 2.7829e-01, Meta loss averaged over last 500 steps = 3.0915e-01, PNorm = 106.0246, GNorm = 0.2679
Meta loss on this task batch = 2.9699e-01, Meta loss averaged over last 500 steps = 3.0900e-01, PNorm = 106.0329, GNorm = 0.2608
Meta loss on this task batch = 3.0772e-01, Meta loss averaged over last 500 steps = 3.0894e-01, PNorm = 106.0377, GNorm = 0.3107
Meta loss on this task batch = 2.9012e-01, Meta loss averaged over last 500 steps = 3.0889e-01, PNorm = 106.0420, GNorm = 0.2753
Meta loss on this task batch = 3.3096e-01, Meta loss averaged over last 500 steps = 3.0890e-01, PNorm = 106.0437, GNorm = 0.2981
Meta loss on this task batch = 3.5415e-01, Meta loss averaged over last 500 steps = 3.0904e-01, PNorm = 106.0445, GNorm = 0.3023
Meta loss on this task batch = 3.5341e-01, Meta loss averaged over last 500 steps = 3.0924e-01, PNorm = 106.0433, GNorm = 0.3092
Meta loss on this task batch = 2.9886e-01, Meta loss averaged over last 500 steps = 3.0924e-01, PNorm = 106.0438, GNorm = 0.2902
Took 131.74191164970398 seconds to complete one epoch of meta training
Took 140.0959575176239 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476610
Epoch 338
Meta loss on this task batch = 2.7668e-01, Meta loss averaged over last 500 steps = 3.0908e-01, PNorm = 106.0464, GNorm = 0.2831
Meta loss on this task batch = 3.0229e-01, Meta loss averaged over last 500 steps = 3.0903e-01, PNorm = 106.0501, GNorm = 0.2778
Meta loss on this task batch = 3.0735e-01, Meta loss averaged over last 500 steps = 3.0905e-01, PNorm = 106.0572, GNorm = 0.3005
Meta loss on this task batch = 2.6797e-01, Meta loss averaged over last 500 steps = 3.0904e-01, PNorm = 106.0678, GNorm = 0.2935
Meta loss on this task batch = 3.4440e-01, Meta loss averaged over last 500 steps = 3.0910e-01, PNorm = 106.0776, GNorm = 0.3059
Meta loss on this task batch = 2.9643e-01, Meta loss averaged over last 500 steps = 3.0909e-01, PNorm = 106.0869, GNorm = 0.3106
Meta loss on this task batch = 3.0161e-01, Meta loss averaged over last 500 steps = 3.0904e-01, PNorm = 106.0956, GNorm = 0.3171
Meta loss on this task batch = 2.9164e-01, Meta loss averaged over last 500 steps = 3.0903e-01, PNorm = 106.1025, GNorm = 0.2683
Meta loss on this task batch = 2.7676e-01, Meta loss averaged over last 500 steps = 3.0906e-01, PNorm = 106.1097, GNorm = 0.2834
Meta loss on this task batch = 3.1541e-01, Meta loss averaged over last 500 steps = 3.0898e-01, PNorm = 106.1157, GNorm = 0.2979
Meta loss on this task batch = 3.6152e-01, Meta loss averaged over last 500 steps = 3.0906e-01, PNorm = 106.1190, GNorm = 0.3011
Meta loss on this task batch = 2.6830e-01, Meta loss averaged over last 500 steps = 3.0910e-01, PNorm = 106.1216, GNorm = 0.2674
Meta loss on this task batch = 3.1193e-01, Meta loss averaged over last 500 steps = 3.0906e-01, PNorm = 106.1228, GNorm = 0.2809
Meta loss on this task batch = 2.8042e-01, Meta loss averaged over last 500 steps = 3.0899e-01, PNorm = 106.1260, GNorm = 0.3019
Meta loss on this task batch = 2.7555e-01, Meta loss averaged over last 500 steps = 3.0897e-01, PNorm = 106.1295, GNorm = 0.2522
Meta loss on this task batch = 2.7059e-01, Meta loss averaged over last 500 steps = 3.0880e-01, PNorm = 106.1319, GNorm = 0.2828
Meta loss on this task batch = 2.8271e-01, Meta loss averaged over last 500 steps = 3.0875e-01, PNorm = 106.1320, GNorm = 0.2956
Meta loss on this task batch = 3.0653e-01, Meta loss averaged over last 500 steps = 3.0865e-01, PNorm = 106.1315, GNorm = 0.3387
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 3.0854e-01, PNorm = 106.1320, GNorm = 0.3447
Took 132.070068359375 seconds to complete one epoch of meta training
Took 140.63556742668152 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492204
Epoch 339
Meta loss on this task batch = 3.5481e-01, Meta loss averaged over last 500 steps = 3.0867e-01, PNorm = 106.1306, GNorm = 0.3645
Meta loss on this task batch = 2.6991e-01, Meta loss averaged over last 500 steps = 3.0857e-01, PNorm = 106.1313, GNorm = 0.2754
Meta loss on this task batch = 3.1615e-01, Meta loss averaged over last 500 steps = 3.0862e-01, PNorm = 106.1322, GNorm = 0.3045
Meta loss on this task batch = 3.1095e-01, Meta loss averaged over last 500 steps = 3.0869e-01, PNorm = 106.1340, GNorm = 0.2918
Meta loss on this task batch = 3.2064e-01, Meta loss averaged over last 500 steps = 3.0863e-01, PNorm = 106.1361, GNorm = 0.2759
Meta loss on this task batch = 3.0096e-01, Meta loss averaged over last 500 steps = 3.0879e-01, PNorm = 106.1387, GNorm = 0.2882
Meta loss on this task batch = 2.9324e-01, Meta loss averaged over last 500 steps = 3.0875e-01, PNorm = 106.1437, GNorm = 0.2744
Meta loss on this task batch = 2.7465e-01, Meta loss averaged over last 500 steps = 3.0869e-01, PNorm = 106.1506, GNorm = 0.2597
Meta loss on this task batch = 3.2775e-01, Meta loss averaged over last 500 steps = 3.0869e-01, PNorm = 106.1584, GNorm = 0.3107
Meta loss on this task batch = 3.0180e-01, Meta loss averaged over last 500 steps = 3.0865e-01, PNorm = 106.1658, GNorm = 0.2907
Meta loss on this task batch = 2.6613e-01, Meta loss averaged over last 500 steps = 3.0857e-01, PNorm = 106.1705, GNorm = 0.2758
Meta loss on this task batch = 3.1132e-01, Meta loss averaged over last 500 steps = 3.0852e-01, PNorm = 106.1720, GNorm = 0.3024
Meta loss on this task batch = 2.5491e-01, Meta loss averaged over last 500 steps = 3.0826e-01, PNorm = 106.1761, GNorm = 0.2177
Meta loss on this task batch = 3.2576e-01, Meta loss averaged over last 500 steps = 3.0831e-01, PNorm = 106.1766, GNorm = 0.3136
Meta loss on this task batch = 2.8738e-01, Meta loss averaged over last 500 steps = 3.0824e-01, PNorm = 106.1793, GNorm = 0.2363
Meta loss on this task batch = 3.1673e-01, Meta loss averaged over last 500 steps = 3.0818e-01, PNorm = 106.1857, GNorm = 0.3010
Meta loss on this task batch = 3.1687e-01, Meta loss averaged over last 500 steps = 3.0827e-01, PNorm = 106.1914, GNorm = 0.3015
Meta loss on this task batch = 2.7595e-01, Meta loss averaged over last 500 steps = 3.0824e-01, PNorm = 106.1979, GNorm = 0.2826
Meta loss on this task batch = 3.3210e-01, Meta loss averaged over last 500 steps = 3.0836e-01, PNorm = 106.2043, GNorm = 0.3435
Took 145.48716259002686 seconds to complete one epoch of meta training
Took 153.86252546310425 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501668
Epoch 340
Meta loss on this task batch = 3.0721e-01, Meta loss averaged over last 500 steps = 3.0851e-01, PNorm = 106.2113, GNorm = 0.3135
Meta loss on this task batch = 3.1605e-01, Meta loss averaged over last 500 steps = 3.0844e-01, PNorm = 106.2198, GNorm = 0.2894
Meta loss on this task batch = 3.9425e-01, Meta loss averaged over last 500 steps = 3.0862e-01, PNorm = 106.2261, GNorm = 0.3271
Meta loss on this task batch = 2.9828e-01, Meta loss averaged over last 500 steps = 3.0849e-01, PNorm = 106.2324, GNorm = 0.2919
Meta loss on this task batch = 2.9141e-01, Meta loss averaged over last 500 steps = 3.0853e-01, PNorm = 106.2380, GNorm = 0.2863
Meta loss on this task batch = 3.0018e-01, Meta loss averaged over last 500 steps = 3.0857e-01, PNorm = 106.2418, GNorm = 0.2600
Meta loss on this task batch = 3.1719e-01, Meta loss averaged over last 500 steps = 3.0859e-01, PNorm = 106.2428, GNorm = 0.3279
Meta loss on this task batch = 2.3399e-01, Meta loss averaged over last 500 steps = 3.0855e-01, PNorm = 106.2453, GNorm = 0.2319
Meta loss on this task batch = 3.0251e-01, Meta loss averaged over last 500 steps = 3.0853e-01, PNorm = 106.2470, GNorm = 0.2867
Meta loss on this task batch = 3.3532e-01, Meta loss averaged over last 500 steps = 3.0861e-01, PNorm = 106.2503, GNorm = 0.3033
Meta loss on this task batch = 3.2606e-01, Meta loss averaged over last 500 steps = 3.0861e-01, PNorm = 106.2536, GNorm = 0.2767
Meta loss on this task batch = 3.1402e-01, Meta loss averaged over last 500 steps = 3.0859e-01, PNorm = 106.2546, GNorm = 0.3255
Meta loss on this task batch = 3.0478e-01, Meta loss averaged over last 500 steps = 3.0865e-01, PNorm = 106.2565, GNorm = 0.2781
Meta loss on this task batch = 2.6573e-01, Meta loss averaged over last 500 steps = 3.0848e-01, PNorm = 106.2583, GNorm = 0.2911
Meta loss on this task batch = 3.3693e-01, Meta loss averaged over last 500 steps = 3.0850e-01, PNorm = 106.2598, GNorm = 0.3002
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 3.0842e-01, PNorm = 106.2648, GNorm = 0.2942
Meta loss on this task batch = 3.1104e-01, Meta loss averaged over last 500 steps = 3.0839e-01, PNorm = 106.2725, GNorm = 0.2858
Meta loss on this task batch = 2.3882e-01, Meta loss averaged over last 500 steps = 3.0822e-01, PNorm = 106.2811, GNorm = 0.2541
Meta loss on this task batch = 3.4801e-01, Meta loss averaged over last 500 steps = 3.0827e-01, PNorm = 106.2870, GNorm = 0.4130
Took 143.40796780586243 seconds to complete one epoch of meta training
Took 151.88041424751282 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487143
Epoch 341
Meta loss on this task batch = 2.8682e-01, Meta loss averaged over last 500 steps = 3.0827e-01, PNorm = 106.2930, GNorm = 0.2873
Meta loss on this task batch = 3.2400e-01, Meta loss averaged over last 500 steps = 3.0831e-01, PNorm = 106.2967, GNorm = 0.3326
Meta loss on this task batch = 3.0827e-01, Meta loss averaged over last 500 steps = 3.0817e-01, PNorm = 106.2982, GNorm = 0.2991
Meta loss on this task batch = 3.1327e-01, Meta loss averaged over last 500 steps = 3.0815e-01, PNorm = 106.3007, GNorm = 0.2936
Meta loss on this task batch = 3.2755e-01, Meta loss averaged over last 500 steps = 3.0823e-01, PNorm = 106.3043, GNorm = 0.2682
Meta loss on this task batch = 3.2982e-01, Meta loss averaged over last 500 steps = 3.0824e-01, PNorm = 106.3099, GNorm = 0.3010
Meta loss on this task batch = 2.4559e-01, Meta loss averaged over last 500 steps = 3.0804e-01, PNorm = 106.3181, GNorm = 0.2711
Meta loss on this task batch = 2.9012e-01, Meta loss averaged over last 500 steps = 3.0799e-01, PNorm = 106.3270, GNorm = 0.2781
Meta loss on this task batch = 2.4943e-01, Meta loss averaged over last 500 steps = 3.0788e-01, PNorm = 106.3369, GNorm = 0.2766
Meta loss on this task batch = 3.0954e-01, Meta loss averaged over last 500 steps = 3.0785e-01, PNorm = 106.3452, GNorm = 0.3054
Meta loss on this task batch = 3.2247e-01, Meta loss averaged over last 500 steps = 3.0797e-01, PNorm = 106.3488, GNorm = 0.3613
Meta loss on this task batch = 2.1977e-01, Meta loss averaged over last 500 steps = 3.0775e-01, PNorm = 106.3534, GNorm = 0.2522
Meta loss on this task batch = 3.2187e-01, Meta loss averaged over last 500 steps = 3.0770e-01, PNorm = 106.3582, GNorm = 0.3510
Meta loss on this task batch = 3.8430e-01, Meta loss averaged over last 500 steps = 3.0797e-01, PNorm = 106.3626, GNorm = 0.5119
Meta loss on this task batch = 2.8528e-01, Meta loss averaged over last 500 steps = 3.0789e-01, PNorm = 106.3671, GNorm = 0.3072
Meta loss on this task batch = 3.6351e-01, Meta loss averaged over last 500 steps = 3.0799e-01, PNorm = 106.3710, GNorm = 0.3040
Meta loss on this task batch = 3.6098e-01, Meta loss averaged over last 500 steps = 3.0801e-01, PNorm = 106.3732, GNorm = 0.3122
Meta loss on this task batch = 2.5416e-01, Meta loss averaged over last 500 steps = 3.0793e-01, PNorm = 106.3791, GNorm = 0.2771
Meta loss on this task batch = 3.5930e-01, Meta loss averaged over last 500 steps = 3.0794e-01, PNorm = 106.3831, GNorm = 0.4215
Took 142.54869484901428 seconds to complete one epoch of meta training
Took 151.24213933944702 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483313
Epoch 342
Meta loss on this task batch = 2.9526e-01, Meta loss averaged over last 500 steps = 3.0788e-01, PNorm = 106.3877, GNorm = 0.2818
Meta loss on this task batch = 2.8324e-01, Meta loss averaged over last 500 steps = 3.0786e-01, PNorm = 106.3954, GNorm = 0.2827
Meta loss on this task batch = 3.2074e-01, Meta loss averaged over last 500 steps = 3.0782e-01, PNorm = 106.3997, GNorm = 0.2951
Meta loss on this task batch = 2.7437e-01, Meta loss averaged over last 500 steps = 3.0770e-01, PNorm = 106.4056, GNorm = 0.2481
Meta loss on this task batch = 3.1944e-01, Meta loss averaged over last 500 steps = 3.0774e-01, PNorm = 106.4125, GNorm = 0.2702
Meta loss on this task batch = 2.7134e-01, Meta loss averaged over last 500 steps = 3.0770e-01, PNorm = 106.4199, GNorm = 0.2813
Meta loss on this task batch = 3.5302e-01, Meta loss averaged over last 500 steps = 3.0790e-01, PNorm = 106.4259, GNorm = 0.3149
Meta loss on this task batch = 3.3867e-01, Meta loss averaged over last 500 steps = 3.0804e-01, PNorm = 106.4336, GNorm = 0.3024
Meta loss on this task batch = 2.9633e-01, Meta loss averaged over last 500 steps = 3.0788e-01, PNorm = 106.4411, GNorm = 0.2699
Meta loss on this task batch = 2.4000e-01, Meta loss averaged over last 500 steps = 3.0768e-01, PNorm = 106.4493, GNorm = 0.2874
Meta loss on this task batch = 3.0625e-01, Meta loss averaged over last 500 steps = 3.0767e-01, PNorm = 106.4543, GNorm = 0.3657
Meta loss on this task batch = 2.7932e-01, Meta loss averaged over last 500 steps = 3.0745e-01, PNorm = 106.4591, GNorm = 0.3182
Meta loss on this task batch = 2.7668e-01, Meta loss averaged over last 500 steps = 3.0739e-01, PNorm = 106.4635, GNorm = 0.3299
Meta loss on this task batch = 2.7886e-01, Meta loss averaged over last 500 steps = 3.0722e-01, PNorm = 106.4704, GNorm = 0.3324
Meta loss on this task batch = 3.1998e-01, Meta loss averaged over last 500 steps = 3.0720e-01, PNorm = 106.4749, GNorm = 0.3249
Meta loss on this task batch = 3.4243e-01, Meta loss averaged over last 500 steps = 3.0719e-01, PNorm = 106.4791, GNorm = 0.3350
Meta loss on this task batch = 3.3459e-01, Meta loss averaged over last 500 steps = 3.0722e-01, PNorm = 106.4823, GNorm = 0.3060
Meta loss on this task batch = 3.7015e-01, Meta loss averaged over last 500 steps = 3.0743e-01, PNorm = 106.4821, GNorm = 0.4011
Meta loss on this task batch = 2.9331e-01, Meta loss averaged over last 500 steps = 3.0745e-01, PNorm = 106.4837, GNorm = 0.3405
Took 145.49133396148682 seconds to complete one epoch of meta training
Took 153.0367546081543 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484069
Epoch 343
Meta loss on this task batch = 3.2246e-01, Meta loss averaged over last 500 steps = 3.0751e-01, PNorm = 106.4855, GNorm = 0.3209
Meta loss on this task batch = 2.7819e-01, Meta loss averaged over last 500 steps = 3.0748e-01, PNorm = 106.4904, GNorm = 0.2835
Meta loss on this task batch = 2.7014e-01, Meta loss averaged over last 500 steps = 3.0747e-01, PNorm = 106.4977, GNorm = 0.2369
Meta loss on this task batch = 2.7827e-01, Meta loss averaged over last 500 steps = 3.0744e-01, PNorm = 106.5079, GNorm = 0.2918
Meta loss on this task batch = 3.4271e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 106.5197, GNorm = 0.2642
Meta loss on this task batch = 2.5666e-01, Meta loss averaged over last 500 steps = 3.0737e-01, PNorm = 106.5352, GNorm = 0.2707
Meta loss on this task batch = 2.7033e-01, Meta loss averaged over last 500 steps = 3.0726e-01, PNorm = 106.5507, GNorm = 0.2666
Meta loss on this task batch = 3.3470e-01, Meta loss averaged over last 500 steps = 3.0736e-01, PNorm = 106.5653, GNorm = 0.3319
Meta loss on this task batch = 2.6112e-01, Meta loss averaged over last 500 steps = 3.0734e-01, PNorm = 106.5796, GNorm = 0.2490
Meta loss on this task batch = 2.6218e-01, Meta loss averaged over last 500 steps = 3.0725e-01, PNorm = 106.5927, GNorm = 0.2628
Meta loss on this task batch = 3.2289e-01, Meta loss averaged over last 500 steps = 3.0729e-01, PNorm = 106.6015, GNorm = 0.4266
Meta loss on this task batch = 3.5306e-01, Meta loss averaged over last 500 steps = 3.0734e-01, PNorm = 106.6056, GNorm = 0.3169
Meta loss on this task batch = 2.9825e-01, Meta loss averaged over last 500 steps = 3.0736e-01, PNorm = 106.6097, GNorm = 0.2699
Meta loss on this task batch = 2.3420e-01, Meta loss averaged over last 500 steps = 3.0725e-01, PNorm = 106.6125, GNorm = 0.2662
Meta loss on this task batch = 3.7378e-01, Meta loss averaged over last 500 steps = 3.0744e-01, PNorm = 106.6106, GNorm = 0.3901
Meta loss on this task batch = 3.8217e-01, Meta loss averaged over last 500 steps = 3.0757e-01, PNorm = 106.6088, GNorm = 0.3272
Meta loss on this task batch = 3.5105e-01, Meta loss averaged over last 500 steps = 3.0760e-01, PNorm = 106.6093, GNorm = 0.3174
Meta loss on this task batch = 3.0143e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 106.6122, GNorm = 0.2809
Meta loss on this task batch = 3.1486e-01, Meta loss averaged over last 500 steps = 3.0762e-01, PNorm = 106.6166, GNorm = 0.3653
Took 152.22983479499817 seconds to complete one epoch of meta training
Took 161.2823212146759 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462980
Epoch 344
Meta loss on this task batch = 2.6164e-01, Meta loss averaged over last 500 steps = 3.0741e-01, PNorm = 106.6240, GNorm = 0.2839
Meta loss on this task batch = 2.5287e-01, Meta loss averaged over last 500 steps = 3.0737e-01, PNorm = 106.6358, GNorm = 0.2942
Meta loss on this task batch = 3.2943e-01, Meta loss averaged over last 500 steps = 3.0728e-01, PNorm = 106.6493, GNorm = 0.3145
Meta loss on this task batch = 3.3086e-01, Meta loss averaged over last 500 steps = 3.0733e-01, PNorm = 106.6650, GNorm = 0.3117
Meta loss on this task batch = 3.0033e-01, Meta loss averaged over last 500 steps = 3.0738e-01, PNorm = 106.6818, GNorm = 0.3159
Meta loss on this task batch = 3.1278e-01, Meta loss averaged over last 500 steps = 3.0732e-01, PNorm = 106.6958, GNorm = 0.3509
Meta loss on this task batch = 2.6245e-01, Meta loss averaged over last 500 steps = 3.0716e-01, PNorm = 106.7073, GNorm = 0.2943
Meta loss on this task batch = 3.0940e-01, Meta loss averaged over last 500 steps = 3.0710e-01, PNorm = 106.7165, GNorm = 0.3286
Meta loss on this task batch = 3.1906e-01, Meta loss averaged over last 500 steps = 3.0721e-01, PNorm = 106.7260, GNorm = 0.2667
Meta loss on this task batch = 2.9338e-01, Meta loss averaged over last 500 steps = 3.0720e-01, PNorm = 106.7347, GNorm = 0.2712
Meta loss on this task batch = 2.8117e-01, Meta loss averaged over last 500 steps = 3.0730e-01, PNorm = 106.7441, GNorm = 0.2888
Meta loss on this task batch = 3.9611e-01, Meta loss averaged over last 500 steps = 3.0748e-01, PNorm = 106.7532, GNorm = 0.3320
Meta loss on this task batch = 2.5478e-01, Meta loss averaged over last 500 steps = 3.0737e-01, PNorm = 106.7632, GNorm = 0.2794
Meta loss on this task batch = 3.0087e-01, Meta loss averaged over last 500 steps = 3.0729e-01, PNorm = 106.7739, GNorm = 0.2781
Meta loss on this task batch = 3.3462e-01, Meta loss averaged over last 500 steps = 3.0727e-01, PNorm = 106.7846, GNorm = 0.2826
Meta loss on this task batch = 3.7658e-01, Meta loss averaged over last 500 steps = 3.0749e-01, PNorm = 106.7932, GNorm = 0.3407
Meta loss on this task batch = 3.3133e-01, Meta loss averaged over last 500 steps = 3.0755e-01, PNorm = 106.8013, GNorm = 0.2968
Meta loss on this task batch = 2.2829e-01, Meta loss averaged over last 500 steps = 3.0742e-01, PNorm = 106.8101, GNorm = 0.2701
Meta loss on this task batch = 4.0588e-01, Meta loss averaged over last 500 steps = 3.0754e-01, PNorm = 106.8161, GNorm = 0.4168
Took 144.2212507724762 seconds to complete one epoch of meta training
Took 152.722323179245 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476602
Epoch 345
Meta loss on this task batch = 3.6506e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 106.8216, GNorm = 0.3125
Meta loss on this task batch = 3.0213e-01, Meta loss averaged over last 500 steps = 3.0754e-01, PNorm = 106.8263, GNorm = 0.2832
Meta loss on this task batch = 3.3148e-01, Meta loss averaged over last 500 steps = 3.0749e-01, PNorm = 106.8257, GNorm = 0.3220
Meta loss on this task batch = 3.2758e-01, Meta loss averaged over last 500 steps = 3.0759e-01, PNorm = 106.8261, GNorm = 0.2836
Meta loss on this task batch = 2.9090e-01, Meta loss averaged over last 500 steps = 3.0755e-01, PNorm = 106.8258, GNorm = 0.2862
Meta loss on this task batch = 3.0263e-01, Meta loss averaged over last 500 steps = 3.0759e-01, PNorm = 106.8257, GNorm = 0.2810
Meta loss on this task batch = 2.4725e-01, Meta loss averaged over last 500 steps = 3.0753e-01, PNorm = 106.8268, GNorm = 0.2523
Meta loss on this task batch = 2.7134e-01, Meta loss averaged over last 500 steps = 3.0734e-01, PNorm = 106.8287, GNorm = 0.2669
Meta loss on this task batch = 2.6760e-01, Meta loss averaged over last 500 steps = 3.0730e-01, PNorm = 106.8330, GNorm = 0.2446
Meta loss on this task batch = 2.8269e-01, Meta loss averaged over last 500 steps = 3.0724e-01, PNorm = 106.8374, GNorm = 0.2504
Meta loss on this task batch = 2.9086e-01, Meta loss averaged over last 500 steps = 3.0727e-01, PNorm = 106.8432, GNorm = 0.2840
Meta loss on this task batch = 2.9499e-01, Meta loss averaged over last 500 steps = 3.0722e-01, PNorm = 106.8492, GNorm = 0.2824
Meta loss on this task batch = 3.2656e-01, Meta loss averaged over last 500 steps = 3.0725e-01, PNorm = 106.8537, GNorm = 0.2877
Meta loss on this task batch = 3.4261e-01, Meta loss averaged over last 500 steps = 3.0729e-01, PNorm = 106.8593, GNorm = 0.2907
Meta loss on this task batch = 3.4590e-01, Meta loss averaged over last 500 steps = 3.0737e-01, PNorm = 106.8641, GNorm = 0.3557
Meta loss on this task batch = 3.2731e-01, Meta loss averaged over last 500 steps = 3.0749e-01, PNorm = 106.8702, GNorm = 0.2817
Meta loss on this task batch = 3.1100e-01, Meta loss averaged over last 500 steps = 3.0764e-01, PNorm = 106.8770, GNorm = 0.2974
Meta loss on this task batch = 2.6226e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 106.8854, GNorm = 0.2612
Meta loss on this task batch = 3.2505e-01, Meta loss averaged over last 500 steps = 3.0752e-01, PNorm = 106.8913, GNorm = 0.3281
Took 140.71548581123352 seconds to complete one epoch of meta training
Took 148.73237466812134 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494437
Epoch 346
Meta loss on this task batch = 2.5225e-01, Meta loss averaged over last 500 steps = 3.0743e-01, PNorm = 106.8968, GNorm = 0.2427
Meta loss on this task batch = 2.8351e-01, Meta loss averaged over last 500 steps = 3.0747e-01, PNorm = 106.9022, GNorm = 0.3091
Meta loss on this task batch = 2.7960e-01, Meta loss averaged over last 500 steps = 3.0734e-01, PNorm = 106.9078, GNorm = 0.2819
Meta loss on this task batch = 3.0512e-01, Meta loss averaged over last 500 steps = 3.0714e-01, PNorm = 106.9107, GNorm = 0.3279
Meta loss on this task batch = 2.7668e-01, Meta loss averaged over last 500 steps = 3.0699e-01, PNorm = 106.9152, GNorm = 0.2580
Meta loss on this task batch = 2.7536e-01, Meta loss averaged over last 500 steps = 3.0690e-01, PNorm = 106.9191, GNorm = 0.2463
Meta loss on this task batch = 3.6080e-01, Meta loss averaged over last 500 steps = 3.0711e-01, PNorm = 106.9218, GNorm = 0.3466
Meta loss on this task batch = 2.8057e-01, Meta loss averaged over last 500 steps = 3.0705e-01, PNorm = 106.9261, GNorm = 0.2759
Meta loss on this task batch = 2.2978e-01, Meta loss averaged over last 500 steps = 3.0689e-01, PNorm = 106.9316, GNorm = 0.2615
Meta loss on this task batch = 3.2121e-01, Meta loss averaged over last 500 steps = 3.0691e-01, PNorm = 106.9365, GNorm = 0.2848
Meta loss on this task batch = 3.9261e-01, Meta loss averaged over last 500 steps = 3.0721e-01, PNorm = 106.9414, GNorm = 0.3117
Meta loss on this task batch = 3.0465e-01, Meta loss averaged over last 500 steps = 3.0719e-01, PNorm = 106.9471, GNorm = 0.3006
Meta loss on this task batch = 3.2152e-01, Meta loss averaged over last 500 steps = 3.0732e-01, PNorm = 106.9558, GNorm = 0.2579
Meta loss on this task batch = 2.6023e-01, Meta loss averaged over last 500 steps = 3.0730e-01, PNorm = 106.9635, GNorm = 0.2728
Meta loss on this task batch = 3.1271e-01, Meta loss averaged over last 500 steps = 3.0718e-01, PNorm = 106.9726, GNorm = 0.3054
Meta loss on this task batch = 3.0905e-01, Meta loss averaged over last 500 steps = 3.0719e-01, PNorm = 106.9819, GNorm = 0.2631
Meta loss on this task batch = 3.3048e-01, Meta loss averaged over last 500 steps = 3.0732e-01, PNorm = 106.9887, GNorm = 0.2733
Meta loss on this task batch = 3.2441e-01, Meta loss averaged over last 500 steps = 3.0720e-01, PNorm = 106.9945, GNorm = 0.3179
Meta loss on this task batch = 3.3271e-01, Meta loss averaged over last 500 steps = 3.0729e-01, PNorm = 107.0014, GNorm = 0.3157
Took 150.0041844844818 seconds to complete one epoch of meta training
Took 158.6709761619568 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504907
Epoch 347
Meta loss on this task batch = 2.4546e-01, Meta loss averaged over last 500 steps = 3.0719e-01, PNorm = 107.0074, GNorm = 0.2526
Meta loss on this task batch = 2.7467e-01, Meta loss averaged over last 500 steps = 3.0724e-01, PNorm = 107.0108, GNorm = 0.2997
Meta loss on this task batch = 2.9599e-01, Meta loss averaged over last 500 steps = 3.0724e-01, PNorm = 107.0139, GNorm = 0.2754
Meta loss on this task batch = 3.1752e-01, Meta loss averaged over last 500 steps = 3.0722e-01, PNorm = 107.0179, GNorm = 0.2530
Meta loss on this task batch = 3.4877e-01, Meta loss averaged over last 500 steps = 3.0728e-01, PNorm = 107.0211, GNorm = 0.3085
Meta loss on this task batch = 2.8693e-01, Meta loss averaged over last 500 steps = 3.0707e-01, PNorm = 107.0247, GNorm = 0.3052
Meta loss on this task batch = 3.2087e-01, Meta loss averaged over last 500 steps = 3.0720e-01, PNorm = 107.0292, GNorm = 0.3113
Meta loss on this task batch = 2.7313e-01, Meta loss averaged over last 500 steps = 3.0721e-01, PNorm = 107.0337, GNorm = 0.2806
Meta loss on this task batch = 3.0654e-01, Meta loss averaged over last 500 steps = 3.0717e-01, PNorm = 107.0390, GNorm = 0.2616
Meta loss on this task batch = 3.2450e-01, Meta loss averaged over last 500 steps = 3.0719e-01, PNorm = 107.0449, GNorm = 0.3166
Meta loss on this task batch = 2.8475e-01, Meta loss averaged over last 500 steps = 3.0717e-01, PNorm = 107.0517, GNorm = 0.3141
Meta loss on this task batch = 2.8094e-01, Meta loss averaged over last 500 steps = 3.0713e-01, PNorm = 107.0594, GNorm = 0.3348
Meta loss on this task batch = 3.6288e-01, Meta loss averaged over last 500 steps = 3.0727e-01, PNorm = 107.0653, GNorm = 0.3289
Meta loss on this task batch = 3.4148e-01, Meta loss averaged over last 500 steps = 3.0725e-01, PNorm = 107.0684, GNorm = 0.2997
Meta loss on this task batch = 2.9272e-01, Meta loss averaged over last 500 steps = 3.0704e-01, PNorm = 107.0703, GNorm = 0.2612
Meta loss on this task batch = 2.6256e-01, Meta loss averaged over last 500 steps = 3.0690e-01, PNorm = 107.0709, GNorm = 0.3184
Meta loss on this task batch = 2.7108e-01, Meta loss averaged over last 500 steps = 3.0680e-01, PNorm = 107.0717, GNorm = 0.2995
Meta loss on this task batch = 2.9353e-01, Meta loss averaged over last 500 steps = 3.0677e-01, PNorm = 107.0738, GNorm = 0.2924
Meta loss on this task batch = 4.0566e-01, Meta loss averaged over last 500 steps = 3.0681e-01, PNorm = 107.0760, GNorm = 0.4257
Took 142.8928999900818 seconds to complete one epoch of meta training
Took 150.4473524093628 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495547
Epoch 348
Meta loss on this task batch = 3.0385e-01, Meta loss averaged over last 500 steps = 3.0684e-01, PNorm = 107.0781, GNorm = 0.2688
Meta loss on this task batch = 3.2218e-01, Meta loss averaged over last 500 steps = 3.0697e-01, PNorm = 107.0827, GNorm = 0.3139
Meta loss on this task batch = 3.0076e-01, Meta loss averaged over last 500 steps = 3.0686e-01, PNorm = 107.0890, GNorm = 0.2893
Meta loss on this task batch = 3.2623e-01, Meta loss averaged over last 500 steps = 3.0688e-01, PNorm = 107.0963, GNorm = 0.3153
Meta loss on this task batch = 3.7574e-01, Meta loss averaged over last 500 steps = 3.0695e-01, PNorm = 107.1004, GNorm = 0.3327
Meta loss on this task batch = 3.2655e-01, Meta loss averaged over last 500 steps = 3.0700e-01, PNorm = 107.1083, GNorm = 0.3395
Meta loss on this task batch = 2.6841e-01, Meta loss averaged over last 500 steps = 3.0680e-01, PNorm = 107.1188, GNorm = 0.2483
Meta loss on this task batch = 3.4782e-01, Meta loss averaged over last 500 steps = 3.0695e-01, PNorm = 107.1311, GNorm = 0.2836
Meta loss on this task batch = 2.3536e-01, Meta loss averaged over last 500 steps = 3.0677e-01, PNorm = 107.1427, GNorm = 0.2538
Meta loss on this task batch = 3.2420e-01, Meta loss averaged over last 500 steps = 3.0674e-01, PNorm = 107.1520, GNorm = 0.3101
Meta loss on this task batch = 3.1180e-01, Meta loss averaged over last 500 steps = 3.0685e-01, PNorm = 107.1579, GNorm = 0.3548
Meta loss on this task batch = 2.6504e-01, Meta loss averaged over last 500 steps = 3.0671e-01, PNorm = 107.1635, GNorm = 0.3116
Meta loss on this task batch = 2.6256e-01, Meta loss averaged over last 500 steps = 3.0663e-01, PNorm = 107.1678, GNorm = 0.2704
Meta loss on this task batch = 3.2280e-01, Meta loss averaged over last 500 steps = 3.0670e-01, PNorm = 107.1721, GNorm = 0.3206
Meta loss on this task batch = 2.8669e-01, Meta loss averaged over last 500 steps = 3.0676e-01, PNorm = 107.1779, GNorm = 0.3634
Meta loss on this task batch = 3.6221e-01, Meta loss averaged over last 500 steps = 3.0697e-01, PNorm = 107.1865, GNorm = 0.2984
Meta loss on this task batch = 2.9725e-01, Meta loss averaged over last 500 steps = 3.0686e-01, PNorm = 107.1959, GNorm = 0.2659
Meta loss on this task batch = 2.8854e-01, Meta loss averaged over last 500 steps = 3.0676e-01, PNorm = 107.2063, GNorm = 0.2712
Meta loss on this task batch = 3.0151e-01, Meta loss averaged over last 500 steps = 3.0674e-01, PNorm = 107.2159, GNorm = 0.3277
Took 155.33622908592224 seconds to complete one epoch of meta training
Took 163.90311217308044 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505005
Epoch 349
Meta loss on this task batch = 2.7803e-01, Meta loss averaged over last 500 steps = 3.0674e-01, PNorm = 107.2243, GNorm = 0.2454
Meta loss on this task batch = 2.7514e-01, Meta loss averaged over last 500 steps = 3.0675e-01, PNorm = 107.2330, GNorm = 0.2799
Meta loss on this task batch = 3.7624e-01, Meta loss averaged over last 500 steps = 3.0703e-01, PNorm = 107.2383, GNorm = 0.3245
Meta loss on this task batch = 2.4747e-01, Meta loss averaged over last 500 steps = 3.0682e-01, PNorm = 107.2422, GNorm = 0.2520
Meta loss on this task batch = 2.3629e-01, Meta loss averaged over last 500 steps = 3.0668e-01, PNorm = 107.2466, GNorm = 0.2506
Meta loss on this task batch = 3.3893e-01, Meta loss averaged over last 500 steps = 3.0672e-01, PNorm = 107.2471, GNorm = 0.3250
Meta loss on this task batch = 3.0713e-01, Meta loss averaged over last 500 steps = 3.0665e-01, PNorm = 107.2480, GNorm = 0.2872
Meta loss on this task batch = 2.8127e-01, Meta loss averaged over last 500 steps = 3.0658e-01, PNorm = 107.2507, GNorm = 0.2968
Meta loss on this task batch = 3.1813e-01, Meta loss averaged over last 500 steps = 3.0658e-01, PNorm = 107.2524, GNorm = 0.3162
Meta loss on this task batch = 3.1237e-01, Meta loss averaged over last 500 steps = 3.0666e-01, PNorm = 107.2564, GNorm = 0.2909
Meta loss on this task batch = 2.4753e-01, Meta loss averaged over last 500 steps = 3.0655e-01, PNorm = 107.2632, GNorm = 0.2653
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 3.0646e-01, PNorm = 107.2715, GNorm = 0.2575
Meta loss on this task batch = 3.1993e-01, Meta loss averaged over last 500 steps = 3.0643e-01, PNorm = 107.2816, GNorm = 0.3138
Meta loss on this task batch = 2.9521e-01, Meta loss averaged over last 500 steps = 3.0644e-01, PNorm = 107.2899, GNorm = 0.3445
Meta loss on this task batch = 3.2628e-01, Meta loss averaged over last 500 steps = 3.0649e-01, PNorm = 107.2950, GNorm = 0.3154
Meta loss on this task batch = 3.1815e-01, Meta loss averaged over last 500 steps = 3.0658e-01, PNorm = 107.2993, GNorm = 0.2762
Meta loss on this task batch = 3.4730e-01, Meta loss averaged over last 500 steps = 3.0689e-01, PNorm = 107.3009, GNorm = 0.3083
Meta loss on this task batch = 3.3933e-01, Meta loss averaged over last 500 steps = 3.0696e-01, PNorm = 107.3030, GNorm = 0.3070
Meta loss on this task batch = 2.5918e-01, Meta loss averaged over last 500 steps = 3.0694e-01, PNorm = 107.3008, GNorm = 0.3033
Took 139.66636657714844 seconds to complete one epoch of meta training
Took 147.77107906341553 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487458
Epoch 350
Meta loss on this task batch = 3.5231e-01, Meta loss averaged over last 500 steps = 3.0707e-01, PNorm = 107.2975, GNorm = 0.3178
Meta loss on this task batch = 3.6201e-01, Meta loss averaged over last 500 steps = 3.0723e-01, PNorm = 107.2931, GNorm = 0.3232
Meta loss on this task batch = 2.9137e-01, Meta loss averaged over last 500 steps = 3.0717e-01, PNorm = 107.2910, GNorm = 0.2714
Meta loss on this task batch = 3.3064e-01, Meta loss averaged over last 500 steps = 3.0717e-01, PNorm = 107.2911, GNorm = 0.2781
Meta loss on this task batch = 3.1122e-01, Meta loss averaged over last 500 steps = 3.0700e-01, PNorm = 107.2946, GNorm = 0.3367
Meta loss on this task batch = 2.8133e-01, Meta loss averaged over last 500 steps = 3.0692e-01, PNorm = 107.2967, GNorm = 0.2401
Meta loss on this task batch = 2.7811e-01, Meta loss averaged over last 500 steps = 3.0690e-01, PNorm = 107.3016, GNorm = 0.2809
Meta loss on this task batch = 2.6996e-01, Meta loss averaged over last 500 steps = 3.0698e-01, PNorm = 107.3097, GNorm = 0.2736
Meta loss on this task batch = 3.1072e-01, Meta loss averaged over last 500 steps = 3.0696e-01, PNorm = 107.3176, GNorm = 0.3377
Meta loss on this task batch = 2.9171e-01, Meta loss averaged over last 500 steps = 3.0692e-01, PNorm = 107.3244, GNorm = 0.3161
Meta loss on this task batch = 3.1630e-01, Meta loss averaged over last 500 steps = 3.0686e-01, PNorm = 107.3301, GNorm = 0.3374
Meta loss on this task batch = 2.7825e-01, Meta loss averaged over last 500 steps = 3.0681e-01, PNorm = 107.3368, GNorm = 0.2865
Meta loss on this task batch = 3.4266e-01, Meta loss averaged over last 500 steps = 3.0678e-01, PNorm = 107.3443, GNorm = 0.3070
Meta loss on this task batch = 2.9026e-01, Meta loss averaged over last 500 steps = 3.0678e-01, PNorm = 107.3515, GNorm = 0.3037
Meta loss on this task batch = 2.8587e-01, Meta loss averaged over last 500 steps = 3.0682e-01, PNorm = 107.3601, GNorm = 0.2849
Meta loss on this task batch = 3.2917e-01, Meta loss averaged over last 500 steps = 3.0682e-01, PNorm = 107.3668, GNorm = 0.2937
Meta loss on this task batch = 2.9185e-01, Meta loss averaged over last 500 steps = 3.0666e-01, PNorm = 107.3728, GNorm = 0.2937
Meta loss on this task batch = 3.3544e-01, Meta loss averaged over last 500 steps = 3.0668e-01, PNorm = 107.3807, GNorm = 0.3294
Meta loss on this task batch = 3.2612e-01, Meta loss averaged over last 500 steps = 3.0664e-01, PNorm = 107.3873, GNorm = 0.3363
Took 139.40599846839905 seconds to complete one epoch of meta training
Took 148.00665640830994 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505214
Epoch 351
Meta loss on this task batch = 3.1471e-01, Meta loss averaged over last 500 steps = 3.0653e-01, PNorm = 107.3960, GNorm = 0.3042
Meta loss on this task batch = 3.1647e-01, Meta loss averaged over last 500 steps = 3.0658e-01, PNorm = 107.4072, GNorm = 0.3124
Meta loss on this task batch = 2.6222e-01, Meta loss averaged over last 500 steps = 3.0654e-01, PNorm = 107.4189, GNorm = 0.2664
Meta loss on this task batch = 2.3781e-01, Meta loss averaged over last 500 steps = 3.0633e-01, PNorm = 107.4312, GNorm = 0.2462
Meta loss on this task batch = 2.7066e-01, Meta loss averaged over last 500 steps = 3.0625e-01, PNorm = 107.4430, GNorm = 0.2711
Meta loss on this task batch = 2.6903e-01, Meta loss averaged over last 500 steps = 3.0618e-01, PNorm = 107.4519, GNorm = 0.2954
Meta loss on this task batch = 2.4274e-01, Meta loss averaged over last 500 steps = 3.0599e-01, PNorm = 107.4589, GNorm = 0.2901
Meta loss on this task batch = 3.1372e-01, Meta loss averaged over last 500 steps = 3.0601e-01, PNorm = 107.4628, GNorm = 0.3167
Meta loss on this task batch = 3.6202e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 107.4643, GNorm = 0.3638
Meta loss on this task batch = 3.3644e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 107.4651, GNorm = 0.3390
Meta loss on this task batch = 3.0973e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 107.4640, GNorm = 0.3781
Meta loss on this task batch = 3.4107e-01, Meta loss averaged over last 500 steps = 3.0618e-01, PNorm = 107.4622, GNorm = 0.3180
Meta loss on this task batch = 2.6240e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 107.4629, GNorm = 0.2684
Meta loss on this task batch = 3.3537e-01, Meta loss averaged over last 500 steps = 3.0616e-01, PNorm = 107.4652, GNorm = 0.2996
Meta loss on this task batch = 3.2274e-01, Meta loss averaged over last 500 steps = 3.0610e-01, PNorm = 107.4713, GNorm = 0.3197
Meta loss on this task batch = 2.9473e-01, Meta loss averaged over last 500 steps = 3.0602e-01, PNorm = 107.4771, GNorm = 0.2914
Meta loss on this task batch = 3.4577e-01, Meta loss averaged over last 500 steps = 3.0611e-01, PNorm = 107.4862, GNorm = 0.3254
Meta loss on this task batch = 3.4100e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 107.4965, GNorm = 0.3187
Meta loss on this task batch = 3.2046e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 107.5093, GNorm = 0.3442
Took 133.5046272277832 seconds to complete one epoch of meta training
Took 142.1233048439026 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513717
Epoch 352
Meta loss on this task batch = 2.9269e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 107.5235, GNorm = 0.2787
Meta loss on this task batch = 2.9186e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 107.5377, GNorm = 0.3413
Meta loss on this task batch = 3.0312e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 107.5497, GNorm = 0.2728
Meta loss on this task batch = 2.7256e-01, Meta loss averaged over last 500 steps = 3.0606e-01, PNorm = 107.5594, GNorm = 0.3343
Meta loss on this task batch = 3.4467e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 107.5662, GNorm = 0.3454
Meta loss on this task batch = 3.5075e-01, Meta loss averaged over last 500 steps = 3.0623e-01, PNorm = 107.5728, GNorm = 0.3523
Meta loss on this task batch = 2.8780e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 107.5783, GNorm = 0.3358
Meta loss on this task batch = 2.1478e-01, Meta loss averaged over last 500 steps = 3.0611e-01, PNorm = 107.5845, GNorm = 0.2353
Meta loss on this task batch = 3.2874e-01, Meta loss averaged over last 500 steps = 3.0621e-01, PNorm = 107.5896, GNorm = 0.3021
Meta loss on this task batch = 3.0672e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 107.5926, GNorm = 0.3264
Meta loss on this task batch = 3.0999e-01, Meta loss averaged over last 500 steps = 3.0607e-01, PNorm = 107.5925, GNorm = 0.3396
Meta loss on this task batch = 3.5156e-01, Meta loss averaged over last 500 steps = 3.0623e-01, PNorm = 107.5916, GNorm = 0.3468
Meta loss on this task batch = 3.1354e-01, Meta loss averaged over last 500 steps = 3.0620e-01, PNorm = 107.5900, GNorm = 0.3441
Meta loss on this task batch = 2.8938e-01, Meta loss averaged over last 500 steps = 3.0607e-01, PNorm = 107.5895, GNorm = 0.2899
Meta loss on this task batch = 3.3078e-01, Meta loss averaged over last 500 steps = 3.0606e-01, PNorm = 107.5917, GNorm = 0.2859
Meta loss on this task batch = 3.6195e-01, Meta loss averaged over last 500 steps = 3.0624e-01, PNorm = 107.5956, GNorm = 0.2940
Meta loss on this task batch = 2.9832e-01, Meta loss averaged over last 500 steps = 3.0621e-01, PNorm = 107.6020, GNorm = 0.3094
Meta loss on this task batch = 3.2391e-01, Meta loss averaged over last 500 steps = 3.0621e-01, PNorm = 107.6110, GNorm = 0.2802
Meta loss on this task batch = 3.1347e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 107.6207, GNorm = 0.2733
Took 131.0301718711853 seconds to complete one epoch of meta training
Took 139.64522671699524 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494253
Epoch 353
Meta loss on this task batch = 2.7518e-01, Meta loss averaged over last 500 steps = 3.0621e-01, PNorm = 107.6316, GNorm = 0.3053
Meta loss on this task batch = 2.9334e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 107.6444, GNorm = 0.2588
Meta loss on this task batch = 2.5374e-01, Meta loss averaged over last 500 steps = 3.0612e-01, PNorm = 107.6564, GNorm = 0.2444
Meta loss on this task batch = 3.7594e-01, Meta loss averaged over last 500 steps = 3.0629e-01, PNorm = 107.6668, GNorm = 0.3569
Meta loss on this task batch = 3.0598e-01, Meta loss averaged over last 500 steps = 3.0620e-01, PNorm = 107.6762, GNorm = 0.2845
Meta loss on this task batch = 2.9776e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 107.6834, GNorm = 0.3141
Meta loss on this task batch = 3.0463e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 107.6891, GNorm = 0.2673
Meta loss on this task batch = 3.3989e-01, Meta loss averaged over last 500 steps = 3.0634e-01, PNorm = 107.6957, GNorm = 0.2975
Meta loss on this task batch = 3.1688e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 107.7025, GNorm = 0.3181
Meta loss on this task batch = 3.5705e-01, Meta loss averaged over last 500 steps = 3.0644e-01, PNorm = 107.7071, GNorm = 0.3300
Meta loss on this task batch = 2.4679e-01, Meta loss averaged over last 500 steps = 3.0625e-01, PNorm = 107.7121, GNorm = 0.3066
Meta loss on this task batch = 3.0396e-01, Meta loss averaged over last 500 steps = 3.0631e-01, PNorm = 107.7186, GNorm = 0.2890
Meta loss on this task batch = 3.1177e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 107.7246, GNorm = 0.3291
Meta loss on this task batch = 2.7192e-01, Meta loss averaged over last 500 steps = 3.0624e-01, PNorm = 107.7309, GNorm = 0.3034
Meta loss on this task batch = 3.2398e-01, Meta loss averaged over last 500 steps = 3.0641e-01, PNorm = 107.7383, GNorm = 0.2853
Meta loss on this task batch = 3.3876e-01, Meta loss averaged over last 500 steps = 3.0650e-01, PNorm = 107.7474, GNorm = 0.3231
Meta loss on this task batch = 2.6197e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 107.7573, GNorm = 0.2801
Meta loss on this task batch = 2.7093e-01, Meta loss averaged over last 500 steps = 3.0625e-01, PNorm = 107.7679, GNorm = 0.2892
Meta loss on this task batch = 3.3924e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 107.7763, GNorm = 0.3570
Took 131.70171403884888 seconds to complete one epoch of meta training
Took 140.3936791419983 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477777
Epoch 354
Meta loss on this task batch = 3.1919e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 107.7815, GNorm = 0.2985
Meta loss on this task batch = 3.3463e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 107.7856, GNorm = 0.2954
Meta loss on this task batch = 2.2427e-01, Meta loss averaged over last 500 steps = 3.0609e-01, PNorm = 107.7894, GNorm = 0.2791
Meta loss on this task batch = 3.1637e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 107.7918, GNorm = 0.2859
Meta loss on this task batch = 3.3415e-01, Meta loss averaged over last 500 steps = 3.0615e-01, PNorm = 107.7925, GNorm = 0.2955
Meta loss on this task batch = 3.3255e-01, Meta loss averaged over last 500 steps = 3.0623e-01, PNorm = 107.7915, GNorm = 0.2916
Meta loss on this task batch = 3.3612e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 107.7898, GNorm = 0.3520
Meta loss on this task batch = 2.8593e-01, Meta loss averaged over last 500 steps = 3.0618e-01, PNorm = 107.7889, GNorm = 0.3112
Meta loss on this task batch = 2.9505e-01, Meta loss averaged over last 500 steps = 3.0608e-01, PNorm = 107.7906, GNorm = 0.3117
Meta loss on this task batch = 3.0749e-01, Meta loss averaged over last 500 steps = 3.0607e-01, PNorm = 107.7936, GNorm = 0.2793
Meta loss on this task batch = 3.0299e-01, Meta loss averaged over last 500 steps = 3.0593e-01, PNorm = 107.7989, GNorm = 0.2561
Meta loss on this task batch = 3.7710e-01, Meta loss averaged over last 500 steps = 3.0608e-01, PNorm = 107.8046, GNorm = 0.3229
Meta loss on this task batch = 2.9440e-01, Meta loss averaged over last 500 steps = 3.0607e-01, PNorm = 107.8099, GNorm = 0.2987
Meta loss on this task batch = 3.2152e-01, Meta loss averaged over last 500 steps = 3.0618e-01, PNorm = 107.8143, GNorm = 0.4010
Meta loss on this task batch = 2.9433e-01, Meta loss averaged over last 500 steps = 3.0616e-01, PNorm = 107.8199, GNorm = 0.2968
Meta loss on this task batch = 2.8698e-01, Meta loss averaged over last 500 steps = 3.0618e-01, PNorm = 107.8256, GNorm = 0.3000
Meta loss on this task batch = 2.9869e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 107.8316, GNorm = 0.3258
Meta loss on this task batch = 3.3210e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 107.8393, GNorm = 0.2827
Meta loss on this task batch = 3.4363e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 107.8430, GNorm = 0.3654
Took 133.36319017410278 seconds to complete one epoch of meta training
Took 142.12107944488525 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469105
Epoch 355
Meta loss on this task batch = 2.6259e-01, Meta loss averaged over last 500 steps = 3.0625e-01, PNorm = 107.8488, GNorm = 0.2571
Meta loss on this task batch = 3.8138e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 107.8532, GNorm = 0.3236
Meta loss on this task batch = 2.9047e-01, Meta loss averaged over last 500 steps = 3.0629e-01, PNorm = 107.8609, GNorm = 0.2668
Meta loss on this task batch = 3.3079e-01, Meta loss averaged over last 500 steps = 3.0633e-01, PNorm = 107.8681, GNorm = 0.2464
Meta loss on this task batch = 3.2088e-01, Meta loss averaged over last 500 steps = 3.0639e-01, PNorm = 107.8748, GNorm = 0.2605
Meta loss on this task batch = 2.4261e-01, Meta loss averaged over last 500 steps = 3.0643e-01, PNorm = 107.8823, GNorm = 0.2846
Meta loss on this task batch = 2.6663e-01, Meta loss averaged over last 500 steps = 3.0627e-01, PNorm = 107.8894, GNorm = 0.2288
Meta loss on this task batch = 2.3872e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 107.8966, GNorm = 0.2239
Meta loss on this task batch = 2.6702e-01, Meta loss averaged over last 500 steps = 3.0614e-01, PNorm = 107.9023, GNorm = 0.2748
Meta loss on this task batch = 3.1910e-01, Meta loss averaged over last 500 steps = 3.0609e-01, PNorm = 107.9050, GNorm = 0.2687
Meta loss on this task batch = 2.9990e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 107.9074, GNorm = 0.2718
Meta loss on this task batch = 3.1556e-01, Meta loss averaged over last 500 steps = 3.0623e-01, PNorm = 107.9103, GNorm = 0.2484
Meta loss on this task batch = 3.2099e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 107.9135, GNorm = 0.2630
Meta loss on this task batch = 3.2737e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 107.9154, GNorm = 0.2845
Meta loss on this task batch = 2.6244e-01, Meta loss averaged over last 500 steps = 3.0627e-01, PNorm = 107.9178, GNorm = 0.2659
Meta loss on this task batch = 3.4885e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 107.9168, GNorm = 0.3898
Meta loss on this task batch = 3.0789e-01, Meta loss averaged over last 500 steps = 3.0627e-01, PNorm = 107.9172, GNorm = 0.2794
Meta loss on this task batch = 3.4281e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 107.9168, GNorm = 0.3245
Meta loss on this task batch = 3.5160e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 107.9151, GNorm = 0.4161
Took 130.87235236167908 seconds to complete one epoch of meta training
Took 139.62939023971558 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493139
Epoch 356
Meta loss on this task batch = 2.9506e-01, Meta loss averaged over last 500 steps = 3.0639e-01, PNorm = 107.9146, GNorm = 0.2969
Meta loss on this task batch = 3.1506e-01, Meta loss averaged over last 500 steps = 3.0639e-01, PNorm = 107.9157, GNorm = 0.3384
Meta loss on this task batch = 2.8061e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 107.9204, GNorm = 0.2695
Meta loss on this task batch = 2.8875e-01, Meta loss averaged over last 500 steps = 3.0637e-01, PNorm = 107.9275, GNorm = 0.2523
Meta loss on this task batch = 3.0911e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 107.9354, GNorm = 0.2958
Meta loss on this task batch = 2.9608e-01, Meta loss averaged over last 500 steps = 3.0623e-01, PNorm = 107.9430, GNorm = 0.2812
Meta loss on this task batch = 3.4608e-01, Meta loss averaged over last 500 steps = 3.0629e-01, PNorm = 107.9516, GNorm = 0.3256
Meta loss on this task batch = 2.9363e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 107.9605, GNorm = 0.2784
Meta loss on this task batch = 3.2939e-01, Meta loss averaged over last 500 steps = 3.0634e-01, PNorm = 107.9663, GNorm = 0.2844
Meta loss on this task batch = 3.3019e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 107.9712, GNorm = 0.3195
Meta loss on this task batch = 2.7575e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 107.9761, GNorm = 0.2794
Meta loss on this task batch = 3.3149e-01, Meta loss averaged over last 500 steps = 3.0641e-01, PNorm = 107.9807, GNorm = 0.3337
Meta loss on this task batch = 3.4135e-01, Meta loss averaged over last 500 steps = 3.0655e-01, PNorm = 107.9872, GNorm = 0.2918
Meta loss on this task batch = 2.7891e-01, Meta loss averaged over last 500 steps = 3.0658e-01, PNorm = 107.9945, GNorm = 0.3119
Meta loss on this task batch = 3.1166e-01, Meta loss averaged over last 500 steps = 3.0646e-01, PNorm = 108.0005, GNorm = 0.2894
Meta loss on this task batch = 3.0983e-01, Meta loss averaged over last 500 steps = 3.0640e-01, PNorm = 108.0060, GNorm = 0.2655
Meta loss on this task batch = 2.6501e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 108.0135, GNorm = 0.2723
Meta loss on this task batch = 3.0396e-01, Meta loss averaged over last 500 steps = 3.0636e-01, PNorm = 108.0221, GNorm = 0.3393
Meta loss on this task batch = 3.0162e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 108.0318, GNorm = 0.2841
Took 133.74885439872742 seconds to complete one epoch of meta training
Took 141.3058316707611 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498147
Epoch 357
Meta loss on this task batch = 2.8908e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 108.0403, GNorm = 0.3019
Meta loss on this task batch = 2.9289e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 108.0490, GNorm = 0.3051
Meta loss on this task batch = 3.5306e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 108.0595, GNorm = 0.2935
Meta loss on this task batch = 2.8565e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 108.0693, GNorm = 0.3090
Meta loss on this task batch = 3.1796e-01, Meta loss averaged over last 500 steps = 3.0626e-01, PNorm = 108.0816, GNorm = 0.3083
Meta loss on this task batch = 3.1640e-01, Meta loss averaged over last 500 steps = 3.0635e-01, PNorm = 108.0933, GNorm = 0.3105
Meta loss on this task batch = 2.5250e-01, Meta loss averaged over last 500 steps = 3.0626e-01, PNorm = 108.1064, GNorm = 0.2592
Meta loss on this task batch = 2.8931e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 108.1184, GNorm = 0.3687
Meta loss on this task batch = 2.5384e-01, Meta loss averaged over last 500 steps = 3.0597e-01, PNorm = 108.1324, GNorm = 0.2644
Meta loss on this task batch = 3.2246e-01, Meta loss averaged over last 500 steps = 3.0608e-01, PNorm = 108.1466, GNorm = 0.3149
Meta loss on this task batch = 2.9592e-01, Meta loss averaged over last 500 steps = 3.0602e-01, PNorm = 108.1587, GNorm = 0.2759
Meta loss on this task batch = 2.9127e-01, Meta loss averaged over last 500 steps = 3.0609e-01, PNorm = 108.1686, GNorm = 0.2831
Meta loss on this task batch = 3.1467e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 108.1772, GNorm = 0.2818
Meta loss on this task batch = 2.9213e-01, Meta loss averaged over last 500 steps = 3.0610e-01, PNorm = 108.1849, GNorm = 0.3206
Meta loss on this task batch = 3.0973e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 108.1897, GNorm = 0.3436
Meta loss on this task batch = 3.3911e-01, Meta loss averaged over last 500 steps = 3.0620e-01, PNorm = 108.1909, GNorm = 0.3440
Meta loss on this task batch = 3.3141e-01, Meta loss averaged over last 500 steps = 3.0618e-01, PNorm = 108.1907, GNorm = 0.2979
Meta loss on this task batch = 3.2314e-01, Meta loss averaged over last 500 steps = 3.0613e-01, PNorm = 108.1916, GNorm = 0.3060
Meta loss on this task batch = 2.8203e-01, Meta loss averaged over last 500 steps = 3.0617e-01, PNorm = 108.1939, GNorm = 0.3208
Took 133.09935665130615 seconds to complete one epoch of meta training
Took 142.58902287483215 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497634
Epoch 358
Meta loss on this task batch = 3.7685e-01, Meta loss averaged over last 500 steps = 3.0625e-01, PNorm = 108.1959, GNorm = 0.3355
Meta loss on this task batch = 3.1046e-01, Meta loss averaged over last 500 steps = 3.0630e-01, PNorm = 108.1974, GNorm = 0.2816
Meta loss on this task batch = 3.2105e-01, Meta loss averaged over last 500 steps = 3.0632e-01, PNorm = 108.2018, GNorm = 0.2773
Meta loss on this task batch = 2.6653e-01, Meta loss averaged over last 500 steps = 3.0616e-01, PNorm = 108.2078, GNorm = 0.2468
Meta loss on this task batch = 3.1668e-01, Meta loss averaged over last 500 steps = 3.0620e-01, PNorm = 108.2136, GNorm = 0.2970
Meta loss on this task batch = 3.0867e-01, Meta loss averaged over last 500 steps = 3.0607e-01, PNorm = 108.2216, GNorm = 0.2904
Meta loss on this task batch = 2.4836e-01, Meta loss averaged over last 500 steps = 3.0585e-01, PNorm = 108.2324, GNorm = 0.2691
Meta loss on this task batch = 2.9857e-01, Meta loss averaged over last 500 steps = 3.0586e-01, PNorm = 108.2430, GNorm = 0.2510
Meta loss on this task batch = 2.7803e-01, Meta loss averaged over last 500 steps = 3.0582e-01, PNorm = 108.2541, GNorm = 0.2733
Meta loss on this task batch = 2.9118e-01, Meta loss averaged over last 500 steps = 3.0586e-01, PNorm = 108.2647, GNorm = 0.2659
Meta loss on this task batch = 3.2091e-01, Meta loss averaged over last 500 steps = 3.0584e-01, PNorm = 108.2734, GNorm = 0.2764
Meta loss on this task batch = 2.7581e-01, Meta loss averaged over last 500 steps = 3.0583e-01, PNorm = 108.2800, GNorm = 0.2501
Meta loss on this task batch = 2.7767e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 108.2857, GNorm = 0.2595
Meta loss on this task batch = 3.5694e-01, Meta loss averaged over last 500 steps = 3.0551e-01, PNorm = 108.2895, GNorm = 0.3376
Meta loss on this task batch = 2.6236e-01, Meta loss averaged over last 500 steps = 3.0543e-01, PNorm = 108.2917, GNorm = 0.2637
Meta loss on this task batch = 3.4582e-01, Meta loss averaged over last 500 steps = 3.0553e-01, PNorm = 108.2900, GNorm = 0.3351
Meta loss on this task batch = 3.2293e-01, Meta loss averaged over last 500 steps = 3.0554e-01, PNorm = 108.2865, GNorm = 0.3586
Meta loss on this task batch = 3.3512e-01, Meta loss averaged over last 500 steps = 3.0562e-01, PNorm = 108.2836, GNorm = 0.3156
Meta loss on this task batch = 2.3494e-01, Meta loss averaged over last 500 steps = 3.0556e-01, PNorm = 108.2834, GNorm = 0.3311
Took 130.79420804977417 seconds to complete one epoch of meta training
Took 139.38809967041016 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.516565
Epoch 359
Meta loss on this task batch = 3.1023e-01, Meta loss averaged over last 500 steps = 3.0563e-01, PNorm = 108.2841, GNorm = 0.2640
Meta loss on this task batch = 3.3503e-01, Meta loss averaged over last 500 steps = 3.0565e-01, PNorm = 108.2858, GNorm = 0.2955
Meta loss on this task batch = 3.2483e-01, Meta loss averaged over last 500 steps = 3.0564e-01, PNorm = 108.2881, GNorm = 0.2729
Meta loss on this task batch = 2.8358e-01, Meta loss averaged over last 500 steps = 3.0565e-01, PNorm = 108.2932, GNorm = 0.2975
Meta loss on this task batch = 3.4299e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 108.2994, GNorm = 0.3017
Meta loss on this task batch = 2.8244e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 108.3078, GNorm = 0.2673
Meta loss on this task batch = 3.1630e-01, Meta loss averaged over last 500 steps = 3.0582e-01, PNorm = 108.3160, GNorm = 0.2886
Meta loss on this task batch = 3.0585e-01, Meta loss averaged over last 500 steps = 3.0579e-01, PNorm = 108.3230, GNorm = 0.2697
Meta loss on this task batch = 2.9419e-01, Meta loss averaged over last 500 steps = 3.0568e-01, PNorm = 108.3309, GNorm = 0.2584
Meta loss on this task batch = 3.1669e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 108.3395, GNorm = 0.2951
Meta loss on this task batch = 3.0222e-01, Meta loss averaged over last 500 steps = 3.0556e-01, PNorm = 108.3487, GNorm = 0.2642
Meta loss on this task batch = 2.8420e-01, Meta loss averaged over last 500 steps = 3.0556e-01, PNorm = 108.3577, GNorm = 0.2673
Meta loss on this task batch = 3.0040e-01, Meta loss averaged over last 500 steps = 3.0567e-01, PNorm = 108.3647, GNorm = 0.2865
Meta loss on this task batch = 3.0948e-01, Meta loss averaged over last 500 steps = 3.0566e-01, PNorm = 108.3662, GNorm = 0.3608
Meta loss on this task batch = 3.0509e-01, Meta loss averaged over last 500 steps = 3.0567e-01, PNorm = 108.3664, GNorm = 0.3021
Meta loss on this task batch = 2.8417e-01, Meta loss averaged over last 500 steps = 3.0562e-01, PNorm = 108.3677, GNorm = 0.3178
Meta loss on this task batch = 3.1542e-01, Meta loss averaged over last 500 steps = 3.0558e-01, PNorm = 108.3693, GNorm = 0.2908
Meta loss on this task batch = 3.3057e-01, Meta loss averaged over last 500 steps = 3.0561e-01, PNorm = 108.3723, GNorm = 0.3244
Meta loss on this task batch = 2.6513e-01, Meta loss averaged over last 500 steps = 3.0556e-01, PNorm = 108.3726, GNorm = 0.3827
Took 130.3988049030304 seconds to complete one epoch of meta training
Took 138.7115399837494 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495007
Epoch 360
Meta loss on this task batch = 2.8028e-01, Meta loss averaged over last 500 steps = 3.0551e-01, PNorm = 108.3740, GNorm = 0.3316
Meta loss on this task batch = 2.6131e-01, Meta loss averaged over last 500 steps = 3.0543e-01, PNorm = 108.3778, GNorm = 0.2980
Meta loss on this task batch = 2.9395e-01, Meta loss averaged over last 500 steps = 3.0544e-01, PNorm = 108.3832, GNorm = 0.2778
Meta loss on this task batch = 3.0369e-01, Meta loss averaged over last 500 steps = 3.0553e-01, PNorm = 108.3913, GNorm = 0.2638
Meta loss on this task batch = 2.9658e-01, Meta loss averaged over last 500 steps = 3.0551e-01, PNorm = 108.3978, GNorm = 0.3103
Meta loss on this task batch = 3.6409e-01, Meta loss averaged over last 500 steps = 3.0559e-01, PNorm = 108.4040, GNorm = 0.2807
Meta loss on this task batch = 2.4516e-01, Meta loss averaged over last 500 steps = 3.0546e-01, PNorm = 108.4118, GNorm = 0.2571
Meta loss on this task batch = 3.6361e-01, Meta loss averaged over last 500 steps = 3.0554e-01, PNorm = 108.4185, GNorm = 0.3168
Meta loss on this task batch = 3.4130e-01, Meta loss averaged over last 500 steps = 3.0562e-01, PNorm = 108.4262, GNorm = 0.3031
Meta loss on this task batch = 2.5859e-01, Meta loss averaged over last 500 steps = 3.0564e-01, PNorm = 108.4377, GNorm = 0.2620
Meta loss on this task batch = 2.9962e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 108.4493, GNorm = 0.3712
Meta loss on this task batch = 3.0129e-01, Meta loss averaged over last 500 steps = 3.0578e-01, PNorm = 108.4604, GNorm = 0.2873
Meta loss on this task batch = 2.9358e-01, Meta loss averaged over last 500 steps = 3.0558e-01, PNorm = 108.4713, GNorm = 0.3110
Meta loss on this task batch = 2.9616e-01, Meta loss averaged over last 500 steps = 3.0554e-01, PNorm = 108.4809, GNorm = 0.3202
Meta loss on this task batch = 3.7177e-01, Meta loss averaged over last 500 steps = 3.0559e-01, PNorm = 108.4883, GNorm = 0.3365
Meta loss on this task batch = 2.7932e-01, Meta loss averaged over last 500 steps = 3.0555e-01, PNorm = 108.4948, GNorm = 0.2998
Meta loss on this task batch = 3.3926e-01, Meta loss averaged over last 500 steps = 3.0562e-01, PNorm = 108.5003, GNorm = 0.3125
Meta loss on this task batch = 2.7874e-01, Meta loss averaged over last 500 steps = 3.0564e-01, PNorm = 108.5070, GNorm = 0.2737
Meta loss on this task batch = 3.5610e-01, Meta loss averaged over last 500 steps = 3.0565e-01, PNorm = 108.5143, GNorm = 0.3658
Took 130.45655703544617 seconds to complete one epoch of meta training
Took 138.6269609928131 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485482
Epoch 361
Meta loss on this task batch = 3.0191e-01, Meta loss averaged over last 500 steps = 3.0568e-01, PNorm = 108.5214, GNorm = 0.3122
Meta loss on this task batch = 3.0955e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 108.5261, GNorm = 0.2876
Meta loss on this task batch = 2.9658e-01, Meta loss averaged over last 500 steps = 3.0556e-01, PNorm = 108.5321, GNorm = 0.2727
Meta loss on this task batch = 3.2061e-01, Meta loss averaged over last 500 steps = 3.0555e-01, PNorm = 108.5390, GNorm = 0.2669
Meta loss on this task batch = 3.7786e-01, Meta loss averaged over last 500 steps = 3.0559e-01, PNorm = 108.5463, GNorm = 0.2842
Meta loss on this task batch = 3.3043e-01, Meta loss averaged over last 500 steps = 3.0575e-01, PNorm = 108.5509, GNorm = 0.3141
Meta loss on this task batch = 2.7394e-01, Meta loss averaged over last 500 steps = 3.0569e-01, PNorm = 108.5555, GNorm = 0.3005
Meta loss on this task batch = 2.1661e-01, Meta loss averaged over last 500 steps = 3.0539e-01, PNorm = 108.5614, GNorm = 0.3206
Meta loss on this task batch = 2.8412e-01, Meta loss averaged over last 500 steps = 3.0527e-01, PNorm = 108.5657, GNorm = 0.2934
Meta loss on this task batch = 3.1067e-01, Meta loss averaged over last 500 steps = 3.0525e-01, PNorm = 108.5700, GNorm = 0.2930
Meta loss on this task batch = 2.8924e-01, Meta loss averaged over last 500 steps = 3.0526e-01, PNorm = 108.5753, GNorm = 0.3270
Meta loss on this task batch = 2.9727e-01, Meta loss averaged over last 500 steps = 3.0530e-01, PNorm = 108.5790, GNorm = 0.2907
Meta loss on this task batch = 3.0645e-01, Meta loss averaged over last 500 steps = 3.0533e-01, PNorm = 108.5845, GNorm = 0.2894
Meta loss on this task batch = 3.0267e-01, Meta loss averaged over last 500 steps = 3.0535e-01, PNorm = 108.5910, GNorm = 0.3075
Meta loss on this task batch = 3.0375e-01, Meta loss averaged over last 500 steps = 3.0536e-01, PNorm = 108.5986, GNorm = 0.2871
Meta loss on this task batch = 2.7837e-01, Meta loss averaged over last 500 steps = 3.0530e-01, PNorm = 108.6060, GNorm = 0.2546
Meta loss on this task batch = 3.5418e-01, Meta loss averaged over last 500 steps = 3.0539e-01, PNorm = 108.6103, GNorm = 0.3354
Meta loss on this task batch = 3.3643e-01, Meta loss averaged over last 500 steps = 3.0541e-01, PNorm = 108.6119, GNorm = 0.3643
Meta loss on this task batch = 3.6351e-01, Meta loss averaged over last 500 steps = 3.0551e-01, PNorm = 108.6112, GNorm = 0.3835
Took 130.20529532432556 seconds to complete one epoch of meta training
Took 138.98779964447021 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488607
Epoch 362
Meta loss on this task batch = 2.9603e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 108.6137, GNorm = 0.2718
Meta loss on this task batch = 2.9993e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 108.6159, GNorm = 0.2877
Meta loss on this task batch = 3.4097e-01, Meta loss averaged over last 500 steps = 3.0570e-01, PNorm = 108.6204, GNorm = 0.3002
Meta loss on this task batch = 3.0679e-01, Meta loss averaged over last 500 steps = 3.0579e-01, PNorm = 108.6253, GNorm = 0.2798
Meta loss on this task batch = 2.9477e-01, Meta loss averaged over last 500 steps = 3.0582e-01, PNorm = 108.6314, GNorm = 0.2685
Meta loss on this task batch = 2.6604e-01, Meta loss averaged over last 500 steps = 3.0579e-01, PNorm = 108.6378, GNorm = 0.2584
Meta loss on this task batch = 3.4352e-01, Meta loss averaged over last 500 steps = 3.0590e-01, PNorm = 108.6449, GNorm = 0.3320
Meta loss on this task batch = 2.7365e-01, Meta loss averaged over last 500 steps = 3.0586e-01, PNorm = 108.6530, GNorm = 0.2659
Meta loss on this task batch = 3.1885e-01, Meta loss averaged over last 500 steps = 3.0592e-01, PNorm = 108.6611, GNorm = 0.3024
Meta loss on this task batch = 2.6564e-01, Meta loss averaged over last 500 steps = 3.0585e-01, PNorm = 108.6701, GNorm = 0.2366
Meta loss on this task batch = 3.2986e-01, Meta loss averaged over last 500 steps = 3.0586e-01, PNorm = 108.6775, GNorm = 0.3202
Meta loss on this task batch = 3.6946e-01, Meta loss averaged over last 500 steps = 3.0597e-01, PNorm = 108.6837, GNorm = 0.3533
Meta loss on this task batch = 2.5019e-01, Meta loss averaged over last 500 steps = 3.0575e-01, PNorm = 108.6895, GNorm = 0.3198
Meta loss on this task batch = 3.7121e-01, Meta loss averaged over last 500 steps = 3.0593e-01, PNorm = 108.6935, GNorm = 0.3512
Meta loss on this task batch = 2.4017e-01, Meta loss averaged over last 500 steps = 3.0592e-01, PNorm = 108.6968, GNorm = 0.2435
Meta loss on this task batch = 2.8896e-01, Meta loss averaged over last 500 steps = 3.0583e-01, PNorm = 108.6999, GNorm = 0.2494
Meta loss on this task batch = 3.3321e-01, Meta loss averaged over last 500 steps = 3.0587e-01, PNorm = 108.7008, GNorm = 0.3065
Meta loss on this task batch = 3.0551e-01, Meta loss averaged over last 500 steps = 3.0579e-01, PNorm = 108.7006, GNorm = 0.2782
Meta loss on this task batch = 3.0615e-01, Meta loss averaged over last 500 steps = 3.0573e-01, PNorm = 108.6985, GNorm = 0.3447
Took 131.70670461654663 seconds to complete one epoch of meta training
Took 140.5498661994934 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491972
Epoch 363
Meta loss on this task batch = 3.0988e-01, Meta loss averaged over last 500 steps = 3.0575e-01, PNorm = 108.6975, GNorm = 0.3073
Meta loss on this task batch = 3.1156e-01, Meta loss averaged over last 500 steps = 3.0574e-01, PNorm = 108.6947, GNorm = 0.3075
Meta loss on this task batch = 2.9084e-01, Meta loss averaged over last 500 steps = 3.0568e-01, PNorm = 108.6951, GNorm = 0.2792
Meta loss on this task batch = 3.1664e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 108.6998, GNorm = 0.3320
Meta loss on this task batch = 3.4402e-01, Meta loss averaged over last 500 steps = 3.0576e-01, PNorm = 108.7049, GNorm = 0.3441
Meta loss on this task batch = 3.0310e-01, Meta loss averaged over last 500 steps = 3.0578e-01, PNorm = 108.7112, GNorm = 0.3026
Meta loss on this task batch = 3.0234e-01, Meta loss averaged over last 500 steps = 3.0579e-01, PNorm = 108.7195, GNorm = 0.2926
Meta loss on this task batch = 3.1570e-01, Meta loss averaged over last 500 steps = 3.0582e-01, PNorm = 108.7274, GNorm = 0.2746
Meta loss on this task batch = 2.6486e-01, Meta loss averaged over last 500 steps = 3.0566e-01, PNorm = 108.7359, GNorm = 0.2604
Meta loss on this task batch = 2.9011e-01, Meta loss averaged over last 500 steps = 3.0555e-01, PNorm = 108.7433, GNorm = 0.3427
Meta loss on this task batch = 3.1469e-01, Meta loss averaged over last 500 steps = 3.0562e-01, PNorm = 108.7505, GNorm = 0.2979
Meta loss on this task batch = 3.2965e-01, Meta loss averaged over last 500 steps = 3.0563e-01, PNorm = 108.7575, GNorm = 0.7302
Meta loss on this task batch = 2.5925e-01, Meta loss averaged over last 500 steps = 3.0558e-01, PNorm = 108.7629, GNorm = 0.2809
Meta loss on this task batch = 3.0942e-01, Meta loss averaged over last 500 steps = 3.0559e-01, PNorm = 108.7677, GNorm = 0.2862
Meta loss on this task batch = 3.2206e-01, Meta loss averaged over last 500 steps = 3.0564e-01, PNorm = 108.7702, GNorm = 0.2753
Meta loss on this task batch = 3.0347e-01, Meta loss averaged over last 500 steps = 3.0559e-01, PNorm = 108.7730, GNorm = 0.3067
Meta loss on this task batch = 3.0016e-01, Meta loss averaged over last 500 steps = 3.0566e-01, PNorm = 108.7751, GNorm = 0.2842
Meta loss on this task batch = 2.9594e-01, Meta loss averaged over last 500 steps = 3.0570e-01, PNorm = 108.7776, GNorm = 0.3040
Meta loss on this task batch = 2.7400e-01, Meta loss averaged over last 500 steps = 3.0565e-01, PNorm = 108.7756, GNorm = 0.3630
Took 131.93813705444336 seconds to complete one epoch of meta training
Took 140.49325037002563 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493679
Epoch 364
Meta loss on this task batch = 2.8497e-01, Meta loss averaged over last 500 steps = 3.0561e-01, PNorm = 108.7703, GNorm = 0.2822
Meta loss on this task batch = 2.8840e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 108.7620, GNorm = 0.2987
Meta loss on this task batch = 3.3716e-01, Meta loss averaged over last 500 steps = 3.0562e-01, PNorm = 108.7550, GNorm = 0.2884
Meta loss on this task batch = 2.9330e-01, Meta loss averaged over last 500 steps = 3.0550e-01, PNorm = 108.7462, GNorm = 0.2982
Meta loss on this task batch = 2.8224e-01, Meta loss averaged over last 500 steps = 3.0535e-01, PNorm = 108.7398, GNorm = 0.3095
Meta loss on this task batch = 2.7519e-01, Meta loss averaged over last 500 steps = 3.0531e-01, PNorm = 108.7344, GNorm = 0.2871
Meta loss on this task batch = 2.9294e-01, Meta loss averaged over last 500 steps = 3.0534e-01, PNorm = 108.7321, GNorm = 0.2757
Meta loss on this task batch = 3.0339e-01, Meta loss averaged over last 500 steps = 3.0534e-01, PNorm = 108.7315, GNorm = 0.3265
Meta loss on this task batch = 3.2196e-01, Meta loss averaged over last 500 steps = 3.0537e-01, PNorm = 108.7341, GNorm = 0.3030
Meta loss on this task batch = 2.9700e-01, Meta loss averaged over last 500 steps = 3.0543e-01, PNorm = 108.7369, GNorm = 0.3188
Meta loss on this task batch = 3.0714e-01, Meta loss averaged over last 500 steps = 3.0535e-01, PNorm = 108.7417, GNorm = 0.3604
Meta loss on this task batch = 2.9147e-01, Meta loss averaged over last 500 steps = 3.0534e-01, PNorm = 108.7485, GNorm = 0.3427
Meta loss on this task batch = 3.3359e-01, Meta loss averaged over last 500 steps = 3.0541e-01, PNorm = 108.7576, GNorm = 0.4540
Meta loss on this task batch = 3.4498e-01, Meta loss averaged over last 500 steps = 3.0551e-01, PNorm = 108.7647, GNorm = 0.3659
Meta loss on this task batch = 3.4124e-01, Meta loss averaged over last 500 steps = 3.0564e-01, PNorm = 108.7737, GNorm = 0.3303
Meta loss on this task batch = 2.7651e-01, Meta loss averaged over last 500 steps = 3.0557e-01, PNorm = 108.7831, GNorm = 0.2808
Meta loss on this task batch = 2.7945e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 108.7954, GNorm = 0.2823
Meta loss on this task batch = 3.7498e-01, Meta loss averaged over last 500 steps = 3.0561e-01, PNorm = 108.8042, GNorm = 0.3118
Meta loss on this task batch = 2.7523e-01, Meta loss averaged over last 500 steps = 3.0554e-01, PNorm = 108.8130, GNorm = 0.3207
Took 129.9878056049347 seconds to complete one epoch of meta training
Took 138.0017671585083 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479000
Epoch 365
Meta loss on this task batch = 2.5930e-01, Meta loss averaged over last 500 steps = 3.0550e-01, PNorm = 108.8226, GNorm = 0.2770
Meta loss on this task batch = 3.2962e-01, Meta loss averaged over last 500 steps = 3.0561e-01, PNorm = 108.8322, GNorm = 0.2972
Meta loss on this task batch = 3.8028e-01, Meta loss averaged over last 500 steps = 3.0583e-01, PNorm = 108.8384, GNorm = 0.2874
Meta loss on this task batch = 2.3944e-01, Meta loss averaged over last 500 steps = 3.0574e-01, PNorm = 108.8462, GNorm = 0.2675
Meta loss on this task batch = 3.1960e-01, Meta loss averaged over last 500 steps = 3.0577e-01, PNorm = 108.8531, GNorm = 0.2498
Meta loss on this task batch = 3.1134e-01, Meta loss averaged over last 500 steps = 3.0577e-01, PNorm = 108.8580, GNorm = 0.2556
Meta loss on this task batch = 2.6724e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 108.8628, GNorm = 0.4487
Meta loss on this task batch = 2.9046e-01, Meta loss averaged over last 500 steps = 3.0564e-01, PNorm = 108.8629, GNorm = 0.3526
Meta loss on this task batch = 3.6943e-01, Meta loss averaged over last 500 steps = 3.0575e-01, PNorm = 108.8610, GNorm = 0.2920
Meta loss on this task batch = 3.2747e-01, Meta loss averaged over last 500 steps = 3.0578e-01, PNorm = 108.8619, GNorm = 0.3582
Meta loss on this task batch = 3.2139e-01, Meta loss averaged over last 500 steps = 3.0578e-01, PNorm = 108.8627, GNorm = 0.2694
Meta loss on this task batch = 2.6684e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 108.8660, GNorm = 0.2368
Meta loss on this task batch = 3.7014e-01, Meta loss averaged over last 500 steps = 3.0587e-01, PNorm = 108.8707, GNorm = 0.3041
Meta loss on this task batch = 2.7994e-01, Meta loss averaged over last 500 steps = 3.0588e-01, PNorm = 108.8778, GNorm = 0.2707
Meta loss on this task batch = 3.4604e-01, Meta loss averaged over last 500 steps = 3.0591e-01, PNorm = 108.8850, GNorm = 0.2917
Meta loss on this task batch = 3.0922e-01, Meta loss averaged over last 500 steps = 3.0593e-01, PNorm = 108.8943, GNorm = 0.2836
Meta loss on this task batch = 2.8439e-01, Meta loss averaged over last 500 steps = 3.0597e-01, PNorm = 108.9047, GNorm = 0.2968
Meta loss on this task batch = 2.7049e-01, Meta loss averaged over last 500 steps = 3.0588e-01, PNorm = 108.9163, GNorm = 0.2549
Meta loss on this task batch = 2.4702e-01, Meta loss averaged over last 500 steps = 3.0587e-01, PNorm = 108.9290, GNorm = 0.2884
Took 129.72107648849487 seconds to complete one epoch of meta training
Took 139.0431830883026 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470085
Epoch 366
Meta loss on this task batch = 3.1727e-01, Meta loss averaged over last 500 steps = 3.0585e-01, PNorm = 108.9399, GNorm = 0.2855
Meta loss on this task batch = 2.9469e-01, Meta loss averaged over last 500 steps = 3.0587e-01, PNorm = 108.9498, GNorm = 0.3287
Meta loss on this task batch = 3.2931e-01, Meta loss averaged over last 500 steps = 3.0589e-01, PNorm = 108.9592, GNorm = 0.3009
Meta loss on this task batch = 3.0364e-01, Meta loss averaged over last 500 steps = 3.0586e-01, PNorm = 108.9676, GNorm = 0.3026
Meta loss on this task batch = 2.8613e-01, Meta loss averaged over last 500 steps = 3.0589e-01, PNorm = 108.9757, GNorm = 0.3069
Meta loss on this task batch = 2.4811e-01, Meta loss averaged over last 500 steps = 3.0572e-01, PNorm = 108.9842, GNorm = 0.2971
Meta loss on this task batch = 3.4329e-01, Meta loss averaged over last 500 steps = 3.0579e-01, PNorm = 108.9902, GNorm = 0.3007
Meta loss on this task batch = 2.5776e-01, Meta loss averaged over last 500 steps = 3.0567e-01, PNorm = 108.9960, GNorm = 0.3101
Meta loss on this task batch = 3.2329e-01, Meta loss averaged over last 500 steps = 3.0553e-01, PNorm = 108.9989, GNorm = 0.3364
Meta loss on this task batch = 3.1092e-01, Meta loss averaged over last 500 steps = 3.0556e-01, PNorm = 109.0028, GNorm = 0.3320
Meta loss on this task batch = 2.3261e-01, Meta loss averaged over last 500 steps = 3.0544e-01, PNorm = 109.0049, GNorm = 0.2740
Meta loss on this task batch = 3.5764e-01, Meta loss averaged over last 500 steps = 3.0555e-01, PNorm = 109.0058, GNorm = 0.3298
Meta loss on this task batch = 2.8083e-01, Meta loss averaged over last 500 steps = 3.0548e-01, PNorm = 109.0061, GNorm = 0.2570
Meta loss on this task batch = 3.3944e-01, Meta loss averaged over last 500 steps = 3.0569e-01, PNorm = 109.0053, GNorm = 0.3197
Meta loss on this task batch = 2.8092e-01, Meta loss averaged over last 500 steps = 3.0565e-01, PNorm = 109.0071, GNorm = 0.2920
Meta loss on this task batch = 2.8176e-01, Meta loss averaged over last 500 steps = 3.0554e-01, PNorm = 109.0092, GNorm = 0.3026
Meta loss on this task batch = 3.0820e-01, Meta loss averaged over last 500 steps = 3.0551e-01, PNorm = 109.0146, GNorm = 0.2851
Meta loss on this task batch = 3.0771e-01, Meta loss averaged over last 500 steps = 3.0549e-01, PNorm = 109.0213, GNorm = 0.2996
Meta loss on this task batch = 2.8766e-01, Meta loss averaged over last 500 steps = 3.0546e-01, PNorm = 109.0282, GNorm = 0.3196
Took 134.45606112480164 seconds to complete one epoch of meta training
Took 142.3060622215271 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490247
Epoch 367
Meta loss on this task batch = 2.9944e-01, Meta loss averaged over last 500 steps = 3.0553e-01, PNorm = 109.0371, GNorm = 0.2919
Meta loss on this task batch = 2.7642e-01, Meta loss averaged over last 500 steps = 3.0541e-01, PNorm = 109.0479, GNorm = 0.2795
Meta loss on this task batch = 2.5889e-01, Meta loss averaged over last 500 steps = 3.0536e-01, PNorm = 109.0583, GNorm = 0.2671
Meta loss on this task batch = 3.2827e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 109.0668, GNorm = 0.2856
Meta loss on this task batch = 3.4664e-01, Meta loss averaged over last 500 steps = 3.0561e-01, PNorm = 109.0704, GNorm = 0.3501
Meta loss on this task batch = 2.5965e-01, Meta loss averaged over last 500 steps = 3.0544e-01, PNorm = 109.0744, GNorm = 0.2930
Meta loss on this task batch = 2.8747e-01, Meta loss averaged over last 500 steps = 3.0544e-01, PNorm = 109.0779, GNorm = 0.2827
Meta loss on this task batch = 2.7075e-01, Meta loss averaged over last 500 steps = 3.0533e-01, PNorm = 109.0801, GNorm = 0.2813
Meta loss on this task batch = 2.8245e-01, Meta loss averaged over last 500 steps = 3.0528e-01, PNorm = 109.0816, GNorm = 0.2846
Meta loss on this task batch = 3.6950e-01, Meta loss averaged over last 500 steps = 3.0539e-01, PNorm = 109.0815, GNorm = 0.3597
Meta loss on this task batch = 3.3276e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 109.0812, GNorm = 0.3225
Meta loss on this task batch = 3.4866e-01, Meta loss averaged over last 500 steps = 3.0544e-01, PNorm = 109.0789, GNorm = 0.2975
Meta loss on this task batch = 3.3679e-01, Meta loss averaged over last 500 steps = 3.0562e-01, PNorm = 109.0797, GNorm = 0.3020
Meta loss on this task batch = 2.7930e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 109.0846, GNorm = 0.2771
Meta loss on this task batch = 3.0238e-01, Meta loss averaged over last 500 steps = 3.0571e-01, PNorm = 109.0938, GNorm = 0.2951
Meta loss on this task batch = 3.2591e-01, Meta loss averaged over last 500 steps = 3.0574e-01, PNorm = 109.1058, GNorm = 0.3227
Meta loss on this task batch = 3.4363e-01, Meta loss averaged over last 500 steps = 3.0578e-01, PNorm = 109.1211, GNorm = 0.3202
Meta loss on this task batch = 2.7551e-01, Meta loss averaged over last 500 steps = 3.0590e-01, PNorm = 109.1361, GNorm = 0.2410
Meta loss on this task batch = 2.5703e-01, Meta loss averaged over last 500 steps = 3.0577e-01, PNorm = 109.1514, GNorm = 0.3011
Took 132.8995840549469 seconds to complete one epoch of meta training
Took 140.48785734176636 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498700
Epoch 368
Meta loss on this task batch = 3.3902e-01, Meta loss averaged over last 500 steps = 3.0567e-01, PNorm = 109.1619, GNorm = 0.3650
Meta loss on this task batch = 3.4773e-01, Meta loss averaged over last 500 steps = 3.0580e-01, PNorm = 109.1704, GNorm = 0.3514
Meta loss on this task batch = 3.1460e-01, Meta loss averaged over last 500 steps = 3.0570e-01, PNorm = 109.1763, GNorm = 0.3070
Meta loss on this task batch = 3.6157e-01, Meta loss averaged over last 500 steps = 3.0570e-01, PNorm = 109.1815, GNorm = 0.3279
Meta loss on this task batch = 2.9255e-01, Meta loss averaged over last 500 steps = 3.0578e-01, PNorm = 109.1859, GNorm = 0.3443
Meta loss on this task batch = 3.3802e-01, Meta loss averaged over last 500 steps = 3.0574e-01, PNorm = 109.1886, GNorm = 0.3150
Meta loss on this task batch = 2.9303e-01, Meta loss averaged over last 500 steps = 3.0573e-01, PNorm = 109.1922, GNorm = 0.2708
Meta loss on this task batch = 3.2026e-01, Meta loss averaged over last 500 steps = 3.0581e-01, PNorm = 109.1977, GNorm = 0.3433
Meta loss on this task batch = 3.0152e-01, Meta loss averaged over last 500 steps = 3.0577e-01, PNorm = 109.2024, GNorm = 0.2738
Meta loss on this task batch = 3.2576e-01, Meta loss averaged over last 500 steps = 3.0587e-01, PNorm = 109.2067, GNorm = 0.2828
Meta loss on this task batch = 2.6894e-01, Meta loss averaged over last 500 steps = 3.0577e-01, PNorm = 109.2141, GNorm = 0.2706
Meta loss on this task batch = 3.0082e-01, Meta loss averaged over last 500 steps = 3.0583e-01, PNorm = 109.2210, GNorm = 0.2843
Meta loss on this task batch = 2.0897e-01, Meta loss averaged over last 500 steps = 3.0554e-01, PNorm = 109.2288, GNorm = 0.2322
Meta loss on this task batch = 3.1272e-01, Meta loss averaged over last 500 steps = 3.0549e-01, PNorm = 109.2380, GNorm = 0.3229
Meta loss on this task batch = 2.5308e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 109.2492, GNorm = 0.2407
Meta loss on this task batch = 2.5877e-01, Meta loss averaged over last 500 steps = 3.0544e-01, PNorm = 109.2608, GNorm = 0.2873
Meta loss on this task batch = 2.8344e-01, Meta loss averaged over last 500 steps = 3.0539e-01, PNorm = 109.2708, GNorm = 0.3000
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 3.0543e-01, PNorm = 109.2774, GNorm = 0.2861
Meta loss on this task batch = 3.2136e-01, Meta loss averaged over last 500 steps = 3.0552e-01, PNorm = 109.2849, GNorm = 0.3780
Took 134.8116934299469 seconds to complete one epoch of meta training
Took 143.60758471488953 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488651
Epoch 369
Meta loss on this task batch = 2.7360e-01, Meta loss averaged over last 500 steps = 3.0551e-01, PNorm = 109.2924, GNorm = 0.3103
Meta loss on this task batch = 3.2168e-01, Meta loss averaged over last 500 steps = 3.0551e-01, PNorm = 109.2982, GNorm = 0.3284
Meta loss on this task batch = 2.6600e-01, Meta loss averaged over last 500 steps = 3.0536e-01, PNorm = 109.3014, GNorm = 0.3395
Meta loss on this task batch = 3.0128e-01, Meta loss averaged over last 500 steps = 3.0529e-01, PNorm = 109.3039, GNorm = 0.2875
Meta loss on this task batch = 2.9407e-01, Meta loss averaged over last 500 steps = 3.0514e-01, PNorm = 109.3076, GNorm = 0.2771
Meta loss on this task batch = 2.5587e-01, Meta loss averaged over last 500 steps = 3.0507e-01, PNorm = 109.3134, GNorm = 0.2629
Meta loss on this task batch = 2.6142e-01, Meta loss averaged over last 500 steps = 3.0494e-01, PNorm = 109.3191, GNorm = 0.2567
Meta loss on this task batch = 3.0160e-01, Meta loss averaged over last 500 steps = 3.0499e-01, PNorm = 109.3258, GNorm = 0.3084
Meta loss on this task batch = 2.9108e-01, Meta loss averaged over last 500 steps = 3.0503e-01, PNorm = 109.3366, GNorm = 0.3214
Meta loss on this task batch = 3.4238e-01, Meta loss averaged over last 500 steps = 3.0516e-01, PNorm = 109.3449, GNorm = 0.3106
Meta loss on this task batch = 2.3866e-01, Meta loss averaged over last 500 steps = 3.0495e-01, PNorm = 109.3556, GNorm = 0.2951
Meta loss on this task batch = 2.8607e-01, Meta loss averaged over last 500 steps = 3.0501e-01, PNorm = 109.3655, GNorm = 0.2896
Meta loss on this task batch = 3.2140e-01, Meta loss averaged over last 500 steps = 3.0511e-01, PNorm = 109.3738, GNorm = 0.3022
Meta loss on this task batch = 3.1482e-01, Meta loss averaged over last 500 steps = 3.0507e-01, PNorm = 109.3808, GNorm = 0.3262
Meta loss on this task batch = 3.5280e-01, Meta loss averaged over last 500 steps = 3.0526e-01, PNorm = 109.3860, GNorm = 0.3423
Meta loss on this task batch = 3.1834e-01, Meta loss averaged over last 500 steps = 3.0537e-01, PNorm = 109.3896, GNorm = 0.2978
Meta loss on this task batch = 3.2687e-01, Meta loss averaged over last 500 steps = 3.0538e-01, PNorm = 109.3913, GNorm = 0.3001
Meta loss on this task batch = 2.9392e-01, Meta loss averaged over last 500 steps = 3.0526e-01, PNorm = 109.3937, GNorm = 0.2988
Meta loss on this task batch = 2.8954e-01, Meta loss averaged over last 500 steps = 3.0524e-01, PNorm = 109.3972, GNorm = 0.3837
Took 133.50934028625488 seconds to complete one epoch of meta training
Took 140.6887366771698 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469964
Epoch 370
Meta loss on this task batch = 3.3930e-01, Meta loss averaged over last 500 steps = 3.0545e-01, PNorm = 109.4002, GNorm = 0.2718
Meta loss on this task batch = 2.5416e-01, Meta loss averaged over last 500 steps = 3.0521e-01, PNorm = 109.4047, GNorm = 0.2397
Meta loss on this task batch = 3.0998e-01, Meta loss averaged over last 500 steps = 3.0507e-01, PNorm = 109.4099, GNorm = 0.2808
Meta loss on this task batch = 3.1390e-01, Meta loss averaged over last 500 steps = 3.0499e-01, PNorm = 109.4165, GNorm = 0.2764
Meta loss on this task batch = 3.6238e-01, Meta loss averaged over last 500 steps = 3.0512e-01, PNorm = 109.4206, GNorm = 0.3148
Meta loss on this task batch = 2.8971e-01, Meta loss averaged over last 500 steps = 3.0507e-01, PNorm = 109.4255, GNorm = 0.2396
Meta loss on this task batch = 3.1878e-01, Meta loss averaged over last 500 steps = 3.0518e-01, PNorm = 109.4329, GNorm = 0.2770
Meta loss on this task batch = 3.1365e-01, Meta loss averaged over last 500 steps = 3.0530e-01, PNorm = 109.4421, GNorm = 0.2615
Meta loss on this task batch = 3.8313e-01, Meta loss averaged over last 500 steps = 3.0541e-01, PNorm = 109.4526, GNorm = 0.2828
Meta loss on this task batch = 2.5167e-01, Meta loss averaged over last 500 steps = 3.0525e-01, PNorm = 109.4636, GNorm = 0.2508
Meta loss on this task batch = 2.5696e-01, Meta loss averaged over last 500 steps = 3.0516e-01, PNorm = 109.4761, GNorm = 0.2519
Meta loss on this task batch = 2.9345e-01, Meta loss averaged over last 500 steps = 3.0513e-01, PNorm = 109.4882, GNorm = 0.3111
Meta loss on this task batch = 3.4592e-01, Meta loss averaged over last 500 steps = 3.0529e-01, PNorm = 109.4995, GNorm = 0.2701
Meta loss on this task batch = 3.0492e-01, Meta loss averaged over last 500 steps = 3.0528e-01, PNorm = 109.5092, GNorm = 0.2658
Meta loss on this task batch = 3.0006e-01, Meta loss averaged over last 500 steps = 3.0525e-01, PNorm = 109.5164, GNorm = 0.2726
Meta loss on this task batch = 2.3627e-01, Meta loss averaged over last 500 steps = 3.0513e-01, PNorm = 109.5235, GNorm = 0.2563
Meta loss on this task batch = 2.3757e-01, Meta loss averaged over last 500 steps = 3.0504e-01, PNorm = 109.5310, GNorm = 0.2669
Meta loss on this task batch = 3.7464e-01, Meta loss averaged over last 500 steps = 3.0500e-01, PNorm = 109.5374, GNorm = 0.3202
Meta loss on this task batch = 2.4535e-01, Meta loss averaged over last 500 steps = 3.0498e-01, PNorm = 109.5424, GNorm = 0.2953
Took 130.42939352989197 seconds to complete one epoch of meta training
Took 138.9104664325714 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481127
Epoch 371
Meta loss on this task batch = 3.4955e-01, Meta loss averaged over last 500 steps = 3.0508e-01, PNorm = 109.5470, GNorm = 0.3239
Meta loss on this task batch = 3.2422e-01, Meta loss averaged over last 500 steps = 3.0506e-01, PNorm = 109.5502, GNorm = 0.4048
Meta loss on this task batch = 2.7562e-01, Meta loss averaged over last 500 steps = 3.0486e-01, PNorm = 109.5558, GNorm = 0.3065
Meta loss on this task batch = 2.9375e-01, Meta loss averaged over last 500 steps = 3.0478e-01, PNorm = 109.5619, GNorm = 0.2632
Meta loss on this task batch = 2.9375e-01, Meta loss averaged over last 500 steps = 3.0491e-01, PNorm = 109.5685, GNorm = 0.2453
Meta loss on this task batch = 3.5431e-01, Meta loss averaged over last 500 steps = 3.0481e-01, PNorm = 109.5732, GNorm = 0.2868
Meta loss on this task batch = 2.8530e-01, Meta loss averaged over last 500 steps = 3.0465e-01, PNorm = 109.5788, GNorm = 0.2613
Meta loss on this task batch = 3.0431e-01, Meta loss averaged over last 500 steps = 3.0465e-01, PNorm = 109.5836, GNorm = 0.2641
Meta loss on this task batch = 2.8437e-01, Meta loss averaged over last 500 steps = 3.0456e-01, PNorm = 109.5900, GNorm = 0.2665
Meta loss on this task batch = 2.7020e-01, Meta loss averaged over last 500 steps = 3.0445e-01, PNorm = 109.5965, GNorm = 0.2616
Meta loss on this task batch = 3.0643e-01, Meta loss averaged over last 500 steps = 3.0448e-01, PNorm = 109.6035, GNorm = 0.2601
Meta loss on this task batch = 3.1818e-01, Meta loss averaged over last 500 steps = 3.0451e-01, PNorm = 109.6096, GNorm = 0.2847
Meta loss on this task batch = 3.0373e-01, Meta loss averaged over last 500 steps = 3.0462e-01, PNorm = 109.6127, GNorm = 0.3118
Meta loss on this task batch = 2.8852e-01, Meta loss averaged over last 500 steps = 3.0466e-01, PNorm = 109.6165, GNorm = 0.2932
Meta loss on this task batch = 2.9378e-01, Meta loss averaged over last 500 steps = 3.0471e-01, PNorm = 109.6196, GNorm = 0.2802
Meta loss on this task batch = 3.0701e-01, Meta loss averaged over last 500 steps = 3.0476e-01, PNorm = 109.6193, GNorm = 0.3202
Meta loss on this task batch = 2.9178e-01, Meta loss averaged over last 500 steps = 3.0476e-01, PNorm = 109.6168, GNorm = 0.3114
Meta loss on this task batch = 2.8126e-01, Meta loss averaged over last 500 steps = 3.0473e-01, PNorm = 109.6132, GNorm = 0.2729
Meta loss on this task batch = 3.6951e-01, Meta loss averaged over last 500 steps = 3.0482e-01, PNorm = 109.6136, GNorm = 0.3983
Took 131.89392399787903 seconds to complete one epoch of meta training
Took 140.41127943992615 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479896
Epoch 372
Meta loss on this task batch = 3.4763e-01, Meta loss averaged over last 500 steps = 3.0483e-01, PNorm = 109.6141, GNorm = 0.2881
Meta loss on this task batch = 3.6621e-01, Meta loss averaged over last 500 steps = 3.0487e-01, PNorm = 109.6140, GNorm = 0.3021
Meta loss on this task batch = 3.1514e-01, Meta loss averaged over last 500 steps = 3.0484e-01, PNorm = 109.6160, GNorm = 0.2802
Meta loss on this task batch = 2.7079e-01, Meta loss averaged over last 500 steps = 3.0476e-01, PNorm = 109.6186, GNorm = 0.2649
Meta loss on this task batch = 2.7072e-01, Meta loss averaged over last 500 steps = 3.0478e-01, PNorm = 109.6251, GNorm = 0.2920
Meta loss on this task batch = 3.1225e-01, Meta loss averaged over last 500 steps = 3.0475e-01, PNorm = 109.6313, GNorm = 0.2976
Meta loss on this task batch = 3.7133e-01, Meta loss averaged over last 500 steps = 3.0499e-01, PNorm = 109.6369, GNorm = 0.3103
Meta loss on this task batch = 3.0392e-01, Meta loss averaged over last 500 steps = 3.0503e-01, PNorm = 109.6426, GNorm = 0.2688
Meta loss on this task batch = 3.1215e-01, Meta loss averaged over last 500 steps = 3.0510e-01, PNorm = 109.6476, GNorm = 0.3136
Meta loss on this task batch = 2.6205e-01, Meta loss averaged over last 500 steps = 3.0501e-01, PNorm = 109.6503, GNorm = 0.2414
Meta loss on this task batch = 2.9138e-01, Meta loss averaged over last 500 steps = 3.0504e-01, PNorm = 109.6537, GNorm = 0.2734
Meta loss on this task batch = 3.1640e-01, Meta loss averaged over last 500 steps = 3.0512e-01, PNorm = 109.6580, GNorm = 0.3175
Meta loss on this task batch = 3.0176e-01, Meta loss averaged over last 500 steps = 3.0500e-01, PNorm = 109.6609, GNorm = 0.3135
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 3.0504e-01, PNorm = 109.6623, GNorm = 0.2749
Meta loss on this task batch = 2.5187e-01, Meta loss averaged over last 500 steps = 3.0508e-01, PNorm = 109.6633, GNorm = 0.2238
Meta loss on this task batch = 3.0207e-01, Meta loss averaged over last 500 steps = 3.0504e-01, PNorm = 109.6648, GNorm = 0.2563
Meta loss on this task batch = 3.1719e-01, Meta loss averaged over last 500 steps = 3.0489e-01, PNorm = 109.6657, GNorm = 0.3050
Meta loss on this task batch = 2.9905e-01, Meta loss averaged over last 500 steps = 3.0488e-01, PNorm = 109.6689, GNorm = 0.2817
Meta loss on this task batch = 2.6438e-01, Meta loss averaged over last 500 steps = 3.0477e-01, PNorm = 109.6760, GNorm = 0.3435
Took 131.99313735961914 seconds to complete one epoch of meta training
Took 140.75850701332092 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496949
Epoch 373
Meta loss on this task batch = 2.5184e-01, Meta loss averaged over last 500 steps = 3.0475e-01, PNorm = 109.6834, GNorm = 0.2406
Meta loss on this task batch = 3.0627e-01, Meta loss averaged over last 500 steps = 3.0474e-01, PNorm = 109.6884, GNorm = 0.3772
Meta loss on this task batch = 2.5645e-01, Meta loss averaged over last 500 steps = 3.0463e-01, PNorm = 109.6931, GNorm = 0.2552
Meta loss on this task batch = 2.5799e-01, Meta loss averaged over last 500 steps = 3.0449e-01, PNorm = 109.6975, GNorm = 0.2522
Meta loss on this task batch = 2.7544e-01, Meta loss averaged over last 500 steps = 3.0439e-01, PNorm = 109.7010, GNorm = 0.3705
Meta loss on this task batch = 3.3518e-01, Meta loss averaged over last 500 steps = 3.0440e-01, PNorm = 109.7026, GNorm = 0.3983
Meta loss on this task batch = 2.9542e-01, Meta loss averaged over last 500 steps = 3.0450e-01, PNorm = 109.7016, GNorm = 0.3520
Meta loss on this task batch = 2.9705e-01, Meta loss averaged over last 500 steps = 3.0454e-01, PNorm = 109.6993, GNorm = 0.2923
Meta loss on this task batch = 3.0593e-01, Meta loss averaged over last 500 steps = 3.0456e-01, PNorm = 109.6980, GNorm = 0.2989
Meta loss on this task batch = 3.0190e-01, Meta loss averaged over last 500 steps = 3.0453e-01, PNorm = 109.6994, GNorm = 0.3021
Meta loss on this task batch = 2.7162e-01, Meta loss averaged over last 500 steps = 3.0437e-01, PNorm = 109.7029, GNorm = 0.2707
Meta loss on this task batch = 3.6461e-01, Meta loss averaged over last 500 steps = 3.0453e-01, PNorm = 109.7036, GNorm = 0.3453
Meta loss on this task batch = 2.7237e-01, Meta loss averaged over last 500 steps = 3.0443e-01, PNorm = 109.7056, GNorm = 0.2869
Meta loss on this task batch = 3.3011e-01, Meta loss averaged over last 500 steps = 3.0455e-01, PNorm = 109.7073, GNorm = 0.3211
Meta loss on this task batch = 3.2993e-01, Meta loss averaged over last 500 steps = 3.0459e-01, PNorm = 109.7084, GNorm = 0.3250
Meta loss on this task batch = 3.0406e-01, Meta loss averaged over last 500 steps = 3.0455e-01, PNorm = 109.7137, GNorm = 0.3178
Meta loss on this task batch = 3.5432e-01, Meta loss averaged over last 500 steps = 3.0469e-01, PNorm = 109.7212, GNorm = 0.3416
Meta loss on this task batch = 3.3034e-01, Meta loss averaged over last 500 steps = 3.0479e-01, PNorm = 109.7301, GNorm = 0.2801
Meta loss on this task batch = 3.0452e-01, Meta loss averaged over last 500 steps = 3.0467e-01, PNorm = 109.7432, GNorm = 0.3139
Took 130.85746550559998 seconds to complete one epoch of meta training
Took 138.27973747253418 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497304
Epoch 374
Meta loss on this task batch = 2.5665e-01, Meta loss averaged over last 500 steps = 3.0450e-01, PNorm = 109.7564, GNorm = 0.2381
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 3.0443e-01, PNorm = 109.7700, GNorm = 0.2525
Meta loss on this task batch = 3.7108e-01, Meta loss averaged over last 500 steps = 3.0465e-01, PNorm = 109.7810, GNorm = 0.2942
Meta loss on this task batch = 2.1333e-01, Meta loss averaged over last 500 steps = 3.0453e-01, PNorm = 109.7892, GNorm = 0.2148
Meta loss on this task batch = 3.2213e-01, Meta loss averaged over last 500 steps = 3.0459e-01, PNorm = 109.7964, GNorm = 0.3092
Meta loss on this task batch = 3.0919e-01, Meta loss averaged over last 500 steps = 3.0440e-01, PNorm = 109.8005, GNorm = 0.3394
Meta loss on this task batch = 3.3403e-01, Meta loss averaged over last 500 steps = 3.0446e-01, PNorm = 109.8002, GNorm = 0.3183
Meta loss on this task batch = 3.5171e-01, Meta loss averaged over last 500 steps = 3.0452e-01, PNorm = 109.7995, GNorm = 0.3173
Meta loss on this task batch = 3.1034e-01, Meta loss averaged over last 500 steps = 3.0454e-01, PNorm = 109.7974, GNorm = 0.2868
Meta loss on this task batch = 2.6136e-01, Meta loss averaged over last 500 steps = 3.0441e-01, PNorm = 109.7973, GNorm = 0.2299
Meta loss on this task batch = 3.0120e-01, Meta loss averaged over last 500 steps = 3.0426e-01, PNorm = 109.7992, GNorm = 0.2769
Meta loss on this task batch = 3.1639e-01, Meta loss averaged over last 500 steps = 3.0424e-01, PNorm = 109.8029, GNorm = 0.2819
Meta loss on this task batch = 2.9924e-01, Meta loss averaged over last 500 steps = 3.0430e-01, PNorm = 109.8071, GNorm = 0.2884
Meta loss on this task batch = 3.4075e-01, Meta loss averaged over last 500 steps = 3.0429e-01, PNorm = 109.8115, GNorm = 0.2971
Meta loss on this task batch = 2.7799e-01, Meta loss averaged over last 500 steps = 3.0437e-01, PNorm = 109.8190, GNorm = 0.3392
Meta loss on this task batch = 3.1672e-01, Meta loss averaged over last 500 steps = 3.0436e-01, PNorm = 109.8270, GNorm = 0.3253
Meta loss on this task batch = 2.9115e-01, Meta loss averaged over last 500 steps = 3.0432e-01, PNorm = 109.8350, GNorm = 0.3091
Meta loss on this task batch = 3.3845e-01, Meta loss averaged over last 500 steps = 3.0446e-01, PNorm = 109.8407, GNorm = 0.3474
Meta loss on this task batch = 3.3792e-01, Meta loss averaged over last 500 steps = 3.0461e-01, PNorm = 109.8465, GNorm = 0.3383
Took 135.10235977172852 seconds to complete one epoch of meta training
Took 143.85406684875488 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492107
Epoch 375
Meta loss on this task batch = 2.8819e-01, Meta loss averaged over last 500 steps = 3.0454e-01, PNorm = 109.8527, GNorm = 0.3021
Meta loss on this task batch = 3.1941e-01, Meta loss averaged over last 500 steps = 3.0461e-01, PNorm = 109.8586, GNorm = 0.3115
Meta loss on this task batch = 3.8215e-01, Meta loss averaged over last 500 steps = 3.0465e-01, PNorm = 109.8645, GNorm = 0.3533
Meta loss on this task batch = 2.9714e-01, Meta loss averaged over last 500 steps = 3.0465e-01, PNorm = 109.8720, GNorm = 0.2735
Meta loss on this task batch = 3.1936e-01, Meta loss averaged over last 500 steps = 3.0471e-01, PNorm = 109.8815, GNorm = 0.3166
Meta loss on this task batch = 3.1565e-01, Meta loss averaged over last 500 steps = 3.0474e-01, PNorm = 109.8926, GNorm = 0.2624
Meta loss on this task batch = 2.7977e-01, Meta loss averaged over last 500 steps = 3.0474e-01, PNorm = 109.9055, GNorm = 0.2884
Meta loss on this task batch = 2.5265e-01, Meta loss averaged over last 500 steps = 3.0470e-01, PNorm = 109.9189, GNorm = 0.2739
Meta loss on this task batch = 2.9789e-01, Meta loss averaged over last 500 steps = 3.0454e-01, PNorm = 109.9316, GNorm = 0.2850
Meta loss on this task batch = 3.1349e-01, Meta loss averaged over last 500 steps = 3.0467e-01, PNorm = 109.9422, GNorm = 0.2822
Meta loss on this task batch = 2.6773e-01, Meta loss averaged over last 500 steps = 3.0474e-01, PNorm = 109.9519, GNorm = 0.2895
Meta loss on this task batch = 3.1176e-01, Meta loss averaged over last 500 steps = 3.0468e-01, PNorm = 109.9608, GNorm = 0.2938
Meta loss on this task batch = 3.5986e-01, Meta loss averaged over last 500 steps = 3.0479e-01, PNorm = 109.9690, GNorm = 0.3042
Meta loss on this task batch = 3.1669e-01, Meta loss averaged over last 500 steps = 3.0486e-01, PNorm = 109.9789, GNorm = 0.2801
Meta loss on this task batch = 3.0795e-01, Meta loss averaged over last 500 steps = 3.0484e-01, PNorm = 109.9875, GNorm = 0.3317
Meta loss on this task batch = 2.1803e-01, Meta loss averaged over last 500 steps = 3.0465e-01, PNorm = 109.9969, GNorm = 0.2344
Meta loss on this task batch = 3.0677e-01, Meta loss averaged over last 500 steps = 3.0477e-01, PNorm = 110.0063, GNorm = 0.2875
Meta loss on this task batch = 3.1468e-01, Meta loss averaged over last 500 steps = 3.0484e-01, PNorm = 110.0146, GNorm = 0.3335
Meta loss on this task batch = 3.2888e-01, Meta loss averaged over last 500 steps = 3.0486e-01, PNorm = 110.0217, GNorm = 0.4441
Took 134.91727113723755 seconds to complete one epoch of meta training
Took 143.16391348838806 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508930
Epoch 376
Meta loss on this task batch = 3.4665e-01, Meta loss averaged over last 500 steps = 3.0496e-01, PNorm = 110.0266, GNorm = 0.3308
Meta loss on this task batch = 3.1928e-01, Meta loss averaged over last 500 steps = 3.0494e-01, PNorm = 110.0320, GNorm = 0.3062
Meta loss on this task batch = 2.2606e-01, Meta loss averaged over last 500 steps = 3.0476e-01, PNorm = 110.0387, GNorm = 0.2397
Meta loss on this task batch = 2.6701e-01, Meta loss averaged over last 500 steps = 3.0460e-01, PNorm = 110.0455, GNorm = 0.2329
Meta loss on this task batch = 2.6530e-01, Meta loss averaged over last 500 steps = 3.0445e-01, PNorm = 110.0530, GNorm = 0.2678
Meta loss on this task batch = 2.8963e-01, Meta loss averaged over last 500 steps = 3.0451e-01, PNorm = 110.0591, GNorm = 0.2781
Meta loss on this task batch = 2.9375e-01, Meta loss averaged over last 500 steps = 3.0440e-01, PNorm = 110.0666, GNorm = 0.3023
Meta loss on this task batch = 3.0523e-01, Meta loss averaged over last 500 steps = 3.0428e-01, PNorm = 110.0766, GNorm = 0.2886
Meta loss on this task batch = 3.3135e-01, Meta loss averaged over last 500 steps = 3.0436e-01, PNorm = 110.0854, GNorm = 0.3556
Meta loss on this task batch = 3.0176e-01, Meta loss averaged over last 500 steps = 3.0430e-01, PNorm = 110.0940, GNorm = 0.3164
Meta loss on this task batch = 2.8256e-01, Meta loss averaged over last 500 steps = 3.0425e-01, PNorm = 110.1026, GNorm = 0.2689
Meta loss on this task batch = 2.9665e-01, Meta loss averaged over last 500 steps = 3.0428e-01, PNorm = 110.1107, GNorm = 0.2812
Meta loss on this task batch = 2.9480e-01, Meta loss averaged over last 500 steps = 3.0431e-01, PNorm = 110.1214, GNorm = 0.2871
Meta loss on this task batch = 3.0701e-01, Meta loss averaged over last 500 steps = 3.0439e-01, PNorm = 110.1304, GNorm = 0.2968
Meta loss on this task batch = 3.4525e-01, Meta loss averaged over last 500 steps = 3.0445e-01, PNorm = 110.1348, GNorm = 0.3397
Meta loss on this task batch = 3.4495e-01, Meta loss averaged over last 500 steps = 3.0456e-01, PNorm = 110.1392, GNorm = 0.3244
Meta loss on this task batch = 2.5500e-01, Meta loss averaged over last 500 steps = 3.0444e-01, PNorm = 110.1428, GNorm = 0.2888
Meta loss on this task batch = 2.9001e-01, Meta loss averaged over last 500 steps = 3.0446e-01, PNorm = 110.1482, GNorm = 0.3155
Meta loss on this task batch = 3.7233e-01, Meta loss averaged over last 500 steps = 3.0452e-01, PNorm = 110.1530, GNorm = 0.3629
Took 139.01507449150085 seconds to complete one epoch of meta training
Took 147.80949926376343 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501672
Epoch 377
Meta loss on this task batch = 3.2339e-01, Meta loss averaged over last 500 steps = 3.0459e-01, PNorm = 110.1558, GNorm = 0.3091
Meta loss on this task batch = 3.1718e-01, Meta loss averaged over last 500 steps = 3.0465e-01, PNorm = 110.1604, GNorm = 0.3127
Meta loss on this task batch = 2.7466e-01, Meta loss averaged over last 500 steps = 3.0454e-01, PNorm = 110.1654, GNorm = 0.2282
Meta loss on this task batch = 3.1584e-01, Meta loss averaged over last 500 steps = 3.0459e-01, PNorm = 110.1708, GNorm = 0.2918
Meta loss on this task batch = 3.1150e-01, Meta loss averaged over last 500 steps = 3.0454e-01, PNorm = 110.1776, GNorm = 0.3724
Meta loss on this task batch = 2.3947e-01, Meta loss averaged over last 500 steps = 3.0437e-01, PNorm = 110.1869, GNorm = 0.2571
Meta loss on this task batch = 2.7065e-01, Meta loss averaged over last 500 steps = 3.0428e-01, PNorm = 110.1956, GNorm = 0.2927
Meta loss on this task batch = 2.9067e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 110.2048, GNorm = 0.3070
Meta loss on this task batch = 3.2596e-01, Meta loss averaged over last 500 steps = 3.0436e-01, PNorm = 110.2116, GNorm = 0.3130
Meta loss on this task batch = 2.8611e-01, Meta loss averaged over last 500 steps = 3.0445e-01, PNorm = 110.2197, GNorm = 0.2711
Meta loss on this task batch = 2.6961e-01, Meta loss averaged over last 500 steps = 3.0445e-01, PNorm = 110.2288, GNorm = 0.2628
Meta loss on this task batch = 3.0790e-01, Meta loss averaged over last 500 steps = 3.0453e-01, PNorm = 110.2329, GNorm = 0.3618
Meta loss on this task batch = 3.0143e-01, Meta loss averaged over last 500 steps = 3.0465e-01, PNorm = 110.2354, GNorm = 0.3148
Meta loss on this task batch = 2.7502e-01, Meta loss averaged over last 500 steps = 3.0457e-01, PNorm = 110.2399, GNorm = 0.2614
Meta loss on this task batch = 3.2115e-01, Meta loss averaged over last 500 steps = 3.0449e-01, PNorm = 110.2438, GNorm = 0.2731
Meta loss on this task batch = 3.4385e-01, Meta loss averaged over last 500 steps = 3.0450e-01, PNorm = 110.2480, GNorm = 0.2846
Meta loss on this task batch = 3.1954e-01, Meta loss averaged over last 500 steps = 3.0452e-01, PNorm = 110.2523, GNorm = 0.3009
Meta loss on this task batch = 3.2383e-01, Meta loss averaged over last 500 steps = 3.0449e-01, PNorm = 110.2585, GNorm = 0.3027
Meta loss on this task batch = 2.3420e-01, Meta loss averaged over last 500 steps = 3.0443e-01, PNorm = 110.2665, GNorm = 0.2858
Took 220.58883118629456 seconds to complete one epoch of meta training
Took 228.87100338935852 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480559
Epoch 378
Meta loss on this task batch = 2.6389e-01, Meta loss averaged over last 500 steps = 3.0429e-01, PNorm = 110.2757, GNorm = 0.2632
Meta loss on this task batch = 3.1156e-01, Meta loss averaged over last 500 steps = 3.0426e-01, PNorm = 110.2821, GNorm = 0.3350
Meta loss on this task batch = 2.9547e-01, Meta loss averaged over last 500 steps = 3.0427e-01, PNorm = 110.2899, GNorm = 0.2966
Meta loss on this task batch = 3.2571e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 110.2956, GNorm = 0.3505
Meta loss on this task batch = 3.7730e-01, Meta loss averaged over last 500 steps = 3.0430e-01, PNorm = 110.3006, GNorm = 0.3173
Meta loss on this task batch = 2.7231e-01, Meta loss averaged over last 500 steps = 3.0420e-01, PNorm = 110.3078, GNorm = 0.2965
Meta loss on this task batch = 3.2834e-01, Meta loss averaged over last 500 steps = 3.0427e-01, PNorm = 110.3085, GNorm = 0.3262
Meta loss on this task batch = 3.1569e-01, Meta loss averaged over last 500 steps = 3.0432e-01, PNorm = 110.3122, GNorm = 0.3197
Meta loss on this task batch = 3.0439e-01, Meta loss averaged over last 500 steps = 3.0432e-01, PNorm = 110.3167, GNorm = 0.2602
Meta loss on this task batch = 2.9673e-01, Meta loss averaged over last 500 steps = 3.0437e-01, PNorm = 110.3185, GNorm = 0.3344
Meta loss on this task batch = 2.9786e-01, Meta loss averaged over last 500 steps = 3.0428e-01, PNorm = 110.3208, GNorm = 0.3004
Meta loss on this task batch = 2.7790e-01, Meta loss averaged over last 500 steps = 3.0413e-01, PNorm = 110.3216, GNorm = 0.2986
Meta loss on this task batch = 2.8474e-01, Meta loss averaged over last 500 steps = 3.0413e-01, PNorm = 110.3244, GNorm = 0.2628
Meta loss on this task batch = 2.9833e-01, Meta loss averaged over last 500 steps = 3.0429e-01, PNorm = 110.3264, GNorm = 0.2955
Meta loss on this task batch = 3.0738e-01, Meta loss averaged over last 500 steps = 3.0425e-01, PNorm = 110.3308, GNorm = 0.3023
Meta loss on this task batch = 2.8818e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 110.3354, GNorm = 0.2758
Meta loss on this task batch = 3.0574e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 110.3399, GNorm = 0.2796
Meta loss on this task batch = 3.4530e-01, Meta loss averaged over last 500 steps = 3.0419e-01, PNorm = 110.3446, GNorm = 0.3058
Meta loss on this task batch = 3.2657e-01, Meta loss averaged over last 500 steps = 3.0422e-01, PNorm = 110.3503, GNorm = 0.3210
Took 144.98612213134766 seconds to complete one epoch of meta training
Took 153.98549962043762 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462086
Epoch 379
Meta loss on this task batch = 2.8908e-01, Meta loss averaged over last 500 steps = 3.0422e-01, PNorm = 110.3574, GNorm = 0.2428
Meta loss on this task batch = 3.1240e-01, Meta loss averaged over last 500 steps = 3.0418e-01, PNorm = 110.3650, GNorm = 0.2723
Meta loss on this task batch = 3.3321e-01, Meta loss averaged over last 500 steps = 3.0412e-01, PNorm = 110.3728, GNorm = 0.3187
Meta loss on this task batch = 2.8537e-01, Meta loss averaged over last 500 steps = 3.0410e-01, PNorm = 110.3798, GNorm = 0.2726
Meta loss on this task batch = 2.9403e-01, Meta loss averaged over last 500 steps = 3.0404e-01, PNorm = 110.3875, GNorm = 0.3123
Meta loss on this task batch = 3.1341e-01, Meta loss averaged over last 500 steps = 3.0404e-01, PNorm = 110.3934, GNorm = 0.2869
Meta loss on this task batch = 2.9826e-01, Meta loss averaged over last 500 steps = 3.0408e-01, PNorm = 110.3990, GNorm = 0.3178
Meta loss on this task batch = 2.3778e-01, Meta loss averaged over last 500 steps = 3.0397e-01, PNorm = 110.4054, GNorm = 0.2249
Meta loss on this task batch = 3.3271e-01, Meta loss averaged over last 500 steps = 3.0413e-01, PNorm = 110.4090, GNorm = 0.3090
Meta loss on this task batch = 2.9082e-01, Meta loss averaged over last 500 steps = 3.0396e-01, PNorm = 110.4124, GNorm = 0.2614
Meta loss on this task batch = 2.6853e-01, Meta loss averaged over last 500 steps = 3.0389e-01, PNorm = 110.4158, GNorm = 0.2560
Meta loss on this task batch = 2.7202e-01, Meta loss averaged over last 500 steps = 3.0383e-01, PNorm = 110.4207, GNorm = 0.2741
Meta loss on this task batch = 3.2847e-01, Meta loss averaged over last 500 steps = 3.0388e-01, PNorm = 110.4262, GNorm = 0.3061
Meta loss on this task batch = 2.9293e-01, Meta loss averaged over last 500 steps = 3.0379e-01, PNorm = 110.4317, GNorm = 0.4060
Meta loss on this task batch = 3.6505e-01, Meta loss averaged over last 500 steps = 3.0388e-01, PNorm = 110.4378, GNorm = 0.3349
Meta loss on this task batch = 3.4648e-01, Meta loss averaged over last 500 steps = 3.0386e-01, PNorm = 110.4432, GNorm = 0.3351
Meta loss on this task batch = 3.1409e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 110.4488, GNorm = 0.3590
Meta loss on this task batch = 3.8120e-01, Meta loss averaged over last 500 steps = 3.0415e-01, PNorm = 110.4551, GNorm = 0.3942
Meta loss on this task batch = 3.0280e-01, Meta loss averaged over last 500 steps = 3.0413e-01, PNorm = 110.4618, GNorm = 0.3484
Took 158.65765738487244 seconds to complete one epoch of meta training
Took 167.1695818901062 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484332
Epoch 380
Meta loss on this task batch = 3.1560e-01, Meta loss averaged over last 500 steps = 3.0422e-01, PNorm = 110.4674, GNorm = 0.2871
Meta loss on this task batch = 2.9361e-01, Meta loss averaged over last 500 steps = 3.0416e-01, PNorm = 110.4755, GNorm = 0.3002
Meta loss on this task batch = 3.0354e-01, Meta loss averaged over last 500 steps = 3.0409e-01, PNorm = 110.4839, GNorm = 0.2695
Meta loss on this task batch = 3.5379e-01, Meta loss averaged over last 500 steps = 3.0427e-01, PNorm = 110.4909, GNorm = 0.3398
Meta loss on this task batch = 2.6318e-01, Meta loss averaged over last 500 steps = 3.0426e-01, PNorm = 110.4980, GNorm = 0.3114
Meta loss on this task batch = 3.1265e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 110.5044, GNorm = 0.3044
Meta loss on this task batch = 2.8155e-01, Meta loss averaged over last 500 steps = 3.0413e-01, PNorm = 110.5103, GNorm = 0.2605
Meta loss on this task batch = 3.9345e-01, Meta loss averaged over last 500 steps = 3.0425e-01, PNorm = 110.5149, GNorm = 0.4921
Meta loss on this task batch = 2.9329e-01, Meta loss averaged over last 500 steps = 3.0439e-01, PNorm = 110.5207, GNorm = 0.2608
Meta loss on this task batch = 3.1149e-01, Meta loss averaged over last 500 steps = 3.0438e-01, PNorm = 110.5274, GNorm = 0.3024
Meta loss on this task batch = 2.9913e-01, Meta loss averaged over last 500 steps = 3.0431e-01, PNorm = 110.5340, GNorm = 0.2834
Meta loss on this task batch = 2.8817e-01, Meta loss averaged over last 500 steps = 3.0422e-01, PNorm = 110.5408, GNorm = 0.2728
Meta loss on this task batch = 3.0229e-01, Meta loss averaged over last 500 steps = 3.0415e-01, PNorm = 110.5477, GNorm = 0.3144
Meta loss on this task batch = 2.7615e-01, Meta loss averaged over last 500 steps = 3.0413e-01, PNorm = 110.5531, GNorm = 0.3044
Meta loss on this task batch = 3.2661e-01, Meta loss averaged over last 500 steps = 3.0419e-01, PNorm = 110.5581, GNorm = 0.3500
Meta loss on this task batch = 3.0398e-01, Meta loss averaged over last 500 steps = 3.0419e-01, PNorm = 110.5624, GNorm = 0.2929
Meta loss on this task batch = 3.1152e-01, Meta loss averaged over last 500 steps = 3.0420e-01, PNorm = 110.5676, GNorm = 0.3779
Meta loss on this task batch = 2.8445e-01, Meta loss averaged over last 500 steps = 3.0402e-01, PNorm = 110.5733, GNorm = 0.3234
Meta loss on this task batch = 2.8432e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 110.5779, GNorm = 0.3870
Took 254.78547382354736 seconds to complete one epoch of meta training
Took 262.03332114219666 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483512
Epoch 381
Meta loss on this task batch = 2.8437e-01, Meta loss averaged over last 500 steps = 3.0392e-01, PNorm = 110.5840, GNorm = 0.2986
Meta loss on this task batch = 2.8572e-01, Meta loss averaged over last 500 steps = 3.0391e-01, PNorm = 110.5923, GNorm = 0.3225
Meta loss on this task batch = 3.3125e-01, Meta loss averaged over last 500 steps = 3.0399e-01, PNorm = 110.6004, GNorm = 0.3496
Meta loss on this task batch = 2.9338e-01, Meta loss averaged over last 500 steps = 3.0398e-01, PNorm = 110.6094, GNorm = 0.2830
Meta loss on this task batch = 3.3811e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 110.6203, GNorm = 0.2994
Meta loss on this task batch = 3.7458e-01, Meta loss averaged over last 500 steps = 3.0406e-01, PNorm = 110.6288, GNorm = 0.3141
Meta loss on this task batch = 3.3605e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 110.6357, GNorm = 0.3310
Meta loss on this task batch = 2.4083e-01, Meta loss averaged over last 500 steps = 3.0392e-01, PNorm = 110.6431, GNorm = 0.2803
Meta loss on this task batch = 2.2203e-01, Meta loss averaged over last 500 steps = 3.0379e-01, PNorm = 110.6488, GNorm = 0.2749
Meta loss on this task batch = 3.0915e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 110.6544, GNorm = 0.3171
Meta loss on this task batch = 2.6206e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 110.6614, GNorm = 0.2917
Meta loss on this task batch = 2.8687e-01, Meta loss averaged over last 500 steps = 3.0371e-01, PNorm = 110.6705, GNorm = 0.2839
Meta loss on this task batch = 3.6248e-01, Meta loss averaged over last 500 steps = 3.0391e-01, PNorm = 110.6771, GNorm = 0.3244
Meta loss on this task batch = 3.1903e-01, Meta loss averaged over last 500 steps = 3.0407e-01, PNorm = 110.6826, GNorm = 0.2834
Meta loss on this task batch = 3.1805e-01, Meta loss averaged over last 500 steps = 3.0417e-01, PNorm = 110.6837, GNorm = 0.3499
Meta loss on this task batch = 3.4693e-01, Meta loss averaged over last 500 steps = 3.0422e-01, PNorm = 110.6809, GNorm = 0.3682
Meta loss on this task batch = 2.7139e-01, Meta loss averaged over last 500 steps = 3.0417e-01, PNorm = 110.6792, GNorm = 0.2902
Meta loss on this task batch = 2.7863e-01, Meta loss averaged over last 500 steps = 3.0409e-01, PNorm = 110.6773, GNorm = 0.2771
Meta loss on this task batch = 2.8140e-01, Meta loss averaged over last 500 steps = 3.0401e-01, PNorm = 110.6766, GNorm = 0.2879
Took 157.8163743019104 seconds to complete one epoch of meta training
Took 166.30977606773376 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504015
Epoch 382
Meta loss on this task batch = 2.7252e-01, Meta loss averaged over last 500 steps = 3.0390e-01, PNorm = 110.6773, GNorm = 0.2523
Meta loss on this task batch = 2.7383e-01, Meta loss averaged over last 500 steps = 3.0393e-01, PNorm = 110.6845, GNorm = 0.3105
Meta loss on this task batch = 3.6294e-01, Meta loss averaged over last 500 steps = 3.0396e-01, PNorm = 110.6911, GNorm = 0.3367
Meta loss on this task batch = 2.8867e-01, Meta loss averaged over last 500 steps = 3.0392e-01, PNorm = 110.7000, GNorm = 0.3162
Meta loss on this task batch = 3.4168e-01, Meta loss averaged over last 500 steps = 3.0392e-01, PNorm = 110.7105, GNorm = 0.3180
Meta loss on this task batch = 2.8494e-01, Meta loss averaged over last 500 steps = 3.0378e-01, PNorm = 110.7235, GNorm = 0.2834
Meta loss on this task batch = 2.4314e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 110.7354, GNorm = 0.2746
Meta loss on this task batch = 3.7482e-01, Meta loss averaged over last 500 steps = 3.0380e-01, PNorm = 110.7432, GNorm = 0.3787
Meta loss on this task batch = 3.2467e-01, Meta loss averaged over last 500 steps = 3.0389e-01, PNorm = 110.7517, GNorm = 0.3552
Meta loss on this task batch = 2.8271e-01, Meta loss averaged over last 500 steps = 3.0387e-01, PNorm = 110.7610, GNorm = 0.2724
Meta loss on this task batch = 3.0119e-01, Meta loss averaged over last 500 steps = 3.0386e-01, PNorm = 110.7687, GNorm = 0.3180
Meta loss on this task batch = 2.5359e-01, Meta loss averaged over last 500 steps = 3.0377e-01, PNorm = 110.7774, GNorm = 0.2672
Meta loss on this task batch = 3.0247e-01, Meta loss averaged over last 500 steps = 3.0369e-01, PNorm = 110.7829, GNorm = 0.3278
Meta loss on this task batch = 2.4863e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 110.7888, GNorm = 0.2906
Meta loss on this task batch = 3.2604e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 110.7963, GNorm = 0.3013
Meta loss on this task batch = 3.2081e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 110.8062, GNorm = 0.3251
Meta loss on this task batch = 3.3256e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 110.8168, GNorm = 0.3153
Meta loss on this task batch = 3.6032e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 110.8254, GNorm = 0.3380
Meta loss on this task batch = 2.6214e-01, Meta loss averaged over last 500 steps = 3.0358e-01, PNorm = 110.8366, GNorm = 0.3444
Took 135.6672604084015 seconds to complete one epoch of meta training
Took 143.10167980194092 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480525
Epoch 383
Meta loss on this task batch = 2.6729e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 110.8488, GNorm = 0.2764
Meta loss on this task batch = 2.8500e-01, Meta loss averaged over last 500 steps = 3.0351e-01, PNorm = 110.8594, GNorm = 0.2864
Meta loss on this task batch = 2.9227e-01, Meta loss averaged over last 500 steps = 3.0347e-01, PNorm = 110.8682, GNorm = 0.2967
Meta loss on this task batch = 3.2749e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 110.8761, GNorm = 0.3247
Meta loss on this task batch = 2.3436e-01, Meta loss averaged over last 500 steps = 3.0346e-01, PNorm = 110.8836, GNorm = 0.2491
Meta loss on this task batch = 2.3928e-01, Meta loss averaged over last 500 steps = 3.0333e-01, PNorm = 110.8884, GNorm = 0.2712
Meta loss on this task batch = 3.4894e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 110.8925, GNorm = 0.3068
Meta loss on this task batch = 3.1754e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 110.8941, GNorm = 0.3073
Meta loss on this task batch = 3.0347e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 110.8942, GNorm = 0.3067
Meta loss on this task batch = 2.7697e-01, Meta loss averaged over last 500 steps = 3.0338e-01, PNorm = 110.8929, GNorm = 0.3469
Meta loss on this task batch = 3.5730e-01, Meta loss averaged over last 500 steps = 3.0346e-01, PNorm = 110.8923, GNorm = 0.2989
Meta loss on this task batch = 3.1167e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 110.8934, GNorm = 0.2947
Meta loss on this task batch = 3.0282e-01, Meta loss averaged over last 500 steps = 3.0355e-01, PNorm = 110.8985, GNorm = 0.2969
Meta loss on this task batch = 2.8486e-01, Meta loss averaged over last 500 steps = 3.0355e-01, PNorm = 110.9031, GNorm = 0.3047
Meta loss on this task batch = 2.9945e-01, Meta loss averaged over last 500 steps = 3.0364e-01, PNorm = 110.9080, GNorm = 0.2877
Meta loss on this task batch = 3.2330e-01, Meta loss averaged over last 500 steps = 3.0364e-01, PNorm = 110.9112, GNorm = 0.3891
Meta loss on this task batch = 3.4694e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 110.9159, GNorm = 0.3202
Meta loss on this task batch = 2.5866e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 110.9220, GNorm = 0.2987
Meta loss on this task batch = 3.5130e-01, Meta loss averaged over last 500 steps = 3.0375e-01, PNorm = 110.9285, GNorm = 0.4019
Took 135.48365592956543 seconds to complete one epoch of meta training
Took 143.7544105052948 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486482
Epoch 384
Meta loss on this task batch = 2.9371e-01, Meta loss averaged over last 500 steps = 3.0375e-01, PNorm = 110.9362, GNorm = 0.3037
Meta loss on this task batch = 2.7313e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 110.9442, GNorm = 0.2570
Meta loss on this task batch = 2.6962e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 110.9530, GNorm = 0.2538
Meta loss on this task batch = 3.2013e-01, Meta loss averaged over last 500 steps = 3.0352e-01, PNorm = 110.9626, GNorm = 0.2972
Meta loss on this task batch = 2.8746e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 110.9679, GNorm = 0.2630
Meta loss on this task batch = 3.3969e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 110.9708, GNorm = 0.3328
Meta loss on this task batch = 3.6518e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 110.9727, GNorm = 0.3060
Meta loss on this task batch = 2.6779e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 110.9777, GNorm = 0.2664
Meta loss on this task batch = 2.8526e-01, Meta loss averaged over last 500 steps = 3.0338e-01, PNorm = 110.9827, GNorm = 0.3192
Meta loss on this task batch = 3.2741e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 110.9846, GNorm = 0.3456
Meta loss on this task batch = 2.8148e-01, Meta loss averaged over last 500 steps = 3.0343e-01, PNorm = 110.9872, GNorm = 0.2849
Meta loss on this task batch = 2.7633e-01, Meta loss averaged over last 500 steps = 3.0337e-01, PNorm = 110.9912, GNorm = 0.2649
Meta loss on this task batch = 3.0976e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 110.9990, GNorm = 0.2816
Meta loss on this task batch = 2.6499e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 111.0081, GNorm = 0.2744
Meta loss on this task batch = 3.6889e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 111.0158, GNorm = 0.3489
Meta loss on this task batch = 3.0544e-01, Meta loss averaged over last 500 steps = 3.0363e-01, PNorm = 111.0250, GNorm = 0.3068
Meta loss on this task batch = 2.7457e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 111.0349, GNorm = 0.3087
Meta loss on this task batch = 2.8536e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 111.0467, GNorm = 0.3167
Meta loss on this task batch = 2.8673e-01, Meta loss averaged over last 500 steps = 3.0358e-01, PNorm = 111.0578, GNorm = 0.3364
Took 137.10331797599792 seconds to complete one epoch of meta training
Took 145.93518567085266 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500308
Epoch 385
Meta loss on this task batch = 3.5094e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 111.0661, GNorm = 0.3035
Meta loss on this task batch = 3.5005e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 111.0674, GNorm = 0.3469
Meta loss on this task batch = 2.9443e-01, Meta loss averaged over last 500 steps = 3.0364e-01, PNorm = 111.0684, GNorm = 0.2669
Meta loss on this task batch = 2.7541e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 111.0685, GNorm = 0.2833
Meta loss on this task batch = 2.5459e-01, Meta loss averaged over last 500 steps = 3.0338e-01, PNorm = 111.0692, GNorm = 0.2703
Meta loss on this task batch = 2.8931e-01, Meta loss averaged over last 500 steps = 3.0349e-01, PNorm = 111.0663, GNorm = 0.3059
Meta loss on this task batch = 2.7395e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 111.0610, GNorm = 0.2688
Meta loss on this task batch = 2.7270e-01, Meta loss averaged over last 500 steps = 3.0329e-01, PNorm = 111.0586, GNorm = 0.2663
Meta loss on this task batch = 3.0049e-01, Meta loss averaged over last 500 steps = 3.0325e-01, PNorm = 111.0589, GNorm = 0.2861
Meta loss on this task batch = 3.3733e-01, Meta loss averaged over last 500 steps = 3.0335e-01, PNorm = 111.0611, GNorm = 0.3027
Meta loss on this task batch = 2.9842e-01, Meta loss averaged over last 500 steps = 3.0326e-01, PNorm = 111.0647, GNorm = 0.3146
Meta loss on this task batch = 3.5299e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 111.0683, GNorm = 0.3190
Meta loss on this task batch = 3.2053e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 111.0730, GNorm = 0.3186
Meta loss on this task batch = 2.5469e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 111.0798, GNorm = 0.2919
Meta loss on this task batch = 2.9796e-01, Meta loss averaged over last 500 steps = 3.0332e-01, PNorm = 111.0879, GNorm = 0.2852
Meta loss on this task batch = 2.9348e-01, Meta loss averaged over last 500 steps = 3.0327e-01, PNorm = 111.0961, GNorm = 0.3702
Meta loss on this task batch = 3.4205e-01, Meta loss averaged over last 500 steps = 3.0335e-01, PNorm = 111.1017, GNorm = 0.3316
Meta loss on this task batch = 2.6435e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 111.1081, GNorm = 0.3421
Meta loss on this task batch = 3.4757e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 111.1133, GNorm = 0.3955
Took 135.28753852844238 seconds to complete one epoch of meta training
Took 143.99741792678833 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467527
Epoch 386
Meta loss on this task batch = 2.9556e-01, Meta loss averaged over last 500 steps = 3.0338e-01, PNorm = 111.1173, GNorm = 0.3288
Meta loss on this task batch = 3.6568e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 111.1182, GNorm = 0.3405
Meta loss on this task batch = 3.2184e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 111.1165, GNorm = 0.3486
Meta loss on this task batch = 2.7809e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 111.1147, GNorm = 0.2776
Meta loss on this task batch = 2.2530e-01, Meta loss averaged over last 500 steps = 3.0329e-01, PNorm = 111.1148, GNorm = 0.2229
Meta loss on this task batch = 3.1854e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 111.1180, GNorm = 0.3141
Meta loss on this task batch = 3.0590e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 111.1218, GNorm = 0.2718
Meta loss on this task batch = 3.0084e-01, Meta loss averaged over last 500 steps = 3.0353e-01, PNorm = 111.1269, GNorm = 0.3301
Meta loss on this task batch = 2.8793e-01, Meta loss averaged over last 500 steps = 3.0351e-01, PNorm = 111.1303, GNorm = 0.2496
Meta loss on this task batch = 3.4693e-01, Meta loss averaged over last 500 steps = 3.0360e-01, PNorm = 111.1320, GNorm = 0.3638
Meta loss on this task batch = 3.0029e-01, Meta loss averaged over last 500 steps = 3.0361e-01, PNorm = 111.1351, GNorm = 0.3023
Meta loss on this task batch = 2.5408e-01, Meta loss averaged over last 500 steps = 3.0339e-01, PNorm = 111.1400, GNorm = 0.2627
Meta loss on this task batch = 3.0398e-01, Meta loss averaged over last 500 steps = 3.0351e-01, PNorm = 111.1436, GNorm = 0.3049
Meta loss on this task batch = 2.7523e-01, Meta loss averaged over last 500 steps = 3.0333e-01, PNorm = 111.1485, GNorm = 0.2701
Meta loss on this task batch = 3.7747e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 111.1519, GNorm = 0.3106
Meta loss on this task batch = 2.6793e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 111.1558, GNorm = 0.2732
Meta loss on this task batch = 3.0093e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 111.1622, GNorm = 0.2893
Meta loss on this task batch = 2.7412e-01, Meta loss averaged over last 500 steps = 3.0337e-01, PNorm = 111.1681, GNorm = 0.2583
Meta loss on this task batch = 3.0788e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 111.1713, GNorm = 0.3070
Took 134.1301770210266 seconds to complete one epoch of meta training
Took 142.24360632896423 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482551
Epoch 387
Meta loss on this task batch = 3.1719e-01, Meta loss averaged over last 500 steps = 3.0344e-01, PNorm = 111.1721, GNorm = 0.3162
Meta loss on this task batch = 2.6797e-01, Meta loss averaged over last 500 steps = 3.0323e-01, PNorm = 111.1725, GNorm = 0.2802
Meta loss on this task batch = 3.0954e-01, Meta loss averaged over last 500 steps = 3.0329e-01, PNorm = 111.1711, GNorm = 0.2915
Meta loss on this task batch = 3.4595e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 111.1703, GNorm = 0.3243
Meta loss on this task batch = 2.6731e-01, Meta loss averaged over last 500 steps = 3.0328e-01, PNorm = 111.1674, GNorm = 0.2727
Meta loss on this task batch = 2.7410e-01, Meta loss averaged over last 500 steps = 3.0312e-01, PNorm = 111.1640, GNorm = 0.2676
Meta loss on this task batch = 2.8952e-01, Meta loss averaged over last 500 steps = 3.0309e-01, PNorm = 111.1632, GNorm = 0.2794
Meta loss on this task batch = 3.0554e-01, Meta loss averaged over last 500 steps = 3.0309e-01, PNorm = 111.1639, GNorm = 0.2507
Meta loss on this task batch = 3.3110e-01, Meta loss averaged over last 500 steps = 3.0316e-01, PNorm = 111.1655, GNorm = 0.2940
Meta loss on this task batch = 2.6049e-01, Meta loss averaged over last 500 steps = 3.0304e-01, PNorm = 111.1710, GNorm = 0.2884
Meta loss on this task batch = 2.7739e-01, Meta loss averaged over last 500 steps = 3.0283e-01, PNorm = 111.1797, GNorm = 0.3158
Meta loss on this task batch = 3.4197e-01, Meta loss averaged over last 500 steps = 3.0286e-01, PNorm = 111.1874, GNorm = 0.3485
Meta loss on this task batch = 2.7679e-01, Meta loss averaged over last 500 steps = 3.0286e-01, PNorm = 111.1975, GNorm = 0.3079
Meta loss on this task batch = 3.1510e-01, Meta loss averaged over last 500 steps = 3.0306e-01, PNorm = 111.2068, GNorm = 0.2971
Meta loss on this task batch = 2.4664e-01, Meta loss averaged over last 500 steps = 3.0298e-01, PNorm = 111.2106, GNorm = 0.4471
Meta loss on this task batch = 3.4445e-01, Meta loss averaged over last 500 steps = 3.0305e-01, PNorm = 111.2107, GNorm = 0.3489
Meta loss on this task batch = 2.7800e-01, Meta loss averaged over last 500 steps = 3.0303e-01, PNorm = 111.2108, GNorm = 0.3149
Meta loss on this task batch = 2.8539e-01, Meta loss averaged over last 500 steps = 3.0301e-01, PNorm = 111.2121, GNorm = 0.3337
Meta loss on this task batch = 3.3689e-01, Meta loss averaged over last 500 steps = 3.0307e-01, PNorm = 111.2126, GNorm = 0.3892
Took 134.71381521224976 seconds to complete one epoch of meta training
Took 143.3556821346283 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498207
Epoch 388
Meta loss on this task batch = 2.9059e-01, Meta loss averaged over last 500 steps = 3.0304e-01, PNorm = 111.2130, GNorm = 0.3149
Meta loss on this task batch = 2.4916e-01, Meta loss averaged over last 500 steps = 3.0293e-01, PNorm = 111.2152, GNorm = 0.2855
Meta loss on this task batch = 2.6215e-01, Meta loss averaged over last 500 steps = 3.0290e-01, PNorm = 111.2173, GNorm = 0.2657
Meta loss on this task batch = 3.0566e-01, Meta loss averaged over last 500 steps = 3.0280e-01, PNorm = 111.2209, GNorm = 0.3216
Meta loss on this task batch = 2.9765e-01, Meta loss averaged over last 500 steps = 3.0273e-01, PNorm = 111.2252, GNorm = 0.2820
Meta loss on this task batch = 3.5640e-01, Meta loss averaged over last 500 steps = 3.0271e-01, PNorm = 111.2286, GNorm = 0.3367
Meta loss on this task batch = 2.5225e-01, Meta loss averaged over last 500 steps = 3.0262e-01, PNorm = 111.2344, GNorm = 0.2767
Meta loss on this task batch = 3.2084e-01, Meta loss averaged over last 500 steps = 3.0267e-01, PNorm = 111.2408, GNorm = 0.2876
Meta loss on this task batch = 3.1321e-01, Meta loss averaged over last 500 steps = 3.0261e-01, PNorm = 111.2495, GNorm = 0.2897
Meta loss on this task batch = 2.8234e-01, Meta loss averaged over last 500 steps = 3.0256e-01, PNorm = 111.2590, GNorm = 0.2538
Meta loss on this task batch = 3.3615e-01, Meta loss averaged over last 500 steps = 3.0265e-01, PNorm = 111.2686, GNorm = 0.2773
Meta loss on this task batch = 2.6605e-01, Meta loss averaged over last 500 steps = 3.0265e-01, PNorm = 111.2783, GNorm = 0.3090
Meta loss on this task batch = 3.5861e-01, Meta loss averaged over last 500 steps = 3.0268e-01, PNorm = 111.2880, GNorm = 0.2841
Meta loss on this task batch = 3.0005e-01, Meta loss averaged over last 500 steps = 3.0273e-01, PNorm = 111.2970, GNorm = 0.2623
Meta loss on this task batch = 3.1219e-01, Meta loss averaged over last 500 steps = 3.0271e-01, PNorm = 111.3055, GNorm = 0.3239
Meta loss on this task batch = 2.7496e-01, Meta loss averaged over last 500 steps = 3.0273e-01, PNorm = 111.3139, GNorm = 0.2744
Meta loss on this task batch = 2.5345e-01, Meta loss averaged over last 500 steps = 3.0258e-01, PNorm = 111.3234, GNorm = 0.2532
Meta loss on this task batch = 2.6368e-01, Meta loss averaged over last 500 steps = 3.0237e-01, PNorm = 111.3341, GNorm = 0.2821
Meta loss on this task batch = 3.5034e-01, Meta loss averaged over last 500 steps = 3.0257e-01, PNorm = 111.3433, GNorm = 0.3592
Took 134.99853205680847 seconds to complete one epoch of meta training
Took 143.49081993103027 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499966
Epoch 389
Meta loss on this task batch = 3.3190e-01, Meta loss averaged over last 500 steps = 3.0249e-01, PNorm = 111.3511, GNorm = 0.2957
Meta loss on this task batch = 3.3638e-01, Meta loss averaged over last 500 steps = 3.0268e-01, PNorm = 111.3600, GNorm = 0.2884
Meta loss on this task batch = 2.7803e-01, Meta loss averaged over last 500 steps = 3.0266e-01, PNorm = 111.3677, GNorm = 0.3009
Meta loss on this task batch = 2.7574e-01, Meta loss averaged over last 500 steps = 3.0255e-01, PNorm = 111.3750, GNorm = 0.2841
Meta loss on this task batch = 2.6390e-01, Meta loss averaged over last 500 steps = 3.0246e-01, PNorm = 111.3832, GNorm = 0.2799
Meta loss on this task batch = 2.8861e-01, Meta loss averaged over last 500 steps = 3.0243e-01, PNorm = 111.3926, GNorm = 0.2771
Meta loss on this task batch = 2.4588e-01, Meta loss averaged over last 500 steps = 3.0230e-01, PNorm = 111.4019, GNorm = 0.2615
Meta loss on this task batch = 3.5281e-01, Meta loss averaged over last 500 steps = 3.0238e-01, PNorm = 111.4097, GNorm = 0.3109
Meta loss on this task batch = 2.8110e-01, Meta loss averaged over last 500 steps = 3.0236e-01, PNorm = 111.4173, GNorm = 0.2984
Meta loss on this task batch = 2.9168e-01, Meta loss averaged over last 500 steps = 3.0231e-01, PNorm = 111.4244, GNorm = 0.3007
Meta loss on this task batch = 2.6844e-01, Meta loss averaged over last 500 steps = 3.0216e-01, PNorm = 111.4277, GNorm = 0.3072
Meta loss on this task batch = 3.5664e-01, Meta loss averaged over last 500 steps = 3.0227e-01, PNorm = 111.4302, GNorm = 0.3270
Meta loss on this task batch = 3.1923e-01, Meta loss averaged over last 500 steps = 3.0230e-01, PNorm = 111.4318, GNorm = 0.2976
Meta loss on this task batch = 2.8252e-01, Meta loss averaged over last 500 steps = 3.0224e-01, PNorm = 111.4328, GNorm = 0.2465
Meta loss on this task batch = 3.1261e-01, Meta loss averaged over last 500 steps = 3.0233e-01, PNorm = 111.4346, GNorm = 0.2957
Meta loss on this task batch = 2.6594e-01, Meta loss averaged over last 500 steps = 3.0228e-01, PNorm = 111.4366, GNorm = 0.2693
Meta loss on this task batch = 2.8636e-01, Meta loss averaged over last 500 steps = 3.0223e-01, PNorm = 111.4393, GNorm = 0.2597
Meta loss on this task batch = 3.5936e-01, Meta loss averaged over last 500 steps = 3.0229e-01, PNorm = 111.4413, GNorm = 0.2984
Meta loss on this task batch = 3.3829e-01, Meta loss averaged over last 500 steps = 3.0244e-01, PNorm = 111.4444, GNorm = 0.3636
Took 132.68304777145386 seconds to complete one epoch of meta training
Took 141.55367279052734 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482050
Epoch 390
Meta loss on this task batch = 3.0154e-01, Meta loss averaged over last 500 steps = 3.0243e-01, PNorm = 111.4499, GNorm = 0.2892
Meta loss on this task batch = 3.0219e-01, Meta loss averaged over last 500 steps = 3.0239e-01, PNorm = 111.4553, GNorm = 0.2833
Meta loss on this task batch = 3.1044e-01, Meta loss averaged over last 500 steps = 3.0240e-01, PNorm = 111.4610, GNorm = 0.2809
Meta loss on this task batch = 2.3924e-01, Meta loss averaged over last 500 steps = 3.0228e-01, PNorm = 111.4669, GNorm = 0.2352
Meta loss on this task batch = 2.6547e-01, Meta loss averaged over last 500 steps = 3.0222e-01, PNorm = 111.4725, GNorm = 0.2768
Meta loss on this task batch = 3.3148e-01, Meta loss averaged over last 500 steps = 3.0234e-01, PNorm = 111.4749, GNorm = 0.3561
Meta loss on this task batch = 2.6321e-01, Meta loss averaged over last 500 steps = 3.0229e-01, PNorm = 111.4774, GNorm = 0.2955
Meta loss on this task batch = 2.9028e-01, Meta loss averaged over last 500 steps = 3.0230e-01, PNorm = 111.4812, GNorm = 0.2623
Meta loss on this task batch = 2.9077e-01, Meta loss averaged over last 500 steps = 3.0220e-01, PNorm = 111.4860, GNorm = 0.2905
Meta loss on this task batch = 2.4023e-01, Meta loss averaged over last 500 steps = 3.0210e-01, PNorm = 111.4911, GNorm = 0.2772
Meta loss on this task batch = 3.6584e-01, Meta loss averaged over last 500 steps = 3.0226e-01, PNorm = 111.4945, GNorm = 0.3359
Meta loss on this task batch = 2.6664e-01, Meta loss averaged over last 500 steps = 3.0225e-01, PNorm = 111.4991, GNorm = 0.2797
Meta loss on this task batch = 3.5487e-01, Meta loss averaged over last 500 steps = 3.0237e-01, PNorm = 111.5058, GNorm = 0.3400
Meta loss on this task batch = 3.1270e-01, Meta loss averaged over last 500 steps = 3.0239e-01, PNorm = 111.5156, GNorm = 0.2992
Meta loss on this task batch = 2.9754e-01, Meta loss averaged over last 500 steps = 3.0234e-01, PNorm = 111.5258, GNorm = 0.3233
Meta loss on this task batch = 2.7707e-01, Meta loss averaged over last 500 steps = 3.0230e-01, PNorm = 111.5367, GNorm = 0.2756
Meta loss on this task batch = 3.2087e-01, Meta loss averaged over last 500 steps = 3.0233e-01, PNorm = 111.5479, GNorm = 0.3203
Meta loss on this task batch = 3.2981e-01, Meta loss averaged over last 500 steps = 3.0240e-01, PNorm = 111.5602, GNorm = 0.2803
Meta loss on this task batch = 2.7385e-01, Meta loss averaged over last 500 steps = 3.0228e-01, PNorm = 111.5718, GNorm = 0.3215
Took 133.6487045288086 seconds to complete one epoch of meta training
Took 142.07941889762878 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463788
Epoch 391
Meta loss on this task batch = 3.5351e-01, Meta loss averaged over last 500 steps = 3.0230e-01, PNorm = 111.5799, GNorm = 0.3482
Meta loss on this task batch = 2.5496e-01, Meta loss averaged over last 500 steps = 3.0213e-01, PNorm = 111.5886, GNorm = 0.2531
Meta loss on this task batch = 3.3951e-01, Meta loss averaged over last 500 steps = 3.0226e-01, PNorm = 111.5933, GNorm = 0.3624
Meta loss on this task batch = 3.7082e-01, Meta loss averaged over last 500 steps = 3.0244e-01, PNorm = 111.5941, GNorm = 0.3505
Meta loss on this task batch = 2.8545e-01, Meta loss averaged over last 500 steps = 3.0226e-01, PNorm = 111.5950, GNorm = 0.2897
Meta loss on this task batch = 2.3243e-01, Meta loss averaged over last 500 steps = 3.0217e-01, PNorm = 111.5992, GNorm = 0.2703
Meta loss on this task batch = 2.6319e-01, Meta loss averaged over last 500 steps = 3.0218e-01, PNorm = 111.6047, GNorm = 0.2705
Meta loss on this task batch = 2.6072e-01, Meta loss averaged over last 500 steps = 3.0204e-01, PNorm = 111.6097, GNorm = 0.2492
Meta loss on this task batch = 3.0833e-01, Meta loss averaged over last 500 steps = 3.0190e-01, PNorm = 111.6149, GNorm = 0.2708
Meta loss on this task batch = 2.4728e-01, Meta loss averaged over last 500 steps = 3.0192e-01, PNorm = 111.6209, GNorm = 0.2719
Meta loss on this task batch = 3.2481e-01, Meta loss averaged over last 500 steps = 3.0193e-01, PNorm = 111.6264, GNorm = 0.2861
Meta loss on this task batch = 3.2448e-01, Meta loss averaged over last 500 steps = 3.0195e-01, PNorm = 111.6306, GNorm = 0.2793
Meta loss on this task batch = 2.7851e-01, Meta loss averaged over last 500 steps = 3.0197e-01, PNorm = 111.6323, GNorm = 0.2774
Meta loss on this task batch = 2.5583e-01, Meta loss averaged over last 500 steps = 3.0191e-01, PNorm = 111.6347, GNorm = 0.2732
Meta loss on this task batch = 2.4269e-01, Meta loss averaged over last 500 steps = 3.0165e-01, PNorm = 111.6386, GNorm = 0.2949
Meta loss on this task batch = 3.3966e-01, Meta loss averaged over last 500 steps = 3.0168e-01, PNorm = 111.6398, GNorm = 0.3137
Meta loss on this task batch = 3.4241e-01, Meta loss averaged over last 500 steps = 3.0172e-01, PNorm = 111.6385, GNorm = 0.3255
Meta loss on this task batch = 3.1641e-01, Meta loss averaged over last 500 steps = 3.0182e-01, PNorm = 111.6392, GNorm = 0.3443
Meta loss on this task batch = 3.1866e-01, Meta loss averaged over last 500 steps = 3.0171e-01, PNorm = 111.6402, GNorm = 0.3554
Took 133.20575976371765 seconds to complete one epoch of meta training
Took 141.90364503860474 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501397
Epoch 392
Meta loss on this task batch = 3.3921e-01, Meta loss averaged over last 500 steps = 3.0183e-01, PNorm = 111.6437, GNorm = 0.3644
Meta loss on this task batch = 2.8257e-01, Meta loss averaged over last 500 steps = 3.0171e-01, PNorm = 111.6493, GNorm = 0.2707
Meta loss on this task batch = 2.8171e-01, Meta loss averaged over last 500 steps = 3.0165e-01, PNorm = 111.6549, GNorm = 0.2766
Meta loss on this task batch = 2.7372e-01, Meta loss averaged over last 500 steps = 3.0163e-01, PNorm = 111.6586, GNorm = 0.2984
Meta loss on this task batch = 2.8978e-01, Meta loss averaged over last 500 steps = 3.0167e-01, PNorm = 111.6642, GNorm = 0.3216
Meta loss on this task batch = 2.3799e-01, Meta loss averaged over last 500 steps = 3.0165e-01, PNorm = 111.6725, GNorm = 0.2553
Meta loss on this task batch = 3.6402e-01, Meta loss averaged over last 500 steps = 3.0174e-01, PNorm = 111.6801, GNorm = 0.2939
Meta loss on this task batch = 3.3867e-01, Meta loss averaged over last 500 steps = 3.0183e-01, PNorm = 111.6844, GNorm = 0.2890
Meta loss on this task batch = 3.4617e-01, Meta loss averaged over last 500 steps = 3.0187e-01, PNorm = 111.6890, GNorm = 0.3639
Meta loss on this task batch = 3.1666e-01, Meta loss averaged over last 500 steps = 3.0189e-01, PNorm = 111.6912, GNorm = 0.3453
Meta loss on this task batch = 3.0858e-01, Meta loss averaged over last 500 steps = 3.0194e-01, PNorm = 111.6938, GNorm = 0.3033
Meta loss on this task batch = 3.1626e-01, Meta loss averaged over last 500 steps = 3.0207e-01, PNorm = 111.6959, GNorm = 0.3277
Meta loss on this task batch = 2.7387e-01, Meta loss averaged over last 500 steps = 3.0193e-01, PNorm = 111.6968, GNorm = 0.2735
Meta loss on this task batch = 2.6381e-01, Meta loss averaged over last 500 steps = 3.0195e-01, PNorm = 111.7003, GNorm = 0.2644
Meta loss on this task batch = 3.1605e-01, Meta loss averaged over last 500 steps = 3.0193e-01, PNorm = 111.7031, GNorm = 0.3356
Meta loss on this task batch = 3.3931e-01, Meta loss averaged over last 500 steps = 3.0199e-01, PNorm = 111.7046, GNorm = 0.2880
Meta loss on this task batch = 2.6352e-01, Meta loss averaged over last 500 steps = 3.0205e-01, PNorm = 111.7070, GNorm = 0.2504
Meta loss on this task batch = 2.9609e-01, Meta loss averaged over last 500 steps = 3.0193e-01, PNorm = 111.7090, GNorm = 0.2742
Meta loss on this task batch = 2.9499e-01, Meta loss averaged over last 500 steps = 3.0196e-01, PNorm = 111.7119, GNorm = 0.3169
Took 131.58232426643372 seconds to complete one epoch of meta training
Took 140.10117959976196 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478274
Epoch 393
Meta loss on this task batch = 2.8629e-01, Meta loss averaged over last 500 steps = 3.0185e-01, PNorm = 111.7154, GNorm = 0.2652
Meta loss on this task batch = 3.0071e-01, Meta loss averaged over last 500 steps = 3.0189e-01, PNorm = 111.7184, GNorm = 0.3557
Meta loss on this task batch = 3.1342e-01, Meta loss averaged over last 500 steps = 3.0195e-01, PNorm = 111.7201, GNorm = 0.3143
Meta loss on this task batch = 3.0697e-01, Meta loss averaged over last 500 steps = 3.0195e-01, PNorm = 111.7234, GNorm = 0.2687
Meta loss on this task batch = 3.4009e-01, Meta loss averaged over last 500 steps = 3.0201e-01, PNorm = 111.7285, GNorm = 0.2825
Meta loss on this task batch = 2.8078e-01, Meta loss averaged over last 500 steps = 3.0200e-01, PNorm = 111.7347, GNorm = 0.2838
Meta loss on this task batch = 2.4963e-01, Meta loss averaged over last 500 steps = 3.0190e-01, PNorm = 111.7427, GNorm = 0.2390
Meta loss on this task batch = 3.2088e-01, Meta loss averaged over last 500 steps = 3.0199e-01, PNorm = 111.7521, GNorm = 0.2926
Meta loss on this task batch = 2.8763e-01, Meta loss averaged over last 500 steps = 3.0205e-01, PNorm = 111.7634, GNorm = 0.2682
Meta loss on this task batch = 3.2938e-01, Meta loss averaged over last 500 steps = 3.0205e-01, PNorm = 111.7744, GNorm = 0.2763
Meta loss on this task batch = 2.8180e-01, Meta loss averaged over last 500 steps = 3.0192e-01, PNorm = 111.7856, GNorm = 0.3288
Meta loss on this task batch = 2.6164e-01, Meta loss averaged over last 500 steps = 3.0192e-01, PNorm = 111.7957, GNorm = 0.2437
Meta loss on this task batch = 3.1173e-01, Meta loss averaged over last 500 steps = 3.0197e-01, PNorm = 111.8041, GNorm = 0.3404
Meta loss on this task batch = 2.1595e-01, Meta loss averaged over last 500 steps = 3.0186e-01, PNorm = 111.8086, GNorm = 0.2513
Meta loss on this task batch = 3.2725e-01, Meta loss averaged over last 500 steps = 3.0195e-01, PNorm = 111.8121, GNorm = 0.3095
Meta loss on this task batch = 3.0807e-01, Meta loss averaged over last 500 steps = 3.0183e-01, PNorm = 111.8096, GNorm = 0.3917
Meta loss on this task batch = 3.5686e-01, Meta loss averaged over last 500 steps = 3.0188e-01, PNorm = 111.8045, GNorm = 0.3435
Meta loss on this task batch = 3.0953e-01, Meta loss averaged over last 500 steps = 3.0180e-01, PNorm = 111.8023, GNorm = 0.3323
Meta loss on this task batch = 2.3841e-01, Meta loss averaged over last 500 steps = 3.0160e-01, PNorm = 111.8035, GNorm = 0.2914
Took 135.60769629478455 seconds to complete one epoch of meta training
Took 144.14339303970337 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505241
Epoch 394
Meta loss on this task batch = 3.4006e-01, Meta loss averaged over last 500 steps = 3.0172e-01, PNorm = 111.8075, GNorm = 0.3095
Meta loss on this task batch = 2.4360e-01, Meta loss averaged over last 500 steps = 3.0161e-01, PNorm = 111.8145, GNorm = 0.2688
Meta loss on this task batch = 2.9148e-01, Meta loss averaged over last 500 steps = 3.0154e-01, PNorm = 111.8251, GNorm = 0.2859
Meta loss on this task batch = 2.7217e-01, Meta loss averaged over last 500 steps = 3.0139e-01, PNorm = 111.8357, GNorm = 0.2598
Meta loss on this task batch = 2.9213e-01, Meta loss averaged over last 500 steps = 3.0143e-01, PNorm = 111.8448, GNorm = 0.2733
Meta loss on this task batch = 3.9749e-01, Meta loss averaged over last 500 steps = 3.0171e-01, PNorm = 111.8524, GNorm = 0.3378
Meta loss on this task batch = 3.1429e-01, Meta loss averaged over last 500 steps = 3.0166e-01, PNorm = 111.8576, GNorm = 0.3350
Meta loss on this task batch = 2.7048e-01, Meta loss averaged over last 500 steps = 3.0150e-01, PNorm = 111.8595, GNorm = 0.2906
Meta loss on this task batch = 2.9769e-01, Meta loss averaged over last 500 steps = 3.0147e-01, PNorm = 111.8602, GNorm = 0.3047
Meta loss on this task batch = 2.8317e-01, Meta loss averaged over last 500 steps = 3.0131e-01, PNorm = 111.8610, GNorm = 0.2683
Meta loss on this task batch = 3.1068e-01, Meta loss averaged over last 500 steps = 3.0135e-01, PNorm = 111.8628, GNorm = 0.3139
Meta loss on this task batch = 2.2894e-01, Meta loss averaged over last 500 steps = 3.0113e-01, PNorm = 111.8619, GNorm = 0.2597
Meta loss on this task batch = 3.3052e-01, Meta loss averaged over last 500 steps = 3.0121e-01, PNorm = 111.8609, GNorm = 0.2748
Meta loss on this task batch = 3.3929e-01, Meta loss averaged over last 500 steps = 3.0125e-01, PNorm = 111.8629, GNorm = 0.3549
Meta loss on this task batch = 2.5252e-01, Meta loss averaged over last 500 steps = 3.0115e-01, PNorm = 111.8680, GNorm = 0.2914
Meta loss on this task batch = 2.8316e-01, Meta loss averaged over last 500 steps = 3.0106e-01, PNorm = 111.8732, GNorm = 0.2753
Meta loss on this task batch = 3.0600e-01, Meta loss averaged over last 500 steps = 3.0114e-01, PNorm = 111.8807, GNorm = 0.2950
Meta loss on this task batch = 3.3096e-01, Meta loss averaged over last 500 steps = 3.0120e-01, PNorm = 111.8901, GNorm = 0.3782
Meta loss on this task batch = 2.9267e-01, Meta loss averaged over last 500 steps = 3.0136e-01, PNorm = 111.9026, GNorm = 0.3414
Took 132.85742282867432 seconds to complete one epoch of meta training
Took 141.21168279647827 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492061
Epoch 395
Meta loss on this task batch = 2.2580e-01, Meta loss averaged over last 500 steps = 3.0119e-01, PNorm = 111.9133, GNorm = 0.2705
Meta loss on this task batch = 2.6833e-01, Meta loss averaged over last 500 steps = 3.0122e-01, PNorm = 111.9232, GNorm = 0.3290
Meta loss on this task batch = 3.0145e-01, Meta loss averaged over last 500 steps = 3.0131e-01, PNorm = 111.9302, GNorm = 0.3180
Meta loss on this task batch = 2.8694e-01, Meta loss averaged over last 500 steps = 3.0131e-01, PNorm = 111.9357, GNorm = 0.2780
Meta loss on this task batch = 2.9161e-01, Meta loss averaged over last 500 steps = 3.0130e-01, PNorm = 111.9414, GNorm = 0.3152
Meta loss on this task batch = 3.3369e-01, Meta loss averaged over last 500 steps = 3.0133e-01, PNorm = 111.9442, GNorm = 0.3188
Meta loss on this task batch = 2.7990e-01, Meta loss averaged over last 500 steps = 3.0134e-01, PNorm = 111.9452, GNorm = 0.3008
Meta loss on this task batch = 3.5779e-01, Meta loss averaged over last 500 steps = 3.0141e-01, PNorm = 111.9470, GNorm = 0.3354
Meta loss on this task batch = 3.2313e-01, Meta loss averaged over last 500 steps = 3.0153e-01, PNorm = 111.9454, GNorm = 0.3097
Meta loss on this task batch = 2.7054e-01, Meta loss averaged over last 500 steps = 3.0146e-01, PNorm = 111.9443, GNorm = 0.2778
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 3.0147e-01, PNorm = 111.9435, GNorm = 0.2817
Meta loss on this task batch = 3.1877e-01, Meta loss averaged over last 500 steps = 3.0160e-01, PNorm = 111.9438, GNorm = 0.2929
Meta loss on this task batch = 3.2494e-01, Meta loss averaged over last 500 steps = 3.0172e-01, PNorm = 111.9461, GNorm = 0.2788
Meta loss on this task batch = 3.1844e-01, Meta loss averaged over last 500 steps = 3.0176e-01, PNorm = 111.9507, GNorm = 0.3298
Meta loss on this task batch = 2.7419e-01, Meta loss averaged over last 500 steps = 3.0172e-01, PNorm = 111.9580, GNorm = 0.2904
Meta loss on this task batch = 3.4810e-01, Meta loss averaged over last 500 steps = 3.0173e-01, PNorm = 111.9678, GNorm = 0.2705
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 3.0183e-01, PNorm = 111.9802, GNorm = 0.2983
Meta loss on this task batch = 2.8987e-01, Meta loss averaged over last 500 steps = 3.0184e-01, PNorm = 111.9961, GNorm = 0.2905
Meta loss on this task batch = 2.6211e-01, Meta loss averaged over last 500 steps = 3.0172e-01, PNorm = 112.0151, GNorm = 0.3661
Took 132.14509296417236 seconds to complete one epoch of meta training
Took 140.49799752235413 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484111
Epoch 396
Meta loss on this task batch = 3.3289e-01, Meta loss averaged over last 500 steps = 3.0176e-01, PNorm = 112.0315, GNorm = 0.3009
Meta loss on this task batch = 2.7264e-01, Meta loss averaged over last 500 steps = 3.0160e-01, PNorm = 112.0443, GNorm = 0.3394
Meta loss on this task batch = 3.0900e-01, Meta loss averaged over last 500 steps = 3.0158e-01, PNorm = 112.0546, GNorm = 0.3089
Meta loss on this task batch = 3.2783e-01, Meta loss averaged over last 500 steps = 3.0158e-01, PNorm = 112.0640, GNorm = 0.3446
Meta loss on this task batch = 2.9307e-01, Meta loss averaged over last 500 steps = 3.0158e-01, PNorm = 112.0694, GNorm = 0.3441
Meta loss on this task batch = 3.0228e-01, Meta loss averaged over last 500 steps = 3.0160e-01, PNorm = 112.0735, GNorm = 0.3193
Meta loss on this task batch = 3.0035e-01, Meta loss averaged over last 500 steps = 3.0153e-01, PNorm = 112.0756, GNorm = 0.3314
Meta loss on this task batch = 3.5721e-01, Meta loss averaged over last 500 steps = 3.0173e-01, PNorm = 112.0757, GNorm = 0.3787
Meta loss on this task batch = 3.1201e-01, Meta loss averaged over last 500 steps = 3.0174e-01, PNorm = 112.0740, GNorm = 0.3380
Meta loss on this task batch = 2.6247e-01, Meta loss averaged over last 500 steps = 3.0163e-01, PNorm = 112.0739, GNorm = 0.2713
Meta loss on this task batch = 2.3571e-01, Meta loss averaged over last 500 steps = 3.0138e-01, PNorm = 112.0756, GNorm = 0.2637
Meta loss on this task batch = 2.8906e-01, Meta loss averaged over last 500 steps = 3.0138e-01, PNorm = 112.0739, GNorm = 0.3201
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 3.0132e-01, PNorm = 112.0724, GNorm = 0.2439
Meta loss on this task batch = 3.0677e-01, Meta loss averaged over last 500 steps = 3.0130e-01, PNorm = 112.0722, GNorm = 0.2893
Meta loss on this task batch = 2.7009e-01, Meta loss averaged over last 500 steps = 3.0108e-01, PNorm = 112.0737, GNorm = 0.2461
Meta loss on this task batch = 3.0515e-01, Meta loss averaged over last 500 steps = 3.0118e-01, PNorm = 112.0770, GNorm = 0.3294
Meta loss on this task batch = 3.6969e-01, Meta loss averaged over last 500 steps = 3.0141e-01, PNorm = 112.0812, GNorm = 0.3087
Meta loss on this task batch = 2.6873e-01, Meta loss averaged over last 500 steps = 3.0136e-01, PNorm = 112.0838, GNorm = 0.3051
Meta loss on this task batch = 2.9904e-01, Meta loss averaged over last 500 steps = 3.0126e-01, PNorm = 112.0876, GNorm = 0.3576
Took 135.8479564189911 seconds to complete one epoch of meta training
Took 144.43091082572937 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485968
Epoch 397
Meta loss on this task batch = 3.0518e-01, Meta loss averaged over last 500 steps = 3.0127e-01, PNorm = 112.0942, GNorm = 0.2882
Meta loss on this task batch = 3.0630e-01, Meta loss averaged over last 500 steps = 3.0128e-01, PNorm = 112.1015, GNorm = 0.2920
Meta loss on this task batch = 2.6360e-01, Meta loss averaged over last 500 steps = 3.0133e-01, PNorm = 112.1104, GNorm = 0.2827
Meta loss on this task batch = 2.7139e-01, Meta loss averaged over last 500 steps = 3.0140e-01, PNorm = 112.1184, GNorm = 0.2901
Meta loss on this task batch = 3.0269e-01, Meta loss averaged over last 500 steps = 3.0126e-01, PNorm = 112.1274, GNorm = 0.2961
Meta loss on this task batch = 3.1954e-01, Meta loss averaged over last 500 steps = 3.0140e-01, PNorm = 112.1352, GNorm = 0.3209
Meta loss on this task batch = 2.2664e-01, Meta loss averaged over last 500 steps = 3.0116e-01, PNorm = 112.1425, GNorm = 0.2591
Meta loss on this task batch = 3.5351e-01, Meta loss averaged over last 500 steps = 3.0122e-01, PNorm = 112.1455, GNorm = 0.3411
Meta loss on this task batch = 2.9806e-01, Meta loss averaged over last 500 steps = 3.0126e-01, PNorm = 112.1477, GNorm = 0.2710
Meta loss on this task batch = 3.0761e-01, Meta loss averaged over last 500 steps = 3.0129e-01, PNorm = 112.1460, GNorm = 0.3283
Meta loss on this task batch = 3.7550e-01, Meta loss averaged over last 500 steps = 3.0145e-01, PNorm = 112.1402, GNorm = 0.3633
Meta loss on this task batch = 2.7064e-01, Meta loss averaged over last 500 steps = 3.0129e-01, PNorm = 112.1330, GNorm = 0.2999
Meta loss on this task batch = 2.9472e-01, Meta loss averaged over last 500 steps = 3.0131e-01, PNorm = 112.1277, GNorm = 0.2591
Meta loss on this task batch = 3.5869e-01, Meta loss averaged over last 500 steps = 3.0141e-01, PNorm = 112.1234, GNorm = 0.3010
Meta loss on this task batch = 2.5963e-01, Meta loss averaged over last 500 steps = 3.0136e-01, PNorm = 112.1232, GNorm = 0.2671
Meta loss on this task batch = 3.2851e-01, Meta loss averaged over last 500 steps = 3.0148e-01, PNorm = 112.1277, GNorm = 0.2657
Meta loss on this task batch = 2.7769e-01, Meta loss averaged over last 500 steps = 3.0142e-01, PNorm = 112.1337, GNorm = 0.2574
Meta loss on this task batch = 2.9966e-01, Meta loss averaged over last 500 steps = 3.0139e-01, PNorm = 112.1425, GNorm = 0.2916
Meta loss on this task batch = 2.3901e-01, Meta loss averaged over last 500 steps = 3.0126e-01, PNorm = 112.1535, GNorm = 0.3234
Took 131.87136149406433 seconds to complete one epoch of meta training
Took 140.38173031806946 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496831
Epoch 398
Meta loss on this task batch = 3.1383e-01, Meta loss averaged over last 500 steps = 3.0131e-01, PNorm = 112.1638, GNorm = 0.2817
Meta loss on this task batch = 3.3064e-01, Meta loss averaged over last 500 steps = 3.0138e-01, PNorm = 112.1741, GNorm = 0.2997
Meta loss on this task batch = 3.3240e-01, Meta loss averaged over last 500 steps = 3.0143e-01, PNorm = 112.1795, GNorm = 0.3328
Meta loss on this task batch = 2.6282e-01, Meta loss averaged over last 500 steps = 3.0137e-01, PNorm = 112.1833, GNorm = 0.2653
Meta loss on this task batch = 2.7239e-01, Meta loss averaged over last 500 steps = 3.0136e-01, PNorm = 112.1861, GNorm = 0.2796
Meta loss on this task batch = 2.9192e-01, Meta loss averaged over last 500 steps = 3.0120e-01, PNorm = 112.1870, GNorm = 0.2898
Meta loss on this task batch = 2.5282e-01, Meta loss averaged over last 500 steps = 3.0101e-01, PNorm = 112.1869, GNorm = 0.3170
Meta loss on this task batch = 3.2810e-01, Meta loss averaged over last 500 steps = 3.0094e-01, PNorm = 112.1839, GNorm = 0.3390
Meta loss on this task batch = 2.8043e-01, Meta loss averaged over last 500 steps = 3.0087e-01, PNorm = 112.1848, GNorm = 0.3152
Meta loss on this task batch = 2.5659e-01, Meta loss averaged over last 500 steps = 3.0084e-01, PNorm = 112.1881, GNorm = 0.2833
Meta loss on this task batch = 2.7653e-01, Meta loss averaged over last 500 steps = 3.0085e-01, PNorm = 112.1929, GNorm = 0.2866
Meta loss on this task batch = 2.4296e-01, Meta loss averaged over last 500 steps = 3.0071e-01, PNorm = 112.1988, GNorm = 0.2865
Meta loss on this task batch = 3.6847e-01, Meta loss averaged over last 500 steps = 3.0070e-01, PNorm = 112.2054, GNorm = 0.3242
Meta loss on this task batch = 3.4509e-01, Meta loss averaged over last 500 steps = 3.0079e-01, PNorm = 112.2120, GNorm = 0.3129
Meta loss on this task batch = 3.1423e-01, Meta loss averaged over last 500 steps = 3.0079e-01, PNorm = 112.2156, GNorm = 0.3271
Meta loss on this task batch = 3.1239e-01, Meta loss averaged over last 500 steps = 3.0089e-01, PNorm = 112.2196, GNorm = 0.3062
Meta loss on this task batch = 2.8329e-01, Meta loss averaged over last 500 steps = 3.0088e-01, PNorm = 112.2242, GNorm = 0.2954
Meta loss on this task batch = 2.2397e-01, Meta loss averaged over last 500 steps = 3.0069e-01, PNorm = 112.2287, GNorm = 0.2352
Meta loss on this task batch = 3.2353e-01, Meta loss averaged over last 500 steps = 3.0073e-01, PNorm = 112.2193, GNorm = 0.6916
Took 132.49242854118347 seconds to complete one epoch of meta training
Took 140.74951577186584 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506414
Epoch 399
Meta loss on this task batch = 2.6956e-01, Meta loss averaged over last 500 steps = 3.0068e-01, PNorm = 112.2088, GNorm = 0.2888
Meta loss on this task batch = 3.3223e-01, Meta loss averaged over last 500 steps = 3.0084e-01, PNorm = 112.2005, GNorm = 0.2719
Meta loss on this task batch = 2.5617e-01, Meta loss averaged over last 500 steps = 3.0075e-01, PNorm = 112.1937, GNorm = 0.3163
Meta loss on this task batch = 3.1034e-01, Meta loss averaged over last 500 steps = 3.0073e-01, PNorm = 112.1887, GNorm = 0.3272
Meta loss on this task batch = 2.4134e-01, Meta loss averaged over last 500 steps = 3.0062e-01, PNorm = 112.1856, GNorm = 0.2688
Meta loss on this task batch = 3.6829e-01, Meta loss averaged over last 500 steps = 3.0083e-01, PNorm = 112.1816, GNorm = 0.3626
Meta loss on this task batch = 3.0166e-01, Meta loss averaged over last 500 steps = 3.0093e-01, PNorm = 112.1790, GNorm = 0.2884
Meta loss on this task batch = 2.6978e-01, Meta loss averaged over last 500 steps = 3.0085e-01, PNorm = 112.1789, GNorm = 0.2641
Meta loss on this task batch = 2.9496e-01, Meta loss averaged over last 500 steps = 3.0093e-01, PNorm = 112.1821, GNorm = 0.3217
Meta loss on this task batch = 2.9765e-01, Meta loss averaged over last 500 steps = 3.0101e-01, PNorm = 112.1849, GNorm = 0.3294
Meta loss on this task batch = 3.1596e-01, Meta loss averaged over last 500 steps = 3.0109e-01, PNorm = 112.1899, GNorm = 0.2763
Meta loss on this task batch = 3.1397e-01, Meta loss averaged over last 500 steps = 3.0105e-01, PNorm = 112.1966, GNorm = 0.3119
Meta loss on this task batch = 2.9768e-01, Meta loss averaged over last 500 steps = 3.0105e-01, PNorm = 112.2017, GNorm = 0.3162
Meta loss on this task batch = 3.4343e-01, Meta loss averaged over last 500 steps = 3.0115e-01, PNorm = 112.2074, GNorm = 0.3360
Meta loss on this task batch = 2.4322e-01, Meta loss averaged over last 500 steps = 3.0102e-01, PNorm = 112.2128, GNorm = 0.2818
Meta loss on this task batch = 3.4474e-01, Meta loss averaged over last 500 steps = 3.0111e-01, PNorm = 112.2152, GNorm = 0.3331
Meta loss on this task batch = 2.7817e-01, Meta loss averaged over last 500 steps = 3.0112e-01, PNorm = 112.2187, GNorm = 0.2977
Meta loss on this task batch = 2.9116e-01, Meta loss averaged over last 500 steps = 3.0097e-01, PNorm = 112.2231, GNorm = 0.2652
Meta loss on this task batch = 3.1160e-01, Meta loss averaged over last 500 steps = 3.0105e-01, PNorm = 112.2278, GNorm = 0.3648
Took 133.3624541759491 seconds to complete one epoch of meta training
Took 142.2747278213501 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517381
Epoch 400
Meta loss on this task batch = 2.9631e-01, Meta loss averaged over last 500 steps = 3.0098e-01, PNorm = 112.2307, GNorm = 0.3206
Meta loss on this task batch = 3.0037e-01, Meta loss averaged over last 500 steps = 3.0092e-01, PNorm = 112.2323, GNorm = 0.2941
Meta loss on this task batch = 2.9647e-01, Meta loss averaged over last 500 steps = 3.0091e-01, PNorm = 112.2340, GNorm = 0.3375
Meta loss on this task batch = 3.5199e-01, Meta loss averaged over last 500 steps = 3.0090e-01, PNorm = 112.2358, GNorm = 0.3037
Meta loss on this task batch = 2.7402e-01, Meta loss averaged over last 500 steps = 3.0079e-01, PNorm = 112.2384, GNorm = 0.2693
Meta loss on this task batch = 3.0711e-01, Meta loss averaged over last 500 steps = 3.0080e-01, PNorm = 112.2435, GNorm = 0.2860
Meta loss on this task batch = 3.2113e-01, Meta loss averaged over last 500 steps = 3.0093e-01, PNorm = 112.2525, GNorm = 0.2961
Meta loss on this task batch = 3.5376e-01, Meta loss averaged over last 500 steps = 3.0112e-01, PNorm = 112.2635, GNorm = 0.3124
Meta loss on this task batch = 2.8809e-01, Meta loss averaged over last 500 steps = 3.0095e-01, PNorm = 112.2768, GNorm = 0.2943
Meta loss on this task batch = 2.8391e-01, Meta loss averaged over last 500 steps = 3.0109e-01, PNorm = 112.2904, GNorm = 0.2511
Meta loss on this task batch = 3.3394e-01, Meta loss averaged over last 500 steps = 3.0112e-01, PNorm = 112.3023, GNorm = 0.3222
Meta loss on this task batch = 2.7409e-01, Meta loss averaged over last 500 steps = 3.0105e-01, PNorm = 112.3161, GNorm = 0.2742
Meta loss on this task batch = 2.6313e-01, Meta loss averaged over last 500 steps = 3.0090e-01, PNorm = 112.3288, GNorm = 0.2848
Meta loss on this task batch = 2.9398e-01, Meta loss averaged over last 500 steps = 3.0079e-01, PNorm = 112.3397, GNorm = 0.3167
Meta loss on this task batch = 2.7330e-01, Meta loss averaged over last 500 steps = 3.0072e-01, PNorm = 112.3490, GNorm = 0.2743
Meta loss on this task batch = 2.5392e-01, Meta loss averaged over last 500 steps = 3.0070e-01, PNorm = 112.3592, GNorm = 0.2683
Meta loss on this task batch = 2.6867e-01, Meta loss averaged over last 500 steps = 3.0064e-01, PNorm = 112.3686, GNorm = 0.2762
Meta loss on this task batch = 3.1265e-01, Meta loss averaged over last 500 steps = 3.0063e-01, PNorm = 112.3786, GNorm = 0.3317
Meta loss on this task batch = 3.4045e-01, Meta loss averaged over last 500 steps = 3.0071e-01, PNorm = 112.3837, GNorm = 0.3900
Took 133.08672881126404 seconds to complete one epoch of meta training
Took 141.62677693367004 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506073
Epoch 401
Meta loss on this task batch = 2.6590e-01, Meta loss averaged over last 500 steps = 3.0056e-01, PNorm = 112.3876, GNorm = 0.3009
Meta loss on this task batch = 2.9770e-01, Meta loss averaged over last 500 steps = 3.0060e-01, PNorm = 112.3913, GNorm = 0.3046
Meta loss on this task batch = 2.9920e-01, Meta loss averaged over last 500 steps = 3.0056e-01, PNorm = 112.3937, GNorm = 0.2791
Meta loss on this task batch = 2.4233e-01, Meta loss averaged over last 500 steps = 3.0047e-01, PNorm = 112.3942, GNorm = 0.2559
Meta loss on this task batch = 3.3448e-01, Meta loss averaged over last 500 steps = 3.0046e-01, PNorm = 112.3951, GNorm = 0.3326
Meta loss on this task batch = 2.8631e-01, Meta loss averaged over last 500 steps = 3.0036e-01, PNorm = 112.3990, GNorm = 0.2911
Meta loss on this task batch = 3.4584e-01, Meta loss averaged over last 500 steps = 3.0047e-01, PNorm = 112.4043, GNorm = 0.2964
Meta loss on this task batch = 2.8999e-01, Meta loss averaged over last 500 steps = 3.0041e-01, PNorm = 112.4109, GNorm = 0.2756
Meta loss on this task batch = 2.9981e-01, Meta loss averaged over last 500 steps = 3.0025e-01, PNorm = 112.4202, GNorm = 0.2858
Meta loss on this task batch = 3.2566e-01, Meta loss averaged over last 500 steps = 3.0031e-01, PNorm = 112.4294, GNorm = 0.3120
Meta loss on this task batch = 2.7138e-01, Meta loss averaged over last 500 steps = 3.0021e-01, PNorm = 112.4399, GNorm = 0.2698
Meta loss on this task batch = 2.6246e-01, Meta loss averaged over last 500 steps = 3.0010e-01, PNorm = 112.4490, GNorm = 0.2658
Meta loss on this task batch = 2.7060e-01, Meta loss averaged over last 500 steps = 3.0008e-01, PNorm = 112.4575, GNorm = 0.3106
Meta loss on this task batch = 2.4156e-01, Meta loss averaged over last 500 steps = 3.0006e-01, PNorm = 112.4648, GNorm = 0.2919
Meta loss on this task batch = 3.4422e-01, Meta loss averaged over last 500 steps = 3.0015e-01, PNorm = 112.4677, GNorm = 0.4124
Meta loss on this task batch = 3.5969e-01, Meta loss averaged over last 500 steps = 3.0025e-01, PNorm = 112.4694, GNorm = 0.3778
Meta loss on this task batch = 3.4764e-01, Meta loss averaged over last 500 steps = 3.0041e-01, PNorm = 112.4667, GNorm = 0.3784
Meta loss on this task batch = 3.1021e-01, Meta loss averaged over last 500 steps = 3.0040e-01, PNorm = 112.4626, GNorm = 0.3046
Meta loss on this task batch = 2.8356e-01, Meta loss averaged over last 500 steps = 3.0025e-01, PNorm = 112.4611, GNorm = 0.3523
Took 132.53553557395935 seconds to complete one epoch of meta training
Took 141.0193953514099 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485466
Epoch 402
Meta loss on this task batch = 2.8301e-01, Meta loss averaged over last 500 steps = 3.0018e-01, PNorm = 112.4619, GNorm = 0.2681
Meta loss on this task batch = 3.1200e-01, Meta loss averaged over last 500 steps = 3.0019e-01, PNorm = 112.4647, GNorm = 0.2683
Meta loss on this task batch = 3.0158e-01, Meta loss averaged over last 500 steps = 3.0036e-01, PNorm = 112.4716, GNorm = 0.2825
Meta loss on this task batch = 3.0224e-01, Meta loss averaged over last 500 steps = 3.0035e-01, PNorm = 112.4818, GNorm = 0.2918
Meta loss on this task batch = 3.3298e-01, Meta loss averaged over last 500 steps = 3.0039e-01, PNorm = 112.4950, GNorm = 0.3392
Meta loss on this task batch = 3.0838e-01, Meta loss averaged over last 500 steps = 3.0035e-01, PNorm = 112.5104, GNorm = 0.3164
Meta loss on this task batch = 3.4558e-01, Meta loss averaged over last 500 steps = 3.0034e-01, PNorm = 112.5227, GNorm = 0.3413
Meta loss on this task batch = 3.0333e-01, Meta loss averaged over last 500 steps = 3.0031e-01, PNorm = 112.5353, GNorm = 0.2984
Meta loss on this task batch = 3.2607e-01, Meta loss averaged over last 500 steps = 3.0051e-01, PNorm = 112.5472, GNorm = 0.3384
Meta loss on this task batch = 2.9547e-01, Meta loss averaged over last 500 steps = 3.0057e-01, PNorm = 112.5577, GNorm = 0.3136
Meta loss on this task batch = 2.9474e-01, Meta loss averaged over last 500 steps = 3.0063e-01, PNorm = 112.5689, GNorm = 0.3212
Meta loss on this task batch = 2.9086e-01, Meta loss averaged over last 500 steps = 3.0063e-01, PNorm = 112.5788, GNorm = 0.3206
Meta loss on this task batch = 2.7546e-01, Meta loss averaged over last 500 steps = 3.0059e-01, PNorm = 112.5864, GNorm = 0.2912
Meta loss on this task batch = 3.1654e-01, Meta loss averaged over last 500 steps = 3.0062e-01, PNorm = 112.5927, GNorm = 0.3124
Meta loss on this task batch = 2.9638e-01, Meta loss averaged over last 500 steps = 3.0055e-01, PNorm = 112.5996, GNorm = 0.2893
Meta loss on this task batch = 3.1852e-01, Meta loss averaged over last 500 steps = 3.0058e-01, PNorm = 112.6054, GNorm = 0.3251
Meta loss on this task batch = 2.4654e-01, Meta loss averaged over last 500 steps = 3.0051e-01, PNorm = 112.6119, GNorm = 0.2477
Meta loss on this task batch = 2.3620e-01, Meta loss averaged over last 500 steps = 3.0039e-01, PNorm = 112.6161, GNorm = 0.2502
Meta loss on this task batch = 2.9742e-01, Meta loss averaged over last 500 steps = 3.0039e-01, PNorm = 112.6214, GNorm = 0.3359
Took 133.02157545089722 seconds to complete one epoch of meta training
Took 141.54328966140747 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496891
Epoch 403
Meta loss on this task batch = 2.6470e-01, Meta loss averaged over last 500 steps = 3.0031e-01, PNorm = 112.6287, GNorm = 0.2676
Meta loss on this task batch = 3.2176e-01, Meta loss averaged over last 500 steps = 3.0026e-01, PNorm = 112.6359, GNorm = 0.2915
Meta loss on this task batch = 3.2766e-01, Meta loss averaged over last 500 steps = 3.0023e-01, PNorm = 112.6445, GNorm = 0.2966
Meta loss on this task batch = 2.8642e-01, Meta loss averaged over last 500 steps = 3.0029e-01, PNorm = 112.6527, GNorm = 0.2768
Meta loss on this task batch = 3.0581e-01, Meta loss averaged over last 500 steps = 3.0032e-01, PNorm = 112.6616, GNorm = 0.3219
Meta loss on this task batch = 3.0622e-01, Meta loss averaged over last 500 steps = 3.0019e-01, PNorm = 112.6686, GNorm = 0.3181
Meta loss on this task batch = 2.7074e-01, Meta loss averaged over last 500 steps = 3.0008e-01, PNorm = 112.6745, GNorm = 0.2701
Meta loss on this task batch = 2.6141e-01, Meta loss averaged over last 500 steps = 2.9997e-01, PNorm = 112.6823, GNorm = 0.2847
Meta loss on this task batch = 2.6129e-01, Meta loss averaged over last 500 steps = 2.9994e-01, PNorm = 112.6904, GNorm = 0.2577
Meta loss on this task batch = 2.6259e-01, Meta loss averaged over last 500 steps = 2.9984e-01, PNorm = 112.6971, GNorm = 0.3184
Meta loss on this task batch = 3.1546e-01, Meta loss averaged over last 500 steps = 2.9985e-01, PNorm = 112.7036, GNorm = 0.4900
Meta loss on this task batch = 2.8701e-01, Meta loss averaged over last 500 steps = 2.9994e-01, PNorm = 112.7093, GNorm = 0.3073
Meta loss on this task batch = 3.3235e-01, Meta loss averaged over last 500 steps = 3.0006e-01, PNorm = 112.7122, GNorm = 0.3871
Meta loss on this task batch = 3.7182e-01, Meta loss averaged over last 500 steps = 3.0023e-01, PNorm = 112.7142, GNorm = 0.5263
Meta loss on this task batch = 3.1643e-01, Meta loss averaged over last 500 steps = 3.0021e-01, PNorm = 112.7173, GNorm = 0.3476
Meta loss on this task batch = 2.7133e-01, Meta loss averaged over last 500 steps = 3.0018e-01, PNorm = 112.7223, GNorm = 0.3048
Meta loss on this task batch = 3.0040e-01, Meta loss averaged over last 500 steps = 3.0024e-01, PNorm = 112.7295, GNorm = 0.2763
Meta loss on this task batch = 2.8676e-01, Meta loss averaged over last 500 steps = 3.0020e-01, PNorm = 112.7396, GNorm = 0.2880
Meta loss on this task batch = 3.6900e-01, Meta loss averaged over last 500 steps = 3.0033e-01, PNorm = 112.7517, GNorm = 0.3817
Took 129.39356303215027 seconds to complete one epoch of meta training
Took 137.6798074245453 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494308
Epoch 404
Meta loss on this task batch = 2.5605e-01, Meta loss averaged over last 500 steps = 3.0029e-01, PNorm = 112.7636, GNorm = 0.2681
Meta loss on this task batch = 3.4732e-01, Meta loss averaged over last 500 steps = 3.0035e-01, PNorm = 112.7752, GNorm = 0.3001
Meta loss on this task batch = 2.8639e-01, Meta loss averaged over last 500 steps = 3.0023e-01, PNorm = 112.7878, GNorm = 0.2419
Meta loss on this task batch = 2.2888e-01, Meta loss averaged over last 500 steps = 3.0005e-01, PNorm = 112.7986, GNorm = 0.2783
Meta loss on this task batch = 2.8075e-01, Meta loss averaged over last 500 steps = 2.9996e-01, PNorm = 112.8072, GNorm = 0.3144
Meta loss on this task batch = 2.5277e-01, Meta loss averaged over last 500 steps = 3.0000e-01, PNorm = 112.8149, GNorm = 0.2862
Meta loss on this task batch = 3.4176e-01, Meta loss averaged over last 500 steps = 3.0016e-01, PNorm = 112.8190, GNorm = 0.3729
Meta loss on this task batch = 3.6494e-01, Meta loss averaged over last 500 steps = 3.0026e-01, PNorm = 112.8191, GNorm = 0.3934
Meta loss on this task batch = 2.6909e-01, Meta loss averaged over last 500 steps = 3.0021e-01, PNorm = 112.8156, GNorm = 0.3194
Meta loss on this task batch = 2.7582e-01, Meta loss averaged over last 500 steps = 3.0011e-01, PNorm = 112.8131, GNorm = 0.2842
Meta loss on this task batch = 3.2673e-01, Meta loss averaged over last 500 steps = 3.0001e-01, PNorm = 112.8087, GNorm = 0.3080
Meta loss on this task batch = 2.1490e-01, Meta loss averaged over last 500 steps = 2.9990e-01, PNorm = 112.8051, GNorm = 0.2499
Meta loss on this task batch = 2.4369e-01, Meta loss averaged over last 500 steps = 2.9973e-01, PNorm = 112.8031, GNorm = 0.2499
Meta loss on this task batch = 2.9313e-01, Meta loss averaged over last 500 steps = 2.9968e-01, PNorm = 112.8038, GNorm = 0.3116
Meta loss on this task batch = 3.4323e-01, Meta loss averaged over last 500 steps = 2.9976e-01, PNorm = 112.8051, GNorm = 0.3170
Meta loss on this task batch = 3.1609e-01, Meta loss averaged over last 500 steps = 2.9980e-01, PNorm = 112.8092, GNorm = 0.2881
Meta loss on this task batch = 3.4972e-01, Meta loss averaged over last 500 steps = 2.9990e-01, PNorm = 112.8171, GNorm = 0.3236
Meta loss on this task batch = 3.2071e-01, Meta loss averaged over last 500 steps = 2.9999e-01, PNorm = 112.8276, GNorm = 0.3268
Meta loss on this task batch = 3.1928e-01, Meta loss averaged over last 500 steps = 3.0006e-01, PNorm = 112.8376, GNorm = 0.3736
Took 134.02772426605225 seconds to complete one epoch of meta training
Took 143.41664123535156 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496028
Epoch 405
Meta loss on this task batch = 3.1454e-01, Meta loss averaged over last 500 steps = 3.0009e-01, PNorm = 112.8499, GNorm = 0.3473
Meta loss on this task batch = 2.3466e-01, Meta loss averaged over last 500 steps = 2.9994e-01, PNorm = 112.8635, GNorm = 0.2403
Meta loss on this task batch = 3.2893e-01, Meta loss averaged over last 500 steps = 3.0002e-01, PNorm = 112.8785, GNorm = 0.2543
Meta loss on this task batch = 2.8892e-01, Meta loss averaged over last 500 steps = 2.9999e-01, PNorm = 112.8941, GNorm = 0.2700
Meta loss on this task batch = 3.3080e-01, Meta loss averaged over last 500 steps = 2.9996e-01, PNorm = 112.9073, GNorm = 0.2867
Meta loss on this task batch = 2.9956e-01, Meta loss averaged over last 500 steps = 2.9991e-01, PNorm = 112.9201, GNorm = 0.2772
Meta loss on this task batch = 3.4526e-01, Meta loss averaged over last 500 steps = 3.0002e-01, PNorm = 112.9314, GNorm = 0.2943
Meta loss on this task batch = 2.8235e-01, Meta loss averaged over last 500 steps = 2.9996e-01, PNorm = 112.9412, GNorm = 0.2655
Meta loss on this task batch = 2.7365e-01, Meta loss averaged over last 500 steps = 2.9984e-01, PNorm = 112.9499, GNorm = 0.2853
Meta loss on this task batch = 3.1905e-01, Meta loss averaged over last 500 steps = 2.9991e-01, PNorm = 112.9570, GNorm = 0.2935
Meta loss on this task batch = 3.0438e-01, Meta loss averaged over last 500 steps = 2.9993e-01, PNorm = 112.9640, GNorm = 0.3280
Meta loss on this task batch = 2.5998e-01, Meta loss averaged over last 500 steps = 2.9982e-01, PNorm = 112.9710, GNorm = 0.2423
Meta loss on this task batch = 3.4079e-01, Meta loss averaged over last 500 steps = 2.9991e-01, PNorm = 112.9775, GNorm = 0.3424
Meta loss on this task batch = 2.4565e-01, Meta loss averaged over last 500 steps = 2.9992e-01, PNorm = 112.9833, GNorm = 0.2779
Meta loss on this task batch = 3.3666e-01, Meta loss averaged over last 500 steps = 2.9993e-01, PNorm = 112.9878, GNorm = 0.2931
Meta loss on this task batch = 3.1387e-01, Meta loss averaged over last 500 steps = 2.9998e-01, PNorm = 112.9914, GNorm = 0.3046
Meta loss on this task batch = 2.8724e-01, Meta loss averaged over last 500 steps = 3.0001e-01, PNorm = 112.9940, GNorm = 0.2854
Meta loss on this task batch = 2.6491e-01, Meta loss averaged over last 500 steps = 3.0000e-01, PNorm = 112.9963, GNorm = 0.2933
Meta loss on this task batch = 3.0143e-01, Meta loss averaged over last 500 steps = 2.9995e-01, PNorm = 112.9985, GNorm = 0.3609
Took 129.65770196914673 seconds to complete one epoch of meta training
Took 138.11185550689697 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502533
Epoch 406
Meta loss on this task batch = 2.5881e-01, Meta loss averaged over last 500 steps = 2.9988e-01, PNorm = 113.0010, GNorm = 0.2703
Meta loss on this task batch = 2.9350e-01, Meta loss averaged over last 500 steps = 2.9973e-01, PNorm = 113.0036, GNorm = 0.3018
Meta loss on this task batch = 2.8714e-01, Meta loss averaged over last 500 steps = 2.9962e-01, PNorm = 113.0068, GNorm = 0.2798
Meta loss on this task batch = 3.3574e-01, Meta loss averaged over last 500 steps = 2.9966e-01, PNorm = 113.0093, GNorm = 0.3054
Meta loss on this task batch = 3.4531e-01, Meta loss averaged over last 500 steps = 2.9959e-01, PNorm = 113.0106, GNorm = 0.3767
Meta loss on this task batch = 3.3231e-01, Meta loss averaged over last 500 steps = 2.9965e-01, PNorm = 113.0142, GNorm = 0.2883
Meta loss on this task batch = 2.8845e-01, Meta loss averaged over last 500 steps = 2.9959e-01, PNorm = 113.0194, GNorm = 0.3084
Meta loss on this task batch = 2.8999e-01, Meta loss averaged over last 500 steps = 2.9958e-01, PNorm = 113.0269, GNorm = 0.3203
Meta loss on this task batch = 2.6313e-01, Meta loss averaged over last 500 steps = 2.9950e-01, PNorm = 113.0355, GNorm = 0.2698
Meta loss on this task batch = 2.5547e-01, Meta loss averaged over last 500 steps = 2.9931e-01, PNorm = 113.0449, GNorm = 0.3279
Meta loss on this task batch = 3.0967e-01, Meta loss averaged over last 500 steps = 2.9940e-01, PNorm = 113.0516, GNorm = 0.3473
Meta loss on this task batch = 2.7313e-01, Meta loss averaged over last 500 steps = 2.9932e-01, PNorm = 113.0592, GNorm = 0.2924
Meta loss on this task batch = 2.7842e-01, Meta loss averaged over last 500 steps = 2.9932e-01, PNorm = 113.0673, GNorm = 0.2879
Meta loss on this task batch = 2.8596e-01, Meta loss averaged over last 500 steps = 2.9910e-01, PNorm = 113.0774, GNorm = 0.2964
Meta loss on this task batch = 2.9160e-01, Meta loss averaged over last 500 steps = 2.9910e-01, PNorm = 113.0866, GNorm = 0.3182
Meta loss on this task batch = 2.9445e-01, Meta loss averaged over last 500 steps = 2.9906e-01, PNorm = 113.0964, GNorm = 0.2792
Meta loss on this task batch = 3.4428e-01, Meta loss averaged over last 500 steps = 2.9915e-01, PNorm = 113.1035, GNorm = 0.3181
Meta loss on this task batch = 3.1304e-01, Meta loss averaged over last 500 steps = 2.9920e-01, PNorm = 113.1083, GNorm = 0.3120
Meta loss on this task batch = 3.2162e-01, Meta loss averaged over last 500 steps = 2.9924e-01, PNorm = 113.1111, GNorm = 0.3528
Took 132.3215458393097 seconds to complete one epoch of meta training
Took 140.73109459877014 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487475
Epoch 407
Meta loss on this task batch = 2.7387e-01, Meta loss averaged over last 500 steps = 2.9924e-01, PNorm = 113.1136, GNorm = 0.2922
Meta loss on this task batch = 2.9293e-01, Meta loss averaged over last 500 steps = 2.9917e-01, PNorm = 113.1155, GNorm = 0.2647
Meta loss on this task batch = 3.0254e-01, Meta loss averaged over last 500 steps = 2.9917e-01, PNorm = 113.1180, GNorm = 0.2794
Meta loss on this task batch = 2.6265e-01, Meta loss averaged over last 500 steps = 2.9907e-01, PNorm = 113.1227, GNorm = 0.2742
Meta loss on this task batch = 2.9232e-01, Meta loss averaged over last 500 steps = 2.9908e-01, PNorm = 113.1270, GNorm = 0.2534
Meta loss on this task batch = 3.3795e-01, Meta loss averaged over last 500 steps = 2.9919e-01, PNorm = 113.1296, GNorm = 0.3097
Meta loss on this task batch = 3.0601e-01, Meta loss averaged over last 500 steps = 2.9924e-01, PNorm = 113.1327, GNorm = 0.2702
Meta loss on this task batch = 2.9047e-01, Meta loss averaged over last 500 steps = 2.9924e-01, PNorm = 113.1371, GNorm = 0.2587
Meta loss on this task batch = 2.9000e-01, Meta loss averaged over last 500 steps = 2.9916e-01, PNorm = 113.1424, GNorm = 0.2879
Meta loss on this task batch = 3.0932e-01, Meta loss averaged over last 500 steps = 2.9919e-01, PNorm = 113.1467, GNorm = 0.3090
Meta loss on this task batch = 2.6269e-01, Meta loss averaged over last 500 steps = 2.9904e-01, PNorm = 113.1532, GNorm = 0.3116
Meta loss on this task batch = 3.1011e-01, Meta loss averaged over last 500 steps = 2.9891e-01, PNorm = 113.1615, GNorm = 0.2840
Meta loss on this task batch = 2.7614e-01, Meta loss averaged over last 500 steps = 2.9879e-01, PNorm = 113.1700, GNorm = 0.2783
Meta loss on this task batch = 3.2483e-01, Meta loss averaged over last 500 steps = 2.9896e-01, PNorm = 113.1787, GNorm = 0.3088
Meta loss on this task batch = 3.3497e-01, Meta loss averaged over last 500 steps = 2.9919e-01, PNorm = 113.1870, GNorm = 0.3612
Meta loss on this task batch = 3.3974e-01, Meta loss averaged over last 500 steps = 2.9925e-01, PNorm = 113.1944, GNorm = 0.3246
Meta loss on this task batch = 3.1815e-01, Meta loss averaged over last 500 steps = 2.9936e-01, PNorm = 113.2027, GNorm = 0.3447
Meta loss on this task batch = 2.6117e-01, Meta loss averaged over last 500 steps = 2.9931e-01, PNorm = 113.2122, GNorm = 0.3689
Meta loss on this task batch = 2.6805e-01, Meta loss averaged over last 500 steps = 2.9912e-01, PNorm = 113.2228, GNorm = 0.3154
Took 134.36413645744324 seconds to complete one epoch of meta training
Took 142.74761986732483 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486820
Epoch 408
Meta loss on this task batch = 2.6693e-01, Meta loss averaged over last 500 steps = 2.9902e-01, PNorm = 113.2327, GNorm = 0.2712
Meta loss on this task batch = 3.6577e-01, Meta loss averaged over last 500 steps = 2.9911e-01, PNorm = 113.2407, GNorm = 0.3304
Meta loss on this task batch = 3.2034e-01, Meta loss averaged over last 500 steps = 2.9906e-01, PNorm = 113.2434, GNorm = 0.3383
Meta loss on this task batch = 2.4387e-01, Meta loss averaged over last 500 steps = 2.9900e-01, PNorm = 113.2458, GNorm = 0.2723
Meta loss on this task batch = 3.0641e-01, Meta loss averaged over last 500 steps = 2.9906e-01, PNorm = 113.2443, GNorm = 0.3081
Meta loss on this task batch = 3.6278e-01, Meta loss averaged over last 500 steps = 2.9922e-01, PNorm = 113.2419, GNorm = 0.3510
Meta loss on this task batch = 3.4325e-01, Meta loss averaged over last 500 steps = 2.9936e-01, PNorm = 113.2383, GNorm = 0.3093
Meta loss on this task batch = 2.9751e-01, Meta loss averaged over last 500 steps = 2.9941e-01, PNorm = 113.2359, GNorm = 0.2720
Meta loss on this task batch = 2.6490e-01, Meta loss averaged over last 500 steps = 2.9922e-01, PNorm = 113.2345, GNorm = 0.2773
Meta loss on this task batch = 2.8402e-01, Meta loss averaged over last 500 steps = 2.9921e-01, PNorm = 113.2335, GNorm = 0.2570
Meta loss on this task batch = 2.7464e-01, Meta loss averaged over last 500 steps = 2.9907e-01, PNorm = 113.2367, GNorm = 0.3039
Meta loss on this task batch = 2.4078e-01, Meta loss averaged over last 500 steps = 2.9898e-01, PNorm = 113.2439, GNorm = 0.2933
Meta loss on this task batch = 2.8569e-01, Meta loss averaged over last 500 steps = 2.9907e-01, PNorm = 113.2539, GNorm = 0.2818
Meta loss on this task batch = 3.2532e-01, Meta loss averaged over last 500 steps = 2.9897e-01, PNorm = 113.2653, GNorm = 0.3029
Meta loss on this task batch = 3.0713e-01, Meta loss averaged over last 500 steps = 2.9893e-01, PNorm = 113.2761, GNorm = 0.3031
Meta loss on this task batch = 3.0148e-01, Meta loss averaged over last 500 steps = 2.9897e-01, PNorm = 113.2878, GNorm = 0.3014
Meta loss on this task batch = 3.5780e-01, Meta loss averaged over last 500 steps = 2.9909e-01, PNorm = 113.2929, GNorm = 0.3732
Meta loss on this task batch = 3.0473e-01, Meta loss averaged over last 500 steps = 2.9919e-01, PNorm = 113.2977, GNorm = 0.2912
Meta loss on this task batch = 2.9135e-01, Meta loss averaged over last 500 steps = 2.9917e-01, PNorm = 113.3016, GNorm = 0.3597
Took 128.49271845817566 seconds to complete one epoch of meta training
Took 136.787451505661 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477714
Epoch 409
Meta loss on this task batch = 2.8944e-01, Meta loss averaged over last 500 steps = 2.9925e-01, PNorm = 113.3036, GNorm = 0.3018
Meta loss on this task batch = 3.1424e-01, Meta loss averaged over last 500 steps = 2.9922e-01, PNorm = 113.3028, GNorm = 0.3152
Meta loss on this task batch = 3.1546e-01, Meta loss averaged over last 500 steps = 2.9921e-01, PNorm = 113.2987, GNorm = 0.3231
Meta loss on this task batch = 2.7310e-01, Meta loss averaged over last 500 steps = 2.9909e-01, PNorm = 113.2938, GNorm = 0.2626
Meta loss on this task batch = 2.7244e-01, Meta loss averaged over last 500 steps = 2.9892e-01, PNorm = 113.2899, GNorm = 0.2701
Meta loss on this task batch = 3.1886e-01, Meta loss averaged over last 500 steps = 2.9903e-01, PNorm = 113.2866, GNorm = 0.3021
Meta loss on this task batch = 3.0910e-01, Meta loss averaged over last 500 steps = 2.9912e-01, PNorm = 113.2850, GNorm = 0.2836
Meta loss on this task batch = 3.3773e-01, Meta loss averaged over last 500 steps = 2.9922e-01, PNorm = 113.2842, GNorm = 0.2897
Meta loss on this task batch = 3.6796e-01, Meta loss averaged over last 500 steps = 2.9937e-01, PNorm = 113.2864, GNorm = 0.3015
Meta loss on this task batch = 2.8186e-01, Meta loss averaged over last 500 steps = 2.9928e-01, PNorm = 113.2883, GNorm = 0.3221
Meta loss on this task batch = 1.9990e-01, Meta loss averaged over last 500 steps = 2.9921e-01, PNorm = 113.2939, GNorm = 0.2311
Meta loss on this task batch = 3.0656e-01, Meta loss averaged over last 500 steps = 2.9935e-01, PNorm = 113.3005, GNorm = 0.2564
Meta loss on this task batch = 3.2157e-01, Meta loss averaged over last 500 steps = 2.9929e-01, PNorm = 113.3067, GNorm = 0.3056
Meta loss on this task batch = 2.8287e-01, Meta loss averaged over last 500 steps = 2.9922e-01, PNorm = 113.3125, GNorm = 0.2940
Meta loss on this task batch = 3.3533e-01, Meta loss averaged over last 500 steps = 2.9929e-01, PNorm = 113.3181, GNorm = 0.4690
Meta loss on this task batch = 3.1233e-01, Meta loss averaged over last 500 steps = 2.9936e-01, PNorm = 113.3250, GNorm = 0.3106
Meta loss on this task batch = 2.7987e-01, Meta loss averaged over last 500 steps = 2.9920e-01, PNorm = 113.3310, GNorm = 0.2799
Meta loss on this task batch = 3.0878e-01, Meta loss averaged over last 500 steps = 2.9920e-01, PNorm = 113.3371, GNorm = 0.4091
Meta loss on this task batch = 2.7266e-01, Meta loss averaged over last 500 steps = 2.9914e-01, PNorm = 113.3437, GNorm = 0.3129
Took 130.98962545394897 seconds to complete one epoch of meta training
Took 139.34022784233093 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501493
Epoch 410
Meta loss on this task batch = 2.8063e-01, Meta loss averaged over last 500 steps = 2.9913e-01, PNorm = 113.3518, GNorm = 0.2639
Meta loss on this task batch = 2.7795e-01, Meta loss averaged over last 500 steps = 2.9908e-01, PNorm = 113.3605, GNorm = 0.2894
Meta loss on this task batch = 3.8843e-01, Meta loss averaged over last 500 steps = 2.9921e-01, PNorm = 113.3698, GNorm = 0.3277
Meta loss on this task batch = 3.1196e-01, Meta loss averaged over last 500 steps = 2.9914e-01, PNorm = 113.3769, GNorm = 0.3371
Meta loss on this task batch = 2.9930e-01, Meta loss averaged over last 500 steps = 2.9923e-01, PNorm = 113.3860, GNorm = 0.3277
Meta loss on this task batch = 3.3826e-01, Meta loss averaged over last 500 steps = 2.9920e-01, PNorm = 113.3944, GNorm = 0.2661
Meta loss on this task batch = 2.9267e-01, Meta loss averaged over last 500 steps = 2.9920e-01, PNorm = 113.4035, GNorm = 0.2909
Meta loss on this task batch = 2.9644e-01, Meta loss averaged over last 500 steps = 2.9924e-01, PNorm = 113.4168, GNorm = 0.3167
Meta loss on this task batch = 2.3864e-01, Meta loss averaged over last 500 steps = 2.9918e-01, PNorm = 113.4298, GNorm = 0.2886
Meta loss on this task batch = 3.0739e-01, Meta loss averaged over last 500 steps = 2.9916e-01, PNorm = 113.4434, GNorm = 0.2864
Meta loss on this task batch = 3.1502e-01, Meta loss averaged over last 500 steps = 2.9921e-01, PNorm = 113.4546, GNorm = 0.2825
Meta loss on this task batch = 2.4645e-01, Meta loss averaged over last 500 steps = 2.9903e-01, PNorm = 113.4659, GNorm = 0.2533
Meta loss on this task batch = 2.7458e-01, Meta loss averaged over last 500 steps = 2.9884e-01, PNorm = 113.4748, GNorm = 0.2941
Meta loss on this task batch = 2.9091e-01, Meta loss averaged over last 500 steps = 2.9889e-01, PNorm = 113.4815, GNorm = 0.3084
Meta loss on this task batch = 2.9495e-01, Meta loss averaged over last 500 steps = 2.9891e-01, PNorm = 113.4880, GNorm = 0.2928
Meta loss on this task batch = 3.1394e-01, Meta loss averaged over last 500 steps = 2.9888e-01, PNorm = 113.4921, GNorm = 0.3042
Meta loss on this task batch = 2.9925e-01, Meta loss averaged over last 500 steps = 2.9892e-01, PNorm = 113.4954, GNorm = 0.2957
Meta loss on this task batch = 2.6036e-01, Meta loss averaged over last 500 steps = 2.9889e-01, PNorm = 113.4982, GNorm = 0.2735
Meta loss on this task batch = 3.3851e-01, Meta loss averaged over last 500 steps = 2.9894e-01, PNorm = 113.4997, GNorm = 0.3670
Took 132.24046349525452 seconds to complete one epoch of meta training
Took 141.1446692943573 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.524291
Epoch 411
Meta loss on this task batch = 2.8314e-01, Meta loss averaged over last 500 steps = 2.9898e-01, PNorm = 113.5011, GNorm = 0.2848
Meta loss on this task batch = 3.0527e-01, Meta loss averaged over last 500 steps = 2.9885e-01, PNorm = 113.5036, GNorm = 0.3091
Meta loss on this task batch = 2.7016e-01, Meta loss averaged over last 500 steps = 2.9878e-01, PNorm = 113.5081, GNorm = 0.2731
Meta loss on this task batch = 3.1320e-01, Meta loss averaged over last 500 steps = 2.9886e-01, PNorm = 113.5137, GNorm = 0.2861
Meta loss on this task batch = 2.8913e-01, Meta loss averaged over last 500 steps = 2.9887e-01, PNorm = 113.5223, GNorm = 0.2752
Meta loss on this task batch = 2.2139e-01, Meta loss averaged over last 500 steps = 2.9874e-01, PNorm = 113.5302, GNorm = 0.2292
Meta loss on this task batch = 3.7639e-01, Meta loss averaged over last 500 steps = 2.9879e-01, PNorm = 113.5364, GNorm = 0.3091
Meta loss on this task batch = 3.0178e-01, Meta loss averaged over last 500 steps = 2.9869e-01, PNorm = 113.5442, GNorm = 0.3082
Meta loss on this task batch = 2.9648e-01, Meta loss averaged over last 500 steps = 2.9870e-01, PNorm = 113.5519, GNorm = 0.2636
Meta loss on this task batch = 3.3652e-01, Meta loss averaged over last 500 steps = 2.9882e-01, PNorm = 113.5584, GNorm = 0.2634
Meta loss on this task batch = 2.4143e-01, Meta loss averaged over last 500 steps = 2.9879e-01, PNorm = 113.5667, GNorm = 0.2617
Meta loss on this task batch = 2.9807e-01, Meta loss averaged over last 500 steps = 2.9881e-01, PNorm = 113.5752, GNorm = 0.2883
Meta loss on this task batch = 3.4428e-01, Meta loss averaged over last 500 steps = 2.9895e-01, PNorm = 113.5813, GNorm = 0.3011
Meta loss on this task batch = 2.5122e-01, Meta loss averaged over last 500 steps = 2.9891e-01, PNorm = 113.5874, GNorm = 0.2841
Meta loss on this task batch = 2.9731e-01, Meta loss averaged over last 500 steps = 2.9890e-01, PNorm = 113.5926, GNorm = 0.3083
Meta loss on this task batch = 2.9691e-01, Meta loss averaged over last 500 steps = 2.9882e-01, PNorm = 113.5990, GNorm = 0.2945
Meta loss on this task batch = 2.8287e-01, Meta loss averaged over last 500 steps = 2.9879e-01, PNorm = 113.6065, GNorm = 0.2819
Meta loss on this task batch = 3.0303e-01, Meta loss averaged over last 500 steps = 2.9869e-01, PNorm = 113.6117, GNorm = 0.2841
Meta loss on this task batch = 3.0557e-01, Meta loss averaged over last 500 steps = 2.9866e-01, PNorm = 113.6148, GNorm = 0.3525
Took 131.09037280082703 seconds to complete one epoch of meta training
Took 138.2921586036682 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506585
Epoch 412
Meta loss on this task batch = 2.7840e-01, Meta loss averaged over last 500 steps = 2.9871e-01, PNorm = 113.6193, GNorm = 0.2715
Meta loss on this task batch = 3.0398e-01, Meta loss averaged over last 500 steps = 2.9872e-01, PNorm = 113.6242, GNorm = 0.3400
Meta loss on this task batch = 2.9105e-01, Meta loss averaged over last 500 steps = 2.9871e-01, PNorm = 113.6298, GNorm = 0.2505
Meta loss on this task batch = 2.9099e-01, Meta loss averaged over last 500 steps = 2.9861e-01, PNorm = 113.6359, GNorm = 0.2565
Meta loss on this task batch = 3.0245e-01, Meta loss averaged over last 500 steps = 2.9869e-01, PNorm = 113.6387, GNorm = 0.2809
Meta loss on this task batch = 3.1087e-01, Meta loss averaged over last 500 steps = 2.9861e-01, PNorm = 113.6380, GNorm = 0.2856
Meta loss on this task batch = 3.3832e-01, Meta loss averaged over last 500 steps = 2.9870e-01, PNorm = 113.6383, GNorm = 0.2853
Meta loss on this task batch = 3.2325e-01, Meta loss averaged over last 500 steps = 2.9861e-01, PNorm = 113.6384, GNorm = 0.3151
Meta loss on this task batch = 3.1971e-01, Meta loss averaged over last 500 steps = 2.9861e-01, PNorm = 113.6364, GNorm = 0.3062
Meta loss on this task batch = 2.6925e-01, Meta loss averaged over last 500 steps = 2.9859e-01, PNorm = 113.6322, GNorm = 0.2884
Meta loss on this task batch = 2.6716e-01, Meta loss averaged over last 500 steps = 2.9868e-01, PNorm = 113.6290, GNorm = 0.2504
Meta loss on this task batch = 2.6620e-01, Meta loss averaged over last 500 steps = 2.9857e-01, PNorm = 113.6282, GNorm = 0.2771
Meta loss on this task batch = 2.8957e-01, Meta loss averaged over last 500 steps = 2.9854e-01, PNorm = 113.6294, GNorm = 0.2409
Meta loss on this task batch = 2.5891e-01, Meta loss averaged over last 500 steps = 2.9845e-01, PNorm = 113.6330, GNorm = 0.2617
Meta loss on this task batch = 2.9608e-01, Meta loss averaged over last 500 steps = 2.9847e-01, PNorm = 113.6387, GNorm = 0.3039
Meta loss on this task batch = 2.7258e-01, Meta loss averaged over last 500 steps = 2.9832e-01, PNorm = 113.6459, GNorm = 0.2757
Meta loss on this task batch = 2.8209e-01, Meta loss averaged over last 500 steps = 2.9829e-01, PNorm = 113.6576, GNorm = 0.3159
Meta loss on this task batch = 2.8891e-01, Meta loss averaged over last 500 steps = 2.9836e-01, PNorm = 113.6694, GNorm = 0.2810
Meta loss on this task batch = 3.3970e-01, Meta loss averaged over last 500 steps = 2.9843e-01, PNorm = 113.6779, GNorm = 0.3870
Took 131.85237789154053 seconds to complete one epoch of meta training
Took 140.18910813331604 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494348
Epoch 413
Meta loss on this task batch = 3.3644e-01, Meta loss averaged over last 500 steps = 2.9855e-01, PNorm = 113.6858, GNorm = 0.3749
Meta loss on this task batch = 2.6334e-01, Meta loss averaged over last 500 steps = 2.9832e-01, PNorm = 113.6945, GNorm = 0.3086
Meta loss on this task batch = 3.0828e-01, Meta loss averaged over last 500 steps = 2.9840e-01, PNorm = 113.7032, GNorm = 0.3234
Meta loss on this task batch = 3.3906e-01, Meta loss averaged over last 500 steps = 2.9848e-01, PNorm = 113.7112, GNorm = 0.3308
Meta loss on this task batch = 2.1858e-01, Meta loss averaged over last 500 steps = 2.9837e-01, PNorm = 113.7200, GNorm = 0.2336
Meta loss on this task batch = 3.2052e-01, Meta loss averaged over last 500 steps = 2.9839e-01, PNorm = 113.7290, GNorm = 0.3186
Meta loss on this task batch = 2.9606e-01, Meta loss averaged over last 500 steps = 2.9835e-01, PNorm = 113.7372, GNorm = 0.2689
Meta loss on this task batch = 2.9945e-01, Meta loss averaged over last 500 steps = 2.9841e-01, PNorm = 113.7451, GNorm = 0.3582
Meta loss on this task batch = 2.9990e-01, Meta loss averaged over last 500 steps = 2.9839e-01, PNorm = 113.7519, GNorm = 0.2880
Meta loss on this task batch = 2.3233e-01, Meta loss averaged over last 500 steps = 2.9817e-01, PNorm = 113.7587, GNorm = 0.2625
Meta loss on this task batch = 2.7394e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 113.7659, GNorm = 0.2646
Meta loss on this task batch = 3.1979e-01, Meta loss averaged over last 500 steps = 2.9827e-01, PNorm = 113.7720, GNorm = 0.3288
Meta loss on this task batch = 2.3888e-01, Meta loss averaged over last 500 steps = 2.9817e-01, PNorm = 113.7769, GNorm = 0.2787
Meta loss on this task batch = 3.1538e-01, Meta loss averaged over last 500 steps = 2.9819e-01, PNorm = 113.7810, GNorm = 0.3024
Meta loss on this task batch = 3.0254e-01, Meta loss averaged over last 500 steps = 2.9813e-01, PNorm = 113.7840, GNorm = 0.2887
Meta loss on this task batch = 3.3517e-01, Meta loss averaged over last 500 steps = 2.9828e-01, PNorm = 113.7844, GNorm = 0.3232
Meta loss on this task batch = 3.0109e-01, Meta loss averaged over last 500 steps = 2.9833e-01, PNorm = 113.7857, GNorm = 0.3102
Meta loss on this task batch = 2.8860e-01, Meta loss averaged over last 500 steps = 2.9822e-01, PNorm = 113.7874, GNorm = 0.3136
Meta loss on this task batch = 2.8614e-01, Meta loss averaged over last 500 steps = 2.9824e-01, PNorm = 113.7894, GNorm = 0.3414
Took 133.12176060676575 seconds to complete one epoch of meta training
Took 142.19579362869263 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481531
Epoch 414
Meta loss on this task batch = 3.2732e-01, Meta loss averaged over last 500 steps = 2.9827e-01, PNorm = 113.7900, GNorm = 0.3433
Meta loss on this task batch = 3.0244e-01, Meta loss averaged over last 500 steps = 2.9838e-01, PNorm = 113.7909, GNorm = 0.4122
Meta loss on this task batch = 2.5607e-01, Meta loss averaged over last 500 steps = 2.9820e-01, PNorm = 113.7921, GNorm = 0.3077
Meta loss on this task batch = 3.1904e-01, Meta loss averaged over last 500 steps = 2.9828e-01, PNorm = 113.7941, GNorm = 0.3024
Meta loss on this task batch = 2.4212e-01, Meta loss averaged over last 500 steps = 2.9820e-01, PNorm = 113.7988, GNorm = 0.2723
Meta loss on this task batch = 2.8183e-01, Meta loss averaged over last 500 steps = 2.9809e-01, PNorm = 113.8026, GNorm = 0.3135
Meta loss on this task batch = 3.7502e-01, Meta loss averaged over last 500 steps = 2.9825e-01, PNorm = 113.8089, GNorm = 0.3308
Meta loss on this task batch = 2.5190e-01, Meta loss averaged over last 500 steps = 2.9826e-01, PNorm = 113.8146, GNorm = 0.3201
Meta loss on this task batch = 3.2625e-01, Meta loss averaged over last 500 steps = 2.9839e-01, PNorm = 113.8213, GNorm = 0.3049
Meta loss on this task batch = 2.7690e-01, Meta loss averaged over last 500 steps = 2.9833e-01, PNorm = 113.8270, GNorm = 0.2904
Meta loss on this task batch = 2.9317e-01, Meta loss averaged over last 500 steps = 2.9832e-01, PNorm = 113.8320, GNorm = 0.2945
Meta loss on this task batch = 2.7900e-01, Meta loss averaged over last 500 steps = 2.9817e-01, PNorm = 113.8354, GNorm = 0.3616
Meta loss on this task batch = 3.3306e-01, Meta loss averaged over last 500 steps = 2.9833e-01, PNorm = 113.8385, GNorm = 0.3346
Meta loss on this task batch = 3.0590e-01, Meta loss averaged over last 500 steps = 2.9830e-01, PNorm = 113.8417, GNorm = 0.2894
Meta loss on this task batch = 3.1160e-01, Meta loss averaged over last 500 steps = 2.9830e-01, PNorm = 113.8433, GNorm = 0.2934
Meta loss on this task batch = 2.4988e-01, Meta loss averaged over last 500 steps = 2.9823e-01, PNorm = 113.8444, GNorm = 0.3158
Meta loss on this task batch = 2.8887e-01, Meta loss averaged over last 500 steps = 2.9814e-01, PNorm = 113.8461, GNorm = 0.2879
Meta loss on this task batch = 3.0210e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 113.8485, GNorm = 0.3105
Meta loss on this task batch = 3.4066e-01, Meta loss averaged over last 500 steps = 2.9817e-01, PNorm = 113.8529, GNorm = 0.3618
Took 129.94158482551575 seconds to complete one epoch of meta training
Took 137.74723863601685 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496328
Epoch 415
Meta loss on this task batch = 3.1205e-01, Meta loss averaged over last 500 steps = 2.9820e-01, PNorm = 113.8605, GNorm = 0.2751
Meta loss on this task batch = 3.0112e-01, Meta loss averaged over last 500 steps = 2.9817e-01, PNorm = 113.8689, GNorm = 0.3044
Meta loss on this task batch = 2.7560e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 113.8801, GNorm = 0.2954
Meta loss on this task batch = 3.4483e-01, Meta loss averaged over last 500 steps = 2.9836e-01, PNorm = 113.8874, GNorm = 0.3285
Meta loss on this task batch = 2.7486e-01, Meta loss averaged over last 500 steps = 2.9838e-01, PNorm = 113.8945, GNorm = 0.2468
Meta loss on this task batch = 3.3435e-01, Meta loss averaged over last 500 steps = 2.9835e-01, PNorm = 113.9009, GNorm = 0.3357
Meta loss on this task batch = 2.8549e-01, Meta loss averaged over last 500 steps = 2.9826e-01, PNorm = 113.9084, GNorm = 0.2953
Meta loss on this task batch = 3.5148e-01, Meta loss averaged over last 500 steps = 2.9829e-01, PNorm = 113.9159, GNorm = 0.3268
Meta loss on this task batch = 3.1624e-01, Meta loss averaged over last 500 steps = 2.9836e-01, PNorm = 113.9203, GNorm = 0.2844
Meta loss on this task batch = 2.9177e-01, Meta loss averaged over last 500 steps = 2.9839e-01, PNorm = 113.9246, GNorm = 0.2746
Meta loss on this task batch = 2.6464e-01, Meta loss averaged over last 500 steps = 2.9840e-01, PNorm = 113.9303, GNorm = 0.3075
Meta loss on this task batch = 2.7222e-01, Meta loss averaged over last 500 steps = 2.9836e-01, PNorm = 113.9351, GNorm = 0.2747
Meta loss on this task batch = 2.8241e-01, Meta loss averaged over last 500 steps = 2.9844e-01, PNorm = 113.9380, GNorm = 0.2769
Meta loss on this task batch = 3.1237e-01, Meta loss averaged over last 500 steps = 2.9836e-01, PNorm = 113.9378, GNorm = 0.4317
Meta loss on this task batch = 3.1532e-01, Meta loss averaged over last 500 steps = 2.9842e-01, PNorm = 113.9361, GNorm = 0.2960
Meta loss on this task batch = 2.8041e-01, Meta loss averaged over last 500 steps = 2.9840e-01, PNorm = 113.9331, GNorm = 0.3158
Meta loss on this task batch = 2.9850e-01, Meta loss averaged over last 500 steps = 2.9846e-01, PNorm = 113.9322, GNorm = 0.3013
Meta loss on this task batch = 3.2372e-01, Meta loss averaged over last 500 steps = 2.9840e-01, PNorm = 113.9297, GNorm = 0.3222
Meta loss on this task batch = 3.0977e-01, Meta loss averaged over last 500 steps = 2.9838e-01, PNorm = 113.9299, GNorm = 0.3138
Took 134.00833129882812 seconds to complete one epoch of meta training
Took 142.72373223304749 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493486
Epoch 416
Meta loss on this task batch = 3.0045e-01, Meta loss averaged over last 500 steps = 2.9841e-01, PNorm = 113.9302, GNorm = 0.3283
Meta loss on this task batch = 2.6481e-01, Meta loss averaged over last 500 steps = 2.9832e-01, PNorm = 113.9332, GNorm = 0.2701
Meta loss on this task batch = 3.0972e-01, Meta loss averaged over last 500 steps = 2.9840e-01, PNorm = 113.9381, GNorm = 0.2771
Meta loss on this task batch = 3.2242e-01, Meta loss averaged over last 500 steps = 2.9848e-01, PNorm = 113.9465, GNorm = 0.2787
Meta loss on this task batch = 3.4637e-01, Meta loss averaged over last 500 steps = 2.9845e-01, PNorm = 113.9585, GNorm = 0.3110
Meta loss on this task batch = 3.1068e-01, Meta loss averaged over last 500 steps = 2.9840e-01, PNorm = 113.9700, GNorm = 0.2682
Meta loss on this task batch = 2.3752e-01, Meta loss averaged over last 500 steps = 2.9827e-01, PNorm = 113.9829, GNorm = 0.2833
Meta loss on this task batch = 2.9218e-01, Meta loss averaged over last 500 steps = 2.9825e-01, PNorm = 113.9925, GNorm = 0.2950
Meta loss on this task batch = 2.6035e-01, Meta loss averaged over last 500 steps = 2.9815e-01, PNorm = 114.0011, GNorm = 0.2855
Meta loss on this task batch = 2.9715e-01, Meta loss averaged over last 500 steps = 2.9826e-01, PNorm = 114.0096, GNorm = 0.3152
Meta loss on this task batch = 2.8826e-01, Meta loss averaged over last 500 steps = 2.9831e-01, PNorm = 114.0175, GNorm = 0.2964
Meta loss on this task batch = 2.3509e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 114.0255, GNorm = 0.2939
Meta loss on this task batch = 2.5908e-01, Meta loss averaged over last 500 steps = 2.9811e-01, PNorm = 114.0322, GNorm = 0.2901
Meta loss on this task batch = 2.8759e-01, Meta loss averaged over last 500 steps = 2.9810e-01, PNorm = 114.0389, GNorm = 0.3317
Meta loss on this task batch = 2.9702e-01, Meta loss averaged over last 500 steps = 2.9811e-01, PNorm = 114.0420, GNorm = 0.3000
Meta loss on this task batch = 2.9267e-01, Meta loss averaged over last 500 steps = 2.9822e-01, PNorm = 114.0422, GNorm = 0.2903
Meta loss on this task batch = 2.9716e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 114.0412, GNorm = 0.3186
Meta loss on this task batch = 3.5460e-01, Meta loss averaged over last 500 steps = 2.9826e-01, PNorm = 114.0413, GNorm = 0.3395
Meta loss on this task batch = 3.3935e-01, Meta loss averaged over last 500 steps = 2.9823e-01, PNorm = 114.0420, GNorm = 0.3571
Took 133.87607264518738 seconds to complete one epoch of meta training
Took 142.29191136360168 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494433
Epoch 417
Meta loss on this task batch = 3.3781e-01, Meta loss averaged over last 500 steps = 2.9828e-01, PNorm = 114.0440, GNorm = 0.3133
Meta loss on this task batch = 3.7698e-01, Meta loss averaged over last 500 steps = 2.9844e-01, PNorm = 114.0515, GNorm = 0.3815
Meta loss on this task batch = 3.2206e-01, Meta loss averaged over last 500 steps = 2.9853e-01, PNorm = 114.0593, GNorm = 0.2781
Meta loss on this task batch = 3.0879e-01, Meta loss averaged over last 500 steps = 2.9850e-01, PNorm = 114.0699, GNorm = 0.2978
Meta loss on this task batch = 2.8632e-01, Meta loss averaged over last 500 steps = 2.9842e-01, PNorm = 114.0818, GNorm = 0.2751
Meta loss on this task batch = 2.5863e-01, Meta loss averaged over last 500 steps = 2.9838e-01, PNorm = 114.0969, GNorm = 0.2863
Meta loss on this task batch = 3.4931e-01, Meta loss averaged over last 500 steps = 2.9838e-01, PNorm = 114.1107, GNorm = 0.3383
Meta loss on this task batch = 2.7640e-01, Meta loss averaged over last 500 steps = 2.9842e-01, PNorm = 114.1242, GNorm = 0.2599
Meta loss on this task batch = 2.8974e-01, Meta loss averaged over last 500 steps = 2.9832e-01, PNorm = 114.1383, GNorm = 0.2488
Meta loss on this task batch = 2.7574e-01, Meta loss averaged over last 500 steps = 2.9813e-01, PNorm = 114.1503, GNorm = 0.2861
Meta loss on this task batch = 3.0153e-01, Meta loss averaged over last 500 steps = 2.9816e-01, PNorm = 114.1589, GNorm = 0.3279
Meta loss on this task batch = 2.6321e-01, Meta loss averaged over last 500 steps = 2.9822e-01, PNorm = 114.1648, GNorm = 0.3145
Meta loss on this task batch = 2.5877e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 114.1679, GNorm = 0.3028
Meta loss on this task batch = 2.5668e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 114.1707, GNorm = 0.2586
Meta loss on this task batch = 2.9090e-01, Meta loss averaged over last 500 steps = 2.9817e-01, PNorm = 114.1727, GNorm = 0.3042
Meta loss on this task batch = 3.0043e-01, Meta loss averaged over last 500 steps = 2.9828e-01, PNorm = 114.1740, GNorm = 0.2918
Meta loss on this task batch = 2.8477e-01, Meta loss averaged over last 500 steps = 2.9820e-01, PNorm = 114.1759, GNorm = 0.2888
Meta loss on this task batch = 2.6428e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 114.1802, GNorm = 0.2848
Meta loss on this task batch = 2.8300e-01, Meta loss averaged over last 500 steps = 2.9809e-01, PNorm = 114.1832, GNorm = 0.3498
Took 133.35489106178284 seconds to complete one epoch of meta training
Took 140.75547623634338 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499497
Epoch 418
Meta loss on this task batch = 3.4782e-01, Meta loss averaged over last 500 steps = 2.9827e-01, PNorm = 114.1850, GNorm = 0.3304
Meta loss on this task batch = 2.9390e-01, Meta loss averaged over last 500 steps = 2.9837e-01, PNorm = 114.1878, GNorm = 0.2975
Meta loss on this task batch = 2.8708e-01, Meta loss averaged over last 500 steps = 2.9827e-01, PNorm = 114.1920, GNorm = 0.2605
Meta loss on this task batch = 3.3593e-01, Meta loss averaged over last 500 steps = 2.9825e-01, PNorm = 114.1990, GNorm = 0.3097
Meta loss on this task batch = 2.9519e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 114.2073, GNorm = 0.2827
Meta loss on this task batch = 2.5401e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 114.2156, GNorm = 0.2739
Meta loss on this task batch = 3.1804e-01, Meta loss averaged over last 500 steps = 2.9804e-01, PNorm = 114.2246, GNorm = 0.2941
Meta loss on this task batch = 2.8020e-01, Meta loss averaged over last 500 steps = 2.9804e-01, PNorm = 114.2352, GNorm = 0.2632
Meta loss on this task batch = 3.0356e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 114.2432, GNorm = 0.2762
Meta loss on this task batch = 2.8249e-01, Meta loss averaged over last 500 steps = 2.9810e-01, PNorm = 114.2536, GNorm = 0.2828
Meta loss on this task batch = 2.8533e-01, Meta loss averaged over last 500 steps = 2.9809e-01, PNorm = 114.2593, GNorm = 0.3741
Meta loss on this task batch = 2.8228e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 114.2656, GNorm = 0.2882
Meta loss on this task batch = 2.9757e-01, Meta loss averaged over last 500 steps = 2.9804e-01, PNorm = 114.2684, GNorm = 0.3123
Meta loss on this task batch = 2.5689e-01, Meta loss averaged over last 500 steps = 2.9788e-01, PNorm = 114.2730, GNorm = 0.2722
Meta loss on this task batch = 2.8457e-01, Meta loss averaged over last 500 steps = 2.9776e-01, PNorm = 114.2775, GNorm = 0.3248
Meta loss on this task batch = 3.5565e-01, Meta loss averaged over last 500 steps = 2.9783e-01, PNorm = 114.2828, GNorm = 0.3016
Meta loss on this task batch = 3.8072e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 114.2883, GNorm = 0.3067
Meta loss on this task batch = 2.9858e-01, Meta loss averaged over last 500 steps = 2.9794e-01, PNorm = 114.2912, GNorm = 0.2730
Meta loss on this task batch = 2.9314e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 114.2926, GNorm = 0.3268
Took 134.36435723304749 seconds to complete one epoch of meta training
Took 142.61616468429565 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513199
Epoch 419
Meta loss on this task batch = 3.2230e-01, Meta loss averaged over last 500 steps = 2.9810e-01, PNorm = 114.2959, GNorm = 0.3131
Meta loss on this task batch = 2.7527e-01, Meta loss averaged over last 500 steps = 2.9802e-01, PNorm = 114.3010, GNorm = 0.2659
Meta loss on this task batch = 3.2003e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 114.3047, GNorm = 0.2990
Meta loss on this task batch = 2.6202e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 114.3080, GNorm = 0.2482
Meta loss on this task batch = 2.7606e-01, Meta loss averaged over last 500 steps = 2.9794e-01, PNorm = 114.3118, GNorm = 0.2618
Meta loss on this task batch = 2.8245e-01, Meta loss averaged over last 500 steps = 2.9791e-01, PNorm = 114.3155, GNorm = 0.3216
Meta loss on this task batch = 2.5489e-01, Meta loss averaged over last 500 steps = 2.9785e-01, PNorm = 114.3194, GNorm = 0.2573
Meta loss on this task batch = 3.5599e-01, Meta loss averaged over last 500 steps = 2.9796e-01, PNorm = 114.3231, GNorm = 0.2915
Meta loss on this task batch = 3.2235e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 114.3243, GNorm = 0.2822
Meta loss on this task batch = 3.2388e-01, Meta loss averaged over last 500 steps = 2.9801e-01, PNorm = 114.3261, GNorm = 0.3110
Meta loss on this task batch = 2.8276e-01, Meta loss averaged over last 500 steps = 2.9790e-01, PNorm = 114.3295, GNorm = 0.3255
Meta loss on this task batch = 3.4991e-01, Meta loss averaged over last 500 steps = 2.9803e-01, PNorm = 114.3382, GNorm = 0.3287
Meta loss on this task batch = 2.8378e-01, Meta loss averaged over last 500 steps = 2.9810e-01, PNorm = 114.3452, GNorm = 0.2495
Meta loss on this task batch = 3.6065e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 114.3501, GNorm = 0.2781
Meta loss on this task batch = 2.5409e-01, Meta loss averaged over last 500 steps = 2.9811e-01, PNorm = 114.3545, GNorm = 0.2772
Meta loss on this task batch = 3.6320e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 114.3581, GNorm = 0.3638
Meta loss on this task batch = 3.4984e-01, Meta loss averaged over last 500 steps = 2.9832e-01, PNorm = 114.3620, GNorm = 0.3128
Meta loss on this task batch = 2.5022e-01, Meta loss averaged over last 500 steps = 2.9830e-01, PNorm = 114.3668, GNorm = 0.2809
Meta loss on this task batch = 3.4461e-01, Meta loss averaged over last 500 steps = 2.9836e-01, PNorm = 114.3733, GNorm = 0.3638
Took 134.64769887924194 seconds to complete one epoch of meta training
Took 143.32332634925842 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491401
Epoch 420
Meta loss on this task batch = 3.1833e-01, Meta loss averaged over last 500 steps = 2.9857e-01, PNorm = 114.3776, GNorm = 0.2913
Meta loss on this task batch = 2.4631e-01, Meta loss averaged over last 500 steps = 2.9840e-01, PNorm = 114.3831, GNorm = 0.2553
Meta loss on this task batch = 2.8291e-01, Meta loss averaged over last 500 steps = 2.9835e-01, PNorm = 114.3891, GNorm = 0.2843
Meta loss on this task batch = 2.7915e-01, Meta loss averaged over last 500 steps = 2.9820e-01, PNorm = 114.3985, GNorm = 0.2779
Meta loss on this task batch = 3.0282e-01, Meta loss averaged over last 500 steps = 2.9819e-01, PNorm = 114.4072, GNorm = 0.3163
Meta loss on this task batch = 2.8319e-01, Meta loss averaged over last 500 steps = 2.9827e-01, PNorm = 114.4156, GNorm = 0.3109
Meta loss on this task batch = 2.4587e-01, Meta loss averaged over last 500 steps = 2.9809e-01, PNorm = 114.4241, GNorm = 0.2908
Meta loss on this task batch = 3.3434e-01, Meta loss averaged over last 500 steps = 2.9827e-01, PNorm = 114.4334, GNorm = 0.3386
Meta loss on this task batch = 2.8085e-01, Meta loss averaged over last 500 steps = 2.9825e-01, PNorm = 114.4408, GNorm = 0.3026
Meta loss on this task batch = 3.3371e-01, Meta loss averaged over last 500 steps = 2.9837e-01, PNorm = 114.4497, GNorm = 0.3432
Meta loss on this task batch = 2.7555e-01, Meta loss averaged over last 500 steps = 2.9834e-01, PNorm = 114.4571, GNorm = 0.3087
Meta loss on this task batch = 2.6293e-01, Meta loss averaged over last 500 steps = 2.9807e-01, PNorm = 114.4638, GNorm = 0.2490
Meta loss on this task batch = 2.8792e-01, Meta loss averaged over last 500 steps = 2.9801e-01, PNorm = 114.4707, GNorm = 0.2790
Meta loss on this task batch = 3.5140e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 114.4764, GNorm = 0.2931
Meta loss on this task batch = 3.5885e-01, Meta loss averaged over last 500 steps = 2.9830e-01, PNorm = 114.4788, GNorm = 0.3752
Meta loss on this task batch = 2.8083e-01, Meta loss averaged over last 500 steps = 2.9829e-01, PNorm = 114.4806, GNorm = 0.2535
Meta loss on this task batch = 2.6967e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 114.4819, GNorm = 0.2673
Meta loss on this task batch = 2.7100e-01, Meta loss averaged over last 500 steps = 2.9830e-01, PNorm = 114.4864, GNorm = 0.2786
Meta loss on this task batch = 2.7366e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 114.4904, GNorm = 0.3716
Took 134.35026025772095 seconds to complete one epoch of meta training
Took 142.7983295917511 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479156
Epoch 421
Meta loss on this task batch = 2.2263e-01, Meta loss averaged over last 500 steps = 2.9795e-01, PNorm = 114.4948, GNorm = 0.2344
Meta loss on this task batch = 2.9347e-01, Meta loss averaged over last 500 steps = 2.9803e-01, PNorm = 114.4992, GNorm = 0.2785
Meta loss on this task batch = 3.0479e-01, Meta loss averaged over last 500 steps = 2.9807e-01, PNorm = 114.5019, GNorm = 0.2870
Meta loss on this task batch = 3.6643e-01, Meta loss averaged over last 500 steps = 2.9820e-01, PNorm = 114.5024, GNorm = 0.3075
Meta loss on this task batch = 2.5302e-01, Meta loss averaged over last 500 steps = 2.9804e-01, PNorm = 114.5025, GNorm = 0.2372
Meta loss on this task batch = 3.2341e-01, Meta loss averaged over last 500 steps = 2.9810e-01, PNorm = 114.5031, GNorm = 0.2938
Meta loss on this task batch = 2.5571e-01, Meta loss averaged over last 500 steps = 2.9816e-01, PNorm = 114.5059, GNorm = 0.2916
Meta loss on this task batch = 2.6533e-01, Meta loss averaged over last 500 steps = 2.9815e-01, PNorm = 114.5092, GNorm = 0.2883
Meta loss on this task batch = 2.6295e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 114.5138, GNorm = 0.2636
Meta loss on this task batch = 2.9554e-01, Meta loss averaged over last 500 steps = 2.9809e-01, PNorm = 114.5185, GNorm = 0.3258
Meta loss on this task batch = 2.6783e-01, Meta loss averaged over last 500 steps = 2.9805e-01, PNorm = 114.5253, GNorm = 0.2458
Meta loss on this task batch = 3.2402e-01, Meta loss averaged over last 500 steps = 2.9803e-01, PNorm = 114.5347, GNorm = 0.3282
Meta loss on this task batch = 2.9244e-01, Meta loss averaged over last 500 steps = 2.9805e-01, PNorm = 114.5410, GNorm = 0.3402
Meta loss on this task batch = 3.2028e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 114.5492, GNorm = 0.2870
Meta loss on this task batch = 3.1303e-01, Meta loss averaged over last 500 steps = 2.9796e-01, PNorm = 114.5557, GNorm = 0.3201
Meta loss on this task batch = 2.5127e-01, Meta loss averaged over last 500 steps = 2.9792e-01, PNorm = 114.5615, GNorm = 0.2909
Meta loss on this task batch = 3.0698e-01, Meta loss averaged over last 500 steps = 2.9794e-01, PNorm = 114.5649, GNorm = 0.2745
Meta loss on this task batch = 3.1198e-01, Meta loss averaged over last 500 steps = 2.9792e-01, PNorm = 114.5653, GNorm = 0.3225
Meta loss on this task batch = 3.4050e-01, Meta loss averaged over last 500 steps = 2.9796e-01, PNorm = 114.5662, GNorm = 0.3932
Took 133.74178981781006 seconds to complete one epoch of meta training
Took 142.29374194145203 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464169
Epoch 422
Meta loss on this task batch = 3.0861e-01, Meta loss averaged over last 500 steps = 2.9794e-01, PNorm = 114.5651, GNorm = 0.3540
Meta loss on this task batch = 2.3865e-01, Meta loss averaged over last 500 steps = 2.9787e-01, PNorm = 114.5631, GNorm = 0.2415
Meta loss on this task batch = 3.5759e-01, Meta loss averaged over last 500 steps = 2.9788e-01, PNorm = 114.5613, GNorm = 0.3099
Meta loss on this task batch = 2.7559e-01, Meta loss averaged over last 500 steps = 2.9786e-01, PNorm = 114.5605, GNorm = 0.2593
Meta loss on this task batch = 2.6451e-01, Meta loss averaged over last 500 steps = 2.9781e-01, PNorm = 114.5613, GNorm = 0.2810
Meta loss on this task batch = 3.0473e-01, Meta loss averaged over last 500 steps = 2.9790e-01, PNorm = 114.5639, GNorm = 0.2482
Meta loss on this task batch = 3.4271e-01, Meta loss averaged over last 500 steps = 2.9792e-01, PNorm = 114.5682, GNorm = 0.2650
Meta loss on this task batch = 3.0499e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 114.5752, GNorm = 0.2886
Meta loss on this task batch = 3.3591e-01, Meta loss averaged over last 500 steps = 2.9803e-01, PNorm = 114.5848, GNorm = 0.3108
Meta loss on this task batch = 2.9816e-01, Meta loss averaged over last 500 steps = 2.9797e-01, PNorm = 114.5957, GNorm = 0.2503
Meta loss on this task batch = 2.7444e-01, Meta loss averaged over last 500 steps = 2.9794e-01, PNorm = 114.6105, GNorm = 0.2650
Meta loss on this task batch = 2.5059e-01, Meta loss averaged over last 500 steps = 2.9783e-01, PNorm = 114.6253, GNorm = 0.2347
Meta loss on this task batch = 2.6782e-01, Meta loss averaged over last 500 steps = 2.9777e-01, PNorm = 114.6388, GNorm = 0.2473
Meta loss on this task batch = 3.1037e-01, Meta loss averaged over last 500 steps = 2.9767e-01, PNorm = 114.6495, GNorm = 0.2697
Meta loss on this task batch = 3.0853e-01, Meta loss averaged over last 500 steps = 2.9767e-01, PNorm = 114.6566, GNorm = 0.2800
Meta loss on this task batch = 2.9289e-01, Meta loss averaged over last 500 steps = 2.9773e-01, PNorm = 114.6627, GNorm = 0.2707
Meta loss on this task batch = 2.5187e-01, Meta loss averaged over last 500 steps = 2.9776e-01, PNorm = 114.6674, GNorm = 0.2449
Meta loss on this task batch = 2.7724e-01, Meta loss averaged over last 500 steps = 2.9774e-01, PNorm = 114.6708, GNorm = 0.2762
Meta loss on this task batch = 3.3287e-01, Meta loss averaged over last 500 steps = 2.9783e-01, PNorm = 114.6693, GNorm = 0.4205
Took 133.82289671897888 seconds to complete one epoch of meta training
Took 142.15517735481262 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508911
Epoch 423
Meta loss on this task batch = 3.4528e-01, Meta loss averaged over last 500 steps = 2.9791e-01, PNorm = 114.6661, GNorm = 0.3351
Meta loss on this task batch = 3.2989e-01, Meta loss averaged over last 500 steps = 2.9803e-01, PNorm = 114.6626, GNorm = 0.3185
Meta loss on this task batch = 2.4134e-01, Meta loss averaged over last 500 steps = 2.9790e-01, PNorm = 114.6605, GNorm = 0.2718
Meta loss on this task batch = 2.6491e-01, Meta loss averaged over last 500 steps = 2.9769e-01, PNorm = 114.6588, GNorm = 0.2830
Meta loss on this task batch = 2.5181e-01, Meta loss averaged over last 500 steps = 2.9765e-01, PNorm = 114.6571, GNorm = 0.2799
Meta loss on this task batch = 3.0475e-01, Meta loss averaged over last 500 steps = 2.9767e-01, PNorm = 114.6576, GNorm = 0.3169
Meta loss on this task batch = 2.8266e-01, Meta loss averaged over last 500 steps = 2.9762e-01, PNorm = 114.6575, GNorm = 0.2926
Meta loss on this task batch = 2.6934e-01, Meta loss averaged over last 500 steps = 2.9755e-01, PNorm = 114.6604, GNorm = 0.2680
Meta loss on this task batch = 3.0037e-01, Meta loss averaged over last 500 steps = 2.9762e-01, PNorm = 114.6639, GNorm = 0.3405
Meta loss on this task batch = 2.8192e-01, Meta loss averaged over last 500 steps = 2.9764e-01, PNorm = 114.6686, GNorm = 0.2599
Meta loss on this task batch = 2.7751e-01, Meta loss averaged over last 500 steps = 2.9759e-01, PNorm = 114.6749, GNorm = 0.2863
Meta loss on this task batch = 2.9393e-01, Meta loss averaged over last 500 steps = 2.9754e-01, PNorm = 114.6835, GNorm = 0.2783
Meta loss on this task batch = 3.1055e-01, Meta loss averaged over last 500 steps = 2.9771e-01, PNorm = 114.6924, GNorm = 0.3218
Meta loss on this task batch = 3.3158e-01, Meta loss averaged over last 500 steps = 2.9766e-01, PNorm = 114.7024, GNorm = 0.2800
Meta loss on this task batch = 3.5541e-01, Meta loss averaged over last 500 steps = 2.9778e-01, PNorm = 114.7119, GNorm = 0.3477
Meta loss on this task batch = 3.1882e-01, Meta loss averaged over last 500 steps = 2.9780e-01, PNorm = 114.7208, GNorm = 0.3000
Meta loss on this task batch = 2.6205e-01, Meta loss averaged over last 500 steps = 2.9757e-01, PNorm = 114.7295, GNorm = 0.2576
Meta loss on this task batch = 3.3408e-01, Meta loss averaged over last 500 steps = 2.9770e-01, PNorm = 114.7361, GNorm = 0.3580
Meta loss on this task batch = 3.1154e-01, Meta loss averaged over last 500 steps = 2.9773e-01, PNorm = 114.7388, GNorm = 0.3300
Took 135.43660163879395 seconds to complete one epoch of meta training
Took 144.2272753715515 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472199
Epoch 424
Meta loss on this task batch = 3.1816e-01, Meta loss averaged over last 500 steps = 2.9765e-01, PNorm = 114.7403, GNorm = 0.3200
Meta loss on this task batch = 2.4192e-01, Meta loss averaged over last 500 steps = 2.9762e-01, PNorm = 114.7425, GNorm = 0.2447
Meta loss on this task batch = 2.5545e-01, Meta loss averaged over last 500 steps = 2.9747e-01, PNorm = 114.7420, GNorm = 0.2338
Meta loss on this task batch = 3.0337e-01, Meta loss averaged over last 500 steps = 2.9752e-01, PNorm = 114.7395, GNorm = 0.2669
Meta loss on this task batch = 3.1189e-01, Meta loss averaged over last 500 steps = 2.9755e-01, PNorm = 114.7374, GNorm = 0.2864
Meta loss on this task batch = 3.6295e-01, Meta loss averaged over last 500 steps = 2.9780e-01, PNorm = 114.7365, GNorm = 0.3188
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 2.9776e-01, PNorm = 114.7356, GNorm = 0.2848
Meta loss on this task batch = 2.9779e-01, Meta loss averaged over last 500 steps = 2.9770e-01, PNorm = 114.7372, GNorm = 0.2958
Meta loss on this task batch = 3.2459e-01, Meta loss averaged over last 500 steps = 2.9768e-01, PNorm = 114.7416, GNorm = 0.2863
Meta loss on this task batch = 3.1381e-01, Meta loss averaged over last 500 steps = 2.9778e-01, PNorm = 114.7504, GNorm = 0.2988
Meta loss on this task batch = 3.1754e-01, Meta loss averaged over last 500 steps = 2.9787e-01, PNorm = 114.7627, GNorm = 0.3287
Meta loss on this task batch = 3.0856e-01, Meta loss averaged over last 500 steps = 2.9791e-01, PNorm = 114.7762, GNorm = 0.2746
Meta loss on this task batch = 2.3851e-01, Meta loss averaged over last 500 steps = 2.9788e-01, PNorm = 114.7899, GNorm = 0.2900
Meta loss on this task batch = 3.4084e-01, Meta loss averaged over last 500 steps = 2.9790e-01, PNorm = 114.8019, GNorm = 0.3128
Meta loss on this task batch = 2.9834e-01, Meta loss averaged over last 500 steps = 2.9794e-01, PNorm = 114.8119, GNorm = 0.2985
Meta loss on this task batch = 3.2057e-01, Meta loss averaged over last 500 steps = 2.9807e-01, PNorm = 114.8199, GNorm = 0.3276
Meta loss on this task batch = 2.6392e-01, Meta loss averaged over last 500 steps = 2.9804e-01, PNorm = 114.8253, GNorm = 0.2711
Meta loss on this task batch = 2.9653e-01, Meta loss averaged over last 500 steps = 2.9815e-01, PNorm = 114.8300, GNorm = 0.2936
Meta loss on this task batch = 3.5542e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 114.8302, GNorm = 0.4535
Took 133.0039026737213 seconds to complete one epoch of meta training
Took 141.1300983428955 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498608
Epoch 425
Meta loss on this task batch = 2.7451e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 114.8302, GNorm = 0.2742
Meta loss on this task batch = 2.3222e-01, Meta loss averaged over last 500 steps = 2.9782e-01, PNorm = 114.8330, GNorm = 0.2585
Meta loss on this task batch = 3.8465e-01, Meta loss averaged over last 500 steps = 2.9796e-01, PNorm = 114.8372, GNorm = 0.3355
Meta loss on this task batch = 3.1554e-01, Meta loss averaged over last 500 steps = 2.9803e-01, PNorm = 114.8417, GNorm = 0.3019
Meta loss on this task batch = 3.1708e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 114.8468, GNorm = 0.3266
Meta loss on this task batch = 3.0757e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 114.8525, GNorm = 0.2539
Meta loss on this task batch = 2.7196e-01, Meta loss averaged over last 500 steps = 2.9819e-01, PNorm = 114.8576, GNorm = 0.2866
Meta loss on this task batch = 2.9024e-01, Meta loss averaged over last 500 steps = 2.9810e-01, PNorm = 114.8637, GNorm = 0.3043
Meta loss on this task batch = 2.8382e-01, Meta loss averaged over last 500 steps = 2.9816e-01, PNorm = 114.8708, GNorm = 0.2705
Meta loss on this task batch = 2.6107e-01, Meta loss averaged over last 500 steps = 2.9806e-01, PNorm = 114.8775, GNorm = 0.2606
Meta loss on this task batch = 2.6076e-01, Meta loss averaged over last 500 steps = 2.9810e-01, PNorm = 114.8838, GNorm = 0.2461
Meta loss on this task batch = 2.3627e-01, Meta loss averaged over last 500 steps = 2.9783e-01, PNorm = 114.8899, GNorm = 0.2257
Meta loss on this task batch = 3.6622e-01, Meta loss averaged over last 500 steps = 2.9796e-01, PNorm = 114.8910, GNorm = 0.4058
Meta loss on this task batch = 2.6587e-01, Meta loss averaged over last 500 steps = 2.9796e-01, PNorm = 114.8935, GNorm = 0.2850
Meta loss on this task batch = 2.6264e-01, Meta loss averaged over last 500 steps = 2.9789e-01, PNorm = 114.8956, GNorm = 0.2963
Meta loss on this task batch = 2.2858e-01, Meta loss averaged over last 500 steps = 2.9775e-01, PNorm = 114.8979, GNorm = 0.2436
Meta loss on this task batch = 2.7487e-01, Meta loss averaged over last 500 steps = 2.9767e-01, PNorm = 114.9013, GNorm = 0.3033
Meta loss on this task batch = 2.6747e-01, Meta loss averaged over last 500 steps = 2.9758e-01, PNorm = 114.9056, GNorm = 0.2311
Meta loss on this task batch = 3.2464e-01, Meta loss averaged over last 500 steps = 2.9763e-01, PNorm = 114.9089, GNorm = 0.3607
Took 134.94768357276917 seconds to complete one epoch of meta training
Took 143.02897810935974 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479218
Epoch 426
Meta loss on this task batch = 3.0266e-01, Meta loss averaged over last 500 steps = 2.9755e-01, PNorm = 114.9130, GNorm = 0.3152
Meta loss on this task batch = 3.0681e-01, Meta loss averaged over last 500 steps = 2.9768e-01, PNorm = 114.9166, GNorm = 0.2818
Meta loss on this task batch = 2.6858e-01, Meta loss averaged over last 500 steps = 2.9752e-01, PNorm = 114.9212, GNorm = 0.2934
Meta loss on this task batch = 3.1171e-01, Meta loss averaged over last 500 steps = 2.9759e-01, PNorm = 114.9245, GNorm = 0.3076
Meta loss on this task batch = 2.2634e-01, Meta loss averaged over last 500 steps = 2.9746e-01, PNorm = 114.9283, GNorm = 0.2447
Meta loss on this task batch = 2.2625e-01, Meta loss averaged over last 500 steps = 2.9729e-01, PNorm = 114.9310, GNorm = 0.2532
Meta loss on this task batch = 3.0248e-01, Meta loss averaged over last 500 steps = 2.9730e-01, PNorm = 114.9339, GNorm = 0.3057
Meta loss on this task batch = 2.8157e-01, Meta loss averaged over last 500 steps = 2.9727e-01, PNorm = 114.9389, GNorm = 0.3135
Meta loss on this task batch = 3.1225e-01, Meta loss averaged over last 500 steps = 2.9730e-01, PNorm = 114.9450, GNorm = 0.3311
Meta loss on this task batch = 2.6415e-01, Meta loss averaged over last 500 steps = 2.9712e-01, PNorm = 114.9538, GNorm = 0.2741
Meta loss on this task batch = 2.9832e-01, Meta loss averaged over last 500 steps = 2.9717e-01, PNorm = 114.9619, GNorm = 0.3101
Meta loss on this task batch = 2.6675e-01, Meta loss averaged over last 500 steps = 2.9709e-01, PNorm = 114.9691, GNorm = 0.3302
Meta loss on this task batch = 3.0933e-01, Meta loss averaged over last 500 steps = 2.9707e-01, PNorm = 114.9742, GNorm = 0.3332
Meta loss on this task batch = 3.5048e-01, Meta loss averaged over last 500 steps = 2.9706e-01, PNorm = 114.9763, GNorm = 0.3791
Meta loss on this task batch = 2.9392e-01, Meta loss averaged over last 500 steps = 2.9707e-01, PNorm = 114.9782, GNorm = 0.2726
Meta loss on this task batch = 3.0768e-01, Meta loss averaged over last 500 steps = 2.9712e-01, PNorm = 114.9774, GNorm = 0.3283
Meta loss on this task batch = 3.0175e-01, Meta loss averaged over last 500 steps = 2.9705e-01, PNorm = 114.9764, GNorm = 0.3058
Meta loss on this task batch = 2.8047e-01, Meta loss averaged over last 500 steps = 2.9707e-01, PNorm = 114.9775, GNorm = 0.2598
Meta loss on this task batch = 2.8549e-01, Meta loss averaged over last 500 steps = 2.9711e-01, PNorm = 114.9816, GNorm = 0.3683
Took 136.40670728683472 seconds to complete one epoch of meta training
Took 144.24140620231628 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496245
Epoch 427
Meta loss on this task batch = 2.4208e-01, Meta loss averaged over last 500 steps = 2.9701e-01, PNorm = 114.9884, GNorm = 0.2639
Meta loss on this task batch = 2.9889e-01, Meta loss averaged over last 500 steps = 2.9706e-01, PNorm = 114.9954, GNorm = 0.3068
Meta loss on this task batch = 2.8970e-01, Meta loss averaged over last 500 steps = 2.9713e-01, PNorm = 115.0024, GNorm = 0.3016
Meta loss on this task batch = 3.2080e-01, Meta loss averaged over last 500 steps = 2.9723e-01, PNorm = 115.0107, GNorm = 0.2988
Meta loss on this task batch = 3.0796e-01, Meta loss averaged over last 500 steps = 2.9723e-01, PNorm = 115.0183, GNorm = 0.2850
Meta loss on this task batch = 2.9017e-01, Meta loss averaged over last 500 steps = 2.9712e-01, PNorm = 115.0277, GNorm = 0.3053
Meta loss on this task batch = 2.8580e-01, Meta loss averaged over last 500 steps = 2.9716e-01, PNorm = 115.0378, GNorm = 0.3164
Meta loss on this task batch = 3.0012e-01, Meta loss averaged over last 500 steps = 2.9717e-01, PNorm = 115.0487, GNorm = 0.3162
Meta loss on this task batch = 2.8382e-01, Meta loss averaged over last 500 steps = 2.9714e-01, PNorm = 115.0589, GNorm = 0.3050
Meta loss on this task batch = 3.0960e-01, Meta loss averaged over last 500 steps = 2.9727e-01, PNorm = 115.0696, GNorm = 0.2737
Meta loss on this task batch = 2.2065e-01, Meta loss averaged over last 500 steps = 2.9705e-01, PNorm = 115.0783, GNorm = 0.2711
Meta loss on this task batch = 2.7410e-01, Meta loss averaged over last 500 steps = 2.9702e-01, PNorm = 115.0859, GNorm = 0.2887
Meta loss on this task batch = 3.4060e-01, Meta loss averaged over last 500 steps = 2.9701e-01, PNorm = 115.0896, GNorm = 0.3177
Meta loss on this task batch = 2.8861e-01, Meta loss averaged over last 500 steps = 2.9701e-01, PNorm = 115.0894, GNorm = 0.3171
Meta loss on this task batch = 3.3587e-01, Meta loss averaged over last 500 steps = 2.9708e-01, PNorm = 115.0894, GNorm = 0.2984
Meta loss on this task batch = 3.0426e-01, Meta loss averaged over last 500 steps = 2.9704e-01, PNorm = 115.0895, GNorm = 0.3062
Meta loss on this task batch = 2.8239e-01, Meta loss averaged over last 500 steps = 2.9706e-01, PNorm = 115.0898, GNorm = 0.2756
Meta loss on this task batch = 2.5603e-01, Meta loss averaged over last 500 steps = 2.9705e-01, PNorm = 115.0911, GNorm = 0.2603
Meta loss on this task batch = 3.1555e-01, Meta loss averaged over last 500 steps = 2.9714e-01, PNorm = 115.0931, GNorm = 0.3866
Took 133.81641840934753 seconds to complete one epoch of meta training
Took 142.446026802063 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478766
Epoch 428
Meta loss on this task batch = 3.1877e-01, Meta loss averaged over last 500 steps = 2.9729e-01, PNorm = 115.0961, GNorm = 0.4061
Meta loss on this task batch = 2.7914e-01, Meta loss averaged over last 500 steps = 2.9716e-01, PNorm = 115.1002, GNorm = 0.2645
Meta loss on this task batch = 2.4073e-01, Meta loss averaged over last 500 steps = 2.9692e-01, PNorm = 115.1061, GNorm = 0.2350
Meta loss on this task batch = 2.9772e-01, Meta loss averaged over last 500 steps = 2.9682e-01, PNorm = 115.1116, GNorm = 0.2874
Meta loss on this task batch = 2.8931e-01, Meta loss averaged over last 500 steps = 2.9678e-01, PNorm = 115.1182, GNorm = 0.2864
Meta loss on this task batch = 2.7466e-01, Meta loss averaged over last 500 steps = 2.9676e-01, PNorm = 115.1279, GNorm = 1.0000
Meta loss on this task batch = 2.9943e-01, Meta loss averaged over last 500 steps = 2.9680e-01, PNorm = 115.1380, GNorm = 0.2525
Meta loss on this task batch = 2.6758e-01, Meta loss averaged over last 500 steps = 2.9671e-01, PNorm = 115.1466, GNorm = 0.2855
Meta loss on this task batch = 2.8286e-01, Meta loss averaged over last 500 steps = 2.9667e-01, PNorm = 115.1538, GNorm = 0.3614
Meta loss on this task batch = 3.3225e-01, Meta loss averaged over last 500 steps = 2.9673e-01, PNorm = 115.1576, GNorm = 0.3024
Meta loss on this task batch = 3.3449e-01, Meta loss averaged over last 500 steps = 2.9673e-01, PNorm = 115.1539, GNorm = 0.4055
Meta loss on this task batch = 2.6592e-01, Meta loss averaged over last 500 steps = 2.9665e-01, PNorm = 115.1448, GNorm = 0.3117
Meta loss on this task batch = 3.4518e-01, Meta loss averaged over last 500 steps = 2.9665e-01, PNorm = 115.1292, GNorm = 0.3596
Meta loss on this task batch = 3.1428e-01, Meta loss averaged over last 500 steps = 2.9667e-01, PNorm = 115.1136, GNorm = 0.2994
Meta loss on this task batch = 3.2441e-01, Meta loss averaged over last 500 steps = 2.9667e-01, PNorm = 115.0992, GNorm = 0.2786
Meta loss on this task batch = 2.8918e-01, Meta loss averaged over last 500 steps = 2.9665e-01, PNorm = 115.0884, GNorm = 0.3014
Meta loss on this task batch = 3.7149e-01, Meta loss averaged over last 500 steps = 2.9681e-01, PNorm = 115.0875, GNorm = 0.3933
Meta loss on this task batch = 3.0832e-01, Meta loss averaged over last 500 steps = 2.9684e-01, PNorm = 115.0930, GNorm = 0.3452
Meta loss on this task batch = 2.9228e-01, Meta loss averaged over last 500 steps = 2.9688e-01, PNorm = 115.1018, GNorm = 0.3444
Took 134.0314085483551 seconds to complete one epoch of meta training
Took 142.57474517822266 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473669
Epoch 429
Meta loss on this task batch = 3.0091e-01, Meta loss averaged over last 500 steps = 2.9684e-01, PNorm = 115.1157, GNorm = 0.2777
Meta loss on this task batch = 3.1847e-01, Meta loss averaged over last 500 steps = 2.9689e-01, PNorm = 115.1335, GNorm = 0.3080
Meta loss on this task batch = 2.4829e-01, Meta loss averaged over last 500 steps = 2.9675e-01, PNorm = 115.1539, GNorm = 0.2885
Meta loss on this task batch = 2.6755e-01, Meta loss averaged over last 500 steps = 2.9679e-01, PNorm = 115.1730, GNorm = 0.2545
Meta loss on this task batch = 3.0122e-01, Meta loss averaged over last 500 steps = 2.9692e-01, PNorm = 115.1862, GNorm = 0.3418
Meta loss on this task batch = 2.8096e-01, Meta loss averaged over last 500 steps = 2.9689e-01, PNorm = 115.1959, GNorm = 0.3489
Meta loss on this task batch = 3.9441e-01, Meta loss averaged over last 500 steps = 2.9715e-01, PNorm = 115.1995, GNorm = 0.4149
Meta loss on this task batch = 2.8111e-01, Meta loss averaged over last 500 steps = 2.9706e-01, PNorm = 115.2034, GNorm = 0.2812
Meta loss on this task batch = 3.1798e-01, Meta loss averaged over last 500 steps = 2.9705e-01, PNorm = 115.2044, GNorm = 0.3757
Meta loss on this task batch = 3.0262e-01, Meta loss averaged over last 500 steps = 2.9708e-01, PNorm = 115.2059, GNorm = 0.3356
Meta loss on this task batch = 3.5450e-01, Meta loss averaged over last 500 steps = 2.9718e-01, PNorm = 115.2068, GNorm = 0.3524
Meta loss on this task batch = 2.9492e-01, Meta loss averaged over last 500 steps = 2.9715e-01, PNorm = 115.2084, GNorm = 0.2914
Meta loss on this task batch = 3.5139e-01, Meta loss averaged over last 500 steps = 2.9731e-01, PNorm = 115.2116, GNorm = 0.3381
Meta loss on this task batch = 3.4406e-01, Meta loss averaged over last 500 steps = 2.9748e-01, PNorm = 115.2200, GNorm = 0.3868
Meta loss on this task batch = 3.1349e-01, Meta loss averaged over last 500 steps = 2.9758e-01, PNorm = 115.2312, GNorm = 0.3306
Meta loss on this task batch = 2.7530e-01, Meta loss averaged over last 500 steps = 2.9761e-01, PNorm = 115.2449, GNorm = 0.3368
Meta loss on this task batch = 3.6717e-01, Meta loss averaged over last 500 steps = 2.9771e-01, PNorm = 115.2606, GNorm = 0.3140
Meta loss on this task batch = 3.5544e-01, Meta loss averaged over last 500 steps = 2.9785e-01, PNorm = 115.2744, GNorm = 0.2828
Meta loss on this task batch = 2.4545e-01, Meta loss averaged over last 500 steps = 2.9768e-01, PNorm = 115.2876, GNorm = 0.2966
Took 131.5973560810089 seconds to complete one epoch of meta training
Took 141.01591181755066 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479266
Epoch 430
Meta loss on this task batch = 3.2571e-01, Meta loss averaged over last 500 steps = 2.9758e-01, PNorm = 115.3010, GNorm = 0.2749
Meta loss on this task batch = 2.9575e-01, Meta loss averaged over last 500 steps = 2.9754e-01, PNorm = 115.3120, GNorm = 0.2987
Meta loss on this task batch = 3.1163e-01, Meta loss averaged over last 500 steps = 2.9762e-01, PNorm = 115.3219, GNorm = 0.2893
Meta loss on this task batch = 2.8843e-01, Meta loss averaged over last 500 steps = 2.9760e-01, PNorm = 115.3283, GNorm = 0.3474
Meta loss on this task batch = 2.9843e-01, Meta loss averaged over last 500 steps = 2.9762e-01, PNorm = 115.3348, GNorm = 0.3092
Meta loss on this task batch = 3.3529e-01, Meta loss averaged over last 500 steps = 2.9755e-01, PNorm = 115.3408, GNorm = 0.2923
Meta loss on this task batch = 2.8003e-01, Meta loss averaged over last 500 steps = 2.9760e-01, PNorm = 115.3471, GNorm = 0.2778
Meta loss on this task batch = 2.7914e-01, Meta loss averaged over last 500 steps = 2.9747e-01, PNorm = 115.3534, GNorm = 0.2782
Meta loss on this task batch = 2.8856e-01, Meta loss averaged over last 500 steps = 2.9747e-01, PNorm = 115.3582, GNorm = 0.3067
Meta loss on this task batch = 3.2853e-01, Meta loss averaged over last 500 steps = 2.9767e-01, PNorm = 115.3628, GNorm = 0.3110
Meta loss on this task batch = 3.0232e-01, Meta loss averaged over last 500 steps = 2.9771e-01, PNorm = 115.3684, GNorm = 0.2685
Meta loss on this task batch = 2.9627e-01, Meta loss averaged over last 500 steps = 2.9780e-01, PNorm = 115.3747, GNorm = 0.2551
Meta loss on this task batch = 3.0915e-01, Meta loss averaged over last 500 steps = 2.9773e-01, PNorm = 115.3801, GNorm = 0.2638
Meta loss on this task batch = 3.3055e-01, Meta loss averaged over last 500 steps = 2.9767e-01, PNorm = 115.3832, GNorm = 0.3164
Meta loss on this task batch = 2.9560e-01, Meta loss averaged over last 500 steps = 2.9772e-01, PNorm = 115.3888, GNorm = 0.2811
Meta loss on this task batch = 3.2683e-01, Meta loss averaged over last 500 steps = 2.9782e-01, PNorm = 115.3959, GNorm = 0.3096
Meta loss on this task batch = 2.9356e-01, Meta loss averaged over last 500 steps = 2.9775e-01, PNorm = 115.4035, GNorm = 0.2787
Meta loss on this task batch = 2.7987e-01, Meta loss averaged over last 500 steps = 2.9788e-01, PNorm = 115.4133, GNorm = 0.2962
Meta loss on this task batch = 3.0233e-01, Meta loss averaged over last 500 steps = 2.9800e-01, PNorm = 115.4237, GNorm = 0.3158
Took 129.8245871067047 seconds to complete one epoch of meta training
Took 138.64632034301758 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481653
Epoch 431
Meta loss on this task batch = 2.7998e-01, Meta loss averaged over last 500 steps = 2.9798e-01, PNorm = 115.4332, GNorm = 0.2536
Meta loss on this task batch = 2.4498e-01, Meta loss averaged over last 500 steps = 2.9778e-01, PNorm = 115.4428, GNorm = 0.2394
Meta loss on this task batch = 3.3350e-01, Meta loss averaged over last 500 steps = 2.9781e-01, PNorm = 115.4512, GNorm = 0.3085
Meta loss on this task batch = 2.7904e-01, Meta loss averaged over last 500 steps = 2.9767e-01, PNorm = 115.4585, GNorm = 0.2679
Meta loss on this task batch = 2.6131e-01, Meta loss averaged over last 500 steps = 2.9755e-01, PNorm = 115.4646, GNorm = 0.2923
Meta loss on this task batch = 2.9073e-01, Meta loss averaged over last 500 steps = 2.9750e-01, PNorm = 115.4639, GNorm = 0.4291
Meta loss on this task batch = 3.2561e-01, Meta loss averaged over last 500 steps = 2.9752e-01, PNorm = 115.4629, GNorm = 0.2908
Meta loss on this task batch = 2.7831e-01, Meta loss averaged over last 500 steps = 2.9761e-01, PNorm = 115.4631, GNorm = 0.3124
Meta loss on this task batch = 2.7818e-01, Meta loss averaged over last 500 steps = 2.9750e-01, PNorm = 115.4631, GNorm = 0.2805
Meta loss on this task batch = 3.5364e-01, Meta loss averaged over last 500 steps = 2.9763e-01, PNorm = 115.4643, GNorm = 0.3071
Meta loss on this task batch = 3.9829e-01, Meta loss averaged over last 500 steps = 2.9777e-01, PNorm = 115.4666, GNorm = 0.3478
Meta loss on this task batch = 3.3311e-01, Meta loss averaged over last 500 steps = 2.9784e-01, PNorm = 115.4700, GNorm = 0.3014
Meta loss on this task batch = 2.7529e-01, Meta loss averaged over last 500 steps = 2.9770e-01, PNorm = 115.4738, GNorm = 0.2826
Meta loss on this task batch = 2.5686e-01, Meta loss averaged over last 500 steps = 2.9765e-01, PNorm = 115.4790, GNorm = 0.2382
Meta loss on this task batch = 2.9509e-01, Meta loss averaged over last 500 steps = 2.9769e-01, PNorm = 115.4832, GNorm = 0.2818
Meta loss on this task batch = 2.9475e-01, Meta loss averaged over last 500 steps = 2.9764e-01, PNorm = 115.4877, GNorm = 0.3040
Meta loss on this task batch = 2.7521e-01, Meta loss averaged over last 500 steps = 2.9758e-01, PNorm = 115.4931, GNorm = 0.3050
Meta loss on this task batch = 2.9932e-01, Meta loss averaged over last 500 steps = 2.9766e-01, PNorm = 115.4971, GNorm = 0.2735
Meta loss on this task batch = 2.9226e-01, Meta loss averaged over last 500 steps = 2.9756e-01, PNorm = 115.5006, GNorm = 0.3398
Took 135.19328713417053 seconds to complete one epoch of meta training
Took 144.06502199172974 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480184
Epoch 432
Meta loss on this task batch = 2.9064e-01, Meta loss averaged over last 500 steps = 2.9765e-01, PNorm = 115.5037, GNorm = 0.2891
Meta loss on this task batch = 3.1026e-01, Meta loss averaged over last 500 steps = 2.9760e-01, PNorm = 115.5066, GNorm = 0.2769
Meta loss on this task batch = 2.7412e-01, Meta loss averaged over last 500 steps = 2.9752e-01, PNorm = 115.5090, GNorm = 0.2637
Meta loss on this task batch = 3.5358e-01, Meta loss averaged over last 500 steps = 2.9765e-01, PNorm = 115.5107, GNorm = 0.2835
Meta loss on this task batch = 3.3857e-01, Meta loss averaged over last 500 steps = 2.9780e-01, PNorm = 115.5111, GNorm = 0.2952
Meta loss on this task batch = 2.4770e-01, Meta loss averaged over last 500 steps = 2.9769e-01, PNorm = 115.5101, GNorm = 0.2639
Meta loss on this task batch = 2.5392e-01, Meta loss averaged over last 500 steps = 2.9768e-01, PNorm = 115.5080, GNorm = 0.2658
Meta loss on this task batch = 2.8539e-01, Meta loss averaged over last 500 steps = 2.9767e-01, PNorm = 115.5064, GNorm = 0.2962
Meta loss on this task batch = 2.9549e-01, Meta loss averaged over last 500 steps = 2.9768e-01, PNorm = 115.5062, GNorm = 0.2506
Meta loss on this task batch = 2.7335e-01, Meta loss averaged over last 500 steps = 2.9756e-01, PNorm = 115.5088, GNorm = 0.2783
Meta loss on this task batch = 3.0367e-01, Meta loss averaged over last 500 steps = 2.9748e-01, PNorm = 115.5106, GNorm = 0.3169
Meta loss on this task batch = 3.0751e-01, Meta loss averaged over last 500 steps = 2.9743e-01, PNorm = 115.5152, GNorm = 0.2834
Meta loss on this task batch = 2.6770e-01, Meta loss averaged over last 500 steps = 2.9738e-01, PNorm = 115.5224, GNorm = 0.2753
Meta loss on this task batch = 2.5370e-01, Meta loss averaged over last 500 steps = 2.9731e-01, PNorm = 115.5307, GNorm = 0.2629
Meta loss on this task batch = 3.5756e-01, Meta loss averaged over last 500 steps = 2.9750e-01, PNorm = 115.5362, GNorm = 0.3535
Meta loss on this task batch = 2.9771e-01, Meta loss averaged over last 500 steps = 2.9759e-01, PNorm = 115.5411, GNorm = 0.2778
Meta loss on this task batch = 2.8670e-01, Meta loss averaged over last 500 steps = 2.9754e-01, PNorm = 115.5450, GNorm = 0.2660
Meta loss on this task batch = 2.9411e-01, Meta loss averaged over last 500 steps = 2.9758e-01, PNorm = 115.5501, GNorm = 0.3168
Meta loss on this task batch = 3.0275e-01, Meta loss averaged over last 500 steps = 2.9763e-01, PNorm = 115.5562, GNorm = 0.3709
Took 131.49095153808594 seconds to complete one epoch of meta training
Took 139.02674865722656 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466703
Epoch 433
Meta loss on this task batch = 2.8194e-01, Meta loss averaged over last 500 steps = 2.9762e-01, PNorm = 115.5602, GNorm = 0.3445
Meta loss on this task batch = 2.7715e-01, Meta loss averaged over last 500 steps = 2.9759e-01, PNorm = 115.5640, GNorm = 0.2841
Meta loss on this task batch = 2.7088e-01, Meta loss averaged over last 500 steps = 2.9755e-01, PNorm = 115.5665, GNorm = 0.2707
Meta loss on this task batch = 2.5784e-01, Meta loss averaged over last 500 steps = 2.9737e-01, PNorm = 115.5670, GNorm = 0.2884
Meta loss on this task batch = 3.2718e-01, Meta loss averaged over last 500 steps = 2.9740e-01, PNorm = 115.5695, GNorm = 0.3083
Meta loss on this task batch = 2.8850e-01, Meta loss averaged over last 500 steps = 2.9734e-01, PNorm = 115.5715, GNorm = 0.2501
Meta loss on this task batch = 3.0166e-01, Meta loss averaged over last 500 steps = 2.9739e-01, PNorm = 115.5742, GNorm = 0.2845
Meta loss on this task batch = 3.1779e-01, Meta loss averaged over last 500 steps = 2.9744e-01, PNorm = 115.5785, GNorm = 0.2841
Meta loss on this task batch = 3.1263e-01, Meta loss averaged over last 500 steps = 2.9746e-01, PNorm = 115.5825, GNorm = 0.2958
Meta loss on this task batch = 3.7020e-01, Meta loss averaged over last 500 steps = 2.9768e-01, PNorm = 115.5869, GNorm = 0.2929
Meta loss on this task batch = 2.7670e-01, Meta loss averaged over last 500 steps = 2.9764e-01, PNorm = 115.5919, GNorm = 0.2465
Meta loss on this task batch = 2.9391e-01, Meta loss averaged over last 500 steps = 2.9756e-01, PNorm = 115.5959, GNorm = 0.2957
Meta loss on this task batch = 2.7699e-01, Meta loss averaged over last 500 steps = 2.9750e-01, PNorm = 115.5994, GNorm = 0.2648
Meta loss on this task batch = 3.2402e-01, Meta loss averaged over last 500 steps = 2.9757e-01, PNorm = 115.5998, GNorm = 0.3090
Meta loss on this task batch = 3.4709e-01, Meta loss averaged over last 500 steps = 2.9768e-01, PNorm = 115.5996, GNorm = 0.2780
Meta loss on this task batch = 3.4093e-01, Meta loss averaged over last 500 steps = 2.9774e-01, PNorm = 115.5983, GNorm = 0.2850
Meta loss on this task batch = 2.6630e-01, Meta loss averaged over last 500 steps = 2.9775e-01, PNorm = 115.5986, GNorm = 0.2834
Meta loss on this task batch = 2.6465e-01, Meta loss averaged over last 500 steps = 2.9766e-01, PNorm = 115.6022, GNorm = 0.2810
Meta loss on this task batch = 2.7844e-01, Meta loss averaged over last 500 steps = 2.9766e-01, PNorm = 115.6063, GNorm = 0.3442
Took 135.34506940841675 seconds to complete one epoch of meta training
Took 143.57554650306702 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472176
Epoch 434
Meta loss on this task batch = 2.8387e-01, Meta loss averaged over last 500 steps = 2.9758e-01, PNorm = 115.6138, GNorm = 0.2883
Meta loss on this task batch = 3.0056e-01, Meta loss averaged over last 500 steps = 2.9751e-01, PNorm = 115.6249, GNorm = 0.3182
Meta loss on this task batch = 3.5415e-01, Meta loss averaged over last 500 steps = 2.9754e-01, PNorm = 115.6364, GNorm = 0.3248
Meta loss on this task batch = 2.7062e-01, Meta loss averaged over last 500 steps = 2.9745e-01, PNorm = 115.6489, GNorm = 0.3137
Meta loss on this task batch = 2.9998e-01, Meta loss averaged over last 500 steps = 2.9752e-01, PNorm = 115.6628, GNorm = 0.2907
Meta loss on this task batch = 3.2406e-01, Meta loss averaged over last 500 steps = 2.9764e-01, PNorm = 115.6753, GNorm = 0.2829
Meta loss on this task batch = 2.4596e-01, Meta loss averaged over last 500 steps = 2.9759e-01, PNorm = 115.6853, GNorm = 0.2432
Meta loss on this task batch = 3.0346e-01, Meta loss averaged over last 500 steps = 2.9747e-01, PNorm = 115.6913, GNorm = 0.2968
Meta loss on this task batch = 3.0740e-01, Meta loss averaged over last 500 steps = 2.9744e-01, PNorm = 115.6961, GNorm = 0.2770
Meta loss on this task batch = 2.7768e-01, Meta loss averaged over last 500 steps = 2.9751e-01, PNorm = 115.7003, GNorm = 0.2415
Meta loss on this task batch = 2.7562e-01, Meta loss averaged over last 500 steps = 2.9745e-01, PNorm = 115.7030, GNorm = 0.2554
Meta loss on this task batch = 3.2245e-01, Meta loss averaged over last 500 steps = 2.9737e-01, PNorm = 115.7057, GNorm = 0.3230
Meta loss on this task batch = 3.2490e-01, Meta loss averaged over last 500 steps = 2.9733e-01, PNorm = 115.7083, GNorm = 0.2620
Meta loss on this task batch = 3.0631e-01, Meta loss averaged over last 500 steps = 2.9735e-01, PNorm = 115.7115, GNorm = 0.2772
Meta loss on this task batch = 3.1303e-01, Meta loss averaged over last 500 steps = 2.9745e-01, PNorm = 115.7159, GNorm = 0.3221
Meta loss on this task batch = 2.4143e-01, Meta loss averaged over last 500 steps = 2.9736e-01, PNorm = 115.7214, GNorm = 0.2717
Meta loss on this task batch = 3.1991e-01, Meta loss averaged over last 500 steps = 2.9745e-01, PNorm = 115.7260, GNorm = 0.2971
Meta loss on this task batch = 2.7619e-01, Meta loss averaged over last 500 steps = 2.9752e-01, PNorm = 115.7316, GNorm = 0.2905
Meta loss on this task batch = 2.7757e-01, Meta loss averaged over last 500 steps = 2.9751e-01, PNorm = 115.7387, GNorm = 0.3230
Took 131.7026560306549 seconds to complete one epoch of meta training
Took 140.41226482391357 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484447
Epoch 435
Meta loss on this task batch = 2.1799e-01, Meta loss averaged over last 500 steps = 2.9729e-01, PNorm = 115.7470, GNorm = 0.2298
Meta loss on this task batch = 3.0551e-01, Meta loss averaged over last 500 steps = 2.9729e-01, PNorm = 115.7564, GNorm = 0.3031
Meta loss on this task batch = 2.9338e-01, Meta loss averaged over last 500 steps = 2.9727e-01, PNorm = 115.7646, GNorm = 0.3157
Meta loss on this task batch = 2.6358e-01, Meta loss averaged over last 500 steps = 2.9708e-01, PNorm = 115.7715, GNorm = 0.2953
Meta loss on this task batch = 3.0833e-01, Meta loss averaged over last 500 steps = 2.9709e-01, PNorm = 115.7766, GNorm = 0.3003
Meta loss on this task batch = 2.9877e-01, Meta loss averaged over last 500 steps = 2.9711e-01, PNorm = 115.7828, GNorm = 0.3030
Meta loss on this task batch = 2.7195e-01, Meta loss averaged over last 500 steps = 2.9707e-01, PNorm = 115.7894, GNorm = 0.2931
Meta loss on this task batch = 2.8394e-01, Meta loss averaged over last 500 steps = 2.9701e-01, PNorm = 115.7946, GNorm = 0.2805
Meta loss on this task batch = 2.3641e-01, Meta loss averaged over last 500 steps = 2.9685e-01, PNorm = 115.8002, GNorm = 0.2641
Meta loss on this task batch = 3.0433e-01, Meta loss averaged over last 500 steps = 2.9691e-01, PNorm = 115.8056, GNorm = 0.3302
Meta loss on this task batch = 3.4973e-01, Meta loss averaged over last 500 steps = 2.9707e-01, PNorm = 115.8075, GNorm = 0.3698
Meta loss on this task batch = 2.8658e-01, Meta loss averaged over last 500 steps = 2.9700e-01, PNorm = 115.8072, GNorm = 0.3121
Meta loss on this task batch = 2.9234e-01, Meta loss averaged over last 500 steps = 2.9697e-01, PNorm = 115.8089, GNorm = 0.2908
Meta loss on this task batch = 2.7633e-01, Meta loss averaged over last 500 steps = 2.9685e-01, PNorm = 115.8132, GNorm = 0.2843
Meta loss on this task batch = 3.0626e-01, Meta loss averaged over last 500 steps = 2.9673e-01, PNorm = 115.8172, GNorm = 0.3420
Meta loss on this task batch = 3.0857e-01, Meta loss averaged over last 500 steps = 2.9678e-01, PNorm = 115.8211, GNorm = 0.3040
Meta loss on this task batch = 3.0759e-01, Meta loss averaged over last 500 steps = 2.9699e-01, PNorm = 115.8225, GNorm = 0.3221
Meta loss on this task batch = 2.6924e-01, Meta loss averaged over last 500 steps = 2.9692e-01, PNorm = 115.8251, GNorm = 0.2498
Meta loss on this task batch = 2.7522e-01, Meta loss averaged over last 500 steps = 2.9683e-01, PNorm = 115.8290, GNorm = 0.2942
Took 129.09564995765686 seconds to complete one epoch of meta training
Took 137.24359035491943 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481458
Epoch 436
Meta loss on this task batch = 2.7136e-01, Meta loss averaged over last 500 steps = 2.9680e-01, PNorm = 115.8327, GNorm = 0.2583
Meta loss on this task batch = 2.5946e-01, Meta loss averaged over last 500 steps = 2.9665e-01, PNorm = 115.8383, GNorm = 0.2550
Meta loss on this task batch = 2.3859e-01, Meta loss averaged over last 500 steps = 2.9650e-01, PNorm = 115.8476, GNorm = 0.3075
Meta loss on this task batch = 3.0856e-01, Meta loss averaged over last 500 steps = 2.9656e-01, PNorm = 115.8573, GNorm = 0.3140
Meta loss on this task batch = 2.9811e-01, Meta loss averaged over last 500 steps = 2.9654e-01, PNorm = 115.8659, GNorm = 0.2905
Meta loss on this task batch = 2.7916e-01, Meta loss averaged over last 500 steps = 2.9655e-01, PNorm = 115.8719, GNorm = 0.2919
Meta loss on this task batch = 2.6403e-01, Meta loss averaged over last 500 steps = 2.9652e-01, PNorm = 115.8780, GNorm = 0.2632
Meta loss on this task batch = 2.9451e-01, Meta loss averaged over last 500 steps = 2.9655e-01, PNorm = 115.8832, GNorm = 0.2929
Meta loss on this task batch = 3.4361e-01, Meta loss averaged over last 500 steps = 2.9646e-01, PNorm = 115.8869, GNorm = 0.2783
Meta loss on this task batch = 2.8459e-01, Meta loss averaged over last 500 steps = 2.9641e-01, PNorm = 115.8892, GNorm = 0.2774
Meta loss on this task batch = 3.8361e-01, Meta loss averaged over last 500 steps = 2.9658e-01, PNorm = 115.8896, GNorm = 0.3043
Meta loss on this task batch = 3.4016e-01, Meta loss averaged over last 500 steps = 2.9658e-01, PNorm = 115.8866, GNorm = 0.3281
Meta loss on this task batch = 3.1792e-01, Meta loss averaged over last 500 steps = 2.9663e-01, PNorm = 115.8855, GNorm = 0.2716
Meta loss on this task batch = 2.7415e-01, Meta loss averaged over last 500 steps = 2.9659e-01, PNorm = 115.8844, GNorm = 0.3484
Meta loss on this task batch = 2.8795e-01, Meta loss averaged over last 500 steps = 2.9669e-01, PNorm = 115.8879, GNorm = 0.2832
Meta loss on this task batch = 2.9675e-01, Meta loss averaged over last 500 steps = 2.9666e-01, PNorm = 115.8924, GNorm = 0.3020
Meta loss on this task batch = 2.6718e-01, Meta loss averaged over last 500 steps = 2.9657e-01, PNorm = 115.8981, GNorm = 0.2839
Meta loss on this task batch = 2.7077e-01, Meta loss averaged over last 500 steps = 2.9662e-01, PNorm = 115.9022, GNorm = 0.3200
Meta loss on this task batch = 2.5297e-01, Meta loss averaged over last 500 steps = 2.9657e-01, PNorm = 115.9089, GNorm = 0.3114
Took 131.85703945159912 seconds to complete one epoch of meta training
Took 140.21135783195496 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483336
Epoch 437
Meta loss on this task batch = 2.7306e-01, Meta loss averaged over last 500 steps = 2.9654e-01, PNorm = 115.9174, GNorm = 0.2898
Meta loss on this task batch = 2.9120e-01, Meta loss averaged over last 500 steps = 2.9653e-01, PNorm = 115.9280, GNorm = 0.2867
Meta loss on this task batch = 3.3527e-01, Meta loss averaged over last 500 steps = 2.9657e-01, PNorm = 115.9390, GNorm = 0.3102
Meta loss on this task batch = 2.3000e-01, Meta loss averaged over last 500 steps = 2.9644e-01, PNorm = 115.9497, GNorm = 0.2460
Meta loss on this task batch = 2.9957e-01, Meta loss averaged over last 500 steps = 2.9651e-01, PNorm = 115.9604, GNorm = 0.3020
Meta loss on this task batch = 3.7142e-01, Meta loss averaged over last 500 steps = 2.9658e-01, PNorm = 115.9691, GNorm = 0.3336
Meta loss on this task batch = 3.0359e-01, Meta loss averaged over last 500 steps = 2.9662e-01, PNorm = 115.9762, GNorm = 0.3130
Meta loss on this task batch = 2.8788e-01, Meta loss averaged over last 500 steps = 2.9659e-01, PNorm = 115.9815, GNorm = 0.2844
Meta loss on this task batch = 2.5835e-01, Meta loss averaged over last 500 steps = 2.9656e-01, PNorm = 115.9859, GNorm = 0.2718
Meta loss on this task batch = 3.4744e-01, Meta loss averaged over last 500 steps = 2.9663e-01, PNorm = 115.9894, GNorm = 0.3895
Meta loss on this task batch = 3.1024e-01, Meta loss averaged over last 500 steps = 2.9667e-01, PNorm = 115.9934, GNorm = 0.2692
Meta loss on this task batch = 2.8541e-01, Meta loss averaged over last 500 steps = 2.9680e-01, PNorm = 115.9991, GNorm = 0.2850
Meta loss on this task batch = 2.4927e-01, Meta loss averaged over last 500 steps = 2.9655e-01, PNorm = 116.0048, GNorm = 0.3871
Meta loss on this task batch = 2.3800e-01, Meta loss averaged over last 500 steps = 2.9642e-01, PNorm = 116.0119, GNorm = 0.2343
Meta loss on this task batch = 3.2697e-01, Meta loss averaged over last 500 steps = 2.9648e-01, PNorm = 116.0202, GNorm = 0.2957
Meta loss on this task batch = 3.0960e-01, Meta loss averaged over last 500 steps = 2.9643e-01, PNorm = 116.0286, GNorm = 0.2928
Meta loss on this task batch = 2.8872e-01, Meta loss averaged over last 500 steps = 2.9652e-01, PNorm = 116.0357, GNorm = 0.3212
Meta loss on this task batch = 2.7962e-01, Meta loss averaged over last 500 steps = 2.9648e-01, PNorm = 116.0434, GNorm = 0.2713
Meta loss on this task batch = 3.2004e-01, Meta loss averaged over last 500 steps = 2.9644e-01, PNorm = 116.0523, GNorm = 0.3316
Took 128.84147334098816 seconds to complete one epoch of meta training
Took 136.09891295433044 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495744
Epoch 438
Meta loss on this task batch = 2.8935e-01, Meta loss averaged over last 500 steps = 2.9651e-01, PNorm = 116.0604, GNorm = 0.2747
Meta loss on this task batch = 2.6423e-01, Meta loss averaged over last 500 steps = 2.9645e-01, PNorm = 116.0686, GNorm = 0.2571
Meta loss on this task batch = 2.8060e-01, Meta loss averaged over last 500 steps = 2.9641e-01, PNorm = 116.0764, GNorm = 0.3579
Meta loss on this task batch = 3.1385e-01, Meta loss averaged over last 500 steps = 2.9647e-01, PNorm = 116.0829, GNorm = 0.3239
Meta loss on this task batch = 2.7606e-01, Meta loss averaged over last 500 steps = 2.9642e-01, PNorm = 116.0883, GNorm = 0.3183
Meta loss on this task batch = 2.9074e-01, Meta loss averaged over last 500 steps = 2.9639e-01, PNorm = 116.0951, GNorm = 0.3791
Meta loss on this task batch = 3.3203e-01, Meta loss averaged over last 500 steps = 2.9650e-01, PNorm = 116.1018, GNorm = 0.3010
Meta loss on this task batch = 3.1026e-01, Meta loss averaged over last 500 steps = 2.9651e-01, PNorm = 116.1084, GNorm = 0.3145
Meta loss on this task batch = 3.0040e-01, Meta loss averaged over last 500 steps = 2.9653e-01, PNorm = 116.1157, GNorm = 0.2739
Meta loss on this task batch = 2.5228e-01, Meta loss averaged over last 500 steps = 2.9645e-01, PNorm = 116.1232, GNorm = 0.2832
Meta loss on this task batch = 2.9596e-01, Meta loss averaged over last 500 steps = 2.9644e-01, PNorm = 116.1307, GNorm = 0.3192
Meta loss on this task batch = 2.9821e-01, Meta loss averaged over last 500 steps = 2.9641e-01, PNorm = 116.1378, GNorm = 0.2704
Meta loss on this task batch = 3.3462e-01, Meta loss averaged over last 500 steps = 2.9641e-01, PNorm = 116.1431, GNorm = 0.3157
Meta loss on this task batch = 3.1432e-01, Meta loss averaged over last 500 steps = 2.9639e-01, PNorm = 116.1485, GNorm = 0.2875
Meta loss on this task batch = 2.9175e-01, Meta loss averaged over last 500 steps = 2.9633e-01, PNorm = 116.1554, GNorm = 0.2491
Meta loss on this task batch = 3.0366e-01, Meta loss averaged over last 500 steps = 2.9640e-01, PNorm = 116.1634, GNorm = 0.2973
Meta loss on this task batch = 2.8078e-01, Meta loss averaged over last 500 steps = 2.9643e-01, PNorm = 116.1726, GNorm = 0.2548
Meta loss on this task batch = 2.7500e-01, Meta loss averaged over last 500 steps = 2.9645e-01, PNorm = 116.1811, GNorm = 0.3075
Meta loss on this task batch = 2.8328e-01, Meta loss averaged over last 500 steps = 2.9643e-01, PNorm = 116.1892, GNorm = 0.3591
Took 129.56727719306946 seconds to complete one epoch of meta training
Took 138.02504873275757 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506428
Epoch 439
Meta loss on this task batch = 3.4580e-01, Meta loss averaged over last 500 steps = 2.9661e-01, PNorm = 116.1959, GNorm = 0.2966
Meta loss on this task batch = 3.1271e-01, Meta loss averaged over last 500 steps = 2.9664e-01, PNorm = 116.2021, GNorm = 0.2819
Meta loss on this task batch = 3.4767e-01, Meta loss averaged over last 500 steps = 2.9679e-01, PNorm = 116.2072, GNorm = 0.3134
Meta loss on this task batch = 3.2783e-01, Meta loss averaged over last 500 steps = 2.9688e-01, PNorm = 116.2109, GNorm = 0.3610
Meta loss on this task batch = 2.9370e-01, Meta loss averaged over last 500 steps = 2.9689e-01, PNorm = 116.2145, GNorm = 0.2876
Meta loss on this task batch = 2.9042e-01, Meta loss averaged over last 500 steps = 2.9679e-01, PNorm = 116.2173, GNorm = 0.2883
Meta loss on this task batch = 2.6356e-01, Meta loss averaged over last 500 steps = 2.9665e-01, PNorm = 116.2222, GNorm = 0.2414
Meta loss on this task batch = 2.6246e-01, Meta loss averaged over last 500 steps = 2.9665e-01, PNorm = 116.2295, GNorm = 0.2601
Meta loss on this task batch = 2.4611e-01, Meta loss averaged over last 500 steps = 2.9652e-01, PNorm = 116.2384, GNorm = 0.2660
Meta loss on this task batch = 2.7873e-01, Meta loss averaged over last 500 steps = 2.9640e-01, PNorm = 116.2477, GNorm = 0.2697
Meta loss on this task batch = 2.3546e-01, Meta loss averaged over last 500 steps = 2.9643e-01, PNorm = 116.2588, GNorm = 0.2756
Meta loss on this task batch = 3.1041e-01, Meta loss averaged over last 500 steps = 2.9641e-01, PNorm = 116.2688, GNorm = 0.3333
Meta loss on this task batch = 3.2029e-01, Meta loss averaged over last 500 steps = 2.9646e-01, PNorm = 116.2771, GNorm = 0.2664
Meta loss on this task batch = 2.3878e-01, Meta loss averaged over last 500 steps = 2.9634e-01, PNorm = 116.2845, GNorm = 0.2540
Meta loss on this task batch = 3.2842e-01, Meta loss averaged over last 500 steps = 2.9640e-01, PNorm = 116.2896, GNorm = 0.3423
Meta loss on this task batch = 2.5123e-01, Meta loss averaged over last 500 steps = 2.9644e-01, PNorm = 116.2943, GNorm = 0.2890
Meta loss on this task batch = 3.0742e-01, Meta loss averaged over last 500 steps = 2.9650e-01, PNorm = 116.2984, GNorm = 0.2868
Meta loss on this task batch = 3.3685e-01, Meta loss averaged over last 500 steps = 2.9654e-01, PNorm = 116.3025, GNorm = 0.3933
Meta loss on this task batch = 2.6793e-01, Meta loss averaged over last 500 steps = 2.9660e-01, PNorm = 116.3048, GNorm = 0.3581
Took 129.19100046157837 seconds to complete one epoch of meta training
Took 137.4949188232422 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.521876
Epoch 440
Meta loss on this task batch = 3.2034e-01, Meta loss averaged over last 500 steps = 2.9661e-01, PNorm = 116.3060, GNorm = 0.3032
Meta loss on this task batch = 2.4204e-01, Meta loss averaged over last 500 steps = 2.9648e-01, PNorm = 116.3087, GNorm = 0.2762
Meta loss on this task batch = 2.9955e-01, Meta loss averaged over last 500 steps = 2.9641e-01, PNorm = 116.3120, GNorm = 0.2839
Meta loss on this task batch = 3.6617e-01, Meta loss averaged over last 500 steps = 2.9654e-01, PNorm = 116.3166, GNorm = 0.3515
Meta loss on this task batch = 3.3169e-01, Meta loss averaged over last 500 steps = 2.9663e-01, PNorm = 116.3246, GNorm = 0.3514
Meta loss on this task batch = 2.5552e-01, Meta loss averaged over last 500 steps = 2.9657e-01, PNorm = 116.3347, GNorm = 0.2863
Meta loss on this task batch = 3.6217e-01, Meta loss averaged over last 500 steps = 2.9664e-01, PNorm = 116.3455, GNorm = 0.4599
Meta loss on this task batch = 2.8664e-01, Meta loss averaged over last 500 steps = 2.9661e-01, PNorm = 116.3578, GNorm = 0.2825
Meta loss on this task batch = 2.9957e-01, Meta loss averaged over last 500 steps = 2.9669e-01, PNorm = 116.3718, GNorm = 0.3484
Meta loss on this task batch = 2.8567e-01, Meta loss averaged over last 500 steps = 2.9663e-01, PNorm = 116.3852, GNorm = 0.2800
Meta loss on this task batch = 2.5615e-01, Meta loss averaged over last 500 steps = 2.9665e-01, PNorm = 116.3980, GNorm = 0.2410
Meta loss on this task batch = 3.1569e-01, Meta loss averaged over last 500 steps = 2.9672e-01, PNorm = 116.4074, GNorm = 0.3006
Meta loss on this task batch = 2.5920e-01, Meta loss averaged over last 500 steps = 2.9649e-01, PNorm = 116.4145, GNorm = 0.2816
Meta loss on this task batch = 3.5631e-01, Meta loss averaged over last 500 steps = 2.9670e-01, PNorm = 116.4180, GNorm = 0.3563
Meta loss on this task batch = 2.9626e-01, Meta loss averaged over last 500 steps = 2.9664e-01, PNorm = 116.4185, GNorm = 0.3075
Meta loss on this task batch = 2.8640e-01, Meta loss averaged over last 500 steps = 2.9666e-01, PNorm = 116.4177, GNorm = 0.2797
Meta loss on this task batch = 3.3033e-01, Meta loss averaged over last 500 steps = 2.9673e-01, PNorm = 116.4166, GNorm = 0.3106
Meta loss on this task batch = 2.6293e-01, Meta loss averaged over last 500 steps = 2.9670e-01, PNorm = 116.4144, GNorm = 0.2695
Meta loss on this task batch = 2.6799e-01, Meta loss averaged over last 500 steps = 2.9657e-01, PNorm = 116.4142, GNorm = 0.2908
Took 129.8656120300293 seconds to complete one epoch of meta training
Took 138.4429066181183 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488572
Epoch 441
Meta loss on this task batch = 3.0627e-01, Meta loss averaged over last 500 steps = 2.9657e-01, PNorm = 116.4157, GNorm = 0.2938
Meta loss on this task batch = 2.9561e-01, Meta loss averaged over last 500 steps = 2.9654e-01, PNorm = 116.4193, GNorm = 0.2795
Meta loss on this task batch = 2.9659e-01, Meta loss averaged over last 500 steps = 2.9663e-01, PNorm = 116.4260, GNorm = 0.2728
Meta loss on this task batch = 2.7106e-01, Meta loss averaged over last 500 steps = 2.9660e-01, PNorm = 116.4341, GNorm = 0.2745
Meta loss on this task batch = 3.2172e-01, Meta loss averaged over last 500 steps = 2.9664e-01, PNorm = 116.4425, GNorm = 0.2640
Meta loss on this task batch = 3.1468e-01, Meta loss averaged over last 500 steps = 2.9658e-01, PNorm = 116.4520, GNorm = 0.3273
Meta loss on this task batch = 2.6483e-01, Meta loss averaged over last 500 steps = 2.9649e-01, PNorm = 116.4621, GNorm = 0.2529
Meta loss on this task batch = 2.7850e-01, Meta loss averaged over last 500 steps = 2.9644e-01, PNorm = 116.4738, GNorm = 0.3939
Meta loss on this task batch = 3.6488e-01, Meta loss averaged over last 500 steps = 2.9662e-01, PNorm = 116.4836, GNorm = 0.3538
Meta loss on this task batch = 2.9700e-01, Meta loss averaged over last 500 steps = 2.9653e-01, PNorm = 116.4926, GNorm = 0.3076
Meta loss on this task batch = 2.9997e-01, Meta loss averaged over last 500 steps = 2.9658e-01, PNorm = 116.5001, GNorm = 0.3311
Meta loss on this task batch = 3.1842e-01, Meta loss averaged over last 500 steps = 2.9655e-01, PNorm = 116.5064, GNorm = 0.2813
Meta loss on this task batch = 2.6797e-01, Meta loss averaged over last 500 steps = 2.9651e-01, PNorm = 116.5122, GNorm = 0.3188
Meta loss on this task batch = 2.8305e-01, Meta loss averaged over last 500 steps = 2.9637e-01, PNorm = 116.5196, GNorm = 0.2539
Meta loss on this task batch = 2.4334e-01, Meta loss averaged over last 500 steps = 2.9623e-01, PNorm = 116.5284, GNorm = 0.3142
Meta loss on this task batch = 2.6463e-01, Meta loss averaged over last 500 steps = 2.9617e-01, PNorm = 116.5379, GNorm = 0.2788
Meta loss on this task batch = 3.1716e-01, Meta loss averaged over last 500 steps = 2.9628e-01, PNorm = 116.5473, GNorm = 0.3119
Meta loss on this task batch = 2.8408e-01, Meta loss averaged over last 500 steps = 2.9630e-01, PNorm = 116.5569, GNorm = 0.3761
Meta loss on this task batch = 3.0864e-01, Meta loss averaged over last 500 steps = 2.9636e-01, PNorm = 116.5653, GNorm = 0.4060
Took 127.91454148292542 seconds to complete one epoch of meta training
Took 136.18724274635315 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507581
Epoch 442
Meta loss on this task batch = 3.1606e-01, Meta loss averaged over last 500 steps = 2.9636e-01, PNorm = 116.5727, GNorm = 0.3116
Meta loss on this task batch = 2.6068e-01, Meta loss averaged over last 500 steps = 2.9625e-01, PNorm = 116.5839, GNorm = 0.3494
Meta loss on this task batch = 3.4644e-01, Meta loss averaged over last 500 steps = 2.9639e-01, PNorm = 116.5936, GNorm = 0.3615
Meta loss on this task batch = 2.9154e-01, Meta loss averaged over last 500 steps = 2.9637e-01, PNorm = 116.6013, GNorm = 0.2918
Meta loss on this task batch = 2.9730e-01, Meta loss averaged over last 500 steps = 2.9632e-01, PNorm = 116.6077, GNorm = 0.3148
Meta loss on this task batch = 3.2075e-01, Meta loss averaged over last 500 steps = 2.9634e-01, PNorm = 116.6122, GNorm = 0.3442
Meta loss on this task batch = 2.8587e-01, Meta loss averaged over last 500 steps = 2.9631e-01, PNorm = 116.6161, GNorm = 0.2881
Meta loss on this task batch = 2.9964e-01, Meta loss averaged over last 500 steps = 2.9638e-01, PNorm = 116.6205, GNorm = 0.3123
Meta loss on this task batch = 2.5808e-01, Meta loss averaged over last 500 steps = 2.9628e-01, PNorm = 116.6258, GNorm = 0.2861
Meta loss on this task batch = 2.7259e-01, Meta loss averaged over last 500 steps = 2.9618e-01, PNorm = 116.6312, GNorm = 0.3075
Meta loss on this task batch = 2.9192e-01, Meta loss averaged over last 500 steps = 2.9607e-01, PNorm = 116.6363, GNorm = 0.3047
Meta loss on this task batch = 3.1149e-01, Meta loss averaged over last 500 steps = 2.9607e-01, PNorm = 116.6420, GNorm = 0.3621
Meta loss on this task batch = 2.6940e-01, Meta loss averaged over last 500 steps = 2.9613e-01, PNorm = 116.6461, GNorm = 0.3156
Meta loss on this task batch = 2.9582e-01, Meta loss averaged over last 500 steps = 2.9614e-01, PNorm = 116.6500, GNorm = 0.3021
Meta loss on this task batch = 3.5423e-01, Meta loss averaged over last 500 steps = 2.9633e-01, PNorm = 116.6520, GNorm = 0.3282
Meta loss on this task batch = 2.8508e-01, Meta loss averaged over last 500 steps = 2.9631e-01, PNorm = 116.6533, GNorm = 0.3148
Meta loss on this task batch = 3.0942e-01, Meta loss averaged over last 500 steps = 2.9635e-01, PNorm = 116.6523, GNorm = 0.2974
Meta loss on this task batch = 3.1072e-01, Meta loss averaged over last 500 steps = 2.9650e-01, PNorm = 116.6505, GNorm = 0.3177
Meta loss on this task batch = 2.3178e-01, Meta loss averaged over last 500 steps = 2.9644e-01, PNorm = 116.6525, GNorm = 0.3308
Took 130.79177618026733 seconds to complete one epoch of meta training
Took 140.115220785141 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478492
Epoch 443
Meta loss on this task batch = 2.5970e-01, Meta loss averaged over last 500 steps = 2.9639e-01, PNorm = 116.6539, GNorm = 0.2600
Meta loss on this task batch = 3.1557e-01, Meta loss averaged over last 500 steps = 2.9643e-01, PNorm = 116.6529, GNorm = 0.3132
Meta loss on this task batch = 3.7770e-01, Meta loss averaged over last 500 steps = 2.9660e-01, PNorm = 116.6511, GNorm = 0.3530
Meta loss on this task batch = 2.5988e-01, Meta loss averaged over last 500 steps = 2.9652e-01, PNorm = 116.6537, GNorm = 0.2714
Meta loss on this task batch = 3.0085e-01, Meta loss averaged over last 500 steps = 2.9641e-01, PNorm = 116.6563, GNorm = 0.3066
Meta loss on this task batch = 2.6786e-01, Meta loss averaged over last 500 steps = 2.9627e-01, PNorm = 116.6588, GNorm = 0.2756
Meta loss on this task batch = 2.2346e-01, Meta loss averaged over last 500 steps = 2.9604e-01, PNorm = 116.6611, GNorm = 0.2513
Meta loss on this task batch = 3.3536e-01, Meta loss averaged over last 500 steps = 2.9596e-01, PNorm = 116.6625, GNorm = 0.2958
Meta loss on this task batch = 2.7709e-01, Meta loss averaged over last 500 steps = 2.9587e-01, PNorm = 116.6644, GNorm = 0.2833
Meta loss on this task batch = 2.6713e-01, Meta loss averaged over last 500 steps = 2.9579e-01, PNorm = 116.6686, GNorm = 0.2882
Meta loss on this task batch = 3.6355e-01, Meta loss averaged over last 500 steps = 2.9594e-01, PNorm = 116.6726, GNorm = 0.3389
Meta loss on this task batch = 3.0959e-01, Meta loss averaged over last 500 steps = 2.9604e-01, PNorm = 116.6759, GNorm = 0.2896
Meta loss on this task batch = 2.7891e-01, Meta loss averaged over last 500 steps = 2.9590e-01, PNorm = 116.6793, GNorm = 0.2860
Meta loss on this task batch = 3.0901e-01, Meta loss averaged over last 500 steps = 2.9597e-01, PNorm = 116.6807, GNorm = 0.3075
Meta loss on this task batch = 3.3113e-01, Meta loss averaged over last 500 steps = 2.9605e-01, PNorm = 116.6844, GNorm = 0.3043
Meta loss on this task batch = 2.4825e-01, Meta loss averaged over last 500 steps = 2.9599e-01, PNorm = 116.6903, GNorm = 0.2947
Meta loss on this task batch = 2.6207e-01, Meta loss averaged over last 500 steps = 2.9592e-01, PNorm = 116.6975, GNorm = 0.2559
Meta loss on this task batch = 3.1199e-01, Meta loss averaged over last 500 steps = 2.9601e-01, PNorm = 116.7051, GNorm = 0.3096
Meta loss on this task batch = 3.7503e-01, Meta loss averaged over last 500 steps = 2.9625e-01, PNorm = 116.7123, GNorm = 0.4645
Took 129.79528307914734 seconds to complete one epoch of meta training
Took 138.13123059272766 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.519308
Epoch 444
Meta loss on this task batch = 2.8113e-01, Meta loss averaged over last 500 steps = 2.9629e-01, PNorm = 116.7168, GNorm = 0.2645
Meta loss on this task batch = 2.8591e-01, Meta loss averaged over last 500 steps = 2.9628e-01, PNorm = 116.7202, GNorm = 0.2756
Meta loss on this task batch = 3.3285e-01, Meta loss averaged over last 500 steps = 2.9635e-01, PNorm = 116.7226, GNorm = 0.3186
Meta loss on this task batch = 2.7937e-01, Meta loss averaged over last 500 steps = 2.9634e-01, PNorm = 116.7251, GNorm = 0.2575
Meta loss on this task batch = 3.0144e-01, Meta loss averaged over last 500 steps = 2.9641e-01, PNorm = 116.7290, GNorm = 0.2625
Meta loss on this task batch = 2.7153e-01, Meta loss averaged over last 500 steps = 2.9639e-01, PNorm = 116.7323, GNorm = 0.2695
Meta loss on this task batch = 2.8835e-01, Meta loss averaged over last 500 steps = 2.9627e-01, PNorm = 116.7365, GNorm = 0.2837
Meta loss on this task batch = 2.7120e-01, Meta loss averaged over last 500 steps = 2.9623e-01, PNorm = 116.7398, GNorm = 0.2873
Meta loss on this task batch = 2.9976e-01, Meta loss averaged over last 500 steps = 2.9625e-01, PNorm = 116.7428, GNorm = 0.3526
Meta loss on this task batch = 2.7127e-01, Meta loss averaged over last 500 steps = 2.9612e-01, PNorm = 116.7462, GNorm = 0.3079
Meta loss on this task batch = 3.1168e-01, Meta loss averaged over last 500 steps = 2.9615e-01, PNorm = 116.7475, GNorm = 0.3289
Meta loss on this task batch = 2.8296e-01, Meta loss averaged over last 500 steps = 2.9621e-01, PNorm = 116.7510, GNorm = 0.2621
Meta loss on this task batch = 2.4905e-01, Meta loss averaged over last 500 steps = 2.9607e-01, PNorm = 116.7569, GNorm = 0.3149
Meta loss on this task batch = 2.9880e-01, Meta loss averaged over last 500 steps = 2.9611e-01, PNorm = 116.7644, GNorm = 0.2982
Meta loss on this task batch = 3.1925e-01, Meta loss averaged over last 500 steps = 2.9614e-01, PNorm = 116.7725, GNorm = 0.2838
Meta loss on this task batch = 2.4231e-01, Meta loss averaged over last 500 steps = 2.9606e-01, PNorm = 116.7796, GNorm = 0.2511
Meta loss on this task batch = 2.7463e-01, Meta loss averaged over last 500 steps = 2.9604e-01, PNorm = 116.7900, GNorm = 0.2644
Meta loss on this task batch = 3.5298e-01, Meta loss averaged over last 500 steps = 2.9618e-01, PNorm = 116.8008, GNorm = 0.3261
Meta loss on this task batch = 3.1643e-01, Meta loss averaged over last 500 steps = 2.9622e-01, PNorm = 116.8133, GNorm = 0.3611
Took 131.43072772026062 seconds to complete one epoch of meta training
Took 139.05845308303833 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498477
Epoch 445
Meta loss on this task batch = 3.0692e-01, Meta loss averaged over last 500 steps = 2.9632e-01, PNorm = 116.8246, GNorm = 0.2996
Meta loss on this task batch = 3.6901e-01, Meta loss averaged over last 500 steps = 2.9649e-01, PNorm = 116.8323, GNorm = 0.3199
Meta loss on this task batch = 3.4392e-01, Meta loss averaged over last 500 steps = 2.9647e-01, PNorm = 116.8401, GNorm = 0.3144
Meta loss on this task batch = 3.4870e-01, Meta loss averaged over last 500 steps = 2.9640e-01, PNorm = 116.8468, GNorm = 0.3268
Meta loss on this task batch = 2.6800e-01, Meta loss averaged over last 500 steps = 2.9634e-01, PNorm = 116.8526, GNorm = 0.2420
Meta loss on this task batch = 2.9403e-01, Meta loss averaged over last 500 steps = 2.9634e-01, PNorm = 116.8597, GNorm = 0.2754
Meta loss on this task batch = 2.9594e-01, Meta loss averaged over last 500 steps = 2.9629e-01, PNorm = 116.8644, GNorm = 0.2559
Meta loss on this task batch = 2.8944e-01, Meta loss averaged over last 500 steps = 2.9632e-01, PNorm = 116.8695, GNorm = 0.3068
Meta loss on this task batch = 2.8248e-01, Meta loss averaged over last 500 steps = 2.9624e-01, PNorm = 116.8737, GNorm = 0.2557
Meta loss on this task batch = 3.3274e-01, Meta loss averaged over last 500 steps = 2.9638e-01, PNorm = 116.8743, GNorm = 0.2970
Meta loss on this task batch = 2.3868e-01, Meta loss averaged over last 500 steps = 2.9631e-01, PNorm = 116.8747, GNorm = 0.2606
Meta loss on this task batch = 2.8237e-01, Meta loss averaged over last 500 steps = 2.9631e-01, PNorm = 116.8758, GNorm = 0.2685
Meta loss on this task batch = 2.6393e-01, Meta loss averaged over last 500 steps = 2.9633e-01, PNorm = 116.8766, GNorm = 0.2732
Meta loss on this task batch = 3.2395e-01, Meta loss averaged over last 500 steps = 2.9626e-01, PNorm = 116.8819, GNorm = 0.3340
Meta loss on this task batch = 2.3675e-01, Meta loss averaged over last 500 steps = 2.9609e-01, PNorm = 116.8882, GNorm = 0.2737
Meta loss on this task batch = 3.4985e-01, Meta loss averaged over last 500 steps = 2.9614e-01, PNorm = 116.8916, GNorm = 0.3043
Meta loss on this task batch = 2.4415e-01, Meta loss averaged over last 500 steps = 2.9607e-01, PNorm = 116.8992, GNorm = 0.2719
Meta loss on this task batch = 2.7096e-01, Meta loss averaged over last 500 steps = 2.9591e-01, PNorm = 116.9096, GNorm = 0.2633
Meta loss on this task batch = 2.4173e-01, Meta loss averaged over last 500 steps = 2.9583e-01, PNorm = 116.9190, GNorm = 0.2874
Took 130.51460480690002 seconds to complete one epoch of meta training
Took 138.7623417377472 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476812
Epoch 446
Meta loss on this task batch = 3.2977e-01, Meta loss averaged over last 500 steps = 2.9576e-01, PNorm = 116.9266, GNorm = 0.2874
Meta loss on this task batch = 2.6761e-01, Meta loss averaged over last 500 steps = 2.9579e-01, PNorm = 116.9368, GNorm = 0.3080
Meta loss on this task batch = 2.5928e-01, Meta loss averaged over last 500 steps = 2.9558e-01, PNorm = 116.9457, GNorm = 0.2907
Meta loss on this task batch = 2.9212e-01, Meta loss averaged over last 500 steps = 2.9547e-01, PNorm = 116.9514, GNorm = 0.3075
Meta loss on this task batch = 3.3566e-01, Meta loss averaged over last 500 steps = 2.9564e-01, PNorm = 116.9555, GNorm = 0.3266
Meta loss on this task batch = 3.2434e-01, Meta loss averaged over last 500 steps = 2.9560e-01, PNorm = 116.9592, GNorm = 0.3426
Meta loss on this task batch = 3.3330e-01, Meta loss averaged over last 500 steps = 2.9563e-01, PNorm = 116.9630, GNorm = 0.3103
Meta loss on this task batch = 3.0439e-01, Meta loss averaged over last 500 steps = 2.9574e-01, PNorm = 116.9687, GNorm = 0.3088
Meta loss on this task batch = 2.7884e-01, Meta loss averaged over last 500 steps = 2.9574e-01, PNorm = 116.9736, GNorm = 0.2910
Meta loss on this task batch = 3.1554e-01, Meta loss averaged over last 500 steps = 2.9581e-01, PNorm = 116.9789, GNorm = 0.3192
Meta loss on this task batch = 2.4089e-01, Meta loss averaged over last 500 steps = 2.9568e-01, PNorm = 116.9843, GNorm = 0.2455
Meta loss on this task batch = 2.8256e-01, Meta loss averaged over last 500 steps = 2.9568e-01, PNorm = 116.9932, GNorm = 0.2854
Meta loss on this task batch = 2.9999e-01, Meta loss averaged over last 500 steps = 2.9579e-01, PNorm = 117.0024, GNorm = 0.3105
Meta loss on this task batch = 2.5608e-01, Meta loss averaged over last 500 steps = 2.9563e-01, PNorm = 117.0115, GNorm = 0.2624
Meta loss on this task batch = 3.1994e-01, Meta loss averaged over last 500 steps = 2.9571e-01, PNorm = 117.0195, GNorm = 0.3238
Meta loss on this task batch = 2.4159e-01, Meta loss averaged over last 500 steps = 2.9553e-01, PNorm = 117.0274, GNorm = 0.2633
Meta loss on this task batch = 2.9356e-01, Meta loss averaged over last 500 steps = 2.9556e-01, PNorm = 117.0357, GNorm = 0.3008
Meta loss on this task batch = 2.7267e-01, Meta loss averaged over last 500 steps = 2.9558e-01, PNorm = 117.0423, GNorm = 0.2878
Meta loss on this task batch = 2.9469e-01, Meta loss averaged over last 500 steps = 2.9560e-01, PNorm = 117.0470, GNorm = 0.3413
Took 128.11005973815918 seconds to complete one epoch of meta training
Took 136.4099612236023 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496531
Epoch 447
Meta loss on this task batch = 2.8805e-01, Meta loss averaged over last 500 steps = 2.9547e-01, PNorm = 117.0505, GNorm = 0.2955
Meta loss on this task batch = 2.9453e-01, Meta loss averaged over last 500 steps = 2.9534e-01, PNorm = 117.0528, GNorm = 0.3151
Meta loss on this task batch = 2.6684e-01, Meta loss averaged over last 500 steps = 2.9531e-01, PNorm = 117.0543, GNorm = 0.2787
Meta loss on this task batch = 3.2823e-01, Meta loss averaged over last 500 steps = 2.9543e-01, PNorm = 117.0515, GNorm = 0.3287
Meta loss on this task batch = 3.2700e-01, Meta loss averaged over last 500 steps = 2.9554e-01, PNorm = 117.0504, GNorm = 0.3119
Meta loss on this task batch = 2.5093e-01, Meta loss averaged over last 500 steps = 2.9550e-01, PNorm = 117.0488, GNorm = 0.2799
Meta loss on this task batch = 3.4038e-01, Meta loss averaged over last 500 steps = 2.9573e-01, PNorm = 117.0480, GNorm = 0.2881
Meta loss on this task batch = 2.9012e-01, Meta loss averaged over last 500 steps = 2.9573e-01, PNorm = 117.0464, GNorm = 0.2966
Meta loss on this task batch = 2.6512e-01, Meta loss averaged over last 500 steps = 2.9565e-01, PNorm = 117.0456, GNorm = 0.2785
Meta loss on this task batch = 3.3504e-01, Meta loss averaged over last 500 steps = 2.9558e-01, PNorm = 117.0417, GNorm = 0.4876
Meta loss on this task batch = 3.0787e-01, Meta loss averaged over last 500 steps = 2.9569e-01, PNorm = 117.0411, GNorm = 0.2962
Meta loss on this task batch = 2.7875e-01, Meta loss averaged over last 500 steps = 2.9561e-01, PNorm = 117.0417, GNorm = 0.2941
Meta loss on this task batch = 2.5748e-01, Meta loss averaged over last 500 steps = 2.9561e-01, PNorm = 117.0432, GNorm = 0.2586
Meta loss on this task batch = 2.6257e-01, Meta loss averaged over last 500 steps = 2.9560e-01, PNorm = 117.0472, GNorm = 0.2496
Meta loss on this task batch = 2.5138e-01, Meta loss averaged over last 500 steps = 2.9558e-01, PNorm = 117.0514, GNorm = 0.3023
Meta loss on this task batch = 2.8798e-01, Meta loss averaged over last 500 steps = 2.9557e-01, PNorm = 117.0549, GNorm = 0.2801
Meta loss on this task batch = 3.4036e-01, Meta loss averaged over last 500 steps = 2.9571e-01, PNorm = 117.0573, GNorm = 0.2926
Meta loss on this task batch = 3.0820e-01, Meta loss averaged over last 500 steps = 2.9568e-01, PNorm = 117.0594, GNorm = 0.2781
Meta loss on this task batch = 2.7591e-01, Meta loss averaged over last 500 steps = 2.9565e-01, PNorm = 117.0584, GNorm = 0.3914
Took 126.22297739982605 seconds to complete one epoch of meta training
Took 134.28171968460083 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496422
Epoch 448
Meta loss on this task batch = 2.8076e-01, Meta loss averaged over last 500 steps = 2.9557e-01, PNorm = 117.0590, GNorm = 0.2908
Meta loss on this task batch = 2.8470e-01, Meta loss averaged over last 500 steps = 2.9551e-01, PNorm = 117.0611, GNorm = 0.2727
Meta loss on this task batch = 2.7617e-01, Meta loss averaged over last 500 steps = 2.9556e-01, PNorm = 117.0631, GNorm = 0.2996
Meta loss on this task batch = 3.0305e-01, Meta loss averaged over last 500 steps = 2.9555e-01, PNorm = 117.0642, GNorm = 0.3071
Meta loss on this task batch = 2.7234e-01, Meta loss averaged over last 500 steps = 2.9547e-01, PNorm = 117.0686, GNorm = 0.2650
Meta loss on this task batch = 3.2194e-01, Meta loss averaged over last 500 steps = 2.9544e-01, PNorm = 117.0708, GNorm = 0.3043
Meta loss on this task batch = 2.3653e-01, Meta loss averaged over last 500 steps = 2.9529e-01, PNorm = 117.0741, GNorm = 0.2499
Meta loss on this task batch = 3.1678e-01, Meta loss averaged over last 500 steps = 2.9545e-01, PNorm = 117.0773, GNorm = 0.2939
Meta loss on this task batch = 2.0879e-01, Meta loss averaged over last 500 steps = 2.9515e-01, PNorm = 117.0835, GNorm = 0.2685
Meta loss on this task batch = 3.6027e-01, Meta loss averaged over last 500 steps = 2.9532e-01, PNorm = 117.0876, GNorm = 0.3515
Meta loss on this task batch = 2.7799e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 117.0949, GNorm = 0.2790
Meta loss on this task batch = 3.0022e-01, Meta loss averaged over last 500 steps = 2.9534e-01, PNorm = 117.1038, GNorm = 0.2960
Meta loss on this task batch = 2.7535e-01, Meta loss averaged over last 500 steps = 2.9520e-01, PNorm = 117.1142, GNorm = 0.2809
Meta loss on this task batch = 3.3736e-01, Meta loss averaged over last 500 steps = 2.9527e-01, PNorm = 117.1248, GNorm = 0.3121
Meta loss on this task batch = 3.0812e-01, Meta loss averaged over last 500 steps = 2.9521e-01, PNorm = 117.1343, GNorm = 0.2682
Meta loss on this task batch = 2.9607e-01, Meta loss averaged over last 500 steps = 2.9521e-01, PNorm = 117.1424, GNorm = 0.3323
Meta loss on this task batch = 3.4783e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 117.1497, GNorm = 0.2947
Meta loss on this task batch = 2.9350e-01, Meta loss averaged over last 500 steps = 2.9544e-01, PNorm = 117.1572, GNorm = 0.3097
Meta loss on this task batch = 2.7559e-01, Meta loss averaged over last 500 steps = 2.9546e-01, PNorm = 117.1611, GNorm = 0.3412
Took 130.41823196411133 seconds to complete one epoch of meta training
Took 139.07190346717834 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491271
Epoch 449
Meta loss on this task batch = 2.5985e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 117.1638, GNorm = 0.2922
Meta loss on this task batch = 2.8260e-01, Meta loss averaged over last 500 steps = 2.9530e-01, PNorm = 117.1643, GNorm = 0.3221
Meta loss on this task batch = 2.6175e-01, Meta loss averaged over last 500 steps = 2.9524e-01, PNorm = 117.1642, GNorm = 0.2618
Meta loss on this task batch = 2.7470e-01, Meta loss averaged over last 500 steps = 2.9529e-01, PNorm = 117.1642, GNorm = 0.2595
Meta loss on this task batch = 3.0753e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 117.1628, GNorm = 0.3102
Meta loss on this task batch = 2.9210e-01, Meta loss averaged over last 500 steps = 2.9527e-01, PNorm = 117.1607, GNorm = 0.2724
Meta loss on this task batch = 2.9366e-01, Meta loss averaged over last 500 steps = 2.9516e-01, PNorm = 117.1571, GNorm = 0.2864
Meta loss on this task batch = 3.3171e-01, Meta loss averaged over last 500 steps = 2.9517e-01, PNorm = 117.1526, GNorm = 0.3161
Meta loss on this task batch = 3.0957e-01, Meta loss averaged over last 500 steps = 2.9530e-01, PNorm = 117.1493, GNorm = 0.2773
Meta loss on this task batch = 2.9482e-01, Meta loss averaged over last 500 steps = 2.9536e-01, PNorm = 117.1470, GNorm = 0.2738
Meta loss on this task batch = 3.3612e-01, Meta loss averaged over last 500 steps = 2.9553e-01, PNorm = 117.1462, GNorm = 0.3785
Meta loss on this task batch = 3.6102e-01, Meta loss averaged over last 500 steps = 2.9564e-01, PNorm = 117.1446, GNorm = 0.3408
Meta loss on this task batch = 2.7559e-01, Meta loss averaged over last 500 steps = 2.9563e-01, PNorm = 117.1455, GNorm = 0.2748
Meta loss on this task batch = 3.0748e-01, Meta loss averaged over last 500 steps = 2.9571e-01, PNorm = 117.1486, GNorm = 0.2965
Meta loss on this task batch = 2.6802e-01, Meta loss averaged over last 500 steps = 2.9564e-01, PNorm = 117.1547, GNorm = 0.2587
Meta loss on this task batch = 2.8676e-01, Meta loss averaged over last 500 steps = 2.9565e-01, PNorm = 117.1630, GNorm = 0.2922
Meta loss on this task batch = 3.2131e-01, Meta loss averaged over last 500 steps = 2.9574e-01, PNorm = 117.1727, GNorm = 0.2856
Meta loss on this task batch = 2.8879e-01, Meta loss averaged over last 500 steps = 2.9573e-01, PNorm = 117.1818, GNorm = 0.3091
Meta loss on this task batch = 2.2543e-01, Meta loss averaged over last 500 steps = 2.9556e-01, PNorm = 117.1901, GNorm = 0.2640
Took 127.33779788017273 seconds to complete one epoch of meta training
Took 135.3959310054779 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476021
Epoch 450
Meta loss on this task batch = 3.1867e-01, Meta loss averaged over last 500 steps = 2.9553e-01, PNorm = 117.1953, GNorm = 0.2973
Meta loss on this task batch = 2.4483e-01, Meta loss averaged over last 500 steps = 2.9531e-01, PNorm = 117.2000, GNorm = 0.2318
Meta loss on this task batch = 2.9839e-01, Meta loss averaged over last 500 steps = 2.9527e-01, PNorm = 117.2026, GNorm = 0.2881
Meta loss on this task batch = 2.7962e-01, Meta loss averaged over last 500 steps = 2.9530e-01, PNorm = 117.2050, GNorm = 0.2800
Meta loss on this task batch = 3.4303e-01, Meta loss averaged over last 500 steps = 2.9532e-01, PNorm = 117.2031, GNorm = 0.3121
Meta loss on this task batch = 3.2650e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 117.2021, GNorm = 0.3409
Meta loss on this task batch = 3.2597e-01, Meta loss averaged over last 500 steps = 2.9537e-01, PNorm = 117.2027, GNorm = 0.2988
Meta loss on this task batch = 3.7037e-01, Meta loss averaged over last 500 steps = 2.9562e-01, PNorm = 117.2039, GNorm = 0.3450
Meta loss on this task batch = 2.7408e-01, Meta loss averaged over last 500 steps = 2.9566e-01, PNorm = 117.2054, GNorm = 0.2772
Meta loss on this task batch = 3.0295e-01, Meta loss averaged over last 500 steps = 2.9566e-01, PNorm = 117.2067, GNorm = 0.3047
Meta loss on this task batch = 2.3167e-01, Meta loss averaged over last 500 steps = 2.9550e-01, PNorm = 117.2081, GNorm = 0.2764
Meta loss on this task batch = 2.7015e-01, Meta loss averaged over last 500 steps = 2.9532e-01, PNorm = 117.2116, GNorm = 0.2869
Meta loss on this task batch = 2.6085e-01, Meta loss averaged over last 500 steps = 2.9524e-01, PNorm = 117.2138, GNorm = 0.2682
Meta loss on this task batch = 2.6130e-01, Meta loss averaged over last 500 steps = 2.9517e-01, PNorm = 117.2167, GNorm = 0.2788
Meta loss on this task batch = 2.7858e-01, Meta loss averaged over last 500 steps = 2.9508e-01, PNorm = 117.2204, GNorm = 0.2748
Meta loss on this task batch = 3.0210e-01, Meta loss averaged over last 500 steps = 2.9505e-01, PNorm = 117.2234, GNorm = 0.3198
Meta loss on this task batch = 3.0252e-01, Meta loss averaged over last 500 steps = 2.9502e-01, PNorm = 117.2283, GNorm = 0.4042
Meta loss on this task batch = 3.0316e-01, Meta loss averaged over last 500 steps = 2.9501e-01, PNorm = 117.2320, GNorm = 0.3328
Meta loss on this task batch = 2.9224e-01, Meta loss averaged over last 500 steps = 2.9512e-01, PNorm = 117.2364, GNorm = 0.3587
Took 125.80691504478455 seconds to complete one epoch of meta training
Took 132.72967529296875 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485973
Epoch 451
Meta loss on this task batch = 2.3773e-01, Meta loss averaged over last 500 steps = 2.9491e-01, PNorm = 117.2421, GNorm = 0.2769
Meta loss on this task batch = 3.3937e-01, Meta loss averaged over last 500 steps = 2.9500e-01, PNorm = 117.2459, GNorm = 0.3232
Meta loss on this task batch = 3.1588e-01, Meta loss averaged over last 500 steps = 2.9499e-01, PNorm = 117.2522, GNorm = 0.3229
Meta loss on this task batch = 2.7786e-01, Meta loss averaged over last 500 steps = 2.9501e-01, PNorm = 117.2595, GNorm = 0.2894
Meta loss on this task batch = 2.9627e-01, Meta loss averaged over last 500 steps = 2.9501e-01, PNorm = 117.2667, GNorm = 0.3083
Meta loss on this task batch = 2.6739e-01, Meta loss averaged over last 500 steps = 2.9484e-01, PNorm = 117.2754, GNorm = 0.2761
Meta loss on this task batch = 2.9521e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 117.2831, GNorm = 0.3216
Meta loss on this task batch = 2.9544e-01, Meta loss averaged over last 500 steps = 2.9501e-01, PNorm = 117.2922, GNorm = 0.3102
Meta loss on this task batch = 2.9107e-01, Meta loss averaged over last 500 steps = 2.9482e-01, PNorm = 117.3012, GNorm = 0.2551
Meta loss on this task batch = 3.0670e-01, Meta loss averaged over last 500 steps = 2.9480e-01, PNorm = 117.3109, GNorm = 0.2970
Meta loss on this task batch = 3.1626e-01, Meta loss averaged over last 500 steps = 2.9480e-01, PNorm = 117.3203, GNorm = 0.2914
Meta loss on this task batch = 2.4876e-01, Meta loss averaged over last 500 steps = 2.9468e-01, PNorm = 117.3300, GNorm = 0.2513
Meta loss on this task batch = 2.7065e-01, Meta loss averaged over last 500 steps = 2.9468e-01, PNorm = 117.3394, GNorm = 0.3166
Meta loss on this task batch = 2.6691e-01, Meta loss averaged over last 500 steps = 2.9463e-01, PNorm = 117.3493, GNorm = 0.2534
Meta loss on this task batch = 2.9704e-01, Meta loss averaged over last 500 steps = 2.9466e-01, PNorm = 117.3578, GNorm = 0.2903
Meta loss on this task batch = 2.7296e-01, Meta loss averaged over last 500 steps = 2.9468e-01, PNorm = 117.3619, GNorm = 0.3608
Meta loss on this task batch = 3.6081e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 117.3627, GNorm = 0.3441
Meta loss on this task batch = 3.1216e-01, Meta loss averaged over last 500 steps = 2.9503e-01, PNorm = 117.3629, GNorm = 0.3050
Meta loss on this task batch = 3.0920e-01, Meta loss averaged over last 500 steps = 2.9492e-01, PNorm = 117.3623, GNorm = 0.3704
Took 127.06519269943237 seconds to complete one epoch of meta training
Took 134.9760992527008 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506286
Epoch 452
Meta loss on this task batch = 3.1899e-01, Meta loss averaged over last 500 steps = 2.9503e-01, PNorm = 117.3614, GNorm = 0.3248
Meta loss on this task batch = 2.8415e-01, Meta loss averaged over last 500 steps = 2.9507e-01, PNorm = 117.3617, GNorm = 0.3198
Meta loss on this task batch = 2.7400e-01, Meta loss averaged over last 500 steps = 2.9516e-01, PNorm = 117.3650, GNorm = 0.2711
Meta loss on this task batch = 2.8232e-01, Meta loss averaged over last 500 steps = 2.9518e-01, PNorm = 117.3709, GNorm = 0.2622
Meta loss on this task batch = 2.6479e-01, Meta loss averaged over last 500 steps = 2.9517e-01, PNorm = 117.3797, GNorm = 0.2755
Meta loss on this task batch = 2.9310e-01, Meta loss averaged over last 500 steps = 2.9511e-01, PNorm = 117.3901, GNorm = 0.2867
Meta loss on this task batch = 3.4595e-01, Meta loss averaged over last 500 steps = 2.9519e-01, PNorm = 117.4020, GNorm = 0.2998
Meta loss on this task batch = 2.9224e-01, Meta loss averaged over last 500 steps = 2.9516e-01, PNorm = 117.4162, GNorm = 0.2569
Meta loss on this task batch = 2.6614e-01, Meta loss averaged over last 500 steps = 2.9516e-01, PNorm = 117.4294, GNorm = 0.2333
Meta loss on this task batch = 2.9387e-01, Meta loss averaged over last 500 steps = 2.9512e-01, PNorm = 117.4404, GNorm = 0.2678
Meta loss on this task batch = 2.7572e-01, Meta loss averaged over last 500 steps = 2.9522e-01, PNorm = 117.4498, GNorm = 0.3160
Meta loss on this task batch = 3.0997e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 117.4576, GNorm = 0.2984
Meta loss on this task batch = 3.0683e-01, Meta loss averaged over last 500 steps = 2.9540e-01, PNorm = 117.4637, GNorm = 0.2838
Meta loss on this task batch = 3.0292e-01, Meta loss averaged over last 500 steps = 2.9544e-01, PNorm = 117.4697, GNorm = 0.2676
Meta loss on this task batch = 2.8318e-01, Meta loss averaged over last 500 steps = 2.9538e-01, PNorm = 117.4732, GNorm = 0.3331
Meta loss on this task batch = 2.3302e-01, Meta loss averaged over last 500 steps = 2.9532e-01, PNorm = 117.4779, GNorm = 0.2814
Meta loss on this task batch = 3.3533e-01, Meta loss averaged over last 500 steps = 2.9540e-01, PNorm = 117.4816, GNorm = 0.3996
Meta loss on this task batch = 2.5820e-01, Meta loss averaged over last 500 steps = 2.9538e-01, PNorm = 117.4851, GNorm = 0.2774
Meta loss on this task batch = 3.1779e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 117.4886, GNorm = 0.3461
Took 127.40024256706238 seconds to complete one epoch of meta training
Took 135.58350563049316 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487675
Epoch 453
Meta loss on this task batch = 3.0249e-01, Meta loss averaged over last 500 steps = 2.9530e-01, PNorm = 117.4912, GNorm = 0.3002
Meta loss on this task batch = 3.3702e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 117.4956, GNorm = 0.2678
Meta loss on this task batch = 3.0809e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 117.5023, GNorm = 0.2940
Meta loss on this task batch = 2.7529e-01, Meta loss averaged over last 500 steps = 2.9533e-01, PNorm = 117.5087, GNorm = 0.2633
Meta loss on this task batch = 2.6612e-01, Meta loss averaged over last 500 steps = 2.9530e-01, PNorm = 117.5166, GNorm = 0.2752
Meta loss on this task batch = 3.1847e-01, Meta loss averaged over last 500 steps = 2.9537e-01, PNorm = 117.5256, GNorm = 0.3140
Meta loss on this task batch = 2.8080e-01, Meta loss averaged over last 500 steps = 2.9545e-01, PNorm = 117.5367, GNorm = 0.2791
Meta loss on this task batch = 2.8436e-01, Meta loss averaged over last 500 steps = 2.9542e-01, PNorm = 117.5488, GNorm = 0.2779
Meta loss on this task batch = 3.0587e-01, Meta loss averaged over last 500 steps = 2.9545e-01, PNorm = 117.5598, GNorm = 0.2840
Meta loss on this task batch = 3.0596e-01, Meta loss averaged over last 500 steps = 2.9542e-01, PNorm = 117.5690, GNorm = 0.3554
Meta loss on this task batch = 2.6011e-01, Meta loss averaged over last 500 steps = 2.9533e-01, PNorm = 117.5758, GNorm = 0.3034
Meta loss on this task batch = 3.1009e-01, Meta loss averaged over last 500 steps = 2.9537e-01, PNorm = 117.5795, GNorm = 0.3496
Meta loss on this task batch = 3.1857e-01, Meta loss averaged over last 500 steps = 2.9543e-01, PNorm = 117.5827, GNorm = 0.3041
Meta loss on this task batch = 2.5569e-01, Meta loss averaged over last 500 steps = 2.9534e-01, PNorm = 117.5867, GNorm = 0.3105
Meta loss on this task batch = 2.4397e-01, Meta loss averaged over last 500 steps = 2.9526e-01, PNorm = 117.5904, GNorm = 0.2762
Meta loss on this task batch = 3.1558e-01, Meta loss averaged over last 500 steps = 2.9527e-01, PNorm = 117.5958, GNorm = 0.3227
Meta loss on this task batch = 2.4182e-01, Meta loss averaged over last 500 steps = 2.9532e-01, PNorm = 117.6031, GNorm = 0.2755
Meta loss on this task batch = 2.6929e-01, Meta loss averaged over last 500 steps = 2.9531e-01, PNorm = 117.6095, GNorm = 0.3153
Meta loss on this task batch = 2.8198e-01, Meta loss averaged over last 500 steps = 2.9519e-01, PNorm = 117.6137, GNorm = 0.3794
Took 127.11002659797668 seconds to complete one epoch of meta training
Took 135.44160723686218 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497343
Epoch 454
Meta loss on this task batch = 2.5556e-01, Meta loss averaged over last 500 steps = 2.9512e-01, PNorm = 117.6186, GNorm = 0.2667
Meta loss on this task batch = 3.2139e-01, Meta loss averaged over last 500 steps = 2.9509e-01, PNorm = 117.6225, GNorm = 0.3322
Meta loss on this task batch = 3.1894e-01, Meta loss averaged over last 500 steps = 2.9512e-01, PNorm = 117.6250, GNorm = 0.3526
Meta loss on this task batch = 3.3730e-01, Meta loss averaged over last 500 steps = 2.9523e-01, PNorm = 117.6270, GNorm = 0.3372
Meta loss on this task batch = 3.3058e-01, Meta loss averaged over last 500 steps = 2.9538e-01, PNorm = 117.6278, GNorm = 0.3200
Meta loss on this task batch = 3.6719e-01, Meta loss averaged over last 500 steps = 2.9549e-01, PNorm = 117.6309, GNorm = 0.3224
Meta loss on this task batch = 2.6987e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 117.6339, GNorm = 0.2849
Meta loss on this task batch = 2.4593e-01, Meta loss averaged over last 500 steps = 2.9532e-01, PNorm = 117.6385, GNorm = 0.2521
Meta loss on this task batch = 2.7507e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 117.6444, GNorm = 0.2999
Meta loss on this task batch = 2.7189e-01, Meta loss averaged over last 500 steps = 2.9534e-01, PNorm = 117.6487, GNorm = 0.2832
Meta loss on this task batch = 3.2443e-01, Meta loss averaged over last 500 steps = 2.9541e-01, PNorm = 117.6520, GNorm = 0.3193
Meta loss on this task batch = 2.3572e-01, Meta loss averaged over last 500 steps = 2.9533e-01, PNorm = 117.6560, GNorm = 0.2557
Meta loss on this task batch = 2.5260e-01, Meta loss averaged over last 500 steps = 2.9524e-01, PNorm = 117.6585, GNorm = 0.2693
Meta loss on this task batch = 2.9173e-01, Meta loss averaged over last 500 steps = 2.9529e-01, PNorm = 117.6584, GNorm = 0.3655
Meta loss on this task batch = 2.3279e-01, Meta loss averaged over last 500 steps = 2.9519e-01, PNorm = 117.6575, GNorm = 0.2816
Meta loss on this task batch = 2.7582e-01, Meta loss averaged over last 500 steps = 2.9507e-01, PNorm = 117.6550, GNorm = 0.3079
Meta loss on this task batch = 2.7148e-01, Meta loss averaged over last 500 steps = 2.9495e-01, PNorm = 117.6523, GNorm = 0.3289
Meta loss on this task batch = 2.7921e-01, Meta loss averaged over last 500 steps = 2.9497e-01, PNorm = 117.6516, GNorm = 0.2988
Meta loss on this task batch = 3.0863e-01, Meta loss averaged over last 500 steps = 2.9490e-01, PNorm = 117.6517, GNorm = 0.3581
Took 129.44592428207397 seconds to complete one epoch of meta training
Took 137.6295325756073 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489056
Epoch 455
Meta loss on this task batch = 3.0555e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 117.6568, GNorm = 0.3178
Meta loss on this task batch = 3.0871e-01, Meta loss averaged over last 500 steps = 2.9485e-01, PNorm = 117.6642, GNorm = 0.2894
Meta loss on this task batch = 2.3218e-01, Meta loss averaged over last 500 steps = 2.9474e-01, PNorm = 117.6741, GNorm = 0.2602
Meta loss on this task batch = 3.0998e-01, Meta loss averaged over last 500 steps = 2.9461e-01, PNorm = 117.6863, GNorm = 0.3230
Meta loss on this task batch = 3.0815e-01, Meta loss averaged over last 500 steps = 2.9461e-01, PNorm = 117.7025, GNorm = 0.3203
Meta loss on this task batch = 2.9352e-01, Meta loss averaged over last 500 steps = 2.9462e-01, PNorm = 117.7222, GNorm = 0.3201
Meta loss on this task batch = 3.0115e-01, Meta loss averaged over last 500 steps = 2.9462e-01, PNorm = 117.7436, GNorm = 0.3528
Meta loss on this task batch = 2.8509e-01, Meta loss averaged over last 500 steps = 2.9455e-01, PNorm = 117.7635, GNorm = 0.2670
Meta loss on this task batch = 2.9733e-01, Meta loss averaged over last 500 steps = 2.9465e-01, PNorm = 117.7809, GNorm = 0.3180
Meta loss on this task batch = 2.8148e-01, Meta loss averaged over last 500 steps = 2.9468e-01, PNorm = 117.7961, GNorm = 0.3083
Meta loss on this task batch = 2.8104e-01, Meta loss averaged over last 500 steps = 2.9464e-01, PNorm = 117.8063, GNorm = 0.3289
Meta loss on this task batch = 3.2136e-01, Meta loss averaged over last 500 steps = 2.9472e-01, PNorm = 117.8134, GNorm = 0.3337
Meta loss on this task batch = 3.2335e-01, Meta loss averaged over last 500 steps = 2.9457e-01, PNorm = 117.8146, GNorm = 0.2933
Meta loss on this task batch = 3.1611e-01, Meta loss averaged over last 500 steps = 2.9464e-01, PNorm = 117.8130, GNorm = 0.2996
Meta loss on this task batch = 2.9993e-01, Meta loss averaged over last 500 steps = 2.9461e-01, PNorm = 117.8104, GNorm = 0.3423
Meta loss on this task batch = 3.3359e-01, Meta loss averaged over last 500 steps = 2.9467e-01, PNorm = 117.8066, GNorm = 0.3773
Meta loss on this task batch = 2.5305e-01, Meta loss averaged over last 500 steps = 2.9447e-01, PNorm = 117.8057, GNorm = 0.2801
Meta loss on this task batch = 3.2795e-01, Meta loss averaged over last 500 steps = 2.9453e-01, PNorm = 117.8044, GNorm = 0.3343
Meta loss on this task batch = 2.6994e-01, Meta loss averaged over last 500 steps = 2.9437e-01, PNorm = 117.8047, GNorm = 0.3255
Took 128.1887595653534 seconds to complete one epoch of meta training
Took 136.86297702789307 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501494
Epoch 456
Meta loss on this task batch = 2.7003e-01, Meta loss averaged over last 500 steps = 2.9422e-01, PNorm = 117.8058, GNorm = 0.2495
Meta loss on this task batch = 2.8495e-01, Meta loss averaged over last 500 steps = 2.9417e-01, PNorm = 117.8085, GNorm = 0.2829
Meta loss on this task batch = 3.6419e-01, Meta loss averaged over last 500 steps = 2.9434e-01, PNorm = 117.8101, GNorm = 0.3609
Meta loss on this task batch = 2.8518e-01, Meta loss averaged over last 500 steps = 2.9418e-01, PNorm = 117.8128, GNorm = 0.2978
Meta loss on this task batch = 3.0017e-01, Meta loss averaged over last 500 steps = 2.9407e-01, PNorm = 117.8148, GNorm = 0.3383
Meta loss on this task batch = 3.0985e-01, Meta loss averaged over last 500 steps = 2.9420e-01, PNorm = 117.8170, GNorm = 0.3032
Meta loss on this task batch = 2.4122e-01, Meta loss averaged over last 500 steps = 2.9403e-01, PNorm = 117.8189, GNorm = 0.2950
Meta loss on this task batch = 3.4043e-01, Meta loss averaged over last 500 steps = 2.9412e-01, PNorm = 117.8226, GNorm = 0.3911
Meta loss on this task batch = 2.6555e-01, Meta loss averaged over last 500 steps = 2.9403e-01, PNorm = 117.8265, GNorm = 0.2695
Meta loss on this task batch = 2.8157e-01, Meta loss averaged over last 500 steps = 2.9401e-01, PNorm = 117.8323, GNorm = 0.3445
Meta loss on this task batch = 3.0896e-01, Meta loss averaged over last 500 steps = 2.9403e-01, PNorm = 117.8386, GNorm = 0.3275
Meta loss on this task batch = 2.6795e-01, Meta loss averaged over last 500 steps = 2.9390e-01, PNorm = 117.8475, GNorm = 0.2807
Meta loss on this task batch = 3.1168e-01, Meta loss averaged over last 500 steps = 2.9396e-01, PNorm = 117.8580, GNorm = 0.2877
Meta loss on this task batch = 2.7732e-01, Meta loss averaged over last 500 steps = 2.9396e-01, PNorm = 117.8667, GNorm = 0.2768
Meta loss on this task batch = 2.9058e-01, Meta loss averaged over last 500 steps = 2.9396e-01, PNorm = 117.8736, GNorm = 0.2513
Meta loss on this task batch = 3.1017e-01, Meta loss averaged over last 500 steps = 2.9393e-01, PNorm = 117.8805, GNorm = 0.2857
Meta loss on this task batch = 3.1794e-01, Meta loss averaged over last 500 steps = 2.9396e-01, PNorm = 117.8847, GNorm = 0.2902
Meta loss on this task batch = 2.4551e-01, Meta loss averaged over last 500 steps = 2.9386e-01, PNorm = 117.8891, GNorm = 0.2721
Meta loss on this task batch = 3.3713e-01, Meta loss averaged over last 500 steps = 2.9391e-01, PNorm = 117.8953, GNorm = 0.3541
Took 129.94961619377136 seconds to complete one epoch of meta training
Took 138.1374065876007 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488141
Epoch 457
Meta loss on this task batch = 2.8400e-01, Meta loss averaged over last 500 steps = 2.9382e-01, PNorm = 117.9021, GNorm = 0.2625
Meta loss on this task batch = 2.8016e-01, Meta loss averaged over last 500 steps = 2.9379e-01, PNorm = 117.9093, GNorm = 0.2700
Meta loss on this task batch = 3.2207e-01, Meta loss averaged over last 500 steps = 2.9378e-01, PNorm = 117.9126, GNorm = 0.3213
Meta loss on this task batch = 2.9557e-01, Meta loss averaged over last 500 steps = 2.9378e-01, PNorm = 117.9160, GNorm = 0.2910
Meta loss on this task batch = 2.9097e-01, Meta loss averaged over last 500 steps = 2.9380e-01, PNorm = 117.9204, GNorm = 0.2810
Meta loss on this task batch = 3.1205e-01, Meta loss averaged over last 500 steps = 2.9382e-01, PNorm = 117.9259, GNorm = 0.2850
Meta loss on this task batch = 3.0019e-01, Meta loss averaged over last 500 steps = 2.9386e-01, PNorm = 117.9318, GNorm = 0.3072
Meta loss on this task batch = 2.8530e-01, Meta loss averaged over last 500 steps = 2.9394e-01, PNorm = 117.9372, GNorm = 0.2668
Meta loss on this task batch = 3.1069e-01, Meta loss averaged over last 500 steps = 2.9390e-01, PNorm = 117.9422, GNorm = 0.2946
Meta loss on this task batch = 2.6506e-01, Meta loss averaged over last 500 steps = 2.9387e-01, PNorm = 117.9494, GNorm = 0.2992
Meta loss on this task batch = 2.5660e-01, Meta loss averaged over last 500 steps = 2.9386e-01, PNorm = 117.9569, GNorm = 0.2665
Meta loss on this task batch = 2.6878e-01, Meta loss averaged over last 500 steps = 2.9382e-01, PNorm = 117.9643, GNorm = 0.2799
Meta loss on this task batch = 2.7885e-01, Meta loss averaged over last 500 steps = 2.9372e-01, PNorm = 117.9711, GNorm = 0.2860
Meta loss on this task batch = 3.0228e-01, Meta loss averaged over last 500 steps = 2.9377e-01, PNorm = 117.9776, GNorm = 0.3175
Meta loss on this task batch = 3.2833e-01, Meta loss averaged over last 500 steps = 2.9387e-01, PNorm = 117.9829, GNorm = 0.3579
Meta loss on this task batch = 2.7449e-01, Meta loss averaged over last 500 steps = 2.9371e-01, PNorm = 117.9862, GNorm = 0.3129
Meta loss on this task batch = 2.6761e-01, Meta loss averaged over last 500 steps = 2.9345e-01, PNorm = 117.9870, GNorm = 0.3474
Meta loss on this task batch = 3.4659e-01, Meta loss averaged over last 500 steps = 2.9348e-01, PNorm = 117.9856, GNorm = 0.3952
Meta loss on this task batch = 3.3680e-01, Meta loss averaged over last 500 steps = 2.9360e-01, PNorm = 117.9864, GNorm = 0.3790
Took 129.2252902984619 seconds to complete one epoch of meta training
Took 137.50577235221863 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482666
Epoch 458
Meta loss on this task batch = 2.8742e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 117.9887, GNorm = 0.2797
Meta loss on this task batch = 3.0390e-01, Meta loss averaged over last 500 steps = 2.9368e-01, PNorm = 117.9902, GNorm = 0.2838
Meta loss on this task batch = 2.8188e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 117.9953, GNorm = 0.3362
Meta loss on this task batch = 2.4716e-01, Meta loss averaged over last 500 steps = 2.9360e-01, PNorm = 118.0015, GNorm = 0.3016
Meta loss on this task batch = 2.8202e-01, Meta loss averaged over last 500 steps = 2.9357e-01, PNorm = 118.0103, GNorm = 0.2718
Meta loss on this task batch = 3.4508e-01, Meta loss averaged over last 500 steps = 2.9367e-01, PNorm = 118.0183, GNorm = 0.2783
Meta loss on this task batch = 3.1135e-01, Meta loss averaged over last 500 steps = 2.9371e-01, PNorm = 118.0240, GNorm = 0.3185
Meta loss on this task batch = 3.0540e-01, Meta loss averaged over last 500 steps = 2.9370e-01, PNorm = 118.0291, GNorm = 0.3422
Meta loss on this task batch = 2.7941e-01, Meta loss averaged over last 500 steps = 2.9371e-01, PNorm = 118.0320, GNorm = 0.2700
Meta loss on this task batch = 2.4736e-01, Meta loss averaged over last 500 steps = 2.9350e-01, PNorm = 118.0341, GNorm = 0.2756
Meta loss on this task batch = 3.3719e-01, Meta loss averaged over last 500 steps = 2.9350e-01, PNorm = 118.0357, GNorm = 0.3136
Meta loss on this task batch = 2.5879e-01, Meta loss averaged over last 500 steps = 2.9352e-01, PNorm = 118.0354, GNorm = 0.2785
Meta loss on this task batch = 3.0344e-01, Meta loss averaged over last 500 steps = 2.9362e-01, PNorm = 118.0343, GNorm = 0.3221
Meta loss on this task batch = 2.5878e-01, Meta loss averaged over last 500 steps = 2.9357e-01, PNorm = 118.0343, GNorm = 0.2826
Meta loss on this task batch = 2.9408e-01, Meta loss averaged over last 500 steps = 2.9356e-01, PNorm = 118.0352, GNorm = 0.3402
Meta loss on this task batch = 2.7822e-01, Meta loss averaged over last 500 steps = 2.9357e-01, PNorm = 118.0368, GNorm = 0.3024
Meta loss on this task batch = 3.4488e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 118.0406, GNorm = 0.3291
Meta loss on this task batch = 3.0855e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 118.0494, GNorm = 0.3863
Meta loss on this task batch = 3.1100e-01, Meta loss averaged over last 500 steps = 2.9374e-01, PNorm = 118.0596, GNorm = 0.3702
Took 129.08139395713806 seconds to complete one epoch of meta training
Took 137.15649104118347 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488151
Epoch 459
Meta loss on this task batch = 2.8500e-01, Meta loss averaged over last 500 steps = 2.9381e-01, PNorm = 118.0698, GNorm = 0.2981
Meta loss on this task batch = 2.7503e-01, Meta loss averaged over last 500 steps = 2.9364e-01, PNorm = 118.0822, GNorm = 0.2959
Meta loss on this task batch = 3.0844e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 118.0940, GNorm = 0.2654
Meta loss on this task batch = 3.3315e-01, Meta loss averaged over last 500 steps = 2.9376e-01, PNorm = 118.1044, GNorm = 0.3252
Meta loss on this task batch = 3.1393e-01, Meta loss averaged over last 500 steps = 2.9380e-01, PNorm = 118.1159, GNorm = 0.3022
Meta loss on this task batch = 3.4456e-01, Meta loss averaged over last 500 steps = 2.9388e-01, PNorm = 118.1236, GNorm = 0.3349
Meta loss on this task batch = 3.0194e-01, Meta loss averaged over last 500 steps = 2.9392e-01, PNorm = 118.1266, GNorm = 0.3613
Meta loss on this task batch = 2.9446e-01, Meta loss averaged over last 500 steps = 2.9395e-01, PNorm = 118.1295, GNorm = 0.2854
Meta loss on this task batch = 3.0134e-01, Meta loss averaged over last 500 steps = 2.9401e-01, PNorm = 118.1341, GNorm = 0.3240
Meta loss on this task batch = 3.4051e-01, Meta loss averaged over last 500 steps = 2.9418e-01, PNorm = 118.1392, GNorm = 0.2924
Meta loss on this task batch = 2.9744e-01, Meta loss averaged over last 500 steps = 2.9412e-01, PNorm = 118.1448, GNorm = 0.2855
Meta loss on this task batch = 2.5235e-01, Meta loss averaged over last 500 steps = 2.9405e-01, PNorm = 118.1502, GNorm = 0.2591
Meta loss on this task batch = 2.7683e-01, Meta loss averaged over last 500 steps = 2.9400e-01, PNorm = 118.1566, GNorm = 0.2628
Meta loss on this task batch = 2.8841e-01, Meta loss averaged over last 500 steps = 2.9394e-01, PNorm = 118.1613, GNorm = 0.2715
Meta loss on this task batch = 3.2195e-01, Meta loss averaged over last 500 steps = 2.9396e-01, PNorm = 118.1644, GNorm = 0.3252
Meta loss on this task batch = 2.6821e-01, Meta loss averaged over last 500 steps = 2.9375e-01, PNorm = 118.1667, GNorm = 0.3045
Meta loss on this task batch = 2.5398e-01, Meta loss averaged over last 500 steps = 2.9371e-01, PNorm = 118.1695, GNorm = 0.2832
Meta loss on this task batch = 3.2156e-01, Meta loss averaged over last 500 steps = 2.9376e-01, PNorm = 118.1727, GNorm = 0.3165
Meta loss on this task batch = 3.3932e-01, Meta loss averaged over last 500 steps = 2.9389e-01, PNorm = 118.1757, GNorm = 0.3468
Took 131.24365067481995 seconds to complete one epoch of meta training
Took 139.2918016910553 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493259
Epoch 460
Meta loss on this task batch = 2.5979e-01, Meta loss averaged over last 500 steps = 2.9376e-01, PNorm = 118.1799, GNorm = 0.2525
Meta loss on this task batch = 3.0915e-01, Meta loss averaged over last 500 steps = 2.9368e-01, PNorm = 118.1858, GNorm = 0.2793
Meta loss on this task batch = 2.7700e-01, Meta loss averaged over last 500 steps = 2.9356e-01, PNorm = 118.1915, GNorm = 0.3082
Meta loss on this task batch = 2.6376e-01, Meta loss averaged over last 500 steps = 2.9355e-01, PNorm = 118.1969, GNorm = 0.3370
Meta loss on this task batch = 2.9897e-01, Meta loss averaged over last 500 steps = 2.9362e-01, PNorm = 118.2031, GNorm = 0.2685
Meta loss on this task batch = 3.0877e-01, Meta loss averaged over last 500 steps = 2.9368e-01, PNorm = 118.2087, GNorm = 0.2756
Meta loss on this task batch = 3.0282e-01, Meta loss averaged over last 500 steps = 2.9372e-01, PNorm = 118.2159, GNorm = 0.2618
Meta loss on this task batch = 2.6676e-01, Meta loss averaged over last 500 steps = 2.9365e-01, PNorm = 118.2213, GNorm = 0.3284
Meta loss on this task batch = 2.8980e-01, Meta loss averaged over last 500 steps = 2.9352e-01, PNorm = 118.2278, GNorm = 0.2871
Meta loss on this task batch = 3.0971e-01, Meta loss averaged over last 500 steps = 2.9360e-01, PNorm = 118.2320, GNorm = 0.3457
Meta loss on this task batch = 3.2106e-01, Meta loss averaged over last 500 steps = 2.9364e-01, PNorm = 118.2350, GNorm = 0.3222
Meta loss on this task batch = 2.7622e-01, Meta loss averaged over last 500 steps = 2.9355e-01, PNorm = 118.2390, GNorm = 0.3066
Meta loss on this task batch = 2.7071e-01, Meta loss averaged over last 500 steps = 2.9360e-01, PNorm = 118.2455, GNorm = 0.2903
Meta loss on this task batch = 2.5345e-01, Meta loss averaged over last 500 steps = 2.9350e-01, PNorm = 118.2512, GNorm = 0.2638
Meta loss on this task batch = 2.9742e-01, Meta loss averaged over last 500 steps = 2.9348e-01, PNorm = 118.2576, GNorm = 0.3000
Meta loss on this task batch = 2.6533e-01, Meta loss averaged over last 500 steps = 2.9345e-01, PNorm = 118.2647, GNorm = 0.2758
Meta loss on this task batch = 3.2406e-01, Meta loss averaged over last 500 steps = 2.9355e-01, PNorm = 118.2732, GNorm = 0.3540
Meta loss on this task batch = 3.5959e-01, Meta loss averaged over last 500 steps = 2.9362e-01, PNorm = 118.2814, GNorm = 0.4169
Meta loss on this task batch = 3.0246e-01, Meta loss averaged over last 500 steps = 2.9358e-01, PNorm = 118.2911, GNorm = 0.3416
Took 132.09215235710144 seconds to complete one epoch of meta training
Took 139.45050024986267 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.516085
Epoch 461
Meta loss on this task batch = 3.4727e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 118.3005, GNorm = 0.3314
Meta loss on this task batch = 2.5887e-01, Meta loss averaged over last 500 steps = 2.9355e-01, PNorm = 118.3105, GNorm = 0.3171
Meta loss on this task batch = 2.9270e-01, Meta loss averaged over last 500 steps = 2.9365e-01, PNorm = 118.3214, GNorm = 0.3239
Meta loss on this task batch = 3.3247e-01, Meta loss averaged over last 500 steps = 2.9368e-01, PNorm = 118.3311, GNorm = 0.3225
Meta loss on this task batch = 2.8398e-01, Meta loss averaged over last 500 steps = 2.9370e-01, PNorm = 118.3379, GNorm = 0.2580
Meta loss on this task batch = 2.5300e-01, Meta loss averaged over last 500 steps = 2.9365e-01, PNorm = 118.3452, GNorm = 0.2814
Meta loss on this task batch = 2.7084e-01, Meta loss averaged over last 500 steps = 2.9375e-01, PNorm = 118.3510, GNorm = 0.2745
Meta loss on this task batch = 3.4307e-01, Meta loss averaged over last 500 steps = 2.9383e-01, PNorm = 118.3528, GNorm = 0.4497
Meta loss on this task batch = 3.0120e-01, Meta loss averaged over last 500 steps = 2.9384e-01, PNorm = 118.3540, GNorm = 0.3392
Meta loss on this task batch = 2.7313e-01, Meta loss averaged over last 500 steps = 2.9386e-01, PNorm = 118.3559, GNorm = 0.3072
Meta loss on this task batch = 3.2301e-01, Meta loss averaged over last 500 steps = 2.9389e-01, PNorm = 118.3548, GNorm = 0.3202
Meta loss on this task batch = 3.0631e-01, Meta loss averaged over last 500 steps = 2.9391e-01, PNorm = 118.3535, GNorm = 0.2823
Meta loss on this task batch = 2.4433e-01, Meta loss averaged over last 500 steps = 2.9385e-01, PNorm = 118.3535, GNorm = 0.2634
Meta loss on this task batch = 3.4057e-01, Meta loss averaged over last 500 steps = 2.9396e-01, PNorm = 118.3545, GNorm = 0.3124
Meta loss on this task batch = 2.4151e-01, Meta loss averaged over last 500 steps = 2.9397e-01, PNorm = 118.3611, GNorm = 0.3120
Meta loss on this task batch = 3.3390e-01, Meta loss averaged over last 500 steps = 2.9403e-01, PNorm = 118.3677, GNorm = 0.3500
Meta loss on this task batch = 3.3270e-01, Meta loss averaged over last 500 steps = 2.9400e-01, PNorm = 118.3774, GNorm = 0.3570
Meta loss on this task batch = 2.6549e-01, Meta loss averaged over last 500 steps = 2.9396e-01, PNorm = 118.3889, GNorm = 0.2909
Meta loss on this task batch = 3.2218e-01, Meta loss averaged over last 500 steps = 2.9402e-01, PNorm = 118.4007, GNorm = 0.3640
Took 127.95816469192505 seconds to complete one epoch of meta training
Took 136.2152614593506 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501887
Epoch 462
Meta loss on this task batch = 3.3417e-01, Meta loss averaged over last 500 steps = 2.9413e-01, PNorm = 118.4078, GNorm = 0.3187
Meta loss on this task batch = 2.8962e-01, Meta loss averaged over last 500 steps = 2.9410e-01, PNorm = 118.4145, GNorm = 0.2507
Meta loss on this task batch = 2.8646e-01, Meta loss averaged over last 500 steps = 2.9405e-01, PNorm = 118.4184, GNorm = 0.3034
Meta loss on this task batch = 3.2815e-01, Meta loss averaged over last 500 steps = 2.9410e-01, PNorm = 118.4176, GNorm = 0.3179
Meta loss on this task batch = 3.0461e-01, Meta loss averaged over last 500 steps = 2.9417e-01, PNorm = 118.4165, GNorm = 0.3227
Meta loss on this task batch = 3.0024e-01, Meta loss averaged over last 500 steps = 2.9422e-01, PNorm = 118.4118, GNorm = 0.3315
Meta loss on this task batch = 2.8791e-01, Meta loss averaged over last 500 steps = 2.9425e-01, PNorm = 118.4062, GNorm = 0.2849
Meta loss on this task batch = 2.8046e-01, Meta loss averaged over last 500 steps = 2.9429e-01, PNorm = 118.4010, GNorm = 0.2462
Meta loss on this task batch = 2.5221e-01, Meta loss averaged over last 500 steps = 2.9432e-01, PNorm = 118.3946, GNorm = 0.3132
Meta loss on this task batch = 3.0111e-01, Meta loss averaged over last 500 steps = 2.9430e-01, PNorm = 118.3898, GNorm = 0.3333
Meta loss on this task batch = 2.8084e-01, Meta loss averaged over last 500 steps = 2.9427e-01, PNorm = 118.3864, GNorm = 0.2482
Meta loss on this task batch = 3.0458e-01, Meta loss averaged over last 500 steps = 2.9432e-01, PNorm = 118.3863, GNorm = 0.2669
Meta loss on this task batch = 2.9825e-01, Meta loss averaged over last 500 steps = 2.9439e-01, PNorm = 118.3856, GNorm = 0.2788
Meta loss on this task batch = 2.3952e-01, Meta loss averaged over last 500 steps = 2.9428e-01, PNorm = 118.3866, GNorm = 0.2560
Meta loss on this task batch = 3.6275e-01, Meta loss averaged over last 500 steps = 2.9432e-01, PNorm = 118.3893, GNorm = 0.3476
Meta loss on this task batch = 2.4356e-01, Meta loss averaged over last 500 steps = 2.9424e-01, PNorm = 118.3948, GNorm = 0.2448
Meta loss on this task batch = 3.2731e-01, Meta loss averaged over last 500 steps = 2.9412e-01, PNorm = 118.4026, GNorm = 0.2994
Meta loss on this task batch = 2.9586e-01, Meta loss averaged over last 500 steps = 2.9403e-01, PNorm = 118.4113, GNorm = 0.2694
Meta loss on this task batch = 3.3655e-01, Meta loss averaged over last 500 steps = 2.9407e-01, PNorm = 118.4208, GNorm = 0.4281
Took 128.91757535934448 seconds to complete one epoch of meta training
Took 136.22526454925537 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492353
Epoch 463
Meta loss on this task batch = 2.9364e-01, Meta loss averaged over last 500 steps = 2.9411e-01, PNorm = 118.4297, GNorm = 0.2780
Meta loss on this task batch = 2.9968e-01, Meta loss averaged over last 500 steps = 2.9413e-01, PNorm = 118.4380, GNorm = 0.3030
Meta loss on this task batch = 3.1120e-01, Meta loss averaged over last 500 steps = 2.9416e-01, PNorm = 118.4451, GNorm = 0.2699
Meta loss on this task batch = 3.2694e-01, Meta loss averaged over last 500 steps = 2.9428e-01, PNorm = 118.4514, GNorm = 0.3103
Meta loss on this task batch = 3.2595e-01, Meta loss averaged over last 500 steps = 2.9439e-01, PNorm = 118.4574, GNorm = 0.2933
Meta loss on this task batch = 3.2258e-01, Meta loss averaged over last 500 steps = 2.9453e-01, PNorm = 118.4628, GNorm = 0.3010
Meta loss on this task batch = 3.0537e-01, Meta loss averaged over last 500 steps = 2.9460e-01, PNorm = 118.4666, GNorm = 0.2651
Meta loss on this task batch = 3.4100e-01, Meta loss averaged over last 500 steps = 2.9470e-01, PNorm = 118.4702, GNorm = 0.3246
Meta loss on this task batch = 2.9752e-01, Meta loss averaged over last 500 steps = 2.9462e-01, PNorm = 118.4738, GNorm = 0.2733
Meta loss on this task batch = 3.1388e-01, Meta loss averaged over last 500 steps = 2.9479e-01, PNorm = 118.4794, GNorm = 0.3081
Meta loss on this task batch = 2.8259e-01, Meta loss averaged over last 500 steps = 2.9475e-01, PNorm = 118.4877, GNorm = 0.2607
Meta loss on this task batch = 2.4432e-01, Meta loss averaged over last 500 steps = 2.9450e-01, PNorm = 118.4959, GNorm = 0.2686
Meta loss on this task batch = 2.8594e-01, Meta loss averaged over last 500 steps = 2.9446e-01, PNorm = 118.5054, GNorm = 0.3099
Meta loss on this task batch = 2.4943e-01, Meta loss averaged over last 500 steps = 2.9439e-01, PNorm = 118.5144, GNorm = 0.2908
Meta loss on this task batch = 2.7978e-01, Meta loss averaged over last 500 steps = 2.9443e-01, PNorm = 118.5232, GNorm = 0.3030
Meta loss on this task batch = 2.6624e-01, Meta loss averaged over last 500 steps = 2.9427e-01, PNorm = 118.5315, GNorm = 0.2826
Meta loss on this task batch = 3.0354e-01, Meta loss averaged over last 500 steps = 2.9426e-01, PNorm = 118.5358, GNorm = 0.3165
Meta loss on this task batch = 2.4300e-01, Meta loss averaged over last 500 steps = 2.9417e-01, PNorm = 118.5389, GNorm = 0.2898
Meta loss on this task batch = 2.5843e-01, Meta loss averaged over last 500 steps = 2.9419e-01, PNorm = 118.5413, GNorm = 0.3333
Took 130.54648089408875 seconds to complete one epoch of meta training
Took 138.9584801197052 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498336
Epoch 464
Meta loss on this task batch = 2.9596e-01, Meta loss averaged over last 500 steps = 2.9430e-01, PNorm = 118.5440, GNorm = 0.2857
Meta loss on this task batch = 3.0581e-01, Meta loss averaged over last 500 steps = 2.9426e-01, PNorm = 118.5456, GNorm = 0.3433
Meta loss on this task batch = 2.9898e-01, Meta loss averaged over last 500 steps = 2.9424e-01, PNorm = 118.5478, GNorm = 0.3136
Meta loss on this task batch = 3.0044e-01, Meta loss averaged over last 500 steps = 2.9426e-01, PNorm = 118.5525, GNorm = 0.2915
Meta loss on this task batch = 2.7411e-01, Meta loss averaged over last 500 steps = 2.9425e-01, PNorm = 118.5566, GNorm = 0.2864
Meta loss on this task batch = 2.5825e-01, Meta loss averaged over last 500 steps = 2.9413e-01, PNorm = 118.5611, GNorm = 0.2796
Meta loss on this task batch = 3.1375e-01, Meta loss averaged over last 500 steps = 2.9418e-01, PNorm = 118.5646, GNorm = 0.3591
Meta loss on this task batch = 3.1255e-01, Meta loss averaged over last 500 steps = 2.9428e-01, PNorm = 118.5670, GNorm = 0.3375
Meta loss on this task batch = 3.8898e-01, Meta loss averaged over last 500 steps = 2.9449e-01, PNorm = 118.5677, GNorm = 0.3377
Meta loss on this task batch = 3.0694e-01, Meta loss averaged over last 500 steps = 2.9448e-01, PNorm = 118.5690, GNorm = 0.2855
Meta loss on this task batch = 3.1116e-01, Meta loss averaged over last 500 steps = 2.9455e-01, PNorm = 118.5717, GNorm = 0.3044
Meta loss on this task batch = 2.6604e-01, Meta loss averaged over last 500 steps = 2.9450e-01, PNorm = 118.5775, GNorm = 0.2988
Meta loss on this task batch = 2.4733e-01, Meta loss averaged over last 500 steps = 2.9433e-01, PNorm = 118.5869, GNorm = 0.2994
Meta loss on this task batch = 3.2594e-01, Meta loss averaged over last 500 steps = 2.9436e-01, PNorm = 118.5968, GNorm = 0.3316
Meta loss on this task batch = 3.2335e-01, Meta loss averaged over last 500 steps = 2.9441e-01, PNorm = 118.6088, GNorm = 0.3359
Meta loss on this task batch = 2.4589e-01, Meta loss averaged over last 500 steps = 2.9439e-01, PNorm = 118.6191, GNorm = 0.2475
Meta loss on this task batch = 3.0318e-01, Meta loss averaged over last 500 steps = 2.9441e-01, PNorm = 118.6285, GNorm = 0.2764
Meta loss on this task batch = 2.1082e-01, Meta loss averaged over last 500 steps = 2.9423e-01, PNorm = 118.6378, GNorm = 0.2517
Meta loss on this task batch = 2.8308e-01, Meta loss averaged over last 500 steps = 2.9413e-01, PNorm = 118.6456, GNorm = 0.3169
Took 129.65577673912048 seconds to complete one epoch of meta training
Took 137.88172960281372 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.526390
Epoch 465
Meta loss on this task batch = 2.6663e-01, Meta loss averaged over last 500 steps = 2.9404e-01, PNorm = 118.6537, GNorm = 0.2615
Meta loss on this task batch = 3.1935e-01, Meta loss averaged over last 500 steps = 2.9409e-01, PNorm = 118.6612, GNorm = 0.3121
Meta loss on this task batch = 2.8760e-01, Meta loss averaged over last 500 steps = 2.9406e-01, PNorm = 118.6674, GNorm = 0.2499
Meta loss on this task batch = 3.3252e-01, Meta loss averaged over last 500 steps = 2.9416e-01, PNorm = 118.6728, GNorm = 0.3247
Meta loss on this task batch = 3.3619e-01, Meta loss averaged over last 500 steps = 2.9428e-01, PNorm = 118.6768, GNorm = 0.2698
Meta loss on this task batch = 2.5252e-01, Meta loss averaged over last 500 steps = 2.9422e-01, PNorm = 118.6822, GNorm = 0.3372
Meta loss on this task batch = 2.7518e-01, Meta loss averaged over last 500 steps = 2.9408e-01, PNorm = 118.6873, GNorm = 0.2692
Meta loss on this task batch = 2.8841e-01, Meta loss averaged over last 500 steps = 2.9403e-01, PNorm = 118.6932, GNorm = 0.3344
Meta loss on this task batch = 3.0495e-01, Meta loss averaged over last 500 steps = 2.9395e-01, PNorm = 118.6980, GNorm = 0.3046
Meta loss on this task batch = 2.8268e-01, Meta loss averaged over last 500 steps = 2.9386e-01, PNorm = 118.7024, GNorm = 0.3798
Meta loss on this task batch = 2.4233e-01, Meta loss averaged over last 500 steps = 2.9375e-01, PNorm = 118.7079, GNorm = 0.2742
Meta loss on this task batch = 2.2969e-01, Meta loss averaged over last 500 steps = 2.9363e-01, PNorm = 118.7144, GNorm = 0.2881
Meta loss on this task batch = 2.9254e-01, Meta loss averaged over last 500 steps = 2.9369e-01, PNorm = 118.7195, GNorm = 0.3161
Meta loss on this task batch = 2.8257e-01, Meta loss averaged over last 500 steps = 2.9373e-01, PNorm = 118.7215, GNorm = 0.2981
Meta loss on this task batch = 3.2075e-01, Meta loss averaged over last 500 steps = 2.9388e-01, PNorm = 118.7226, GNorm = 0.2985
Meta loss on this task batch = 3.3141e-01, Meta loss averaged over last 500 steps = 2.9399e-01, PNorm = 118.7238, GNorm = 0.3601
Meta loss on this task batch = 2.7111e-01, Meta loss averaged over last 500 steps = 2.9406e-01, PNorm = 118.7263, GNorm = 0.2733
Meta loss on this task batch = 2.5735e-01, Meta loss averaged over last 500 steps = 2.9395e-01, PNorm = 118.7301, GNorm = 0.2733
Meta loss on this task batch = 3.3620e-01, Meta loss averaged over last 500 steps = 2.9398e-01, PNorm = 118.7352, GNorm = 0.3649
Took 128.1481499671936 seconds to complete one epoch of meta training
Took 136.421324968338 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463261
Epoch 466
Meta loss on this task batch = 3.4304e-01, Meta loss averaged over last 500 steps = 2.9419e-01, PNorm = 118.7419, GNorm = 0.2899
Meta loss on this task batch = 3.2456e-01, Meta loss averaged over last 500 steps = 2.9418e-01, PNorm = 118.7508, GNorm = 0.2852
Meta loss on this task batch = 3.0912e-01, Meta loss averaged over last 500 steps = 2.9430e-01, PNorm = 118.7612, GNorm = 0.2779
Meta loss on this task batch = 3.0795e-01, Meta loss averaged over last 500 steps = 2.9430e-01, PNorm = 118.7717, GNorm = 0.3055
Meta loss on this task batch = 2.8124e-01, Meta loss averaged over last 500 steps = 2.9419e-01, PNorm = 118.7826, GNorm = 0.2616
Meta loss on this task batch = 2.8755e-01, Meta loss averaged over last 500 steps = 2.9423e-01, PNorm = 118.7933, GNorm = 0.2880
Meta loss on this task batch = 2.7402e-01, Meta loss averaged over last 500 steps = 2.9414e-01, PNorm = 118.8056, GNorm = 0.2570
Meta loss on this task batch = 3.0527e-01, Meta loss averaged over last 500 steps = 2.9426e-01, PNorm = 118.8183, GNorm = 0.3129
Meta loss on this task batch = 2.8391e-01, Meta loss averaged over last 500 steps = 2.9423e-01, PNorm = 118.8289, GNorm = 0.2975
Meta loss on this task batch = 2.6094e-01, Meta loss averaged over last 500 steps = 2.9402e-01, PNorm = 118.8369, GNorm = 0.3175
Meta loss on this task batch = 3.1801e-01, Meta loss averaged over last 500 steps = 2.9399e-01, PNorm = 118.8433, GNorm = 0.5639
Meta loss on this task batch = 2.4321e-01, Meta loss averaged over last 500 steps = 2.9397e-01, PNorm = 118.8484, GNorm = 0.2803
Meta loss on this task batch = 2.7004e-01, Meta loss averaged over last 500 steps = 2.9378e-01, PNorm = 118.8524, GNorm = 0.2728
Meta loss on this task batch = 2.8227e-01, Meta loss averaged over last 500 steps = 2.9378e-01, PNorm = 118.8518, GNorm = 0.3540
Meta loss on this task batch = 2.9164e-01, Meta loss averaged over last 500 steps = 2.9376e-01, PNorm = 118.8492, GNorm = 0.3169
Meta loss on this task batch = 3.5048e-01, Meta loss averaged over last 500 steps = 2.9389e-01, PNorm = 118.8439, GNorm = 0.3160
Meta loss on this task batch = 2.7269e-01, Meta loss averaged over last 500 steps = 2.9392e-01, PNorm = 118.8387, GNorm = 0.3126
Meta loss on this task batch = 3.0923e-01, Meta loss averaged over last 500 steps = 2.9391e-01, PNorm = 118.8316, GNorm = 0.3155
Meta loss on this task batch = 3.3297e-01, Meta loss averaged over last 500 steps = 2.9406e-01, PNorm = 118.8270, GNorm = 0.3786
Took 129.52420473098755 seconds to complete one epoch of meta training
Took 137.85903406143188 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474899
Epoch 467
Meta loss on this task batch = 2.9875e-01, Meta loss averaged over last 500 steps = 2.9394e-01, PNorm = 118.8251, GNorm = 0.2646
Meta loss on this task batch = 3.4531e-01, Meta loss averaged over last 500 steps = 2.9404e-01, PNorm = 118.8235, GNorm = 0.3323
Meta loss on this task batch = 3.2936e-01, Meta loss averaged over last 500 steps = 2.9413e-01, PNorm = 118.8235, GNorm = 0.3229
Meta loss on this task batch = 3.0276e-01, Meta loss averaged over last 500 steps = 2.9407e-01, PNorm = 118.8276, GNorm = 0.2967
Meta loss on this task batch = 3.0915e-01, Meta loss averaged over last 500 steps = 2.9416e-01, PNorm = 118.8354, GNorm = 0.3038
Meta loss on this task batch = 2.4983e-01, Meta loss averaged over last 500 steps = 2.9413e-01, PNorm = 118.8464, GNorm = 0.3639
Meta loss on this task batch = 2.8937e-01, Meta loss averaged over last 500 steps = 2.9409e-01, PNorm = 118.8587, GNorm = 0.2822
Meta loss on this task batch = 2.5230e-01, Meta loss averaged over last 500 steps = 2.9401e-01, PNorm = 118.8707, GNorm = 0.2732
Meta loss on this task batch = 2.7044e-01, Meta loss averaged over last 500 steps = 2.9395e-01, PNorm = 118.8821, GNorm = 0.2801
Meta loss on this task batch = 3.2689e-01, Meta loss averaged over last 500 steps = 2.9407e-01, PNorm = 118.8905, GNorm = 0.3173
Meta loss on this task batch = 3.0396e-01, Meta loss averaged over last 500 steps = 2.9403e-01, PNorm = 118.8957, GNorm = 0.3332
Meta loss on this task batch = 3.1599e-01, Meta loss averaged over last 500 steps = 2.9403e-01, PNorm = 118.9000, GNorm = 0.3030
Meta loss on this task batch = 2.2263e-01, Meta loss averaged over last 500 steps = 2.9395e-01, PNorm = 118.9058, GNorm = 0.2263
Meta loss on this task batch = 2.3893e-01, Meta loss averaged over last 500 steps = 2.9387e-01, PNorm = 118.9114, GNorm = 0.2518
Meta loss on this task batch = 2.9560e-01, Meta loss averaged over last 500 steps = 2.9373e-01, PNorm = 118.9162, GNorm = 0.2865
Meta loss on this task batch = 2.9582e-01, Meta loss averaged over last 500 steps = 2.9373e-01, PNorm = 118.9203, GNorm = 0.2982
Meta loss on this task batch = 2.7079e-01, Meta loss averaged over last 500 steps = 2.9367e-01, PNorm = 118.9231, GNorm = 0.2734
Meta loss on this task batch = 3.1781e-01, Meta loss averaged over last 500 steps = 2.9367e-01, PNorm = 118.9276, GNorm = 0.3104
Meta loss on this task batch = 2.7048e-01, Meta loss averaged over last 500 steps = 2.9367e-01, PNorm = 118.9313, GNorm = 0.3729
Took 309.34011483192444 seconds to complete one epoch of meta training
Took 318.10013604164124 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507045
Epoch 468
Meta loss on this task batch = 2.9111e-01, Meta loss averaged over last 500 steps = 2.9369e-01, PNorm = 118.9323, GNorm = 0.2859
Meta loss on this task batch = 3.1181e-01, Meta loss averaged over last 500 steps = 2.9383e-01, PNorm = 118.9310, GNorm = 0.2975
Meta loss on this task batch = 2.7800e-01, Meta loss averaged over last 500 steps = 2.9385e-01, PNorm = 118.9295, GNorm = 0.3196
Meta loss on this task batch = 2.7692e-01, Meta loss averaged over last 500 steps = 2.9377e-01, PNorm = 118.9280, GNorm = 0.2690
Meta loss on this task batch = 2.5544e-01, Meta loss averaged over last 500 steps = 2.9372e-01, PNorm = 118.9277, GNorm = 0.3272
Meta loss on this task batch = 2.7182e-01, Meta loss averaged over last 500 steps = 2.9364e-01, PNorm = 118.9278, GNorm = 0.3310
Meta loss on this task batch = 2.7803e-01, Meta loss averaged over last 500 steps = 2.9357e-01, PNorm = 118.9289, GNorm = 0.2956
Meta loss on this task batch = 3.1055e-01, Meta loss averaged over last 500 steps = 2.9367e-01, PNorm = 118.9281, GNorm = 0.3966
Meta loss on this task batch = 2.9180e-01, Meta loss averaged over last 500 steps = 2.9356e-01, PNorm = 118.9286, GNorm = 0.2830
Meta loss on this task batch = 3.1999e-01, Meta loss averaged over last 500 steps = 2.9361e-01, PNorm = 118.9298, GNorm = 0.3060
Meta loss on this task batch = 3.1989e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 118.9326, GNorm = 0.3238
Meta loss on this task batch = 2.9536e-01, Meta loss averaged over last 500 steps = 2.9361e-01, PNorm = 118.9357, GNorm = 0.3481
Meta loss on this task batch = 2.6952e-01, Meta loss averaged over last 500 steps = 2.9358e-01, PNorm = 118.9396, GNorm = 0.2956
Meta loss on this task batch = 2.3344e-01, Meta loss averaged over last 500 steps = 2.9344e-01, PNorm = 118.9479, GNorm = 0.3070
Meta loss on this task batch = 2.9585e-01, Meta loss averaged over last 500 steps = 2.9352e-01, PNorm = 118.9548, GNorm = 0.3094
Meta loss on this task batch = 2.8630e-01, Meta loss averaged over last 500 steps = 2.9355e-01, PNorm = 118.9599, GNorm = 0.2982
Meta loss on this task batch = 3.1136e-01, Meta loss averaged over last 500 steps = 2.9358e-01, PNorm = 118.9648, GNorm = 0.3585
Meta loss on this task batch = 2.8845e-01, Meta loss averaged over last 500 steps = 2.9354e-01, PNorm = 118.9695, GNorm = 0.2996
Meta loss on this task batch = 2.8640e-01, Meta loss averaged over last 500 steps = 2.9357e-01, PNorm = 118.9742, GNorm = 0.3598
Took 167.0410041809082 seconds to complete one epoch of meta training
Took 174.41404390335083 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467055
Epoch 469
Meta loss on this task batch = 2.7033e-01, Meta loss averaged over last 500 steps = 2.9352e-01, PNorm = 118.9784, GNorm = 0.2441
Meta loss on this task batch = 2.5491e-01, Meta loss averaged over last 500 steps = 2.9332e-01, PNorm = 118.9826, GNorm = 0.2860
Meta loss on this task batch = 2.6604e-01, Meta loss averaged over last 500 steps = 2.9328e-01, PNorm = 118.9891, GNorm = 0.2484
Meta loss on this task batch = 2.7435e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 118.9969, GNorm = 0.2856
Meta loss on this task batch = 2.7099e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 119.0027, GNorm = 0.2641
Meta loss on this task batch = 2.6779e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 119.0122, GNorm = 0.3025
Meta loss on this task batch = 3.2073e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 119.0207, GNorm = 0.2813
Meta loss on this task batch = 3.3113e-01, Meta loss averaged over last 500 steps = 2.9336e-01, PNorm = 119.0272, GNorm = 0.3071
Meta loss on this task batch = 2.3904e-01, Meta loss averaged over last 500 steps = 2.9308e-01, PNorm = 119.0323, GNorm = 0.2519
Meta loss on this task batch = 3.2359e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 119.0352, GNorm = 0.3402
Meta loss on this task batch = 2.6647e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 119.0369, GNorm = 0.3146
Meta loss on this task batch = 2.7611e-01, Meta loss averaged over last 500 steps = 2.9316e-01, PNorm = 119.0379, GNorm = 0.2835
Meta loss on this task batch = 2.7932e-01, Meta loss averaged over last 500 steps = 2.9327e-01, PNorm = 119.0392, GNorm = 0.2998
Meta loss on this task batch = 2.7569e-01, Meta loss averaged over last 500 steps = 2.9315e-01, PNorm = 119.0408, GNorm = 0.2970
Meta loss on this task batch = 3.3489e-01, Meta loss averaged over last 500 steps = 2.9327e-01, PNorm = 119.0433, GNorm = 0.3145
Meta loss on this task batch = 3.0915e-01, Meta loss averaged over last 500 steps = 2.9335e-01, PNorm = 119.0477, GNorm = 0.3476
Meta loss on this task batch = 3.0040e-01, Meta loss averaged over last 500 steps = 2.9322e-01, PNorm = 119.0514, GNorm = 0.3049
Meta loss on this task batch = 2.8783e-01, Meta loss averaged over last 500 steps = 2.9318e-01, PNorm = 119.0577, GNorm = 0.3319
Meta loss on this task batch = 3.8580e-01, Meta loss averaged over last 500 steps = 2.9339e-01, PNorm = 119.0630, GNorm = 0.4300
Took 131.3259539604187 seconds to complete one epoch of meta training
Took 139.86622619628906 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498461
Epoch 470
Meta loss on this task batch = 2.7763e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 119.0616, GNorm = 0.5642
Meta loss on this task batch = 3.0138e-01, Meta loss averaged over last 500 steps = 2.9327e-01, PNorm = 119.0615, GNorm = 0.3531
Meta loss on this task batch = 3.0449e-01, Meta loss averaged over last 500 steps = 2.9338e-01, PNorm = 119.0635, GNorm = 0.3552
Meta loss on this task batch = 3.2020e-01, Meta loss averaged over last 500 steps = 2.9350e-01, PNorm = 119.0648, GNorm = 0.3513
Meta loss on this task batch = 3.1332e-01, Meta loss averaged over last 500 steps = 2.9350e-01, PNorm = 119.0675, GNorm = 0.3467
Meta loss on this task batch = 2.7876e-01, Meta loss averaged over last 500 steps = 2.9331e-01, PNorm = 119.0696, GNorm = 0.3070
Meta loss on this task batch = 2.5708e-01, Meta loss averaged over last 500 steps = 2.9326e-01, PNorm = 119.0746, GNorm = 0.2556
Meta loss on this task batch = 2.2915e-01, Meta loss averaged over last 500 steps = 2.9315e-01, PNorm = 119.0806, GNorm = 0.2750
Meta loss on this task batch = 3.3190e-01, Meta loss averaged over last 500 steps = 2.9315e-01, PNorm = 119.0874, GNorm = 0.2890
Meta loss on this task batch = 2.9107e-01, Meta loss averaged over last 500 steps = 2.9317e-01, PNorm = 119.0946, GNorm = 0.2814
Meta loss on this task batch = 2.7522e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 119.1038, GNorm = 0.2796
Meta loss on this task batch = 3.2489e-01, Meta loss averaged over last 500 steps = 2.9322e-01, PNorm = 119.1137, GNorm = 0.3051
Meta loss on this task batch = 3.3068e-01, Meta loss averaged over last 500 steps = 2.9331e-01, PNorm = 119.1253, GNorm = 0.3021
Meta loss on this task batch = 3.0328e-01, Meta loss averaged over last 500 steps = 2.9337e-01, PNorm = 119.1365, GNorm = 0.2924
Meta loss on this task batch = 3.0673e-01, Meta loss averaged over last 500 steps = 2.9339e-01, PNorm = 119.1484, GNorm = 0.3544
Meta loss on this task batch = 2.6704e-01, Meta loss averaged over last 500 steps = 2.9338e-01, PNorm = 119.1612, GNorm = 0.3021
Meta loss on this task batch = 2.7912e-01, Meta loss averaged over last 500 steps = 2.9331e-01, PNorm = 119.1709, GNorm = 0.2644
Meta loss on this task batch = 2.9611e-01, Meta loss averaged over last 500 steps = 2.9334e-01, PNorm = 119.1799, GNorm = 0.2860
Meta loss on this task batch = 3.1634e-01, Meta loss averaged over last 500 steps = 2.9348e-01, PNorm = 119.1897, GNorm = 0.3132
Took 130.8040463924408 seconds to complete one epoch of meta training
Took 139.6510112285614 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468593
Epoch 471
Meta loss on this task batch = 2.5024e-01, Meta loss averaged over last 500 steps = 2.9338e-01, PNorm = 119.1971, GNorm = 0.2567
Meta loss on this task batch = 2.6546e-01, Meta loss averaged over last 500 steps = 2.9327e-01, PNorm = 119.2039, GNorm = 0.2683
Meta loss on this task batch = 3.1415e-01, Meta loss averaged over last 500 steps = 2.9341e-01, PNorm = 119.2067, GNorm = 0.3222
Meta loss on this task batch = 2.8577e-01, Meta loss averaged over last 500 steps = 2.9344e-01, PNorm = 119.2070, GNorm = 0.2903
Meta loss on this task batch = 3.0313e-01, Meta loss averaged over last 500 steps = 2.9334e-01, PNorm = 119.2059, GNorm = 0.2942
Meta loss on this task batch = 2.9090e-01, Meta loss averaged over last 500 steps = 2.9329e-01, PNorm = 119.2046, GNorm = 0.2867
Meta loss on this task batch = 2.9840e-01, Meta loss averaged over last 500 steps = 2.9327e-01, PNorm = 119.2023, GNorm = 0.3647
Meta loss on this task batch = 2.9938e-01, Meta loss averaged over last 500 steps = 2.9313e-01, PNorm = 119.2021, GNorm = 0.2857
Meta loss on this task batch = 2.3743e-01, Meta loss averaged over last 500 steps = 2.9292e-01, PNorm = 119.2024, GNorm = 0.2407
Meta loss on this task batch = 2.9750e-01, Meta loss averaged over last 500 steps = 2.9281e-01, PNorm = 119.2042, GNorm = 0.2805
Meta loss on this task batch = 3.1827e-01, Meta loss averaged over last 500 steps = 2.9291e-01, PNorm = 119.2053, GNorm = 0.3964
Meta loss on this task batch = 3.6223e-01, Meta loss averaged over last 500 steps = 2.9305e-01, PNorm = 119.2072, GNorm = 0.3392
Meta loss on this task batch = 3.2677e-01, Meta loss averaged over last 500 steps = 2.9311e-01, PNorm = 119.2114, GNorm = 0.3749
Meta loss on this task batch = 2.3969e-01, Meta loss averaged over last 500 steps = 2.9301e-01, PNorm = 119.2176, GNorm = 0.2716
Meta loss on this task batch = 2.8786e-01, Meta loss averaged over last 500 steps = 2.9302e-01, PNorm = 119.2246, GNorm = 0.2768
Meta loss on this task batch = 3.0441e-01, Meta loss averaged over last 500 steps = 2.9297e-01, PNorm = 119.2327, GNorm = 0.3095
Meta loss on this task batch = 3.0042e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 119.2417, GNorm = 0.3362
Meta loss on this task batch = 2.8162e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 119.2518, GNorm = 0.2744
Meta loss on this task batch = 2.6495e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 119.2657, GNorm = 0.3402
Took 136.51902437210083 seconds to complete one epoch of meta training
Took 145.21531128883362 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503226
Epoch 472
Meta loss on this task batch = 3.0018e-01, Meta loss averaged over last 500 steps = 2.9304e-01, PNorm = 119.2797, GNorm = 0.3001
Meta loss on this task batch = 2.6631e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 119.2930, GNorm = 0.2867
Meta loss on this task batch = 2.1651e-01, Meta loss averaged over last 500 steps = 2.9284e-01, PNorm = 119.3036, GNorm = 0.2625
Meta loss on this task batch = 2.5166e-01, Meta loss averaged over last 500 steps = 2.9285e-01, PNorm = 119.3122, GNorm = 0.2608
Meta loss on this task batch = 2.8056e-01, Meta loss averaged over last 500 steps = 2.9287e-01, PNorm = 119.3192, GNorm = 0.2914
Meta loss on this task batch = 2.4125e-01, Meta loss averaged over last 500 steps = 2.9287e-01, PNorm = 119.3256, GNorm = 0.2442
Meta loss on this task batch = 2.5905e-01, Meta loss averaged over last 500 steps = 2.9273e-01, PNorm = 119.3278, GNorm = 0.2702
Meta loss on this task batch = 3.6321e-01, Meta loss averaged over last 500 steps = 2.9292e-01, PNorm = 119.3276, GNorm = 0.3366
Meta loss on this task batch = 3.3880e-01, Meta loss averaged over last 500 steps = 2.9308e-01, PNorm = 119.3255, GNorm = 0.3480
Meta loss on this task batch = 2.7930e-01, Meta loss averaged over last 500 steps = 2.9305e-01, PNorm = 119.3218, GNorm = 0.2785
Meta loss on this task batch = 2.3481e-01, Meta loss averaged over last 500 steps = 2.9285e-01, PNorm = 119.3163, GNorm = 0.2640
Meta loss on this task batch = 3.1709e-01, Meta loss averaged over last 500 steps = 2.9284e-01, PNorm = 119.3081, GNorm = 0.3503
Meta loss on this task batch = 2.7029e-01, Meta loss averaged over last 500 steps = 2.9271e-01, PNorm = 119.3019, GNorm = 0.2696
Meta loss on this task batch = 2.8447e-01, Meta loss averaged over last 500 steps = 2.9267e-01, PNorm = 119.2991, GNorm = 0.3042
Meta loss on this task batch = 3.5368e-01, Meta loss averaged over last 500 steps = 2.9282e-01, PNorm = 119.2995, GNorm = 0.3215
Meta loss on this task batch = 2.9680e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 119.3045, GNorm = 0.2871
Meta loss on this task batch = 2.5903e-01, Meta loss averaged over last 500 steps = 2.9282e-01, PNorm = 119.3147, GNorm = 0.3053
Meta loss on this task batch = 3.5647e-01, Meta loss averaged over last 500 steps = 2.9297e-01, PNorm = 119.3253, GNorm = 0.3253
Meta loss on this task batch = 3.0472e-01, Meta loss averaged over last 500 steps = 2.9298e-01, PNorm = 119.3369, GNorm = 0.3583
Took 136.62458896636963 seconds to complete one epoch of meta training
Took 145.41146278381348 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474193
Epoch 473
Meta loss on this task batch = 3.1669e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 119.3492, GNorm = 0.3318
Meta loss on this task batch = 2.4988e-01, Meta loss averaged over last 500 steps = 2.9296e-01, PNorm = 119.3643, GNorm = 0.2622
Meta loss on this task batch = 3.0500e-01, Meta loss averaged over last 500 steps = 2.9308e-01, PNorm = 119.3800, GNorm = 0.2969
Meta loss on this task batch = 2.9160e-01, Meta loss averaged over last 500 steps = 2.9308e-01, PNorm = 119.3955, GNorm = 0.2829
Meta loss on this task batch = 2.9745e-01, Meta loss averaged over last 500 steps = 2.9313e-01, PNorm = 119.4068, GNorm = 0.2917
Meta loss on this task batch = 2.5951e-01, Meta loss averaged over last 500 steps = 2.9306e-01, PNorm = 119.4167, GNorm = 0.2610
Meta loss on this task batch = 3.1332e-01, Meta loss averaged over last 500 steps = 2.9311e-01, PNorm = 119.4231, GNorm = 0.3047
Meta loss on this task batch = 3.8329e-01, Meta loss averaged over last 500 steps = 2.9329e-01, PNorm = 119.4267, GNorm = 0.3142
Meta loss on this task batch = 3.1691e-01, Meta loss averaged over last 500 steps = 2.9339e-01, PNorm = 119.4285, GNorm = 0.2972
Meta loss on this task batch = 2.9950e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 119.4319, GNorm = 0.3187
Meta loss on this task batch = 2.8280e-01, Meta loss averaged over last 500 steps = 2.9324e-01, PNorm = 119.4360, GNorm = 0.2835
Meta loss on this task batch = 2.3933e-01, Meta loss averaged over last 500 steps = 2.9322e-01, PNorm = 119.4410, GNorm = 0.2537
Meta loss on this task batch = 2.7532e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 119.4469, GNorm = 0.2977
Meta loss on this task batch = 3.0534e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 119.4521, GNorm = 0.2911
Meta loss on this task batch = 2.9684e-01, Meta loss averaged over last 500 steps = 2.9318e-01, PNorm = 119.4598, GNorm = 0.3155
Meta loss on this task batch = 3.0924e-01, Meta loss averaged over last 500 steps = 2.9313e-01, PNorm = 119.4648, GNorm = 0.3192
Meta loss on this task batch = 2.3816e-01, Meta loss averaged over last 500 steps = 2.9299e-01, PNorm = 119.4708, GNorm = 0.2413
Meta loss on this task batch = 2.5500e-01, Meta loss averaged over last 500 steps = 2.9294e-01, PNorm = 119.4767, GNorm = 0.2655
Meta loss on this task batch = 3.4552e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 119.4826, GNorm = 0.3992
Took 135.19684267044067 seconds to complete one epoch of meta training
Took 144.0116264820099 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494417
Epoch 474
Meta loss on this task batch = 2.6429e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 119.4890, GNorm = 0.2515
Meta loss on this task batch = 2.5845e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 119.4968, GNorm = 0.2950
Meta loss on this task batch = 3.0572e-01, Meta loss averaged over last 500 steps = 2.9317e-01, PNorm = 119.5022, GNorm = 0.3421
Meta loss on this task batch = 3.5103e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 119.5076, GNorm = 0.3642
Meta loss on this task batch = 2.8300e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 119.5115, GNorm = 0.3260
Meta loss on this task batch = 2.9996e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 119.5152, GNorm = 0.3017
Meta loss on this task batch = 2.5779e-01, Meta loss averaged over last 500 steps = 2.9315e-01, PNorm = 119.5169, GNorm = 0.3284
Meta loss on this task batch = 2.8650e-01, Meta loss averaged over last 500 steps = 2.9315e-01, PNorm = 119.5194, GNorm = 0.3196
Meta loss on this task batch = 2.9646e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 119.5220, GNorm = 0.3250
Meta loss on this task batch = 3.4209e-01, Meta loss averaged over last 500 steps = 2.9327e-01, PNorm = 119.5252, GNorm = 0.3186
Meta loss on this task batch = 3.3574e-01, Meta loss averaged over last 500 steps = 2.9339e-01, PNorm = 119.5291, GNorm = 0.3464
Meta loss on this task batch = 2.9577e-01, Meta loss averaged over last 500 steps = 2.9334e-01, PNorm = 119.5354, GNorm = 0.2947
Meta loss on this task batch = 3.0412e-01, Meta loss averaged over last 500 steps = 2.9348e-01, PNorm = 119.5427, GNorm = 0.3199
Meta loss on this task batch = 2.2118e-01, Meta loss averaged over last 500 steps = 2.9329e-01, PNorm = 119.5520, GNorm = 0.2868
Meta loss on this task batch = 2.8809e-01, Meta loss averaged over last 500 steps = 2.9345e-01, PNorm = 119.5613, GNorm = 0.3012
Meta loss on this task batch = 2.9837e-01, Meta loss averaged over last 500 steps = 2.9332e-01, PNorm = 119.5707, GNorm = 0.2853
Meta loss on this task batch = 2.9026e-01, Meta loss averaged over last 500 steps = 2.9335e-01, PNorm = 119.5785, GNorm = 0.2876
Meta loss on this task batch = 3.1782e-01, Meta loss averaged over last 500 steps = 2.9338e-01, PNorm = 119.5844, GNorm = 0.3150
Meta loss on this task batch = 2.3492e-01, Meta loss averaged over last 500 steps = 2.9330e-01, PNorm = 119.5910, GNorm = 0.3247
Took 134.89475393295288 seconds to complete one epoch of meta training
Took 143.5802526473999 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469063
Epoch 475
Meta loss on this task batch = 3.1619e-01, Meta loss averaged over last 500 steps = 2.9326e-01, PNorm = 119.5974, GNorm = 0.2899
Meta loss on this task batch = 3.0630e-01, Meta loss averaged over last 500 steps = 2.9325e-01, PNorm = 119.6029, GNorm = 0.2685
Meta loss on this task batch = 3.0674e-01, Meta loss averaged over last 500 steps = 2.9328e-01, PNorm = 119.6078, GNorm = 0.4000
Meta loss on this task batch = 3.1198e-01, Meta loss averaged over last 500 steps = 2.9320e-01, PNorm = 119.6110, GNorm = 0.2963
Meta loss on this task batch = 2.9679e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 119.6132, GNorm = 0.2558
Meta loss on this task batch = 2.9532e-01, Meta loss averaged over last 500 steps = 2.9325e-01, PNorm = 119.6152, GNorm = 0.2742
Meta loss on this task batch = 2.9210e-01, Meta loss averaged over last 500 steps = 2.9331e-01, PNorm = 119.6181, GNorm = 0.2706
Meta loss on this task batch = 3.0333e-01, Meta loss averaged over last 500 steps = 2.9336e-01, PNorm = 119.6228, GNorm = 0.3007
Meta loss on this task batch = 2.5373e-01, Meta loss averaged over last 500 steps = 2.9334e-01, PNorm = 119.6292, GNorm = 0.2446
Meta loss on this task batch = 2.8759e-01, Meta loss averaged over last 500 steps = 2.9337e-01, PNorm = 119.6370, GNorm = 0.2534
Meta loss on this task batch = 3.4251e-01, Meta loss averaged over last 500 steps = 2.9344e-01, PNorm = 119.6432, GNorm = 0.3453
Meta loss on this task batch = 3.0166e-01, Meta loss averaged over last 500 steps = 2.9345e-01, PNorm = 119.6476, GNorm = 0.3187
Meta loss on this task batch = 2.9873e-01, Meta loss averaged over last 500 steps = 2.9346e-01, PNorm = 119.6521, GNorm = 0.2938
Meta loss on this task batch = 2.7612e-01, Meta loss averaged over last 500 steps = 2.9335e-01, PNorm = 119.6567, GNorm = 0.2630
Meta loss on this task batch = 2.8444e-01, Meta loss averaged over last 500 steps = 2.9330e-01, PNorm = 119.6616, GNorm = 0.2651
Meta loss on this task batch = 3.0874e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 119.6671, GNorm = 0.3262
Meta loss on this task batch = 2.4964e-01, Meta loss averaged over last 500 steps = 2.9316e-01, PNorm = 119.6717, GNorm = 0.2628
Meta loss on this task batch = 2.2940e-01, Meta loss averaged over last 500 steps = 2.9289e-01, PNorm = 119.6754, GNorm = 0.2506
Meta loss on this task batch = 3.0265e-01, Meta loss averaged over last 500 steps = 2.9295e-01, PNorm = 119.6818, GNorm = 0.3508
Took 133.98558449745178 seconds to complete one epoch of meta training
Took 142.56286454200745 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497419
Epoch 476
Meta loss on this task batch = 3.0201e-01, Meta loss averaged over last 500 steps = 2.9294e-01, PNorm = 119.6862, GNorm = 0.3452
Meta loss on this task batch = 3.3839e-01, Meta loss averaged over last 500 steps = 2.9308e-01, PNorm = 119.6910, GNorm = 0.2960
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 119.6945, GNorm = 0.2932
Meta loss on this task batch = 2.8820e-01, Meta loss averaged over last 500 steps = 2.9305e-01, PNorm = 119.6971, GNorm = 0.3018
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 2.9299e-01, PNorm = 119.7000, GNorm = 0.2644
Meta loss on this task batch = 2.7876e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 119.7030, GNorm = 0.3063
Meta loss on this task batch = 2.3753e-01, Meta loss averaged over last 500 steps = 2.9294e-01, PNorm = 119.7094, GNorm = 0.2862
Meta loss on this task batch = 2.0424e-01, Meta loss averaged over last 500 steps = 2.9286e-01, PNorm = 119.7177, GNorm = 0.2511
Meta loss on this task batch = 2.7928e-01, Meta loss averaged over last 500 steps = 2.9282e-01, PNorm = 119.7274, GNorm = 0.2474
Meta loss on this task batch = 3.1421e-01, Meta loss averaged over last 500 steps = 2.9289e-01, PNorm = 119.7364, GNorm = 0.2877
Meta loss on this task batch = 2.9175e-01, Meta loss averaged over last 500 steps = 2.9279e-01, PNorm = 119.7445, GNorm = 0.2743
Meta loss on this task batch = 2.9569e-01, Meta loss averaged over last 500 steps = 2.9272e-01, PNorm = 119.7503, GNorm = 0.3302
Meta loss on this task batch = 2.8489e-01, Meta loss averaged over last 500 steps = 2.9264e-01, PNorm = 119.7569, GNorm = 0.2629
Meta loss on this task batch = 2.9110e-01, Meta loss averaged over last 500 steps = 2.9248e-01, PNorm = 119.7620, GNorm = 0.2947
Meta loss on this task batch = 3.5064e-01, Meta loss averaged over last 500 steps = 2.9264e-01, PNorm = 119.7652, GNorm = 0.3349
Meta loss on this task batch = 3.7772e-01, Meta loss averaged over last 500 steps = 2.9279e-01, PNorm = 119.7673, GNorm = 0.3380
Meta loss on this task batch = 2.1111e-01, Meta loss averaged over last 500 steps = 2.9274e-01, PNorm = 119.7733, GNorm = 0.2719
Meta loss on this task batch = 2.7384e-01, Meta loss averaged over last 500 steps = 2.9275e-01, PNorm = 119.7792, GNorm = 0.2622
Meta loss on this task batch = 2.6076e-01, Meta loss averaged over last 500 steps = 2.9275e-01, PNorm = 119.7854, GNorm = 0.2837
Took 135.52221632003784 seconds to complete one epoch of meta training
Took 144.35984468460083 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501958
Epoch 477
Meta loss on this task batch = 3.0607e-01, Meta loss averaged over last 500 steps = 2.9284e-01, PNorm = 119.7925, GNorm = 0.3534
Meta loss on this task batch = 3.5225e-01, Meta loss averaged over last 500 steps = 2.9299e-01, PNorm = 119.7997, GNorm = 0.3000
Meta loss on this task batch = 2.8342e-01, Meta loss averaged over last 500 steps = 2.9295e-01, PNorm = 119.8079, GNorm = 0.2994
Meta loss on this task batch = 2.6353e-01, Meta loss averaged over last 500 steps = 2.9287e-01, PNorm = 119.8182, GNorm = 0.2966
Meta loss on this task batch = 2.9274e-01, Meta loss averaged over last 500 steps = 2.9285e-01, PNorm = 119.8270, GNorm = 0.2996
Meta loss on this task batch = 2.8092e-01, Meta loss averaged over last 500 steps = 2.9283e-01, PNorm = 119.8357, GNorm = 0.3050
Meta loss on this task batch = 2.9773e-01, Meta loss averaged over last 500 steps = 2.9295e-01, PNorm = 119.8446, GNorm = 0.2924
Meta loss on this task batch = 2.8595e-01, Meta loss averaged over last 500 steps = 2.9284e-01, PNorm = 119.8514, GNorm = 0.3053
Meta loss on this task batch = 3.1374e-01, Meta loss averaged over last 500 steps = 2.9284e-01, PNorm = 119.8584, GNorm = 0.3039
Meta loss on this task batch = 2.5896e-01, Meta loss averaged over last 500 steps = 2.9280e-01, PNorm = 119.8635, GNorm = 0.2735
Meta loss on this task batch = 2.7892e-01, Meta loss averaged over last 500 steps = 2.9277e-01, PNorm = 119.8675, GNorm = 0.2943
Meta loss on this task batch = 3.4454e-01, Meta loss averaged over last 500 steps = 2.9292e-01, PNorm = 119.8738, GNorm = 0.3529
Meta loss on this task batch = 3.2251e-01, Meta loss averaged over last 500 steps = 2.9298e-01, PNorm = 119.8817, GNorm = 0.2895
Meta loss on this task batch = 2.4664e-01, Meta loss averaged over last 500 steps = 2.9288e-01, PNorm = 119.8906, GNorm = 0.3105
Meta loss on this task batch = 2.3988e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 119.9000, GNorm = 0.3030
Meta loss on this task batch = 3.1823e-01, Meta loss averaged over last 500 steps = 2.9280e-01, PNorm = 119.9073, GNorm = 0.3467
Meta loss on this task batch = 2.8850e-01, Meta loss averaged over last 500 steps = 2.9274e-01, PNorm = 119.9147, GNorm = 0.2838
Meta loss on this task batch = 2.5161e-01, Meta loss averaged over last 500 steps = 2.9275e-01, PNorm = 119.9244, GNorm = 0.3254
Meta loss on this task batch = 3.3659e-01, Meta loss averaged over last 500 steps = 2.9288e-01, PNorm = 119.9332, GNorm = 0.3489
Took 134.74198603630066 seconds to complete one epoch of meta training
Took 143.1660029888153 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499539
Epoch 478
Meta loss on this task batch = 2.1461e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 119.9422, GNorm = 0.2500
Meta loss on this task batch = 2.5455e-01, Meta loss averaged over last 500 steps = 2.9269e-01, PNorm = 119.9518, GNorm = 0.2647
Meta loss on this task batch = 2.9250e-01, Meta loss averaged over last 500 steps = 2.9273e-01, PNorm = 119.9577, GNorm = 0.3824
Meta loss on this task batch = 3.0727e-01, Meta loss averaged over last 500 steps = 2.9262e-01, PNorm = 119.9637, GNorm = 0.2999
Meta loss on this task batch = 2.5040e-01, Meta loss averaged over last 500 steps = 2.9250e-01, PNorm = 119.9686, GNorm = 0.2721
Meta loss on this task batch = 3.0490e-01, Meta loss averaged over last 500 steps = 2.9249e-01, PNorm = 119.9725, GNorm = 0.3204
Meta loss on this task batch = 2.7865e-01, Meta loss averaged over last 500 steps = 2.9241e-01, PNorm = 119.9753, GNorm = 0.3179
Meta loss on this task batch = 3.1662e-01, Meta loss averaged over last 500 steps = 2.9247e-01, PNorm = 119.9788, GNorm = 0.3052
Meta loss on this task batch = 2.6051e-01, Meta loss averaged over last 500 steps = 2.9245e-01, PNorm = 119.9803, GNorm = 0.3115
Meta loss on this task batch = 2.5577e-01, Meta loss averaged over last 500 steps = 2.9239e-01, PNorm = 119.9827, GNorm = 0.2896
Meta loss on this task batch = 3.0978e-01, Meta loss averaged over last 500 steps = 2.9248e-01, PNorm = 119.9870, GNorm = 0.2943
Meta loss on this task batch = 3.1210e-01, Meta loss averaged over last 500 steps = 2.9252e-01, PNorm = 119.9884, GNorm = 0.3220
Meta loss on this task batch = 3.5392e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 119.9910, GNorm = 0.3250
Meta loss on this task batch = 3.3631e-01, Meta loss averaged over last 500 steps = 2.9263e-01, PNorm = 119.9955, GNorm = 0.2971
Meta loss on this task batch = 2.7091e-01, Meta loss averaged over last 500 steps = 2.9264e-01, PNorm = 120.0015, GNorm = 0.2777
Meta loss on this task batch = 3.1501e-01, Meta loss averaged over last 500 steps = 2.9268e-01, PNorm = 120.0077, GNorm = 0.2821
Meta loss on this task batch = 2.4778e-01, Meta loss averaged over last 500 steps = 2.9262e-01, PNorm = 120.0161, GNorm = 0.2623
Meta loss on this task batch = 2.8699e-01, Meta loss averaged over last 500 steps = 2.9258e-01, PNorm = 120.0259, GNorm = 0.3232
Meta loss on this task batch = 2.3536e-01, Meta loss averaged over last 500 steps = 2.9243e-01, PNorm = 120.0365, GNorm = 0.3194
Took 137.00637435913086 seconds to complete one epoch of meta training
Took 146.26376104354858 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500739
Epoch 479
Meta loss on this task batch = 2.6897e-01, Meta loss averaged over last 500 steps = 2.9237e-01, PNorm = 120.0464, GNorm = 0.2841
Meta loss on this task batch = 2.4138e-01, Meta loss averaged over last 500 steps = 2.9228e-01, PNorm = 120.0541, GNorm = 0.2472
Meta loss on this task batch = 2.6753e-01, Meta loss averaged over last 500 steps = 2.9235e-01, PNorm = 120.0609, GNorm = 0.3204
Meta loss on this task batch = 2.6079e-01, Meta loss averaged over last 500 steps = 2.9220e-01, PNorm = 120.0649, GNorm = 0.2656
Meta loss on this task batch = 3.4172e-01, Meta loss averaged over last 500 steps = 2.9237e-01, PNorm = 120.0648, GNorm = 0.4092
Meta loss on this task batch = 2.4954e-01, Meta loss averaged over last 500 steps = 2.9223e-01, PNorm = 120.0641, GNorm = 0.2636
Meta loss on this task batch = 2.8992e-01, Meta loss averaged over last 500 steps = 2.9221e-01, PNorm = 120.0626, GNorm = 0.2837
Meta loss on this task batch = 2.9066e-01, Meta loss averaged over last 500 steps = 2.9211e-01, PNorm = 120.0610, GNorm = 0.3014
Meta loss on this task batch = 2.9203e-01, Meta loss averaged over last 500 steps = 2.9208e-01, PNorm = 120.0595, GNorm = 0.3286
Meta loss on this task batch = 2.8527e-01, Meta loss averaged over last 500 steps = 2.9210e-01, PNorm = 120.0604, GNorm = 0.2810
Meta loss on this task batch = 3.0738e-01, Meta loss averaged over last 500 steps = 2.9219e-01, PNorm = 120.0632, GNorm = 0.3127
Meta loss on this task batch = 3.5394e-01, Meta loss averaged over last 500 steps = 2.9226e-01, PNorm = 120.0670, GNorm = 0.3422
Meta loss on this task batch = 2.4951e-01, Meta loss averaged over last 500 steps = 2.9219e-01, PNorm = 120.0757, GNorm = 0.2992
Meta loss on this task batch = 3.2995e-01, Meta loss averaged over last 500 steps = 2.9228e-01, PNorm = 120.0845, GNorm = 0.3210
Meta loss on this task batch = 3.1931e-01, Meta loss averaged over last 500 steps = 2.9231e-01, PNorm = 120.0930, GNorm = 0.3029
Meta loss on this task batch = 3.0969e-01, Meta loss averaged over last 500 steps = 2.9232e-01, PNorm = 120.1032, GNorm = 0.4013
Meta loss on this task batch = 3.3126e-01, Meta loss averaged over last 500 steps = 2.9246e-01, PNorm = 120.1121, GNorm = 0.2939
Meta loss on this task batch = 2.7304e-01, Meta loss averaged over last 500 steps = 2.9239e-01, PNorm = 120.1205, GNorm = 0.2708
Meta loss on this task batch = 2.6681e-01, Meta loss averaged over last 500 steps = 2.9228e-01, PNorm = 120.1260, GNorm = 0.3260
Took 133.96166253089905 seconds to complete one epoch of meta training
Took 142.56535005569458 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478567
Epoch 480
Meta loss on this task batch = 2.7694e-01, Meta loss averaged over last 500 steps = 2.9233e-01, PNorm = 120.1308, GNorm = 0.2527
Meta loss on this task batch = 2.6102e-01, Meta loss averaged over last 500 steps = 2.9236e-01, PNorm = 120.1350, GNorm = 0.2917
Meta loss on this task batch = 2.7721e-01, Meta loss averaged over last 500 steps = 2.9228e-01, PNorm = 120.1392, GNorm = 0.2692
Meta loss on this task batch = 3.2129e-01, Meta loss averaged over last 500 steps = 2.9244e-01, PNorm = 120.1421, GNorm = 0.2765
Meta loss on this task batch = 3.1604e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 120.1455, GNorm = 0.3220
Meta loss on this task batch = 3.1210e-01, Meta loss averaged over last 500 steps = 2.9260e-01, PNorm = 120.1476, GNorm = 0.2586
Meta loss on this task batch = 2.6001e-01, Meta loss averaged over last 500 steps = 2.9261e-01, PNorm = 120.1499, GNorm = 0.2239
Meta loss on this task batch = 2.8894e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 120.1522, GNorm = 0.2694
Meta loss on this task batch = 2.8011e-01, Meta loss averaged over last 500 steps = 2.9246e-01, PNorm = 120.1553, GNorm = 0.2585
Meta loss on this task batch = 3.2864e-01, Meta loss averaged over last 500 steps = 2.9245e-01, PNorm = 120.1577, GNorm = 0.3161
Meta loss on this task batch = 3.4433e-01, Meta loss averaged over last 500 steps = 2.9247e-01, PNorm = 120.1567, GNorm = 0.3460
Meta loss on this task batch = 2.7979e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 120.1587, GNorm = 0.2908
Meta loss on this task batch = 2.2632e-01, Meta loss averaged over last 500 steps = 2.9221e-01, PNorm = 120.1604, GNorm = 0.2787
Meta loss on this task batch = 3.2247e-01, Meta loss averaged over last 500 steps = 2.9236e-01, PNorm = 120.1612, GNorm = 0.3281
Meta loss on this task batch = 2.9769e-01, Meta loss averaged over last 500 steps = 2.9241e-01, PNorm = 120.1617, GNorm = 0.2843
Meta loss on this task batch = 2.8248e-01, Meta loss averaged over last 500 steps = 2.9243e-01, PNorm = 120.1645, GNorm = 0.3280
Meta loss on this task batch = 2.5295e-01, Meta loss averaged over last 500 steps = 2.9229e-01, PNorm = 120.1691, GNorm = 0.2478
Meta loss on this task batch = 2.7079e-01, Meta loss averaged over last 500 steps = 2.9236e-01, PNorm = 120.1745, GNorm = 0.2853
Meta loss on this task batch = 3.5473e-01, Meta loss averaged over last 500 steps = 2.9256e-01, PNorm = 120.1814, GNorm = 0.4590
Took 134.07265949249268 seconds to complete one epoch of meta training
Took 143.33504104614258 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494166
Epoch 481
Meta loss on this task batch = 3.2097e-01, Meta loss averaged over last 500 steps = 2.9262e-01, PNorm = 120.1871, GNorm = 0.3188
Meta loss on this task batch = 2.6751e-01, Meta loss averaged over last 500 steps = 2.9269e-01, PNorm = 120.1935, GNorm = 0.2685
Meta loss on this task batch = 2.8252e-01, Meta loss averaged over last 500 steps = 2.9270e-01, PNorm = 120.2005, GNorm = 0.2551
Meta loss on this task batch = 2.8504e-01, Meta loss averaged over last 500 steps = 2.9273e-01, PNorm = 120.2082, GNorm = 0.2671
Meta loss on this task batch = 2.7745e-01, Meta loss averaged over last 500 steps = 2.9273e-01, PNorm = 120.2155, GNorm = 0.2878
Meta loss on this task batch = 2.9022e-01, Meta loss averaged over last 500 steps = 2.9269e-01, PNorm = 120.2218, GNorm = 0.2762
Meta loss on this task batch = 2.9556e-01, Meta loss averaged over last 500 steps = 2.9267e-01, PNorm = 120.2286, GNorm = 0.3404
Meta loss on this task batch = 2.9521e-01, Meta loss averaged over last 500 steps = 2.9264e-01, PNorm = 120.2325, GNorm = 0.3145
Meta loss on this task batch = 3.0713e-01, Meta loss averaged over last 500 steps = 2.9279e-01, PNorm = 120.2352, GNorm = 0.3386
Meta loss on this task batch = 3.2505e-01, Meta loss averaged over last 500 steps = 2.9282e-01, PNorm = 120.2363, GNorm = 0.3000
Meta loss on this task batch = 2.3025e-01, Meta loss averaged over last 500 steps = 2.9267e-01, PNorm = 120.2357, GNorm = 0.2525
Meta loss on this task batch = 3.2563e-01, Meta loss averaged over last 500 steps = 2.9273e-01, PNorm = 120.2352, GNorm = 0.3067
Meta loss on this task batch = 2.6019e-01, Meta loss averaged over last 500 steps = 2.9265e-01, PNorm = 120.2356, GNorm = 0.3055
Meta loss on this task batch = 2.9407e-01, Meta loss averaged over last 500 steps = 2.9267e-01, PNorm = 120.2355, GNorm = 0.2869
Meta loss on this task batch = 2.4892e-01, Meta loss averaged over last 500 steps = 2.9257e-01, PNorm = 120.2357, GNorm = 0.2569
Meta loss on this task batch = 2.5487e-01, Meta loss averaged over last 500 steps = 2.9252e-01, PNorm = 120.2355, GNorm = 0.2451
Meta loss on this task batch = 2.8131e-01, Meta loss averaged over last 500 steps = 2.9252e-01, PNorm = 120.2370, GNorm = 0.3080
Meta loss on this task batch = 3.3005e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 120.2371, GNorm = 0.3431
Meta loss on this task batch = 2.8835e-01, Meta loss averaged over last 500 steps = 2.9247e-01, PNorm = 120.2381, GNorm = 0.2864
Took 131.42641711235046 seconds to complete one epoch of meta training
Took 139.88382411003113 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469061
Epoch 482
Meta loss on this task batch = 2.7496e-01, Meta loss averaged over last 500 steps = 2.9238e-01, PNorm = 120.2366, GNorm = 0.3051
Meta loss on this task batch = 2.7845e-01, Meta loss averaged over last 500 steps = 2.9234e-01, PNorm = 120.2351, GNorm = 0.2613
Meta loss on this task batch = 3.1175e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 120.2379, GNorm = 0.2923
Meta loss on this task batch = 2.7896e-01, Meta loss averaged over last 500 steps = 2.9235e-01, PNorm = 120.2427, GNorm = 0.2868
Meta loss on this task batch = 3.3935e-01, Meta loss averaged over last 500 steps = 2.9237e-01, PNorm = 120.2469, GNorm = 0.2636
Meta loss on this task batch = 2.8984e-01, Meta loss averaged over last 500 steps = 2.9241e-01, PNorm = 120.2524, GNorm = 0.2653
Meta loss on this task batch = 2.7701e-01, Meta loss averaged over last 500 steps = 2.9242e-01, PNorm = 120.2587, GNorm = 0.2432
Meta loss on this task batch = 2.8870e-01, Meta loss averaged over last 500 steps = 2.9243e-01, PNorm = 120.2653, GNorm = 0.2594
Meta loss on this task batch = 2.7273e-01, Meta loss averaged over last 500 steps = 2.9225e-01, PNorm = 120.2710, GNorm = 0.2688
Meta loss on this task batch = 3.0998e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 120.2745, GNorm = 0.2902
Meta loss on this task batch = 2.7136e-01, Meta loss averaged over last 500 steps = 2.9224e-01, PNorm = 120.2788, GNorm = 0.3009
Meta loss on this task batch = 2.8795e-01, Meta loss averaged over last 500 steps = 2.9220e-01, PNorm = 120.2832, GNorm = 0.3167
Meta loss on this task batch = 3.2244e-01, Meta loss averaged over last 500 steps = 2.9236e-01, PNorm = 120.2859, GNorm = 0.3058
Meta loss on this task batch = 2.7327e-01, Meta loss averaged over last 500 steps = 2.9223e-01, PNorm = 120.2910, GNorm = 0.2746
Meta loss on this task batch = 2.8786e-01, Meta loss averaged over last 500 steps = 2.9227e-01, PNorm = 120.2960, GNorm = 0.2897
Meta loss on this task batch = 3.3223e-01, Meta loss averaged over last 500 steps = 2.9237e-01, PNorm = 120.3019, GNorm = 0.3451
Meta loss on this task batch = 2.1685e-01, Meta loss averaged over last 500 steps = 2.9219e-01, PNorm = 120.3078, GNorm = 0.2743
Meta loss on this task batch = 2.5061e-01, Meta loss averaged over last 500 steps = 2.9215e-01, PNorm = 120.3143, GNorm = 0.3112
Meta loss on this task batch = 2.7006e-01, Meta loss averaged over last 500 steps = 2.9207e-01, PNorm = 120.3212, GNorm = 0.3356
Took 133.06417846679688 seconds to complete one epoch of meta training
Took 140.75671792030334 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486804
Epoch 483
Meta loss on this task batch = 3.1712e-01, Meta loss averaged over last 500 steps = 2.9215e-01, PNorm = 120.3288, GNorm = 0.3083
Meta loss on this task batch = 2.6150e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 120.3377, GNorm = 0.2726
Meta loss on this task batch = 2.8358e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 120.3486, GNorm = 0.3180
Meta loss on this task batch = 2.7918e-01, Meta loss averaged over last 500 steps = 2.9196e-01, PNorm = 120.3564, GNorm = 0.3251
Meta loss on this task batch = 2.9500e-01, Meta loss averaged over last 500 steps = 2.9206e-01, PNorm = 120.3644, GNorm = 0.2715
Meta loss on this task batch = 3.2649e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 120.3713, GNorm = 0.3680
Meta loss on this task batch = 3.0591e-01, Meta loss averaged over last 500 steps = 2.9208e-01, PNorm = 120.3766, GNorm = 0.3256
Meta loss on this task batch = 2.8608e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 120.3825, GNorm = 0.3297
Meta loss on this task batch = 3.5523e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 120.3870, GNorm = 0.3633
Meta loss on this task batch = 2.9343e-01, Meta loss averaged over last 500 steps = 2.9216e-01, PNorm = 120.3920, GNorm = 0.3392
Meta loss on this task batch = 2.2412e-01, Meta loss averaged over last 500 steps = 2.9202e-01, PNorm = 120.3965, GNorm = 0.2666
Meta loss on this task batch = 2.6340e-01, Meta loss averaged over last 500 steps = 2.9192e-01, PNorm = 120.3989, GNorm = 0.3086
Meta loss on this task batch = 2.7638e-01, Meta loss averaged over last 500 steps = 2.9188e-01, PNorm = 120.4019, GNorm = 0.2593
Meta loss on this task batch = 3.3245e-01, Meta loss averaged over last 500 steps = 2.9197e-01, PNorm = 120.4065, GNorm = 0.3155
Meta loss on this task batch = 2.4839e-01, Meta loss averaged over last 500 steps = 2.9185e-01, PNorm = 120.4143, GNorm = 0.2818
Meta loss on this task batch = 2.9513e-01, Meta loss averaged over last 500 steps = 2.9191e-01, PNorm = 120.4189, GNorm = 0.4132
Meta loss on this task batch = 2.7480e-01, Meta loss averaged over last 500 steps = 2.9194e-01, PNorm = 120.4259, GNorm = 0.3083
Meta loss on this task batch = 3.1103e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 120.4341, GNorm = 0.2939
Meta loss on this task batch = 2.3203e-01, Meta loss averaged over last 500 steps = 2.9193e-01, PNorm = 120.4463, GNorm = 0.3057
Took 131.87308549880981 seconds to complete one epoch of meta training
Took 140.67298769950867 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484444
Epoch 484
Meta loss on this task batch = 3.1452e-01, Meta loss averaged over last 500 steps = 2.9196e-01, PNorm = 120.4580, GNorm = 0.2626
Meta loss on this task batch = 2.6557e-01, Meta loss averaged over last 500 steps = 2.9183e-01, PNorm = 120.4688, GNorm = 0.2734
Meta loss on this task batch = 2.6788e-01, Meta loss averaged over last 500 steps = 2.9182e-01, PNorm = 120.4769, GNorm = 0.3150
Meta loss on this task batch = 3.1788e-01, Meta loss averaged over last 500 steps = 2.9192e-01, PNorm = 120.4852, GNorm = 0.2722
Meta loss on this task batch = 3.1105e-01, Meta loss averaged over last 500 steps = 2.9185e-01, PNorm = 120.4918, GNorm = 0.3036
Meta loss on this task batch = 2.4730e-01, Meta loss averaged over last 500 steps = 2.9167e-01, PNorm = 120.4978, GNorm = 0.2692
Meta loss on this task batch = 3.2749e-01, Meta loss averaged over last 500 steps = 2.9175e-01, PNorm = 120.5019, GNorm = 0.3045
Meta loss on this task batch = 3.0145e-01, Meta loss averaged over last 500 steps = 2.9175e-01, PNorm = 120.5038, GNorm = 0.2817
Meta loss on this task batch = 2.8380e-01, Meta loss averaged over last 500 steps = 2.9175e-01, PNorm = 120.5054, GNorm = 0.3063
Meta loss on this task batch = 2.6546e-01, Meta loss averaged over last 500 steps = 2.9179e-01, PNorm = 120.5070, GNorm = 0.2759
Meta loss on this task batch = 3.3122e-01, Meta loss averaged over last 500 steps = 2.9188e-01, PNorm = 120.5078, GNorm = 0.3137
Meta loss on this task batch = 3.0939e-01, Meta loss averaged over last 500 steps = 2.9181e-01, PNorm = 120.5092, GNorm = 0.3314
Meta loss on this task batch = 2.8819e-01, Meta loss averaged over last 500 steps = 2.9177e-01, PNorm = 120.5115, GNorm = 0.2759
Meta loss on this task batch = 2.8381e-01, Meta loss averaged over last 500 steps = 2.9172e-01, PNorm = 120.5155, GNorm = 0.3591
Meta loss on this task batch = 2.8700e-01, Meta loss averaged over last 500 steps = 2.9174e-01, PNorm = 120.5214, GNorm = 0.3794
Meta loss on this task batch = 2.6240e-01, Meta loss averaged over last 500 steps = 2.9177e-01, PNorm = 120.5250, GNorm = 0.2665
Meta loss on this task batch = 2.8869e-01, Meta loss averaged over last 500 steps = 2.9167e-01, PNorm = 120.5284, GNorm = 0.2810
Meta loss on this task batch = 3.2016e-01, Meta loss averaged over last 500 steps = 2.9179e-01, PNorm = 120.5311, GNorm = 0.3093
Meta loss on this task batch = 2.2181e-01, Meta loss averaged over last 500 steps = 2.9163e-01, PNorm = 120.5355, GNorm = 0.2663
Took 134.9653444290161 seconds to complete one epoch of meta training
Took 143.53741693496704 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478833
Epoch 485
Meta loss on this task batch = 3.6287e-01, Meta loss averaged over last 500 steps = 2.9184e-01, PNorm = 120.5391, GNorm = 0.3195
Meta loss on this task batch = 2.5280e-01, Meta loss averaged over last 500 steps = 2.9176e-01, PNorm = 120.5433, GNorm = 0.2492
Meta loss on this task batch = 3.3681e-01, Meta loss averaged over last 500 steps = 2.9187e-01, PNorm = 120.5463, GNorm = 0.3448
Meta loss on this task batch = 3.4236e-01, Meta loss averaged over last 500 steps = 2.9187e-01, PNorm = 120.5488, GNorm = 0.3091
Meta loss on this task batch = 2.6674e-01, Meta loss averaged over last 500 steps = 2.9179e-01, PNorm = 120.5515, GNorm = 0.2604
Meta loss on this task batch = 2.4133e-01, Meta loss averaged over last 500 steps = 2.9165e-01, PNorm = 120.5547, GNorm = 0.2275
Meta loss on this task batch = 2.5569e-01, Meta loss averaged over last 500 steps = 2.9159e-01, PNorm = 120.5585, GNorm = 0.2925
Meta loss on this task batch = 2.7891e-01, Meta loss averaged over last 500 steps = 2.9160e-01, PNorm = 120.5593, GNorm = 0.2719
Meta loss on this task batch = 2.9688e-01, Meta loss averaged over last 500 steps = 2.9157e-01, PNorm = 120.5614, GNorm = 0.2789
Meta loss on this task batch = 3.0315e-01, Meta loss averaged over last 500 steps = 2.9151e-01, PNorm = 120.5638, GNorm = 0.3033
Meta loss on this task batch = 3.6313e-01, Meta loss averaged over last 500 steps = 2.9161e-01, PNorm = 120.5664, GNorm = 0.3475
Meta loss on this task batch = 2.7102e-01, Meta loss averaged over last 500 steps = 2.9146e-01, PNorm = 120.5699, GNorm = 0.2744
Meta loss on this task batch = 2.8407e-01, Meta loss averaged over last 500 steps = 2.9143e-01, PNorm = 120.5737, GNorm = 0.3014
Meta loss on this task batch = 2.6501e-01, Meta loss averaged over last 500 steps = 2.9137e-01, PNorm = 120.5795, GNorm = 0.2911
Meta loss on this task batch = 2.9713e-01, Meta loss averaged over last 500 steps = 2.9136e-01, PNorm = 120.5844, GNorm = 0.2837
Meta loss on this task batch = 2.5260e-01, Meta loss averaged over last 500 steps = 2.9118e-01, PNorm = 120.5904, GNorm = 0.3009
Meta loss on this task batch = 2.8752e-01, Meta loss averaged over last 500 steps = 2.9116e-01, PNorm = 120.5950, GNorm = 0.2902
Meta loss on this task batch = 2.6659e-01, Meta loss averaged over last 500 steps = 2.9119e-01, PNorm = 120.5994, GNorm = 0.2869
Meta loss on this task batch = 3.0197e-01, Meta loss averaged over last 500 steps = 2.9124e-01, PNorm = 120.6023, GNorm = 0.4658
Took 135.7378430366516 seconds to complete one epoch of meta training
Took 145.3374047279358 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493008
Epoch 486
Meta loss on this task batch = 3.1942e-01, Meta loss averaged over last 500 steps = 2.9131e-01, PNorm = 120.6047, GNorm = 0.3044
Meta loss on this task batch = 3.7076e-01, Meta loss averaged over last 500 steps = 2.9140e-01, PNorm = 120.6062, GNorm = 0.3551
Meta loss on this task batch = 2.5985e-01, Meta loss averaged over last 500 steps = 2.9139e-01, PNorm = 120.6087, GNorm = 0.2607
Meta loss on this task batch = 2.8836e-01, Meta loss averaged over last 500 steps = 2.9146e-01, PNorm = 120.6131, GNorm = 0.2643
Meta loss on this task batch = 3.3053e-01, Meta loss averaged over last 500 steps = 2.9147e-01, PNorm = 120.6175, GNorm = 0.3202
Meta loss on this task batch = 3.0123e-01, Meta loss averaged over last 500 steps = 2.9140e-01, PNorm = 120.6231, GNorm = 0.2922
Meta loss on this task batch = 2.8037e-01, Meta loss averaged over last 500 steps = 2.9144e-01, PNorm = 120.6275, GNorm = 0.2875
Meta loss on this task batch = 2.7128e-01, Meta loss averaged over last 500 steps = 2.9136e-01, PNorm = 120.6329, GNorm = 0.3051
Meta loss on this task batch = 2.9206e-01, Meta loss averaged over last 500 steps = 2.9139e-01, PNorm = 120.6376, GNorm = 0.3010
Meta loss on this task batch = 2.2852e-01, Meta loss averaged over last 500 steps = 2.9132e-01, PNorm = 120.6427, GNorm = 0.2724
Meta loss on this task batch = 2.5928e-01, Meta loss averaged over last 500 steps = 2.9124e-01, PNorm = 120.6469, GNorm = 0.3064
Meta loss on this task batch = 3.3414e-01, Meta loss averaged over last 500 steps = 2.9129e-01, PNorm = 120.6497, GNorm = 0.3021
Meta loss on this task batch = 3.0792e-01, Meta loss averaged over last 500 steps = 2.9130e-01, PNorm = 120.6497, GNorm = 0.3018
Meta loss on this task batch = 3.0159e-01, Meta loss averaged over last 500 steps = 2.9137e-01, PNorm = 120.6471, GNorm = 0.3385
Meta loss on this task batch = 2.6210e-01, Meta loss averaged over last 500 steps = 2.9132e-01, PNorm = 120.6456, GNorm = 0.2675
Meta loss on this task batch = 3.2915e-01, Meta loss averaged over last 500 steps = 2.9136e-01, PNorm = 120.6421, GNorm = 0.3409
Meta loss on this task batch = 2.5566e-01, Meta loss averaged over last 500 steps = 2.9123e-01, PNorm = 120.6409, GNorm = 0.2725
Meta loss on this task batch = 2.9607e-01, Meta loss averaged over last 500 steps = 2.9127e-01, PNorm = 120.6412, GNorm = 0.3102
Meta loss on this task batch = 2.6803e-01, Meta loss averaged over last 500 steps = 2.9126e-01, PNorm = 120.6405, GNorm = 0.3361
Took 136.01077961921692 seconds to complete one epoch of meta training
Took 144.61357593536377 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487329
Epoch 487
Meta loss on this task batch = 2.9849e-01, Meta loss averaged over last 500 steps = 2.9135e-01, PNorm = 120.6413, GNorm = 0.2774
Meta loss on this task batch = 2.8875e-01, Meta loss averaged over last 500 steps = 2.9133e-01, PNorm = 120.6440, GNorm = 0.2620
Meta loss on this task batch = 2.5479e-01, Meta loss averaged over last 500 steps = 2.9131e-01, PNorm = 120.6477, GNorm = 0.2406
Meta loss on this task batch = 3.2397e-01, Meta loss averaged over last 500 steps = 2.9131e-01, PNorm = 120.6518, GNorm = 0.3307
Meta loss on this task batch = 2.4003e-01, Meta loss averaged over last 500 steps = 2.9107e-01, PNorm = 120.6577, GNorm = 0.2743
Meta loss on this task batch = 2.6693e-01, Meta loss averaged over last 500 steps = 2.9100e-01, PNorm = 120.6638, GNorm = 0.2951
Meta loss on this task batch = 3.1506e-01, Meta loss averaged over last 500 steps = 2.9094e-01, PNorm = 120.6711, GNorm = 0.3282
Meta loss on this task batch = 2.6464e-01, Meta loss averaged over last 500 steps = 2.9095e-01, PNorm = 120.6779, GNorm = 0.3063
Meta loss on this task batch = 3.0479e-01, Meta loss averaged over last 500 steps = 2.9097e-01, PNorm = 120.6805, GNorm = 0.3234
Meta loss on this task batch = 2.6449e-01, Meta loss averaged over last 500 steps = 2.9084e-01, PNorm = 120.6796, GNorm = 0.3105
Meta loss on this task batch = 2.9378e-01, Meta loss averaged over last 500 steps = 2.9086e-01, PNorm = 120.6798, GNorm = 0.3050
Meta loss on this task batch = 2.9626e-01, Meta loss averaged over last 500 steps = 2.9094e-01, PNorm = 120.6791, GNorm = 0.3408
Meta loss on this task batch = 3.0693e-01, Meta loss averaged over last 500 steps = 2.9102e-01, PNorm = 120.6767, GNorm = 0.2827
Meta loss on this task batch = 2.8627e-01, Meta loss averaged over last 500 steps = 2.9090e-01, PNorm = 120.6742, GNorm = 0.2963
Meta loss on this task batch = 2.9832e-01, Meta loss averaged over last 500 steps = 2.9090e-01, PNorm = 120.6739, GNorm = 0.2679
Meta loss on this task batch = 3.0885e-01, Meta loss averaged over last 500 steps = 2.9097e-01, PNorm = 120.6758, GNorm = 0.3043
Meta loss on this task batch = 3.0955e-01, Meta loss averaged over last 500 steps = 2.9094e-01, PNorm = 120.6792, GNorm = 0.3471
Meta loss on this task batch = 2.8439e-01, Meta loss averaged over last 500 steps = 2.9090e-01, PNorm = 120.6831, GNorm = 0.2931
Meta loss on this task batch = 3.0887e-01, Meta loss averaged over last 500 steps = 2.9103e-01, PNorm = 120.6918, GNorm = 0.3590
Took 136.32037997245789 seconds to complete one epoch of meta training
Took 144.87223386764526 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466035
Epoch 488
Meta loss on this task batch = 2.6314e-01, Meta loss averaged over last 500 steps = 2.9087e-01, PNorm = 120.6989, GNorm = 0.2781
Meta loss on this task batch = 3.0297e-01, Meta loss averaged over last 500 steps = 2.9099e-01, PNorm = 120.7057, GNorm = 0.3384
Meta loss on this task batch = 2.4700e-01, Meta loss averaged over last 500 steps = 2.9082e-01, PNorm = 120.7137, GNorm = 0.2801
Meta loss on this task batch = 2.9164e-01, Meta loss averaged over last 500 steps = 2.9074e-01, PNorm = 120.7216, GNorm = 0.2963
Meta loss on this task batch = 2.5637e-01, Meta loss averaged over last 500 steps = 2.9072e-01, PNorm = 120.7302, GNorm = 0.2529
Meta loss on this task batch = 3.2325e-01, Meta loss averaged over last 500 steps = 2.9072e-01, PNorm = 120.7380, GNorm = 0.2955
Meta loss on this task batch = 3.1550e-01, Meta loss averaged over last 500 steps = 2.9068e-01, PNorm = 120.7449, GNorm = 0.2748
Meta loss on this task batch = 2.4165e-01, Meta loss averaged over last 500 steps = 2.9059e-01, PNorm = 120.7523, GNorm = 0.2431
Meta loss on this task batch = 3.2421e-01, Meta loss averaged over last 500 steps = 2.9066e-01, PNorm = 120.7593, GNorm = 0.3516
Meta loss on this task batch = 2.6817e-01, Meta loss averaged over last 500 steps = 2.9054e-01, PNorm = 120.7652, GNorm = 0.2854
Meta loss on this task batch = 2.8845e-01, Meta loss averaged over last 500 steps = 2.9051e-01, PNorm = 120.7697, GNorm = 0.3126
Meta loss on this task batch = 3.0502e-01, Meta loss averaged over last 500 steps = 2.9052e-01, PNorm = 120.7734, GNorm = 0.3023
Meta loss on this task batch = 2.6178e-01, Meta loss averaged over last 500 steps = 2.9047e-01, PNorm = 120.7769, GNorm = 0.2689
Meta loss on this task batch = 2.8737e-01, Meta loss averaged over last 500 steps = 2.9048e-01, PNorm = 120.7803, GNorm = 0.2375
Meta loss on this task batch = 3.7397e-01, Meta loss averaged over last 500 steps = 2.9073e-01, PNorm = 120.7832, GNorm = 0.3020
Meta loss on this task batch = 2.9137e-01, Meta loss averaged over last 500 steps = 2.9071e-01, PNorm = 120.7875, GNorm = 0.2714
Meta loss on this task batch = 3.0263e-01, Meta loss averaged over last 500 steps = 2.9075e-01, PNorm = 120.7903, GNorm = 0.3239
Meta loss on this task batch = 3.1149e-01, Meta loss averaged over last 500 steps = 2.9076e-01, PNorm = 120.7945, GNorm = 0.2616
Meta loss on this task batch = 3.3794e-01, Meta loss averaged over last 500 steps = 2.9084e-01, PNorm = 120.7984, GNorm = 0.3508
Took 134.06394219398499 seconds to complete one epoch of meta training
Took 142.46454286575317 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469006
Epoch 489
Meta loss on this task batch = 2.6900e-01, Meta loss averaged over last 500 steps = 2.9090e-01, PNorm = 120.8045, GNorm = 0.2457
Meta loss on this task batch = 2.8043e-01, Meta loss averaged over last 500 steps = 2.9074e-01, PNorm = 120.8135, GNorm = 0.3197
Meta loss on this task batch = 3.2210e-01, Meta loss averaged over last 500 steps = 2.9089e-01, PNorm = 120.8214, GNorm = 0.2772
Meta loss on this task batch = 3.0259e-01, Meta loss averaged over last 500 steps = 2.9085e-01, PNorm = 120.8288, GNorm = 0.3383
Meta loss on this task batch = 2.7517e-01, Meta loss averaged over last 500 steps = 2.9080e-01, PNorm = 120.8341, GNorm = 0.3009
Meta loss on this task batch = 2.8589e-01, Meta loss averaged over last 500 steps = 2.9070e-01, PNorm = 120.8377, GNorm = 0.3085
Meta loss on this task batch = 2.8704e-01, Meta loss averaged over last 500 steps = 2.9069e-01, PNorm = 120.8418, GNorm = 0.3092
Meta loss on this task batch = 2.1904e-01, Meta loss averaged over last 500 steps = 2.9053e-01, PNorm = 120.8451, GNorm = 0.2673
Meta loss on this task batch = 2.7269e-01, Meta loss averaged over last 500 steps = 2.9045e-01, PNorm = 120.8502, GNorm = 0.3412
Meta loss on this task batch = 2.6304e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 120.8546, GNorm = 0.2610
Meta loss on this task batch = 3.0258e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 120.8589, GNorm = 0.3382
Meta loss on this task batch = 2.9528e-01, Meta loss averaged over last 500 steps = 2.9022e-01, PNorm = 120.8609, GNorm = 0.3351
Meta loss on this task batch = 2.2035e-01, Meta loss averaged over last 500 steps = 2.9005e-01, PNorm = 120.8658, GNorm = 0.2667
Meta loss on this task batch = 3.0099e-01, Meta loss averaged over last 500 steps = 2.8997e-01, PNorm = 120.8715, GNorm = 0.3104
Meta loss on this task batch = 3.1877e-01, Meta loss averaged over last 500 steps = 2.9001e-01, PNorm = 120.8781, GNorm = 0.2942
Meta loss on this task batch = 3.1012e-01, Meta loss averaged over last 500 steps = 2.9001e-01, PNorm = 120.8861, GNorm = 0.3290
Meta loss on this task batch = 3.1902e-01, Meta loss averaged over last 500 steps = 2.9008e-01, PNorm = 120.8952, GNorm = 0.3650
Meta loss on this task batch = 3.0617e-01, Meta loss averaged over last 500 steps = 2.9020e-01, PNorm = 120.9033, GNorm = 0.3026
Meta loss on this task batch = 2.3782e-01, Meta loss averaged over last 500 steps = 2.9011e-01, PNorm = 120.9112, GNorm = 0.3575
Took 132.394948720932 seconds to complete one epoch of meta training
Took 141.303227186203 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486946
Epoch 490
Meta loss on this task batch = 2.7807e-01, Meta loss averaged over last 500 steps = 2.9016e-01, PNorm = 120.9190, GNorm = 0.3005
Meta loss on this task batch = 3.1945e-01, Meta loss averaged over last 500 steps = 2.9024e-01, PNorm = 120.9279, GNorm = 0.3404
Meta loss on this task batch = 2.9721e-01, Meta loss averaged over last 500 steps = 2.9031e-01, PNorm = 120.9364, GNorm = 0.2918
Meta loss on this task batch = 2.8940e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 120.9452, GNorm = 0.3115
Meta loss on this task batch = 2.4380e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 120.9533, GNorm = 0.2730
Meta loss on this task batch = 2.7374e-01, Meta loss averaged over last 500 steps = 2.9031e-01, PNorm = 120.9606, GNorm = 0.2745
Meta loss on this task batch = 2.3730e-01, Meta loss averaged over last 500 steps = 2.9019e-01, PNorm = 120.9665, GNorm = 0.2532
Meta loss on this task batch = 3.1440e-01, Meta loss averaged over last 500 steps = 2.9021e-01, PNorm = 120.9709, GNorm = 0.2934
Meta loss on this task batch = 2.4436e-01, Meta loss averaged over last 500 steps = 2.9010e-01, PNorm = 120.9729, GNorm = 0.2821
Meta loss on this task batch = 2.8176e-01, Meta loss averaged over last 500 steps = 2.9006e-01, PNorm = 120.9704, GNorm = 0.3226
Meta loss on this task batch = 3.3121e-01, Meta loss averaged over last 500 steps = 2.9018e-01, PNorm = 120.9664, GNorm = 0.3340
Meta loss on this task batch = 3.2579e-01, Meta loss averaged over last 500 steps = 2.9031e-01, PNorm = 120.9628, GNorm = 0.3301
Meta loss on this task batch = 3.2119e-01, Meta loss averaged over last 500 steps = 2.9033e-01, PNorm = 120.9575, GNorm = 0.3357
Meta loss on this task batch = 3.2197e-01, Meta loss averaged over last 500 steps = 2.9035e-01, PNorm = 120.9534, GNorm = 0.2823
Meta loss on this task batch = 3.1944e-01, Meta loss averaged over last 500 steps = 2.9021e-01, PNorm = 120.9514, GNorm = 0.2897
Meta loss on this task batch = 3.0129e-01, Meta loss averaged over last 500 steps = 2.9020e-01, PNorm = 120.9505, GNorm = 0.2635
Meta loss on this task batch = 2.6098e-01, Meta loss averaged over last 500 steps = 2.9010e-01, PNorm = 120.9514, GNorm = 0.2400
Meta loss on this task batch = 3.0258e-01, Meta loss averaged over last 500 steps = 2.9017e-01, PNorm = 120.9533, GNorm = 0.2670
Meta loss on this task batch = 2.8198e-01, Meta loss averaged over last 500 steps = 2.9024e-01, PNorm = 120.9571, GNorm = 0.3192
Took 134.16972637176514 seconds to complete one epoch of meta training
Took 142.55609107017517 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471902
Epoch 491
Meta loss on this task batch = 2.5784e-01, Meta loss averaged over last 500 steps = 2.9010e-01, PNorm = 120.9626, GNorm = 0.2885
Meta loss on this task batch = 2.7812e-01, Meta loss averaged over last 500 steps = 2.9001e-01, PNorm = 120.9674, GNorm = 0.2881
Meta loss on this task batch = 2.9867e-01, Meta loss averaged over last 500 steps = 2.9012e-01, PNorm = 120.9753, GNorm = 0.2835
Meta loss on this task batch = 3.2183e-01, Meta loss averaged over last 500 steps = 2.9015e-01, PNorm = 120.9840, GNorm = 0.3477
Meta loss on this task batch = 2.8233e-01, Meta loss averaged over last 500 steps = 2.9030e-01, PNorm = 120.9938, GNorm = 0.3008
Meta loss on this task batch = 2.8346e-01, Meta loss averaged over last 500 steps = 2.9030e-01, PNorm = 120.9989, GNorm = 0.3481
Meta loss on this task batch = 3.2509e-01, Meta loss averaged over last 500 steps = 2.9041e-01, PNorm = 121.0031, GNorm = 0.2998
Meta loss on this task batch = 2.8027e-01, Meta loss averaged over last 500 steps = 2.9034e-01, PNorm = 121.0055, GNorm = 0.3055
Meta loss on this task batch = 2.8424e-01, Meta loss averaged over last 500 steps = 2.9033e-01, PNorm = 121.0066, GNorm = 0.2869
Meta loss on this task batch = 2.8389e-01, Meta loss averaged over last 500 steps = 2.9023e-01, PNorm = 121.0082, GNorm = 0.3030
Meta loss on this task batch = 2.9551e-01, Meta loss averaged over last 500 steps = 2.9015e-01, PNorm = 121.0091, GNorm = 0.3150
Meta loss on this task batch = 2.6262e-01, Meta loss averaged over last 500 steps = 2.9017e-01, PNorm = 121.0105, GNorm = 0.2763
Meta loss on this task batch = 2.6174e-01, Meta loss averaged over last 500 steps = 2.9014e-01, PNorm = 121.0122, GNorm = 0.2676
Meta loss on this task batch = 2.7468e-01, Meta loss averaged over last 500 steps = 2.9012e-01, PNorm = 121.0157, GNorm = 0.3177
Meta loss on this task batch = 2.8178e-01, Meta loss averaged over last 500 steps = 2.9007e-01, PNorm = 121.0193, GNorm = 0.2886
Meta loss on this task batch = 3.2507e-01, Meta loss averaged over last 500 steps = 2.9016e-01, PNorm = 121.0208, GNorm = 0.3217
Meta loss on this task batch = 2.4280e-01, Meta loss averaged over last 500 steps = 2.9016e-01, PNorm = 121.0257, GNorm = 0.2661
Meta loss on this task batch = 2.9247e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 121.0287, GNorm = 0.2735
Meta loss on this task batch = 3.0550e-01, Meta loss averaged over last 500 steps = 2.9031e-01, PNorm = 121.0298, GNorm = 0.3398
Took 134.64890217781067 seconds to complete one epoch of meta training
Took 143.2815384864807 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472010
Epoch 492
Meta loss on this task batch = 3.3283e-01, Meta loss averaged over last 500 steps = 2.9041e-01, PNorm = 121.0307, GNorm = 0.3399
Meta loss on this task batch = 2.9192e-01, Meta loss averaged over last 500 steps = 2.9035e-01, PNorm = 121.0305, GNorm = 0.3007
Meta loss on this task batch = 3.1472e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 121.0335, GNorm = 0.2907
Meta loss on this task batch = 3.5375e-01, Meta loss averaged over last 500 steps = 2.9048e-01, PNorm = 121.0370, GNorm = 0.2983
Meta loss on this task batch = 2.9124e-01, Meta loss averaged over last 500 steps = 2.9055e-01, PNorm = 121.0422, GNorm = 0.2826
Meta loss on this task batch = 2.5789e-01, Meta loss averaged over last 500 steps = 2.9039e-01, PNorm = 121.0499, GNorm = 0.2712
Meta loss on this task batch = 2.4673e-01, Meta loss averaged over last 500 steps = 2.9020e-01, PNorm = 121.0588, GNorm = 0.2902
Meta loss on this task batch = 3.4644e-01, Meta loss averaged over last 500 steps = 2.9025e-01, PNorm = 121.0683, GNorm = 0.3268
Meta loss on this task batch = 2.7886e-01, Meta loss averaged over last 500 steps = 2.9018e-01, PNorm = 121.0763, GNorm = 0.2561
Meta loss on this task batch = 3.5950e-01, Meta loss averaged over last 500 steps = 2.9029e-01, PNorm = 121.0801, GNorm = 0.3445
Meta loss on this task batch = 2.5521e-01, Meta loss averaged over last 500 steps = 2.9024e-01, PNorm = 121.0837, GNorm = 0.2581
Meta loss on this task batch = 2.3274e-01, Meta loss averaged over last 500 steps = 2.9013e-01, PNorm = 121.0876, GNorm = 0.3047
Meta loss on this task batch = 2.9667e-01, Meta loss averaged over last 500 steps = 2.9017e-01, PNorm = 121.0912, GNorm = 0.3041
Meta loss on this task batch = 2.7769e-01, Meta loss averaged over last 500 steps = 2.9012e-01, PNorm = 121.0971, GNorm = 0.3026
Meta loss on this task batch = 3.2637e-01, Meta loss averaged over last 500 steps = 2.9020e-01, PNorm = 121.1039, GNorm = 0.3135
Meta loss on this task batch = 2.3661e-01, Meta loss averaged over last 500 steps = 2.9015e-01, PNorm = 121.1115, GNorm = 0.2718
Meta loss on this task batch = 2.7569e-01, Meta loss averaged over last 500 steps = 2.9007e-01, PNorm = 121.1191, GNorm = 0.2998
Meta loss on this task batch = 3.8133e-01, Meta loss averaged over last 500 steps = 2.9034e-01, PNorm = 121.1248, GNorm = 0.3300
Meta loss on this task batch = 2.6382e-01, Meta loss averaged over last 500 steps = 2.9033e-01, PNorm = 121.1295, GNorm = 0.3064
Took 132.47026681900024 seconds to complete one epoch of meta training
Took 141.05344820022583 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480424
Epoch 493
Meta loss on this task batch = 2.6721e-01, Meta loss averaged over last 500 steps = 2.9030e-01, PNorm = 121.1342, GNorm = 0.3013
Meta loss on this task batch = 2.7779e-01, Meta loss averaged over last 500 steps = 2.9027e-01, PNorm = 121.1395, GNorm = 0.2685
Meta loss on this task batch = 2.8134e-01, Meta loss averaged over last 500 steps = 2.9014e-01, PNorm = 121.1454, GNorm = 0.3014
Meta loss on this task batch = 3.4629e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 121.1507, GNorm = 0.2992
Meta loss on this task batch = 3.3128e-01, Meta loss averaged over last 500 steps = 2.9033e-01, PNorm = 121.1553, GNorm = 0.3075
Meta loss on this task batch = 3.4896e-01, Meta loss averaged over last 500 steps = 2.9036e-01, PNorm = 121.1599, GNorm = 0.3343
Meta loss on this task batch = 2.4397e-01, Meta loss averaged over last 500 steps = 2.9025e-01, PNorm = 121.1657, GNorm = 0.2777
Meta loss on this task batch = 2.7407e-01, Meta loss averaged over last 500 steps = 2.9011e-01, PNorm = 121.1734, GNorm = 0.2864
Meta loss on this task batch = 2.8994e-01, Meta loss averaged over last 500 steps = 2.9003e-01, PNorm = 121.1814, GNorm = 0.3114
Meta loss on this task batch = 2.8580e-01, Meta loss averaged over last 500 steps = 2.8999e-01, PNorm = 121.1908, GNorm = 0.3020
Meta loss on this task batch = 2.6066e-01, Meta loss averaged over last 500 steps = 2.8990e-01, PNorm = 121.2023, GNorm = 0.3195
Meta loss on this task batch = 2.9414e-01, Meta loss averaged over last 500 steps = 2.8999e-01, PNorm = 121.2121, GNorm = 0.2820
Meta loss on this task batch = 2.4949e-01, Meta loss averaged over last 500 steps = 2.8991e-01, PNorm = 121.2210, GNorm = 0.2914
Meta loss on this task batch = 2.9311e-01, Meta loss averaged over last 500 steps = 2.8999e-01, PNorm = 121.2286, GNorm = 0.2833
Meta loss on this task batch = 2.7581e-01, Meta loss averaged over last 500 steps = 2.9000e-01, PNorm = 121.2327, GNorm = 0.3293
Meta loss on this task batch = 2.8267e-01, Meta loss averaged over last 500 steps = 2.8991e-01, PNorm = 121.2369, GNorm = 0.2615
Meta loss on this task batch = 2.8167e-01, Meta loss averaged over last 500 steps = 2.8987e-01, PNorm = 121.2377, GNorm = 0.3083
Meta loss on this task batch = 3.0372e-01, Meta loss averaged over last 500 steps = 2.8984e-01, PNorm = 121.2388, GNorm = 0.3431
Meta loss on this task batch = 2.9986e-01, Meta loss averaged over last 500 steps = 2.9000e-01, PNorm = 121.2387, GNorm = 0.3772
Took 132.41224193572998 seconds to complete one epoch of meta training
Took 141.3117277622223 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469277
Epoch 494
Meta loss on this task batch = 3.2361e-01, Meta loss averaged over last 500 steps = 2.9016e-01, PNorm = 121.2407, GNorm = 0.3152
Meta loss on this task batch = 2.4301e-01, Meta loss averaged over last 500 steps = 2.9006e-01, PNorm = 121.2430, GNorm = 0.2646
Meta loss on this task batch = 3.1402e-01, Meta loss averaged over last 500 steps = 2.9010e-01, PNorm = 121.2455, GNorm = 0.2911
Meta loss on this task batch = 2.9936e-01, Meta loss averaged over last 500 steps = 2.9015e-01, PNorm = 121.2493, GNorm = 0.3138
Meta loss on this task batch = 2.3306e-01, Meta loss averaged over last 500 steps = 2.8998e-01, PNorm = 121.2524, GNorm = 0.3070
Meta loss on this task batch = 3.1987e-01, Meta loss averaged over last 500 steps = 2.9008e-01, PNorm = 121.2559, GNorm = 0.2915
Meta loss on this task batch = 2.6836e-01, Meta loss averaged over last 500 steps = 2.9004e-01, PNorm = 121.2609, GNorm = 0.2655
Meta loss on this task batch = 2.7917e-01, Meta loss averaged over last 500 steps = 2.8997e-01, PNorm = 121.2674, GNorm = 0.2698
Meta loss on this task batch = 3.0478e-01, Meta loss averaged over last 500 steps = 2.9002e-01, PNorm = 121.2743, GNorm = 0.2787
Meta loss on this task batch = 2.9601e-01, Meta loss averaged over last 500 steps = 2.9006e-01, PNorm = 121.2811, GNorm = 0.3048
Meta loss on this task batch = 3.1777e-01, Meta loss averaged over last 500 steps = 2.9019e-01, PNorm = 121.2895, GNorm = 0.3320
Meta loss on this task batch = 3.2279e-01, Meta loss averaged over last 500 steps = 2.9029e-01, PNorm = 121.2982, GNorm = 0.3139
Meta loss on this task batch = 2.1104e-01, Meta loss averaged over last 500 steps = 2.9016e-01, PNorm = 121.3049, GNorm = 0.2649
Meta loss on this task batch = 2.9794e-01, Meta loss averaged over last 500 steps = 2.9013e-01, PNorm = 121.3101, GNorm = 0.3139
Meta loss on this task batch = 3.4156e-01, Meta loss averaged over last 500 steps = 2.9023e-01, PNorm = 121.3130, GNorm = 0.3122
Meta loss on this task batch = 3.1984e-01, Meta loss averaged over last 500 steps = 2.9023e-01, PNorm = 121.3127, GNorm = 0.3738
Meta loss on this task batch = 3.6095e-01, Meta loss averaged over last 500 steps = 2.9031e-01, PNorm = 121.3148, GNorm = 0.3256
Meta loss on this task batch = 2.9045e-01, Meta loss averaged over last 500 steps = 2.9030e-01, PNorm = 121.3133, GNorm = 0.3001
Meta loss on this task batch = 2.8385e-01, Meta loss averaged over last 500 steps = 2.9033e-01, PNorm = 121.3109, GNorm = 0.3489
Took 133.69453120231628 seconds to complete one epoch of meta training
Took 141.5126974582672 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478752
Epoch 495
Meta loss on this task batch = 2.9115e-01, Meta loss averaged over last 500 steps = 2.9045e-01, PNorm = 121.3095, GNorm = 0.2998
Meta loss on this task batch = 2.6000e-01, Meta loss averaged over last 500 steps = 2.9037e-01, PNorm = 121.3112, GNorm = 0.2843
Meta loss on this task batch = 3.3620e-01, Meta loss averaged over last 500 steps = 2.9047e-01, PNorm = 121.3135, GNorm = 0.2687
Meta loss on this task batch = 2.9068e-01, Meta loss averaged over last 500 steps = 2.9043e-01, PNorm = 121.3151, GNorm = 0.3017
Meta loss on this task batch = 3.1224e-01, Meta loss averaged over last 500 steps = 2.9048e-01, PNorm = 121.3171, GNorm = 0.2993
Meta loss on this task batch = 2.4692e-01, Meta loss averaged over last 500 steps = 2.9040e-01, PNorm = 121.3207, GNorm = 0.2540
Meta loss on this task batch = 2.7873e-01, Meta loss averaged over last 500 steps = 2.9042e-01, PNorm = 121.3241, GNorm = 0.2748
Meta loss on this task batch = 2.9017e-01, Meta loss averaged over last 500 steps = 2.9049e-01, PNorm = 121.3280, GNorm = 0.2819
Meta loss on this task batch = 3.1412e-01, Meta loss averaged over last 500 steps = 2.9058e-01, PNorm = 121.3320, GNorm = 0.2579
Meta loss on this task batch = 3.1625e-01, Meta loss averaged over last 500 steps = 2.9067e-01, PNorm = 121.3377, GNorm = 0.2668
Meta loss on this task batch = 3.0110e-01, Meta loss averaged over last 500 steps = 2.9073e-01, PNorm = 121.3439, GNorm = 0.2503
Meta loss on this task batch = 2.4428e-01, Meta loss averaged over last 500 steps = 2.9068e-01, PNorm = 121.3513, GNorm = 0.2736
Meta loss on this task batch = 3.0215e-01, Meta loss averaged over last 500 steps = 2.9064e-01, PNorm = 121.3569, GNorm = 0.2979
Meta loss on this task batch = 2.6131e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 121.3650, GNorm = 0.2741
Meta loss on this task batch = 2.6853e-01, Meta loss averaged over last 500 steps = 2.9056e-01, PNorm = 121.3726, GNorm = 0.2716
Meta loss on this task batch = 2.9221e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 121.3783, GNorm = 0.2944
Meta loss on this task batch = 3.7590e-01, Meta loss averaged over last 500 steps = 2.9072e-01, PNorm = 121.3806, GNorm = 0.3319
Meta loss on this task batch = 2.8731e-01, Meta loss averaged over last 500 steps = 2.9074e-01, PNorm = 121.3824, GNorm = 0.2893
Meta loss on this task batch = 2.7359e-01, Meta loss averaged over last 500 steps = 2.9073e-01, PNorm = 121.3833, GNorm = 0.3267
Took 134.11187481880188 seconds to complete one epoch of meta training
Took 143.13227033615112 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499287
Epoch 496
Meta loss on this task batch = 2.7505e-01, Meta loss averaged over last 500 steps = 2.9073e-01, PNorm = 121.3842, GNorm = 0.2725
Meta loss on this task batch = 2.4348e-01, Meta loss averaged over last 500 steps = 2.9055e-01, PNorm = 121.3834, GNorm = 0.2908
Meta loss on this task batch = 3.1027e-01, Meta loss averaged over last 500 steps = 2.9055e-01, PNorm = 121.3807, GNorm = 0.3002
Meta loss on this task batch = 2.7781e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 121.3782, GNorm = 0.2491
Meta loss on this task batch = 3.2512e-01, Meta loss averaged over last 500 steps = 2.9058e-01, PNorm = 121.3767, GNorm = 0.2968
Meta loss on this task batch = 2.9338e-01, Meta loss averaged over last 500 steps = 2.9039e-01, PNorm = 121.3778, GNorm = 0.2997
Meta loss on this task batch = 3.3623e-01, Meta loss averaged over last 500 steps = 2.9051e-01, PNorm = 121.3803, GNorm = 0.2695
Meta loss on this task batch = 2.6378e-01, Meta loss averaged over last 500 steps = 2.9044e-01, PNorm = 121.3846, GNorm = 0.2508
Meta loss on this task batch = 3.0991e-01, Meta loss averaged over last 500 steps = 2.9045e-01, PNorm = 121.3892, GNorm = 0.2862
Meta loss on this task batch = 2.8310e-01, Meta loss averaged over last 500 steps = 2.9037e-01, PNorm = 121.3934, GNorm = 0.2648
Meta loss on this task batch = 2.8635e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 121.3995, GNorm = 0.2860
Meta loss on this task batch = 2.6002e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 121.4055, GNorm = 0.2965
Meta loss on this task batch = 2.5618e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 121.4136, GNorm = 0.2836
Meta loss on this task batch = 2.9403e-01, Meta loss averaged over last 500 steps = 2.9041e-01, PNorm = 121.4224, GNorm = 0.2927
Meta loss on this task batch = 3.2272e-01, Meta loss averaged over last 500 steps = 2.9039e-01, PNorm = 121.4294, GNorm = 0.3166
Meta loss on this task batch = 3.1430e-01, Meta loss averaged over last 500 steps = 2.9044e-01, PNorm = 121.4371, GNorm = 0.2923
Meta loss on this task batch = 2.7878e-01, Meta loss averaged over last 500 steps = 2.9044e-01, PNorm = 121.4460, GNorm = 0.3282
Meta loss on this task batch = 2.6048e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 121.4544, GNorm = 0.3335
Meta loss on this task batch = 3.0204e-01, Meta loss averaged over last 500 steps = 2.9026e-01, PNorm = 121.4610, GNorm = 0.3847
Took 132.24778079986572 seconds to complete one epoch of meta training
Took 140.8402874469757 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478517
Epoch 497
Meta loss on this task batch = 3.3338e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 121.4669, GNorm = 0.3148
Meta loss on this task batch = 3.0634e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 121.4711, GNorm = 0.3493
Meta loss on this task batch = 3.3197e-01, Meta loss averaged over last 500 steps = 2.9045e-01, PNorm = 121.4702, GNorm = 0.3443
Meta loss on this task batch = 2.5440e-01, Meta loss averaged over last 500 steps = 2.9040e-01, PNorm = 121.4719, GNorm = 0.2504
Meta loss on this task batch = 2.5705e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 121.4753, GNorm = 0.3095
Meta loss on this task batch = 2.8771e-01, Meta loss averaged over last 500 steps = 2.9026e-01, PNorm = 121.4811, GNorm = 0.2804
Meta loss on this task batch = 3.0349e-01, Meta loss averaged over last 500 steps = 2.9037e-01, PNorm = 121.4882, GNorm = 0.2870
Meta loss on this task batch = 2.8068e-01, Meta loss averaged over last 500 steps = 2.9040e-01, PNorm = 121.4950, GNorm = 0.2861
Meta loss on this task batch = 2.5354e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 121.5017, GNorm = 0.2551
Meta loss on this task batch = 3.0611e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 121.5092, GNorm = 0.3072
Meta loss on this task batch = 3.0293e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 121.5176, GNorm = 0.2589
Meta loss on this task batch = 2.8118e-01, Meta loss averaged over last 500 steps = 2.9030e-01, PNorm = 121.5254, GNorm = 0.2793
Meta loss on this task batch = 2.7791e-01, Meta loss averaged over last 500 steps = 2.9026e-01, PNorm = 121.5329, GNorm = 0.2601
Meta loss on this task batch = 3.2161e-01, Meta loss averaged over last 500 steps = 2.9030e-01, PNorm = 121.5412, GNorm = 0.2951
Meta loss on this task batch = 2.6915e-01, Meta loss averaged over last 500 steps = 2.9037e-01, PNorm = 121.5486, GNorm = 0.2687
Meta loss on this task batch = 2.2941e-01, Meta loss averaged over last 500 steps = 2.9023e-01, PNorm = 121.5545, GNorm = 0.2624
Meta loss on this task batch = 2.7745e-01, Meta loss averaged over last 500 steps = 2.9015e-01, PNorm = 121.5578, GNorm = 0.2745
Meta loss on this task batch = 2.6101e-01, Meta loss averaged over last 500 steps = 2.8995e-01, PNorm = 121.5594, GNorm = 0.2686
Meta loss on this task batch = 2.8085e-01, Meta loss averaged over last 500 steps = 2.8985e-01, PNorm = 121.5626, GNorm = 0.3697
Took 133.32078075408936 seconds to complete one epoch of meta training
Took 142.1346778869629 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492151
Epoch 498
Meta loss on this task batch = 2.4186e-01, Meta loss averaged over last 500 steps = 2.8986e-01, PNorm = 121.5662, GNorm = 0.2712
Meta loss on this task batch = 3.1361e-01, Meta loss averaged over last 500 steps = 2.8991e-01, PNorm = 121.5683, GNorm = 0.3401
Meta loss on this task batch = 3.1157e-01, Meta loss averaged over last 500 steps = 2.8992e-01, PNorm = 121.5697, GNorm = 0.3211
Meta loss on this task batch = 2.9911e-01, Meta loss averaged over last 500 steps = 2.8992e-01, PNorm = 121.5689, GNorm = 0.3285
Meta loss on this task batch = 3.3900e-01, Meta loss averaged over last 500 steps = 2.9004e-01, PNorm = 121.5682, GNorm = 0.3021
Meta loss on this task batch = 2.9881e-01, Meta loss averaged over last 500 steps = 2.9010e-01, PNorm = 121.5699, GNorm = 0.2880
Meta loss on this task batch = 2.6293e-01, Meta loss averaged over last 500 steps = 2.9003e-01, PNorm = 121.5719, GNorm = 0.2747
Meta loss on this task batch = 2.4942e-01, Meta loss averaged over last 500 steps = 2.9000e-01, PNorm = 121.5771, GNorm = 0.2745
Meta loss on this task batch = 2.7171e-01, Meta loss averaged over last 500 steps = 2.9011e-01, PNorm = 121.5818, GNorm = 0.2513
Meta loss on this task batch = 3.5279e-01, Meta loss averaged over last 500 steps = 2.9031e-01, PNorm = 121.5864, GNorm = 0.2909
Meta loss on this task batch = 2.7006e-01, Meta loss averaged over last 500 steps = 2.9029e-01, PNorm = 121.5910, GNorm = 0.2819
Meta loss on this task batch = 3.2600e-01, Meta loss averaged over last 500 steps = 2.9046e-01, PNorm = 121.5979, GNorm = 0.3010
Meta loss on this task batch = 2.8296e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 121.6059, GNorm = 0.2715
Meta loss on this task batch = 2.4967e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 121.6150, GNorm = 0.2591
Meta loss on this task batch = 2.5987e-01, Meta loss averaged over last 500 steps = 2.9012e-01, PNorm = 121.6264, GNorm = 0.2690
Meta loss on this task batch = 2.9333e-01, Meta loss averaged over last 500 steps = 2.9015e-01, PNorm = 121.6361, GNorm = 0.2909
Meta loss on this task batch = 2.7064e-01, Meta loss averaged over last 500 steps = 2.9022e-01, PNorm = 121.6459, GNorm = 0.3166
Meta loss on this task batch = 2.6890e-01, Meta loss averaged over last 500 steps = 2.9012e-01, PNorm = 121.6538, GNorm = 0.2677
Meta loss on this task batch = 2.4708e-01, Meta loss averaged over last 500 steps = 2.9008e-01, PNorm = 121.6582, GNorm = 0.3395
Took 131.9758744239807 seconds to complete one epoch of meta training
Took 140.20046019554138 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479649
Epoch 499
Meta loss on this task batch = 3.3206e-01, Meta loss averaged over last 500 steps = 2.9017e-01, PNorm = 121.6584, GNorm = 0.3311
Meta loss on this task batch = 2.8213e-01, Meta loss averaged over last 500 steps = 2.9003e-01, PNorm = 121.6583, GNorm = 0.3224
Meta loss on this task batch = 2.3865e-01, Meta loss averaged over last 500 steps = 2.8991e-01, PNorm = 121.6589, GNorm = 0.2729
Meta loss on this task batch = 2.8959e-01, Meta loss averaged over last 500 steps = 2.8997e-01, PNorm = 121.6619, GNorm = 0.3254
Meta loss on this task batch = 2.9328e-01, Meta loss averaged over last 500 steps = 2.8985e-01, PNorm = 121.6658, GNorm = 0.3028
Meta loss on this task batch = 2.6663e-01, Meta loss averaged over last 500 steps = 2.8977e-01, PNorm = 121.6718, GNorm = 0.2737
Meta loss on this task batch = 2.4527e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 121.6801, GNorm = 0.2958
Meta loss on this task batch = 3.3418e-01, Meta loss averaged over last 500 steps = 2.8980e-01, PNorm = 121.6880, GNorm = 0.5439
Meta loss on this task batch = 3.2276e-01, Meta loss averaged over last 500 steps = 2.8983e-01, PNorm = 121.6971, GNorm = 0.3142
Meta loss on this task batch = 2.7556e-01, Meta loss averaged over last 500 steps = 2.8980e-01, PNorm = 121.7079, GNorm = 0.2993
Meta loss on this task batch = 2.9850e-01, Meta loss averaged over last 500 steps = 2.8980e-01, PNorm = 121.7188, GNorm = 0.2864
Meta loss on this task batch = 2.9576e-01, Meta loss averaged over last 500 steps = 2.8987e-01, PNorm = 121.7298, GNorm = 0.2983
Meta loss on this task batch = 3.3246e-01, Meta loss averaged over last 500 steps = 2.8991e-01, PNorm = 121.7425, GNorm = 0.3148
Meta loss on this task batch = 2.6691e-01, Meta loss averaged over last 500 steps = 2.8968e-01, PNorm = 121.7556, GNorm = 0.3042
Meta loss on this task batch = 2.8944e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 121.7686, GNorm = 0.2951
Meta loss on this task batch = 2.5002e-01, Meta loss averaged over last 500 steps = 2.8953e-01, PNorm = 121.7788, GNorm = 0.2990
Meta loss on this task batch = 3.0145e-01, Meta loss averaged over last 500 steps = 2.8956e-01, PNorm = 121.7870, GNorm = 0.3063
Meta loss on this task batch = 3.0984e-01, Meta loss averaged over last 500 steps = 2.8970e-01, PNorm = 121.7942, GNorm = 0.3670
Meta loss on this task batch = 3.1601e-01, Meta loss averaged over last 500 steps = 2.8979e-01, PNorm = 121.8018, GNorm = 0.3327
Took 134.4316759109497 seconds to complete one epoch of meta training
Took 143.48688220977783 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492114
Epoch 500
Meta loss on this task batch = 2.6636e-01, Meta loss averaged over last 500 steps = 2.8971e-01, PNorm = 121.8060, GNorm = 0.4231
Meta loss on this task batch = 2.2722e-01, Meta loss averaged over last 500 steps = 2.8957e-01, PNorm = 121.8095, GNorm = 0.2477
Meta loss on this task batch = 2.8538e-01, Meta loss averaged over last 500 steps = 2.8952e-01, PNorm = 121.8128, GNorm = 0.2977
Meta loss on this task batch = 2.6836e-01, Meta loss averaged over last 500 steps = 2.8958e-01, PNorm = 121.8153, GNorm = 0.2984
Meta loss on this task batch = 3.0672e-01, Meta loss averaged over last 500 steps = 2.8969e-01, PNorm = 121.8142, GNorm = 0.3768
Meta loss on this task batch = 3.0062e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 121.8150, GNorm = 0.2872
Meta loss on this task batch = 3.0519e-01, Meta loss averaged over last 500 steps = 2.8968e-01, PNorm = 121.8146, GNorm = 0.3568
Meta loss on this task batch = 2.5407e-01, Meta loss averaged over last 500 steps = 2.8967e-01, PNorm = 121.8162, GNorm = 0.3033
Meta loss on this task batch = 2.7384e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 121.8168, GNorm = 0.3270
Meta loss on this task batch = 3.5276e-01, Meta loss averaged over last 500 steps = 2.8961e-01, PNorm = 121.8183, GNorm = 0.3673
Meta loss on this task batch = 2.9210e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 121.8199, GNorm = 0.3058
Meta loss on this task batch = 3.1388e-01, Meta loss averaged over last 500 steps = 2.8965e-01, PNorm = 121.8225, GNorm = 0.3015
Meta loss on this task batch = 3.0119e-01, Meta loss averaged over last 500 steps = 2.8974e-01, PNorm = 121.8289, GNorm = 0.3623
Meta loss on this task batch = 2.8779e-01, Meta loss averaged over last 500 steps = 2.8974e-01, PNorm = 121.8373, GNorm = 0.3008
Meta loss on this task batch = 2.3062e-01, Meta loss averaged over last 500 steps = 2.8961e-01, PNorm = 121.8477, GNorm = 0.2660
Meta loss on this task batch = 2.9955e-01, Meta loss averaged over last 500 steps = 2.8953e-01, PNorm = 121.8578, GNorm = 0.2773
Meta loss on this task batch = 2.9927e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 121.8681, GNorm = 0.3024
Meta loss on this task batch = 3.2110e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 121.8786, GNorm = 0.3297
Meta loss on this task batch = 2.7656e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 121.8889, GNorm = 0.3982
Took 134.73933577537537 seconds to complete one epoch of meta training
Took 143.74536681175232 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.514479
Epoch 501
Meta loss on this task batch = 3.2649e-01, Meta loss averaged over last 500 steps = 2.8966e-01, PNorm = 121.8907, GNorm = 0.4257
Meta loss on this task batch = 3.1642e-01, Meta loss averaged over last 500 steps = 2.8972e-01, PNorm = 121.8922, GNorm = 0.3209
Meta loss on this task batch = 2.7594e-01, Meta loss averaged over last 500 steps = 2.8967e-01, PNorm = 121.8889, GNorm = 0.4837
Meta loss on this task batch = 2.9307e-01, Meta loss averaged over last 500 steps = 2.8968e-01, PNorm = 121.8847, GNorm = 0.3296
Meta loss on this task batch = 2.6797e-01, Meta loss averaged over last 500 steps = 2.8958e-01, PNorm = 121.8813, GNorm = 0.2908
Meta loss on this task batch = 2.4434e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 121.8808, GNorm = 0.2582
Meta loss on this task batch = 2.3820e-01, Meta loss averaged over last 500 steps = 2.8944e-01, PNorm = 121.8823, GNorm = 0.2974
Meta loss on this task batch = 3.2801e-01, Meta loss averaged over last 500 steps = 2.8948e-01, PNorm = 121.8828, GNorm = 0.3216
Meta loss on this task batch = 2.8554e-01, Meta loss averaged over last 500 steps = 2.8944e-01, PNorm = 121.8850, GNorm = 0.2601
Meta loss on this task batch = 3.0615e-01, Meta loss averaged over last 500 steps = 2.8943e-01, PNorm = 121.8890, GNorm = 0.2770
Meta loss on this task batch = 3.1637e-01, Meta loss averaged over last 500 steps = 2.8947e-01, PNorm = 121.8944, GNorm = 0.2802
Meta loss on this task batch = 3.1607e-01, Meta loss averaged over last 500 steps = 2.8951e-01, PNorm = 121.9022, GNorm = 0.2753
Meta loss on this task batch = 3.1601e-01, Meta loss averaged over last 500 steps = 2.8956e-01, PNorm = 121.9129, GNorm = 0.2868
Meta loss on this task batch = 2.6646e-01, Meta loss averaged over last 500 steps = 2.8948e-01, PNorm = 121.9249, GNorm = 0.2880
Meta loss on this task batch = 3.0005e-01, Meta loss averaged over last 500 steps = 2.8958e-01, PNorm = 121.9356, GNorm = 0.3629
Meta loss on this task batch = 3.1590e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 121.9439, GNorm = 0.3349
Meta loss on this task batch = 2.7652e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 121.9524, GNorm = 0.3019
Meta loss on this task batch = 2.8372e-01, Meta loss averaged over last 500 steps = 2.8947e-01, PNorm = 121.9594, GNorm = 0.2930
Meta loss on this task batch = 2.4396e-01, Meta loss averaged over last 500 steps = 2.8936e-01, PNorm = 121.9649, GNorm = 0.3795
Took 133.61721348762512 seconds to complete one epoch of meta training
Took 141.9367368221283 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497702
Epoch 502
Meta loss on this task batch = 3.5376e-01, Meta loss averaged over last 500 steps = 2.8951e-01, PNorm = 121.9698, GNorm = 0.3412
Meta loss on this task batch = 3.5320e-01, Meta loss averaged over last 500 steps = 2.8965e-01, PNorm = 121.9718, GNorm = 0.3705
Meta loss on this task batch = 3.1201e-01, Meta loss averaged over last 500 steps = 2.8966e-01, PNorm = 121.9725, GNorm = 0.3390
Meta loss on this task batch = 2.6690e-01, Meta loss averaged over last 500 steps = 2.8969e-01, PNorm = 121.9735, GNorm = 0.2680
Meta loss on this task batch = 3.0952e-01, Meta loss averaged over last 500 steps = 2.8985e-01, PNorm = 121.9730, GNorm = 0.3199
Meta loss on this task batch = 2.7946e-01, Meta loss averaged over last 500 steps = 2.8980e-01, PNorm = 121.9721, GNorm = 0.3043
Meta loss on this task batch = 3.1756e-01, Meta loss averaged over last 500 steps = 2.8984e-01, PNorm = 121.9697, GNorm = 0.3450
Meta loss on this task batch = 2.7868e-01, Meta loss averaged over last 500 steps = 2.8972e-01, PNorm = 121.9692, GNorm = 0.2680
Meta loss on this task batch = 2.5151e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 121.9708, GNorm = 0.2471
Meta loss on this task batch = 2.8463e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 121.9750, GNorm = 0.2654
Meta loss on this task batch = 3.2701e-01, Meta loss averaged over last 500 steps = 2.8973e-01, PNorm = 121.9835, GNorm = 0.3306
Meta loss on this task batch = 2.7124e-01, Meta loss averaged over last 500 steps = 2.8972e-01, PNorm = 121.9910, GNorm = 0.2876
Meta loss on this task batch = 2.5126e-01, Meta loss averaged over last 500 steps = 2.8975e-01, PNorm = 121.9993, GNorm = 0.2704
Meta loss on this task batch = 2.8553e-01, Meta loss averaged over last 500 steps = 2.8991e-01, PNorm = 122.0089, GNorm = 0.2935
Meta loss on this task batch = 2.7209e-01, Meta loss averaged over last 500 steps = 2.8989e-01, PNorm = 122.0199, GNorm = 0.3132
Meta loss on this task batch = 3.2099e-01, Meta loss averaged over last 500 steps = 2.8991e-01, PNorm = 122.0298, GNorm = 0.2738
Meta loss on this task batch = 2.3348e-01, Meta loss averaged over last 500 steps = 2.8979e-01, PNorm = 122.0403, GNorm = 0.2558
Meta loss on this task batch = 2.6562e-01, Meta loss averaged over last 500 steps = 2.8973e-01, PNorm = 122.0486, GNorm = 0.2871
Meta loss on this task batch = 3.0467e-01, Meta loss averaged over last 500 steps = 2.8977e-01, PNorm = 122.0567, GNorm = 0.3447
Took 133.8103060722351 seconds to complete one epoch of meta training
Took 142.9347369670868 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474889
Epoch 503
Meta loss on this task batch = 3.1957e-01, Meta loss averaged over last 500 steps = 2.8983e-01, PNorm = 122.0609, GNorm = 0.3749
Meta loss on this task batch = 2.4003e-01, Meta loss averaged over last 500 steps = 2.8961e-01, PNorm = 122.0640, GNorm = 0.2647
Meta loss on this task batch = 2.4196e-01, Meta loss averaged over last 500 steps = 2.8933e-01, PNorm = 122.0672, GNorm = 0.2721
Meta loss on this task batch = 3.3013e-01, Meta loss averaged over last 500 steps = 2.8957e-01, PNorm = 122.0668, GNorm = 0.3942
Meta loss on this task batch = 3.1660e-01, Meta loss averaged over last 500 steps = 2.8966e-01, PNorm = 122.0635, GNorm = 0.3771
Meta loss on this task batch = 3.1341e-01, Meta loss averaged over last 500 steps = 2.8976e-01, PNorm = 122.0605, GNorm = 0.3299
Meta loss on this task batch = 3.0151e-01, Meta loss averaged over last 500 steps = 2.8975e-01, PNorm = 122.0570, GNorm = 0.2833
Meta loss on this task batch = 2.7080e-01, Meta loss averaged over last 500 steps = 2.8959e-01, PNorm = 122.0572, GNorm = 0.2834
Meta loss on this task batch = 2.5507e-01, Meta loss averaged over last 500 steps = 2.8953e-01, PNorm = 122.0594, GNorm = 0.2587
Meta loss on this task batch = 3.1005e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 122.0640, GNorm = 0.2596
Meta loss on this task batch = 2.8015e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 122.0688, GNorm = 0.2717
Meta loss on this task batch = 2.4355e-01, Meta loss averaged over last 500 steps = 2.8953e-01, PNorm = 122.0767, GNorm = 0.2754
Meta loss on this task batch = 2.9151e-01, Meta loss averaged over last 500 steps = 2.8952e-01, PNorm = 122.0851, GNorm = 0.2665
Meta loss on this task batch = 3.4533e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 122.0929, GNorm = 0.2820
Meta loss on this task batch = 2.7757e-01, Meta loss averaged over last 500 steps = 2.8956e-01, PNorm = 122.1018, GNorm = 0.2874
Meta loss on this task batch = 3.1996e-01, Meta loss averaged over last 500 steps = 2.8968e-01, PNorm = 122.1092, GNorm = 0.2922
Meta loss on this task batch = 2.4977e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 122.1185, GNorm = 0.2424
Meta loss on this task batch = 3.4267e-01, Meta loss averaged over last 500 steps = 2.8962e-01, PNorm = 122.1241, GNorm = 0.3357
Meta loss on this task batch = 2.6260e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 122.1294, GNorm = 0.3106
Took 136.29353308677673 seconds to complete one epoch of meta training
Took 144.99113655090332 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479010
Epoch 504
Meta loss on this task batch = 3.1387e-01, Meta loss averaged over last 500 steps = 2.8964e-01, PNorm = 122.1346, GNorm = 0.3204
Meta loss on this task batch = 2.8913e-01, Meta loss averaged over last 500 steps = 2.8974e-01, PNorm = 122.1394, GNorm = 0.3087
Meta loss on this task batch = 3.1968e-01, Meta loss averaged over last 500 steps = 2.8974e-01, PNorm = 122.1415, GNorm = 0.3025
Meta loss on this task batch = 2.7338e-01, Meta loss averaged over last 500 steps = 2.8971e-01, PNorm = 122.1443, GNorm = 0.2859
Meta loss on this task batch = 2.9225e-01, Meta loss averaged over last 500 steps = 2.8979e-01, PNorm = 122.1465, GNorm = 0.3236
Meta loss on this task batch = 2.6781e-01, Meta loss averaged over last 500 steps = 2.8965e-01, PNorm = 122.1511, GNorm = 0.2895
Meta loss on this task batch = 3.1892e-01, Meta loss averaged over last 500 steps = 2.8986e-01, PNorm = 122.1552, GNorm = 0.2773
Meta loss on this task batch = 3.2704e-01, Meta loss averaged over last 500 steps = 2.9001e-01, PNorm = 122.1585, GNorm = 0.3082
Meta loss on this task batch = 2.9460e-01, Meta loss averaged over last 500 steps = 2.9001e-01, PNorm = 122.1603, GNorm = 0.2950
Meta loss on this task batch = 2.5328e-01, Meta loss averaged over last 500 steps = 2.8990e-01, PNorm = 122.1651, GNorm = 0.3095
Meta loss on this task batch = 2.7008e-01, Meta loss averaged over last 500 steps = 2.8994e-01, PNorm = 122.1679, GNorm = 0.2888
Meta loss on this task batch = 2.7335e-01, Meta loss averaged over last 500 steps = 2.8988e-01, PNorm = 122.1690, GNorm = 0.2683
Meta loss on this task batch = 3.1168e-01, Meta loss averaged over last 500 steps = 2.8994e-01, PNorm = 122.1687, GNorm = 0.2894
Meta loss on this task batch = 3.3823e-01, Meta loss averaged over last 500 steps = 2.8999e-01, PNorm = 122.1671, GNorm = 0.3236
Meta loss on this task batch = 2.6091e-01, Meta loss averaged over last 500 steps = 2.8999e-01, PNorm = 122.1677, GNorm = 0.2643
Meta loss on this task batch = 2.7091e-01, Meta loss averaged over last 500 steps = 2.9002e-01, PNorm = 122.1667, GNorm = 0.3003
Meta loss on this task batch = 2.4166e-01, Meta loss averaged over last 500 steps = 2.8988e-01, PNorm = 122.1674, GNorm = 0.2750
Meta loss on this task batch = 2.6298e-01, Meta loss averaged over last 500 steps = 2.8978e-01, PNorm = 122.1680, GNorm = 0.3098
Meta loss on this task batch = 2.5195e-01, Meta loss averaged over last 500 steps = 2.8958e-01, PNorm = 122.1707, GNorm = 0.3301
Took 134.51686334609985 seconds to complete one epoch of meta training
Took 143.10977864265442 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481990
Epoch 505
Meta loss on this task batch = 2.7535e-01, Meta loss averaged over last 500 steps = 2.8946e-01, PNorm = 122.1751, GNorm = 0.3168
Meta loss on this task batch = 2.5940e-01, Meta loss averaged over last 500 steps = 2.8943e-01, PNorm = 122.1809, GNorm = 0.2852
Meta loss on this task batch = 2.8172e-01, Meta loss averaged over last 500 steps = 2.8937e-01, PNorm = 122.1884, GNorm = 0.2832
Meta loss on this task batch = 2.8453e-01, Meta loss averaged over last 500 steps = 2.8944e-01, PNorm = 122.1956, GNorm = 0.3003
Meta loss on this task batch = 3.1901e-01, Meta loss averaged over last 500 steps = 2.8951e-01, PNorm = 122.2046, GNorm = 0.3199
Meta loss on this task batch = 2.9467e-01, Meta loss averaged over last 500 steps = 2.8962e-01, PNorm = 122.2138, GNorm = 0.2966
Meta loss on this task batch = 2.7451e-01, Meta loss averaged over last 500 steps = 2.8964e-01, PNorm = 122.2223, GNorm = 0.3022
Meta loss on this task batch = 2.1734e-01, Meta loss averaged over last 500 steps = 2.8959e-01, PNorm = 122.2321, GNorm = 0.2424
Meta loss on this task batch = 3.0600e-01, Meta loss averaged over last 500 steps = 2.8966e-01, PNorm = 122.2401, GNorm = 0.3687
Meta loss on this task batch = 3.1350e-01, Meta loss averaged over last 500 steps = 2.8977e-01, PNorm = 122.2415, GNorm = 0.4792
Meta loss on this task batch = 2.9604e-01, Meta loss averaged over last 500 steps = 2.8968e-01, PNorm = 122.2438, GNorm = 0.3058
Meta loss on this task batch = 2.9417e-01, Meta loss averaged over last 500 steps = 2.8977e-01, PNorm = 122.2476, GNorm = 0.3117
Meta loss on this task batch = 2.0234e-01, Meta loss averaged over last 500 steps = 2.8959e-01, PNorm = 122.2520, GNorm = 0.2523
Meta loss on this task batch = 2.6515e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 122.2561, GNorm = 0.2746
Meta loss on this task batch = 3.2871e-01, Meta loss averaged over last 500 steps = 2.8961e-01, PNorm = 122.2603, GNorm = 0.3268
Meta loss on this task batch = 2.7177e-01, Meta loss averaged over last 500 steps = 2.8959e-01, PNorm = 122.2667, GNorm = 0.3138
Meta loss on this task batch = 2.8360e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 122.2729, GNorm = 0.3114
Meta loss on this task batch = 3.1363e-01, Meta loss averaged over last 500 steps = 2.8946e-01, PNorm = 122.2813, GNorm = 0.3267
Meta loss on this task batch = 3.3305e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 122.2897, GNorm = 0.4008
Took 135.6573247909546 seconds to complete one epoch of meta training
Took 144.43255305290222 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471067
Epoch 506
Meta loss on this task batch = 2.8125e-01, Meta loss averaged over last 500 steps = 2.8953e-01, PNorm = 122.2987, GNorm = 0.3131
Meta loss on this task batch = 2.7390e-01, Meta loss averaged over last 500 steps = 2.8944e-01, PNorm = 122.3079, GNorm = 0.2922
Meta loss on this task batch = 3.0261e-01, Meta loss averaged over last 500 steps = 2.8942e-01, PNorm = 122.3150, GNorm = 0.3447
Meta loss on this task batch = 2.6285e-01, Meta loss averaged over last 500 steps = 2.8929e-01, PNorm = 122.3191, GNorm = 0.2856
Meta loss on this task batch = 2.8878e-01, Meta loss averaged over last 500 steps = 2.8932e-01, PNorm = 122.3241, GNorm = 0.3334
Meta loss on this task batch = 2.8104e-01, Meta loss averaged over last 500 steps = 2.8935e-01, PNorm = 122.3270, GNorm = 0.3172
Meta loss on this task batch = 2.8968e-01, Meta loss averaged over last 500 steps = 2.8937e-01, PNorm = 122.3313, GNorm = 0.2997
Meta loss on this task batch = 3.1919e-01, Meta loss averaged over last 500 steps = 2.8949e-01, PNorm = 122.3362, GNorm = 0.4021
Meta loss on this task batch = 2.9546e-01, Meta loss averaged over last 500 steps = 2.8953e-01, PNorm = 122.3429, GNorm = 0.2872
Meta loss on this task batch = 2.3551e-01, Meta loss averaged over last 500 steps = 2.8935e-01, PNorm = 122.3509, GNorm = 0.3158
Meta loss on this task batch = 2.7759e-01, Meta loss averaged over last 500 steps = 2.8928e-01, PNorm = 122.3570, GNorm = 0.2937
Meta loss on this task batch = 2.6223e-01, Meta loss averaged over last 500 steps = 2.8918e-01, PNorm = 122.3647, GNorm = 0.2775
Meta loss on this task batch = 2.9061e-01, Meta loss averaged over last 500 steps = 2.8924e-01, PNorm = 122.3722, GNorm = 0.2908
Meta loss on this task batch = 3.1981e-01, Meta loss averaged over last 500 steps = 2.8930e-01, PNorm = 122.3795, GNorm = 0.2718
Meta loss on this task batch = 3.3012e-01, Meta loss averaged over last 500 steps = 2.8940e-01, PNorm = 122.3855, GNorm = 0.2798
Meta loss on this task batch = 2.9723e-01, Meta loss averaged over last 500 steps = 2.8934e-01, PNorm = 122.3926, GNorm = 0.2965
Meta loss on this task batch = 3.2052e-01, Meta loss averaged over last 500 steps = 2.8929e-01, PNorm = 122.4002, GNorm = 0.2989
Meta loss on this task batch = 2.7114e-01, Meta loss averaged over last 500 steps = 2.8927e-01, PNorm = 122.4086, GNorm = 0.3041
Meta loss on this task batch = 2.9605e-01, Meta loss averaged over last 500 steps = 2.8941e-01, PNorm = 122.4175, GNorm = 0.3764
Took 135.45484042167664 seconds to complete one epoch of meta training
Took 144.5307388305664 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509308
Epoch 507
Meta loss on this task batch = 2.6595e-01, Meta loss averaged over last 500 steps = 2.8930e-01, PNorm = 122.4254, GNorm = 0.2816
Meta loss on this task batch = 2.4375e-01, Meta loss averaged over last 500 steps = 2.8919e-01, PNorm = 122.4310, GNorm = 0.3125
Meta loss on this task batch = 3.1174e-01, Meta loss averaged over last 500 steps = 2.8925e-01, PNorm = 122.4332, GNorm = 0.3319
Meta loss on this task batch = 2.2770e-01, Meta loss averaged over last 500 steps = 2.8920e-01, PNorm = 122.4348, GNorm = 0.2523
Meta loss on this task batch = 3.5470e-01, Meta loss averaged over last 500 steps = 2.8937e-01, PNorm = 122.4355, GNorm = 0.3462
Meta loss on this task batch = 3.2659e-01, Meta loss averaged over last 500 steps = 2.8931e-01, PNorm = 122.4323, GNorm = 0.3562
Meta loss on this task batch = 3.1033e-01, Meta loss averaged over last 500 steps = 2.8929e-01, PNorm = 122.4280, GNorm = 0.3300
Meta loss on this task batch = 2.4183e-01, Meta loss averaged over last 500 steps = 2.8924e-01, PNorm = 122.4250, GNorm = 0.2658
Meta loss on this task batch = 2.5587e-01, Meta loss averaged over last 500 steps = 2.8918e-01, PNorm = 122.4259, GNorm = 0.2824
Meta loss on this task batch = 3.4635e-01, Meta loss averaged over last 500 steps = 2.8931e-01, PNorm = 122.4277, GNorm = 0.2839
Meta loss on this task batch = 2.9311e-01, Meta loss averaged over last 500 steps = 2.8934e-01, PNorm = 122.4308, GNorm = 0.2700
Meta loss on this task batch = 2.8903e-01, Meta loss averaged over last 500 steps = 2.8934e-01, PNorm = 122.4379, GNorm = 0.3198
Meta loss on this task batch = 3.2539e-01, Meta loss averaged over last 500 steps = 2.8940e-01, PNorm = 122.4473, GNorm = 0.3569
Meta loss on this task batch = 2.8851e-01, Meta loss averaged over last 500 steps = 2.8938e-01, PNorm = 122.4595, GNorm = 0.3084
Meta loss on this task batch = 2.7440e-01, Meta loss averaged over last 500 steps = 2.8932e-01, PNorm = 122.4726, GNorm = 0.2637
Meta loss on this task batch = 3.1532e-01, Meta loss averaged over last 500 steps = 2.8930e-01, PNorm = 122.4852, GNorm = 0.3369
Meta loss on this task batch = 3.5939e-01, Meta loss averaged over last 500 steps = 2.8956e-01, PNorm = 122.4953, GNorm = 0.3444
Meta loss on this task batch = 2.1479e-01, Meta loss averaged over last 500 steps = 2.8933e-01, PNorm = 122.5060, GNorm = 0.2505
Meta loss on this task batch = 2.3678e-01, Meta loss averaged over last 500 steps = 2.8929e-01, PNorm = 122.5164, GNorm = 0.2909
Took 136.4550461769104 seconds to complete one epoch of meta training
Took 144.09175825119019 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469773
Epoch 508
Meta loss on this task batch = 3.1427e-01, Meta loss averaged over last 500 steps = 2.8933e-01, PNorm = 122.5225, GNorm = 0.3671
Meta loss on this task batch = 2.8423e-01, Meta loss averaged over last 500 steps = 2.8940e-01, PNorm = 122.5221, GNorm = 0.4099
Meta loss on this task batch = 2.8240e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 122.5207, GNorm = 0.2749
Meta loss on this task batch = 1.9861e-01, Meta loss averaged over last 500 steps = 2.8929e-01, PNorm = 122.5207, GNorm = 0.2511
Meta loss on this task batch = 2.8498e-01, Meta loss averaged over last 500 steps = 2.8920e-01, PNorm = 122.5199, GNorm = 0.3119
Meta loss on this task batch = 2.6486e-01, Meta loss averaged over last 500 steps = 2.8915e-01, PNorm = 122.5186, GNorm = 0.2604
Meta loss on this task batch = 3.3204e-01, Meta loss averaged over last 500 steps = 2.8927e-01, PNorm = 122.5186, GNorm = 0.3036
Meta loss on this task batch = 2.7827e-01, Meta loss averaged over last 500 steps = 2.8926e-01, PNorm = 122.5187, GNorm = 0.3147
Meta loss on this task batch = 2.5614e-01, Meta loss averaged over last 500 steps = 2.8915e-01, PNorm = 122.5201, GNorm = 0.3035
Meta loss on this task batch = 2.7886e-01, Meta loss averaged over last 500 steps = 2.8915e-01, PNorm = 122.5238, GNorm = 0.2848
Meta loss on this task batch = 3.4083e-01, Meta loss averaged over last 500 steps = 2.8916e-01, PNorm = 122.5285, GNorm = 0.3170
Meta loss on this task batch = 2.2152e-01, Meta loss averaged over last 500 steps = 2.8902e-01, PNorm = 122.5355, GNorm = 0.2377
Meta loss on this task batch = 3.1003e-01, Meta loss averaged over last 500 steps = 2.8909e-01, PNorm = 122.5433, GNorm = 0.2695
Meta loss on this task batch = 3.0697e-01, Meta loss averaged over last 500 steps = 2.8912e-01, PNorm = 122.5516, GNorm = 0.2801
Meta loss on this task batch = 3.0431e-01, Meta loss averaged over last 500 steps = 2.8919e-01, PNorm = 122.5593, GNorm = 0.2828
Meta loss on this task batch = 2.7993e-01, Meta loss averaged over last 500 steps = 2.8913e-01, PNorm = 122.5688, GNorm = 0.3026
Meta loss on this task batch = 2.7711e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 122.5780, GNorm = 0.2856
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 122.5851, GNorm = 0.3062
Meta loss on this task batch = 3.5731e-01, Meta loss averaged over last 500 steps = 2.8921e-01, PNorm = 122.5901, GNorm = 0.3745
Took 132.97529458999634 seconds to complete one epoch of meta training
Took 141.66127109527588 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493983
Epoch 509
Meta loss on this task batch = 2.5483e-01, Meta loss averaged over last 500 steps = 2.8917e-01, PNorm = 122.5925, GNorm = 0.3251
Meta loss on this task batch = 2.4022e-01, Meta loss averaged over last 500 steps = 2.8907e-01, PNorm = 122.5934, GNorm = 0.2625
Meta loss on this task batch = 2.3848e-01, Meta loss averaged over last 500 steps = 2.8889e-01, PNorm = 122.5952, GNorm = 0.2822
Meta loss on this task batch = 2.9185e-01, Meta loss averaged over last 500 steps = 2.8904e-01, PNorm = 122.5969, GNorm = 0.2600
Meta loss on this task batch = 2.7759e-01, Meta loss averaged over last 500 steps = 2.8909e-01, PNorm = 122.5991, GNorm = 0.2993
Meta loss on this task batch = 3.0362e-01, Meta loss averaged over last 500 steps = 2.8916e-01, PNorm = 122.6028, GNorm = 0.3190
Meta loss on this task batch = 2.8630e-01, Meta loss averaged over last 500 steps = 2.8909e-01, PNorm = 122.6075, GNorm = 0.2915
Meta loss on this task batch = 2.3665e-01, Meta loss averaged over last 500 steps = 2.8905e-01, PNorm = 122.6128, GNorm = 0.2329
Meta loss on this task batch = 3.4080e-01, Meta loss averaged over last 500 steps = 2.8916e-01, PNorm = 122.6180, GNorm = 0.3544
Meta loss on this task batch = 2.8380e-01, Meta loss averaged over last 500 steps = 2.8917e-01, PNorm = 122.6253, GNorm = 0.2812
Meta loss on this task batch = 2.7811e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 122.6330, GNorm = 0.2985
Meta loss on this task batch = 3.0665e-01, Meta loss averaged over last 500 steps = 2.8910e-01, PNorm = 122.6409, GNorm = 0.2734
Meta loss on this task batch = 2.9045e-01, Meta loss averaged over last 500 steps = 2.8906e-01, PNorm = 122.6464, GNorm = 0.3156
Meta loss on this task batch = 2.8008e-01, Meta loss averaged over last 500 steps = 2.8905e-01, PNorm = 122.6497, GNorm = 0.2677
Meta loss on this task batch = 3.0430e-01, Meta loss averaged over last 500 steps = 2.8895e-01, PNorm = 122.6527, GNorm = 0.3090
Meta loss on this task batch = 2.3118e-01, Meta loss averaged over last 500 steps = 2.8883e-01, PNorm = 122.6540, GNorm = 0.2465
Meta loss on this task batch = 2.5720e-01, Meta loss averaged over last 500 steps = 2.8889e-01, PNorm = 122.6559, GNorm = 0.2660
Meta loss on this task batch = 3.0454e-01, Meta loss averaged over last 500 steps = 2.8897e-01, PNorm = 122.6564, GNorm = 0.3068
Meta loss on this task batch = 3.0162e-01, Meta loss averaged over last 500 steps = 2.8903e-01, PNorm = 122.6566, GNorm = 0.3620
Took 136.1805830001831 seconds to complete one epoch of meta training
Took 144.6716947555542 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501340
Epoch 510
Meta loss on this task batch = 2.9859e-01, Meta loss averaged over last 500 steps = 2.8896e-01, PNorm = 122.6555, GNorm = 0.2970
Meta loss on this task batch = 2.7339e-01, Meta loss averaged over last 500 steps = 2.8901e-01, PNorm = 122.6543, GNorm = 0.2959
Meta loss on this task batch = 2.6368e-01, Meta loss averaged over last 500 steps = 2.8894e-01, PNorm = 122.6557, GNorm = 0.2771
Meta loss on this task batch = 2.3829e-01, Meta loss averaged over last 500 steps = 2.8887e-01, PNorm = 122.6571, GNorm = 0.2715
Meta loss on this task batch = 3.2261e-01, Meta loss averaged over last 500 steps = 2.8889e-01, PNorm = 122.6610, GNorm = 0.3531
Meta loss on this task batch = 3.4113e-01, Meta loss averaged over last 500 steps = 2.8911e-01, PNorm = 122.6661, GNorm = 0.3114
Meta loss on this task batch = 2.9351e-01, Meta loss averaged over last 500 steps = 2.8907e-01, PNorm = 122.6718, GNorm = 0.3289
Meta loss on this task batch = 2.7624e-01, Meta loss averaged over last 500 steps = 2.8909e-01, PNorm = 122.6774, GNorm = 0.3119
Meta loss on this task batch = 2.9177e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 122.6828, GNorm = 0.3123
Meta loss on this task batch = 3.0179e-01, Meta loss averaged over last 500 steps = 2.8911e-01, PNorm = 122.6875, GNorm = 0.3192
Meta loss on this task batch = 2.6400e-01, Meta loss averaged over last 500 steps = 2.8901e-01, PNorm = 122.6908, GNorm = 0.2702
Meta loss on this task batch = 2.8101e-01, Meta loss averaged over last 500 steps = 2.8908e-01, PNorm = 122.6962, GNorm = 0.3226
Meta loss on this task batch = 2.5628e-01, Meta loss averaged over last 500 steps = 2.8894e-01, PNorm = 122.7020, GNorm = 0.3263
Meta loss on this task batch = 3.7896e-01, Meta loss averaged over last 500 steps = 2.8909e-01, PNorm = 122.7069, GNorm = 0.3582
Meta loss on this task batch = 2.5849e-01, Meta loss averaged over last 500 steps = 2.8904e-01, PNorm = 122.7096, GNorm = 0.3090
Meta loss on this task batch = 2.6573e-01, Meta loss averaged over last 500 steps = 2.8904e-01, PNorm = 122.7124, GNorm = 0.2958
Meta loss on this task batch = 2.7197e-01, Meta loss averaged over last 500 steps = 2.8893e-01, PNorm = 122.7178, GNorm = 0.2689
Meta loss on this task batch = 2.8224e-01, Meta loss averaged over last 500 steps = 2.8887e-01, PNorm = 122.7254, GNorm = 0.2986
Meta loss on this task batch = 3.3300e-01, Meta loss averaged over last 500 steps = 2.8896e-01, PNorm = 122.7328, GNorm = 0.3142
Took 134.2806077003479 seconds to complete one epoch of meta training
Took 143.03325748443604 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484621
Epoch 511
Meta loss on this task batch = 2.0925e-01, Meta loss averaged over last 500 steps = 2.8881e-01, PNorm = 122.7416, GNorm = 0.2405
Meta loss on this task batch = 2.7693e-01, Meta loss averaged over last 500 steps = 2.8879e-01, PNorm = 122.7493, GNorm = 0.2838
Meta loss on this task batch = 3.3203e-01, Meta loss averaged over last 500 steps = 2.8893e-01, PNorm = 122.7571, GNorm = 0.3193
Meta loss on this task batch = 3.0987e-01, Meta loss averaged over last 500 steps = 2.8897e-01, PNorm = 122.7647, GNorm = 0.2820
Meta loss on this task batch = 2.7252e-01, Meta loss averaged over last 500 steps = 2.8888e-01, PNorm = 122.7714, GNorm = 0.2749
Meta loss on this task batch = 3.1816e-01, Meta loss averaged over last 500 steps = 2.8907e-01, PNorm = 122.7747, GNorm = 0.3019
Meta loss on this task batch = 2.5026e-01, Meta loss averaged over last 500 steps = 2.8885e-01, PNorm = 122.7788, GNorm = 0.2525
Meta loss on this task batch = 2.4878e-01, Meta loss averaged over last 500 steps = 2.8884e-01, PNorm = 122.7837, GNorm = 0.2474
Meta loss on this task batch = 3.1250e-01, Meta loss averaged over last 500 steps = 2.8879e-01, PNorm = 122.7882, GNorm = 0.2960
Meta loss on this task batch = 2.6191e-01, Meta loss averaged over last 500 steps = 2.8863e-01, PNorm = 122.7906, GNorm = 0.2665
Meta loss on this task batch = 2.8573e-01, Meta loss averaged over last 500 steps = 2.8867e-01, PNorm = 122.7913, GNorm = 0.2804
Meta loss on this task batch = 3.3033e-01, Meta loss averaged over last 500 steps = 2.8884e-01, PNorm = 122.7916, GNorm = 0.2772
Meta loss on this task batch = 3.2755e-01, Meta loss averaged over last 500 steps = 2.8899e-01, PNorm = 122.7916, GNorm = 0.3081
Meta loss on this task batch = 2.4998e-01, Meta loss averaged over last 500 steps = 2.8893e-01, PNorm = 122.7938, GNorm = 0.2658
Meta loss on this task batch = 2.5083e-01, Meta loss averaged over last 500 steps = 2.8884e-01, PNorm = 122.7947, GNorm = 0.2769
Meta loss on this task batch = 2.7615e-01, Meta loss averaged over last 500 steps = 2.8878e-01, PNorm = 122.7985, GNorm = 0.2982
Meta loss on this task batch = 2.6964e-01, Meta loss averaged over last 500 steps = 2.8860e-01, PNorm = 122.8024, GNorm = 0.3108
Meta loss on this task batch = 3.0583e-01, Meta loss averaged over last 500 steps = 2.8867e-01, PNorm = 122.8052, GNorm = 0.3067
Meta loss on this task batch = 2.9243e-01, Meta loss averaged over last 500 steps = 2.8868e-01, PNorm = 122.8069, GNorm = 0.3523
Took 134.15320253372192 seconds to complete one epoch of meta training
Took 142.845796585083 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466704
Epoch 512
Meta loss on this task batch = 3.0055e-01, Meta loss averaged over last 500 steps = 2.8875e-01, PNorm = 122.8079, GNorm = 0.3042
Meta loss on this task batch = 3.1758e-01, Meta loss averaged over last 500 steps = 2.8879e-01, PNorm = 122.8091, GNorm = 0.3251
Meta loss on this task batch = 2.6662e-01, Meta loss averaged over last 500 steps = 2.8882e-01, PNorm = 122.8116, GNorm = 0.2761
Meta loss on this task batch = 2.9527e-01, Meta loss averaged over last 500 steps = 2.8884e-01, PNorm = 122.8123, GNorm = 0.3243
Meta loss on this task batch = 3.0979e-01, Meta loss averaged over last 500 steps = 2.8892e-01, PNorm = 122.8149, GNorm = 0.2941
Meta loss on this task batch = 2.8110e-01, Meta loss averaged over last 500 steps = 2.8888e-01, PNorm = 122.8204, GNorm = 0.3368
Meta loss on this task batch = 3.0640e-01, Meta loss averaged over last 500 steps = 2.8886e-01, PNorm = 122.8257, GNorm = 0.4324
Meta loss on this task batch = 1.7956e-01, Meta loss averaged over last 500 steps = 2.8847e-01, PNorm = 122.8329, GNorm = 0.2381
Meta loss on this task batch = 3.3684e-01, Meta loss averaged over last 500 steps = 2.8863e-01, PNorm = 122.8387, GNorm = 0.3348
Meta loss on this task batch = 2.5037e-01, Meta loss averaged over last 500 steps = 2.8855e-01, PNorm = 122.8464, GNorm = 0.2666
Meta loss on this task batch = 2.5354e-01, Meta loss averaged over last 500 steps = 2.8840e-01, PNorm = 122.8536, GNorm = 0.2880
Meta loss on this task batch = 2.8386e-01, Meta loss averaged over last 500 steps = 2.8836e-01, PNorm = 122.8571, GNorm = 0.3394
Meta loss on this task batch = 2.6057e-01, Meta loss averaged over last 500 steps = 2.8832e-01, PNorm = 122.8619, GNorm = 0.2872
Meta loss on this task batch = 2.7340e-01, Meta loss averaged over last 500 steps = 2.8833e-01, PNorm = 122.8656, GNorm = 0.2968
Meta loss on this task batch = 2.9623e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 122.8705, GNorm = 0.3168
Meta loss on this task batch = 2.8458e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 122.8737, GNorm = 0.3418
Meta loss on this task batch = 2.5829e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 122.8781, GNorm = 0.3305
Meta loss on this task batch = 2.9211e-01, Meta loss averaged over last 500 steps = 2.8836e-01, PNorm = 122.8818, GNorm = 0.3016
Meta loss on this task batch = 2.6703e-01, Meta loss averaged over last 500 steps = 2.8828e-01, PNorm = 122.8827, GNorm = 0.4486
Took 134.7016520500183 seconds to complete one epoch of meta training
Took 143.36084961891174 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460968
Epoch 513
Meta loss on this task batch = 2.4572e-01, Meta loss averaged over last 500 steps = 2.8817e-01, PNorm = 122.8834, GNorm = 0.2717
Meta loss on this task batch = 2.9938e-01, Meta loss averaged over last 500 steps = 2.8824e-01, PNorm = 122.8819, GNorm = 0.3203
Meta loss on this task batch = 2.4054e-01, Meta loss averaged over last 500 steps = 2.8807e-01, PNorm = 122.8804, GNorm = 0.3152
Meta loss on this task batch = 2.2113e-01, Meta loss averaged over last 500 steps = 2.8800e-01, PNorm = 122.8802, GNorm = 0.2432
Meta loss on this task batch = 2.7205e-01, Meta loss averaged over last 500 steps = 2.8795e-01, PNorm = 122.8810, GNorm = 0.3461
Meta loss on this task batch = 3.7664e-01, Meta loss averaged over last 500 steps = 2.8817e-01, PNorm = 122.8830, GNorm = 0.3006
Meta loss on this task batch = 3.0515e-01, Meta loss averaged over last 500 steps = 2.8818e-01, PNorm = 122.8841, GNorm = 0.2937
Meta loss on this task batch = 3.1233e-01, Meta loss averaged over last 500 steps = 2.8823e-01, PNorm = 122.8865, GNorm = 0.3110
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 2.8830e-01, PNorm = 122.8881, GNorm = 0.2917
Meta loss on this task batch = 2.2773e-01, Meta loss averaged over last 500 steps = 2.8811e-01, PNorm = 122.8925, GNorm = 0.2408
Meta loss on this task batch = 2.9483e-01, Meta loss averaged over last 500 steps = 2.8822e-01, PNorm = 122.8955, GNorm = 0.3392
Meta loss on this task batch = 2.8689e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 122.9000, GNorm = 0.2806
Meta loss on this task batch = 3.4756e-01, Meta loss averaged over last 500 steps = 2.8832e-01, PNorm = 122.9056, GNorm = 0.3105
Meta loss on this task batch = 2.6896e-01, Meta loss averaged over last 500 steps = 2.8833e-01, PNorm = 122.9112, GNorm = 0.3089
Meta loss on this task batch = 3.1052e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 122.9168, GNorm = 0.2981
Meta loss on this task batch = 3.6225e-01, Meta loss averaged over last 500 steps = 2.8854e-01, PNorm = 122.9228, GNorm = 0.3086
Meta loss on this task batch = 2.9264e-01, Meta loss averaged over last 500 steps = 2.8854e-01, PNorm = 122.9307, GNorm = 0.3075
Meta loss on this task batch = 2.8790e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 122.9406, GNorm = 0.3039
Meta loss on this task batch = 3.0776e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 122.9492, GNorm = 0.3523
Took 133.82305359840393 seconds to complete one epoch of meta training
Took 142.45915389060974 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472421
Epoch 514
Meta loss on this task batch = 2.7130e-01, Meta loss averaged over last 500 steps = 2.8849e-01, PNorm = 122.9589, GNorm = 0.2926
Meta loss on this task batch = 2.6003e-01, Meta loss averaged over last 500 steps = 2.8841e-01, PNorm = 122.9680, GNorm = 0.2198
Meta loss on this task batch = 3.1649e-01, Meta loss averaged over last 500 steps = 2.8843e-01, PNorm = 122.9755, GNorm = 0.2951
Meta loss on this task batch = 2.8402e-01, Meta loss averaged over last 500 steps = 2.8838e-01, PNorm = 122.9822, GNorm = 0.2685
Meta loss on this task batch = 2.6720e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 122.9886, GNorm = 0.2737
Meta loss on this task batch = 3.0374e-01, Meta loss averaged over last 500 steps = 2.8833e-01, PNorm = 122.9920, GNorm = 0.3223
Meta loss on this task batch = 2.5651e-01, Meta loss averaged over last 500 steps = 2.8832e-01, PNorm = 122.9934, GNorm = 0.3017
Meta loss on this task batch = 3.2742e-01, Meta loss averaged over last 500 steps = 2.8837e-01, PNorm = 122.9919, GNorm = 0.2985
Meta loss on this task batch = 3.2402e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 122.9901, GNorm = 0.2935
Meta loss on this task batch = 2.9716e-01, Meta loss averaged over last 500 steps = 2.8853e-01, PNorm = 122.9896, GNorm = 0.5876
Meta loss on this task batch = 2.4554e-01, Meta loss averaged over last 500 steps = 2.8851e-01, PNorm = 122.9902, GNorm = 0.2533
Meta loss on this task batch = 3.2659e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 122.9927, GNorm = 0.3100
Meta loss on this task batch = 2.8817e-01, Meta loss averaged over last 500 steps = 2.8847e-01, PNorm = 122.9944, GNorm = 0.2691
Meta loss on this task batch = 3.0235e-01, Meta loss averaged over last 500 steps = 2.8859e-01, PNorm = 122.9955, GNorm = 0.2887
Meta loss on this task batch = 2.9283e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 122.9969, GNorm = 0.2613
Meta loss on this task batch = 3.4710e-01, Meta loss averaged over last 500 steps = 2.8868e-01, PNorm = 122.9986, GNorm = 0.3139
Meta loss on this task batch = 2.4055e-01, Meta loss averaged over last 500 steps = 2.8859e-01, PNorm = 123.0008, GNorm = 0.2621
Meta loss on this task batch = 2.5063e-01, Meta loss averaged over last 500 steps = 2.8848e-01, PNorm = 123.0054, GNorm = 0.2781
Meta loss on this task batch = 2.6555e-01, Meta loss averaged over last 500 steps = 2.8848e-01, PNorm = 123.0125, GNorm = 0.2760
Took 133.2111644744873 seconds to complete one epoch of meta training
Took 141.9808051586151 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480950
Epoch 515
Meta loss on this task batch = 2.6898e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 123.0202, GNorm = 0.2696
Meta loss on this task batch = 2.8151e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 123.0284, GNorm = 0.2771
Meta loss on this task batch = 2.8901e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 123.0339, GNorm = 0.2815
Meta loss on this task batch = 2.7944e-01, Meta loss averaged over last 500 steps = 2.8821e-01, PNorm = 123.0398, GNorm = 0.2837
Meta loss on this task batch = 2.1635e-01, Meta loss averaged over last 500 steps = 2.8802e-01, PNorm = 123.0460, GNorm = 0.2555
Meta loss on this task batch = 3.0893e-01, Meta loss averaged over last 500 steps = 2.8796e-01, PNorm = 123.0513, GNorm = 0.2961
Meta loss on this task batch = 2.1897e-01, Meta loss averaged over last 500 steps = 2.8786e-01, PNorm = 123.0595, GNorm = 0.2997
Meta loss on this task batch = 2.9039e-01, Meta loss averaged over last 500 steps = 2.8788e-01, PNorm = 123.0678, GNorm = 0.2490
Meta loss on this task batch = 2.7497e-01, Meta loss averaged over last 500 steps = 2.8779e-01, PNorm = 123.0742, GNorm = 0.2998
Meta loss on this task batch = 2.5818e-01, Meta loss averaged over last 500 steps = 2.8770e-01, PNorm = 123.0803, GNorm = 0.2838
Meta loss on this task batch = 2.9121e-01, Meta loss averaged over last 500 steps = 2.8773e-01, PNorm = 123.0834, GNorm = 0.3298
Meta loss on this task batch = 3.1065e-01, Meta loss averaged over last 500 steps = 2.8778e-01, PNorm = 123.0859, GNorm = 0.3970
Meta loss on this task batch = 2.7365e-01, Meta loss averaged over last 500 steps = 2.8776e-01, PNorm = 123.0872, GNorm = 0.3014
Meta loss on this task batch = 2.8175e-01, Meta loss averaged over last 500 steps = 2.8788e-01, PNorm = 123.0855, GNorm = 0.3568
Meta loss on this task batch = 3.1861e-01, Meta loss averaged over last 500 steps = 2.8797e-01, PNorm = 123.0826, GNorm = 0.3272
Meta loss on this task batch = 3.1181e-01, Meta loss averaged over last 500 steps = 2.8807e-01, PNorm = 123.0793, GNorm = 0.2866
Meta loss on this task batch = 3.0463e-01, Meta loss averaged over last 500 steps = 2.8807e-01, PNorm = 123.0756, GNorm = 0.2904
Meta loss on this task batch = 2.8887e-01, Meta loss averaged over last 500 steps = 2.8806e-01, PNorm = 123.0708, GNorm = 0.3359
Meta loss on this task batch = 3.1761e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 123.0690, GNorm = 0.3957
Took 133.43434357643127 seconds to complete one epoch of meta training
Took 141.88129258155823 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473819
Epoch 516
Meta loss on this task batch = 3.1683e-01, Meta loss averaged over last 500 steps = 2.8829e-01, PNorm = 123.0719, GNorm = 0.3089
Meta loss on this task batch = 2.3823e-01, Meta loss averaged over last 500 steps = 2.8813e-01, PNorm = 123.0781, GNorm = 0.2939
Meta loss on this task batch = 2.4297e-01, Meta loss averaged over last 500 steps = 2.8799e-01, PNorm = 123.0863, GNorm = 0.3130
Meta loss on this task batch = 2.6031e-01, Meta loss averaged over last 500 steps = 2.8787e-01, PNorm = 123.0979, GNorm = 0.2796
Meta loss on this task batch = 3.2964e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 123.1090, GNorm = 0.2945
Meta loss on this task batch = 2.8619e-01, Meta loss averaged over last 500 steps = 2.8802e-01, PNorm = 123.1209, GNorm = 0.3200
Meta loss on this task batch = 2.9280e-01, Meta loss averaged over last 500 steps = 2.8805e-01, PNorm = 123.1329, GNorm = 0.3125
Meta loss on this task batch = 2.8135e-01, Meta loss averaged over last 500 steps = 2.8797e-01, PNorm = 123.1424, GNorm = 0.3153
Meta loss on this task batch = 3.0220e-01, Meta loss averaged over last 500 steps = 2.8798e-01, PNorm = 123.1483, GNorm = 0.3574
Meta loss on this task batch = 2.9405e-01, Meta loss averaged over last 500 steps = 2.8799e-01, PNorm = 123.1533, GNorm = 0.2875
Meta loss on this task batch = 2.5951e-01, Meta loss averaged over last 500 steps = 2.8802e-01, PNorm = 123.1576, GNorm = 0.2589
Meta loss on this task batch = 3.2940e-01, Meta loss averaged over last 500 steps = 2.8813e-01, PNorm = 123.1625, GNorm = 0.3383
Meta loss on this task batch = 3.1018e-01, Meta loss averaged over last 500 steps = 2.8828e-01, PNorm = 123.1675, GNorm = 0.2993
Meta loss on this task batch = 2.8562e-01, Meta loss averaged over last 500 steps = 2.8822e-01, PNorm = 123.1743, GNorm = 0.2834
Meta loss on this task batch = 3.1453e-01, Meta loss averaged over last 500 steps = 2.8836e-01, PNorm = 123.1810, GNorm = 0.2896
Meta loss on this task batch = 2.7918e-01, Meta loss averaged over last 500 steps = 2.8836e-01, PNorm = 123.1887, GNorm = 0.2737
Meta loss on this task batch = 2.9410e-01, Meta loss averaged over last 500 steps = 2.8828e-01, PNorm = 123.1980, GNorm = 0.2770
Meta loss on this task batch = 3.2576e-01, Meta loss averaged over last 500 steps = 2.8828e-01, PNorm = 123.2064, GNorm = 0.2973
Meta loss on this task batch = 2.6893e-01, Meta loss averaged over last 500 steps = 2.8818e-01, PNorm = 123.2148, GNorm = 0.3061
Took 134.55956864356995 seconds to complete one epoch of meta training
Took 142.9897801876068 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500401
Epoch 517
Meta loss on this task batch = 2.2870e-01, Meta loss averaged over last 500 steps = 2.8799e-01, PNorm = 123.2230, GNorm = 0.2173
Meta loss on this task batch = 3.1593e-01, Meta loss averaged over last 500 steps = 2.8798e-01, PNorm = 123.2279, GNorm = 0.2852
Meta loss on this task batch = 2.6232e-01, Meta loss averaged over last 500 steps = 2.8791e-01, PNorm = 123.2328, GNorm = 0.2660
Meta loss on this task batch = 2.6712e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 123.2379, GNorm = 0.2654
Meta loss on this task batch = 3.1007e-01, Meta loss averaged over last 500 steps = 2.8793e-01, PNorm = 123.2429, GNorm = 0.3044
Meta loss on this task batch = 2.7866e-01, Meta loss averaged over last 500 steps = 2.8793e-01, PNorm = 123.2485, GNorm = 0.2497
Meta loss on this task batch = 2.7341e-01, Meta loss averaged over last 500 steps = 2.8796e-01, PNorm = 123.2532, GNorm = 0.2767
Meta loss on this task batch = 2.7049e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 123.2559, GNorm = 0.3044
Meta loss on this task batch = 2.8857e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 123.2605, GNorm = 0.3067
Meta loss on this task batch = 2.5025e-01, Meta loss averaged over last 500 steps = 2.8778e-01, PNorm = 123.2666, GNorm = 0.2705
Meta loss on this task batch = 3.2840e-01, Meta loss averaged over last 500 steps = 2.8787e-01, PNorm = 123.2735, GNorm = 0.2949
Meta loss on this task batch = 2.5358e-01, Meta loss averaged over last 500 steps = 2.8781e-01, PNorm = 123.2778, GNorm = 0.2954
Meta loss on this task batch = 2.6792e-01, Meta loss averaged over last 500 steps = 2.8770e-01, PNorm = 123.2828, GNorm = 0.2608
Meta loss on this task batch = 3.5090e-01, Meta loss averaged over last 500 steps = 2.8784e-01, PNorm = 123.2885, GNorm = 0.3215
Meta loss on this task batch = 2.8720e-01, Meta loss averaged over last 500 steps = 2.8785e-01, PNorm = 123.2945, GNorm = 0.3164
Meta loss on this task batch = 3.0900e-01, Meta loss averaged over last 500 steps = 2.8790e-01, PNorm = 123.2994, GNorm = 0.3097
Meta loss on this task batch = 2.6242e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 123.3040, GNorm = 0.3086
Meta loss on this task batch = 3.1706e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 123.3064, GNorm = 0.3536
Meta loss on this task batch = 2.6369e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 123.3074, GNorm = 0.4582
Took 135.11590576171875 seconds to complete one epoch of meta training
Took 144.21647453308105 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502686
Epoch 518
Meta loss on this task batch = 2.5664e-01, Meta loss averaged over last 500 steps = 2.8791e-01, PNorm = 123.3086, GNorm = 0.2587
Meta loss on this task batch = 2.7894e-01, Meta loss averaged over last 500 steps = 2.8790e-01, PNorm = 123.3111, GNorm = 0.2955
Meta loss on this task batch = 2.6322e-01, Meta loss averaged over last 500 steps = 2.8778e-01, PNorm = 123.3142, GNorm = 0.2678
Meta loss on this task batch = 3.0148e-01, Meta loss averaged over last 500 steps = 2.8789e-01, PNorm = 123.3179, GNorm = 0.2755
Meta loss on this task batch = 2.2683e-01, Meta loss averaged over last 500 steps = 2.8776e-01, PNorm = 123.3242, GNorm = 0.2726
Meta loss on this task batch = 2.6040e-01, Meta loss averaged over last 500 steps = 2.8767e-01, PNorm = 123.3296, GNorm = 0.2839
Meta loss on this task batch = 2.8680e-01, Meta loss averaged over last 500 steps = 2.8758e-01, PNorm = 123.3338, GNorm = 0.3121
Meta loss on this task batch = 2.7652e-01, Meta loss averaged over last 500 steps = 2.8755e-01, PNorm = 123.3397, GNorm = 0.3008
Meta loss on this task batch = 2.8221e-01, Meta loss averaged over last 500 steps = 2.8748e-01, PNorm = 123.3456, GNorm = 0.2785
Meta loss on this task batch = 2.8028e-01, Meta loss averaged over last 500 steps = 2.8734e-01, PNorm = 123.3492, GNorm = 0.3545
Meta loss on this task batch = 2.7005e-01, Meta loss averaged over last 500 steps = 2.8730e-01, PNorm = 123.3529, GNorm = 0.2908
Meta loss on this task batch = 3.2943e-01, Meta loss averaged over last 500 steps = 2.8744e-01, PNorm = 123.3555, GNorm = 0.3290
Meta loss on this task batch = 3.5552e-01, Meta loss averaged over last 500 steps = 2.8766e-01, PNorm = 123.3572, GNorm = 0.3290
Meta loss on this task batch = 3.1907e-01, Meta loss averaged over last 500 steps = 2.8760e-01, PNorm = 123.3594, GNorm = 0.3542
Meta loss on this task batch = 2.4467e-01, Meta loss averaged over last 500 steps = 2.8753e-01, PNorm = 123.3632, GNorm = 0.3059
Meta loss on this task batch = 2.8600e-01, Meta loss averaged over last 500 steps = 2.8739e-01, PNorm = 123.3678, GNorm = 0.2919
Meta loss on this task batch = 3.2038e-01, Meta loss averaged over last 500 steps = 2.8752e-01, PNorm = 123.3690, GNorm = 0.3347
Meta loss on this task batch = 2.9247e-01, Meta loss averaged over last 500 steps = 2.8764e-01, PNorm = 123.3707, GNorm = 0.2814
Meta loss on this task batch = 2.6930e-01, Meta loss averaged over last 500 steps = 2.8758e-01, PNorm = 123.3733, GNorm = 0.3119
Took 133.01228094100952 seconds to complete one epoch of meta training
Took 142.15347266197205 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474464
Epoch 519
Meta loss on this task batch = 2.1585e-01, Meta loss averaged over last 500 steps = 2.8746e-01, PNorm = 123.3778, GNorm = 0.2515
Meta loss on this task batch = 3.0845e-01, Meta loss averaged over last 500 steps = 2.8742e-01, PNorm = 123.3829, GNorm = 0.2778
Meta loss on this task batch = 3.0617e-01, Meta loss averaged over last 500 steps = 2.8756e-01, PNorm = 123.3876, GNorm = 0.2866
Meta loss on this task batch = 2.5406e-01, Meta loss averaged over last 500 steps = 2.8752e-01, PNorm = 123.3969, GNorm = 0.2751
Meta loss on this task batch = 2.8723e-01, Meta loss averaged over last 500 steps = 2.8733e-01, PNorm = 123.4071, GNorm = 0.3355
Meta loss on this task batch = 2.4372e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 123.4197, GNorm = 0.2690
Meta loss on this task batch = 3.1660e-01, Meta loss averaged over last 500 steps = 2.8739e-01, PNorm = 123.4334, GNorm = 0.3195
Meta loss on this task batch = 2.4023e-01, Meta loss averaged over last 500 steps = 2.8731e-01, PNorm = 123.4471, GNorm = 0.3032
Meta loss on this task batch = 3.0915e-01, Meta loss averaged over last 500 steps = 2.8737e-01, PNorm = 123.4569, GNorm = 0.2945
Meta loss on this task batch = 3.1505e-01, Meta loss averaged over last 500 steps = 2.8731e-01, PNorm = 123.4628, GNorm = 0.3544
Meta loss on this task batch = 2.8463e-01, Meta loss averaged over last 500 steps = 2.8721e-01, PNorm = 123.4656, GNorm = 0.3262
Meta loss on this task batch = 2.5670e-01, Meta loss averaged over last 500 steps = 2.8703e-01, PNorm = 123.4662, GNorm = 0.2910
Meta loss on this task batch = 2.6717e-01, Meta loss averaged over last 500 steps = 2.8707e-01, PNorm = 123.4652, GNorm = 0.3043
Meta loss on this task batch = 3.0024e-01, Meta loss averaged over last 500 steps = 2.8713e-01, PNorm = 123.4639, GNorm = 0.3117
Meta loss on this task batch = 2.7264e-01, Meta loss averaged over last 500 steps = 2.8709e-01, PNorm = 123.4653, GNorm = 0.2772
Meta loss on this task batch = 2.6122e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 123.4658, GNorm = 0.3258
Meta loss on this task batch = 2.7370e-01, Meta loss averaged over last 500 steps = 2.8707e-01, PNorm = 123.4676, GNorm = 0.2848
Meta loss on this task batch = 2.5304e-01, Meta loss averaged over last 500 steps = 2.8699e-01, PNorm = 123.4691, GNorm = 0.2828
Meta loss on this task batch = 3.0934e-01, Meta loss averaged over last 500 steps = 2.8711e-01, PNorm = 123.4717, GNorm = 0.3534
Took 134.6186351776123 seconds to complete one epoch of meta training
Took 142.48863744735718 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487634
Epoch 520
Meta loss on this task batch = 2.6948e-01, Meta loss averaged over last 500 steps = 2.8706e-01, PNorm = 123.4760, GNorm = 0.3102
Meta loss on this task batch = 2.6596e-01, Meta loss averaged over last 500 steps = 2.8704e-01, PNorm = 123.4819, GNorm = 0.2708
Meta loss on this task batch = 2.5928e-01, Meta loss averaged over last 500 steps = 2.8699e-01, PNorm = 123.4898, GNorm = 0.2865
Meta loss on this task batch = 2.9363e-01, Meta loss averaged over last 500 steps = 2.8702e-01, PNorm = 123.4974, GNorm = 0.3131
Meta loss on this task batch = 2.7889e-01, Meta loss averaged over last 500 steps = 2.8697e-01, PNorm = 123.5019, GNorm = 0.3144
Meta loss on this task batch = 3.2238e-01, Meta loss averaged over last 500 steps = 2.8701e-01, PNorm = 123.5050, GNorm = 0.2757
Meta loss on this task batch = 3.0490e-01, Meta loss averaged over last 500 steps = 2.8697e-01, PNorm = 123.5084, GNorm = 0.3398
Meta loss on this task batch = 2.8211e-01, Meta loss averaged over last 500 steps = 2.8705e-01, PNorm = 123.5109, GNorm = 0.2890
Meta loss on this task batch = 2.8851e-01, Meta loss averaged over last 500 steps = 2.8700e-01, PNorm = 123.5148, GNorm = 0.2826
Meta loss on this task batch = 2.5088e-01, Meta loss averaged over last 500 steps = 2.8690e-01, PNorm = 123.5206, GNorm = 0.2786
Meta loss on this task batch = 2.8815e-01, Meta loss averaged over last 500 steps = 2.8701e-01, PNorm = 123.5275, GNorm = 0.2623
Meta loss on this task batch = 2.3090e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 123.5370, GNorm = 0.2731
Meta loss on this task batch = 3.0840e-01, Meta loss averaged over last 500 steps = 2.8692e-01, PNorm = 123.5484, GNorm = 0.3114
Meta loss on this task batch = 2.8319e-01, Meta loss averaged over last 500 steps = 2.8692e-01, PNorm = 123.5600, GNorm = 0.3055
Meta loss on this task batch = 3.6591e-01, Meta loss averaged over last 500 steps = 2.8705e-01, PNorm = 123.5701, GNorm = 0.3183
Meta loss on this task batch = 2.9808e-01, Meta loss averaged over last 500 steps = 2.8705e-01, PNorm = 123.5784, GNorm = 0.3236
Meta loss on this task batch = 2.5039e-01, Meta loss averaged over last 500 steps = 2.8692e-01, PNorm = 123.5869, GNorm = 0.2729
Meta loss on this task batch = 3.0867e-01, Meta loss averaged over last 500 steps = 2.8689e-01, PNorm = 123.5937, GNorm = 0.2913
Meta loss on this task batch = 2.3666e-01, Meta loss averaged over last 500 steps = 2.8694e-01, PNorm = 123.6007, GNorm = 0.2925
Took 130.1455717086792 seconds to complete one epoch of meta training
Took 138.23307132720947 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485319
Epoch 521
Meta loss on this task batch = 2.5458e-01, Meta loss averaged over last 500 steps = 2.8685e-01, PNorm = 123.6071, GNorm = 0.2820
Meta loss on this task batch = 2.1420e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 123.6137, GNorm = 0.2657
Meta loss on this task batch = 2.9740e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 123.6189, GNorm = 0.3260
Meta loss on this task batch = 3.0723e-01, Meta loss averaged over last 500 steps = 2.8645e-01, PNorm = 123.6214, GNorm = 0.3342
Meta loss on this task batch = 2.7044e-01, Meta loss averaged over last 500 steps = 2.8641e-01, PNorm = 123.6234, GNorm = 0.2906
Meta loss on this task batch = 2.9392e-01, Meta loss averaged over last 500 steps = 2.8643e-01, PNorm = 123.6245, GNorm = 0.3076
Meta loss on this task batch = 2.5907e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 123.6247, GNorm = 0.3008
Meta loss on this task batch = 3.1691e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 123.6211, GNorm = 0.3720
Meta loss on this task batch = 3.0275e-01, Meta loss averaged over last 500 steps = 2.8641e-01, PNorm = 123.6182, GNorm = 0.2997
Meta loss on this task batch = 3.0148e-01, Meta loss averaged over last 500 steps = 2.8643e-01, PNorm = 123.6171, GNorm = 0.3498
Meta loss on this task batch = 3.2523e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 123.6178, GNorm = 0.3164
Meta loss on this task batch = 3.3767e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 123.6228, GNorm = 0.3303
Meta loss on this task batch = 2.9725e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 123.6280, GNorm = 0.3079
Meta loss on this task batch = 3.1455e-01, Meta loss averaged over last 500 steps = 2.8672e-01, PNorm = 123.6372, GNorm = 0.3503
Meta loss on this task batch = 2.6773e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 123.6471, GNorm = 0.2800
Meta loss on this task batch = 2.6178e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 123.6578, GNorm = 0.2943
Meta loss on this task batch = 3.4250e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 123.6712, GNorm = 0.3317
Meta loss on this task batch = 2.6361e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 123.6862, GNorm = 0.2790
Meta loss on this task batch = 2.4226e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 123.7022, GNorm = 0.2737
Took 140.40110540390015 seconds to complete one epoch of meta training
Took 149.60959577560425 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491401
Epoch 522
Meta loss on this task batch = 2.6961e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 123.7178, GNorm = 0.3011
Meta loss on this task batch = 3.0291e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 123.7320, GNorm = 0.2980
Meta loss on this task batch = 2.7786e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 123.7454, GNorm = 0.2844
Meta loss on this task batch = 2.9395e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 123.7555, GNorm = 0.2988
Meta loss on this task batch = 2.1786e-01, Meta loss averaged over last 500 steps = 2.8628e-01, PNorm = 123.7624, GNorm = 0.2877
Meta loss on this task batch = 2.7262e-01, Meta loss averaged over last 500 steps = 2.8628e-01, PNorm = 123.7674, GNorm = 0.2581
Meta loss on this task batch = 2.8003e-01, Meta loss averaged over last 500 steps = 2.8629e-01, PNorm = 123.7721, GNorm = 0.3196
Meta loss on this task batch = 2.7759e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 123.7713, GNorm = 0.3222
Meta loss on this task batch = 3.3556e-01, Meta loss averaged over last 500 steps = 2.8640e-01, PNorm = 123.7689, GNorm = 0.3718
Meta loss on this task batch = 3.4756e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 123.7468, GNorm = 0.8998
Meta loss on this task batch = 2.4570e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 123.7256, GNorm = 0.2832
Meta loss on this task batch = 2.9302e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 123.7068, GNorm = 0.2934
Meta loss on this task batch = 3.2247e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 123.6927, GNorm = 0.2768
Meta loss on this task batch = 3.4326e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 123.6834, GNorm = 0.3034
Meta loss on this task batch = 3.1210e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 123.6766, GNorm = 0.2954
Meta loss on this task batch = 2.2541e-01, Meta loss averaged over last 500 steps = 2.8640e-01, PNorm = 123.6736, GNorm = 0.2320
Meta loss on this task batch = 2.5802e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 123.6731, GNorm = 0.2519
Meta loss on this task batch = 3.6130e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 123.6732, GNorm = 0.3171
Meta loss on this task batch = 2.7672e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 123.6730, GNorm = 0.2973
Took 132.68212842941284 seconds to complete one epoch of meta training
Took 141.06285333633423 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482621
Epoch 523
Meta loss on this task batch = 3.2598e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 123.6727, GNorm = 0.3127
Meta loss on this task batch = 3.1442e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 123.6729, GNorm = 0.2940
Meta loss on this task batch = 2.6248e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 123.6749, GNorm = 0.2941
Meta loss on this task batch = 3.1050e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 123.6781, GNorm = 0.3041
Meta loss on this task batch = 2.1783e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 123.6842, GNorm = 0.2859
Meta loss on this task batch = 2.8812e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 123.6911, GNorm = 0.2738
Meta loss on this task batch = 2.3787e-01, Meta loss averaged over last 500 steps = 2.8629e-01, PNorm = 123.6992, GNorm = 0.2605
Meta loss on this task batch = 2.9332e-01, Meta loss averaged over last 500 steps = 2.8627e-01, PNorm = 123.7067, GNorm = 0.2982
Meta loss on this task batch = 3.1803e-01, Meta loss averaged over last 500 steps = 2.8624e-01, PNorm = 123.7139, GNorm = 0.3096
Meta loss on this task batch = 3.7364e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 123.7221, GNorm = 0.3477
Meta loss on this task batch = 2.3901e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 123.7316, GNorm = 0.2615
Meta loss on this task batch = 2.7203e-01, Meta loss averaged over last 500 steps = 2.8641e-01, PNorm = 123.7392, GNorm = 0.2789
Meta loss on this task batch = 2.5346e-01, Meta loss averaged over last 500 steps = 2.8631e-01, PNorm = 123.7451, GNorm = 0.2750
Meta loss on this task batch = 3.1028e-01, Meta loss averaged over last 500 steps = 2.8637e-01, PNorm = 123.7474, GNorm = 0.3193
Meta loss on this task batch = 2.4854e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 123.7490, GNorm = 0.2345
Meta loss on this task batch = 2.9363e-01, Meta loss averaged over last 500 steps = 2.8634e-01, PNorm = 123.7493, GNorm = 0.2952
Meta loss on this task batch = 2.9436e-01, Meta loss averaged over last 500 steps = 2.8632e-01, PNorm = 123.7491, GNorm = 0.3144
Meta loss on this task batch = 2.9060e-01, Meta loss averaged over last 500 steps = 2.8634e-01, PNorm = 123.7504, GNorm = 0.2995
Meta loss on this task batch = 2.9691e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 123.7520, GNorm = 0.3198
Took 133.62172174453735 seconds to complete one epoch of meta training
Took 141.43498969078064 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455918
Epoch 524
Meta loss on this task batch = 2.9662e-01, Meta loss averaged over last 500 steps = 2.8633e-01, PNorm = 123.7549, GNorm = 0.2621
Meta loss on this task batch = 2.8824e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 123.7581, GNorm = 0.2826
Meta loss on this task batch = 2.6312e-01, Meta loss averaged over last 500 steps = 2.8643e-01, PNorm = 123.7619, GNorm = 0.3211
Meta loss on this task batch = 3.1817e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 123.7657, GNorm = 0.3099
Meta loss on this task batch = 3.0097e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 123.7691, GNorm = 0.2658
Meta loss on this task batch = 2.8934e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 123.7749, GNorm = 0.3006
Meta loss on this task batch = 2.8787e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 123.7833, GNorm = 0.3087
Meta loss on this task batch = 2.8773e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 123.7912, GNorm = 0.2884
Meta loss on this task batch = 3.3408e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 123.7981, GNorm = 0.3128
Meta loss on this task batch = 2.3388e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 123.8065, GNorm = 0.2523
Meta loss on this task batch = 3.0530e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 123.8135, GNorm = 0.3135
Meta loss on this task batch = 2.2694e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 123.8194, GNorm = 0.2793
Meta loss on this task batch = 3.1469e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 123.8249, GNorm = 0.3379
Meta loss on this task batch = 3.2547e-01, Meta loss averaged over last 500 steps = 2.8661e-01, PNorm = 123.8275, GNorm = 0.3072
Meta loss on this task batch = 2.9648e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 123.8293, GNorm = 0.2567
Meta loss on this task batch = 2.7757e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 123.8286, GNorm = 0.2696
Meta loss on this task batch = 2.9879e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 123.8296, GNorm = 0.3904
Meta loss on this task batch = 2.4424e-01, Meta loss averaged over last 500 steps = 2.8640e-01, PNorm = 123.8312, GNorm = 0.2936
Meta loss on this task batch = 2.6302e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 123.8349, GNorm = 0.3069
Took 137.07611513137817 seconds to complete one epoch of meta training
Took 144.75774240493774 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502203
Epoch 525
Meta loss on this task batch = 2.2700e-01, Meta loss averaged over last 500 steps = 2.8632e-01, PNorm = 123.8406, GNorm = 0.2235
Meta loss on this task batch = 3.1946e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 123.8455, GNorm = 0.3706
Meta loss on this task batch = 3.2634e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 123.8495, GNorm = 0.2743
Meta loss on this task batch = 2.7814e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 123.8545, GNorm = 0.2970
Meta loss on this task batch = 2.8114e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 123.8602, GNorm = 0.3278
Meta loss on this task batch = 2.2822e-01, Meta loss averaged over last 500 steps = 2.8650e-01, PNorm = 123.8661, GNorm = 0.2258
Meta loss on this task batch = 2.9081e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 123.8692, GNorm = 0.2997
Meta loss on this task batch = 2.8409e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 123.8701, GNorm = 0.3264
Meta loss on this task batch = 2.8354e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 123.8700, GNorm = 0.3378
Meta loss on this task batch = 3.2213e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 123.8698, GNorm = 0.3013
Meta loss on this task batch = 2.6307e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 123.8709, GNorm = 0.3024
Meta loss on this task batch = 2.8277e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 123.8732, GNorm = 0.2929
Meta loss on this task batch = 2.5750e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 123.8794, GNorm = 0.2593
Meta loss on this task batch = 2.7582e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 123.8850, GNorm = 0.2584
Meta loss on this task batch = 3.0185e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 123.8934, GNorm = 0.2986
Meta loss on this task batch = 3.3080e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 123.9021, GNorm = 0.2729
Meta loss on this task batch = 3.0895e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 123.9105, GNorm = 0.2883
Meta loss on this task batch = 3.2142e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 123.9167, GNorm = 0.3089
Meta loss on this task batch = 3.1478e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 123.9215, GNorm = 0.3531
Took 135.96677088737488 seconds to complete one epoch of meta training
Took 144.18857860565186 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450160
Epoch 526
Meta loss on this task batch = 3.0846e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 123.9239, GNorm = 0.2784
Meta loss on this task batch = 2.8221e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 123.9263, GNorm = 0.2610
Meta loss on this task batch = 2.7944e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 123.9286, GNorm = 0.2707
Meta loss on this task batch = 2.2320e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 123.9315, GNorm = 0.2324
Meta loss on this task batch = 3.0167e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 123.9341, GNorm = 0.2509
Meta loss on this task batch = 2.9343e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 123.9361, GNorm = 0.2968
Meta loss on this task batch = 3.2031e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 123.9369, GNorm = 0.2843
Meta loss on this task batch = 2.5941e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 123.9380, GNorm = 0.2439
Meta loss on this task batch = 3.4430e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 123.9403, GNorm = 0.3267
Meta loss on this task batch = 3.1275e-01, Meta loss averaged over last 500 steps = 2.8685e-01, PNorm = 123.9422, GNorm = 0.3113
Meta loss on this task batch = 2.8503e-01, Meta loss averaged over last 500 steps = 2.8681e-01, PNorm = 123.9429, GNorm = 0.3397
Meta loss on this task batch = 2.9278e-01, Meta loss averaged over last 500 steps = 2.8679e-01, PNorm = 123.9457, GNorm = 0.2754
Meta loss on this task batch = 2.5715e-01, Meta loss averaged over last 500 steps = 2.8670e-01, PNorm = 123.9490, GNorm = 0.2317
Meta loss on this task batch = 2.4145e-01, Meta loss averaged over last 500 steps = 2.8667e-01, PNorm = 123.9562, GNorm = 0.2812
Meta loss on this task batch = 2.4976e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 123.9645, GNorm = 0.3058
Meta loss on this task batch = 2.7404e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 123.9718, GNorm = 0.2913
Meta loss on this task batch = 2.8731e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 123.9805, GNorm = 0.3722
Meta loss on this task batch = 1.9901e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 123.9908, GNorm = 0.2561
Meta loss on this task batch = 2.7416e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 124.0018, GNorm = 0.3288
Took 136.78827738761902 seconds to complete one epoch of meta training
Took 144.24909949302673 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470246
Epoch 527
Meta loss on this task batch = 3.1020e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 124.0116, GNorm = 0.3208
Meta loss on this task batch = 2.9502e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 124.0179, GNorm = 0.3055
Meta loss on this task batch = 3.6885e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 124.0204, GNorm = 0.3341
Meta loss on this task batch = 2.5202e-01, Meta loss averaged over last 500 steps = 2.8639e-01, PNorm = 124.0224, GNorm = 0.2505
Meta loss on this task batch = 2.9351e-01, Meta loss averaged over last 500 steps = 2.8634e-01, PNorm = 124.0220, GNorm = 0.3186
Meta loss on this task batch = 2.4440e-01, Meta loss averaged over last 500 steps = 2.8627e-01, PNorm = 124.0204, GNorm = 0.2823
Meta loss on this task batch = 2.5846e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 124.0180, GNorm = 0.3150
Meta loss on this task batch = 2.7712e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 124.0172, GNorm = 0.2697
Meta loss on this task batch = 2.9686e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 124.0178, GNorm = 0.2733
Meta loss on this task batch = 2.2529e-01, Meta loss averaged over last 500 steps = 2.8596e-01, PNorm = 124.0209, GNorm = 0.2735
Meta loss on this task batch = 3.7277e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 124.0234, GNorm = 0.3264
Meta loss on this task batch = 3.0900e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 124.0283, GNorm = 0.3146
Meta loss on this task batch = 2.7636e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 124.0345, GNorm = 0.2843
Meta loss on this task batch = 2.6945e-01, Meta loss averaged over last 500 steps = 2.8626e-01, PNorm = 124.0445, GNorm = 0.3243
Meta loss on this task batch = 2.7798e-01, Meta loss averaged over last 500 steps = 2.8625e-01, PNorm = 124.0546, GNorm = 0.2693
Meta loss on this task batch = 2.8114e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 124.0621, GNorm = 0.3283
Meta loss on this task batch = 3.2550e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 124.0672, GNorm = 0.3129
Meta loss on this task batch = 2.5244e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 124.0705, GNorm = 0.2906
Meta loss on this task batch = 2.8281e-01, Meta loss averaged over last 500 steps = 2.8602e-01, PNorm = 124.0731, GNorm = 0.3232
Took 135.9026939868927 seconds to complete one epoch of meta training
Took 143.81894159317017 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478649
Epoch 528
Meta loss on this task batch = 3.3668e-01, Meta loss averaged over last 500 steps = 2.8616e-01, PNorm = 124.0737, GNorm = 0.2993
Meta loss on this task batch = 2.6308e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 124.0724, GNorm = 0.2596
Meta loss on this task batch = 3.1451e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 124.0698, GNorm = 0.2582
Meta loss on this task batch = 2.7598e-01, Meta loss averaged over last 500 steps = 2.8608e-01, PNorm = 124.0696, GNorm = 0.3012
Meta loss on this task batch = 2.4951e-01, Meta loss averaged over last 500 steps = 2.8602e-01, PNorm = 124.0706, GNorm = 0.2674
Meta loss on this task batch = 3.0186e-01, Meta loss averaged over last 500 steps = 2.8613e-01, PNorm = 124.0736, GNorm = 0.2841
Meta loss on this task batch = 2.4953e-01, Meta loss averaged over last 500 steps = 2.8592e-01, PNorm = 124.0799, GNorm = 0.2717
Meta loss on this task batch = 2.7164e-01, Meta loss averaged over last 500 steps = 2.8576e-01, PNorm = 124.0846, GNorm = 0.2791
Meta loss on this task batch = 2.3553e-01, Meta loss averaged over last 500 steps = 2.8561e-01, PNorm = 124.0904, GNorm = 0.2552
Meta loss on this task batch = 2.9850e-01, Meta loss averaged over last 500 steps = 2.8567e-01, PNorm = 124.0952, GNorm = 0.3240
Meta loss on this task batch = 2.9573e-01, Meta loss averaged over last 500 steps = 2.8564e-01, PNorm = 124.1008, GNorm = 0.3355
Meta loss on this task batch = 2.9651e-01, Meta loss averaged over last 500 steps = 2.8568e-01, PNorm = 124.1071, GNorm = 0.2830
Meta loss on this task batch = 3.2833e-01, Meta loss averaged over last 500 steps = 2.8570e-01, PNorm = 124.1141, GNorm = 0.3620
Meta loss on this task batch = 3.2636e-01, Meta loss averaged over last 500 steps = 2.8579e-01, PNorm = 124.1198, GNorm = 0.3554
Meta loss on this task batch = 3.4439e-01, Meta loss averaged over last 500 steps = 2.8598e-01, PNorm = 124.1262, GNorm = 0.3336
Meta loss on this task batch = 2.7499e-01, Meta loss averaged over last 500 steps = 2.8596e-01, PNorm = 124.1328, GNorm = 0.2653
Meta loss on this task batch = 3.4663e-01, Meta loss averaged over last 500 steps = 2.8600e-01, PNorm = 124.1427, GNorm = 0.4214
Meta loss on this task batch = 2.1672e-01, Meta loss averaged over last 500 steps = 2.8589e-01, PNorm = 124.1539, GNorm = 0.2732
Meta loss on this task batch = 2.7939e-01, Meta loss averaged over last 500 steps = 2.8595e-01, PNorm = 124.1647, GNorm = 0.3693
Took 133.17023825645447 seconds to complete one epoch of meta training
Took 141.67599487304688 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472911
Epoch 529
Meta loss on this task batch = 3.1869e-01, Meta loss averaged over last 500 steps = 2.8601e-01, PNorm = 124.1740, GNorm = 0.3062
Meta loss on this task batch = 2.7119e-01, Meta loss averaged over last 500 steps = 2.8601e-01, PNorm = 124.1845, GNorm = 0.3058
Meta loss on this task batch = 2.4463e-01, Meta loss averaged over last 500 steps = 2.8586e-01, PNorm = 124.1947, GNorm = 0.2453
Meta loss on this task batch = 2.7635e-01, Meta loss averaged over last 500 steps = 2.8594e-01, PNorm = 124.2055, GNorm = 0.2886
Meta loss on this task batch = 3.3746e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 124.2136, GNorm = 0.3362
Meta loss on this task batch = 3.0663e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 124.2202, GNorm = 0.3022
Meta loss on this task batch = 2.7496e-01, Meta loss averaged over last 500 steps = 2.8600e-01, PNorm = 124.2265, GNorm = 0.2770
Meta loss on this task batch = 2.3830e-01, Meta loss averaged over last 500 steps = 2.8600e-01, PNorm = 124.2329, GNorm = 0.3009
Meta loss on this task batch = 3.2799e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 124.2381, GNorm = 0.3119
Meta loss on this task batch = 2.9291e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 124.2433, GNorm = 0.2822
Meta loss on this task batch = 3.1696e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 124.2506, GNorm = 0.3494
Meta loss on this task batch = 2.8617e-01, Meta loss averaged over last 500 steps = 2.8604e-01, PNorm = 124.2591, GNorm = 0.2902
Meta loss on this task batch = 2.6569e-01, Meta loss averaged over last 500 steps = 2.8597e-01, PNorm = 124.2679, GNorm = 0.2567
Meta loss on this task batch = 2.7690e-01, Meta loss averaged over last 500 steps = 2.8598e-01, PNorm = 124.2761, GNorm = 0.2703
Meta loss on this task batch = 2.9587e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 124.2850, GNorm = 0.2783
Meta loss on this task batch = 3.6592e-01, Meta loss averaged over last 500 steps = 2.8618e-01, PNorm = 124.2926, GNorm = 0.3395
Meta loss on this task batch = 2.9177e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 124.2997, GNorm = 0.2994
Meta loss on this task batch = 2.6302e-01, Meta loss averaged over last 500 steps = 2.8624e-01, PNorm = 124.3082, GNorm = 0.2338
Meta loss on this task batch = 3.0538e-01, Meta loss averaged over last 500 steps = 2.8627e-01, PNorm = 124.3173, GNorm = 0.3288
Took 133.6336748600006 seconds to complete one epoch of meta training
Took 142.61497855186462 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473210
Epoch 530
Meta loss on this task batch = 2.8770e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 124.3278, GNorm = 0.2882
Meta loss on this task batch = 2.7895e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 124.3386, GNorm = 0.2559
Meta loss on this task batch = 2.4161e-01, Meta loss averaged over last 500 steps = 2.8600e-01, PNorm = 124.3473, GNorm = 0.2617
Meta loss on this task batch = 3.3486e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 124.3541, GNorm = 0.3187
Meta loss on this task batch = 3.1027e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 124.3594, GNorm = 0.2936
Meta loss on this task batch = 2.8311e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 124.3639, GNorm = 0.2932
Meta loss on this task batch = 3.3654e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 124.3666, GNorm = 0.3271
Meta loss on this task batch = 2.5726e-01, Meta loss averaged over last 500 steps = 2.8613e-01, PNorm = 124.3684, GNorm = 0.3005
Meta loss on this task batch = 3.0510e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 124.3696, GNorm = 0.2939
Meta loss on this task batch = 2.9022e-01, Meta loss averaged over last 500 steps = 2.8613e-01, PNorm = 124.3713, GNorm = 0.3125
Meta loss on this task batch = 2.6637e-01, Meta loss averaged over last 500 steps = 2.8608e-01, PNorm = 124.3734, GNorm = 0.2985
Meta loss on this task batch = 2.5142e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 124.3771, GNorm = 0.2999
Meta loss on this task batch = 3.0314e-01, Meta loss averaged over last 500 steps = 2.8601e-01, PNorm = 124.3809, GNorm = 0.2799
Meta loss on this task batch = 2.5426e-01, Meta loss averaged over last 500 steps = 2.8587e-01, PNorm = 124.3848, GNorm = 0.2820
Meta loss on this task batch = 2.5244e-01, Meta loss averaged over last 500 steps = 2.8578e-01, PNorm = 124.3898, GNorm = 0.3387
Meta loss on this task batch = 3.6293e-01, Meta loss averaged over last 500 steps = 2.8600e-01, PNorm = 124.3930, GNorm = 0.3290
Meta loss on this task batch = 2.8223e-01, Meta loss averaged over last 500 steps = 2.8603e-01, PNorm = 124.3953, GNorm = 0.2862
Meta loss on this task batch = 2.9655e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 124.3969, GNorm = 0.2982
Meta loss on this task batch = 2.6452e-01, Meta loss averaged over last 500 steps = 2.8598e-01, PNorm = 124.3995, GNorm = 0.3536
Took 133.40677690505981 seconds to complete one epoch of meta training
Took 141.9359109401703 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500489
Epoch 531
Meta loss on this task batch = 2.7982e-01, Meta loss averaged over last 500 steps = 2.8586e-01, PNorm = 124.4030, GNorm = 0.2682
Meta loss on this task batch = 2.7548e-01, Meta loss averaged over last 500 steps = 2.8589e-01, PNorm = 124.4065, GNorm = 0.3030
Meta loss on this task batch = 2.6027e-01, Meta loss averaged over last 500 steps = 2.8587e-01, PNorm = 124.4114, GNorm = 0.2503
Meta loss on this task batch = 2.8816e-01, Meta loss averaged over last 500 steps = 2.8596e-01, PNorm = 124.4155, GNorm = 0.2963
Meta loss on this task batch = 2.4741e-01, Meta loss averaged over last 500 steps = 2.8593e-01, PNorm = 124.4209, GNorm = 0.2579
Meta loss on this task batch = 2.3944e-01, Meta loss averaged over last 500 steps = 2.8591e-01, PNorm = 124.4275, GNorm = 0.2835
Meta loss on this task batch = 2.6246e-01, Meta loss averaged over last 500 steps = 2.8588e-01, PNorm = 124.4361, GNorm = 0.2834
Meta loss on this task batch = 2.8144e-01, Meta loss averaged over last 500 steps = 2.8593e-01, PNorm = 124.4418, GNorm = 0.3002
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 2.8592e-01, PNorm = 124.4471, GNorm = 0.2941
Meta loss on this task batch = 2.4770e-01, Meta loss averaged over last 500 steps = 2.8585e-01, PNorm = 124.4519, GNorm = 0.2900
Meta loss on this task batch = 2.7706e-01, Meta loss averaged over last 500 steps = 2.8576e-01, PNorm = 124.4559, GNorm = 0.3260
Meta loss on this task batch = 3.0022e-01, Meta loss averaged over last 500 steps = 2.8577e-01, PNorm = 124.4592, GNorm = 0.2962
Meta loss on this task batch = 2.8594e-01, Meta loss averaged over last 500 steps = 2.8580e-01, PNorm = 124.4609, GNorm = 0.3004
Meta loss on this task batch = 2.9017e-01, Meta loss averaged over last 500 steps = 2.8594e-01, PNorm = 124.4606, GNorm = 0.3513
Meta loss on this task batch = 3.1263e-01, Meta loss averaged over last 500 steps = 2.8596e-01, PNorm = 124.4581, GNorm = 0.3068
Meta loss on this task batch = 3.0669e-01, Meta loss averaged over last 500 steps = 2.8594e-01, PNorm = 124.4564, GNorm = 0.3313
Meta loss on this task batch = 3.1387e-01, Meta loss averaged over last 500 steps = 2.8598e-01, PNorm = 124.4566, GNorm = 0.3088
Meta loss on this task batch = 3.2630e-01, Meta loss averaged over last 500 steps = 2.8604e-01, PNorm = 124.4584, GNorm = 0.3209
Meta loss on this task batch = 3.7028e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 124.4585, GNorm = 0.4116
Took 169.0788037776947 seconds to complete one epoch of meta training
Took 178.19677662849426 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484790
Epoch 532
Meta loss on this task batch = 3.1327e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 124.4601, GNorm = 0.3363
Meta loss on this task batch = 3.1255e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 124.4624, GNorm = 0.2865
Meta loss on this task batch = 3.1444e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 124.4674, GNorm = 0.3068
Meta loss on this task batch = 2.0662e-01, Meta loss averaged over last 500 steps = 2.8637e-01, PNorm = 124.4713, GNorm = 0.2225
Meta loss on this task batch = 2.9790e-01, Meta loss averaged over last 500 steps = 2.8634e-01, PNorm = 124.4764, GNorm = 0.3128
Meta loss on this task batch = 2.9792e-01, Meta loss averaged over last 500 steps = 2.8627e-01, PNorm = 124.4825, GNorm = 0.2878
Meta loss on this task batch = 2.3961e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 124.4901, GNorm = 0.2531
Meta loss on this task batch = 2.3448e-01, Meta loss averaged over last 500 steps = 2.8611e-01, PNorm = 124.4947, GNorm = 0.3004
Meta loss on this task batch = 2.8609e-01, Meta loss averaged over last 500 steps = 2.8608e-01, PNorm = 124.5003, GNorm = 0.3281
Meta loss on this task batch = 3.2384e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 124.5044, GNorm = 0.3015
Meta loss on this task batch = 3.0062e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 124.5103, GNorm = 0.3075
Meta loss on this task batch = 2.6514e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 124.5171, GNorm = 0.2574
Meta loss on this task batch = 2.9615e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 124.5265, GNorm = 0.3166
Meta loss on this task batch = 3.3888e-01, Meta loss averaged over last 500 steps = 2.8624e-01, PNorm = 124.5351, GNorm = 0.3115
Meta loss on this task batch = 2.8037e-01, Meta loss averaged over last 500 steps = 2.8621e-01, PNorm = 124.5450, GNorm = 0.2952
Meta loss on this task batch = 2.4685e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 124.5543, GNorm = 0.2909
Meta loss on this task batch = 3.4777e-01, Meta loss averaged over last 500 steps = 2.8638e-01, PNorm = 124.5608, GNorm = 0.3062
Meta loss on this task batch = 3.3985e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 124.5666, GNorm = 0.2781
Meta loss on this task batch = 2.4411e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 124.5738, GNorm = 0.4275
Took 132.4415237903595 seconds to complete one epoch of meta training
Took 141.03517174720764 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450018
Epoch 533
Meta loss on this task batch = 3.1394e-01, Meta loss averaged over last 500 steps = 2.8643e-01, PNorm = 124.5811, GNorm = 0.3091
Meta loss on this task batch = 2.8898e-01, Meta loss averaged over last 500 steps = 2.8634e-01, PNorm = 124.5897, GNorm = 0.2721
Meta loss on this task batch = 2.5826e-01, Meta loss averaged over last 500 steps = 2.8627e-01, PNorm = 124.5995, GNorm = 0.2887
Meta loss on this task batch = 3.4410e-01, Meta loss averaged over last 500 steps = 2.8631e-01, PNorm = 124.6072, GNorm = 0.3390
Meta loss on this task batch = 2.8819e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 124.6143, GNorm = 0.2973
Meta loss on this task batch = 2.6998e-01, Meta loss averaged over last 500 steps = 2.8629e-01, PNorm = 124.6189, GNorm = 0.2673
Meta loss on this task batch = 2.8753e-01, Meta loss averaged over last 500 steps = 2.8634e-01, PNorm = 124.6227, GNorm = 0.3347
Meta loss on this task batch = 3.0991e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 124.6245, GNorm = 0.2861
Meta loss on this task batch = 2.7023e-01, Meta loss averaged over last 500 steps = 2.8639e-01, PNorm = 124.6273, GNorm = 0.2751
Meta loss on this task batch = 2.8898e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 124.6309, GNorm = 0.2804
Meta loss on this task batch = 2.8666e-01, Meta loss averaged over last 500 steps = 2.8637e-01, PNorm = 124.6339, GNorm = 0.2887
Meta loss on this task batch = 2.6580e-01, Meta loss averaged over last 500 steps = 2.8625e-01, PNorm = 124.6392, GNorm = 0.2735
Meta loss on this task batch = 2.4065e-01, Meta loss averaged over last 500 steps = 2.8611e-01, PNorm = 124.6455, GNorm = 0.2643
Meta loss on this task batch = 2.9935e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 124.6505, GNorm = 0.2625
Meta loss on this task batch = 2.5081e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 124.6552, GNorm = 0.2496
Meta loss on this task batch = 2.5711e-01, Meta loss averaged over last 500 steps = 2.8604e-01, PNorm = 124.6615, GNorm = 0.2923
Meta loss on this task batch = 3.1453e-01, Meta loss averaged over last 500 steps = 2.8608e-01, PNorm = 124.6694, GNorm = 0.3051
Meta loss on this task batch = 2.6556e-01, Meta loss averaged over last 500 steps = 2.8604e-01, PNorm = 124.6777, GNorm = 0.2807
Meta loss on this task batch = 2.9066e-01, Meta loss averaged over last 500 steps = 2.8597e-01, PNorm = 124.6887, GNorm = 0.3971
Took 133.06118369102478 seconds to complete one epoch of meta training
Took 141.64258360862732 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493837
Epoch 534
Meta loss on this task batch = 3.2741e-01, Meta loss averaged over last 500 steps = 2.8604e-01, PNorm = 124.6991, GNorm = 0.2871
Meta loss on this task batch = 3.2901e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 124.7093, GNorm = 0.2916
Meta loss on this task batch = 2.6529e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 124.7186, GNorm = 0.2518
Meta loss on this task batch = 2.3749e-01, Meta loss averaged over last 500 steps = 2.8581e-01, PNorm = 124.7276, GNorm = 0.2765
Meta loss on this task batch = 2.8784e-01, Meta loss averaged over last 500 steps = 2.8595e-01, PNorm = 124.7384, GNorm = 0.2798
Meta loss on this task batch = 3.2037e-01, Meta loss averaged over last 500 steps = 2.8612e-01, PNorm = 124.7487, GNorm = 0.3634
Meta loss on this task batch = 2.7953e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 124.7578, GNorm = 0.3071
Meta loss on this task batch = 2.6887e-01, Meta loss averaged over last 500 steps = 2.8602e-01, PNorm = 124.7652, GNorm = 0.2999
Meta loss on this task batch = 2.4564e-01, Meta loss averaged over last 500 steps = 2.8595e-01, PNorm = 124.7712, GNorm = 0.2688
Meta loss on this task batch = 2.4528e-01, Meta loss averaged over last 500 steps = 2.8604e-01, PNorm = 124.7772, GNorm = 0.3271
Meta loss on this task batch = 3.1653e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 124.7808, GNorm = 0.3529
Meta loss on this task batch = 2.4460e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 124.7839, GNorm = 0.2786
Meta loss on this task batch = 3.7856e-01, Meta loss averaged over last 500 steps = 2.8616e-01, PNorm = 124.7856, GNorm = 0.3653
Meta loss on this task batch = 2.4224e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 124.7879, GNorm = 0.2517
Meta loss on this task batch = 3.2201e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 124.7896, GNorm = 0.3320
Meta loss on this task batch = 2.9386e-01, Meta loss averaged over last 500 steps = 2.8625e-01, PNorm = 124.7913, GNorm = 0.3151
Meta loss on this task batch = 2.4254e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 124.7923, GNorm = 0.2720
Meta loss on this task batch = 2.8257e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 124.7939, GNorm = 0.2972
Meta loss on this task batch = 3.2764e-01, Meta loss averaged over last 500 steps = 2.8621e-01, PNorm = 124.7966, GNorm = 0.3792
Took 131.97781467437744 seconds to complete one epoch of meta training
Took 141.00289058685303 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478564
Epoch 535
Meta loss on this task batch = 3.0522e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 124.8014, GNorm = 0.3045
Meta loss on this task batch = 2.5266e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 124.8085, GNorm = 0.2789
Meta loss on this task batch = 3.0109e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 124.8159, GNorm = 0.3670
Meta loss on this task batch = 2.9339e-01, Meta loss averaged over last 500 steps = 2.8618e-01, PNorm = 124.8258, GNorm = 0.2780
Meta loss on this task batch = 3.1008e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 124.8351, GNorm = 0.3162
Meta loss on this task batch = 2.9222e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 124.8434, GNorm = 0.2979
Meta loss on this task batch = 2.9053e-01, Meta loss averaged over last 500 steps = 2.8616e-01, PNorm = 124.8518, GNorm = 0.3240
Meta loss on this task batch = 2.7360e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 124.8604, GNorm = 0.2808
Meta loss on this task batch = 3.0091e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 124.8674, GNorm = 0.3005
Meta loss on this task batch = 2.7887e-01, Meta loss averaged over last 500 steps = 2.8633e-01, PNorm = 124.8743, GNorm = 0.3092
Meta loss on this task batch = 2.7661e-01, Meta loss averaged over last 500 steps = 2.8633e-01, PNorm = 124.8814, GNorm = 0.2884
Meta loss on this task batch = 2.4611e-01, Meta loss averaged over last 500 steps = 2.8621e-01, PNorm = 124.8881, GNorm = 0.2483
Meta loss on this task batch = 3.2823e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 124.8944, GNorm = 0.3182
Meta loss on this task batch = 2.4512e-01, Meta loss averaged over last 500 steps = 2.8631e-01, PNorm = 124.9004, GNorm = 0.2391
Meta loss on this task batch = 3.0034e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 124.9040, GNorm = 0.3392
Meta loss on this task batch = 2.8036e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 124.9077, GNorm = 0.2898
Meta loss on this task batch = 2.9708e-01, Meta loss averaged over last 500 steps = 2.8626e-01, PNorm = 124.9083, GNorm = 0.3063
Meta loss on this task batch = 2.6746e-01, Meta loss averaged over last 500 steps = 2.8618e-01, PNorm = 124.9070, GNorm = 0.3103
Meta loss on this task batch = 2.4807e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 124.9072, GNorm = 0.2782
Took 135.02023434638977 seconds to complete one epoch of meta training
Took 142.7397220134735 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473659
Epoch 536
Meta loss on this task batch = 2.3482e-01, Meta loss averaged over last 500 steps = 2.8601e-01, PNorm = 124.9077, GNorm = 0.2357
Meta loss on this task batch = 2.6936e-01, Meta loss averaged over last 500 steps = 2.8594e-01, PNorm = 124.9093, GNorm = 0.2681
Meta loss on this task batch = 2.9635e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 124.9123, GNorm = 0.3072
Meta loss on this task batch = 2.9248e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 124.9152, GNorm = 0.2938
Meta loss on this task batch = 2.4908e-01, Meta loss averaged over last 500 steps = 2.8603e-01, PNorm = 124.9180, GNorm = 0.2876
Meta loss on this task batch = 3.3527e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 124.9206, GNorm = 0.2937
Meta loss on this task batch = 2.3121e-01, Meta loss averaged over last 500 steps = 2.8596e-01, PNorm = 124.9231, GNorm = 0.3123
Meta loss on this task batch = 3.2854e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 124.9244, GNorm = 0.3204
Meta loss on this task batch = 2.8852e-01, Meta loss averaged over last 500 steps = 2.8612e-01, PNorm = 124.9265, GNorm = 0.3276
Meta loss on this task batch = 2.9190e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 124.9289, GNorm = 0.2751
Meta loss on this task batch = 3.1963e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 124.9339, GNorm = 0.3043
Meta loss on this task batch = 2.7309e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 124.9401, GNorm = 0.2885
Meta loss on this task batch = 3.2163e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 124.9466, GNorm = 0.2984
Meta loss on this task batch = 3.1389e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 124.9528, GNorm = 0.3107
Meta loss on this task batch = 2.7046e-01, Meta loss averaged over last 500 steps = 2.8618e-01, PNorm = 124.9582, GNorm = 0.2889
Meta loss on this task batch = 3.3276e-01, Meta loss averaged over last 500 steps = 2.8624e-01, PNorm = 124.9625, GNorm = 0.2875
Meta loss on this task batch = 3.4568e-01, Meta loss averaged over last 500 steps = 2.8640e-01, PNorm = 124.9661, GNorm = 0.3249
Meta loss on this task batch = 3.0270e-01, Meta loss averaged over last 500 steps = 2.8644e-01, PNorm = 124.9699, GNorm = 0.2901
Meta loss on this task batch = 2.4398e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 124.9747, GNorm = 0.3154
Took 140.04014420509338 seconds to complete one epoch of meta training
Took 148.62891030311584 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487266
Epoch 537
Meta loss on this task batch = 2.5535e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 124.9804, GNorm = 0.2575
Meta loss on this task batch = 2.8452e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 124.9875, GNorm = 0.3058
Meta loss on this task batch = 3.0066e-01, Meta loss averaged over last 500 steps = 2.8629e-01, PNorm = 124.9928, GNorm = 0.2937
Meta loss on this task batch = 2.7562e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 124.9993, GNorm = 0.2901
Meta loss on this task batch = 2.4368e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 125.0056, GNorm = 0.2603
Meta loss on this task batch = 3.0401e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 125.0099, GNorm = 0.3502
Meta loss on this task batch = 3.1184e-01, Meta loss averaged over last 500 steps = 2.8637e-01, PNorm = 125.0146, GNorm = 0.3131
Meta loss on this task batch = 2.5275e-01, Meta loss averaged over last 500 steps = 2.8632e-01, PNorm = 125.0185, GNorm = 0.3098
Meta loss on this task batch = 2.6950e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 125.0212, GNorm = 0.3372
Meta loss on this task batch = 2.9217e-01, Meta loss averaged over last 500 steps = 2.8616e-01, PNorm = 125.0235, GNorm = 0.2954
Meta loss on this task batch = 2.9421e-01, Meta loss averaged over last 500 steps = 2.8621e-01, PNorm = 125.0260, GNorm = 0.2667
Meta loss on this task batch = 3.1442e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 125.0304, GNorm = 0.2963
Meta loss on this task batch = 2.7561e-01, Meta loss averaged over last 500 steps = 2.8625e-01, PNorm = 125.0364, GNorm = 0.2689
Meta loss on this task batch = 2.7497e-01, Meta loss averaged over last 500 steps = 2.8630e-01, PNorm = 125.0428, GNorm = 0.2908
Meta loss on this task batch = 2.5882e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 125.0498, GNorm = 0.2641
Meta loss on this task batch = 2.6537e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 125.0550, GNorm = 0.2674
Meta loss on this task batch = 3.0198e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 125.0587, GNorm = 0.3373
Meta loss on this task batch = 2.5391e-01, Meta loss averaged over last 500 steps = 2.8608e-01, PNorm = 125.0619, GNorm = 0.2566
Meta loss on this task batch = 2.7617e-01, Meta loss averaged over last 500 steps = 2.8598e-01, PNorm = 125.0663, GNorm = 0.3068
Took 143.16834712028503 seconds to complete one epoch of meta training
Took 150.95954155921936 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470403
Epoch 538
Meta loss on this task batch = 3.0972e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 125.0712, GNorm = 0.3118
Meta loss on this task batch = 2.2256e-01, Meta loss averaged over last 500 steps = 2.8604e-01, PNorm = 125.0752, GNorm = 0.2478
Meta loss on this task batch = 2.5235e-01, Meta loss averaged over last 500 steps = 2.8599e-01, PNorm = 125.0775, GNorm = 0.3086
Meta loss on this task batch = 3.1616e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 125.0809, GNorm = 0.3098
Meta loss on this task batch = 2.3702e-01, Meta loss averaged over last 500 steps = 2.8595e-01, PNorm = 125.0833, GNorm = 0.2616
Meta loss on this task batch = 3.2238e-01, Meta loss averaged over last 500 steps = 2.8601e-01, PNorm = 125.0871, GNorm = 0.3434
Meta loss on this task batch = 3.1020e-01, Meta loss averaged over last 500 steps = 2.8603e-01, PNorm = 125.0912, GNorm = 0.2883
Meta loss on this task batch = 2.5247e-01, Meta loss averaged over last 500 steps = 2.8590e-01, PNorm = 125.0948, GNorm = 0.2640
Meta loss on this task batch = 2.9064e-01, Meta loss averaged over last 500 steps = 2.8595e-01, PNorm = 125.1002, GNorm = 0.3488
Meta loss on this task batch = 2.9456e-01, Meta loss averaged over last 500 steps = 2.8594e-01, PNorm = 125.1036, GNorm = 0.3049
Meta loss on this task batch = 2.9690e-01, Meta loss averaged over last 500 steps = 2.8592e-01, PNorm = 125.1060, GNorm = 0.3063
Meta loss on this task batch = 2.5628e-01, Meta loss averaged over last 500 steps = 2.8587e-01, PNorm = 125.1089, GNorm = 0.2687
Meta loss on this task batch = 3.1059e-01, Meta loss averaged over last 500 steps = 2.8588e-01, PNorm = 125.1147, GNorm = 0.3142
Meta loss on this task batch = 2.7777e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 125.1214, GNorm = 0.3101
Meta loss on this task batch = 2.9452e-01, Meta loss averaged over last 500 steps = 2.8599e-01, PNorm = 125.1276, GNorm = 0.2927
Meta loss on this task batch = 2.8127e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 125.1326, GNorm = 0.2957
Meta loss on this task batch = 2.6389e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 125.1372, GNorm = 0.2745
Meta loss on this task batch = 2.6217e-01, Meta loss averaged over last 500 steps = 2.8603e-01, PNorm = 125.1419, GNorm = 0.3040
Meta loss on this task batch = 3.2351e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 125.1471, GNorm = 0.3559
Took 157.3921456336975 seconds to complete one epoch of meta training
Took 169.71782636642456 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488331
Epoch 539
Meta loss on this task batch = 2.7138e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 125.1543, GNorm = 0.2970
Meta loss on this task batch = 2.8045e-01, Meta loss averaged over last 500 steps = 2.8612e-01, PNorm = 125.1594, GNorm = 0.3002
Meta loss on this task batch = 2.4853e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 125.1665, GNorm = 0.2727
Meta loss on this task batch = 2.3666e-01, Meta loss averaged over last 500 steps = 2.8600e-01, PNorm = 125.1719, GNorm = 0.2512
Meta loss on this task batch = 2.5806e-01, Meta loss averaged over last 500 steps = 2.8594e-01, PNorm = 125.1753, GNorm = 0.2896
Meta loss on this task batch = 2.6672e-01, Meta loss averaged over last 500 steps = 2.8593e-01, PNorm = 125.1771, GNorm = 0.2849
Meta loss on this task batch = 3.0351e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 125.1798, GNorm = 0.3167
Meta loss on this task batch = 3.2165e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 125.1823, GNorm = 0.2883
Meta loss on this task batch = 2.7989e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 125.1843, GNorm = 0.3007
Meta loss on this task batch = 3.0973e-01, Meta loss averaged over last 500 steps = 2.8635e-01, PNorm = 125.1870, GNorm = 0.2954
Meta loss on this task batch = 2.7463e-01, Meta loss averaged over last 500 steps = 2.8636e-01, PNorm = 125.1900, GNorm = 0.2686
Meta loss on this task batch = 3.1552e-01, Meta loss averaged over last 500 steps = 2.8623e-01, PNorm = 125.1935, GNorm = 0.3116
Meta loss on this task batch = 3.1603e-01, Meta loss averaged over last 500 steps = 2.8626e-01, PNorm = 125.1961, GNorm = 0.3471
Meta loss on this task batch = 2.5602e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 125.2002, GNorm = 0.2710
Meta loss on this task batch = 2.7088e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 125.2029, GNorm = 0.2691
Meta loss on this task batch = 2.5311e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 125.2036, GNorm = 0.2682
Meta loss on this task batch = 3.4241e-01, Meta loss averaged over last 500 steps = 2.8625e-01, PNorm = 125.2025, GNorm = 0.2848
Meta loss on this task batch = 3.2008e-01, Meta loss averaged over last 500 steps = 2.8631e-01, PNorm = 125.2032, GNorm = 0.3025
Meta loss on this task batch = 2.6526e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 125.2051, GNorm = 0.3108
Took 160.00689959526062 seconds to complete one epoch of meta training
Took 171.35110521316528 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505590
Epoch 540
Meta loss on this task batch = 3.0591e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 125.2065, GNorm = 0.3322
Meta loss on this task batch = 3.0016e-01, Meta loss averaged over last 500 steps = 2.8620e-01, PNorm = 125.2109, GNorm = 0.3030
Meta loss on this task batch = 3.0939e-01, Meta loss averaged over last 500 steps = 2.8610e-01, PNorm = 125.2151, GNorm = 0.2861
Meta loss on this task batch = 2.6975e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 125.2211, GNorm = 0.2929
Meta loss on this task batch = 3.1715e-01, Meta loss averaged over last 500 steps = 2.8611e-01, PNorm = 125.2284, GNorm = 0.3031
Meta loss on this task batch = 2.8846e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 125.2350, GNorm = 0.2612
Meta loss on this task batch = 3.4101e-01, Meta loss averaged over last 500 steps = 2.8621e-01, PNorm = 125.2378, GNorm = 0.3767
Meta loss on this task batch = 2.3961e-01, Meta loss averaged over last 500 steps = 2.8617e-01, PNorm = 125.2413, GNorm = 0.2701
Meta loss on this task batch = 2.9960e-01, Meta loss averaged over last 500 steps = 2.8614e-01, PNorm = 125.2444, GNorm = 0.3243
Meta loss on this task batch = 2.8097e-01, Meta loss averaged over last 500 steps = 2.8613e-01, PNorm = 125.2466, GNorm = 0.2807
Meta loss on this task batch = 3.2140e-01, Meta loss averaged over last 500 steps = 2.8624e-01, PNorm = 125.2467, GNorm = 0.2917
Meta loss on this task batch = 2.8102e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 125.2456, GNorm = 0.2727
Meta loss on this task batch = 2.3362e-01, Meta loss averaged over last 500 steps = 2.8615e-01, PNorm = 125.2447, GNorm = 0.2577
Meta loss on this task batch = 2.6335e-01, Meta loss averaged over last 500 steps = 2.8602e-01, PNorm = 125.2447, GNorm = 0.2636
Meta loss on this task batch = 2.7829e-01, Meta loss averaged over last 500 steps = 2.8593e-01, PNorm = 125.2449, GNorm = 0.2788
Meta loss on this task batch = 3.2045e-01, Meta loss averaged over last 500 steps = 2.8597e-01, PNorm = 125.2453, GNorm = 0.3251
Meta loss on this task batch = 2.5293e-01, Meta loss averaged over last 500 steps = 2.8599e-01, PNorm = 125.2469, GNorm = 0.2559
Meta loss on this task batch = 2.9790e-01, Meta loss averaged over last 500 steps = 2.8593e-01, PNorm = 125.2477, GNorm = 0.2941
Meta loss on this task batch = 2.6695e-01, Meta loss averaged over last 500 steps = 2.8589e-01, PNorm = 125.2476, GNorm = 0.3631
Took 164.52188348770142 seconds to complete one epoch of meta training
Took 175.9434666633606 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480032
Epoch 541
Meta loss on this task batch = 2.9863e-01, Meta loss averaged over last 500 steps = 2.8588e-01, PNorm = 125.2493, GNorm = 0.3084
Meta loss on this task batch = 3.0016e-01, Meta loss averaged over last 500 steps = 2.8590e-01, PNorm = 125.2510, GNorm = 0.2887
Meta loss on this task batch = 2.9110e-01, Meta loss averaged over last 500 steps = 2.8578e-01, PNorm = 125.2537, GNorm = 0.2897
Meta loss on this task batch = 3.3824e-01, Meta loss averaged over last 500 steps = 2.8598e-01, PNorm = 125.2574, GNorm = 0.2994
Meta loss on this task batch = 2.5952e-01, Meta loss averaged over last 500 steps = 2.8600e-01, PNorm = 125.2617, GNorm = 0.2813
Meta loss on this task batch = 2.4146e-01, Meta loss averaged over last 500 steps = 2.8595e-01, PNorm = 125.2656, GNorm = 0.2865
Meta loss on this task batch = 2.5292e-01, Meta loss averaged over last 500 steps = 2.8592e-01, PNorm = 125.2713, GNorm = 0.2916
Meta loss on this task batch = 2.9354e-01, Meta loss averaged over last 500 steps = 2.8594e-01, PNorm = 125.2767, GNorm = 0.3189
Meta loss on this task batch = 2.5285e-01, Meta loss averaged over last 500 steps = 2.8587e-01, PNorm = 125.2821, GNorm = 0.3141
Meta loss on this task batch = 2.5359e-01, Meta loss averaged over last 500 steps = 2.8582e-01, PNorm = 125.2867, GNorm = 0.2642
Meta loss on this task batch = 3.0171e-01, Meta loss averaged over last 500 steps = 2.8599e-01, PNorm = 125.2902, GNorm = 0.3310
Meta loss on this task batch = 3.2718e-01, Meta loss averaged over last 500 steps = 2.8602e-01, PNorm = 125.2941, GNorm = 0.2954
Meta loss on this task batch = 2.9994e-01, Meta loss averaged over last 500 steps = 2.8619e-01, PNorm = 125.2976, GNorm = 0.2930
Meta loss on this task batch = 2.3481e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 125.3007, GNorm = 0.2477
Meta loss on this task batch = 3.0131e-01, Meta loss averaged over last 500 steps = 2.8613e-01, PNorm = 125.3013, GNorm = 0.3057
Meta loss on this task batch = 2.5689e-01, Meta loss averaged over last 500 steps = 2.8612e-01, PNorm = 125.3029, GNorm = 0.3128
Meta loss on this task batch = 2.7493e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 125.3053, GNorm = 0.3020
Meta loss on this task batch = 3.0842e-01, Meta loss averaged over last 500 steps = 2.8609e-01, PNorm = 125.3080, GNorm = 0.3017
Meta loss on this task batch = 2.3116e-01, Meta loss averaged over last 500 steps = 2.8600e-01, PNorm = 125.3110, GNorm = 0.2965
Took 166.58161568641663 seconds to complete one epoch of meta training
Took 176.851331949234 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477208
Epoch 542
Meta loss on this task batch = 3.1247e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 125.3143, GNorm = 0.2781
Meta loss on this task batch = 2.3187e-01, Meta loss averaged over last 500 steps = 2.8589e-01, PNorm = 125.3214, GNorm = 0.2493
Meta loss on this task batch = 2.8869e-01, Meta loss averaged over last 500 steps = 2.8584e-01, PNorm = 125.3289, GNorm = 0.2811
Meta loss on this task batch = 2.5494e-01, Meta loss averaged over last 500 steps = 2.8574e-01, PNorm = 125.3362, GNorm = 0.2699
Meta loss on this task batch = 2.5481e-01, Meta loss averaged over last 500 steps = 2.8568e-01, PNorm = 125.3439, GNorm = 0.2651
Meta loss on this task batch = 2.7735e-01, Meta loss averaged over last 500 steps = 2.8560e-01, PNorm = 125.3525, GNorm = 0.3153
Meta loss on this task batch = 2.4712e-01, Meta loss averaged over last 500 steps = 2.8546e-01, PNorm = 125.3593, GNorm = 0.2946
Meta loss on this task batch = 2.3279e-01, Meta loss averaged over last 500 steps = 2.8545e-01, PNorm = 125.3665, GNorm = 0.2908
Meta loss on this task batch = 2.6797e-01, Meta loss averaged over last 500 steps = 2.8550e-01, PNorm = 125.3728, GNorm = 0.2905
Meta loss on this task batch = 3.0210e-01, Meta loss averaged over last 500 steps = 2.8558e-01, PNorm = 125.3787, GNorm = 0.2733
Meta loss on this task batch = 3.2289e-01, Meta loss averaged over last 500 steps = 2.8557e-01, PNorm = 125.3834, GNorm = 0.3075
Meta loss on this task batch = 3.4008e-01, Meta loss averaged over last 500 steps = 2.8567e-01, PNorm = 125.3858, GNorm = 0.3600
Meta loss on this task batch = 2.4909e-01, Meta loss averaged over last 500 steps = 2.8559e-01, PNorm = 125.3890, GNorm = 0.3248
Meta loss on this task batch = 3.5867e-01, Meta loss averaged over last 500 steps = 2.8574e-01, PNorm = 125.3913, GNorm = 0.4446
Meta loss on this task batch = 3.3865e-01, Meta loss averaged over last 500 steps = 2.8581e-01, PNorm = 125.3932, GNorm = 0.3772
Meta loss on this task batch = 3.1091e-01, Meta loss averaged over last 500 steps = 2.8585e-01, PNorm = 125.3951, GNorm = 0.3568
Meta loss on this task batch = 2.6860e-01, Meta loss averaged over last 500 steps = 2.8587e-01, PNorm = 125.3982, GNorm = 0.2810
Meta loss on this task batch = 2.8333e-01, Meta loss averaged over last 500 steps = 2.8577e-01, PNorm = 125.4024, GNorm = 0.3176
Meta loss on this task batch = 2.4416e-01, Meta loss averaged over last 500 steps = 2.8564e-01, PNorm = 125.4081, GNorm = 0.3532
Took 164.58307886123657 seconds to complete one epoch of meta training
Took 175.72703313827515 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485448
Epoch 543
Meta loss on this task batch = 2.8520e-01, Meta loss averaged over last 500 steps = 2.8564e-01, PNorm = 125.4142, GNorm = 0.2843
Meta loss on this task batch = 2.9669e-01, Meta loss averaged over last 500 steps = 2.8560e-01, PNorm = 125.4210, GNorm = 0.2696
Meta loss on this task batch = 2.8729e-01, Meta loss averaged over last 500 steps = 2.8562e-01, PNorm = 125.4282, GNorm = 0.3216
Meta loss on this task batch = 2.2334e-01, Meta loss averaged over last 500 steps = 2.8548e-01, PNorm = 125.4364, GNorm = 0.2668
Meta loss on this task batch = 2.4889e-01, Meta loss averaged over last 500 steps = 2.8533e-01, PNorm = 125.4450, GNorm = 0.2628
Meta loss on this task batch = 2.7886e-01, Meta loss averaged over last 500 steps = 2.8535e-01, PNorm = 125.4523, GNorm = 0.2697
Meta loss on this task batch = 3.1807e-01, Meta loss averaged over last 500 steps = 2.8552e-01, PNorm = 125.4589, GNorm = 0.3143
Meta loss on this task batch = 2.8161e-01, Meta loss averaged over last 500 steps = 2.8546e-01, PNorm = 125.4645, GNorm = 0.3014
Meta loss on this task batch = 2.7900e-01, Meta loss averaged over last 500 steps = 2.8549e-01, PNorm = 125.4697, GNorm = 0.2748
Meta loss on this task batch = 2.5485e-01, Meta loss averaged over last 500 steps = 2.8546e-01, PNorm = 125.4754, GNorm = 0.2470
Meta loss on this task batch = 3.0802e-01, Meta loss averaged over last 500 steps = 2.8546e-01, PNorm = 125.4812, GNorm = 0.3487
Meta loss on this task batch = 2.7290e-01, Meta loss averaged over last 500 steps = 2.8545e-01, PNorm = 125.4863, GNorm = 0.3248
Meta loss on this task batch = 2.9710e-01, Meta loss averaged over last 500 steps = 2.8550e-01, PNorm = 125.4926, GNorm = 0.2853
Meta loss on this task batch = 2.9888e-01, Meta loss averaged over last 500 steps = 2.8555e-01, PNorm = 125.4955, GNorm = 0.3303
Meta loss on this task batch = 3.1378e-01, Meta loss averaged over last 500 steps = 2.8560e-01, PNorm = 125.4988, GNorm = 0.3181
Meta loss on this task batch = 2.7254e-01, Meta loss averaged over last 500 steps = 2.8565e-01, PNorm = 125.5043, GNorm = 0.2853
Meta loss on this task batch = 2.6886e-01, Meta loss averaged over last 500 steps = 2.8553e-01, PNorm = 125.5108, GNorm = 0.2736
Meta loss on this task batch = 3.1120e-01, Meta loss averaged over last 500 steps = 2.8564e-01, PNorm = 125.5146, GNorm = 0.3240
Meta loss on this task batch = 3.3280e-01, Meta loss averaged over last 500 steps = 2.8577e-01, PNorm = 125.5210, GNorm = 0.3848
Took 159.79698085784912 seconds to complete one epoch of meta training
Took 170.34790015220642 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490034
Epoch 544
Meta loss on this task batch = 2.4554e-01, Meta loss averaged over last 500 steps = 2.8556e-01, PNorm = 125.5299, GNorm = 0.2837
Meta loss on this task batch = 2.4330e-01, Meta loss averaged over last 500 steps = 2.8548e-01, PNorm = 125.5406, GNorm = 0.2735
Meta loss on this task batch = 2.3886e-01, Meta loss averaged over last 500 steps = 2.8534e-01, PNorm = 125.5520, GNorm = 0.2483
Meta loss on this task batch = 2.5423e-01, Meta loss averaged over last 500 steps = 2.8532e-01, PNorm = 125.5631, GNorm = 0.2667
Meta loss on this task batch = 2.9271e-01, Meta loss averaged over last 500 steps = 2.8527e-01, PNorm = 125.5711, GNorm = 0.3728
Meta loss on this task batch = 2.5979e-01, Meta loss averaged over last 500 steps = 2.8526e-01, PNorm = 125.5784, GNorm = 0.2428
Meta loss on this task batch = 3.0372e-01, Meta loss averaged over last 500 steps = 2.8536e-01, PNorm = 125.5845, GNorm = 0.3008
Meta loss on this task batch = 2.7533e-01, Meta loss averaged over last 500 steps = 2.8535e-01, PNorm = 125.5890, GNorm = 0.2823
Meta loss on this task batch = 3.1644e-01, Meta loss averaged over last 500 steps = 2.8546e-01, PNorm = 125.5918, GNorm = 0.3268
Meta loss on this task batch = 2.3583e-01, Meta loss averaged over last 500 steps = 2.8532e-01, PNorm = 125.5930, GNorm = 0.2609
Meta loss on this task batch = 3.1377e-01, Meta loss averaged over last 500 steps = 2.8550e-01, PNorm = 125.5948, GNorm = 0.2661
Meta loss on this task batch = 2.7738e-01, Meta loss averaged over last 500 steps = 2.8553e-01, PNorm = 125.5979, GNorm = 0.2833
Meta loss on this task batch = 2.6375e-01, Meta loss averaged over last 500 steps = 2.8549e-01, PNorm = 125.6048, GNorm = 0.3286
Meta loss on this task batch = 3.5879e-01, Meta loss averaged over last 500 steps = 2.8565e-01, PNorm = 125.6093, GNorm = 0.3678
Meta loss on this task batch = 2.6612e-01, Meta loss averaged over last 500 steps = 2.8562e-01, PNorm = 125.6157, GNorm = 0.2772
Meta loss on this task batch = 3.3645e-01, Meta loss averaged over last 500 steps = 2.8573e-01, PNorm = 125.6229, GNorm = 0.3126
Meta loss on this task batch = 2.4918e-01, Meta loss averaged over last 500 steps = 2.8569e-01, PNorm = 125.6331, GNorm = 0.3080
Meta loss on this task batch = 2.8948e-01, Meta loss averaged over last 500 steps = 2.8561e-01, PNorm = 125.6436, GNorm = 0.3165
Meta loss on this task batch = 3.1456e-01, Meta loss averaged over last 500 steps = 2.8553e-01, PNorm = 125.6547, GNorm = 0.3876
Took 159.68114376068115 seconds to complete one epoch of meta training
Took 169.20351028442383 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494788
Epoch 545
Meta loss on this task batch = 2.7598e-01, Meta loss averaged over last 500 steps = 2.8544e-01, PNorm = 125.6659, GNorm = 0.2741
Meta loss on this task batch = 3.0625e-01, Meta loss averaged over last 500 steps = 2.8556e-01, PNorm = 125.6769, GNorm = 0.3131
Meta loss on this task batch = 2.7112e-01, Meta loss averaged over last 500 steps = 2.8553e-01, PNorm = 125.6868, GNorm = 0.3017
Meta loss on this task batch = 2.7940e-01, Meta loss averaged over last 500 steps = 2.8545e-01, PNorm = 125.6965, GNorm = 0.2728
Meta loss on this task batch = 2.9586e-01, Meta loss averaged over last 500 steps = 2.8546e-01, PNorm = 125.7034, GNorm = 0.3267
Meta loss on this task batch = 3.1328e-01, Meta loss averaged over last 500 steps = 2.8555e-01, PNorm = 125.7072, GNorm = 0.3469
Meta loss on this task batch = 3.0377e-01, Meta loss averaged over last 500 steps = 2.8572e-01, PNorm = 125.7080, GNorm = 0.3169
Meta loss on this task batch = 2.6899e-01, Meta loss averaged over last 500 steps = 2.8564e-01, PNorm = 125.7072, GNorm = 0.3056
Meta loss on this task batch = 3.0114e-01, Meta loss averaged over last 500 steps = 2.8563e-01, PNorm = 125.7070, GNorm = 0.3277
Meta loss on this task batch = 2.4959e-01, Meta loss averaged over last 500 steps = 2.8563e-01, PNorm = 125.7076, GNorm = 0.2693
Meta loss on this task batch = 2.2246e-01, Meta loss averaged over last 500 steps = 2.8550e-01, PNorm = 125.7100, GNorm = 0.2632
Meta loss on this task batch = 2.3996e-01, Meta loss averaged over last 500 steps = 2.8549e-01, PNorm = 125.7148, GNorm = 0.2819
Meta loss on this task batch = 2.7351e-01, Meta loss averaged over last 500 steps = 2.8540e-01, PNorm = 125.7164, GNorm = 0.3271
Meta loss on this task batch = 3.0413e-01, Meta loss averaged over last 500 steps = 2.8553e-01, PNorm = 125.7195, GNorm = 0.2911
Meta loss on this task batch = 2.3261e-01, Meta loss averaged over last 500 steps = 2.8538e-01, PNorm = 125.7239, GNorm = 0.2418
Meta loss on this task batch = 2.6542e-01, Meta loss averaged over last 500 steps = 2.8528e-01, PNorm = 125.7303, GNorm = 0.3316
Meta loss on this task batch = 3.1482e-01, Meta loss averaged over last 500 steps = 2.8534e-01, PNorm = 125.7379, GNorm = 0.3167
Meta loss on this task batch = 2.9589e-01, Meta loss averaged over last 500 steps = 2.8542e-01, PNorm = 125.7443, GNorm = 0.3090
Meta loss on this task batch = 3.1341e-01, Meta loss averaged over last 500 steps = 2.8551e-01, PNorm = 125.7503, GNorm = 0.3407
Took 160.7429234981537 seconds to complete one epoch of meta training
Took 171.85154223442078 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488150
Epoch 546
Meta loss on this task batch = 2.9793e-01, Meta loss averaged over last 500 steps = 2.8550e-01, PNorm = 125.7562, GNorm = 0.2888
Meta loss on this task batch = 3.0566e-01, Meta loss averaged over last 500 steps = 2.8557e-01, PNorm = 125.7611, GNorm = 0.2902
Meta loss on this task batch = 2.7323e-01, Meta loss averaged over last 500 steps = 2.8559e-01, PNorm = 125.7670, GNorm = 0.2873
Meta loss on this task batch = 3.0052e-01, Meta loss averaged over last 500 steps = 2.8565e-01, PNorm = 125.7737, GNorm = 0.3003
Meta loss on this task batch = 2.7231e-01, Meta loss averaged over last 500 steps = 2.8569e-01, PNorm = 125.7769, GNorm = 0.2871
Meta loss on this task batch = 2.4772e-01, Meta loss averaged over last 500 steps = 2.8556e-01, PNorm = 125.7782, GNorm = 0.2527
Meta loss on this task batch = 2.8998e-01, Meta loss averaged over last 500 steps = 2.8560e-01, PNorm = 125.7779, GNorm = 0.2552
Meta loss on this task batch = 2.1241e-01, Meta loss averaged over last 500 steps = 2.8550e-01, PNorm = 125.7779, GNorm = 0.2194
Meta loss on this task batch = 2.6013e-01, Meta loss averaged over last 500 steps = 2.8550e-01, PNorm = 125.7797, GNorm = 0.2765
Meta loss on this task batch = 2.7877e-01, Meta loss averaged over last 500 steps = 2.8547e-01, PNorm = 125.7807, GNorm = 0.2895
Meta loss on this task batch = 3.1275e-01, Meta loss averaged over last 500 steps = 2.8554e-01, PNorm = 125.7815, GNorm = 0.3026
Meta loss on this task batch = 2.7785e-01, Meta loss averaged over last 500 steps = 2.8545e-01, PNorm = 125.7827, GNorm = 0.3006
Meta loss on this task batch = 2.7712e-01, Meta loss averaged over last 500 steps = 2.8539e-01, PNorm = 125.7866, GNorm = 0.2960
Meta loss on this task batch = 2.6564e-01, Meta loss averaged over last 500 steps = 2.8536e-01, PNorm = 125.7911, GNorm = 0.2782
Meta loss on this task batch = 3.0944e-01, Meta loss averaged over last 500 steps = 2.8540e-01, PNorm = 125.7964, GNorm = 0.3299
Meta loss on this task batch = 2.6135e-01, Meta loss averaged over last 500 steps = 2.8542e-01, PNorm = 125.8032, GNorm = 0.2772
Meta loss on this task batch = 2.9370e-01, Meta loss averaged over last 500 steps = 2.8543e-01, PNorm = 125.8124, GNorm = 0.2849
Meta loss on this task batch = 3.1586e-01, Meta loss averaged over last 500 steps = 2.8560e-01, PNorm = 125.8194, GNorm = 0.3470
Meta loss on this task batch = 3.1923e-01, Meta loss averaged over last 500 steps = 2.8562e-01, PNorm = 125.8225, GNorm = 0.3747
Took 155.62926292419434 seconds to complete one epoch of meta training
Took 167.00662803649902 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.516346
Epoch 547
Meta loss on this task batch = 2.2491e-01, Meta loss averaged over last 500 steps = 2.8551e-01, PNorm = 125.8264, GNorm = 0.2706
Meta loss on this task batch = 3.0174e-01, Meta loss averaged over last 500 steps = 2.8538e-01, PNorm = 125.8304, GNorm = 0.3176
Meta loss on this task batch = 2.6043e-01, Meta loss averaged over last 500 steps = 2.8530e-01, PNorm = 125.8353, GNorm = 0.2532
Meta loss on this task batch = 2.8038e-01, Meta loss averaged over last 500 steps = 2.8536e-01, PNorm = 125.8399, GNorm = 0.2583
Meta loss on this task batch = 2.7393e-01, Meta loss averaged over last 500 steps = 2.8529e-01, PNorm = 125.8457, GNorm = 0.2826
Meta loss on this task batch = 2.9932e-01, Meta loss averaged over last 500 steps = 2.8542e-01, PNorm = 125.8513, GNorm = 0.2967
Meta loss on this task batch = 2.6708e-01, Meta loss averaged over last 500 steps = 2.8545e-01, PNorm = 125.8560, GNorm = 0.2980
Meta loss on this task batch = 3.3818e-01, Meta loss averaged over last 500 steps = 2.8569e-01, PNorm = 125.8603, GNorm = 0.3046
Meta loss on this task batch = 2.9145e-01, Meta loss averaged over last 500 steps = 2.8568e-01, PNorm = 125.8642, GNorm = 0.2953
Meta loss on this task batch = 3.1351e-01, Meta loss averaged over last 500 steps = 2.8569e-01, PNorm = 125.8684, GNorm = 0.2869
Meta loss on this task batch = 3.0526e-01, Meta loss averaged over last 500 steps = 2.8576e-01, PNorm = 125.8703, GNorm = 0.2942
Meta loss on this task batch = 2.8766e-01, Meta loss averaged over last 500 steps = 2.8575e-01, PNorm = 125.8735, GNorm = 0.2893
Meta loss on this task batch = 2.2138e-01, Meta loss averaged over last 500 steps = 2.8568e-01, PNorm = 125.8782, GNorm = 0.2628
Meta loss on this task batch = 2.6043e-01, Meta loss averaged over last 500 steps = 2.8556e-01, PNorm = 125.8829, GNorm = 0.2535
Meta loss on this task batch = 2.6664e-01, Meta loss averaged over last 500 steps = 2.8549e-01, PNorm = 125.8878, GNorm = 0.3040
Meta loss on this task batch = 2.1504e-01, Meta loss averaged over last 500 steps = 2.8532e-01, PNorm = 125.8922, GNorm = 0.2819
Meta loss on this task batch = 3.0956e-01, Meta loss averaged over last 500 steps = 2.8529e-01, PNorm = 125.8952, GNorm = 0.3443
Meta loss on this task batch = 3.5977e-01, Meta loss averaged over last 500 steps = 2.8533e-01, PNorm = 125.8951, GNorm = 0.3752
Meta loss on this task batch = 2.5838e-01, Meta loss averaged over last 500 steps = 2.8525e-01, PNorm = 125.8953, GNorm = 0.3177
Took 154.47057461738586 seconds to complete one epoch of meta training
Took 164.33790469169617 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509769
Epoch 548
Meta loss on this task batch = 2.8348e-01, Meta loss averaged over last 500 steps = 2.8519e-01, PNorm = 125.8949, GNorm = 0.2938
Meta loss on this task batch = 2.4421e-01, Meta loss averaged over last 500 steps = 2.8514e-01, PNorm = 125.8954, GNorm = 0.2579
Meta loss on this task batch = 2.7954e-01, Meta loss averaged over last 500 steps = 2.8518e-01, PNorm = 125.8983, GNorm = 0.2942
Meta loss on this task batch = 2.9619e-01, Meta loss averaged over last 500 steps = 2.8509e-01, PNorm = 125.9022, GNorm = 0.2771
Meta loss on this task batch = 3.2792e-01, Meta loss averaged over last 500 steps = 2.8521e-01, PNorm = 125.9051, GNorm = 0.3153
Meta loss on this task batch = 2.3702e-01, Meta loss averaged over last 500 steps = 2.8520e-01, PNorm = 125.9073, GNorm = 0.2743
Meta loss on this task batch = 2.4585e-01, Meta loss averaged over last 500 steps = 2.8516e-01, PNorm = 125.9125, GNorm = 0.2787
Meta loss on this task batch = 2.5889e-01, Meta loss averaged over last 500 steps = 2.8507e-01, PNorm = 125.9179, GNorm = 0.2924
Meta loss on this task batch = 2.3300e-01, Meta loss averaged over last 500 steps = 2.8498e-01, PNorm = 125.9249, GNorm = 0.2800
Meta loss on this task batch = 2.8941e-01, Meta loss averaged over last 500 steps = 2.8497e-01, PNorm = 125.9316, GNorm = 0.2884
Meta loss on this task batch = 3.1364e-01, Meta loss averaged over last 500 steps = 2.8516e-01, PNorm = 125.9389, GNorm = 0.3489
Meta loss on this task batch = 3.0469e-01, Meta loss averaged over last 500 steps = 2.8523e-01, PNorm = 125.9459, GNorm = 0.2927
Meta loss on this task batch = 3.3719e-01, Meta loss averaged over last 500 steps = 2.8534e-01, PNorm = 125.9517, GNorm = 0.3073
Meta loss on this task batch = 2.9241e-01, Meta loss averaged over last 500 steps = 2.8537e-01, PNorm = 125.9563, GNorm = 0.2752
Meta loss on this task batch = 2.6626e-01, Meta loss averaged over last 500 steps = 2.8523e-01, PNorm = 125.9614, GNorm = 0.2669
Meta loss on this task batch = 2.5734e-01, Meta loss averaged over last 500 steps = 2.8505e-01, PNorm = 125.9648, GNorm = 0.2717
Meta loss on this task batch = 3.0711e-01, Meta loss averaged over last 500 steps = 2.8517e-01, PNorm = 125.9680, GNorm = 0.2756
Meta loss on this task batch = 2.9074e-01, Meta loss averaged over last 500 steps = 2.8517e-01, PNorm = 125.9751, GNorm = 0.2968
Meta loss on this task batch = 2.7460e-01, Meta loss averaged over last 500 steps = 2.8507e-01, PNorm = 125.9821, GNorm = 0.3069
Took 157.62461304664612 seconds to complete one epoch of meta training
Took 169.05133247375488 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491864
Epoch 549
Meta loss on this task batch = 2.9403e-01, Meta loss averaged over last 500 steps = 2.8497e-01, PNorm = 125.9878, GNorm = 0.2587
Meta loss on this task batch = 2.9525e-01, Meta loss averaged over last 500 steps = 2.8494e-01, PNorm = 125.9927, GNorm = 0.3002
Meta loss on this task batch = 3.5919e-01, Meta loss averaged over last 500 steps = 2.8521e-01, PNorm = 125.9970, GNorm = 0.3684
Meta loss on this task batch = 2.7591e-01, Meta loss averaged over last 500 steps = 2.8524e-01, PNorm = 126.0007, GNorm = 0.2775
Meta loss on this task batch = 2.8091e-01, Meta loss averaged over last 500 steps = 2.8508e-01, PNorm = 126.0051, GNorm = 0.2763
Meta loss on this task batch = 2.9887e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 126.0095, GNorm = 0.2861
Meta loss on this task batch = 2.7596e-01, Meta loss averaged over last 500 steps = 2.8503e-01, PNorm = 126.0138, GNorm = 0.2710
Meta loss on this task batch = 2.6035e-01, Meta loss averaged over last 500 steps = 2.8492e-01, PNorm = 126.0169, GNorm = 0.2858
Meta loss on this task batch = 3.0389e-01, Meta loss averaged over last 500 steps = 2.8500e-01, PNorm = 126.0206, GNorm = 0.3039
Meta loss on this task batch = 2.6623e-01, Meta loss averaged over last 500 steps = 2.8491e-01, PNorm = 126.0251, GNorm = 0.2675
Meta loss on this task batch = 3.3984e-01, Meta loss averaged over last 500 steps = 2.8516e-01, PNorm = 126.0282, GNorm = 0.2778
Meta loss on this task batch = 2.7650e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 126.0327, GNorm = 0.2568
Meta loss on this task batch = 2.5946e-01, Meta loss averaged over last 500 steps = 2.8518e-01, PNorm = 126.0388, GNorm = 0.2476
Meta loss on this task batch = 2.5299e-01, Meta loss averaged over last 500 steps = 2.8510e-01, PNorm = 126.0459, GNorm = 0.2735
Meta loss on this task batch = 3.0342e-01, Meta loss averaged over last 500 steps = 2.8507e-01, PNorm = 126.0554, GNorm = 0.3210
Meta loss on this task batch = 2.5408e-01, Meta loss averaged over last 500 steps = 2.8483e-01, PNorm = 126.0647, GNorm = 0.2332
Meta loss on this task batch = 2.4684e-01, Meta loss averaged over last 500 steps = 2.8484e-01, PNorm = 126.0731, GNorm = 0.2374
Meta loss on this task batch = 2.2205e-01, Meta loss averaged over last 500 steps = 2.8474e-01, PNorm = 126.0826, GNorm = 0.2694
Meta loss on this task batch = 2.9243e-01, Meta loss averaged over last 500 steps = 2.8482e-01, PNorm = 126.0919, GNorm = 0.3310
Took 159.59664964675903 seconds to complete one epoch of meta training
Took 169.6691391468048 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464989
Epoch 550
Meta loss on this task batch = 2.7575e-01, Meta loss averaged over last 500 steps = 2.8475e-01, PNorm = 126.0985, GNorm = 0.2735
Meta loss on this task batch = 2.5684e-01, Meta loss averaged over last 500 steps = 2.8477e-01, PNorm = 126.1041, GNorm = 0.2516
Meta loss on this task batch = 2.9994e-01, Meta loss averaged over last 500 steps = 2.8478e-01, PNorm = 126.1077, GNorm = 0.2872
Meta loss on this task batch = 2.9783e-01, Meta loss averaged over last 500 steps = 2.8479e-01, PNorm = 126.1106, GNorm = 0.2865
Meta loss on this task batch = 2.7165e-01, Meta loss averaged over last 500 steps = 2.8475e-01, PNorm = 126.1128, GNorm = 0.3218
Meta loss on this task batch = 2.7382e-01, Meta loss averaged over last 500 steps = 2.8471e-01, PNorm = 126.1153, GNorm = 0.3170
Meta loss on this task batch = 4.0175e-01, Meta loss averaged over last 500 steps = 2.8492e-01, PNorm = 126.1143, GNorm = 0.3999
Meta loss on this task batch = 1.8978e-01, Meta loss averaged over last 500 steps = 2.8472e-01, PNorm = 126.1163, GNorm = 0.2816
Meta loss on this task batch = 2.5365e-01, Meta loss averaged over last 500 steps = 2.8470e-01, PNorm = 126.1203, GNorm = 0.3195
Meta loss on this task batch = 2.6379e-01, Meta loss averaged over last 500 steps = 2.8459e-01, PNorm = 126.1271, GNorm = 0.3037
Meta loss on this task batch = 3.0713e-01, Meta loss averaged over last 500 steps = 2.8460e-01, PNorm = 126.1353, GNorm = 0.3212
Meta loss on this task batch = 2.7660e-01, Meta loss averaged over last 500 steps = 2.8458e-01, PNorm = 126.1422, GNorm = 0.3007
Meta loss on this task batch = 2.8781e-01, Meta loss averaged over last 500 steps = 2.8458e-01, PNorm = 126.1496, GNorm = 0.3059
Meta loss on this task batch = 3.2310e-01, Meta loss averaged over last 500 steps = 2.8465e-01, PNorm = 126.1585, GNorm = 0.3450
Meta loss on this task batch = 2.5873e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 126.1667, GNorm = 0.3271
Meta loss on this task batch = 2.3464e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 126.1729, GNorm = 0.2654
Meta loss on this task batch = 3.0357e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 126.1798, GNorm = 0.2906
Meta loss on this task batch = 2.3776e-01, Meta loss averaged over last 500 steps = 2.8452e-01, PNorm = 126.1845, GNorm = 0.2807
Meta loss on this task batch = 2.9722e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 126.1881, GNorm = 0.4204
Took 161.76577973365784 seconds to complete one epoch of meta training
Took 172.0088529586792 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493385
Epoch 551
Meta loss on this task batch = 2.8869e-01, Meta loss averaged over last 500 steps = 2.8441e-01, PNorm = 126.1918, GNorm = 0.3221
Meta loss on this task batch = 2.9067e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 126.1949, GNorm = 0.2714
Meta loss on this task batch = 2.3375e-01, Meta loss averaged over last 500 steps = 2.8431e-01, PNorm = 126.1984, GNorm = 0.2563
Meta loss on this task batch = 2.9625e-01, Meta loss averaged over last 500 steps = 2.8430e-01, PNorm = 126.2027, GNorm = 0.2862
Meta loss on this task batch = 2.7550e-01, Meta loss averaged over last 500 steps = 2.8437e-01, PNorm = 126.2083, GNorm = 0.2853
Meta loss on this task batch = 3.3155e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 126.2147, GNorm = 0.3443
Meta loss on this task batch = 3.1221e-01, Meta loss averaged over last 500 steps = 2.8467e-01, PNorm = 126.2194, GNorm = 0.3174
Meta loss on this task batch = 2.5239e-01, Meta loss averaged over last 500 steps = 2.8454e-01, PNorm = 126.2253, GNorm = 0.2625
Meta loss on this task batch = 2.7744e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 126.2305, GNorm = 0.2692
Meta loss on this task batch = 2.9825e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 126.2335, GNorm = 0.2588
Meta loss on this task batch = 2.9022e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 126.2368, GNorm = 0.2806
Meta loss on this task batch = 2.8052e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 126.2417, GNorm = 0.3454
Meta loss on this task batch = 2.3436e-01, Meta loss averaged over last 500 steps = 2.8449e-01, PNorm = 126.2463, GNorm = 0.2551
Meta loss on this task batch = 3.0752e-01, Meta loss averaged over last 500 steps = 2.8454e-01, PNorm = 126.2527, GNorm = 0.2890
Meta loss on this task batch = 2.9657e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 126.2569, GNorm = 0.2669
Meta loss on this task batch = 3.4340e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 126.2593, GNorm = 0.3323
Meta loss on this task batch = 2.8379e-01, Meta loss averaged over last 500 steps = 2.8465e-01, PNorm = 126.2612, GNorm = 0.3478
Meta loss on this task batch = 2.5406e-01, Meta loss averaged over last 500 steps = 2.8459e-01, PNorm = 126.2617, GNorm = 0.2631
Meta loss on this task batch = 2.6479e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 126.2631, GNorm = 0.3281
Took 157.36229491233826 seconds to complete one epoch of meta training
Took 167.31945180892944 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481115
Epoch 552
Meta loss on this task batch = 3.2147e-01, Meta loss averaged over last 500 steps = 2.8470e-01, PNorm = 126.2640, GNorm = 0.3373
Meta loss on this task batch = 2.4332e-01, Meta loss averaged over last 500 steps = 2.8458e-01, PNorm = 126.2672, GNorm = 0.2515
Meta loss on this task batch = 3.0301e-01, Meta loss averaged over last 500 steps = 2.8453e-01, PNorm = 126.2711, GNorm = 0.2713
Meta loss on this task batch = 2.4582e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 126.2773, GNorm = 0.2553
Meta loss on this task batch = 2.6581e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 126.2840, GNorm = 0.2600
Meta loss on this task batch = 2.6735e-01, Meta loss averaged over last 500 steps = 2.8419e-01, PNorm = 126.2916, GNorm = 0.2410
Meta loss on this task batch = 2.9573e-01, Meta loss averaged over last 500 steps = 2.8417e-01, PNorm = 126.2996, GNorm = 0.2653
Meta loss on this task batch = 2.1464e-01, Meta loss averaged over last 500 steps = 2.8403e-01, PNorm = 126.3086, GNorm = 0.2613
Meta loss on this task batch = 3.1734e-01, Meta loss averaged over last 500 steps = 2.8411e-01, PNorm = 126.3164, GNorm = 0.3084
Meta loss on this task batch = 2.9290e-01, Meta loss averaged over last 500 steps = 2.8425e-01, PNorm = 126.3266, GNorm = 0.2897
Meta loss on this task batch = 2.6725e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 126.3364, GNorm = 0.3180
Meta loss on this task batch = 3.0965e-01, Meta loss averaged over last 500 steps = 2.8421e-01, PNorm = 126.3478, GNorm = 0.3222
Meta loss on this task batch = 2.2392e-01, Meta loss averaged over last 500 steps = 2.8402e-01, PNorm = 126.3590, GNorm = 0.2717
Meta loss on this task batch = 2.6902e-01, Meta loss averaged over last 500 steps = 2.8404e-01, PNorm = 126.3705, GNorm = 0.3436
Meta loss on this task batch = 2.5827e-01, Meta loss averaged over last 500 steps = 2.8387e-01, PNorm = 126.3810, GNorm = 0.2922
Meta loss on this task batch = 3.6989e-01, Meta loss averaged over last 500 steps = 2.8398e-01, PNorm = 126.3901, GNorm = 0.3146
Meta loss on this task batch = 3.0337e-01, Meta loss averaged over last 500 steps = 2.8402e-01, PNorm = 126.3972, GNorm = 0.3510
Meta loss on this task batch = 2.7288e-01, Meta loss averaged over last 500 steps = 2.8398e-01, PNorm = 126.4031, GNorm = 0.2526
Meta loss on this task batch = 3.2745e-01, Meta loss averaged over last 500 steps = 2.8412e-01, PNorm = 126.4097, GNorm = 0.3312
Took 161.40494513511658 seconds to complete one epoch of meta training
Took 171.1542408466339 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466964
Epoch 553
Meta loss on this task batch = 2.6497e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 126.4171, GNorm = 0.2404
Meta loss on this task batch = 2.8832e-01, Meta loss averaged over last 500 steps = 2.8424e-01, PNorm = 126.4242, GNorm = 0.2481
Meta loss on this task batch = 2.3147e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 126.4305, GNorm = 0.2998
Meta loss on this task batch = 2.8902e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 126.4358, GNorm = 0.2731
Meta loss on this task batch = 3.1158e-01, Meta loss averaged over last 500 steps = 2.8439e-01, PNorm = 126.4407, GNorm = 0.2834
Meta loss on this task batch = 2.2857e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 126.4470, GNorm = 0.2338
Meta loss on this task batch = 2.3971e-01, Meta loss averaged over last 500 steps = 2.8415e-01, PNorm = 126.4518, GNorm = 0.2958
Meta loss on this task batch = 2.8783e-01, Meta loss averaged over last 500 steps = 2.8414e-01, PNorm = 126.4559, GNorm = 0.2680
Meta loss on this task batch = 3.4248e-01, Meta loss averaged over last 500 steps = 2.8409e-01, PNorm = 126.4585, GNorm = 0.3226
Meta loss on this task batch = 2.9938e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 126.4636, GNorm = 0.2974
Meta loss on this task batch = 2.5369e-01, Meta loss averaged over last 500 steps = 2.8410e-01, PNorm = 126.4681, GNorm = 0.2787
Meta loss on this task batch = 2.8614e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 126.4733, GNorm = 0.2968
Meta loss on this task batch = 2.4936e-01, Meta loss averaged over last 500 steps = 2.8417e-01, PNorm = 126.4788, GNorm = 0.2767
Meta loss on this task batch = 3.1832e-01, Meta loss averaged over last 500 steps = 2.8425e-01, PNorm = 126.4833, GNorm = 0.3383
Meta loss on this task batch = 2.5873e-01, Meta loss averaged over last 500 steps = 2.8417e-01, PNorm = 126.4882, GNorm = 0.2867
Meta loss on this task batch = 2.7342e-01, Meta loss averaged over last 500 steps = 2.8427e-01, PNorm = 126.4930, GNorm = 0.3272
Meta loss on this task batch = 2.9540e-01, Meta loss averaged over last 500 steps = 2.8411e-01, PNorm = 126.4973, GNorm = 0.3908
Meta loss on this task batch = 2.8008e-01, Meta loss averaged over last 500 steps = 2.8406e-01, PNorm = 126.5016, GNorm = 0.3495
Meta loss on this task batch = 3.0202e-01, Meta loss averaged over last 500 steps = 2.8411e-01, PNorm = 126.5063, GNorm = 0.3616
Took 161.43809008598328 seconds to complete one epoch of meta training
Took 172.163756608963 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481229
Epoch 554
Meta loss on this task batch = 1.9678e-01, Meta loss averaged over last 500 steps = 2.8396e-01, PNorm = 126.5103, GNorm = 0.2203
Meta loss on this task batch = 3.2573e-01, Meta loss averaged over last 500 steps = 2.8406e-01, PNorm = 126.5131, GNorm = 0.5011
Meta loss on this task batch = 2.8578e-01, Meta loss averaged over last 500 steps = 2.8407e-01, PNorm = 126.5159, GNorm = 0.2959
Meta loss on this task batch = 2.6875e-01, Meta loss averaged over last 500 steps = 2.8395e-01, PNorm = 126.5199, GNorm = 0.2636
Meta loss on this task batch = 2.6834e-01, Meta loss averaged over last 500 steps = 2.8399e-01, PNorm = 126.5243, GNorm = 0.2858
Meta loss on this task batch = 2.7401e-01, Meta loss averaged over last 500 steps = 2.8397e-01, PNorm = 126.5291, GNorm = 0.2640
Meta loss on this task batch = 2.7501e-01, Meta loss averaged over last 500 steps = 2.8384e-01, PNorm = 126.5317, GNorm = 0.2770
Meta loss on this task batch = 2.5261e-01, Meta loss averaged over last 500 steps = 2.8382e-01, PNorm = 126.5356, GNorm = 0.2550
Meta loss on this task batch = 2.4339e-01, Meta loss averaged over last 500 steps = 2.8368e-01, PNorm = 126.5397, GNorm = 0.2577
Meta loss on this task batch = 3.4614e-01, Meta loss averaged over last 500 steps = 2.8382e-01, PNorm = 126.5435, GNorm = 0.3163
Meta loss on this task batch = 2.9704e-01, Meta loss averaged over last 500 steps = 2.8392e-01, PNorm = 126.5476, GNorm = 0.2597
Meta loss on this task batch = 3.0207e-01, Meta loss averaged over last 500 steps = 2.8392e-01, PNorm = 126.5518, GNorm = 0.2724
Meta loss on this task batch = 2.9865e-01, Meta loss averaged over last 500 steps = 2.8402e-01, PNorm = 126.5567, GNorm = 0.2672
Meta loss on this task batch = 2.7297e-01, Meta loss averaged over last 500 steps = 2.8402e-01, PNorm = 126.5631, GNorm = 0.2720
Meta loss on this task batch = 3.0331e-01, Meta loss averaged over last 500 steps = 2.8415e-01, PNorm = 126.5699, GNorm = 0.3389
Meta loss on this task batch = 2.7893e-01, Meta loss averaged over last 500 steps = 2.8411e-01, PNorm = 126.5779, GNorm = 0.3059
Meta loss on this task batch = 2.9266e-01, Meta loss averaged over last 500 steps = 2.8411e-01, PNorm = 126.5873, GNorm = 0.2509
Meta loss on this task batch = 2.6138e-01, Meta loss averaged over last 500 steps = 2.8404e-01, PNorm = 126.5977, GNorm = 0.2719
Meta loss on this task batch = 3.3605e-01, Meta loss averaged over last 500 steps = 2.8405e-01, PNorm = 126.6086, GNorm = 0.3639
Took 159.5928361415863 seconds to complete one epoch of meta training
Took 169.97324657440186 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488520
Epoch 555
Meta loss on this task batch = 2.9372e-01, Meta loss averaged over last 500 steps = 2.8399e-01, PNorm = 126.6191, GNorm = 0.3023
Meta loss on this task batch = 2.8492e-01, Meta loss averaged over last 500 steps = 2.8387e-01, PNorm = 126.6282, GNorm = 0.2854
Meta loss on this task batch = 2.7930e-01, Meta loss averaged over last 500 steps = 2.8388e-01, PNorm = 126.6374, GNorm = 0.3031
Meta loss on this task batch = 3.0112e-01, Meta loss averaged over last 500 steps = 2.8379e-01, PNorm = 126.6470, GNorm = 0.3185
Meta loss on this task batch = 2.7531e-01, Meta loss averaged over last 500 steps = 2.8390e-01, PNorm = 126.6551, GNorm = 0.2828
Meta loss on this task batch = 2.9383e-01, Meta loss averaged over last 500 steps = 2.8393e-01, PNorm = 126.6613, GNorm = 0.2888
Meta loss on this task batch = 2.8395e-01, Meta loss averaged over last 500 steps = 2.8386e-01, PNorm = 126.6671, GNorm = 0.2967
Meta loss on this task batch = 2.6363e-01, Meta loss averaged over last 500 steps = 2.8385e-01, PNorm = 126.6746, GNorm = 0.3209
Meta loss on this task batch = 2.3792e-01, Meta loss averaged over last 500 steps = 2.8383e-01, PNorm = 126.6814, GNorm = 0.2634
Meta loss on this task batch = 2.4324e-01, Meta loss averaged over last 500 steps = 2.8377e-01, PNorm = 126.6850, GNorm = 0.2725
Meta loss on this task batch = 3.0573e-01, Meta loss averaged over last 500 steps = 2.8371e-01, PNorm = 126.6877, GNorm = 0.3382
Meta loss on this task batch = 2.5963e-01, Meta loss averaged over last 500 steps = 2.8361e-01, PNorm = 126.6892, GNorm = 0.2773
Meta loss on this task batch = 3.1350e-01, Meta loss averaged over last 500 steps = 2.8369e-01, PNorm = 126.6914, GNorm = 0.3146
Meta loss on this task batch = 2.5638e-01, Meta loss averaged over last 500 steps = 2.8372e-01, PNorm = 126.6928, GNorm = 0.3067
Meta loss on this task batch = 2.5296e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 126.6918, GNorm = 0.3138
Meta loss on this task batch = 3.1719e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 126.6882, GNorm = 0.2835
Meta loss on this task batch = 2.7823e-01, Meta loss averaged over last 500 steps = 2.8355e-01, PNorm = 126.6854, GNorm = 0.2988
Meta loss on this task batch = 3.8352e-01, Meta loss averaged over last 500 steps = 2.8374e-01, PNorm = 126.6830, GNorm = 0.3275
Meta loss on this task batch = 3.0567e-01, Meta loss averaged over last 500 steps = 2.8382e-01, PNorm = 126.6819, GNorm = 0.3777
Took 160.12534189224243 seconds to complete one epoch of meta training
Took 169.49484395980835 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483880
Epoch 556
Meta loss on this task batch = 2.6331e-01, Meta loss averaged over last 500 steps = 2.8379e-01, PNorm = 126.6841, GNorm = 0.2805
Meta loss on this task batch = 2.5670e-01, Meta loss averaged over last 500 steps = 2.8371e-01, PNorm = 126.6911, GNorm = 0.2980
Meta loss on this task batch = 2.8538e-01, Meta loss averaged over last 500 steps = 2.8355e-01, PNorm = 126.7004, GNorm = 0.2517
Meta loss on this task batch = 2.9707e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 126.7121, GNorm = 0.3195
Meta loss on this task batch = 3.0392e-01, Meta loss averaged over last 500 steps = 2.8365e-01, PNorm = 126.7252, GNorm = 0.3005
Meta loss on this task batch = 3.1869e-01, Meta loss averaged over last 500 steps = 2.8367e-01, PNorm = 126.7402, GNorm = 0.3686
Meta loss on this task batch = 2.6853e-01, Meta loss averaged over last 500 steps = 2.8363e-01, PNorm = 126.7567, GNorm = 0.2849
Meta loss on this task batch = 3.4590e-01, Meta loss averaged over last 500 steps = 2.8377e-01, PNorm = 126.7718, GNorm = 0.3007
Meta loss on this task batch = 2.5913e-01, Meta loss averaged over last 500 steps = 2.8380e-01, PNorm = 126.7842, GNorm = 0.3057
Meta loss on this task batch = 3.2628e-01, Meta loss averaged over last 500 steps = 2.8379e-01, PNorm = 126.7936, GNorm = 0.3126
Meta loss on this task batch = 2.9298e-01, Meta loss averaged over last 500 steps = 2.8375e-01, PNorm = 126.8048, GNorm = 0.3097
Meta loss on this task batch = 2.7709e-01, Meta loss averaged over last 500 steps = 2.8374e-01, PNorm = 126.8152, GNorm = 0.2882
Meta loss on this task batch = 2.8531e-01, Meta loss averaged over last 500 steps = 2.8364e-01, PNorm = 126.8236, GNorm = 0.3290
Meta loss on this task batch = 2.5021e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 126.8318, GNorm = 0.3074
Meta loss on this task batch = 2.9474e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 126.8364, GNorm = 0.3638
Meta loss on this task batch = 2.7103e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 126.8375, GNorm = 0.3426
Meta loss on this task batch = 2.7518e-01, Meta loss averaged over last 500 steps = 2.8358e-01, PNorm = 126.8377, GNorm = 0.3021
Meta loss on this task batch = 2.3609e-01, Meta loss averaged over last 500 steps = 2.8355e-01, PNorm = 126.8386, GNorm = 0.2518
Meta loss on this task batch = 3.6864e-01, Meta loss averaged over last 500 steps = 2.8368e-01, PNorm = 126.8398, GNorm = 0.3647
Took 162.55436539649963 seconds to complete one epoch of meta training
Took 172.12578415870667 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513880
Epoch 557
Meta loss on this task batch = 3.2877e-01, Meta loss averaged over last 500 steps = 2.8383e-01, PNorm = 126.8434, GNorm = 0.2777
Meta loss on this task batch = 3.0220e-01, Meta loss averaged over last 500 steps = 2.8393e-01, PNorm = 126.8521, GNorm = 0.3953
Meta loss on this task batch = 3.0393e-01, Meta loss averaged over last 500 steps = 2.8381e-01, PNorm = 126.8624, GNorm = 0.3691
Meta loss on this task batch = 3.1503e-01, Meta loss averaged over last 500 steps = 2.8388e-01, PNorm = 126.8732, GNorm = 0.2726
Meta loss on this task batch = 3.1335e-01, Meta loss averaged over last 500 steps = 2.8391e-01, PNorm = 126.8870, GNorm = 0.3016
Meta loss on this task batch = 2.8759e-01, Meta loss averaged over last 500 steps = 2.8396e-01, PNorm = 126.9039, GNorm = 0.3459
Meta loss on this task batch = 2.8021e-01, Meta loss averaged over last 500 steps = 2.8396e-01, PNorm = 126.9209, GNorm = 0.3055
Meta loss on this task batch = 2.8697e-01, Meta loss averaged over last 500 steps = 2.8398e-01, PNorm = 126.9382, GNorm = 0.2976
Meta loss on this task batch = 2.7238e-01, Meta loss averaged over last 500 steps = 2.8401e-01, PNorm = 126.9523, GNorm = 0.3513
Meta loss on this task batch = 2.5649e-01, Meta loss averaged over last 500 steps = 2.8394e-01, PNorm = 126.9629, GNorm = 0.2951
Meta loss on this task batch = 2.6985e-01, Meta loss averaged over last 500 steps = 2.8399e-01, PNorm = 126.9704, GNorm = 0.2904
Meta loss on this task batch = 3.2356e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 126.9745, GNorm = 0.3557
Meta loss on this task batch = 3.2996e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 126.9759, GNorm = 0.3618
Meta loss on this task batch = 2.9271e-01, Meta loss averaged over last 500 steps = 2.8431e-01, PNorm = 126.9771, GNorm = 0.3451
Meta loss on this task batch = 3.1217e-01, Meta loss averaged over last 500 steps = 2.8438e-01, PNorm = 126.9771, GNorm = 0.3845
Meta loss on this task batch = 2.5631e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 126.9781, GNorm = 0.2638
Meta loss on this task batch = 2.8261e-01, Meta loss averaged over last 500 steps = 2.8441e-01, PNorm = 126.9779, GNorm = 0.2955
Meta loss on this task batch = 2.9392e-01, Meta loss averaged over last 500 steps = 2.8439e-01, PNorm = 126.9799, GNorm = 0.3012
Meta loss on this task batch = 2.7045e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 126.9825, GNorm = 0.3565
Took 157.37229251861572 seconds to complete one epoch of meta training
Took 167.27993154525757 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494793
Epoch 558
Meta loss on this task batch = 2.5351e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 126.9891, GNorm = 0.2531
Meta loss on this task batch = 2.5309e-01, Meta loss averaged over last 500 steps = 2.8417e-01, PNorm = 126.9971, GNorm = 0.2743
Meta loss on this task batch = 2.2825e-01, Meta loss averaged over last 500 steps = 2.8401e-01, PNorm = 127.0034, GNorm = 0.2749
Meta loss on this task batch = 3.0158e-01, Meta loss averaged over last 500 steps = 2.8399e-01, PNorm = 127.0103, GNorm = 0.2822
Meta loss on this task batch = 2.7942e-01, Meta loss averaged over last 500 steps = 2.8390e-01, PNorm = 127.0164, GNorm = 0.2933
Meta loss on this task batch = 2.4789e-01, Meta loss averaged over last 500 steps = 2.8365e-01, PNorm = 127.0243, GNorm = 0.2982
Meta loss on this task batch = 2.8327e-01, Meta loss averaged over last 500 steps = 2.8359e-01, PNorm = 127.0316, GNorm = 0.3419
Meta loss on this task batch = 3.5151e-01, Meta loss averaged over last 500 steps = 2.8367e-01, PNorm = 127.0395, GNorm = 0.2759
Meta loss on this task batch = 3.0378e-01, Meta loss averaged over last 500 steps = 2.8365e-01, PNorm = 127.0464, GNorm = 0.2825
Meta loss on this task batch = 2.9493e-01, Meta loss averaged over last 500 steps = 2.8382e-01, PNorm = 127.0533, GNorm = 0.2751
Meta loss on this task batch = 2.8171e-01, Meta loss averaged over last 500 steps = 2.8379e-01, PNorm = 127.0591, GNorm = 0.3254
Meta loss on this task batch = 2.5091e-01, Meta loss averaged over last 500 steps = 2.8370e-01, PNorm = 127.0637, GNorm = 0.2677
Meta loss on this task batch = 2.7815e-01, Meta loss averaged over last 500 steps = 2.8378e-01, PNorm = 127.0667, GNorm = 0.3033
Meta loss on this task batch = 3.4374e-01, Meta loss averaged over last 500 steps = 2.8399e-01, PNorm = 127.0687, GNorm = 0.3577
Meta loss on this task batch = 3.4065e-01, Meta loss averaged over last 500 steps = 2.8410e-01, PNorm = 127.0707, GNorm = 0.2918
Meta loss on this task batch = 3.2857e-01, Meta loss averaged over last 500 steps = 2.8411e-01, PNorm = 127.0712, GNorm = 0.3588
Meta loss on this task batch = 2.7034e-01, Meta loss averaged over last 500 steps = 2.8405e-01, PNorm = 127.0720, GNorm = 0.2811
Meta loss on this task batch = 2.5777e-01, Meta loss averaged over last 500 steps = 2.8404e-01, PNorm = 127.0741, GNorm = 0.3444
Meta loss on this task batch = 2.5807e-01, Meta loss averaged over last 500 steps = 2.8396e-01, PNorm = 127.0790, GNorm = 0.3272
Took 159.83904600143433 seconds to complete one epoch of meta training
Took 171.21217846870422 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492622
Epoch 559
Meta loss on this task batch = 2.9591e-01, Meta loss averaged over last 500 steps = 2.8388e-01, PNorm = 127.0846, GNorm = 0.3041
Meta loss on this task batch = 2.0420e-01, Meta loss averaged over last 500 steps = 2.8372e-01, PNorm = 127.0907, GNorm = 0.3077
Meta loss on this task batch = 2.9435e-01, Meta loss averaged over last 500 steps = 2.8382e-01, PNorm = 127.0946, GNorm = 0.3341
Meta loss on this task batch = 2.0289e-01, Meta loss averaged over last 500 steps = 2.8353e-01, PNorm = 127.1007, GNorm = 0.3049
Meta loss on this task batch = 2.8651e-01, Meta loss averaged over last 500 steps = 2.8342e-01, PNorm = 127.1069, GNorm = 0.3172
Meta loss on this task batch = 2.9898e-01, Meta loss averaged over last 500 steps = 2.8353e-01, PNorm = 127.1116, GNorm = 0.2971
Meta loss on this task batch = 2.8192e-01, Meta loss averaged over last 500 steps = 2.8347e-01, PNorm = 127.1174, GNorm = 0.3089
Meta loss on this task batch = 2.7348e-01, Meta loss averaged over last 500 steps = 2.8344e-01, PNorm = 127.1241, GNorm = 0.2673
Meta loss on this task batch = 3.0795e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 127.1303, GNorm = 0.2827
Meta loss on this task batch = 2.6375e-01, Meta loss averaged over last 500 steps = 2.8337e-01, PNorm = 127.1359, GNorm = 0.2423
Meta loss on this task batch = 3.2698e-01, Meta loss averaged over last 500 steps = 2.8345e-01, PNorm = 127.1402, GNorm = 0.2864
Meta loss on this task batch = 2.9835e-01, Meta loss averaged over last 500 steps = 2.8351e-01, PNorm = 127.1434, GNorm = 0.3090
Meta loss on this task batch = 2.8463e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 127.1463, GNorm = 0.2721
Meta loss on this task batch = 3.6766e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 127.1468, GNorm = 0.3663
Meta loss on this task batch = 2.7388e-01, Meta loss averaged over last 500 steps = 2.8363e-01, PNorm = 127.1453, GNorm = 0.2728
Meta loss on this task batch = 2.5611e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 127.1449, GNorm = 0.2645
Meta loss on this task batch = 2.2865e-01, Meta loss averaged over last 500 steps = 2.8344e-01, PNorm = 127.1476, GNorm = 0.2437
Meta loss on this task batch = 3.4631e-01, Meta loss averaged over last 500 steps = 2.8361e-01, PNorm = 127.1490, GNorm = 0.4164
Meta loss on this task batch = 2.5842e-01, Meta loss averaged over last 500 steps = 2.8364e-01, PNorm = 127.1504, GNorm = 0.3619
Took 160.66431069374084 seconds to complete one epoch of meta training
Took 172.09096217155457 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464818
Epoch 560
Meta loss on this task batch = 2.9144e-01, Meta loss averaged over last 500 steps = 2.8363e-01, PNorm = 127.1529, GNorm = 0.3189
Meta loss on this task batch = 2.4698e-01, Meta loss averaged over last 500 steps = 2.8362e-01, PNorm = 127.1574, GNorm = 0.2628
Meta loss on this task batch = 2.3686e-01, Meta loss averaged over last 500 steps = 2.8358e-01, PNorm = 127.1639, GNorm = 0.2571
Meta loss on this task batch = 2.7274e-01, Meta loss averaged over last 500 steps = 2.8349e-01, PNorm = 127.1684, GNorm = 0.2917
Meta loss on this task batch = 2.6791e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 127.1746, GNorm = 0.3056
Meta loss on this task batch = 2.5180e-01, Meta loss averaged over last 500 steps = 2.8342e-01, PNorm = 127.1818, GNorm = 0.2850
Meta loss on this task batch = 3.1124e-01, Meta loss averaged over last 500 steps = 2.8339e-01, PNorm = 127.1893, GNorm = 0.3477
Meta loss on this task batch = 3.2385e-01, Meta loss averaged over last 500 steps = 2.8338e-01, PNorm = 127.1930, GNorm = 0.3453
Meta loss on this task batch = 2.9555e-01, Meta loss averaged over last 500 steps = 2.8344e-01, PNorm = 127.1953, GNorm = 0.2949
Meta loss on this task batch = 2.9833e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 127.1966, GNorm = 0.3434
Meta loss on this task batch = 2.6104e-01, Meta loss averaged over last 500 steps = 2.8351e-01, PNorm = 127.1978, GNorm = 0.2909
Meta loss on this task batch = 2.8813e-01, Meta loss averaged over last 500 steps = 2.8344e-01, PNorm = 127.1989, GNorm = 0.3002
Meta loss on this task batch = 2.7975e-01, Meta loss averaged over last 500 steps = 2.8344e-01, PNorm = 127.2005, GNorm = 0.3050
Meta loss on this task batch = 2.5563e-01, Meta loss averaged over last 500 steps = 2.8342e-01, PNorm = 127.2020, GNorm = 0.2550
Meta loss on this task batch = 2.8444e-01, Meta loss averaged over last 500 steps = 2.8349e-01, PNorm = 127.2050, GNorm = 0.2490
Meta loss on this task batch = 2.3519e-01, Meta loss averaged over last 500 steps = 2.8347e-01, PNorm = 127.2070, GNorm = 0.2487
Meta loss on this task batch = 2.6511e-01, Meta loss averaged over last 500 steps = 2.8337e-01, PNorm = 127.2089, GNorm = 0.2652
Meta loss on this task batch = 3.1120e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 127.2104, GNorm = 0.2900
Meta loss on this task batch = 2.3993e-01, Meta loss averaged over last 500 steps = 2.8323e-01, PNorm = 127.2107, GNorm = 0.3136
Took 162.18336367607117 seconds to complete one epoch of meta training
Took 173.60040926933289 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488601
Epoch 561
Meta loss on this task batch = 2.6886e-01, Meta loss averaged over last 500 steps = 2.8328e-01, PNorm = 127.2116, GNorm = 0.3006
Meta loss on this task batch = 2.4512e-01, Meta loss averaged over last 500 steps = 2.8313e-01, PNorm = 127.2140, GNorm = 0.2594
Meta loss on this task batch = 2.8103e-01, Meta loss averaged over last 500 steps = 2.8310e-01, PNorm = 127.2173, GNorm = 0.2844
Meta loss on this task batch = 3.0498e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 127.2200, GNorm = 0.2865
Meta loss on this task batch = 2.3428e-01, Meta loss averaged over last 500 steps = 2.8313e-01, PNorm = 127.2239, GNorm = 0.2736
Meta loss on this task batch = 2.7395e-01, Meta loss averaged over last 500 steps = 2.8302e-01, PNorm = 127.2281, GNorm = 0.2472
Meta loss on this task batch = 3.3286e-01, Meta loss averaged over last 500 steps = 2.8308e-01, PNorm = 127.2305, GNorm = 0.2941
Meta loss on this task batch = 2.4999e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 127.2348, GNorm = 0.2848
Meta loss on this task batch = 2.7102e-01, Meta loss averaged over last 500 steps = 2.8301e-01, PNorm = 127.2388, GNorm = 0.2839
Meta loss on this task batch = 2.6621e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 127.2441, GNorm = 0.2788
Meta loss on this task batch = 2.6141e-01, Meta loss averaged over last 500 steps = 2.8286e-01, PNorm = 127.2484, GNorm = 0.2735
Meta loss on this task batch = 2.5886e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 127.2530, GNorm = 0.2803
Meta loss on this task batch = 2.6698e-01, Meta loss averaged over last 500 steps = 2.8275e-01, PNorm = 127.2579, GNorm = 0.2793
Meta loss on this task batch = 2.8508e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 127.2601, GNorm = 0.2890
Meta loss on this task batch = 3.0949e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 127.2637, GNorm = 0.3328
Meta loss on this task batch = 2.7988e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 127.2680, GNorm = 0.2923
Meta loss on this task batch = 3.0453e-01, Meta loss averaged over last 500 steps = 2.8284e-01, PNorm = 127.2737, GNorm = 0.3007
Meta loss on this task batch = 2.8833e-01, Meta loss averaged over last 500 steps = 2.8293e-01, PNorm = 127.2790, GNorm = 0.3040
Meta loss on this task batch = 2.7853e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 127.2855, GNorm = 0.3217
Took 163.9162197113037 seconds to complete one epoch of meta training
Took 175.3283851146698 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474941
Epoch 562
Meta loss on this task batch = 2.9247e-01, Meta loss averaged over last 500 steps = 2.8292e-01, PNorm = 127.2898, GNorm = 0.2940
Meta loss on this task batch = 3.0057e-01, Meta loss averaged over last 500 steps = 2.8292e-01, PNorm = 127.2936, GNorm = 0.3007
Meta loss on this task batch = 3.2422e-01, Meta loss averaged over last 500 steps = 2.8301e-01, PNorm = 127.2987, GNorm = 0.3519
Meta loss on this task batch = 2.6945e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 127.3039, GNorm = 0.2667
Meta loss on this task batch = 2.8084e-01, Meta loss averaged over last 500 steps = 2.8298e-01, PNorm = 127.3086, GNorm = 0.3846
Meta loss on this task batch = 2.6534e-01, Meta loss averaged over last 500 steps = 2.8302e-01, PNorm = 127.3138, GNorm = 0.2798
Meta loss on this task batch = 2.9942e-01, Meta loss averaged over last 500 steps = 2.8315e-01, PNorm = 127.3169, GNorm = 0.2907
Meta loss on this task batch = 3.2095e-01, Meta loss averaged over last 500 steps = 2.8325e-01, PNorm = 127.3211, GNorm = 0.3148
Meta loss on this task batch = 2.7316e-01, Meta loss averaged over last 500 steps = 2.8320e-01, PNorm = 127.3257, GNorm = 0.2721
Meta loss on this task batch = 2.5601e-01, Meta loss averaged over last 500 steps = 2.8313e-01, PNorm = 127.3328, GNorm = 0.2913
Meta loss on this task batch = 3.4799e-01, Meta loss averaged over last 500 steps = 2.8333e-01, PNorm = 127.3390, GNorm = 0.3297
Meta loss on this task batch = 2.7391e-01, Meta loss averaged over last 500 steps = 2.8321e-01, PNorm = 127.3450, GNorm = 0.2779
Meta loss on this task batch = 3.0647e-01, Meta loss averaged over last 500 steps = 2.8336e-01, PNorm = 127.3499, GNorm = 0.3375
Meta loss on this task batch = 2.6404e-01, Meta loss averaged over last 500 steps = 2.8323e-01, PNorm = 127.3554, GNorm = 0.2678
Meta loss on this task batch = 2.8572e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 127.3620, GNorm = 0.2948
Meta loss on this task batch = 2.5115e-01, Meta loss averaged over last 500 steps = 2.8314e-01, PNorm = 127.3693, GNorm = 0.2586
Meta loss on this task batch = 2.9802e-01, Meta loss averaged over last 500 steps = 2.8310e-01, PNorm = 127.3754, GNorm = 0.3217
Meta loss on this task batch = 2.9752e-01, Meta loss averaged over last 500 steps = 2.8315e-01, PNorm = 127.3823, GNorm = 0.3030
Meta loss on this task batch = 2.5149e-01, Meta loss averaged over last 500 steps = 2.8301e-01, PNorm = 127.3885, GNorm = 0.3709
Took 158.41534113883972 seconds to complete one epoch of meta training
Took 168.07850646972656 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464404
Epoch 563
Meta loss on this task batch = 3.0872e-01, Meta loss averaged over last 500 steps = 2.8299e-01, PNorm = 127.3937, GNorm = 0.2978
Meta loss on this task batch = 2.9296e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 127.3979, GNorm = 0.2943
Meta loss on this task batch = 3.0714e-01, Meta loss averaged over last 500 steps = 2.8299e-01, PNorm = 127.3989, GNorm = 0.3982
Meta loss on this task batch = 2.7013e-01, Meta loss averaged over last 500 steps = 2.8284e-01, PNorm = 127.3996, GNorm = 0.3245
Meta loss on this task batch = 2.9154e-01, Meta loss averaged over last 500 steps = 2.8282e-01, PNorm = 127.3998, GNorm = 0.3247
Meta loss on this task batch = 2.7130e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 127.4013, GNorm = 0.2658
Meta loss on this task batch = 2.9937e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 127.4030, GNorm = 0.2851
Meta loss on this task batch = 2.5975e-01, Meta loss averaged over last 500 steps = 2.8291e-01, PNorm = 127.4044, GNorm = 0.2702
Meta loss on this task batch = 2.7791e-01, Meta loss averaged over last 500 steps = 2.8286e-01, PNorm = 127.4081, GNorm = 0.2980
Meta loss on this task batch = 2.5148e-01, Meta loss averaged over last 500 steps = 2.8281e-01, PNorm = 127.4136, GNorm = 0.3021
Meta loss on this task batch = 2.9208e-01, Meta loss averaged over last 500 steps = 2.8291e-01, PNorm = 127.4219, GNorm = 0.3400
Meta loss on this task batch = 3.2189e-01, Meta loss averaged over last 500 steps = 2.8295e-01, PNorm = 127.4314, GNorm = 0.3357
Meta loss on this task batch = 2.7267e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 127.4396, GNorm = 0.2569
Meta loss on this task batch = 3.3244e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 127.4455, GNorm = 0.3298
Meta loss on this task batch = 2.9377e-01, Meta loss averaged over last 500 steps = 2.8308e-01, PNorm = 127.4514, GNorm = 0.2909
Meta loss on this task batch = 2.6970e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 127.4568, GNorm = 0.3018
Meta loss on this task batch = 2.5577e-01, Meta loss averaged over last 500 steps = 2.8295e-01, PNorm = 127.4631, GNorm = 0.2898
Meta loss on this task batch = 2.6967e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 127.4665, GNorm = 0.2908
Meta loss on this task batch = 2.2172e-01, Meta loss averaged over last 500 steps = 2.8276e-01, PNorm = 127.4711, GNorm = 0.3312
Took 163.80998706817627 seconds to complete one epoch of meta training
Took 174.65976524353027 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491889
Epoch 564
Meta loss on this task batch = 2.5825e-01, Meta loss averaged over last 500 steps = 2.8272e-01, PNorm = 127.4760, GNorm = 0.2783
Meta loss on this task batch = 3.0294e-01, Meta loss averaged over last 500 steps = 2.8281e-01, PNorm = 127.4809, GNorm = 0.3170
Meta loss on this task batch = 2.7852e-01, Meta loss averaged over last 500 steps = 2.8284e-01, PNorm = 127.4848, GNorm = 0.3088
Meta loss on this task batch = 2.6955e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 127.4887, GNorm = 0.3057
Meta loss on this task batch = 2.5338e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 127.4961, GNorm = 0.3219
Meta loss on this task batch = 2.6086e-01, Meta loss averaged over last 500 steps = 2.8274e-01, PNorm = 127.5019, GNorm = 0.3071
Meta loss on this task batch = 3.0527e-01, Meta loss averaged over last 500 steps = 2.8273e-01, PNorm = 127.5084, GNorm = 0.3076
Meta loss on this task batch = 3.1041e-01, Meta loss averaged over last 500 steps = 2.8291e-01, PNorm = 127.5133, GNorm = 0.3049
Meta loss on this task batch = 2.9084e-01, Meta loss averaged over last 500 steps = 2.8299e-01, PNorm = 127.5187, GNorm = 0.2776
Meta loss on this task batch = 2.8313e-01, Meta loss averaged over last 500 steps = 2.8292e-01, PNorm = 127.5237, GNorm = 0.2699
Meta loss on this task batch = 2.5816e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 127.5278, GNorm = 0.2623
Meta loss on this task batch = 2.7790e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 127.5308, GNorm = 0.3027
Meta loss on this task batch = 3.1532e-01, Meta loss averaged over last 500 steps = 2.8288e-01, PNorm = 127.5362, GNorm = 0.2900
Meta loss on this task batch = 2.7196e-01, Meta loss averaged over last 500 steps = 2.8292e-01, PNorm = 127.5405, GNorm = 0.2503
Meta loss on this task batch = 2.7031e-01, Meta loss averaged over last 500 steps = 2.8288e-01, PNorm = 127.5423, GNorm = 0.2871
Meta loss on this task batch = 2.7290e-01, Meta loss averaged over last 500 steps = 2.8284e-01, PNorm = 127.5419, GNorm = 0.2811
Meta loss on this task batch = 2.7368e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 127.5401, GNorm = 0.2768
Meta loss on this task batch = 2.6985e-01, Meta loss averaged over last 500 steps = 2.8282e-01, PNorm = 127.5391, GNorm = 0.2652
Meta loss on this task batch = 3.3411e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 127.5365, GNorm = 0.3620
Took 163.1724624633789 seconds to complete one epoch of meta training
Took 173.50483393669128 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471083
Epoch 565
Meta loss on this task batch = 3.2534e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 127.5342, GNorm = 0.3400
Meta loss on this task batch = 2.7742e-01, Meta loss averaged over last 500 steps = 2.8293e-01, PNorm = 127.5327, GNorm = 0.2909
Meta loss on this task batch = 2.2066e-01, Meta loss averaged over last 500 steps = 2.8281e-01, PNorm = 127.5340, GNorm = 0.2613
Meta loss on this task batch = 2.9177e-01, Meta loss averaged over last 500 steps = 2.8286e-01, PNorm = 127.5379, GNorm = 0.2720
Meta loss on this task batch = 2.6427e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 127.5434, GNorm = 0.2858
Meta loss on this task batch = 3.5235e-01, Meta loss averaged over last 500 steps = 2.8292e-01, PNorm = 127.5496, GNorm = 0.2976
Meta loss on this task batch = 2.9538e-01, Meta loss averaged over last 500 steps = 2.8297e-01, PNorm = 127.5568, GNorm = 0.2640
Meta loss on this task batch = 3.1918e-01, Meta loss averaged over last 500 steps = 2.8305e-01, PNorm = 127.5648, GNorm = 0.2687
Meta loss on this task batch = 2.8458e-01, Meta loss averaged over last 500 steps = 2.8312e-01, PNorm = 127.5734, GNorm = 0.3198
Meta loss on this task batch = 3.3348e-01, Meta loss averaged over last 500 steps = 2.8331e-01, PNorm = 127.5789, GNorm = 0.3162
Meta loss on this task batch = 2.3524e-01, Meta loss averaged over last 500 steps = 2.8327e-01, PNorm = 127.5849, GNorm = 0.2576
Meta loss on this task batch = 2.7102e-01, Meta loss averaged over last 500 steps = 2.8328e-01, PNorm = 127.5896, GNorm = 0.2904
Meta loss on this task batch = 2.3614e-01, Meta loss averaged over last 500 steps = 2.8314e-01, PNorm = 127.5949, GNorm = 0.2521
Meta loss on this task batch = 3.0743e-01, Meta loss averaged over last 500 steps = 2.8311e-01, PNorm = 127.5949, GNorm = 0.3487
Meta loss on this task batch = 2.3545e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 127.5965, GNorm = 0.2790
Meta loss on this task batch = 2.9931e-01, Meta loss averaged over last 500 steps = 2.8300e-01, PNorm = 127.5962, GNorm = 0.2883
Meta loss on this task batch = 3.2912e-01, Meta loss averaged over last 500 steps = 2.8311e-01, PNorm = 127.5929, GNorm = 0.3413
Meta loss on this task batch = 2.6716e-01, Meta loss averaged over last 500 steps = 2.8302e-01, PNorm = 127.5900, GNorm = 0.2627
Meta loss on this task batch = 2.5093e-01, Meta loss averaged over last 500 steps = 2.8289e-01, PNorm = 127.5879, GNorm = 0.3358
Took 159.3439290523529 seconds to complete one epoch of meta training
Took 169.14835286140442 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.519283
Epoch 566
Meta loss on this task batch = 2.8745e-01, Meta loss averaged over last 500 steps = 2.8295e-01, PNorm = 127.5886, GNorm = 0.2957
Meta loss on this task batch = 3.3181e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 127.5904, GNorm = 0.3234
Meta loss on this task batch = 3.6446e-01, Meta loss averaged over last 500 steps = 2.8329e-01, PNorm = 127.5932, GNorm = 0.3267
Meta loss on this task batch = 2.7073e-01, Meta loss averaged over last 500 steps = 2.8315e-01, PNorm = 127.5999, GNorm = 0.2818
Meta loss on this task batch = 2.1588e-01, Meta loss averaged over last 500 steps = 2.8294e-01, PNorm = 127.6087, GNorm = 0.2638
Meta loss on this task batch = 2.9101e-01, Meta loss averaged over last 500 steps = 2.8299e-01, PNorm = 127.6190, GNorm = 0.3086
Meta loss on this task batch = 2.9008e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 127.6291, GNorm = 0.2603
Meta loss on this task batch = 2.9317e-01, Meta loss averaged over last 500 steps = 2.8295e-01, PNorm = 127.6392, GNorm = 0.2934
Meta loss on this task batch = 2.8595e-01, Meta loss averaged over last 500 steps = 2.8290e-01, PNorm = 127.6490, GNorm = 0.2668
Meta loss on this task batch = 2.2345e-01, Meta loss averaged over last 500 steps = 2.8281e-01, PNorm = 127.6577, GNorm = 0.2882
Meta loss on this task batch = 2.9368e-01, Meta loss averaged over last 500 steps = 2.8276e-01, PNorm = 127.6639, GNorm = 0.3282
Meta loss on this task batch = 3.0643e-01, Meta loss averaged over last 500 steps = 2.8280e-01, PNorm = 127.6668, GNorm = 0.3215
Meta loss on this task batch = 2.8220e-01, Meta loss averaged over last 500 steps = 2.8268e-01, PNorm = 127.6690, GNorm = 0.3044
Meta loss on this task batch = 2.6900e-01, Meta loss averaged over last 500 steps = 2.8274e-01, PNorm = 127.6699, GNorm = 0.2810
Meta loss on this task batch = 3.1560e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 127.6708, GNorm = 0.3165
Meta loss on this task batch = 3.0855e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 127.6686, GNorm = 0.2915
Meta loss on this task batch = 3.1056e-01, Meta loss averaged over last 500 steps = 2.8280e-01, PNorm = 127.6667, GNorm = 0.3232
Meta loss on this task batch = 2.8134e-01, Meta loss averaged over last 500 steps = 2.8281e-01, PNorm = 127.6664, GNorm = 0.2995
Meta loss on this task batch = 2.6503e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 127.6716, GNorm = 0.3452
Took 159.23409390449524 seconds to complete one epoch of meta training
Took 168.57392859458923 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450419
Epoch 567
Meta loss on this task batch = 2.7349e-01, Meta loss averaged over last 500 steps = 2.8289e-01, PNorm = 127.6792, GNorm = 0.3055
Meta loss on this task batch = 2.7123e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 127.6884, GNorm = 0.2806
Meta loss on this task batch = 2.5459e-01, Meta loss averaged over last 500 steps = 2.8274e-01, PNorm = 127.6990, GNorm = 0.2578
Meta loss on this task batch = 2.5265e-01, Meta loss averaged over last 500 steps = 2.8274e-01, PNorm = 127.7099, GNorm = 0.2585
Meta loss on this task batch = 2.6500e-01, Meta loss averaged over last 500 steps = 2.8268e-01, PNorm = 127.7202, GNorm = 0.2765
Meta loss on this task batch = 2.9471e-01, Meta loss averaged over last 500 steps = 2.8273e-01, PNorm = 127.7318, GNorm = 0.3205
Meta loss on this task batch = 3.3067e-01, Meta loss averaged over last 500 steps = 2.8280e-01, PNorm = 127.7392, GNorm = 0.3328
Meta loss on this task batch = 3.3805e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 127.7477, GNorm = 0.3424
Meta loss on this task batch = 3.5617e-01, Meta loss averaged over last 500 steps = 2.8300e-01, PNorm = 127.7533, GNorm = 0.3790
Meta loss on this task batch = 3.1372e-01, Meta loss averaged over last 500 steps = 2.8295e-01, PNorm = 127.7583, GNorm = 0.3064
Meta loss on this task batch = 3.0117e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 127.7636, GNorm = 0.2884
Meta loss on this task batch = 2.7540e-01, Meta loss averaged over last 500 steps = 2.8310e-01, PNorm = 127.7671, GNorm = 0.3153
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 2.8317e-01, PNorm = 127.7698, GNorm = 0.3212
Meta loss on this task batch = 2.7255e-01, Meta loss averaged over last 500 steps = 2.8313e-01, PNorm = 127.7724, GNorm = 0.2736
Meta loss on this task batch = 2.2745e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 127.7764, GNorm = 0.2225
Meta loss on this task batch = 3.3789e-01, Meta loss averaged over last 500 steps = 2.8324e-01, PNorm = 127.7795, GNorm = 0.2664
Meta loss on this task batch = 2.2945e-01, Meta loss averaged over last 500 steps = 2.8310e-01, PNorm = 127.7823, GNorm = 0.2566
Meta loss on this task batch = 2.7979e-01, Meta loss averaged over last 500 steps = 2.8300e-01, PNorm = 127.7851, GNorm = 0.2753
Meta loss on this task batch = 2.8464e-01, Meta loss averaged over last 500 steps = 2.8297e-01, PNorm = 127.7857, GNorm = 0.3489
Took 159.44261956214905 seconds to complete one epoch of meta training
Took 169.84396839141846 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471599
Epoch 568
Meta loss on this task batch = 2.6420e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 127.7864, GNorm = 0.2835
Meta loss on this task batch = 2.7794e-01, Meta loss averaged over last 500 steps = 2.8299e-01, PNorm = 127.7860, GNorm = 0.2566
Meta loss on this task batch = 2.4005e-01, Meta loss averaged over last 500 steps = 2.8295e-01, PNorm = 127.7849, GNorm = 0.2517
Meta loss on this task batch = 2.9161e-01, Meta loss averaged over last 500 steps = 2.8298e-01, PNorm = 127.7865, GNorm = 0.2920
Meta loss on this task batch = 2.8595e-01, Meta loss averaged over last 500 steps = 2.8294e-01, PNorm = 127.7880, GNorm = 0.2384
Meta loss on this task batch = 2.8192e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 127.7904, GNorm = 0.2574
Meta loss on this task batch = 3.0416e-01, Meta loss averaged over last 500 steps = 2.8302e-01, PNorm = 127.7941, GNorm = 0.2742
Meta loss on this task batch = 3.0036e-01, Meta loss averaged over last 500 steps = 2.8316e-01, PNorm = 127.8001, GNorm = 0.2900
Meta loss on this task batch = 3.8357e-01, Meta loss averaged over last 500 steps = 2.8335e-01, PNorm = 127.8076, GNorm = 0.3269
Meta loss on this task batch = 2.7773e-01, Meta loss averaged over last 500 steps = 2.8340e-01, PNorm = 127.8157, GNorm = 0.2925
Meta loss on this task batch = 2.3200e-01, Meta loss averaged over last 500 steps = 2.8335e-01, PNorm = 127.8257, GNorm = 0.3022
Meta loss on this task batch = 2.7225e-01, Meta loss averaged over last 500 steps = 2.8334e-01, PNorm = 127.8367, GNorm = 0.2714
Meta loss on this task batch = 2.7127e-01, Meta loss averaged over last 500 steps = 2.8339e-01, PNorm = 127.8477, GNorm = 0.2619
Meta loss on this task batch = 2.6773e-01, Meta loss averaged over last 500 steps = 2.8346e-01, PNorm = 127.8576, GNorm = 0.3154
Meta loss on this task batch = 2.2849e-01, Meta loss averaged over last 500 steps = 2.8338e-01, PNorm = 127.8673, GNorm = 0.2775
Meta loss on this task batch = 2.4013e-01, Meta loss averaged over last 500 steps = 2.8326e-01, PNorm = 127.8754, GNorm = 0.2629
Meta loss on this task batch = 3.0298e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 127.8775, GNorm = 0.3800
Meta loss on this task batch = 3.8032e-01, Meta loss averaged over last 500 steps = 2.8330e-01, PNorm = 127.8774, GNorm = 0.3933
Meta loss on this task batch = 2.5537e-01, Meta loss averaged over last 500 steps = 2.8331e-01, PNorm = 127.8782, GNorm = 0.3367
Took 159.1326620578766 seconds to complete one epoch of meta training
Took 170.2819378376007 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465529
Epoch 569
Meta loss on this task batch = 3.1195e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 127.8788, GNorm = 0.2637
Meta loss on this task batch = 2.7255e-01, Meta loss averaged over last 500 steps = 2.8308e-01, PNorm = 127.8838, GNorm = 0.2885
Meta loss on this task batch = 3.0436e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 127.8892, GNorm = 0.3383
Meta loss on this task batch = 2.8675e-01, Meta loss averaged over last 500 steps = 2.8311e-01, PNorm = 127.8964, GNorm = 0.3261
Meta loss on this task batch = 2.7255e-01, Meta loss averaged over last 500 steps = 2.8309e-01, PNorm = 127.9053, GNorm = 0.2747
Meta loss on this task batch = 2.7807e-01, Meta loss averaged over last 500 steps = 2.8315e-01, PNorm = 127.9169, GNorm = 0.2884
Meta loss on this task batch = 2.5742e-01, Meta loss averaged over last 500 steps = 2.8310e-01, PNorm = 127.9280, GNorm = 0.3066
Meta loss on this task batch = 2.3127e-01, Meta loss averaged over last 500 steps = 2.8297e-01, PNorm = 127.9391, GNorm = 0.2620
Meta loss on this task batch = 2.5810e-01, Meta loss averaged over last 500 steps = 2.8291e-01, PNorm = 127.9497, GNorm = 0.3036
Meta loss on this task batch = 3.0560e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 127.9603, GNorm = 0.3239
Meta loss on this task batch = 3.0729e-01, Meta loss averaged over last 500 steps = 2.8319e-01, PNorm = 127.9686, GNorm = 0.3193
Meta loss on this task batch = 3.0686e-01, Meta loss averaged over last 500 steps = 2.8325e-01, PNorm = 127.9741, GNorm = 0.3326
Meta loss on this task batch = 3.2437e-01, Meta loss averaged over last 500 steps = 2.8326e-01, PNorm = 127.9765, GNorm = 0.3323
Meta loss on this task batch = 2.6204e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 127.9761, GNorm = 0.3126
Meta loss on this task batch = 2.7716e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 127.9752, GNorm = 0.3022
Meta loss on this task batch = 2.8853e-01, Meta loss averaged over last 500 steps = 2.8328e-01, PNorm = 127.9728, GNorm = 0.3355
Meta loss on this task batch = 2.8611e-01, Meta loss averaged over last 500 steps = 2.8324e-01, PNorm = 127.9709, GNorm = 0.3154
Meta loss on this task batch = 2.9014e-01, Meta loss averaged over last 500 steps = 2.8327e-01, PNorm = 127.9662, GNorm = 0.3594
Meta loss on this task batch = 2.6332e-01, Meta loss averaged over last 500 steps = 2.8321e-01, PNorm = 127.9601, GNorm = 0.4963
Took 157.93480849266052 seconds to complete one epoch of meta training
Took 169.28229117393494 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467499
Epoch 570
Meta loss on this task batch = 3.2836e-01, Meta loss averaged over last 500 steps = 2.8327e-01, PNorm = 127.9554, GNorm = 0.3302
Meta loss on this task batch = 3.0813e-01, Meta loss averaged over last 500 steps = 2.8325e-01, PNorm = 127.9499, GNorm = 0.3335
Meta loss on this task batch = 3.0109e-01, Meta loss averaged over last 500 steps = 2.8331e-01, PNorm = 127.9496, GNorm = 0.3020
Meta loss on this task batch = 2.4844e-01, Meta loss averaged over last 500 steps = 2.8327e-01, PNorm = 127.9527, GNorm = 0.2975
Meta loss on this task batch = 2.5289e-01, Meta loss averaged over last 500 steps = 2.8315e-01, PNorm = 127.9612, GNorm = 0.2997
Meta loss on this task batch = 2.3596e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 127.9716, GNorm = 0.3022
Meta loss on this task batch = 3.0402e-01, Meta loss averaged over last 500 steps = 2.8308e-01, PNorm = 127.9858, GNorm = 0.3098
Meta loss on this task batch = 2.7409e-01, Meta loss averaged over last 500 steps = 2.8314e-01, PNorm = 128.0015, GNorm = 0.2542
Meta loss on this task batch = 3.0840e-01, Meta loss averaged over last 500 steps = 2.8328e-01, PNorm = 128.0157, GNorm = 0.3598
Meta loss on this task batch = 3.0821e-01, Meta loss averaged over last 500 steps = 2.8339e-01, PNorm = 128.0281, GNorm = 0.2997
Meta loss on this task batch = 2.5730e-01, Meta loss averaged over last 500 steps = 2.8331e-01, PNorm = 128.0383, GNorm = 0.2808
Meta loss on this task batch = 2.7296e-01, Meta loss averaged over last 500 steps = 2.8334e-01, PNorm = 128.0456, GNorm = 0.2942
Meta loss on this task batch = 2.8518e-01, Meta loss averaged over last 500 steps = 2.8330e-01, PNorm = 128.0493, GNorm = 0.3067
Meta loss on this task batch = 2.4745e-01, Meta loss averaged over last 500 steps = 2.8325e-01, PNorm = 128.0515, GNorm = 0.2519
Meta loss on this task batch = 2.8135e-01, Meta loss averaged over last 500 steps = 2.8318e-01, PNorm = 128.0541, GNorm = 0.2857
Meta loss on this task batch = 2.7272e-01, Meta loss averaged over last 500 steps = 2.8325e-01, PNorm = 128.0541, GNorm = 0.3092
Meta loss on this task batch = 3.1708e-01, Meta loss averaged over last 500 steps = 2.8326e-01, PNorm = 128.0549, GNorm = 0.3121
Meta loss on this task batch = 2.8958e-01, Meta loss averaged over last 500 steps = 2.8328e-01, PNorm = 128.0537, GNorm = 0.2700
Meta loss on this task batch = 2.8941e-01, Meta loss averaged over last 500 steps = 2.8333e-01, PNorm = 128.0522, GNorm = 0.3286
Took 156.2318937778473 seconds to complete one epoch of meta training
Took 166.1599142551422 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461823
Epoch 571
Meta loss on this task batch = 2.9445e-01, Meta loss averaged over last 500 steps = 2.8321e-01, PNorm = 128.0523, GNorm = 0.2848
Meta loss on this task batch = 3.7060e-01, Meta loss averaged over last 500 steps = 2.8341e-01, PNorm = 128.0492, GNorm = 0.3467
Meta loss on this task batch = 2.9053e-01, Meta loss averaged over last 500 steps = 2.8332e-01, PNorm = 128.0462, GNorm = 0.2763
Meta loss on this task batch = 2.0748e-01, Meta loss averaged over last 500 steps = 2.8324e-01, PNorm = 128.0455, GNorm = 0.2396
Meta loss on this task batch = 2.4173e-01, Meta loss averaged over last 500 steps = 2.8314e-01, PNorm = 128.0485, GNorm = 0.2617
Meta loss on this task batch = 2.5817e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 128.0528, GNorm = 0.2542
Meta loss on this task batch = 3.1724e-01, Meta loss averaged over last 500 steps = 2.8311e-01, PNorm = 128.0592, GNorm = 0.2761
Meta loss on this task batch = 2.6213e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 128.0663, GNorm = 0.2712
Meta loss on this task batch = 2.5723e-01, Meta loss averaged over last 500 steps = 2.8300e-01, PNorm = 128.0744, GNorm = 0.2392
Meta loss on this task batch = 3.2349e-01, Meta loss averaged over last 500 steps = 2.8309e-01, PNorm = 128.0820, GNorm = 0.3167
Meta loss on this task batch = 2.7051e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 128.0905, GNorm = 0.2832
Meta loss on this task batch = 2.8434e-01, Meta loss averaged over last 500 steps = 2.8298e-01, PNorm = 128.0960, GNorm = 0.3300
Meta loss on this task batch = 3.0062e-01, Meta loss averaged over last 500 steps = 2.8297e-01, PNorm = 128.0964, GNorm = 0.3317
Meta loss on this task batch = 2.7175e-01, Meta loss averaged over last 500 steps = 2.8298e-01, PNorm = 128.0972, GNorm = 0.2759
Meta loss on this task batch = 3.3573e-01, Meta loss averaged over last 500 steps = 2.8305e-01, PNorm = 128.0975, GNorm = 0.2767
Meta loss on this task batch = 2.8050e-01, Meta loss averaged over last 500 steps = 2.8311e-01, PNorm = 128.0985, GNorm = 0.3040
Meta loss on this task batch = 2.4714e-01, Meta loss averaged over last 500 steps = 2.8316e-01, PNorm = 128.1005, GNorm = 0.2885
Meta loss on this task batch = 2.6800e-01, Meta loss averaged over last 500 steps = 2.8321e-01, PNorm = 128.1019, GNorm = 0.2742
Meta loss on this task batch = 2.6579e-01, Meta loss averaged over last 500 steps = 2.8320e-01, PNorm = 128.1040, GNorm = 0.2935
Took 167.84053802490234 seconds to complete one epoch of meta training
Took 178.7301950454712 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481593
Epoch 572
Meta loss on this task batch = 2.4253e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 128.1090, GNorm = 0.2708
Meta loss on this task batch = 3.0232e-01, Meta loss averaged over last 500 steps = 2.8321e-01, PNorm = 128.1131, GNorm = 0.3010
Meta loss on this task batch = 3.3435e-01, Meta loss averaged over last 500 steps = 2.8335e-01, PNorm = 128.1191, GNorm = 0.3583
Meta loss on this task batch = 2.6138e-01, Meta loss averaged over last 500 steps = 2.8324e-01, PNorm = 128.1255, GNorm = 0.2738
Meta loss on this task batch = 3.0650e-01, Meta loss averaged over last 500 steps = 2.8327e-01, PNorm = 128.1319, GNorm = 0.2832
Meta loss on this task batch = 2.1202e-01, Meta loss averaged over last 500 steps = 2.8306e-01, PNorm = 128.1390, GNorm = 0.2819
Meta loss on this task batch = 2.6841e-01, Meta loss averaged over last 500 steps = 2.8300e-01, PNorm = 128.1437, GNorm = 0.2746
Meta loss on this task batch = 3.1195e-01, Meta loss averaged over last 500 steps = 2.8302e-01, PNorm = 128.1486, GNorm = 0.2992
Meta loss on this task batch = 3.3389e-01, Meta loss averaged over last 500 steps = 2.8314e-01, PNorm = 128.1516, GNorm = 0.3086
Meta loss on this task batch = 2.4960e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 128.1530, GNorm = 0.2874
Meta loss on this task batch = 2.5527e-01, Meta loss averaged over last 500 steps = 2.8300e-01, PNorm = 128.1563, GNorm = 0.2718
Meta loss on this task batch = 2.5511e-01, Meta loss averaged over last 500 steps = 2.8302e-01, PNorm = 128.1585, GNorm = 0.2529
Meta loss on this task batch = 2.9562e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 128.1621, GNorm = 0.2893
Meta loss on this task batch = 3.0792e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 128.1659, GNorm = 0.2943
Meta loss on this task batch = 2.9134e-01, Meta loss averaged over last 500 steps = 2.8328e-01, PNorm = 128.1699, GNorm = 0.3028
Meta loss on this task batch = 2.6645e-01, Meta loss averaged over last 500 steps = 2.8326e-01, PNorm = 128.1763, GNorm = 0.2819
Meta loss on this task batch = 3.1595e-01, Meta loss averaged over last 500 steps = 2.8326e-01, PNorm = 128.1823, GNorm = 0.2851
Meta loss on this task batch = 2.8174e-01, Meta loss averaged over last 500 steps = 2.8327e-01, PNorm = 128.1878, GNorm = 0.2902
Meta loss on this task batch = 2.6163e-01, Meta loss averaged over last 500 steps = 2.8324e-01, PNorm = 128.1938, GNorm = 0.3527
Took 163.19585275650024 seconds to complete one epoch of meta training
Took 175.64260983467102 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494415
Epoch 573
Meta loss on this task batch = 2.8497e-01, Meta loss averaged over last 500 steps = 2.8328e-01, PNorm = 128.2000, GNorm = 0.2815
Meta loss on this task batch = 2.5409e-01, Meta loss averaged over last 500 steps = 2.8317e-01, PNorm = 128.2044, GNorm = 0.2529
Meta loss on this task batch = 2.7627e-01, Meta loss averaged over last 500 steps = 2.8320e-01, PNorm = 128.2096, GNorm = 0.2913
Meta loss on this task batch = 2.8424e-01, Meta loss averaged over last 500 steps = 2.8318e-01, PNorm = 128.2146, GNorm = 0.2647
Meta loss on this task batch = 2.6170e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 128.2208, GNorm = 0.2730
Meta loss on this task batch = 2.9189e-01, Meta loss averaged over last 500 steps = 2.8302e-01, PNorm = 128.2286, GNorm = 0.3177
Meta loss on this task batch = 3.6240e-01, Meta loss averaged over last 500 steps = 2.8329e-01, PNorm = 128.2350, GNorm = 0.3329
Meta loss on this task batch = 3.3066e-01, Meta loss averaged over last 500 steps = 2.8335e-01, PNorm = 128.2390, GNorm = 0.2901
Meta loss on this task batch = 2.5570e-01, Meta loss averaged over last 500 steps = 2.8334e-01, PNorm = 128.2438, GNorm = 0.2918
Meta loss on this task batch = 2.4725e-01, Meta loss averaged over last 500 steps = 2.8327e-01, PNorm = 128.2489, GNorm = 0.2352
Meta loss on this task batch = 2.5519e-01, Meta loss averaged over last 500 steps = 2.8324e-01, PNorm = 128.2547, GNorm = 0.2822
Meta loss on this task batch = 2.8872e-01, Meta loss averaged over last 500 steps = 2.8321e-01, PNorm = 128.2618, GNorm = 0.2962
Meta loss on this task batch = 2.5786e-01, Meta loss averaged over last 500 steps = 2.8320e-01, PNorm = 128.2706, GNorm = 0.2927
Meta loss on this task batch = 2.7848e-01, Meta loss averaged over last 500 steps = 2.8308e-01, PNorm = 128.2785, GNorm = 0.2870
Meta loss on this task batch = 2.7431e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 128.2876, GNorm = 0.2859
Meta loss on this task batch = 2.8811e-01, Meta loss averaged over last 500 steps = 2.8299e-01, PNorm = 128.2950, GNorm = 0.3123
Meta loss on this task batch = 1.9885e-01, Meta loss averaged over last 500 steps = 2.8278e-01, PNorm = 128.3035, GNorm = 0.2543
Meta loss on this task batch = 2.6604e-01, Meta loss averaged over last 500 steps = 2.8274e-01, PNorm = 128.3117, GNorm = 0.3038
Meta loss on this task batch = 2.3921e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 128.3207, GNorm = 0.3381
Took 163.26078510284424 seconds to complete one epoch of meta training
Took 172.80620646476746 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502483
Epoch 574
Meta loss on this task batch = 2.8153e-01, Meta loss averaged over last 500 steps = 2.8281e-01, PNorm = 128.3280, GNorm = 0.3312
Meta loss on this task batch = 2.7156e-01, Meta loss averaged over last 500 steps = 2.8282e-01, PNorm = 128.3364, GNorm = 0.2851
Meta loss on this task batch = 2.5222e-01, Meta loss averaged over last 500 steps = 2.8290e-01, PNorm = 128.3437, GNorm = 0.2819
Meta loss on this task batch = 2.9519e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 128.3489, GNorm = 0.3397
Meta loss on this task batch = 2.9840e-01, Meta loss averaged over last 500 steps = 2.8275e-01, PNorm = 128.3544, GNorm = 0.2998
Meta loss on this task batch = 2.7593e-01, Meta loss averaged over last 500 steps = 2.8278e-01, PNorm = 128.3577, GNorm = 0.2691
Meta loss on this task batch = 2.7876e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 128.3590, GNorm = 0.3034
Meta loss on this task batch = 2.5060e-01, Meta loss averaged over last 500 steps = 2.8278e-01, PNorm = 128.3607, GNorm = 0.2836
Meta loss on this task batch = 2.9846e-01, Meta loss averaged over last 500 steps = 2.8282e-01, PNorm = 128.3609, GNorm = 0.3001
Meta loss on this task batch = 3.0678e-01, Meta loss averaged over last 500 steps = 2.8284e-01, PNorm = 128.3614, GNorm = 0.3411
Meta loss on this task batch = 3.2507e-01, Meta loss averaged over last 500 steps = 2.8284e-01, PNorm = 128.3636, GNorm = 0.3005
Meta loss on this task batch = 2.9184e-01, Meta loss averaged over last 500 steps = 2.8295e-01, PNorm = 128.3643, GNorm = 0.3220
Meta loss on this task batch = 2.8480e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 128.3633, GNorm = 0.3084
Meta loss on this task batch = 3.2102e-01, Meta loss averaged over last 500 steps = 2.8315e-01, PNorm = 128.3606, GNorm = 0.3929
Meta loss on this task batch = 3.3962e-01, Meta loss averaged over last 500 steps = 2.8336e-01, PNorm = 128.3570, GNorm = 0.3231
Meta loss on this task batch = 2.6942e-01, Meta loss averaged over last 500 steps = 2.8332e-01, PNorm = 128.3561, GNorm = 0.2764
Meta loss on this task batch = 2.8926e-01, Meta loss averaged over last 500 steps = 2.8327e-01, PNorm = 128.3559, GNorm = 0.3104
Meta loss on this task batch = 2.6396e-01, Meta loss averaged over last 500 steps = 2.8319e-01, PNorm = 128.3574, GNorm = 0.2633
Meta loss on this task batch = 2.9088e-01, Meta loss averaged over last 500 steps = 2.8310e-01, PNorm = 128.3576, GNorm = 0.3147
Took 167.3143858909607 seconds to complete one epoch of meta training
Took 176.8525698184967 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502292
Epoch 575
Meta loss on this task batch = 3.1124e-01, Meta loss averaged over last 500 steps = 2.8314e-01, PNorm = 128.3564, GNorm = 0.2912
Meta loss on this task batch = 2.5781e-01, Meta loss averaged over last 500 steps = 2.8312e-01, PNorm = 128.3574, GNorm = 0.2349
Meta loss on this task batch = 3.0319e-01, Meta loss averaged over last 500 steps = 2.8321e-01, PNorm = 128.3599, GNorm = 0.3123
Meta loss on this task batch = 2.6179e-01, Meta loss averaged over last 500 steps = 2.8312e-01, PNorm = 128.3639, GNorm = 0.2420
Meta loss on this task batch = 2.0982e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 128.3705, GNorm = 0.2913
Meta loss on this task batch = 2.4164e-01, Meta loss averaged over last 500 steps = 2.8289e-01, PNorm = 128.3756, GNorm = 0.2590
Meta loss on this task batch = 2.6816e-01, Meta loss averaged over last 500 steps = 2.8284e-01, PNorm = 128.3807, GNorm = 0.2543
Meta loss on this task batch = 2.9110e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 128.3850, GNorm = 0.3005
Meta loss on this task batch = 2.8615e-01, Meta loss averaged over last 500 steps = 2.8269e-01, PNorm = 128.3882, GNorm = 0.2692
Meta loss on this task batch = 3.0506e-01, Meta loss averaged over last 500 steps = 2.8275e-01, PNorm = 128.3900, GNorm = 0.3046
Meta loss on this task batch = 2.8492e-01, Meta loss averaged over last 500 steps = 2.8275e-01, PNorm = 128.3910, GNorm = 0.3121
Meta loss on this task batch = 3.4239e-01, Meta loss averaged over last 500 steps = 2.8284e-01, PNorm = 128.3921, GNorm = 0.3042
Meta loss on this task batch = 2.8911e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 128.3940, GNorm = 0.2695
Meta loss on this task batch = 3.1861e-01, Meta loss averaged over last 500 steps = 2.8298e-01, PNorm = 128.3962, GNorm = 0.3230
Meta loss on this task batch = 2.8988e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 128.4016, GNorm = 0.3001
Meta loss on this task batch = 2.5805e-01, Meta loss averaged over last 500 steps = 2.8294e-01, PNorm = 128.4075, GNorm = 0.2586
Meta loss on this task batch = 2.2263e-01, Meta loss averaged over last 500 steps = 2.8271e-01, PNorm = 128.4167, GNorm = 0.2498
Meta loss on this task batch = 2.6240e-01, Meta loss averaged over last 500 steps = 2.8268e-01, PNorm = 128.4233, GNorm = 0.2899
Meta loss on this task batch = 2.7984e-01, Meta loss averaged over last 500 steps = 2.8272e-01, PNorm = 128.4320, GNorm = 0.3020
Took 162.2767572402954 seconds to complete one epoch of meta training
Took 172.5241997241974 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484553
Epoch 576
Meta loss on this task batch = 2.7552e-01, Meta loss averaged over last 500 steps = 2.8276e-01, PNorm = 128.4402, GNorm = 0.3143
Meta loss on this task batch = 3.3477e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 128.4467, GNorm = 0.3270
Meta loss on this task batch = 2.1538e-01, Meta loss averaged over last 500 steps = 2.8275e-01, PNorm = 128.4542, GNorm = 0.2646
Meta loss on this task batch = 2.9925e-01, Meta loss averaged over last 500 steps = 2.8285e-01, PNorm = 128.4604, GNorm = 0.3399
Meta loss on this task batch = 3.2275e-01, Meta loss averaged over last 500 steps = 2.8305e-01, PNorm = 128.4654, GNorm = 0.3158
Meta loss on this task batch = 2.8530e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 128.4710, GNorm = 0.2878
Meta loss on this task batch = 3.1230e-01, Meta loss averaged over last 500 steps = 2.8311e-01, PNorm = 128.4776, GNorm = 0.3207
Meta loss on this task batch = 2.7440e-01, Meta loss averaged over last 500 steps = 2.8315e-01, PNorm = 128.4860, GNorm = 0.2577
Meta loss on this task batch = 2.5840e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 128.4938, GNorm = 0.2832
Meta loss on this task batch = 2.7941e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 128.4978, GNorm = 0.2943
Meta loss on this task batch = 2.9821e-01, Meta loss averaged over last 500 steps = 2.8308e-01, PNorm = 128.5019, GNorm = 0.3031
Meta loss on this task batch = 2.8152e-01, Meta loss averaged over last 500 steps = 2.8310e-01, PNorm = 128.5052, GNorm = 0.2911
Meta loss on this task batch = 2.8050e-01, Meta loss averaged over last 500 steps = 2.8285e-01, PNorm = 128.5085, GNorm = 0.2974
Meta loss on this task batch = 2.8784e-01, Meta loss averaged over last 500 steps = 2.8305e-01, PNorm = 128.5098, GNorm = 0.3629
Meta loss on this task batch = 2.7051e-01, Meta loss averaged over last 500 steps = 2.8308e-01, PNorm = 128.5133, GNorm = 0.3025
Meta loss on this task batch = 3.2579e-01, Meta loss averaged over last 500 steps = 2.8321e-01, PNorm = 128.5169, GNorm = 0.2811
Meta loss on this task batch = 2.2856e-01, Meta loss averaged over last 500 steps = 2.8305e-01, PNorm = 128.5216, GNorm = 0.2881
Meta loss on this task batch = 2.7237e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 128.5258, GNorm = 0.2857
Meta loss on this task batch = 2.7998e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 128.5277, GNorm = 0.2987
Took 159.45674967765808 seconds to complete one epoch of meta training
Took 169.65631794929504 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476101
Epoch 577
Meta loss on this task batch = 2.6277e-01, Meta loss averaged over last 500 steps = 2.8291e-01, PNorm = 128.5307, GNorm = 0.2591
Meta loss on this task batch = 2.9056e-01, Meta loss averaged over last 500 steps = 2.8297e-01, PNorm = 128.5352, GNorm = 0.3236
Meta loss on this task batch = 2.9238e-01, Meta loss averaged over last 500 steps = 2.8309e-01, PNorm = 128.5394, GNorm = 0.3046
Meta loss on this task batch = 2.8392e-01, Meta loss averaged over last 500 steps = 2.8305e-01, PNorm = 128.5437, GNorm = 0.2925
Meta loss on this task batch = 2.7072e-01, Meta loss averaged over last 500 steps = 2.8311e-01, PNorm = 128.5473, GNorm = 0.2763
Meta loss on this task batch = 2.7229e-01, Meta loss averaged over last 500 steps = 2.8306e-01, PNorm = 128.5499, GNorm = 0.2900
Meta loss on this task batch = 2.7635e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 128.5518, GNorm = 0.2853
Meta loss on this task batch = 2.3463e-01, Meta loss averaged over last 500 steps = 2.8293e-01, PNorm = 128.5541, GNorm = 0.2621
Meta loss on this task batch = 2.9708e-01, Meta loss averaged over last 500 steps = 2.8305e-01, PNorm = 128.5565, GNorm = 0.2675
Meta loss on this task batch = 2.6234e-01, Meta loss averaged over last 500 steps = 2.8298e-01, PNorm = 128.5543, GNorm = 0.3357
Meta loss on this task batch = 2.6439e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 128.5528, GNorm = 0.2911
Meta loss on this task batch = 3.1703e-01, Meta loss averaged over last 500 steps = 2.8293e-01, PNorm = 128.5514, GNorm = 0.3300
Meta loss on this task batch = 2.8000e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 128.5517, GNorm = 0.3084
Meta loss on this task batch = 3.1312e-01, Meta loss averaged over last 500 steps = 2.8299e-01, PNorm = 128.5521, GNorm = 0.2860
Meta loss on this task batch = 3.0463e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 128.5545, GNorm = 0.2682
Meta loss on this task batch = 2.7602e-01, Meta loss averaged over last 500 steps = 2.8300e-01, PNorm = 128.5597, GNorm = 0.3170
Meta loss on this task batch = 2.2545e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 128.5673, GNorm = 0.2572
Meta loss on this task batch = 3.1122e-01, Meta loss averaged over last 500 steps = 2.8293e-01, PNorm = 128.5737, GNorm = 0.2900
Meta loss on this task batch = 3.2100e-01, Meta loss averaged over last 500 steps = 2.8311e-01, PNorm = 128.5826, GNorm = 0.3550
Took 158.2236008644104 seconds to complete one epoch of meta training
Took 168.1030089855194 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508104
Epoch 578
Meta loss on this task batch = 2.8900e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 128.5906, GNorm = 0.2685
Meta loss on this task batch = 2.4329e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 128.5993, GNorm = 0.2380
Meta loss on this task batch = 2.5972e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 128.6090, GNorm = 0.2683
Meta loss on this task batch = 2.6902e-01, Meta loss averaged over last 500 steps = 2.8276e-01, PNorm = 128.6168, GNorm = 0.2929
Meta loss on this task batch = 3.0198e-01, Meta loss averaged over last 500 steps = 2.8286e-01, PNorm = 128.6223, GNorm = 0.2790
Meta loss on this task batch = 2.7935e-01, Meta loss averaged over last 500 steps = 2.8289e-01, PNorm = 128.6268, GNorm = 0.2892
Meta loss on this task batch = 2.6553e-01, Meta loss averaged over last 500 steps = 2.8278e-01, PNorm = 128.6309, GNorm = 0.2766
Meta loss on this task batch = 3.0136e-01, Meta loss averaged over last 500 steps = 2.8289e-01, PNorm = 128.6333, GNorm = 0.3499
Meta loss on this task batch = 2.5555e-01, Meta loss averaged over last 500 steps = 2.8280e-01, PNorm = 128.6349, GNorm = 0.2341
Meta loss on this task batch = 3.0214e-01, Meta loss averaged over last 500 steps = 2.8291e-01, PNorm = 128.6354, GNorm = 0.2890
Meta loss on this task batch = 2.9029e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 128.6396, GNorm = 0.3054
Meta loss on this task batch = 3.1629e-01, Meta loss averaged over last 500 steps = 2.8306e-01, PNorm = 128.6439, GNorm = 0.2853
Meta loss on this task batch = 2.3631e-01, Meta loss averaged over last 500 steps = 2.8294e-01, PNorm = 128.6478, GNorm = 0.2583
Meta loss on this task batch = 2.6657e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 128.6529, GNorm = 0.2952
Meta loss on this task batch = 2.4011e-01, Meta loss averaged over last 500 steps = 2.8289e-01, PNorm = 128.6567, GNorm = 0.2492
Meta loss on this task batch = 2.6514e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 128.6614, GNorm = 0.2696
Meta loss on this task batch = 3.1737e-01, Meta loss averaged over last 500 steps = 2.8293e-01, PNorm = 128.6643, GNorm = 0.3390
Meta loss on this task batch = 2.7961e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 128.6685, GNorm = 0.3146
Meta loss on this task batch = 2.7453e-01, Meta loss averaged over last 500 steps = 2.8297e-01, PNorm = 128.6734, GNorm = 0.3831
Took 160.0625810623169 seconds to complete one epoch of meta training
Took 171.21391034126282 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481205
Epoch 579
Meta loss on this task batch = 2.2827e-01, Meta loss averaged over last 500 steps = 2.8289e-01, PNorm = 128.6783, GNorm = 0.3339
Meta loss on this task batch = 3.1932e-01, Meta loss averaged over last 500 steps = 2.8302e-01, PNorm = 128.6819, GNorm = 0.3394
Meta loss on this task batch = 2.7960e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 128.6850, GNorm = 0.3149
Meta loss on this task batch = 2.7963e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 128.6858, GNorm = 0.3263
Meta loss on this task batch = 2.9375e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 128.6879, GNorm = 0.2751
Meta loss on this task batch = 2.9857e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 128.6899, GNorm = 0.3173
Meta loss on this task batch = 2.6273e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 128.6940, GNorm = 0.2931
Meta loss on this task batch = 2.6211e-01, Meta loss averaged over last 500 steps = 2.8271e-01, PNorm = 128.7006, GNorm = 0.2957
Meta loss on this task batch = 2.5686e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 128.7093, GNorm = 0.2564
Meta loss on this task batch = 3.0409e-01, Meta loss averaged over last 500 steps = 2.8280e-01, PNorm = 128.7200, GNorm = 0.2885
Meta loss on this task batch = 2.8546e-01, Meta loss averaged over last 500 steps = 2.8274e-01, PNorm = 128.7332, GNorm = 0.2925
Meta loss on this task batch = 2.8253e-01, Meta loss averaged over last 500 steps = 2.8285e-01, PNorm = 128.7454, GNorm = 0.2906
Meta loss on this task batch = 3.0232e-01, Meta loss averaged over last 500 steps = 2.8298e-01, PNorm = 128.7557, GNorm = 0.3492
Meta loss on this task batch = 2.7906e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 128.7658, GNorm = 0.2885
Meta loss on this task batch = 2.7689e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 128.7757, GNorm = 0.2692
Meta loss on this task batch = 2.9137e-01, Meta loss averaged over last 500 steps = 2.8281e-01, PNorm = 128.7836, GNorm = 0.3063
Meta loss on this task batch = 2.5328e-01, Meta loss averaged over last 500 steps = 2.8281e-01, PNorm = 128.7918, GNorm = 0.2613
Meta loss on this task batch = 2.4243e-01, Meta loss averaged over last 500 steps = 2.8272e-01, PNorm = 128.7995, GNorm = 0.3038
Meta loss on this task batch = 3.1101e-01, Meta loss averaged over last 500 steps = 2.8285e-01, PNorm = 128.8040, GNorm = 0.4308
Took 157.34043908119202 seconds to complete one epoch of meta training
Took 167.27844262123108 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489863
Epoch 580
Meta loss on this task batch = 2.9038e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 128.8084, GNorm = 0.2874
Meta loss on this task batch = 3.1284e-01, Meta loss averaged over last 500 steps = 2.8290e-01, PNorm = 128.8117, GNorm = 0.3593
Meta loss on this task batch = 2.8716e-01, Meta loss averaged over last 500 steps = 2.8293e-01, PNorm = 128.8146, GNorm = 0.3146
Meta loss on this task batch = 2.1812e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 128.8182, GNorm = 0.2668
Meta loss on this task batch = 2.2809e-01, Meta loss averaged over last 500 steps = 2.8267e-01, PNorm = 128.8213, GNorm = 0.2581
Meta loss on this task batch = 2.8005e-01, Meta loss averaged over last 500 steps = 2.8262e-01, PNorm = 128.8240, GNorm = 0.3222
Meta loss on this task batch = 3.4604e-01, Meta loss averaged over last 500 steps = 2.8292e-01, PNorm = 128.8266, GNorm = 0.3283
Meta loss on this task batch = 2.4713e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 128.8322, GNorm = 0.2858
Meta loss on this task batch = 3.0601e-01, Meta loss averaged over last 500 steps = 2.8281e-01, PNorm = 128.8419, GNorm = 0.3621
Meta loss on this task batch = 2.7618e-01, Meta loss averaged over last 500 steps = 2.8282e-01, PNorm = 128.8535, GNorm = 0.3595
Meta loss on this task batch = 2.7338e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 128.8655, GNorm = 0.2835
Meta loss on this task batch = 2.4876e-01, Meta loss averaged over last 500 steps = 2.8278e-01, PNorm = 128.8767, GNorm = 0.2867
Meta loss on this task batch = 2.5773e-01, Meta loss averaged over last 500 steps = 2.8275e-01, PNorm = 128.8859, GNorm = 0.2918
Meta loss on this task batch = 3.5614e-01, Meta loss averaged over last 500 steps = 2.8295e-01, PNorm = 128.8946, GNorm = 0.3555
Meta loss on this task batch = 2.7477e-01, Meta loss averaged over last 500 steps = 2.8302e-01, PNorm = 128.9003, GNorm = 0.3025
Meta loss on this task batch = 2.3808e-01, Meta loss averaged over last 500 steps = 2.8280e-01, PNorm = 128.9060, GNorm = 0.2876
Meta loss on this task batch = 2.9143e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 128.9093, GNorm = 0.3557
Meta loss on this task batch = 2.5830e-01, Meta loss averaged over last 500 steps = 2.8270e-01, PNorm = 128.9125, GNorm = 0.2768
Meta loss on this task batch = 3.1428e-01, Meta loss averaged over last 500 steps = 2.8273e-01, PNorm = 128.9130, GNorm = 0.4170
Took 156.59743118286133 seconds to complete one epoch of meta training
Took 166.662912607193 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453054
Epoch 581
Meta loss on this task batch = 2.5430e-01, Meta loss averaged over last 500 steps = 2.8269e-01, PNorm = 128.9116, GNorm = 0.2878
Meta loss on this task batch = 2.8722e-01, Meta loss averaged over last 500 steps = 2.8266e-01, PNorm = 128.9118, GNorm = 0.2797
Meta loss on this task batch = 2.7212e-01, Meta loss averaged over last 500 steps = 2.8265e-01, PNorm = 128.9141, GNorm = 0.3120
Meta loss on this task batch = 3.1835e-01, Meta loss averaged over last 500 steps = 2.8270e-01, PNorm = 128.9167, GNorm = 0.3347
Meta loss on this task batch = 2.9668e-01, Meta loss averaged over last 500 steps = 2.8277e-01, PNorm = 128.9211, GNorm = 0.2744
Meta loss on this task batch = 2.7984e-01, Meta loss averaged over last 500 steps = 2.8266e-01, PNorm = 128.9263, GNorm = 0.2930
Meta loss on this task batch = 3.0026e-01, Meta loss averaged over last 500 steps = 2.8267e-01, PNorm = 128.9292, GNorm = 0.3112
Meta loss on this task batch = 3.0923e-01, Meta loss averaged over last 500 steps = 2.8272e-01, PNorm = 128.9331, GNorm = 0.2821
Meta loss on this task batch = 3.3236e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 128.9377, GNorm = 0.2892
Meta loss on this task batch = 3.4097e-01, Meta loss averaged over last 500 steps = 2.8291e-01, PNorm = 128.9410, GNorm = 0.3126
Meta loss on this task batch = 2.3753e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 128.9451, GNorm = 0.2757
Meta loss on this task batch = 2.1364e-01, Meta loss averaged over last 500 steps = 2.8267e-01, PNorm = 128.9495, GNorm = 0.2581
Meta loss on this task batch = 2.7156e-01, Meta loss averaged over last 500 steps = 2.8265e-01, PNorm = 128.9551, GNorm = 0.2842
Meta loss on this task batch = 2.8903e-01, Meta loss averaged over last 500 steps = 2.8270e-01, PNorm = 128.9632, GNorm = 0.2998
Meta loss on this task batch = 2.8236e-01, Meta loss averaged over last 500 steps = 2.8278e-01, PNorm = 128.9697, GNorm = 0.2909
Meta loss on this task batch = 2.3065e-01, Meta loss averaged over last 500 steps = 2.8276e-01, PNorm = 128.9748, GNorm = 0.2575
Meta loss on this task batch = 2.5903e-01, Meta loss averaged over last 500 steps = 2.8267e-01, PNorm = 128.9805, GNorm = 0.2495
Meta loss on this task batch = 2.9442e-01, Meta loss averaged over last 500 steps = 2.8274e-01, PNorm = 128.9834, GNorm = 0.3067
Meta loss on this task batch = 2.6463e-01, Meta loss averaged over last 500 steps = 2.8264e-01, PNorm = 128.9856, GNorm = 0.3014
Took 159.413183927536 seconds to complete one epoch of meta training
Took 169.89695763587952 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497356
Epoch 582
Meta loss on this task batch = 2.9974e-01, Meta loss averaged over last 500 steps = 2.8272e-01, PNorm = 128.9863, GNorm = 0.3566
Meta loss on this task batch = 2.3967e-01, Meta loss averaged over last 500 steps = 2.8270e-01, PNorm = 128.9866, GNorm = 0.2575
Meta loss on this task batch = 2.7244e-01, Meta loss averaged over last 500 steps = 2.8261e-01, PNorm = 128.9872, GNorm = 0.3189
Meta loss on this task batch = 2.4193e-01, Meta loss averaged over last 500 steps = 2.8254e-01, PNorm = 128.9888, GNorm = 0.2515
Meta loss on this task batch = 2.8053e-01, Meta loss averaged over last 500 steps = 2.8233e-01, PNorm = 128.9915, GNorm = 0.3061
Meta loss on this task batch = 3.1328e-01, Meta loss averaged over last 500 steps = 2.8235e-01, PNorm = 128.9951, GNorm = 0.3680
Meta loss on this task batch = 2.7197e-01, Meta loss averaged over last 500 steps = 2.8236e-01, PNorm = 128.9992, GNorm = 0.2778
Meta loss on this task batch = 2.7068e-01, Meta loss averaged over last 500 steps = 2.8239e-01, PNorm = 129.0051, GNorm = 0.2771
Meta loss on this task batch = 3.0314e-01, Meta loss averaged over last 500 steps = 2.8243e-01, PNorm = 129.0120, GNorm = 0.3076
Meta loss on this task batch = 2.1105e-01, Meta loss averaged over last 500 steps = 2.8225e-01, PNorm = 129.0190, GNorm = 0.2443
Meta loss on this task batch = 3.0099e-01, Meta loss averaged over last 500 steps = 2.8225e-01, PNorm = 129.0256, GNorm = 0.2798
Meta loss on this task batch = 2.4630e-01, Meta loss averaged over last 500 steps = 2.8210e-01, PNorm = 129.0329, GNorm = 0.3058
Meta loss on this task batch = 3.4904e-01, Meta loss averaged over last 500 steps = 2.8226e-01, PNorm = 129.0398, GNorm = 0.3733
Meta loss on this task batch = 2.7063e-01, Meta loss averaged over last 500 steps = 2.8211e-01, PNorm = 129.0494, GNorm = 0.3144
Meta loss on this task batch = 3.1654e-01, Meta loss averaged over last 500 steps = 2.8223e-01, PNorm = 129.0584, GNorm = 0.3432
Meta loss on this task batch = 2.6380e-01, Meta loss averaged over last 500 steps = 2.8210e-01, PNorm = 129.0689, GNorm = 0.2936
Meta loss on this task batch = 2.7820e-01, Meta loss averaged over last 500 steps = 2.8207e-01, PNorm = 129.0768, GNorm = 0.2761
Meta loss on this task batch = 3.0050e-01, Meta loss averaged over last 500 steps = 2.8212e-01, PNorm = 129.0829, GNorm = 0.2938
Meta loss on this task batch = 2.9463e-01, Meta loss averaged over last 500 steps = 2.8214e-01, PNorm = 129.0890, GNorm = 0.3335
Took 159.71511960029602 seconds to complete one epoch of meta training
Took 171.21875047683716 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468047
Epoch 583
Meta loss on this task batch = 2.3222e-01, Meta loss averaged over last 500 steps = 2.8210e-01, PNorm = 129.0920, GNorm = 0.2693
Meta loss on this task batch = 2.3705e-01, Meta loss averaged over last 500 steps = 2.8199e-01, PNorm = 129.0947, GNorm = 0.2508
Meta loss on this task batch = 3.2054e-01, Meta loss averaged over last 500 steps = 2.8209e-01, PNorm = 129.0972, GNorm = 0.2790
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 2.8215e-01, PNorm = 129.0974, GNorm = 0.3293
Meta loss on this task batch = 2.6316e-01, Meta loss averaged over last 500 steps = 2.8221e-01, PNorm = 129.0986, GNorm = 0.2601
Meta loss on this task batch = 2.4428e-01, Meta loss averaged over last 500 steps = 2.8196e-01, PNorm = 129.1020, GNorm = 0.2940
Meta loss on this task batch = 3.1066e-01, Meta loss averaged over last 500 steps = 2.8192e-01, PNorm = 129.1042, GNorm = 0.3187
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 2.8190e-01, PNorm = 129.1053, GNorm = 0.3012
Meta loss on this task batch = 3.2844e-01, Meta loss averaged over last 500 steps = 2.8195e-01, PNorm = 129.1063, GNorm = 0.3141
Meta loss on this task batch = 3.0260e-01, Meta loss averaged over last 500 steps = 2.8192e-01, PNorm = 129.1077, GNorm = 0.3315
Meta loss on this task batch = 3.2369e-01, Meta loss averaged over last 500 steps = 2.8194e-01, PNorm = 129.1123, GNorm = 0.5005
Meta loss on this task batch = 2.7789e-01, Meta loss averaged over last 500 steps = 2.8192e-01, PNorm = 129.1182, GNorm = 0.3444
Meta loss on this task batch = 3.1443e-01, Meta loss averaged over last 500 steps = 2.8199e-01, PNorm = 129.1231, GNorm = 0.3027
Meta loss on this task batch = 2.2802e-01, Meta loss averaged over last 500 steps = 2.8188e-01, PNorm = 129.1275, GNorm = 0.2413
Meta loss on this task batch = 2.6652e-01, Meta loss averaged over last 500 steps = 2.8186e-01, PNorm = 129.1323, GNorm = 0.3358
Meta loss on this task batch = 3.0894e-01, Meta loss averaged over last 500 steps = 2.8197e-01, PNorm = 129.1362, GNorm = 0.3129
Meta loss on this task batch = 3.1200e-01, Meta loss averaged over last 500 steps = 2.8205e-01, PNorm = 129.1389, GNorm = 0.3476
Meta loss on this task batch = 2.8051e-01, Meta loss averaged over last 500 steps = 2.8197e-01, PNorm = 129.1405, GNorm = 0.2856
Meta loss on this task batch = 3.2446e-01, Meta loss averaged over last 500 steps = 2.8196e-01, PNorm = 129.1427, GNorm = 0.3836
Took 164.26442956924438 seconds to complete one epoch of meta training
Took 175.03644180297852 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478270
Epoch 584
Meta loss on this task batch = 3.3694e-01, Meta loss averaged over last 500 steps = 2.8204e-01, PNorm = 129.1478, GNorm = 0.3228
Meta loss on this task batch = 2.5844e-01, Meta loss averaged over last 500 steps = 2.8194e-01, PNorm = 129.1519, GNorm = 0.2826
Meta loss on this task batch = 2.8679e-01, Meta loss averaged over last 500 steps = 2.8200e-01, PNorm = 129.1528, GNorm = 0.3037
Meta loss on this task batch = 3.1579e-01, Meta loss averaged over last 500 steps = 2.8206e-01, PNorm = 129.1550, GNorm = 0.2922
Meta loss on this task batch = 2.7972e-01, Meta loss averaged over last 500 steps = 2.8204e-01, PNorm = 129.1583, GNorm = 0.2551
Meta loss on this task batch = 2.3201e-01, Meta loss averaged over last 500 steps = 2.8196e-01, PNorm = 129.1614, GNorm = 0.2770
Meta loss on this task batch = 3.2226e-01, Meta loss averaged over last 500 steps = 2.8210e-01, PNorm = 129.1631, GNorm = 0.2723
Meta loss on this task batch = 2.9232e-01, Meta loss averaged over last 500 steps = 2.8217e-01, PNorm = 129.1661, GNorm = 0.2623
Meta loss on this task batch = 2.8144e-01, Meta loss averaged over last 500 steps = 2.8228e-01, PNorm = 129.1678, GNorm = 0.3253
Meta loss on this task batch = 2.1039e-01, Meta loss averaged over last 500 steps = 2.8210e-01, PNorm = 129.1704, GNorm = 0.2469
Meta loss on this task batch = 2.6414e-01, Meta loss averaged over last 500 steps = 2.8207e-01, PNorm = 129.1744, GNorm = 0.3216
Meta loss on this task batch = 3.0336e-01, Meta loss averaged over last 500 steps = 2.8218e-01, PNorm = 129.1792, GNorm = 0.3414
Meta loss on this task batch = 3.0418e-01, Meta loss averaged over last 500 steps = 2.8222e-01, PNorm = 129.1841, GNorm = 0.3127
Meta loss on this task batch = 2.7093e-01, Meta loss averaged over last 500 steps = 2.8206e-01, PNorm = 129.1913, GNorm = 0.3251
Meta loss on this task batch = 2.9481e-01, Meta loss averaged over last 500 steps = 2.8204e-01, PNorm = 129.1977, GNorm = 0.3222
Meta loss on this task batch = 2.4776e-01, Meta loss averaged over last 500 steps = 2.8195e-01, PNorm = 129.2033, GNorm = 0.2949
Meta loss on this task batch = 3.0334e-01, Meta loss averaged over last 500 steps = 2.8199e-01, PNorm = 129.2092, GNorm = 0.2701
Meta loss on this task batch = 2.8872e-01, Meta loss averaged over last 500 steps = 2.8207e-01, PNorm = 129.2145, GNorm = 0.2892
Meta loss on this task batch = 2.5723e-01, Meta loss averaged over last 500 steps = 2.8202e-01, PNorm = 129.2200, GNorm = 0.3374
Took 168.23403668403625 seconds to complete one epoch of meta training
Took 177.91026210784912 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487684
Epoch 585
Meta loss on this task batch = 2.3055e-01, Meta loss averaged over last 500 steps = 2.8180e-01, PNorm = 129.2250, GNorm = 0.2857
Meta loss on this task batch = 2.6143e-01, Meta loss averaged over last 500 steps = 2.8164e-01, PNorm = 129.2303, GNorm = 0.2838
Meta loss on this task batch = 2.6200e-01, Meta loss averaged over last 500 steps = 2.8151e-01, PNorm = 129.2330, GNorm = 0.2752
Meta loss on this task batch = 2.7609e-01, Meta loss averaged over last 500 steps = 2.8152e-01, PNorm = 129.2357, GNorm = 0.2608
Meta loss on this task batch = 2.7636e-01, Meta loss averaged over last 500 steps = 2.8156e-01, PNorm = 129.2379, GNorm = 0.2833
Meta loss on this task batch = 2.6156e-01, Meta loss averaged over last 500 steps = 2.8156e-01, PNorm = 129.2390, GNorm = 0.3103
Meta loss on this task batch = 3.0637e-01, Meta loss averaged over last 500 steps = 2.8158e-01, PNorm = 129.2385, GNorm = 0.4159
Meta loss on this task batch = 3.1898e-01, Meta loss averaged over last 500 steps = 2.8181e-01, PNorm = 129.2398, GNorm = 0.3416
Meta loss on this task batch = 3.0260e-01, Meta loss averaged over last 500 steps = 2.8183e-01, PNorm = 129.2408, GNorm = 0.2849
Meta loss on this task batch = 2.3620e-01, Meta loss averaged over last 500 steps = 2.8190e-01, PNorm = 129.2428, GNorm = 0.2769
Meta loss on this task batch = 3.5836e-01, Meta loss averaged over last 500 steps = 2.8204e-01, PNorm = 129.2442, GNorm = 0.3449
Meta loss on this task batch = 3.1691e-01, Meta loss averaged over last 500 steps = 2.8208e-01, PNorm = 129.2468, GNorm = 0.2895
Meta loss on this task batch = 2.7958e-01, Meta loss averaged over last 500 steps = 2.8207e-01, PNorm = 129.2512, GNorm = 0.3210
Meta loss on this task batch = 2.3839e-01, Meta loss averaged over last 500 steps = 2.8200e-01, PNorm = 129.2574, GNorm = 0.3288
Meta loss on this task batch = 3.1692e-01, Meta loss averaged over last 500 steps = 2.8202e-01, PNorm = 129.2650, GNorm = 0.2557
Meta loss on this task batch = 3.1043e-01, Meta loss averaged over last 500 steps = 2.8211e-01, PNorm = 129.2734, GNorm = 0.2801
Meta loss on this task batch = 2.7791e-01, Meta loss averaged over last 500 steps = 2.8201e-01, PNorm = 129.2810, GNorm = 0.2653
Meta loss on this task batch = 2.7986e-01, Meta loss averaged over last 500 steps = 2.8198e-01, PNorm = 129.2878, GNorm = 0.2716
Meta loss on this task batch = 2.5973e-01, Meta loss averaged over last 500 steps = 2.8193e-01, PNorm = 129.2934, GNorm = 0.3026
Took 167.7508463859558 seconds to complete one epoch of meta training
Took 179.51042819023132 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485996
Epoch 586
Meta loss on this task batch = 2.5341e-01, Meta loss averaged over last 500 steps = 2.8170e-01, PNorm = 129.2953, GNorm = 0.2755
Meta loss on this task batch = 2.4966e-01, Meta loss averaged over last 500 steps = 2.8165e-01, PNorm = 129.2995, GNorm = 0.2788
Meta loss on this task batch = 2.9804e-01, Meta loss averaged over last 500 steps = 2.8173e-01, PNorm = 129.3034, GNorm = 0.2728
Meta loss on this task batch = 2.5922e-01, Meta loss averaged over last 500 steps = 2.8179e-01, PNorm = 129.3066, GNorm = 0.2663
Meta loss on this task batch = 2.8953e-01, Meta loss averaged over last 500 steps = 2.8168e-01, PNorm = 129.3092, GNorm = 0.3095
Meta loss on this task batch = 2.2339e-01, Meta loss averaged over last 500 steps = 2.8161e-01, PNorm = 129.3122, GNorm = 0.2603
Meta loss on this task batch = 2.2528e-01, Meta loss averaged over last 500 steps = 2.8148e-01, PNorm = 129.3154, GNorm = 0.2652
Meta loss on this task batch = 3.0727e-01, Meta loss averaged over last 500 steps = 2.8160e-01, PNorm = 129.3182, GNorm = 0.2850
Meta loss on this task batch = 2.8922e-01, Meta loss averaged over last 500 steps = 2.8170e-01, PNorm = 129.3219, GNorm = 0.2878
Meta loss on this task batch = 3.2910e-01, Meta loss averaged over last 500 steps = 2.8182e-01, PNorm = 129.3282, GNorm = 0.3265
Meta loss on this task batch = 2.2998e-01, Meta loss averaged over last 500 steps = 2.8174e-01, PNorm = 129.3361, GNorm = 0.2858
Meta loss on this task batch = 3.1181e-01, Meta loss averaged over last 500 steps = 2.8186e-01, PNorm = 129.3438, GNorm = 0.2950
Meta loss on this task batch = 2.8215e-01, Meta loss averaged over last 500 steps = 2.8180e-01, PNorm = 129.3515, GNorm = 0.2640
Meta loss on this task batch = 2.9044e-01, Meta loss averaged over last 500 steps = 2.8174e-01, PNorm = 129.3576, GNorm = 0.2822
Meta loss on this task batch = 3.1443e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 129.3629, GNorm = 0.3202
Meta loss on this task batch = 3.1463e-01, Meta loss averaged over last 500 steps = 2.8181e-01, PNorm = 129.3669, GNorm = 0.3086
Meta loss on this task batch = 2.7693e-01, Meta loss averaged over last 500 steps = 2.8184e-01, PNorm = 129.3728, GNorm = 0.3185
Meta loss on this task batch = 2.5581e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 129.3804, GNorm = 0.3019
Meta loss on this task batch = 2.7961e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 129.3870, GNorm = 0.3497
Took 164.65615558624268 seconds to complete one epoch of meta training
Took 174.46437859535217 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492730
Epoch 587
Meta loss on this task batch = 3.1359e-01, Meta loss averaged over last 500 steps = 2.8189e-01, PNorm = 129.3928, GNorm = 0.3084
Meta loss on this task batch = 3.2425e-01, Meta loss averaged over last 500 steps = 2.8197e-01, PNorm = 129.3985, GNorm = 0.2960
Meta loss on this task batch = 2.8341e-01, Meta loss averaged over last 500 steps = 2.8207e-01, PNorm = 129.4034, GNorm = 0.2780
Meta loss on this task batch = 2.5586e-01, Meta loss averaged over last 500 steps = 2.8205e-01, PNorm = 129.4096, GNorm = 0.2836
Meta loss on this task batch = 2.6423e-01, Meta loss averaged over last 500 steps = 2.8195e-01, PNorm = 129.4156, GNorm = 0.3317
Meta loss on this task batch = 2.9128e-01, Meta loss averaged over last 500 steps = 2.8206e-01, PNorm = 129.4223, GNorm = 0.2727
Meta loss on this task batch = 2.9012e-01, Meta loss averaged over last 500 steps = 2.8210e-01, PNorm = 129.4303, GNorm = 0.2980
Meta loss on this task batch = 2.9525e-01, Meta loss averaged over last 500 steps = 2.8220e-01, PNorm = 129.4397, GNorm = 0.2955
Meta loss on this task batch = 2.5129e-01, Meta loss averaged over last 500 steps = 2.8214e-01, PNorm = 129.4501, GNorm = 0.2948
Meta loss on this task batch = 2.7313e-01, Meta loss averaged over last 500 steps = 2.8208e-01, PNorm = 129.4581, GNorm = 0.2926
Meta loss on this task batch = 3.1532e-01, Meta loss averaged over last 500 steps = 2.8224e-01, PNorm = 129.4628, GNorm = 0.3259
Meta loss on this task batch = 2.8287e-01, Meta loss averaged over last 500 steps = 2.8225e-01, PNorm = 129.4675, GNorm = 0.3223
Meta loss on this task batch = 3.1925e-01, Meta loss averaged over last 500 steps = 2.8223e-01, PNorm = 129.4739, GNorm = 0.3199
Meta loss on this task batch = 2.7862e-01, Meta loss averaged over last 500 steps = 2.8228e-01, PNorm = 129.4813, GNorm = 0.2800
Meta loss on this task batch = 2.7503e-01, Meta loss averaged over last 500 steps = 2.8229e-01, PNorm = 129.4864, GNorm = 0.3342
Meta loss on this task batch = 2.8141e-01, Meta loss averaged over last 500 steps = 2.8232e-01, PNorm = 129.4936, GNorm = 0.2950
Meta loss on this task batch = 2.3681e-01, Meta loss averaged over last 500 steps = 2.8227e-01, PNorm = 129.4994, GNorm = 0.3093
Meta loss on this task batch = 2.3756e-01, Meta loss averaged over last 500 steps = 2.8223e-01, PNorm = 129.5053, GNorm = 0.2509
Meta loss on this task batch = 2.8590e-01, Meta loss averaged over last 500 steps = 2.8227e-01, PNorm = 129.5097, GNorm = 0.2950
Took 157.63019037246704 seconds to complete one epoch of meta training
Took 169.50524926185608 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503987
Epoch 588
Meta loss on this task batch = 2.8181e-01, Meta loss averaged over last 500 steps = 2.8226e-01, PNorm = 129.5110, GNorm = 0.2848
Meta loss on this task batch = 2.8317e-01, Meta loss averaged over last 500 steps = 2.8221e-01, PNorm = 129.5100, GNorm = 0.2966
Meta loss on this task batch = 2.8479e-01, Meta loss averaged over last 500 steps = 2.8222e-01, PNorm = 129.5074, GNorm = 0.2575
Meta loss on this task batch = 2.7817e-01, Meta loss averaged over last 500 steps = 2.8217e-01, PNorm = 129.5053, GNorm = 0.2567
Meta loss on this task batch = 2.9425e-01, Meta loss averaged over last 500 steps = 2.8218e-01, PNorm = 129.5034, GNorm = 0.2833
Meta loss on this task batch = 2.5694e-01, Meta loss averaged over last 500 steps = 2.8214e-01, PNorm = 129.5022, GNorm = 0.3172
Meta loss on this task batch = 2.7353e-01, Meta loss averaged over last 500 steps = 2.8210e-01, PNorm = 129.5028, GNorm = 0.2784
Meta loss on this task batch = 2.3663e-01, Meta loss averaged over last 500 steps = 2.8197e-01, PNorm = 129.5043, GNorm = 0.2434
Meta loss on this task batch = 3.1143e-01, Meta loss averaged over last 500 steps = 2.8194e-01, PNorm = 129.5054, GNorm = 0.2802
Meta loss on this task batch = 2.7988e-01, Meta loss averaged over last 500 steps = 2.8197e-01, PNorm = 129.5087, GNorm = 0.2888
Meta loss on this task batch = 3.1647e-01, Meta loss averaged over last 500 steps = 2.8204e-01, PNorm = 129.5100, GNorm = 0.3139
Meta loss on this task batch = 2.3565e-01, Meta loss averaged over last 500 steps = 2.8198e-01, PNorm = 129.5125, GNorm = 0.3307
Meta loss on this task batch = 3.0202e-01, Meta loss averaged over last 500 steps = 2.8198e-01, PNorm = 129.5154, GNorm = 0.3493
Meta loss on this task batch = 2.3041e-01, Meta loss averaged over last 500 steps = 2.8180e-01, PNorm = 129.5183, GNorm = 0.2730
Meta loss on this task batch = 3.1727e-01, Meta loss averaged over last 500 steps = 2.8189e-01, PNorm = 129.5198, GNorm = 0.3266
Meta loss on this task batch = 2.5267e-01, Meta loss averaged over last 500 steps = 2.8188e-01, PNorm = 129.5195, GNorm = 0.2604
Meta loss on this task batch = 3.3306e-01, Meta loss averaged over last 500 steps = 2.8185e-01, PNorm = 129.5185, GNorm = 0.3149
Meta loss on this task batch = 3.5347e-01, Meta loss averaged over last 500 steps = 2.8201e-01, PNorm = 129.5194, GNorm = 0.3379
Meta loss on this task batch = 2.3110e-01, Meta loss averaged over last 500 steps = 2.8186e-01, PNorm = 129.5211, GNorm = 0.2773
Took 160.0322277545929 seconds to complete one epoch of meta training
Took 170.86854219436646 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464320
Epoch 589
Meta loss on this task batch = 2.6441e-01, Meta loss averaged over last 500 steps = 2.8186e-01, PNorm = 129.5250, GNorm = 0.2798
Meta loss on this task batch = 2.3857e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 129.5298, GNorm = 0.2508
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 2.8182e-01, PNorm = 129.5353, GNorm = 0.3057
Meta loss on this task batch = 2.6251e-01, Meta loss averaged over last 500 steps = 2.8175e-01, PNorm = 129.5397, GNorm = 0.3161
Meta loss on this task batch = 2.6022e-01, Meta loss averaged over last 500 steps = 2.8168e-01, PNorm = 129.5456, GNorm = 0.2495
Meta loss on this task batch = 2.7998e-01, Meta loss averaged over last 500 steps = 2.8174e-01, PNorm = 129.5524, GNorm = 0.2736
Meta loss on this task batch = 3.0288e-01, Meta loss averaged over last 500 steps = 2.8172e-01, PNorm = 129.5580, GNorm = 0.4194
Meta loss on this task batch = 2.5116e-01, Meta loss averaged over last 500 steps = 2.8164e-01, PNorm = 129.5645, GNorm = 0.2758
Meta loss on this task batch = 2.7267e-01, Meta loss averaged over last 500 steps = 2.8157e-01, PNorm = 129.5722, GNorm = 0.2701
Meta loss on this task batch = 2.5483e-01, Meta loss averaged over last 500 steps = 2.8154e-01, PNorm = 129.5816, GNorm = 0.2830
Meta loss on this task batch = 2.7664e-01, Meta loss averaged over last 500 steps = 2.8151e-01, PNorm = 129.5896, GNorm = 0.3190
Meta loss on this task batch = 2.6924e-01, Meta loss averaged over last 500 steps = 2.8151e-01, PNorm = 129.5964, GNorm = 0.2879
Meta loss on this task batch = 3.9620e-01, Meta loss averaged over last 500 steps = 2.8170e-01, PNorm = 129.5995, GNorm = 0.3730
Meta loss on this task batch = 3.0102e-01, Meta loss averaged over last 500 steps = 2.8178e-01, PNorm = 129.6032, GNorm = 0.2964
Meta loss on this task batch = 2.0089e-01, Meta loss averaged over last 500 steps = 2.8163e-01, PNorm = 129.6065, GNorm = 0.2798
Meta loss on this task batch = 3.4452e-01, Meta loss averaged over last 500 steps = 2.8181e-01, PNorm = 129.6099, GNorm = 0.3414
Meta loss on this task batch = 2.7363e-01, Meta loss averaged over last 500 steps = 2.8178e-01, PNorm = 129.6131, GNorm = 0.3125
Meta loss on this task batch = 2.4413e-01, Meta loss averaged over last 500 steps = 2.8162e-01, PNorm = 129.6145, GNorm = 0.2930
Meta loss on this task batch = 3.4066e-01, Meta loss averaged over last 500 steps = 2.8176e-01, PNorm = 129.6129, GNorm = 0.4026
Took 161.28953003883362 seconds to complete one epoch of meta training
Took 172.23913741111755 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464084
Epoch 590
Meta loss on this task batch = 2.7410e-01, Meta loss averaged over last 500 steps = 2.8164e-01, PNorm = 129.6117, GNorm = 0.2739
Meta loss on this task batch = 3.1166e-01, Meta loss averaged over last 500 steps = 2.8168e-01, PNorm = 129.6125, GNorm = 0.2940
Meta loss on this task batch = 3.1230e-01, Meta loss averaged over last 500 steps = 2.8176e-01, PNorm = 129.6171, GNorm = 0.3256
Meta loss on this task batch = 2.4715e-01, Meta loss averaged over last 500 steps = 2.8175e-01, PNorm = 129.6234, GNorm = 0.2862
Meta loss on this task batch = 2.7748e-01, Meta loss averaged over last 500 steps = 2.8176e-01, PNorm = 129.6296, GNorm = 0.2895
Meta loss on this task batch = 2.2580e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 129.6363, GNorm = 0.2454
Meta loss on this task batch = 3.0833e-01, Meta loss averaged over last 500 steps = 2.8187e-01, PNorm = 129.6430, GNorm = 0.3146
Meta loss on this task batch = 2.2253e-01, Meta loss averaged over last 500 steps = 2.8171e-01, PNorm = 129.6485, GNorm = 0.2583
Meta loss on this task batch = 2.6573e-01, Meta loss averaged over last 500 steps = 2.8168e-01, PNorm = 129.6538, GNorm = 0.3055
Meta loss on this task batch = 3.1251e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 129.6566, GNorm = 0.3512
Meta loss on this task batch = 3.1865e-01, Meta loss averaged over last 500 steps = 2.8190e-01, PNorm = 129.6593, GNorm = 0.3228
Meta loss on this task batch = 2.5485e-01, Meta loss averaged over last 500 steps = 2.8189e-01, PNorm = 129.6618, GNorm = 0.2995
Meta loss on this task batch = 2.2402e-01, Meta loss averaged over last 500 steps = 2.8172e-01, PNorm = 129.6619, GNorm = 0.3085
Meta loss on this task batch = 2.7622e-01, Meta loss averaged over last 500 steps = 2.8166e-01, PNorm = 129.6616, GNorm = 0.2972
Meta loss on this task batch = 2.5345e-01, Meta loss averaged over last 500 steps = 2.8158e-01, PNorm = 129.6618, GNorm = 0.2709
Meta loss on this task batch = 3.4512e-01, Meta loss averaged over last 500 steps = 2.8171e-01, PNorm = 129.6617, GNorm = 0.3724
Meta loss on this task batch = 2.7199e-01, Meta loss averaged over last 500 steps = 2.8173e-01, PNorm = 129.6618, GNorm = 0.3238
Meta loss on this task batch = 2.9498e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 129.6622, GNorm = 0.2837
Meta loss on this task batch = 2.9693e-01, Meta loss averaged over last 500 steps = 2.8173e-01, PNorm = 129.6634, GNorm = 0.3585
Took 157.4668664932251 seconds to complete one epoch of meta training
Took 167.0360894203186 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503709
Epoch 591
Meta loss on this task batch = 3.0782e-01, Meta loss averaged over last 500 steps = 2.8180e-01, PNorm = 129.6626, GNorm = 0.3121
Meta loss on this task batch = 2.8038e-01, Meta loss averaged over last 500 steps = 2.8182e-01, PNorm = 129.6624, GNorm = 0.3282
Meta loss on this task batch = 2.7737e-01, Meta loss averaged over last 500 steps = 2.8183e-01, PNorm = 129.6627, GNorm = 0.2619
Meta loss on this task batch = 2.5753e-01, Meta loss averaged over last 500 steps = 2.8180e-01, PNorm = 129.6656, GNorm = 0.2814
Meta loss on this task batch = 2.6487e-01, Meta loss averaged over last 500 steps = 2.8179e-01, PNorm = 129.6685, GNorm = 0.2749
Meta loss on this task batch = 2.7352e-01, Meta loss averaged over last 500 steps = 2.8167e-01, PNorm = 129.6723, GNorm = 0.3055
Meta loss on this task batch = 3.1088e-01, Meta loss averaged over last 500 steps = 2.8164e-01, PNorm = 129.6748, GNorm = 0.3278
Meta loss on this task batch = 2.7751e-01, Meta loss averaged over last 500 steps = 2.8164e-01, PNorm = 129.6762, GNorm = 0.2712
Meta loss on this task batch = 3.2208e-01, Meta loss averaged over last 500 steps = 2.8184e-01, PNorm = 129.6767, GNorm = 0.3103
Meta loss on this task batch = 2.4826e-01, Meta loss averaged over last 500 steps = 2.8175e-01, PNorm = 129.6785, GNorm = 0.2395
Meta loss on this task batch = 2.7338e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 129.6810, GNorm = 0.2920
Meta loss on this task batch = 2.1800e-01, Meta loss averaged over last 500 steps = 2.8150e-01, PNorm = 129.6863, GNorm = 0.2803
Meta loss on this task batch = 2.2179e-01, Meta loss averaged over last 500 steps = 2.8136e-01, PNorm = 129.6916, GNorm = 0.2464
Meta loss on this task batch = 2.8540e-01, Meta loss averaged over last 500 steps = 2.8129e-01, PNorm = 129.6947, GNorm = 0.3036
Meta loss on this task batch = 3.1108e-01, Meta loss averaged over last 500 steps = 2.8134e-01, PNorm = 129.6970, GNorm = 0.3253
Meta loss on this task batch = 3.4927e-01, Meta loss averaged over last 500 steps = 2.8137e-01, PNorm = 129.6976, GNorm = 0.3052
Meta loss on this task batch = 3.2914e-01, Meta loss averaged over last 500 steps = 2.8156e-01, PNorm = 129.6985, GNorm = 0.2946
Meta loss on this task batch = 2.9268e-01, Meta loss averaged over last 500 steps = 2.8161e-01, PNorm = 129.7003, GNorm = 0.2866
Meta loss on this task batch = 3.2581e-01, Meta loss averaged over last 500 steps = 2.8178e-01, PNorm = 129.7034, GNorm = 0.3471
Took 158.04501867294312 seconds to complete one epoch of meta training
Took 168.39535975456238 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468890
Epoch 592
Meta loss on this task batch = 2.5140e-01, Meta loss averaged over last 500 steps = 2.8167e-01, PNorm = 129.7083, GNorm = 0.2476
Meta loss on this task batch = 3.3256e-01, Meta loss averaged over last 500 steps = 2.8187e-01, PNorm = 129.7123, GNorm = 0.3398
Meta loss on this task batch = 2.7431e-01, Meta loss averaged over last 500 steps = 2.8182e-01, PNorm = 129.7205, GNorm = 0.2753
Meta loss on this task batch = 3.3902e-01, Meta loss averaged over last 500 steps = 2.8184e-01, PNorm = 129.7292, GNorm = 0.3039
Meta loss on this task batch = 2.7061e-01, Meta loss averaged over last 500 steps = 2.8184e-01, PNorm = 129.7388, GNorm = 0.2751
Meta loss on this task batch = 2.4706e-01, Meta loss averaged over last 500 steps = 2.8184e-01, PNorm = 129.7451, GNorm = 0.2829
Meta loss on this task batch = 3.0196e-01, Meta loss averaged over last 500 steps = 2.8186e-01, PNorm = 129.7502, GNorm = 0.3280
Meta loss on this task batch = 2.8196e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 129.7545, GNorm = 0.2702
Meta loss on this task batch = 3.0266e-01, Meta loss averaged over last 500 steps = 2.8164e-01, PNorm = 129.7598, GNorm = 0.3373
Meta loss on this task batch = 2.9895e-01, Meta loss averaged over last 500 steps = 2.8170e-01, PNorm = 129.7651, GNorm = 0.2615
Meta loss on this task batch = 2.6833e-01, Meta loss averaged over last 500 steps = 2.8180e-01, PNorm = 129.7718, GNorm = 0.2702
Meta loss on this task batch = 2.9759e-01, Meta loss averaged over last 500 steps = 2.8182e-01, PNorm = 129.7794, GNorm = 0.3326
Meta loss on this task batch = 2.9731e-01, Meta loss averaged over last 500 steps = 2.8183e-01, PNorm = 129.7860, GNorm = 0.2951
Meta loss on this task batch = 2.0845e-01, Meta loss averaged over last 500 steps = 2.8166e-01, PNorm = 129.7942, GNorm = 0.2360
Meta loss on this task batch = 2.7861e-01, Meta loss averaged over last 500 steps = 2.8165e-01, PNorm = 129.8031, GNorm = 0.3171
Meta loss on this task batch = 2.5440e-01, Meta loss averaged over last 500 steps = 2.8171e-01, PNorm = 129.8124, GNorm = 0.2647
Meta loss on this task batch = 3.0395e-01, Meta loss averaged over last 500 steps = 2.8173e-01, PNorm = 129.8216, GNorm = 0.2989
Meta loss on this task batch = 2.9558e-01, Meta loss averaged over last 500 steps = 2.8171e-01, PNorm = 129.8314, GNorm = 0.3284
Meta loss on this task batch = 3.0673e-01, Meta loss averaged over last 500 steps = 2.8176e-01, PNorm = 129.8403, GNorm = 0.3555
Took 185.10082507133484 seconds to complete one epoch of meta training
Took 194.4207718372345 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480708
Epoch 593
Meta loss on this task batch = 2.0112e-01, Meta loss averaged over last 500 steps = 2.8162e-01, PNorm = 129.8503, GNorm = 0.2352
Meta loss on this task batch = 2.9984e-01, Meta loss averaged over last 500 steps = 2.8159e-01, PNorm = 129.8583, GNorm = 0.3010
Meta loss on this task batch = 3.0884e-01, Meta loss averaged over last 500 steps = 2.8159e-01, PNorm = 129.8657, GNorm = 0.2941
Meta loss on this task batch = 2.4178e-01, Meta loss averaged over last 500 steps = 2.8145e-01, PNorm = 129.8720, GNorm = 0.2903
Meta loss on this task batch = 2.7486e-01, Meta loss averaged over last 500 steps = 2.8144e-01, PNorm = 129.8783, GNorm = 0.2831
Meta loss on this task batch = 2.7732e-01, Meta loss averaged over last 500 steps = 2.8146e-01, PNorm = 129.8818, GNorm = 0.3104
Meta loss on this task batch = 2.5203e-01, Meta loss averaged over last 500 steps = 2.8142e-01, PNorm = 129.8850, GNorm = 0.3184
Meta loss on this task batch = 2.8726e-01, Meta loss averaged over last 500 steps = 2.8145e-01, PNorm = 129.8870, GNorm = 0.2941
Meta loss on this task batch = 3.0400e-01, Meta loss averaged over last 500 steps = 2.8155e-01, PNorm = 129.8882, GNorm = 0.3235
Meta loss on this task batch = 2.7763e-01, Meta loss averaged over last 500 steps = 2.8160e-01, PNorm = 129.8884, GNorm = 0.3167
Meta loss on this task batch = 3.3693e-01, Meta loss averaged over last 500 steps = 2.8175e-01, PNorm = 129.8893, GNorm = 0.3169
Meta loss on this task batch = 2.9127e-01, Meta loss averaged over last 500 steps = 2.8174e-01, PNorm = 129.8919, GNorm = 0.2887
Meta loss on this task batch = 2.5015e-01, Meta loss averaged over last 500 steps = 2.8158e-01, PNorm = 129.8950, GNorm = 0.2214
Meta loss on this task batch = 2.4922e-01, Meta loss averaged over last 500 steps = 2.8140e-01, PNorm = 129.8989, GNorm = 0.2738
Meta loss on this task batch = 2.8165e-01, Meta loss averaged over last 500 steps = 2.8125e-01, PNorm = 129.9016, GNorm = 0.2801
Meta loss on this task batch = 2.6184e-01, Meta loss averaged over last 500 steps = 2.8115e-01, PNorm = 129.9051, GNorm = 0.3188
Meta loss on this task batch = 2.8343e-01, Meta loss averaged over last 500 steps = 2.8111e-01, PNorm = 129.9102, GNorm = 0.2754
Meta loss on this task batch = 3.1784e-01, Meta loss averaged over last 500 steps = 2.8120e-01, PNorm = 129.9149, GNorm = 0.2914
Meta loss on this task batch = 2.8249e-01, Meta loss averaged over last 500 steps = 2.8119e-01, PNorm = 129.9186, GNorm = 0.3677
Took 157.71765446662903 seconds to complete one epoch of meta training
Took 168.76161217689514 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456915
Epoch 594
Meta loss on this task batch = 2.3754e-01, Meta loss averaged over last 500 steps = 2.8112e-01, PNorm = 129.9220, GNorm = 0.2871
Meta loss on this task batch = 2.5378e-01, Meta loss averaged over last 500 steps = 2.8117e-01, PNorm = 129.9257, GNorm = 0.3021
Meta loss on this task batch = 2.7526e-01, Meta loss averaged over last 500 steps = 2.8105e-01, PNorm = 129.9294, GNorm = 0.2855
Meta loss on this task batch = 2.8394e-01, Meta loss averaged over last 500 steps = 2.8116e-01, PNorm = 129.9332, GNorm = 0.2827
Meta loss on this task batch = 3.4552e-01, Meta loss averaged over last 500 steps = 2.8129e-01, PNorm = 129.9392, GNorm = 0.3445
Meta loss on this task batch = 2.7478e-01, Meta loss averaged over last 500 steps = 2.8127e-01, PNorm = 129.9447, GNorm = 0.2923
Meta loss on this task batch = 2.5371e-01, Meta loss averaged over last 500 steps = 2.8125e-01, PNorm = 129.9511, GNorm = 0.2515
Meta loss on this task batch = 1.9379e-01, Meta loss averaged over last 500 steps = 2.8108e-01, PNorm = 129.9581, GNorm = 0.2328
Meta loss on this task batch = 3.1800e-01, Meta loss averaged over last 500 steps = 2.8124e-01, PNorm = 129.9625, GNorm = 0.3296
Meta loss on this task batch = 3.1245e-01, Meta loss averaged over last 500 steps = 2.8128e-01, PNorm = 129.9665, GNorm = 0.3191
Meta loss on this task batch = 2.6612e-01, Meta loss averaged over last 500 steps = 2.8124e-01, PNorm = 129.9714, GNorm = 0.3419
Meta loss on this task batch = 3.4146e-01, Meta loss averaged over last 500 steps = 2.8136e-01, PNorm = 129.9759, GNorm = 0.3027
Meta loss on this task batch = 3.2789e-01, Meta loss averaged over last 500 steps = 2.8141e-01, PNorm = 129.9772, GNorm = 0.3486
Meta loss on this task batch = 2.2597e-01, Meta loss averaged over last 500 steps = 2.8126e-01, PNorm = 129.9788, GNorm = 0.3032
Meta loss on this task batch = 2.7755e-01, Meta loss averaged over last 500 steps = 2.8104e-01, PNorm = 129.9788, GNorm = 0.3451
Meta loss on this task batch = 2.7790e-01, Meta loss averaged over last 500 steps = 2.8104e-01, PNorm = 129.9795, GNorm = 0.2754
Meta loss on this task batch = 2.7025e-01, Meta loss averaged over last 500 steps = 2.8112e-01, PNorm = 129.9817, GNorm = 0.2813
Meta loss on this task batch = 3.3175e-01, Meta loss averaged over last 500 steps = 2.8124e-01, PNorm = 129.9837, GNorm = 0.2769
Meta loss on this task batch = 3.2782e-01, Meta loss averaged over last 500 steps = 2.8135e-01, PNorm = 129.9851, GNorm = 0.3626
Took 155.81455373764038 seconds to complete one epoch of meta training
Took 166.2280731201172 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476838
Epoch 595
Meta loss on this task batch = 2.3970e-01, Meta loss averaged over last 500 steps = 2.8130e-01, PNorm = 129.9881, GNorm = 0.2317
Meta loss on this task batch = 2.5196e-01, Meta loss averaged over last 500 steps = 2.8134e-01, PNorm = 129.9921, GNorm = 0.2498
Meta loss on this task batch = 2.7236e-01, Meta loss averaged over last 500 steps = 2.8141e-01, PNorm = 129.9984, GNorm = 0.2784
Meta loss on this task batch = 2.6022e-01, Meta loss averaged over last 500 steps = 2.8132e-01, PNorm = 130.0051, GNorm = 0.2181
Meta loss on this task batch = 3.1043e-01, Meta loss averaged over last 500 steps = 2.8118e-01, PNorm = 130.0121, GNorm = 0.2815
Meta loss on this task batch = 2.7291e-01, Meta loss averaged over last 500 steps = 2.8122e-01, PNorm = 130.0195, GNorm = 0.3316
Meta loss on this task batch = 2.9386e-01, Meta loss averaged over last 500 steps = 2.8118e-01, PNorm = 130.0266, GNorm = 0.3087
Meta loss on this task batch = 2.7430e-01, Meta loss averaged over last 500 steps = 2.8119e-01, PNorm = 130.0348, GNorm = 0.2717
Meta loss on this task batch = 2.6841e-01, Meta loss averaged over last 500 steps = 2.8111e-01, PNorm = 130.0421, GNorm = 0.2635
Meta loss on this task batch = 2.9704e-01, Meta loss averaged over last 500 steps = 2.8113e-01, PNorm = 130.0458, GNorm = 0.3252
Meta loss on this task batch = 2.6108e-01, Meta loss averaged over last 500 steps = 2.8111e-01, PNorm = 130.0485, GNorm = 0.2903
Meta loss on this task batch = 3.0069e-01, Meta loss averaged over last 500 steps = 2.8116e-01, PNorm = 130.0489, GNorm = 0.3192
Meta loss on this task batch = 2.7603e-01, Meta loss averaged over last 500 steps = 2.8119e-01, PNorm = 130.0484, GNorm = 0.3171
Meta loss on this task batch = 2.4114e-01, Meta loss averaged over last 500 steps = 2.8121e-01, PNorm = 130.0477, GNorm = 0.3219
Meta loss on this task batch = 2.6372e-01, Meta loss averaged over last 500 steps = 2.8122e-01, PNorm = 130.0455, GNorm = 0.2761
Meta loss on this task batch = 2.9407e-01, Meta loss averaged over last 500 steps = 2.8120e-01, PNorm = 130.0445, GNorm = 0.2885
Meta loss on this task batch = 2.6807e-01, Meta loss averaged over last 500 steps = 2.8112e-01, PNorm = 130.0469, GNorm = 0.3688
Meta loss on this task batch = 2.9924e-01, Meta loss averaged over last 500 steps = 2.8111e-01, PNorm = 130.0507, GNorm = 0.3023
Meta loss on this task batch = 3.4724e-01, Meta loss averaged over last 500 steps = 2.8115e-01, PNorm = 130.0533, GNorm = 0.4428
Took 158.78036999702454 seconds to complete one epoch of meta training
Took 169.81981134414673 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478093
Epoch 596
Meta loss on this task batch = 3.0288e-01, Meta loss averaged over last 500 steps = 2.8124e-01, PNorm = 130.0553, GNorm = 0.2918
Meta loss on this task batch = 2.7467e-01, Meta loss averaged over last 500 steps = 2.8123e-01, PNorm = 130.0593, GNorm = 0.2734
Meta loss on this task batch = 2.7699e-01, Meta loss averaged over last 500 steps = 2.8121e-01, PNorm = 130.0631, GNorm = 0.2418
Meta loss on this task batch = 2.5764e-01, Meta loss averaged over last 500 steps = 2.8115e-01, PNorm = 130.0687, GNorm = 0.3417
Meta loss on this task batch = 3.1477e-01, Meta loss averaged over last 500 steps = 2.8120e-01, PNorm = 130.0729, GNorm = 0.2945
Meta loss on this task batch = 2.8822e-01, Meta loss averaged over last 500 steps = 2.8125e-01, PNorm = 130.0777, GNorm = 0.2672
Meta loss on this task batch = 2.7628e-01, Meta loss averaged over last 500 steps = 2.8115e-01, PNorm = 130.0805, GNorm = 0.3085
Meta loss on this task batch = 2.5276e-01, Meta loss averaged over last 500 steps = 2.8103e-01, PNorm = 130.0846, GNorm = 0.2705
Meta loss on this task batch = 2.5862e-01, Meta loss averaged over last 500 steps = 2.8095e-01, PNorm = 130.0890, GNorm = 0.3140
Meta loss on this task batch = 2.0907e-01, Meta loss averaged over last 500 steps = 2.8087e-01, PNorm = 130.0953, GNorm = 0.2324
Meta loss on this task batch = 2.1443e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 130.1007, GNorm = 0.3055
Meta loss on this task batch = 3.2765e-01, Meta loss averaged over last 500 steps = 2.8098e-01, PNorm = 130.1052, GNorm = 0.3297
Meta loss on this task batch = 2.8393e-01, Meta loss averaged over last 500 steps = 2.8094e-01, PNorm = 130.1090, GNorm = 0.2732
Meta loss on this task batch = 2.5341e-01, Meta loss averaged over last 500 steps = 2.8090e-01, PNorm = 130.1110, GNorm = 0.2634
Meta loss on this task batch = 2.5503e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 130.1114, GNorm = 0.2657
Meta loss on this task batch = 2.8892e-01, Meta loss averaged over last 500 steps = 2.8075e-01, PNorm = 130.1119, GNorm = 0.2844
Meta loss on this task batch = 2.4701e-01, Meta loss averaged over last 500 steps = 2.8073e-01, PNorm = 130.1105, GNorm = 0.2624
Meta loss on this task batch = 1.8885e-01, Meta loss averaged over last 500 steps = 2.8056e-01, PNorm = 130.1088, GNorm = 0.2304
Meta loss on this task batch = 3.7437e-01, Meta loss averaged over last 500 steps = 2.8074e-01, PNorm = 130.1077, GNorm = 0.4232
Took 157.67378640174866 seconds to complete one epoch of meta training
Took 167.4546036720276 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459106
Epoch 597
Meta loss on this task batch = 2.7940e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 130.1080, GNorm = 0.2869
Meta loss on this task batch = 2.7676e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 130.1084, GNorm = 0.3090
Meta loss on this task batch = 2.5750e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 130.1102, GNorm = 0.2632
Meta loss on this task batch = 3.3917e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 130.1110, GNorm = 0.3350
Meta loss on this task batch = 2.7794e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 130.1131, GNorm = 0.3211
Meta loss on this task batch = 2.3721e-01, Meta loss averaged over last 500 steps = 2.8068e-01, PNorm = 130.1157, GNorm = 0.2939
Meta loss on this task batch = 2.5439e-01, Meta loss averaged over last 500 steps = 2.8060e-01, PNorm = 130.1177, GNorm = 0.2893
Meta loss on this task batch = 2.5842e-01, Meta loss averaged over last 500 steps = 2.8038e-01, PNorm = 130.1206, GNorm = 0.2632
Meta loss on this task batch = 3.3188e-01, Meta loss averaged over last 500 steps = 2.8046e-01, PNorm = 130.1255, GNorm = 0.3284
Meta loss on this task batch = 2.9135e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 130.1303, GNorm = 0.3288
Meta loss on this task batch = 2.8618e-01, Meta loss averaged over last 500 steps = 2.8072e-01, PNorm = 130.1352, GNorm = 0.2904
Meta loss on this task batch = 2.9734e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 130.1414, GNorm = 0.2911
Meta loss on this task batch = 3.1679e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 130.1491, GNorm = 0.2934
Meta loss on this task batch = 2.4439e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 130.1569, GNorm = 0.2829
Meta loss on this task batch = 2.5683e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 130.1650, GNorm = 0.3054
Meta loss on this task batch = 3.0171e-01, Meta loss averaged over last 500 steps = 2.8071e-01, PNorm = 130.1711, GNorm = 0.3856
Meta loss on this task batch = 3.0932e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 130.1772, GNorm = 0.2991
Meta loss on this task batch = 2.2117e-01, Meta loss averaged over last 500 steps = 2.8066e-01, PNorm = 130.1831, GNorm = 0.2453
Meta loss on this task batch = 3.1559e-01, Meta loss averaged over last 500 steps = 2.8069e-01, PNorm = 130.1883, GNorm = 0.3817
Took 158.12466406822205 seconds to complete one epoch of meta training
Took 169.30521321296692 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486177
Epoch 598
Meta loss on this task batch = 2.1368e-01, Meta loss averaged over last 500 steps = 2.8058e-01, PNorm = 130.1932, GNorm = 0.2326
Meta loss on this task batch = 2.6875e-01, Meta loss averaged over last 500 steps = 2.8044e-01, PNorm = 130.1952, GNorm = 0.3017
Meta loss on this task batch = 2.5330e-01, Meta loss averaged over last 500 steps = 2.8039e-01, PNorm = 130.1982, GNorm = 0.2661
Meta loss on this task batch = 3.0416e-01, Meta loss averaged over last 500 steps = 2.8050e-01, PNorm = 130.2025, GNorm = 0.2751
Meta loss on this task batch = 3.1751e-01, Meta loss averaged over last 500 steps = 2.8060e-01, PNorm = 130.2080, GNorm = 0.2582
Meta loss on this task batch = 2.7791e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 130.2155, GNorm = 0.2782
Meta loss on this task batch = 2.6544e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 130.2224, GNorm = 0.2720
Meta loss on this task batch = 3.1586e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 130.2267, GNorm = 0.3282
Meta loss on this task batch = 2.6432e-01, Meta loss averaged over last 500 steps = 2.8056e-01, PNorm = 130.2322, GNorm = 0.2770
Meta loss on this task batch = 2.6102e-01, Meta loss averaged over last 500 steps = 2.8056e-01, PNorm = 130.2392, GNorm = 0.2525
Meta loss on this task batch = 2.9900e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 130.2436, GNorm = 0.3275
Meta loss on this task batch = 3.2904e-01, Meta loss averaged over last 500 steps = 2.8078e-01, PNorm = 130.2454, GNorm = 0.3088
Meta loss on this task batch = 2.7933e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 130.2461, GNorm = 0.3207
Meta loss on this task batch = 3.2560e-01, Meta loss averaged over last 500 steps = 2.8083e-01, PNorm = 130.2468, GNorm = 0.2903
Meta loss on this task batch = 2.6825e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 130.2504, GNorm = 0.2969
Meta loss on this task batch = 2.8091e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 130.2557, GNorm = 0.3032
Meta loss on this task batch = 2.9634e-01, Meta loss averaged over last 500 steps = 2.8084e-01, PNorm = 130.2624, GNorm = 0.3055
Meta loss on this task batch = 2.7208e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 130.2648, GNorm = 0.2831
Meta loss on this task batch = 1.8357e-01, Meta loss averaged over last 500 steps = 2.8065e-01, PNorm = 130.2675, GNorm = 0.3021
Took 157.8426775932312 seconds to complete one epoch of meta training
Took 168.23067903518677 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456651
Epoch 599
Meta loss on this task batch = 2.9469e-01, Meta loss averaged over last 500 steps = 2.8062e-01, PNorm = 130.2695, GNorm = 0.2623
Meta loss on this task batch = 2.4765e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 130.2710, GNorm = 0.2901
Meta loss on this task batch = 2.4683e-01, Meta loss averaged over last 500 steps = 2.8050e-01, PNorm = 130.2728, GNorm = 0.2638
Meta loss on this task batch = 2.9253e-01, Meta loss averaged over last 500 steps = 2.8045e-01, PNorm = 130.2748, GNorm = 0.2834
Meta loss on this task batch = 2.4795e-01, Meta loss averaged over last 500 steps = 2.8038e-01, PNorm = 130.2758, GNorm = 0.3228
Meta loss on this task batch = 2.4950e-01, Meta loss averaged over last 500 steps = 2.8036e-01, PNorm = 130.2781, GNorm = 0.2844
Meta loss on this task batch = 3.2015e-01, Meta loss averaged over last 500 steps = 2.8043e-01, PNorm = 130.2804, GNorm = 0.3023
Meta loss on this task batch = 2.0601e-01, Meta loss averaged over last 500 steps = 2.8033e-01, PNorm = 130.2829, GNorm = 0.2638
Meta loss on this task batch = 2.3723e-01, Meta loss averaged over last 500 steps = 2.8026e-01, PNorm = 130.2865, GNorm = 0.2749
Meta loss on this task batch = 3.1291e-01, Meta loss averaged over last 500 steps = 2.8031e-01, PNorm = 130.2901, GNorm = 0.3298
Meta loss on this task batch = 2.5157e-01, Meta loss averaged over last 500 steps = 2.8029e-01, PNorm = 130.2949, GNorm = 0.2729
Meta loss on this task batch = 3.0904e-01, Meta loss averaged over last 500 steps = 2.8033e-01, PNorm = 130.3010, GNorm = 0.2932
Meta loss on this task batch = 3.3686e-01, Meta loss averaged over last 500 steps = 2.8028e-01, PNorm = 130.3064, GNorm = 0.3039
Meta loss on this task batch = 2.8238e-01, Meta loss averaged over last 500 steps = 2.8018e-01, PNorm = 130.3140, GNorm = 0.2777
Meta loss on this task batch = 3.2479e-01, Meta loss averaged over last 500 steps = 2.8032e-01, PNorm = 130.3219, GNorm = 0.3145
Meta loss on this task batch = 2.7921e-01, Meta loss averaged over last 500 steps = 2.8038e-01, PNorm = 130.3289, GNorm = 0.2952
Meta loss on this task batch = 2.9136e-01, Meta loss averaged over last 500 steps = 2.8045e-01, PNorm = 130.3339, GNorm = 0.3408
Meta loss on this task batch = 2.9405e-01, Meta loss averaged over last 500 steps = 2.8046e-01, PNorm = 130.3388, GNorm = 0.2819
Meta loss on this task batch = 2.5486e-01, Meta loss averaged over last 500 steps = 2.8046e-01, PNorm = 130.3446, GNorm = 0.3065
Took 158.4306025505066 seconds to complete one epoch of meta training
Took 168.08775544166565 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498171
Epoch 600
Meta loss on this task batch = 2.4743e-01, Meta loss averaged over last 500 steps = 2.8040e-01, PNorm = 130.3497, GNorm = 0.2748
Meta loss on this task batch = 3.0377e-01, Meta loss averaged over last 500 steps = 2.8046e-01, PNorm = 130.3523, GNorm = 0.2901
Meta loss on this task batch = 2.7831e-01, Meta loss averaged over last 500 steps = 2.8044e-01, PNorm = 130.3533, GNorm = 0.3437
Meta loss on this task batch = 3.1702e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 130.3540, GNorm = 0.3334
Meta loss on this task batch = 2.4650e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 130.3518, GNorm = 0.2767
Meta loss on this task batch = 3.4195e-01, Meta loss averaged over last 500 steps = 2.8084e-01, PNorm = 130.3497, GNorm = 0.3605
Meta loss on this task batch = 3.0660e-01, Meta loss averaged over last 500 steps = 2.8089e-01, PNorm = 130.3490, GNorm = 0.3233
Meta loss on this task batch = 3.4178e-01, Meta loss averaged over last 500 steps = 2.8103e-01, PNorm = 130.3474, GNorm = 0.3569
Meta loss on this task batch = 2.3655e-01, Meta loss averaged over last 500 steps = 2.8100e-01, PNorm = 130.3446, GNorm = 0.2670
Meta loss on this task batch = 2.8679e-01, Meta loss averaged over last 500 steps = 2.8098e-01, PNorm = 130.3451, GNorm = 0.3121
Meta loss on this task batch = 3.5083e-01, Meta loss averaged over last 500 steps = 2.8109e-01, PNorm = 130.3501, GNorm = 0.3273
Meta loss on this task batch = 2.5824e-01, Meta loss averaged over last 500 steps = 2.8105e-01, PNorm = 130.3569, GNorm = 0.2747
Meta loss on this task batch = 2.7508e-01, Meta loss averaged over last 500 steps = 2.8104e-01, PNorm = 130.3678, GNorm = 0.3097
Meta loss on this task batch = 2.4115e-01, Meta loss averaged over last 500 steps = 2.8102e-01, PNorm = 130.3764, GNorm = 0.2597
Meta loss on this task batch = 3.1819e-01, Meta loss averaged over last 500 steps = 2.8106e-01, PNorm = 130.3841, GNorm = 0.2944
Meta loss on this task batch = 2.2416e-01, Meta loss averaged over last 500 steps = 2.8090e-01, PNorm = 130.3907, GNorm = 0.2567
Meta loss on this task batch = 2.8098e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 130.3956, GNorm = 0.3023
Meta loss on this task batch = 3.8560e-01, Meta loss averaged over last 500 steps = 2.8100e-01, PNorm = 130.3981, GNorm = 0.3813
Meta loss on this task batch = 2.0133e-01, Meta loss averaged over last 500 steps = 2.8083e-01, PNorm = 130.4013, GNorm = 0.3165
Took 157.64118766784668 seconds to complete one epoch of meta training
Took 167.90592646598816 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480033
Epoch 601
Meta loss on this task batch = 2.7866e-01, Meta loss averaged over last 500 steps = 2.8075e-01, PNorm = 130.4056, GNorm = 0.2768
Meta loss on this task batch = 2.3636e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 130.4091, GNorm = 0.2648
Meta loss on this task batch = 2.7586e-01, Meta loss averaged over last 500 steps = 2.8055e-01, PNorm = 130.4127, GNorm = 0.3008
Meta loss on this task batch = 2.7220e-01, Meta loss averaged over last 500 steps = 2.8052e-01, PNorm = 130.4160, GNorm = 0.2790
Meta loss on this task batch = 2.9007e-01, Meta loss averaged over last 500 steps = 2.8057e-01, PNorm = 130.4186, GNorm = 0.2697
Meta loss on this task batch = 2.7557e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 130.4213, GNorm = 0.2768
Meta loss on this task batch = 2.6768e-01, Meta loss averaged over last 500 steps = 2.8045e-01, PNorm = 130.4224, GNorm = 0.2869
Meta loss on this task batch = 2.6400e-01, Meta loss averaged over last 500 steps = 2.8047e-01, PNorm = 130.4215, GNorm = 0.3401
Meta loss on this task batch = 3.0893e-01, Meta loss averaged over last 500 steps = 2.8048e-01, PNorm = 130.4198, GNorm = 0.3050
Meta loss on this task batch = 2.5742e-01, Meta loss averaged over last 500 steps = 2.8047e-01, PNorm = 130.4166, GNorm = 0.2921
Meta loss on this task batch = 3.2223e-01, Meta loss averaged over last 500 steps = 2.8069e-01, PNorm = 130.4122, GNorm = 0.2869
Meta loss on this task batch = 2.5023e-01, Meta loss averaged over last 500 steps = 2.8071e-01, PNorm = 130.4082, GNorm = 0.2489
Meta loss on this task batch = 2.8866e-01, Meta loss averaged over last 500 steps = 2.8075e-01, PNorm = 130.4077, GNorm = 0.2771
Meta loss on this task batch = 2.9075e-01, Meta loss averaged over last 500 steps = 2.8075e-01, PNorm = 130.4096, GNorm = 0.3078
Meta loss on this task batch = 2.7766e-01, Meta loss averaged over last 500 steps = 2.8073e-01, PNorm = 130.4120, GNorm = 0.3070
Meta loss on this task batch = 2.7304e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 130.4178, GNorm = 0.2613
Meta loss on this task batch = 2.8392e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 130.4244, GNorm = 0.3007
Meta loss on this task batch = 3.5499e-01, Meta loss averaged over last 500 steps = 2.8069e-01, PNorm = 130.4315, GNorm = 0.3023
Meta loss on this task batch = 3.4618e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 130.4374, GNorm = 0.3440
Took 159.46654081344604 seconds to complete one epoch of meta training
Took 170.6796417236328 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475373
Epoch 602
Meta loss on this task batch = 3.0968e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 130.4431, GNorm = 0.2922
Meta loss on this task batch = 2.5401e-01, Meta loss averaged over last 500 steps = 2.8072e-01, PNorm = 130.4512, GNorm = 0.2602
Meta loss on this task batch = 2.3537e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 130.4594, GNorm = 0.2581
Meta loss on this task batch = 2.8195e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 130.4694, GNorm = 0.2741
Meta loss on this task batch = 2.7961e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 130.4797, GNorm = 0.2937
Meta loss on this task batch = 2.6797e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 130.4896, GNorm = 0.2684
Meta loss on this task batch = 3.2100e-01, Meta loss averaged over last 500 steps = 2.8089e-01, PNorm = 130.4955, GNorm = 0.3426
Meta loss on this task batch = 3.2204e-01, Meta loss averaged over last 500 steps = 2.8087e-01, PNorm = 130.5009, GNorm = 0.2670
Meta loss on this task batch = 3.3109e-01, Meta loss averaged over last 500 steps = 2.8110e-01, PNorm = 130.5029, GNorm = 0.3157
Meta loss on this task batch = 2.4676e-01, Meta loss averaged over last 500 steps = 2.8099e-01, PNorm = 130.5055, GNorm = 0.2973
Meta loss on this task batch = 2.8226e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 130.5096, GNorm = 0.2722
Meta loss on this task batch = 2.6492e-01, Meta loss averaged over last 500 steps = 2.8087e-01, PNorm = 130.5129, GNorm = 0.2805
Meta loss on this task batch = 2.9128e-01, Meta loss averaged over last 500 steps = 2.8083e-01, PNorm = 130.5158, GNorm = 0.2747
Meta loss on this task batch = 3.3554e-01, Meta loss averaged over last 500 steps = 2.8095e-01, PNorm = 130.5188, GNorm = 0.3139
Meta loss on this task batch = 2.3561e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 130.5234, GNorm = 0.2387
Meta loss on this task batch = 3.0091e-01, Meta loss averaged over last 500 steps = 2.8095e-01, PNorm = 130.5285, GNorm = 0.3114
Meta loss on this task batch = 2.8541e-01, Meta loss averaged over last 500 steps = 2.8092e-01, PNorm = 130.5342, GNorm = 0.2850
Meta loss on this task batch = 2.3280e-01, Meta loss averaged over last 500 steps = 2.8083e-01, PNorm = 130.5390, GNorm = 0.2654
Meta loss on this task batch = 3.0273e-01, Meta loss averaged over last 500 steps = 2.8087e-01, PNorm = 130.5455, GNorm = 0.2976
Took 161.73651909828186 seconds to complete one epoch of meta training
Took 172.7724208831787 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470090
Epoch 603
Meta loss on this task batch = 2.5814e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 130.5505, GNorm = 0.2802
Meta loss on this task batch = 2.6249e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 130.5544, GNorm = 0.2696
Meta loss on this task batch = 3.2634e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 130.5570, GNorm = 0.3274
Meta loss on this task batch = 3.2383e-01, Meta loss averaged over last 500 steps = 2.8099e-01, PNorm = 130.5589, GNorm = 0.3063
Meta loss on this task batch = 2.9577e-01, Meta loss averaged over last 500 steps = 2.8103e-01, PNorm = 130.5589, GNorm = 0.2773
Meta loss on this task batch = 2.2258e-01, Meta loss averaged over last 500 steps = 2.8092e-01, PNorm = 130.5618, GNorm = 0.2683
Meta loss on this task batch = 2.7445e-01, Meta loss averaged over last 500 steps = 2.8094e-01, PNorm = 130.5648, GNorm = 0.2786
Meta loss on this task batch = 2.4177e-01, Meta loss averaged over last 500 steps = 2.8084e-01, PNorm = 130.5685, GNorm = 0.2624
Meta loss on this task batch = 2.7011e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 130.5720, GNorm = 0.2808
Meta loss on this task batch = 3.2103e-01, Meta loss averaged over last 500 steps = 2.8087e-01, PNorm = 130.5726, GNorm = 0.3538
Meta loss on this task batch = 2.8858e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 130.5746, GNorm = 0.3104
Meta loss on this task batch = 2.7046e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 130.5764, GNorm = 0.2741
Meta loss on this task batch = 2.9132e-01, Meta loss averaged over last 500 steps = 2.8094e-01, PNorm = 130.5797, GNorm = 0.2768
Meta loss on this task batch = 3.2482e-01, Meta loss averaged over last 500 steps = 2.8112e-01, PNorm = 130.5807, GNorm = 0.3673
Meta loss on this task batch = 2.6747e-01, Meta loss averaged over last 500 steps = 2.8106e-01, PNorm = 130.5839, GNorm = 0.2783
Meta loss on this task batch = 2.7335e-01, Meta loss averaged over last 500 steps = 2.8108e-01, PNorm = 130.5893, GNorm = 0.2824
Meta loss on this task batch = 3.3291e-01, Meta loss averaged over last 500 steps = 2.8122e-01, PNorm = 130.5971, GNorm = 0.3284
Meta loss on this task batch = 2.6382e-01, Meta loss averaged over last 500 steps = 2.8111e-01, PNorm = 130.6058, GNorm = 0.2747
Meta loss on this task batch = 2.7627e-01, Meta loss averaged over last 500 steps = 2.8110e-01, PNorm = 130.6129, GNorm = 0.3241
Took 157.63065695762634 seconds to complete one epoch of meta training
Took 167.8275420665741 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474148
Epoch 604
Meta loss on this task batch = 2.7171e-01, Meta loss averaged over last 500 steps = 2.8102e-01, PNorm = 130.6188, GNorm = 0.2646
Meta loss on this task batch = 3.4269e-01, Meta loss averaged over last 500 steps = 2.8110e-01, PNorm = 130.6230, GNorm = 0.3707
Meta loss on this task batch = 2.0415e-01, Meta loss averaged over last 500 steps = 2.8095e-01, PNorm = 130.6267, GNorm = 0.2749
Meta loss on this task batch = 2.4478e-01, Meta loss averaged over last 500 steps = 2.8099e-01, PNorm = 130.6300, GNorm = 0.2650
Meta loss on this task batch = 3.5092e-01, Meta loss averaged over last 500 steps = 2.8107e-01, PNorm = 130.6338, GNorm = 0.2988
Meta loss on this task batch = 3.0963e-01, Meta loss averaged over last 500 steps = 2.8105e-01, PNorm = 130.6366, GNorm = 0.2918
Meta loss on this task batch = 2.7355e-01, Meta loss averaged over last 500 steps = 2.8102e-01, PNorm = 130.6389, GNorm = 0.3387
Meta loss on this task batch = 2.3013e-01, Meta loss averaged over last 500 steps = 2.8099e-01, PNorm = 130.6415, GNorm = 0.2636
Meta loss on this task batch = 3.0546e-01, Meta loss averaged over last 500 steps = 2.8108e-01, PNorm = 130.6430, GNorm = 0.3449
Meta loss on this task batch = 2.8090e-01, Meta loss averaged over last 500 steps = 2.8110e-01, PNorm = 130.6449, GNorm = 0.2966
Meta loss on this task batch = 2.7906e-01, Meta loss averaged over last 500 steps = 2.8106e-01, PNorm = 130.6460, GNorm = 0.2888
Meta loss on this task batch = 2.9088e-01, Meta loss averaged over last 500 steps = 2.8108e-01, PNorm = 130.6458, GNorm = 0.2869
Meta loss on this task batch = 2.8508e-01, Meta loss averaged over last 500 steps = 2.8112e-01, PNorm = 130.6484, GNorm = 0.2844
Meta loss on this task batch = 2.7751e-01, Meta loss averaged over last 500 steps = 2.8107e-01, PNorm = 130.6524, GNorm = 0.2824
Meta loss on this task batch = 2.8020e-01, Meta loss averaged over last 500 steps = 2.8112e-01, PNorm = 130.6570, GNorm = 0.2569
Meta loss on this task batch = 2.9605e-01, Meta loss averaged over last 500 steps = 2.8111e-01, PNorm = 130.6610, GNorm = 0.3067
Meta loss on this task batch = 2.7757e-01, Meta loss averaged over last 500 steps = 2.8109e-01, PNorm = 130.6659, GNorm = 0.2843
Meta loss on this task batch = 2.7112e-01, Meta loss averaged over last 500 steps = 2.8099e-01, PNorm = 130.6722, GNorm = 0.2725
Meta loss on this task batch = 2.3629e-01, Meta loss averaged over last 500 steps = 2.8099e-01, PNorm = 130.6797, GNorm = 0.3000
Took 163.22509765625 seconds to complete one epoch of meta training
Took 174.49544215202332 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472962
Epoch 605
Meta loss on this task batch = 2.8308e-01, Meta loss averaged over last 500 steps = 2.8103e-01, PNorm = 130.6846, GNorm = 0.2766
Meta loss on this task batch = 2.6606e-01, Meta loss averaged over last 500 steps = 2.8108e-01, PNorm = 130.6922, GNorm = 0.2719
Meta loss on this task batch = 2.4361e-01, Meta loss averaged over last 500 steps = 2.8104e-01, PNorm = 130.6984, GNorm = 0.2965
Meta loss on this task batch = 2.5481e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 130.7058, GNorm = 0.2750
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 130.7122, GNorm = 0.2925
Meta loss on this task batch = 2.6854e-01, Meta loss averaged over last 500 steps = 2.8090e-01, PNorm = 130.7180, GNorm = 0.2972
Meta loss on this task batch = 2.2663e-01, Meta loss averaged over last 500 steps = 2.8089e-01, PNorm = 130.7236, GNorm = 0.2540
Meta loss on this task batch = 3.1099e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 130.7278, GNorm = 0.2890
Meta loss on this task batch = 2.5552e-01, Meta loss averaged over last 500 steps = 2.8083e-01, PNorm = 130.7298, GNorm = 0.3299
Meta loss on this task batch = 2.7555e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 130.7310, GNorm = 0.3092
Meta loss on this task batch = 2.8923e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 130.7317, GNorm = 0.2966
Meta loss on this task batch = 2.5590e-01, Meta loss averaged over last 500 steps = 2.8073e-01, PNorm = 130.7344, GNorm = 0.2621
Meta loss on this task batch = 2.6900e-01, Meta loss averaged over last 500 steps = 2.8074e-01, PNorm = 130.7356, GNorm = 0.2854
Meta loss on this task batch = 2.6199e-01, Meta loss averaged over last 500 steps = 2.8074e-01, PNorm = 130.7388, GNorm = 0.2936
Meta loss on this task batch = 3.1396e-01, Meta loss averaged over last 500 steps = 2.8085e-01, PNorm = 130.7421, GNorm = 0.2872
Meta loss on this task batch = 2.8131e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 130.7444, GNorm = 0.3036
Meta loss on this task batch = 3.1431e-01, Meta loss averaged over last 500 steps = 2.8087e-01, PNorm = 130.7462, GNorm = 0.3034
Meta loss on this task batch = 3.4300e-01, Meta loss averaged over last 500 steps = 2.8099e-01, PNorm = 130.7477, GNorm = 0.3096
Meta loss on this task batch = 2.4981e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 130.7486, GNorm = 0.3616
Took 157.16727948188782 seconds to complete one epoch of meta training
Took 166.9862654209137 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476271
Epoch 606
Meta loss on this task batch = 2.8966e-01, Meta loss averaged over last 500 steps = 2.8090e-01, PNorm = 130.7497, GNorm = 0.2709
Meta loss on this task batch = 2.7383e-01, Meta loss averaged over last 500 steps = 2.8090e-01, PNorm = 130.7541, GNorm = 0.2695
Meta loss on this task batch = 2.6177e-01, Meta loss averaged over last 500 steps = 2.8084e-01, PNorm = 130.7578, GNorm = 0.2738
Meta loss on this task batch = 2.9524e-01, Meta loss averaged over last 500 steps = 2.8092e-01, PNorm = 130.7593, GNorm = 0.3238
Meta loss on this task batch = 2.9897e-01, Meta loss averaged over last 500 steps = 2.8104e-01, PNorm = 130.7619, GNorm = 0.3625
Meta loss on this task batch = 2.9954e-01, Meta loss averaged over last 500 steps = 2.8101e-01, PNorm = 130.7655, GNorm = 0.2951
Meta loss on this task batch = 2.3784e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 130.7694, GNorm = 0.2524
Meta loss on this task batch = 3.2628e-01, Meta loss averaged over last 500 steps = 2.8093e-01, PNorm = 130.7743, GNorm = 0.2743
Meta loss on this task batch = 1.9934e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 130.7816, GNorm = 0.2524
Meta loss on this task batch = 2.5148e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 130.7882, GNorm = 0.2563
Meta loss on this task batch = 2.4978e-01, Meta loss averaged over last 500 steps = 2.8087e-01, PNorm = 130.7964, GNorm = 0.2704
Meta loss on this task batch = 2.7630e-01, Meta loss averaged over last 500 steps = 2.8086e-01, PNorm = 130.8037, GNorm = 0.2637
Meta loss on this task batch = 2.6388e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 130.8100, GNorm = 0.2439
Meta loss on this task batch = 2.8948e-01, Meta loss averaged over last 500 steps = 2.8078e-01, PNorm = 130.8147, GNorm = 0.3050
Meta loss on this task batch = 3.2853e-01, Meta loss averaged over last 500 steps = 2.8083e-01, PNorm = 130.8180, GNorm = 0.3629
Meta loss on this task batch = 2.5031e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 130.8195, GNorm = 0.2649
Meta loss on this task batch = 3.0435e-01, Meta loss averaged over last 500 steps = 2.8084e-01, PNorm = 130.8212, GNorm = 0.3054
Meta loss on this task batch = 2.9821e-01, Meta loss averaged over last 500 steps = 2.8094e-01, PNorm = 130.8236, GNorm = 0.2776
Meta loss on this task batch = 2.7917e-01, Meta loss averaged over last 500 steps = 2.8098e-01, PNorm = 130.8256, GNorm = 0.3226
Took 157.45097374916077 seconds to complete one epoch of meta training
Took 168.99444794654846 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475971
Epoch 607
Meta loss on this task batch = 2.8111e-01, Meta loss averaged over last 500 steps = 2.8083e-01, PNorm = 130.8288, GNorm = 0.2829
Meta loss on this task batch = 2.7112e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 130.8344, GNorm = 0.2683
Meta loss on this task batch = 2.5907e-01, Meta loss averaged over last 500 steps = 2.8086e-01, PNorm = 130.8389, GNorm = 0.2920
Meta loss on this task batch = 2.5589e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 130.8436, GNorm = 0.2707
Meta loss on this task batch = 3.4853e-01, Meta loss averaged over last 500 steps = 2.8097e-01, PNorm = 130.8495, GNorm = 0.3367
Meta loss on this task batch = 2.7465e-01, Meta loss averaged over last 500 steps = 2.8089e-01, PNorm = 130.8547, GNorm = 0.3284
Meta loss on this task batch = 2.8722e-01, Meta loss averaged over last 500 steps = 2.8096e-01, PNorm = 130.8616, GNorm = 0.2695
Meta loss on this task batch = 2.8264e-01, Meta loss averaged over last 500 steps = 2.8095e-01, PNorm = 130.8680, GNorm = 0.2802
Meta loss on this task batch = 2.8426e-01, Meta loss averaged over last 500 steps = 2.8097e-01, PNorm = 130.8750, GNorm = 0.2827
Meta loss on this task batch = 2.9882e-01, Meta loss averaged over last 500 steps = 2.8093e-01, PNorm = 130.8792, GNorm = 0.3421
Meta loss on this task batch = 2.9395e-01, Meta loss averaged over last 500 steps = 2.8093e-01, PNorm = 130.8834, GNorm = 0.2771
Meta loss on this task batch = 2.8565e-01, Meta loss averaged over last 500 steps = 2.8094e-01, PNorm = 130.8877, GNorm = 0.2577
Meta loss on this task batch = 2.2852e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 130.8910, GNorm = 0.2624
Meta loss on this task batch = 2.5959e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 130.8950, GNorm = 0.2882
Meta loss on this task batch = 2.6500e-01, Meta loss averaged over last 500 steps = 2.8056e-01, PNorm = 130.9000, GNorm = 0.3246
Meta loss on this task batch = 2.5382e-01, Meta loss averaged over last 500 steps = 2.8039e-01, PNorm = 130.9019, GNorm = 0.2790
Meta loss on this task batch = 2.9760e-01, Meta loss averaged over last 500 steps = 2.8051e-01, PNorm = 130.9049, GNorm = 0.3048
Meta loss on this task batch = 2.9132e-01, Meta loss averaged over last 500 steps = 2.8066e-01, PNorm = 130.9050, GNorm = 0.3442
Meta loss on this task batch = 1.9183e-01, Meta loss averaged over last 500 steps = 2.8051e-01, PNorm = 130.9055, GNorm = 0.2822
Took 155.80493712425232 seconds to complete one epoch of meta training
Took 165.8793342113495 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481805
Epoch 608
Meta loss on this task batch = 3.2577e-01, Meta loss averaged over last 500 steps = 2.8058e-01, PNorm = 130.9061, GNorm = 0.3125
Meta loss on this task batch = 2.3437e-01, Meta loss averaged over last 500 steps = 2.8048e-01, PNorm = 130.9089, GNorm = 0.2482
Meta loss on this task batch = 2.6013e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 130.9136, GNorm = 0.2912
Meta loss on this task batch = 2.7232e-01, Meta loss averaged over last 500 steps = 2.8057e-01, PNorm = 130.9189, GNorm = 0.2950
Meta loss on this task batch = 2.7443e-01, Meta loss averaged over last 500 steps = 2.8053e-01, PNorm = 130.9263, GNorm = 0.2632
Meta loss on this task batch = 2.8004e-01, Meta loss averaged over last 500 steps = 2.8056e-01, PNorm = 130.9375, GNorm = 0.3103
Meta loss on this task batch = 2.4628e-01, Meta loss averaged over last 500 steps = 2.8045e-01, PNorm = 130.9507, GNorm = 0.2770
Meta loss on this task batch = 2.9923e-01, Meta loss averaged over last 500 steps = 2.8057e-01, PNorm = 130.9649, GNorm = 0.3376
Meta loss on this task batch = 2.6411e-01, Meta loss averaged over last 500 steps = 2.8055e-01, PNorm = 130.9792, GNorm = 0.3019
Meta loss on this task batch = 3.3268e-01, Meta loss averaged over last 500 steps = 2.8074e-01, PNorm = 130.9901, GNorm = 0.3445
Meta loss on this task batch = 2.7819e-01, Meta loss averaged over last 500 steps = 2.8073e-01, PNorm = 130.9973, GNorm = 0.2928
Meta loss on this task batch = 3.2292e-01, Meta loss averaged over last 500 steps = 2.8075e-01, PNorm = 131.0011, GNorm = 0.3379
Meta loss on this task batch = 2.6293e-01, Meta loss averaged over last 500 steps = 2.8073e-01, PNorm = 131.0052, GNorm = 0.2606
Meta loss on this task batch = 3.1584e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 131.0081, GNorm = 0.3460
Meta loss on this task batch = 2.7676e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 131.0078, GNorm = 0.3216
Meta loss on this task batch = 2.7905e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 131.0036, GNorm = 0.3089
Meta loss on this task batch = 2.7675e-01, Meta loss averaged over last 500 steps = 2.8086e-01, PNorm = 131.0009, GNorm = 0.3022
Meta loss on this task batch = 3.4066e-01, Meta loss averaged over last 500 steps = 2.8105e-01, PNorm = 130.9949, GNorm = 0.4034
Meta loss on this task batch = 2.6246e-01, Meta loss averaged over last 500 steps = 2.8087e-01, PNorm = 130.9908, GNorm = 0.3333
Took 157.4804995059967 seconds to complete one epoch of meta training
Took 168.2403531074524 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496593
Epoch 609
Meta loss on this task batch = 2.9205e-01, Meta loss averaged over last 500 steps = 2.8092e-01, PNorm = 130.9895, GNorm = 0.3073
Meta loss on this task batch = 2.6587e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 130.9892, GNorm = 0.2627
Meta loss on this task batch = 2.4220e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 130.9915, GNorm = 0.2544
Meta loss on this task batch = 2.8416e-01, Meta loss averaged over last 500 steps = 2.8078e-01, PNorm = 130.9975, GNorm = 0.3147
Meta loss on this task batch = 3.3994e-01, Meta loss averaged over last 500 steps = 2.8086e-01, PNorm = 131.0075, GNorm = 0.3605
Meta loss on this task batch = 3.3143e-01, Meta loss averaged over last 500 steps = 2.8094e-01, PNorm = 131.0181, GNorm = 0.3028
Meta loss on this task batch = 2.5427e-01, Meta loss averaged over last 500 steps = 2.8098e-01, PNorm = 131.0293, GNorm = 0.2726
Meta loss on this task batch = 2.6435e-01, Meta loss averaged over last 500 steps = 2.8103e-01, PNorm = 131.0426, GNorm = 0.3113
Meta loss on this task batch = 2.7611e-01, Meta loss averaged over last 500 steps = 2.8095e-01, PNorm = 131.0547, GNorm = 0.2951
Meta loss on this task batch = 3.2637e-01, Meta loss averaged over last 500 steps = 2.8098e-01, PNorm = 131.0655, GNorm = 0.3175
Meta loss on this task batch = 3.1305e-01, Meta loss averaged over last 500 steps = 2.8108e-01, PNorm = 131.0743, GNorm = 0.3239
Meta loss on this task batch = 3.0102e-01, Meta loss averaged over last 500 steps = 2.8120e-01, PNorm = 131.0812, GNorm = 0.3122
Meta loss on this task batch = 3.1370e-01, Meta loss averaged over last 500 steps = 2.8120e-01, PNorm = 131.0853, GNorm = 0.3223
Meta loss on this task batch = 2.5552e-01, Meta loss averaged over last 500 steps = 2.8113e-01, PNorm = 131.0876, GNorm = 0.2624
Meta loss on this task batch = 2.3608e-01, Meta loss averaged over last 500 steps = 2.8095e-01, PNorm = 131.0914, GNorm = 0.2700
Meta loss on this task batch = 2.4290e-01, Meta loss averaged over last 500 steps = 2.8083e-01, PNorm = 131.0961, GNorm = 0.2650
Meta loss on this task batch = 2.5119e-01, Meta loss averaged over last 500 steps = 2.8068e-01, PNorm = 131.0998, GNorm = 0.2384
Meta loss on this task batch = 2.9962e-01, Meta loss averaged over last 500 steps = 2.8073e-01, PNorm = 131.1051, GNorm = 0.3171
Meta loss on this task batch = 2.8302e-01, Meta loss averaged over last 500 steps = 2.8066e-01, PNorm = 131.1107, GNorm = 0.3036
Took 157.81492137908936 seconds to complete one epoch of meta training
Took 168.4028160572052 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470717
Epoch 610
Meta loss on this task batch = 2.4626e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 131.1160, GNorm = 0.2658
Meta loss on this task batch = 3.0236e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 131.1189, GNorm = 0.3099
Meta loss on this task batch = 2.8313e-01, Meta loss averaged over last 500 steps = 2.8072e-01, PNorm = 131.1223, GNorm = 0.2701
Meta loss on this task batch = 2.9283e-01, Meta loss averaged over last 500 steps = 2.8068e-01, PNorm = 131.1243, GNorm = 0.2733
Meta loss on this task batch = 2.5240e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 131.1266, GNorm = 0.2842
Meta loss on this task batch = 3.6030e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 131.1284, GNorm = 0.3259
Meta loss on this task batch = 3.0181e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 131.1301, GNorm = 0.3317
Meta loss on this task batch = 2.6335e-01, Meta loss averaged over last 500 steps = 2.8064e-01, PNorm = 131.1332, GNorm = 0.3008
Meta loss on this task batch = 3.0372e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 131.1391, GNorm = 0.2928
Meta loss on this task batch = 3.0042e-01, Meta loss averaged over last 500 steps = 2.8064e-01, PNorm = 131.1481, GNorm = 0.3110
Meta loss on this task batch = 2.5304e-01, Meta loss averaged over last 500 steps = 2.8059e-01, PNorm = 131.1585, GNorm = 0.2790
Meta loss on this task batch = 2.5289e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 131.1686, GNorm = 0.2664
Meta loss on this task batch = 2.5974e-01, Meta loss averaged over last 500 steps = 2.8050e-01, PNorm = 131.1790, GNorm = 0.2608
Meta loss on this task batch = 2.8129e-01, Meta loss averaged over last 500 steps = 2.8048e-01, PNorm = 131.1869, GNorm = 0.3699
Meta loss on this task batch = 2.7175e-01, Meta loss averaged over last 500 steps = 2.8046e-01, PNorm = 131.1934, GNorm = 0.2917
Meta loss on this task batch = 3.1579e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 131.1983, GNorm = 0.2655
Meta loss on this task batch = 2.6371e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 131.2009, GNorm = 0.3118
Meta loss on this task batch = 2.9049e-01, Meta loss averaged over last 500 steps = 2.8065e-01, PNorm = 131.2029, GNorm = 0.3042
Meta loss on this task batch = 2.5780e-01, Meta loss averaged over last 500 steps = 2.8055e-01, PNorm = 131.2036, GNorm = 0.3505
Took 155.90352869033813 seconds to complete one epoch of meta training
Took 165.70174169540405 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487713
Epoch 611
Meta loss on this task batch = 3.0262e-01, Meta loss averaged over last 500 steps = 2.8062e-01, PNorm = 131.2044, GNorm = 0.3376
Meta loss on this task batch = 2.8361e-01, Meta loss averaged over last 500 steps = 2.8059e-01, PNorm = 131.2032, GNorm = 0.2779
Meta loss on this task batch = 2.7417e-01, Meta loss averaged over last 500 steps = 2.8065e-01, PNorm = 131.2030, GNorm = 0.2940
Meta loss on this task batch = 2.5026e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 131.2054, GNorm = 0.2982
Meta loss on this task batch = 2.8948e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 131.2106, GNorm = 0.2967
Meta loss on this task batch = 2.4641e-01, Meta loss averaged over last 500 steps = 2.8052e-01, PNorm = 131.2169, GNorm = 0.2524
Meta loss on this task batch = 3.2172e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 131.2223, GNorm = 0.3031
Meta loss on this task batch = 3.1879e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 131.2307, GNorm = 0.3063
Meta loss on this task batch = 2.8001e-01, Meta loss averaged over last 500 steps = 2.8085e-01, PNorm = 131.2387, GNorm = 0.2780
Meta loss on this task batch = 3.2969e-01, Meta loss averaged over last 500 steps = 2.8096e-01, PNorm = 131.2454, GNorm = 0.2976
Meta loss on this task batch = 3.0426e-01, Meta loss averaged over last 500 steps = 2.8102e-01, PNorm = 131.2538, GNorm = 0.3388
Meta loss on this task batch = 2.4610e-01, Meta loss averaged over last 500 steps = 2.8099e-01, PNorm = 131.2617, GNorm = 0.2654
Meta loss on this task batch = 2.2585e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 131.2702, GNorm = 0.2703
Meta loss on this task batch = 2.2754e-01, Meta loss averaged over last 500 steps = 2.8064e-01, PNorm = 131.2759, GNorm = 0.3017
Meta loss on this task batch = 3.1042e-01, Meta loss averaged over last 500 steps = 2.8066e-01, PNorm = 131.2810, GNorm = 0.3341
Meta loss on this task batch = 2.9243e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 131.2844, GNorm = 0.3014
Meta loss on this task batch = 2.7876e-01, Meta loss averaged over last 500 steps = 2.8061e-01, PNorm = 131.2871, GNorm = 0.2869
Meta loss on this task batch = 2.7318e-01, Meta loss averaged over last 500 steps = 2.8052e-01, PNorm = 131.2874, GNorm = 0.3620
Meta loss on this task batch = 2.5287e-01, Meta loss averaged over last 500 steps = 2.8047e-01, PNorm = 131.2871, GNorm = 0.3696
Took 157.21604299545288 seconds to complete one epoch of meta training
Took 166.80886912345886 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491709
Epoch 612
Meta loss on this task batch = 3.1649e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 131.2856, GNorm = 0.3415
Meta loss on this task batch = 3.1066e-01, Meta loss averaged over last 500 steps = 2.8061e-01, PNorm = 131.2839, GNorm = 0.3200
Meta loss on this task batch = 2.8382e-01, Meta loss averaged over last 500 steps = 2.8056e-01, PNorm = 131.2850, GNorm = 0.3045
Meta loss on this task batch = 2.7522e-01, Meta loss averaged over last 500 steps = 2.8055e-01, PNorm = 131.2852, GNorm = 0.2909
Meta loss on this task batch = 2.8429e-01, Meta loss averaged over last 500 steps = 2.8056e-01, PNorm = 131.2870, GNorm = 0.4519
Meta loss on this task batch = 2.8319e-01, Meta loss averaged over last 500 steps = 2.8061e-01, PNorm = 131.2920, GNorm = 0.2850
Meta loss on this task batch = 2.7061e-01, Meta loss averaged over last 500 steps = 2.8064e-01, PNorm = 131.3003, GNorm = 0.2863
Meta loss on this task batch = 2.5761e-01, Meta loss averaged over last 500 steps = 2.8066e-01, PNorm = 131.3110, GNorm = 0.3204
Meta loss on this task batch = 2.6960e-01, Meta loss averaged over last 500 steps = 2.8060e-01, PNorm = 131.3224, GNorm = 0.2876
Meta loss on this task batch = 2.6685e-01, Meta loss averaged over last 500 steps = 2.8062e-01, PNorm = 131.3331, GNorm = 0.2973
Meta loss on this task batch = 2.5826e-01, Meta loss averaged over last 500 steps = 2.8056e-01, PNorm = 131.3443, GNorm = 0.2768
Meta loss on this task batch = 2.3122e-01, Meta loss averaged over last 500 steps = 2.8057e-01, PNorm = 131.3555, GNorm = 0.2943
Meta loss on this task batch = 2.9278e-01, Meta loss averaged over last 500 steps = 2.8071e-01, PNorm = 131.3667, GNorm = 0.3622
Meta loss on this task batch = 3.5438e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 131.3773, GNorm = 0.3586
Meta loss on this task batch = 3.1065e-01, Meta loss averaged over last 500 steps = 2.8084e-01, PNorm = 131.3864, GNorm = 0.3275
Meta loss on this task batch = 2.8295e-01, Meta loss averaged over last 500 steps = 2.8075e-01, PNorm = 131.3932, GNorm = 0.3568
Meta loss on this task batch = 2.6652e-01, Meta loss averaged over last 500 steps = 2.8083e-01, PNorm = 131.3986, GNorm = 0.2811
Meta loss on this task batch = 2.8550e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 131.4017, GNorm = 0.3457
Meta loss on this task batch = 2.7939e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 131.4036, GNorm = 0.3608
Took 157.90544509887695 seconds to complete one epoch of meta training
Took 168.73980808258057 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504560
Epoch 613
Meta loss on this task batch = 2.6361e-01, Meta loss averaged over last 500 steps = 2.8071e-01, PNorm = 131.4033, GNorm = 0.2993
Meta loss on this task batch = 2.8260e-01, Meta loss averaged over last 500 steps = 2.8065e-01, PNorm = 131.4018, GNorm = 0.3260
Meta loss on this task batch = 2.4938e-01, Meta loss averaged over last 500 steps = 2.8052e-01, PNorm = 131.4040, GNorm = 0.3090
Meta loss on this task batch = 3.1235e-01, Meta loss averaged over last 500 steps = 2.8059e-01, PNorm = 131.4062, GNorm = 0.2703
Meta loss on this task batch = 2.2931e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 131.4083, GNorm = 0.2539
Meta loss on this task batch = 3.1860e-01, Meta loss averaged over last 500 steps = 2.8061e-01, PNorm = 131.4128, GNorm = 0.2892
Meta loss on this task batch = 2.4760e-01, Meta loss averaged over last 500 steps = 2.8048e-01, PNorm = 131.4186, GNorm = 0.2641
Meta loss on this task batch = 2.7398e-01, Meta loss averaged over last 500 steps = 2.8038e-01, PNorm = 131.4260, GNorm = 0.2813
Meta loss on this task batch = 2.9484e-01, Meta loss averaged over last 500 steps = 2.8041e-01, PNorm = 131.4341, GNorm = 0.2552
Meta loss on this task batch = 2.7884e-01, Meta loss averaged over last 500 steps = 2.8045e-01, PNorm = 131.4423, GNorm = 0.3291
Meta loss on this task batch = 3.0629e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 131.4495, GNorm = 0.3302
Meta loss on this task batch = 2.5971e-01, Meta loss averaged over last 500 steps = 2.8047e-01, PNorm = 131.4577, GNorm = 0.2718
Meta loss on this task batch = 2.6262e-01, Meta loss averaged over last 500 steps = 2.8042e-01, PNorm = 131.4664, GNorm = 0.2755
Meta loss on this task batch = 2.3709e-01, Meta loss averaged over last 500 steps = 2.8030e-01, PNorm = 131.4741, GNorm = 0.2814
Meta loss on this task batch = 2.5116e-01, Meta loss averaged over last 500 steps = 2.8030e-01, PNorm = 131.4810, GNorm = 0.2820
Meta loss on this task batch = 2.2127e-01, Meta loss averaged over last 500 steps = 2.8020e-01, PNorm = 131.4863, GNorm = 0.2814
Meta loss on this task batch = 2.5493e-01, Meta loss averaged over last 500 steps = 2.8008e-01, PNorm = 131.4907, GNorm = 0.3249
Meta loss on this task batch = 3.0335e-01, Meta loss averaged over last 500 steps = 2.8012e-01, PNorm = 131.4946, GNorm = 0.3209
Meta loss on this task batch = 3.3611e-01, Meta loss averaged over last 500 steps = 2.8015e-01, PNorm = 131.4947, GNorm = 0.3981
Took 190.25320529937744 seconds to complete one epoch of meta training
Took 200.27247428894043 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502806
Epoch 614
Meta loss on this task batch = 2.2060e-01, Meta loss averaged over last 500 steps = 2.8003e-01, PNorm = 131.4934, GNorm = 0.2384
Meta loss on this task batch = 2.8531e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 131.4910, GNorm = 0.2940
Meta loss on this task batch = 2.9461e-01, Meta loss averaged over last 500 steps = 2.8008e-01, PNorm = 131.4899, GNorm = 0.2976
Meta loss on this task batch = 3.4206e-01, Meta loss averaged over last 500 steps = 2.8029e-01, PNorm = 131.4911, GNorm = 0.3399
Meta loss on this task batch = 2.6332e-01, Meta loss averaged over last 500 steps = 2.8034e-01, PNorm = 131.4914, GNorm = 0.2864
Meta loss on this task batch = 2.8611e-01, Meta loss averaged over last 500 steps = 2.8034e-01, PNorm = 131.4920, GNorm = 0.2814
Meta loss on this task batch = 2.7801e-01, Meta loss averaged over last 500 steps = 2.8034e-01, PNorm = 131.4942, GNorm = 0.3154
Meta loss on this task batch = 2.4719e-01, Meta loss averaged over last 500 steps = 2.8026e-01, PNorm = 131.4996, GNorm = 0.2850
Meta loss on this task batch = 3.0254e-01, Meta loss averaged over last 500 steps = 2.8030e-01, PNorm = 131.5053, GNorm = 0.3233
Meta loss on this task batch = 2.7508e-01, Meta loss averaged over last 500 steps = 2.8029e-01, PNorm = 131.5111, GNorm = 0.2690
Meta loss on this task batch = 2.0485e-01, Meta loss averaged over last 500 steps = 2.8012e-01, PNorm = 131.5170, GNorm = 0.2275
Meta loss on this task batch = 3.4862e-01, Meta loss averaged over last 500 steps = 2.8030e-01, PNorm = 131.5200, GNorm = 0.3037
Meta loss on this task batch = 2.6578e-01, Meta loss averaged over last 500 steps = 2.8028e-01, PNorm = 131.5240, GNorm = 0.2928
Meta loss on this task batch = 3.0590e-01, Meta loss averaged over last 500 steps = 2.8042e-01, PNorm = 131.5283, GNorm = 0.2991
Meta loss on this task batch = 2.2683e-01, Meta loss averaged over last 500 steps = 2.8025e-01, PNorm = 131.5316, GNorm = 0.2521
Meta loss on this task batch = 2.7980e-01, Meta loss averaged over last 500 steps = 2.8025e-01, PNorm = 131.5335, GNorm = 0.2979
Meta loss on this task batch = 2.8865e-01, Meta loss averaged over last 500 steps = 2.8020e-01, PNorm = 131.5364, GNorm = 0.2838
Meta loss on this task batch = 2.7099e-01, Meta loss averaged over last 500 steps = 2.8027e-01, PNorm = 131.5382, GNorm = 0.2925
Meta loss on this task batch = 2.6196e-01, Meta loss averaged over last 500 steps = 2.8019e-01, PNorm = 131.5415, GNorm = 0.3553
Took 157.38315081596375 seconds to complete one epoch of meta training
Took 167.1546413898468 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.448961
Epoch 615
Meta loss on this task batch = 2.9423e-01, Meta loss averaged over last 500 steps = 2.8031e-01, PNorm = 131.5430, GNorm = 0.2949
Meta loss on this task batch = 2.6972e-01, Meta loss averaged over last 500 steps = 2.8022e-01, PNorm = 131.5452, GNorm = 0.2680
Meta loss on this task batch = 2.8528e-01, Meta loss averaged over last 500 steps = 2.8028e-01, PNorm = 131.5475, GNorm = 0.2776
Meta loss on this task batch = 3.0451e-01, Meta loss averaged over last 500 steps = 2.8023e-01, PNorm = 131.5497, GNorm = 0.2932
Meta loss on this task batch = 2.7470e-01, Meta loss averaged over last 500 steps = 2.8007e-01, PNorm = 131.5507, GNorm = 0.2728
Meta loss on this task batch = 2.6697e-01, Meta loss averaged over last 500 steps = 2.8014e-01, PNorm = 131.5511, GNorm = 0.2619
Meta loss on this task batch = 2.6314e-01, Meta loss averaged over last 500 steps = 2.8014e-01, PNorm = 131.5505, GNorm = 0.3190
Meta loss on this task batch = 2.9740e-01, Meta loss averaged over last 500 steps = 2.8026e-01, PNorm = 131.5503, GNorm = 0.3197
Meta loss on this task batch = 2.6110e-01, Meta loss averaged over last 500 steps = 2.8022e-01, PNorm = 131.5485, GNorm = 0.2807
Meta loss on this task batch = 2.6909e-01, Meta loss averaged over last 500 steps = 2.8023e-01, PNorm = 131.5478, GNorm = 0.3840
Meta loss on this task batch = 2.4096e-01, Meta loss averaged over last 500 steps = 2.8020e-01, PNorm = 131.5491, GNorm = 0.3041
Meta loss on this task batch = 3.3991e-01, Meta loss averaged over last 500 steps = 2.8032e-01, PNorm = 131.5501, GNorm = 0.3049
Meta loss on this task batch = 3.1707e-01, Meta loss averaged over last 500 steps = 2.8034e-01, PNorm = 131.5509, GNorm = 0.3221
Meta loss on this task batch = 2.3656e-01, Meta loss averaged over last 500 steps = 2.8032e-01, PNorm = 131.5543, GNorm = 0.2876
Meta loss on this task batch = 2.9231e-01, Meta loss averaged over last 500 steps = 2.8035e-01, PNorm = 131.5627, GNorm = 0.3288
Meta loss on this task batch = 2.8305e-01, Meta loss averaged over last 500 steps = 2.8041e-01, PNorm = 131.5730, GNorm = 0.2891
Meta loss on this task batch = 2.7164e-01, Meta loss averaged over last 500 steps = 2.8040e-01, PNorm = 131.5848, GNorm = 0.2781
Meta loss on this task batch = 2.7665e-01, Meta loss averaged over last 500 steps = 2.8042e-01, PNorm = 131.5968, GNorm = 0.2970
Meta loss on this task batch = 3.1063e-01, Meta loss averaged over last 500 steps = 2.8024e-01, PNorm = 131.6082, GNorm = 0.3469
Took 153.2578330039978 seconds to complete one epoch of meta training
Took 163.049578666687 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504738
Epoch 616
Meta loss on this task batch = 3.0927e-01, Meta loss averaged over last 500 steps = 2.8026e-01, PNorm = 131.6155, GNorm = 0.3231
Meta loss on this task batch = 3.4669e-01, Meta loss averaged over last 500 steps = 2.8055e-01, PNorm = 131.6204, GNorm = 0.3635
Meta loss on this task batch = 2.3788e-01, Meta loss averaged over last 500 steps = 2.8034e-01, PNorm = 131.6248, GNorm = 0.2445
Meta loss on this task batch = 2.8700e-01, Meta loss averaged over last 500 steps = 2.8037e-01, PNorm = 131.6275, GNorm = 0.2797
Meta loss on this task batch = 3.2564e-01, Meta loss averaged over last 500 steps = 2.8053e-01, PNorm = 131.6287, GNorm = 0.3032
Meta loss on this task batch = 2.5000e-01, Meta loss averaged over last 500 steps = 2.8035e-01, PNorm = 131.6305, GNorm = 0.2591
Meta loss on this task batch = 2.1887e-01, Meta loss averaged over last 500 steps = 2.8024e-01, PNorm = 131.6331, GNorm = 0.2466
Meta loss on this task batch = 3.4248e-01, Meta loss averaged over last 500 steps = 2.8030e-01, PNorm = 131.6345, GNorm = 0.2968
Meta loss on this task batch = 2.7783e-01, Meta loss averaged over last 500 steps = 2.8023e-01, PNorm = 131.6368, GNorm = 0.2829
Meta loss on this task batch = 3.0438e-01, Meta loss averaged over last 500 steps = 2.8034e-01, PNorm = 131.6405, GNorm = 0.3232
Meta loss on this task batch = 2.7743e-01, Meta loss averaged over last 500 steps = 2.8034e-01, PNorm = 131.6466, GNorm = 0.2793
Meta loss on this task batch = 2.6978e-01, Meta loss averaged over last 500 steps = 2.8043e-01, PNorm = 131.6531, GNorm = 0.2669
Meta loss on this task batch = 3.1744e-01, Meta loss averaged over last 500 steps = 2.8045e-01, PNorm = 131.6607, GNorm = 0.2788
Meta loss on this task batch = 3.0546e-01, Meta loss averaged over last 500 steps = 2.8062e-01, PNorm = 131.6696, GNorm = 0.2708
Meta loss on this task batch = 2.8455e-01, Meta loss averaged over last 500 steps = 2.8065e-01, PNorm = 131.6792, GNorm = 0.2894
Meta loss on this task batch = 2.6874e-01, Meta loss averaged over last 500 steps = 2.8057e-01, PNorm = 131.6886, GNorm = 0.2645
Meta loss on this task batch = 2.7065e-01, Meta loss averaged over last 500 steps = 2.8047e-01, PNorm = 131.6983, GNorm = 0.2673
Meta loss on this task batch = 2.6275e-01, Meta loss averaged over last 500 steps = 2.8049e-01, PNorm = 131.7071, GNorm = 0.3245
Meta loss on this task batch = 2.6703e-01, Meta loss averaged over last 500 steps = 2.8057e-01, PNorm = 131.7143, GNorm = 0.3940
Took 157.05423974990845 seconds to complete one epoch of meta training
Took 167.88087487220764 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478451
Epoch 617
Meta loss on this task batch = 2.5982e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 131.7211, GNorm = 0.2725
Meta loss on this task batch = 2.3817e-01, Meta loss averaged over last 500 steps = 2.8051e-01, PNorm = 131.7280, GNorm = 0.2883
Meta loss on this task batch = 2.7874e-01, Meta loss averaged over last 500 steps = 2.8038e-01, PNorm = 131.7332, GNorm = 0.2854
Meta loss on this task batch = 2.7527e-01, Meta loss averaged over last 500 steps = 2.8038e-01, PNorm = 131.7394, GNorm = 0.3286
Meta loss on this task batch = 2.9486e-01, Meta loss averaged over last 500 steps = 2.8038e-01, PNorm = 131.7455, GNorm = 0.3496
Meta loss on this task batch = 2.8639e-01, Meta loss averaged over last 500 steps = 2.8036e-01, PNorm = 131.7501, GNorm = 0.3012
Meta loss on this task batch = 2.7215e-01, Meta loss averaged over last 500 steps = 2.8029e-01, PNorm = 131.7544, GNorm = 0.2672
Meta loss on this task batch = 3.0168e-01, Meta loss averaged over last 500 steps = 2.8033e-01, PNorm = 131.7553, GNorm = 0.3454
Meta loss on this task batch = 2.3590e-01, Meta loss averaged over last 500 steps = 2.8025e-01, PNorm = 131.7563, GNorm = 0.2763
Meta loss on this task batch = 2.8725e-01, Meta loss averaged over last 500 steps = 2.8031e-01, PNorm = 131.7577, GNorm = 0.3021
Meta loss on this task batch = 3.3795e-01, Meta loss averaged over last 500 steps = 2.8046e-01, PNorm = 131.7572, GNorm = 0.3222
Meta loss on this task batch = 3.4561e-01, Meta loss averaged over last 500 steps = 2.8060e-01, PNorm = 131.7549, GNorm = 0.3500
Meta loss on this task batch = 3.1949e-01, Meta loss averaged over last 500 steps = 2.8062e-01, PNorm = 131.7513, GNorm = 0.3951
Meta loss on this task batch = 2.8421e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 131.7496, GNorm = 0.2508
Meta loss on this task batch = 2.9232e-01, Meta loss averaged over last 500 steps = 2.8057e-01, PNorm = 131.7502, GNorm = 0.2574
Meta loss on this task batch = 2.9878e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 131.7503, GNorm = 0.2979
Meta loss on this task batch = 2.2905e-01, Meta loss averaged over last 500 steps = 2.8058e-01, PNorm = 131.7527, GNorm = 0.2302
Meta loss on this task batch = 2.5514e-01, Meta loss averaged over last 500 steps = 2.8066e-01, PNorm = 131.7548, GNorm = 0.2618
Meta loss on this task batch = 2.8744e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 131.7569, GNorm = 0.3854
Took 156.39086627960205 seconds to complete one epoch of meta training
Took 166.49290347099304 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484582
Epoch 618
Meta loss on this task batch = 3.1236e-01, Meta loss averaged over last 500 steps = 2.8084e-01, PNorm = 131.7600, GNorm = 0.2969
Meta loss on this task batch = 2.2756e-01, Meta loss averaged over last 500 steps = 2.8068e-01, PNorm = 131.7631, GNorm = 0.2576
Meta loss on this task batch = 2.8678e-01, Meta loss averaged over last 500 steps = 2.8055e-01, PNorm = 131.7667, GNorm = 0.2791
Meta loss on this task batch = 2.7716e-01, Meta loss averaged over last 500 steps = 2.8045e-01, PNorm = 131.7711, GNorm = 0.2616
Meta loss on this task batch = 3.0511e-01, Meta loss averaged over last 500 steps = 2.8047e-01, PNorm = 131.7761, GNorm = 0.2781
Meta loss on this task batch = 3.0584e-01, Meta loss averaged over last 500 steps = 2.8043e-01, PNorm = 131.7825, GNorm = 0.2418
Meta loss on this task batch = 2.5622e-01, Meta loss averaged over last 500 steps = 2.8044e-01, PNorm = 131.7894, GNorm = 0.2487
Meta loss on this task batch = 2.7557e-01, Meta loss averaged over last 500 steps = 2.8033e-01, PNorm = 131.7983, GNorm = 0.2257
Meta loss on this task batch = 2.6280e-01, Meta loss averaged over last 500 steps = 2.8030e-01, PNorm = 131.8069, GNorm = 0.3041
Meta loss on this task batch = 3.0272e-01, Meta loss averaged over last 500 steps = 2.8023e-01, PNorm = 131.8162, GNorm = 0.2946
Meta loss on this task batch = 3.0324e-01, Meta loss averaged over last 500 steps = 2.8030e-01, PNorm = 131.8248, GNorm = 0.3068
Meta loss on this task batch = 2.7722e-01, Meta loss averaged over last 500 steps = 2.8036e-01, PNorm = 131.8328, GNorm = 0.2988
Meta loss on this task batch = 2.6338e-01, Meta loss averaged over last 500 steps = 2.8028e-01, PNorm = 131.8398, GNorm = 0.3200
Meta loss on this task batch = 3.3413e-01, Meta loss averaged over last 500 steps = 2.8038e-01, PNorm = 131.8436, GNorm = 0.3236
Meta loss on this task batch = 2.6447e-01, Meta loss averaged over last 500 steps = 2.8031e-01, PNorm = 131.8470, GNorm = 0.2495
Meta loss on this task batch = 2.9155e-01, Meta loss averaged over last 500 steps = 2.8029e-01, PNorm = 131.8470, GNorm = 0.2889
Meta loss on this task batch = 1.9959e-01, Meta loss averaged over last 500 steps = 2.8016e-01, PNorm = 131.8472, GNorm = 0.2298
Meta loss on this task batch = 3.3773e-01, Meta loss averaged over last 500 steps = 2.8024e-01, PNorm = 131.8458, GNorm = 0.2814
Meta loss on this task batch = 1.8505e-01, Meta loss averaged over last 500 steps = 2.8001e-01, PNorm = 131.8462, GNorm = 0.2691
Took 152.29690623283386 seconds to complete one epoch of meta training
Took 161.34480023384094 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480822
Epoch 619
Meta loss on this task batch = 2.9229e-01, Meta loss averaged over last 500 steps = 2.8018e-01, PNorm = 131.8460, GNorm = 0.3147
Meta loss on this task batch = 2.8153e-01, Meta loss averaged over last 500 steps = 2.8018e-01, PNorm = 131.8474, GNorm = 0.3118
Meta loss on this task batch = 2.2593e-01, Meta loss averaged over last 500 steps = 2.8013e-01, PNorm = 131.8509, GNorm = 0.2632
Meta loss on this task batch = 2.7039e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 131.8526, GNorm = 0.3353
Meta loss on this task batch = 2.7641e-01, Meta loss averaged over last 500 steps = 2.8002e-01, PNorm = 131.8548, GNorm = 0.2639
Meta loss on this task batch = 2.8635e-01, Meta loss averaged over last 500 steps = 2.7998e-01, PNorm = 131.8559, GNorm = 0.3162
Meta loss on this task batch = 2.6874e-01, Meta loss averaged over last 500 steps = 2.8012e-01, PNorm = 131.8579, GNorm = 0.2852
Meta loss on this task batch = 2.6932e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 131.8600, GNorm = 0.3119
Meta loss on this task batch = 2.9778e-01, Meta loss averaged over last 500 steps = 2.8003e-01, PNorm = 131.8631, GNorm = 0.3069
Meta loss on this task batch = 2.5368e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 131.8685, GNorm = 0.3417
Meta loss on this task batch = 3.0540e-01, Meta loss averaged over last 500 steps = 2.8012e-01, PNorm = 131.8753, GNorm = 0.2956
Meta loss on this task batch = 2.7473e-01, Meta loss averaged over last 500 steps = 2.8011e-01, PNorm = 131.8808, GNorm = 0.2888
Meta loss on this task batch = 2.3973e-01, Meta loss averaged over last 500 steps = 2.8009e-01, PNorm = 131.8854, GNorm = 0.2973
Meta loss on this task batch = 2.6895e-01, Meta loss averaged over last 500 steps = 2.8005e-01, PNorm = 131.8888, GNorm = 0.3247
Meta loss on this task batch = 3.1311e-01, Meta loss averaged over last 500 steps = 2.8007e-01, PNorm = 131.8926, GNorm = 0.3119
Meta loss on this task batch = 2.2008e-01, Meta loss averaged over last 500 steps = 2.7996e-01, PNorm = 131.8959, GNorm = 0.2745
Meta loss on this task batch = 2.7486e-01, Meta loss averaged over last 500 steps = 2.7983e-01, PNorm = 131.8990, GNorm = 0.2825
Meta loss on this task batch = 3.2155e-01, Meta loss averaged over last 500 steps = 2.7989e-01, PNorm = 131.9019, GNorm = 0.3119
Meta loss on this task batch = 2.6688e-01, Meta loss averaged over last 500 steps = 2.7993e-01, PNorm = 131.9034, GNorm = 0.3452
Took 153.43006992340088 seconds to complete one epoch of meta training
Took 163.7998926639557 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485329
Epoch 620
Meta loss on this task batch = 2.9080e-01, Meta loss averaged over last 500 steps = 2.8001e-01, PNorm = 131.9065, GNorm = 0.3663
Meta loss on this task batch = 2.7818e-01, Meta loss averaged over last 500 steps = 2.8000e-01, PNorm = 131.9103, GNorm = 0.3039
Meta loss on this task batch = 2.5570e-01, Meta loss averaged over last 500 steps = 2.7999e-01, PNorm = 131.9132, GNorm = 0.2485
Meta loss on this task batch = 3.1670e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 131.9151, GNorm = 0.2919
Meta loss on this task batch = 2.7238e-01, Meta loss averaged over last 500 steps = 2.7996e-01, PNorm = 131.9165, GNorm = 0.2633
Meta loss on this task batch = 3.0195e-01, Meta loss averaged over last 500 steps = 2.8000e-01, PNorm = 131.9191, GNorm = 0.3171
Meta loss on this task batch = 2.2380e-01, Meta loss averaged over last 500 steps = 2.7998e-01, PNorm = 131.9241, GNorm = 0.2442
Meta loss on this task batch = 2.6564e-01, Meta loss averaged over last 500 steps = 2.8000e-01, PNorm = 131.9293, GNorm = 0.2809
Meta loss on this task batch = 2.9819e-01, Meta loss averaged over last 500 steps = 2.8005e-01, PNorm = 131.9333, GNorm = 0.3054
Meta loss on this task batch = 2.7892e-01, Meta loss averaged over last 500 steps = 2.8004e-01, PNorm = 131.9388, GNorm = 0.2815
Meta loss on this task batch = 2.8017e-01, Meta loss averaged over last 500 steps = 2.7991e-01, PNorm = 131.9451, GNorm = 0.2635
Meta loss on this task batch = 2.1727e-01, Meta loss averaged over last 500 steps = 2.7979e-01, PNorm = 131.9504, GNorm = 0.2588
Meta loss on this task batch = 2.5260e-01, Meta loss averaged over last 500 steps = 2.7979e-01, PNorm = 131.9555, GNorm = 0.2664
Meta loss on this task batch = 2.4756e-01, Meta loss averaged over last 500 steps = 2.7990e-01, PNorm = 131.9590, GNorm = 0.2867
Meta loss on this task batch = 2.8362e-01, Meta loss averaged over last 500 steps = 2.7983e-01, PNorm = 131.9598, GNorm = 0.3386
Meta loss on this task batch = 3.0875e-01, Meta loss averaged over last 500 steps = 2.7982e-01, PNorm = 131.9595, GNorm = 0.3272
Meta loss on this task batch = 2.7745e-01, Meta loss averaged over last 500 steps = 2.7984e-01, PNorm = 131.9580, GNorm = 0.2945
Meta loss on this task batch = 2.8271e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 131.9571, GNorm = 0.3028
Meta loss on this task batch = 2.7716e-01, Meta loss averaged over last 500 steps = 2.7962e-01, PNorm = 131.9539, GNorm = 0.3679
Took 154.57297205924988 seconds to complete one epoch of meta training
Took 164.8437421321869 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485415
Epoch 621
Meta loss on this task batch = 2.4001e-01, Meta loss averaged over last 500 steps = 2.7965e-01, PNorm = 131.9532, GNorm = 0.2670
Meta loss on this task batch = 2.9678e-01, Meta loss averaged over last 500 steps = 2.7969e-01, PNorm = 131.9537, GNorm = 0.3411
Meta loss on this task batch = 2.5737e-01, Meta loss averaged over last 500 steps = 2.7965e-01, PNorm = 131.9571, GNorm = 0.3370
Meta loss on this task batch = 3.0734e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 131.9645, GNorm = 0.3431
Meta loss on this task batch = 2.8326e-01, Meta loss averaged over last 500 steps = 2.7963e-01, PNorm = 131.9752, GNorm = 0.3502
Meta loss on this task batch = 2.2807e-01, Meta loss averaged over last 500 steps = 2.7943e-01, PNorm = 131.9888, GNorm = 0.2728
Meta loss on this task batch = 3.1689e-01, Meta loss averaged over last 500 steps = 2.7958e-01, PNorm = 132.0006, GNorm = 0.3264
Meta loss on this task batch = 2.4565e-01, Meta loss averaged over last 500 steps = 2.7957e-01, PNorm = 132.0119, GNorm = 0.2870
Meta loss on this task batch = 3.2814e-01, Meta loss averaged over last 500 steps = 2.7968e-01, PNorm = 132.0207, GNorm = 0.3462
Meta loss on this task batch = 3.4064e-01, Meta loss averaged over last 500 steps = 2.7984e-01, PNorm = 132.0271, GNorm = 0.3245
Meta loss on this task batch = 2.7734e-01, Meta loss averaged over last 500 steps = 2.7977e-01, PNorm = 132.0312, GNorm = 0.3519
Meta loss on this task batch = 2.4547e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 132.0370, GNorm = 0.2725
Meta loss on this task batch = 2.9935e-01, Meta loss averaged over last 500 steps = 2.7973e-01, PNorm = 132.0410, GNorm = 0.3073
Meta loss on this task batch = 2.7914e-01, Meta loss averaged over last 500 steps = 2.7974e-01, PNorm = 132.0446, GNorm = 0.3136
Meta loss on this task batch = 2.4582e-01, Meta loss averaged over last 500 steps = 2.7969e-01, PNorm = 132.0463, GNorm = 0.3165
Meta loss on this task batch = 2.5619e-01, Meta loss averaged over last 500 steps = 2.7961e-01, PNorm = 132.0478, GNorm = 0.2718
Meta loss on this task batch = 2.5497e-01, Meta loss averaged over last 500 steps = 2.7960e-01, PNorm = 132.0503, GNorm = 0.2717
Meta loss on this task batch = 2.4478e-01, Meta loss averaged over last 500 steps = 2.7949e-01, PNorm = 132.0526, GNorm = 0.2912
Meta loss on this task batch = 2.1940e-01, Meta loss averaged over last 500 steps = 2.7938e-01, PNorm = 132.0558, GNorm = 0.3297
Took 154.9347438812256 seconds to complete one epoch of meta training
Took 166.17582988739014 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474278
Epoch 622
Meta loss on this task batch = 3.0604e-01, Meta loss averaged over last 500 steps = 2.7951e-01, PNorm = 132.0598, GNorm = 0.3008
Meta loss on this task batch = 3.0599e-01, Meta loss averaged over last 500 steps = 2.7959e-01, PNorm = 132.0648, GNorm = 0.2901
Meta loss on this task batch = 2.7962e-01, Meta loss averaged over last 500 steps = 2.7956e-01, PNorm = 132.0689, GNorm = 0.3427
Meta loss on this task batch = 2.6879e-01, Meta loss averaged over last 500 steps = 2.7956e-01, PNorm = 132.0723, GNorm = 0.3057
Meta loss on this task batch = 2.2282e-01, Meta loss averaged over last 500 steps = 2.7941e-01, PNorm = 132.0793, GNorm = 0.2447
Meta loss on this task batch = 2.9223e-01, Meta loss averaged over last 500 steps = 2.7930e-01, PNorm = 132.0863, GNorm = 0.2935
Meta loss on this task batch = 2.7612e-01, Meta loss averaged over last 500 steps = 2.7925e-01, PNorm = 132.0955, GNorm = 0.2881
Meta loss on this task batch = 3.2013e-01, Meta loss averaged over last 500 steps = 2.7934e-01, PNorm = 132.1054, GNorm = 0.3259
Meta loss on this task batch = 2.6387e-01, Meta loss averaged over last 500 steps = 2.7931e-01, PNorm = 132.1153, GNorm = 0.3050
Meta loss on this task batch = 2.2773e-01, Meta loss averaged over last 500 steps = 2.7925e-01, PNorm = 132.1263, GNorm = 0.2633
Meta loss on this task batch = 2.8565e-01, Meta loss averaged over last 500 steps = 2.7919e-01, PNorm = 132.1366, GNorm = 0.3122
Meta loss on this task batch = 2.7046e-01, Meta loss averaged over last 500 steps = 2.7916e-01, PNorm = 132.1472, GNorm = 0.2694
Meta loss on this task batch = 2.8950e-01, Meta loss averaged over last 500 steps = 2.7918e-01, PNorm = 132.1566, GNorm = 0.2881
Meta loss on this task batch = 3.0214e-01, Meta loss averaged over last 500 steps = 2.7928e-01, PNorm = 132.1655, GNorm = 0.2734
Meta loss on this task batch = 2.8269e-01, Meta loss averaged over last 500 steps = 2.7933e-01, PNorm = 132.1737, GNorm = 0.2669
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.7948e-01, PNorm = 132.1796, GNorm = 0.2913
Meta loss on this task batch = 2.2510e-01, Meta loss averaged over last 500 steps = 2.7951e-01, PNorm = 132.1849, GNorm = 0.2619
Meta loss on this task batch = 2.6975e-01, Meta loss averaged over last 500 steps = 2.7939e-01, PNorm = 132.1889, GNorm = 0.2673
Meta loss on this task batch = 3.3132e-01, Meta loss averaged over last 500 steps = 2.7948e-01, PNorm = 132.1918, GNorm = 0.4001
Took 190.51703524589539 seconds to complete one epoch of meta training
Took 200.79185366630554 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488980
Epoch 623
Meta loss on this task batch = 2.6431e-01, Meta loss averaged over last 500 steps = 2.7951e-01, PNorm = 132.1953, GNorm = 0.3232
Meta loss on this task batch = 3.2730e-01, Meta loss averaged over last 500 steps = 2.7965e-01, PNorm = 132.1972, GNorm = 0.3234
Meta loss on this task batch = 2.9566e-01, Meta loss averaged over last 500 steps = 2.7966e-01, PNorm = 132.1974, GNorm = 0.2739
Meta loss on this task batch = 2.6756e-01, Meta loss averaged over last 500 steps = 2.7971e-01, PNorm = 132.1984, GNorm = 0.2441
Meta loss on this task batch = 2.4235e-01, Meta loss averaged over last 500 steps = 2.7981e-01, PNorm = 132.1999, GNorm = 0.3022
Meta loss on this task batch = 2.5737e-01, Meta loss averaged over last 500 steps = 2.7958e-01, PNorm = 132.2009, GNorm = 0.3119
Meta loss on this task batch = 3.5440e-01, Meta loss averaged over last 500 steps = 2.7973e-01, PNorm = 132.1997, GNorm = 0.3099
Meta loss on this task batch = 3.0804e-01, Meta loss averaged over last 500 steps = 2.7979e-01, PNorm = 132.1990, GNorm = 0.2884
Meta loss on this task batch = 2.6491e-01, Meta loss averaged over last 500 steps = 2.7981e-01, PNorm = 132.1996, GNorm = 0.2534
Meta loss on this task batch = 3.1772e-01, Meta loss averaged over last 500 steps = 2.7976e-01, PNorm = 132.2013, GNorm = 0.3218
Meta loss on this task batch = 2.6355e-01, Meta loss averaged over last 500 steps = 2.7973e-01, PNorm = 132.2058, GNorm = 0.3038
Meta loss on this task batch = 2.4277e-01, Meta loss averaged over last 500 steps = 2.7975e-01, PNorm = 132.2126, GNorm = 0.2643
Meta loss on this task batch = 2.7414e-01, Meta loss averaged over last 500 steps = 2.7979e-01, PNorm = 132.2193, GNorm = 0.2723
Meta loss on this task batch = 3.0711e-01, Meta loss averaged over last 500 steps = 2.7988e-01, PNorm = 132.2263, GNorm = 0.3069
Meta loss on this task batch = 2.8602e-01, Meta loss averaged over last 500 steps = 2.7979e-01, PNorm = 132.2352, GNorm = 0.3556
Meta loss on this task batch = 2.5826e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 132.2464, GNorm = 0.2588
Meta loss on this task batch = 2.4403e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 132.2567, GNorm = 0.2594
Meta loss on this task batch = 2.9714e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 132.2660, GNorm = 0.3097
Meta loss on this task batch = 2.4377e-01, Meta loss averaged over last 500 steps = 2.7949e-01, PNorm = 132.2758, GNorm = 0.2761
Took 156.85633659362793 seconds to complete one epoch of meta training
Took 168.01705026626587 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484426
Epoch 624
Meta loss on this task batch = 2.7171e-01, Meta loss averaged over last 500 steps = 2.7955e-01, PNorm = 132.2849, GNorm = 0.2707
Meta loss on this task batch = 2.7489e-01, Meta loss averaged over last 500 steps = 2.7958e-01, PNorm = 132.2939, GNorm = 0.3159
Meta loss on this task batch = 3.2599e-01, Meta loss averaged over last 500 steps = 2.7963e-01, PNorm = 132.3020, GNorm = 0.2900
Meta loss on this task batch = 3.4136e-01, Meta loss averaged over last 500 steps = 2.7970e-01, PNorm = 132.3074, GNorm = 0.3555
Meta loss on this task batch = 2.9178e-01, Meta loss averaged over last 500 steps = 2.7984e-01, PNorm = 132.3135, GNorm = 0.3348
Meta loss on this task batch = 2.5078e-01, Meta loss averaged over last 500 steps = 2.7971e-01, PNorm = 132.3198, GNorm = 0.2856
Meta loss on this task batch = 3.6948e-01, Meta loss averaged over last 500 steps = 2.8002e-01, PNorm = 132.3225, GNorm = 0.4837
Meta loss on this task batch = 2.3702e-01, Meta loss averaged over last 500 steps = 2.7996e-01, PNorm = 132.3211, GNorm = 0.3283
Meta loss on this task batch = 2.9233e-01, Meta loss averaged over last 500 steps = 2.8003e-01, PNorm = 132.3173, GNorm = 0.2957
Meta loss on this task batch = 2.8845e-01, Meta loss averaged over last 500 steps = 2.8000e-01, PNorm = 132.3137, GNorm = 0.2938
Meta loss on this task batch = 2.8907e-01, Meta loss averaged over last 500 steps = 2.7995e-01, PNorm = 132.3120, GNorm = 0.2823
Meta loss on this task batch = 2.6791e-01, Meta loss averaged over last 500 steps = 2.7993e-01, PNorm = 132.3126, GNorm = 0.3366
Meta loss on this task batch = 2.8663e-01, Meta loss averaged over last 500 steps = 2.7997e-01, PNorm = 132.3144, GNorm = 0.2848
Meta loss on this task batch = 2.3492e-01, Meta loss averaged over last 500 steps = 2.7981e-01, PNorm = 132.3182, GNorm = 0.2649
Meta loss on this task batch = 3.1443e-01, Meta loss averaged over last 500 steps = 2.7991e-01, PNorm = 132.3246, GNorm = 0.2824
Meta loss on this task batch = 2.6955e-01, Meta loss averaged over last 500 steps = 2.7992e-01, PNorm = 132.3335, GNorm = 0.2861
Meta loss on this task batch = 2.6522e-01, Meta loss averaged over last 500 steps = 2.7986e-01, PNorm = 132.3428, GNorm = 0.2651
Meta loss on this task batch = 2.6266e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 132.3517, GNorm = 0.2847
Meta loss on this task batch = 3.2666e-01, Meta loss averaged over last 500 steps = 2.7982e-01, PNorm = 132.3568, GNorm = 0.3845
Took 158.5937795639038 seconds to complete one epoch of meta training
Took 169.23975253105164 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486504
Epoch 625
Meta loss on this task batch = 2.7429e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 132.3618, GNorm = 0.2915
Meta loss on this task batch = 3.0819e-01, Meta loss averaged over last 500 steps = 2.7980e-01, PNorm = 132.3657, GNorm = 0.3577
Meta loss on this task batch = 2.6528e-01, Meta loss averaged over last 500 steps = 2.7976e-01, PNorm = 132.3678, GNorm = 0.3255
Meta loss on this task batch = 2.6117e-01, Meta loss averaged over last 500 steps = 2.7969e-01, PNorm = 132.3696, GNorm = 0.3282
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 132.3719, GNorm = 0.2928
Meta loss on this task batch = 2.9060e-01, Meta loss averaged over last 500 steps = 2.7994e-01, PNorm = 132.3762, GNorm = 0.3152
Meta loss on this task batch = 2.3377e-01, Meta loss averaged over last 500 steps = 2.7981e-01, PNorm = 132.3812, GNorm = 0.2353
Meta loss on this task batch = 2.4181e-01, Meta loss averaged over last 500 steps = 2.7980e-01, PNorm = 132.3868, GNorm = 0.2514
Meta loss on this task batch = 3.5136e-01, Meta loss averaged over last 500 steps = 2.8001e-01, PNorm = 132.3912, GNorm = 0.3524
Meta loss on this task batch = 2.3575e-01, Meta loss averaged over last 500 steps = 2.7990e-01, PNorm = 132.3974, GNorm = 0.2442
Meta loss on this task batch = 2.5024e-01, Meta loss averaged over last 500 steps = 2.7990e-01, PNorm = 132.4025, GNorm = 0.2556
Meta loss on this task batch = 2.8881e-01, Meta loss averaged over last 500 steps = 2.7998e-01, PNorm = 132.4066, GNorm = 0.3309
Meta loss on this task batch = 2.9008e-01, Meta loss averaged over last 500 steps = 2.7992e-01, PNorm = 132.4111, GNorm = 0.2777
Meta loss on this task batch = 2.9072e-01, Meta loss averaged over last 500 steps = 2.8009e-01, PNorm = 132.4156, GNorm = 0.3140
Meta loss on this task batch = 3.3407e-01, Meta loss averaged over last 500 steps = 2.8028e-01, PNorm = 132.4198, GNorm = 0.3334
Meta loss on this task batch = 2.6032e-01, Meta loss averaged over last 500 steps = 2.8018e-01, PNorm = 132.4237, GNorm = 0.3299
Meta loss on this task batch = 3.0038e-01, Meta loss averaged over last 500 steps = 2.8028e-01, PNorm = 132.4274, GNorm = 0.3206
Meta loss on this task batch = 3.0453e-01, Meta loss averaged over last 500 steps = 2.8027e-01, PNorm = 132.4321, GNorm = 0.3169
Meta loss on this task batch = 2.3819e-01, Meta loss averaged over last 500 steps = 2.8007e-01, PNorm = 132.4356, GNorm = 0.3091
Took 282.15163493156433 seconds to complete one epoch of meta training
Took 292.6946220397949 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476684
Epoch 626
Meta loss on this task batch = 2.9403e-01, Meta loss averaged over last 500 steps = 2.8009e-01, PNorm = 132.4413, GNorm = 0.3219
Meta loss on this task batch = 2.6996e-01, Meta loss averaged over last 500 steps = 2.7998e-01, PNorm = 132.4479, GNorm = 0.2785
Meta loss on this task batch = 2.6144e-01, Meta loss averaged over last 500 steps = 2.7995e-01, PNorm = 132.4558, GNorm = 0.2494
Meta loss on this task batch = 3.1822e-01, Meta loss averaged over last 500 steps = 2.8000e-01, PNorm = 132.4650, GNorm = 0.2823
Meta loss on this task batch = 1.9329e-01, Meta loss averaged over last 500 steps = 2.7980e-01, PNorm = 132.4752, GNorm = 0.2532
Meta loss on this task batch = 2.3178e-01, Meta loss averaged over last 500 steps = 2.7975e-01, PNorm = 132.4841, GNorm = 0.2651
Meta loss on this task batch = 3.1651e-01, Meta loss averaged over last 500 steps = 2.7989e-01, PNorm = 132.4913, GNorm = 0.2859
Meta loss on this task batch = 2.8189e-01, Meta loss averaged over last 500 steps = 2.7985e-01, PNorm = 132.4982, GNorm = 0.3169
Meta loss on this task batch = 2.2904e-01, Meta loss averaged over last 500 steps = 2.7975e-01, PNorm = 132.5029, GNorm = 0.2921
Meta loss on this task batch = 2.8724e-01, Meta loss averaged over last 500 steps = 2.7969e-01, PNorm = 132.5047, GNorm = 0.3544
Meta loss on this task batch = 2.5870e-01, Meta loss averaged over last 500 steps = 2.7971e-01, PNorm = 132.5038, GNorm = 0.2927
Meta loss on this task batch = 2.6961e-01, Meta loss averaged over last 500 steps = 2.7957e-01, PNorm = 132.5016, GNorm = 0.3030
Meta loss on this task batch = 2.8898e-01, Meta loss averaged over last 500 steps = 2.7953e-01, PNorm = 132.5003, GNorm = 0.2876
Meta loss on this task batch = 2.7353e-01, Meta loss averaged over last 500 steps = 2.7940e-01, PNorm = 132.5011, GNorm = 0.2781
Meta loss on this task batch = 2.9949e-01, Meta loss averaged over last 500 steps = 2.7952e-01, PNorm = 132.5038, GNorm = 0.3218
Meta loss on this task batch = 3.0805e-01, Meta loss averaged over last 500 steps = 2.7957e-01, PNorm = 132.5067, GNorm = 0.3021
Meta loss on this task batch = 3.5355e-01, Meta loss averaged over last 500 steps = 2.7957e-01, PNorm = 132.5145, GNorm = 0.3788
Meta loss on this task batch = 2.6425e-01, Meta loss averaged over last 500 steps = 2.7958e-01, PNorm = 132.5227, GNorm = 0.2757
Meta loss on this task batch = 3.0642e-01, Meta loss averaged over last 500 steps = 2.7965e-01, PNorm = 132.5309, GNorm = 0.3894
Took 285.2259945869446 seconds to complete one epoch of meta training
Took 294.4800720214844 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478358
Epoch 627
Meta loss on this task batch = 2.6692e-01, Meta loss averaged over last 500 steps = 2.7970e-01, PNorm = 132.5389, GNorm = 0.2693
Meta loss on this task batch = 2.6876e-01, Meta loss averaged over last 500 steps = 2.7960e-01, PNorm = 132.5465, GNorm = 0.3059
Meta loss on this task batch = 3.1274e-01, Meta loss averaged over last 500 steps = 2.7978e-01, PNorm = 132.5520, GNorm = 0.3582
Meta loss on this task batch = 2.1338e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 132.5568, GNorm = 0.2634
Meta loss on this task batch = 3.0538e-01, Meta loss averaged over last 500 steps = 2.7948e-01, PNorm = 132.5566, GNorm = 0.3675
Meta loss on this task batch = 3.2307e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 132.5558, GNorm = 0.3092
Meta loss on this task batch = 2.6750e-01, Meta loss averaged over last 500 steps = 2.7970e-01, PNorm = 132.5531, GNorm = 0.2578
Meta loss on this task batch = 2.4573e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 132.5485, GNorm = 0.2851
Meta loss on this task batch = 3.0184e-01, Meta loss averaged over last 500 steps = 2.7977e-01, PNorm = 132.5422, GNorm = 0.3397
Meta loss on this task batch = 2.5462e-01, Meta loss averaged over last 500 steps = 2.7974e-01, PNorm = 132.5381, GNorm = 0.2715
Meta loss on this task batch = 2.8808e-01, Meta loss averaged over last 500 steps = 2.7973e-01, PNorm = 132.5339, GNorm = 0.3077
Meta loss on this task batch = 2.6750e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 132.5300, GNorm = 0.3040
Meta loss on this task batch = 2.8620e-01, Meta loss averaged over last 500 steps = 2.7975e-01, PNorm = 132.5270, GNorm = 0.3038
Meta loss on this task batch = 2.4811e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 132.5257, GNorm = 0.2714
Meta loss on this task batch = 2.8280e-01, Meta loss averaged over last 500 steps = 2.7967e-01, PNorm = 132.5270, GNorm = 0.2734
Meta loss on this task batch = 3.0679e-01, Meta loss averaged over last 500 steps = 2.7977e-01, PNorm = 132.5319, GNorm = 0.3064
Meta loss on this task batch = 3.3318e-01, Meta loss averaged over last 500 steps = 2.7979e-01, PNorm = 132.5418, GNorm = 0.3442
Meta loss on this task batch = 2.8077e-01, Meta loss averaged over last 500 steps = 2.7985e-01, PNorm = 132.5549, GNorm = 0.3006
Meta loss on this task batch = 2.7673e-01, Meta loss averaged over last 500 steps = 2.7983e-01, PNorm = 132.5655, GNorm = 0.3304
Took 164.88601875305176 seconds to complete one epoch of meta training
Took 174.6206374168396 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473543
Epoch 628
Meta loss on this task batch = 2.5412e-01, Meta loss averaged over last 500 steps = 2.7975e-01, PNorm = 132.5765, GNorm = 0.2816
Meta loss on this task batch = 2.1572e-01, Meta loss averaged over last 500 steps = 2.7963e-01, PNorm = 132.5876, GNorm = 0.2484
Meta loss on this task batch = 2.4737e-01, Meta loss averaged over last 500 steps = 2.7958e-01, PNorm = 132.5989, GNorm = 0.2574
Meta loss on this task batch = 2.8182e-01, Meta loss averaged over last 500 steps = 2.7958e-01, PNorm = 132.6097, GNorm = 0.2734
Meta loss on this task batch = 2.6026e-01, Meta loss averaged over last 500 steps = 2.7939e-01, PNorm = 132.6195, GNorm = 0.2593
Meta loss on this task batch = 3.5440e-01, Meta loss averaged over last 500 steps = 2.7940e-01, PNorm = 132.6278, GNorm = 0.3628
Meta loss on this task batch = 2.3809e-01, Meta loss averaged over last 500 steps = 2.7926e-01, PNorm = 132.6373, GNorm = 0.2791
Meta loss on this task batch = 3.3618e-01, Meta loss averaged over last 500 steps = 2.7942e-01, PNorm = 132.6444, GNorm = 0.3324
Meta loss on this task batch = 2.9511e-01, Meta loss averaged over last 500 steps = 2.7954e-01, PNorm = 132.6481, GNorm = 0.3001
Meta loss on this task batch = 3.1819e-01, Meta loss averaged over last 500 steps = 2.7962e-01, PNorm = 132.6504, GNorm = 0.3244
Meta loss on this task batch = 2.9106e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 132.6519, GNorm = 0.2850
Meta loss on this task batch = 2.6087e-01, Meta loss averaged over last 500 steps = 2.7962e-01, PNorm = 132.6528, GNorm = 0.2489
Meta loss on this task batch = 3.1554e-01, Meta loss averaged over last 500 steps = 2.7961e-01, PNorm = 132.6543, GNorm = 0.3404
Meta loss on this task batch = 2.1672e-01, Meta loss averaged over last 500 steps = 2.7940e-01, PNorm = 132.6560, GNorm = 0.2287
Meta loss on this task batch = 2.7582e-01, Meta loss averaged over last 500 steps = 2.7929e-01, PNorm = 132.6591, GNorm = 0.2812
Meta loss on this task batch = 2.7719e-01, Meta loss averaged over last 500 steps = 2.7935e-01, PNorm = 132.6641, GNorm = 0.3515
Meta loss on this task batch = 2.2157e-01, Meta loss averaged over last 500 steps = 2.7923e-01, PNorm = 132.6701, GNorm = 0.2485
Meta loss on this task batch = 2.8454e-01, Meta loss averaged over last 500 steps = 2.7927e-01, PNorm = 132.6756, GNorm = 0.3281
Meta loss on this task batch = 3.2738e-01, Meta loss averaged over last 500 steps = 2.7934e-01, PNorm = 132.6810, GNorm = 0.3661
Took 151.8931291103363 seconds to complete one epoch of meta training
Took 161.50637102127075 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469163
Epoch 629
Meta loss on this task batch = 2.5519e-01, Meta loss averaged over last 500 steps = 2.7918e-01, PNorm = 132.6878, GNorm = 0.2469
Meta loss on this task batch = 2.9534e-01, Meta loss averaged over last 500 steps = 2.7930e-01, PNorm = 132.6920, GNorm = 0.2859
Meta loss on this task batch = 2.2703e-01, Meta loss averaged over last 500 steps = 2.7915e-01, PNorm = 132.6964, GNorm = 0.2295
Meta loss on this task batch = 2.2247e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 132.7017, GNorm = 0.2789
Meta loss on this task batch = 2.6985e-01, Meta loss averaged over last 500 steps = 2.7910e-01, PNorm = 132.7066, GNorm = 0.3256
Meta loss on this task batch = 3.4318e-01, Meta loss averaged over last 500 steps = 2.7918e-01, PNorm = 132.7094, GNorm = 0.3529
Meta loss on this task batch = 2.5793e-01, Meta loss averaged over last 500 steps = 2.7918e-01, PNorm = 132.7140, GNorm = 0.2739
Meta loss on this task batch = 2.5304e-01, Meta loss averaged over last 500 steps = 2.7916e-01, PNorm = 132.7182, GNorm = 0.2602
Meta loss on this task batch = 2.4936e-01, Meta loss averaged over last 500 steps = 2.7901e-01, PNorm = 132.7254, GNorm = 0.2853
Meta loss on this task batch = 3.0900e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 132.7303, GNorm = 0.3268
Meta loss on this task batch = 2.6546e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 132.7353, GNorm = 0.2936
Meta loss on this task batch = 3.2418e-01, Meta loss averaged over last 500 steps = 2.7912e-01, PNorm = 132.7384, GNorm = 0.3281
Meta loss on this task batch = 2.7008e-01, Meta loss averaged over last 500 steps = 2.7911e-01, PNorm = 132.7421, GNorm = 0.3073
Meta loss on this task batch = 2.5713e-01, Meta loss averaged over last 500 steps = 2.7914e-01, PNorm = 132.7453, GNorm = 0.2589
Meta loss on this task batch = 3.3725e-01, Meta loss averaged over last 500 steps = 2.7928e-01, PNorm = 132.7465, GNorm = 0.3813
Meta loss on this task batch = 2.5575e-01, Meta loss averaged over last 500 steps = 2.7915e-01, PNorm = 132.7484, GNorm = 0.2438
Meta loss on this task batch = 2.8660e-01, Meta loss averaged over last 500 steps = 2.7914e-01, PNorm = 132.7476, GNorm = 0.3351
Meta loss on this task batch = 3.0220e-01, Meta loss averaged over last 500 steps = 2.7921e-01, PNorm = 132.7483, GNorm = 0.3173
Meta loss on this task batch = 2.4945e-01, Meta loss averaged over last 500 steps = 2.7912e-01, PNorm = 132.7513, GNorm = 0.3133
Took 151.67381978034973 seconds to complete one epoch of meta training
Took 163.711008310318 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477437
Epoch 630
Meta loss on this task batch = 2.4614e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 132.7551, GNorm = 0.2929
Meta loss on this task batch = 2.7997e-01, Meta loss averaged over last 500 steps = 2.7899e-01, PNorm = 132.7580, GNorm = 0.3368
Meta loss on this task batch = 2.9162e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 132.7611, GNorm = 0.2792
Meta loss on this task batch = 2.6399e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 132.7657, GNorm = 0.2734
Meta loss on this task batch = 2.8934e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 132.7690, GNorm = 0.3178
Meta loss on this task batch = 2.6597e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 132.7722, GNorm = 0.2935
Meta loss on this task batch = 2.2930e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 132.7780, GNorm = 0.2709
Meta loss on this task batch = 2.6782e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 132.7843, GNorm = 0.3020
Meta loss on this task batch = 3.3379e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 132.7895, GNorm = 0.3246
Meta loss on this task batch = 2.7647e-01, Meta loss averaged over last 500 steps = 2.7901e-01, PNorm = 132.7929, GNorm = 0.2761
Meta loss on this task batch = 2.8816e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 132.7956, GNorm = 0.3072
Meta loss on this task batch = 3.1103e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 132.7974, GNorm = 0.2997
Meta loss on this task batch = 2.1530e-01, Meta loss averaged over last 500 steps = 2.7877e-01, PNorm = 132.8012, GNorm = 0.2692
Meta loss on this task batch = 2.7224e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 132.8063, GNorm = 0.2969
Meta loss on this task batch = 2.4104e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 132.8142, GNorm = 0.2524
Meta loss on this task batch = 2.6509e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 132.8196, GNorm = 0.2982
Meta loss on this task batch = 3.7336e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 132.8236, GNorm = 0.4129
Meta loss on this task batch = 2.4300e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 132.8284, GNorm = 0.2822
Meta loss on this task batch = 3.1147e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 132.8331, GNorm = 0.3624
Took 149.9499580860138 seconds to complete one epoch of meta training
Took 159.85721039772034 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488855
Epoch 631
Meta loss on this task batch = 2.4304e-01, Meta loss averaged over last 500 steps = 2.7877e-01, PNorm = 132.8376, GNorm = 0.2826
Meta loss on this task batch = 2.7550e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 132.8429, GNorm = 0.2603
Meta loss on this task batch = 2.7052e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 132.8502, GNorm = 0.2784
Meta loss on this task batch = 3.4244e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 132.8435, GNorm = 1.0000
Meta loss on this task batch = 3.1976e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 132.8380, GNorm = 0.3035
Meta loss on this task batch = 2.6739e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 132.8347, GNorm = 0.2772
Meta loss on this task batch = 2.6620e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 132.8325, GNorm = 0.2705
Meta loss on this task batch = 2.5337e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 132.8306, GNorm = 0.2800
Meta loss on this task batch = 2.6596e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 132.8293, GNorm = 0.2410
Meta loss on this task batch = 2.7601e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 132.8278, GNorm = 0.2811
Meta loss on this task batch = 2.9695e-01, Meta loss averaged over last 500 steps = 2.7906e-01, PNorm = 132.8277, GNorm = 0.3003
Meta loss on this task batch = 2.9671e-01, Meta loss averaged over last 500 steps = 2.7912e-01, PNorm = 132.8296, GNorm = 0.2822
Meta loss on this task batch = 2.4314e-01, Meta loss averaged over last 500 steps = 2.7915e-01, PNorm = 132.8348, GNorm = 0.2872
Meta loss on this task batch = 2.8782e-01, Meta loss averaged over last 500 steps = 2.7911e-01, PNorm = 132.8455, GNorm = 0.5315
Meta loss on this task batch = 2.8241e-01, Meta loss averaged over last 500 steps = 2.7916e-01, PNorm = 132.8537, GNorm = 0.3086
Meta loss on this task batch = 2.4992e-01, Meta loss averaged over last 500 steps = 2.7911e-01, PNorm = 132.8612, GNorm = 0.2419
Meta loss on this task batch = 2.9992e-01, Meta loss averaged over last 500 steps = 2.7913e-01, PNorm = 132.8662, GNorm = 0.3076
Meta loss on this task batch = 3.0824e-01, Meta loss averaged over last 500 steps = 2.7923e-01, PNorm = 132.8688, GNorm = 0.3125
Meta loss on this task batch = 2.9278e-01, Meta loss averaged over last 500 steps = 2.7928e-01, PNorm = 132.8716, GNorm = 0.3520
Took 156.76207327842712 seconds to complete one epoch of meta training
Took 167.21234583854675 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481646
Epoch 632
Meta loss on this task batch = 1.8940e-01, Meta loss averaged over last 500 steps = 2.7914e-01, PNorm = 132.8750, GNorm = 0.2478
Meta loss on this task batch = 2.6676e-01, Meta loss averaged over last 500 steps = 2.7904e-01, PNorm = 132.8786, GNorm = 0.2861
Meta loss on this task batch = 3.1918e-01, Meta loss averaged over last 500 steps = 2.7912e-01, PNorm = 132.8828, GNorm = 0.2936
Meta loss on this task batch = 3.3193e-01, Meta loss averaged over last 500 steps = 2.7915e-01, PNorm = 132.8879, GNorm = 0.2813
Meta loss on this task batch = 2.6164e-01, Meta loss averaged over last 500 steps = 2.7899e-01, PNorm = 132.8931, GNorm = 0.2626
Meta loss on this task batch = 2.8437e-01, Meta loss averaged over last 500 steps = 2.7906e-01, PNorm = 132.8976, GNorm = 0.3350
Meta loss on this task batch = 2.2741e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 132.9022, GNorm = 0.2493
Meta loss on this task batch = 3.4997e-01, Meta loss averaged over last 500 steps = 2.7909e-01, PNorm = 132.9056, GNorm = 0.3658
Meta loss on this task batch = 2.4685e-01, Meta loss averaged over last 500 steps = 2.7906e-01, PNorm = 132.9101, GNorm = 0.3094
Meta loss on this task batch = 2.6540e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 132.9159, GNorm = 0.2963
Meta loss on this task batch = 2.4648e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 132.9233, GNorm = 0.2765
Meta loss on this task batch = 2.7769e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 132.9326, GNorm = 0.2997
Meta loss on this task batch = 2.7638e-01, Meta loss averaged over last 500 steps = 2.7893e-01, PNorm = 132.9424, GNorm = 0.3118
Meta loss on this task batch = 2.6575e-01, Meta loss averaged over last 500 steps = 2.7881e-01, PNorm = 132.9533, GNorm = 0.3114
Meta loss on this task batch = 3.0035e-01, Meta loss averaged over last 500 steps = 2.7901e-01, PNorm = 132.9622, GNorm = 0.3246
Meta loss on this task batch = 2.4621e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 132.9706, GNorm = 0.2640
Meta loss on this task batch = 3.0825e-01, Meta loss averaged over last 500 steps = 2.7911e-01, PNorm = 132.9767, GNorm = 0.3660
Meta loss on this task batch = 2.7251e-01, Meta loss averaged over last 500 steps = 2.7911e-01, PNorm = 132.9827, GNorm = 0.2823
Meta loss on this task batch = 3.1579e-01, Meta loss averaged over last 500 steps = 2.7921e-01, PNorm = 132.9903, GNorm = 0.3418
Took 156.7652313709259 seconds to complete one epoch of meta training
Took 166.68976950645447 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472910
Epoch 633
Meta loss on this task batch = 2.5223e-01, Meta loss averaged over last 500 steps = 2.7914e-01, PNorm = 132.9973, GNorm = 0.2774
Meta loss on this task batch = 2.7371e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 133.0064, GNorm = 0.3438
Meta loss on this task batch = 2.8302e-01, Meta loss averaged over last 500 steps = 2.7909e-01, PNorm = 133.0128, GNorm = 0.3149
Meta loss on this task batch = 2.6692e-01, Meta loss averaged over last 500 steps = 2.7902e-01, PNorm = 133.0169, GNorm = 0.2434
Meta loss on this task batch = 3.5376e-01, Meta loss averaged over last 500 steps = 2.7913e-01, PNorm = 133.0175, GNorm = 0.3301
Meta loss on this task batch = 2.5618e-01, Meta loss averaged over last 500 steps = 2.7908e-01, PNorm = 133.0169, GNorm = 0.2516
Meta loss on this task batch = 2.9071e-01, Meta loss averaged over last 500 steps = 2.7910e-01, PNorm = 133.0146, GNorm = 0.2860
Meta loss on this task batch = 2.2306e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 133.0115, GNorm = 0.2915
Meta loss on this task batch = 2.7940e-01, Meta loss averaged over last 500 steps = 2.7905e-01, PNorm = 133.0085, GNorm = 0.2839
Meta loss on this task batch = 2.8986e-01, Meta loss averaged over last 500 steps = 2.7911e-01, PNorm = 133.0081, GNorm = 0.2821
Meta loss on this task batch = 2.4354e-01, Meta loss averaged over last 500 steps = 2.7890e-01, PNorm = 133.0106, GNorm = 0.2550
Meta loss on this task batch = 2.5340e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 133.0161, GNorm = 0.2792
Meta loss on this task batch = 3.3535e-01, Meta loss averaged over last 500 steps = 2.7896e-01, PNorm = 133.0237, GNorm = 0.3382
Meta loss on this task batch = 2.6010e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 133.0326, GNorm = 0.3084
Meta loss on this task batch = 3.0194e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 133.0420, GNorm = 0.3160
Meta loss on this task batch = 2.7268e-01, Meta loss averaged over last 500 steps = 2.7890e-01, PNorm = 133.0504, GNorm = 0.3020
Meta loss on this task batch = 3.4553e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 133.0586, GNorm = 0.3398
Meta loss on this task batch = 2.4436e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 133.0674, GNorm = 0.2779
Meta loss on this task batch = 2.4730e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 133.0767, GNorm = 0.2985
Took 157.9725477695465 seconds to complete one epoch of meta training
Took 168.6926724910736 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456688
Epoch 634
Meta loss on this task batch = 3.0116e-01, Meta loss averaged over last 500 steps = 2.7904e-01, PNorm = 133.0839, GNorm = 0.3502
Meta loss on this task batch = 2.8131e-01, Meta loss averaged over last 500 steps = 2.7907e-01, PNorm = 133.0888, GNorm = 0.2993
Meta loss on this task batch = 2.8407e-01, Meta loss averaged over last 500 steps = 2.7913e-01, PNorm = 133.0940, GNorm = 0.2892
Meta loss on this task batch = 2.2699e-01, Meta loss averaged over last 500 steps = 2.7899e-01, PNorm = 133.0989, GNorm = 0.2648
Meta loss on this task batch = 2.8883e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 133.1044, GNorm = 0.2461
Meta loss on this task batch = 2.2552e-01, Meta loss averaged over last 500 steps = 2.7905e-01, PNorm = 133.1115, GNorm = 0.2536
Meta loss on this task batch = 2.3156e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 133.1178, GNorm = 0.2361
Meta loss on this task batch = 2.9510e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 133.1251, GNorm = 0.3016
Meta loss on this task batch = 2.2466e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 133.1347, GNorm = 0.2720
Meta loss on this task batch = 2.3726e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 133.1430, GNorm = 0.2762
Meta loss on this task batch = 2.9542e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 133.1518, GNorm = 0.2809
Meta loss on this task batch = 2.8443e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 133.1617, GNorm = 0.3231
Meta loss on this task batch = 3.6202e-01, Meta loss averaged over last 500 steps = 2.7913e-01, PNorm = 133.1694, GNorm = 0.3425
Meta loss on this task batch = 2.4477e-01, Meta loss averaged over last 500 steps = 2.7902e-01, PNorm = 133.1751, GNorm = 0.2802
Meta loss on this task batch = 3.3065e-01, Meta loss averaged over last 500 steps = 2.7915e-01, PNorm = 133.1778, GNorm = 0.3107
Meta loss on this task batch = 2.8633e-01, Meta loss averaged over last 500 steps = 2.7906e-01, PNorm = 133.1821, GNorm = 0.3303
Meta loss on this task batch = 1.9883e-01, Meta loss averaged over last 500 steps = 2.7890e-01, PNorm = 133.1863, GNorm = 0.2268
Meta loss on this task batch = 2.9685e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 133.1909, GNorm = 0.3189
Meta loss on this task batch = 2.6760e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 133.1954, GNorm = 0.3039
Took 151.51719331741333 seconds to complete one epoch of meta training
Took 162.92041492462158 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494323
Epoch 635
Meta loss on this task batch = 3.0848e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 133.2014, GNorm = 0.2798
Meta loss on this task batch = 2.7557e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 133.2064, GNorm = 0.2692
Meta loss on this task batch = 2.2485e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 133.2129, GNorm = 0.2523
Meta loss on this task batch = 2.3778e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 133.2186, GNorm = 0.3655
Meta loss on this task batch = 3.0434e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 133.2251, GNorm = 0.3071
Meta loss on this task batch = 2.6259e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 133.2294, GNorm = 0.3162
Meta loss on this task batch = 2.9924e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 133.2327, GNorm = 0.3456
Meta loss on this task batch = 2.5882e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 133.2352, GNorm = 0.2732
Meta loss on this task batch = 2.7475e-01, Meta loss averaged over last 500 steps = 2.7864e-01, PNorm = 133.2360, GNorm = 0.3069
Meta loss on this task batch = 2.7232e-01, Meta loss averaged over last 500 steps = 2.7862e-01, PNorm = 133.2368, GNorm = 0.2709
Meta loss on this task batch = 2.4443e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 133.2389, GNorm = 0.2566
Meta loss on this task batch = 3.2751e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 133.2441, GNorm = 0.3484
Meta loss on this task batch = 2.7503e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 133.2506, GNorm = 0.2591
Meta loss on this task batch = 3.0061e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 133.2578, GNorm = 0.3006
Meta loss on this task batch = 2.7476e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 133.2640, GNorm = 0.3015
Meta loss on this task batch = 3.0840e-01, Meta loss averaged over last 500 steps = 2.7850e-01, PNorm = 133.2686, GNorm = 0.3358
Meta loss on this task batch = 2.7485e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 133.2745, GNorm = 0.3044
Meta loss on this task batch = 2.6621e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 133.2809, GNorm = 0.2703
Meta loss on this task batch = 2.8657e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 133.2867, GNorm = 0.3906
Took 181.94923496246338 seconds to complete one epoch of meta training
Took 193.16398119926453 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493874
Epoch 636
Meta loss on this task batch = 2.7880e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 133.2922, GNorm = 0.3055
Meta loss on this task batch = 2.3316e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 133.2957, GNorm = 0.2668
Meta loss on this task batch = 2.9481e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 133.2998, GNorm = 0.2748
Meta loss on this task batch = 2.9088e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 133.3019, GNorm = 0.2950
Meta loss on this task batch = 2.2386e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 133.3021, GNorm = 0.2749
Meta loss on this task batch = 2.8502e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 133.3015, GNorm = 0.3790
Meta loss on this task batch = 2.3974e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 133.3009, GNorm = 0.2719
Meta loss on this task batch = 2.9350e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 133.3037, GNorm = 0.3385
Meta loss on this task batch = 3.2178e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 133.3077, GNorm = 0.3104
Meta loss on this task batch = 2.6869e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 133.3115, GNorm = 0.2960
Meta loss on this task batch = 2.4668e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 133.3150, GNorm = 0.3020
Meta loss on this task batch = 2.6915e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 133.3205, GNorm = 0.2735
Meta loss on this task batch = 3.1099e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 133.3255, GNorm = 0.3032
Meta loss on this task batch = 2.1014e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 133.3323, GNorm = 0.2532
Meta loss on this task batch = 2.9530e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 133.3402, GNorm = 0.3000
Meta loss on this task batch = 3.0968e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 133.3467, GNorm = 0.3237
Meta loss on this task batch = 2.5147e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 133.3539, GNorm = 0.2573
Meta loss on this task batch = 2.9867e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 133.3635, GNorm = 0.3111
Meta loss on this task batch = 2.4770e-01, Meta loss averaged over last 500 steps = 2.7816e-01, PNorm = 133.3710, GNorm = 0.3043
Took 154.98548817634583 seconds to complete one epoch of meta training
Took 165.58666944503784 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515419
Epoch 637
Meta loss on this task batch = 3.0504e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 133.3780, GNorm = 0.3001
Meta loss on this task batch = 2.6708e-01, Meta loss averaged over last 500 steps = 2.7819e-01, PNorm = 133.3841, GNorm = 0.3070
Meta loss on this task batch = 2.5951e-01, Meta loss averaged over last 500 steps = 2.7808e-01, PNorm = 133.3906, GNorm = 0.2913
Meta loss on this task batch = 2.8983e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 133.3972, GNorm = 0.3283
Meta loss on this task batch = 2.7946e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 133.4055, GNorm = 0.2913
Meta loss on this task batch = 2.3795e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 133.4153, GNorm = 0.2723
Meta loss on this task batch = 2.7334e-01, Meta loss averaged over last 500 steps = 2.7801e-01, PNorm = 133.4260, GNorm = 0.2915
Meta loss on this task batch = 3.1048e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 133.4362, GNorm = 0.3035
Meta loss on this task batch = 2.9028e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 133.4458, GNorm = 0.3195
Meta loss on this task batch = 2.7523e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 133.4563, GNorm = 0.3154
Meta loss on this task batch = 2.7022e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 133.4657, GNorm = 0.2957
Meta loss on this task batch = 3.0740e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 133.4725, GNorm = 0.3069
Meta loss on this task batch = 2.8874e-01, Meta loss averaged over last 500 steps = 2.7817e-01, PNorm = 133.4781, GNorm = 0.2966
Meta loss on this task batch = 3.1599e-01, Meta loss averaged over last 500 steps = 2.7816e-01, PNorm = 133.4818, GNorm = 0.3278
Meta loss on this task batch = 2.6098e-01, Meta loss averaged over last 500 steps = 2.7812e-01, PNorm = 133.4856, GNorm = 0.2393
Meta loss on this task batch = 2.9720e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 133.4892, GNorm = 0.3126
Meta loss on this task batch = 2.9155e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 133.4901, GNorm = 0.3339
Meta loss on this task batch = 2.5555e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 133.4908, GNorm = 0.2839
Meta loss on this task batch = 3.0953e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 133.4914, GNorm = 0.3614
Took 157.37796688079834 seconds to complete one epoch of meta training
Took 167.922132730484 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498277
Epoch 638
Meta loss on this task batch = 3.2148e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 133.4890, GNorm = 0.3904
Meta loss on this task batch = 2.1892e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 133.4876, GNorm = 0.2438
Meta loss on this task batch = 3.3975e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 133.4871, GNorm = 0.3418
Meta loss on this task batch = 2.6518e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 133.4878, GNorm = 0.2840
Meta loss on this task batch = 3.0837e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 133.4902, GNorm = 0.2688
Meta loss on this task batch = 2.4904e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 133.4934, GNorm = 0.2885
Meta loss on this task batch = 2.8873e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 133.4983, GNorm = 0.3140
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 133.5068, GNorm = 0.3567
Meta loss on this task batch = 3.0471e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 133.5172, GNorm = 0.3487
Meta loss on this task batch = 2.6323e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 133.5276, GNorm = 0.2953
Meta loss on this task batch = 2.7233e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 133.5390, GNorm = 0.2706
Meta loss on this task batch = 2.2929e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 133.5507, GNorm = 0.2437
Meta loss on this task batch = 2.5273e-01, Meta loss averaged over last 500 steps = 2.7811e-01, PNorm = 133.5616, GNorm = 0.2628
Meta loss on this task batch = 2.4177e-01, Meta loss averaged over last 500 steps = 2.7808e-01, PNorm = 133.5698, GNorm = 0.2767
Meta loss on this task batch = 2.5016e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 133.5777, GNorm = 0.2870
Meta loss on this task batch = 2.8000e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 133.5831, GNorm = 0.3050
Meta loss on this task batch = 3.0922e-01, Meta loss averaged over last 500 steps = 2.7817e-01, PNorm = 133.5878, GNorm = 0.3670
Meta loss on this task batch = 2.8386e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 133.5917, GNorm = 0.2984
Meta loss on this task batch = 3.4930e-01, Meta loss averaged over last 500 steps = 2.7839e-01, PNorm = 133.5926, GNorm = 0.4107
Took 156.77377367019653 seconds to complete one epoch of meta training
Took 167.23199558258057 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467417
Epoch 639
Meta loss on this task batch = 2.8314e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 133.5924, GNorm = 0.3065
Meta loss on this task batch = 2.9743e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 133.5910, GNorm = 0.3233
Meta loss on this task batch = 3.0758e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 133.5918, GNorm = 0.3304
Meta loss on this task batch = 3.0899e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 133.5915, GNorm = 0.3643
Meta loss on this task batch = 2.8638e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 133.5908, GNorm = 0.2832
Meta loss on this task batch = 2.9909e-01, Meta loss averaged over last 500 steps = 2.7839e-01, PNorm = 133.5911, GNorm = 0.2691
Meta loss on this task batch = 2.7149e-01, Meta loss averaged over last 500 steps = 2.7841e-01, PNorm = 133.5955, GNorm = 0.3023
Meta loss on this task batch = 2.2878e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 133.6027, GNorm = 0.2794
Meta loss on this task batch = 2.6889e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 133.6113, GNorm = 0.3243
Meta loss on this task batch = 2.5659e-01, Meta loss averaged over last 500 steps = 2.7823e-01, PNorm = 133.6236, GNorm = 0.3029
Meta loss on this task batch = 2.6207e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 133.6374, GNorm = 0.2757
Meta loss on this task batch = 2.4833e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 133.6521, GNorm = 0.2329
Meta loss on this task batch = 2.9128e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 133.6643, GNorm = 0.3183
Meta loss on this task batch = 2.7228e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 133.6727, GNorm = 0.3070
Meta loss on this task batch = 2.9960e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 133.6775, GNorm = 0.3389
Meta loss on this task batch = 3.6617e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 133.6773, GNorm = 0.4362
Meta loss on this task batch = 3.1346e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 133.6726, GNorm = 0.4927
Meta loss on this task batch = 3.0829e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 133.6677, GNorm = 0.3233
Meta loss on this task batch = 2.4402e-01, Meta loss averaged over last 500 steps = 2.7850e-01, PNorm = 133.6643, GNorm = 0.2901
Took 156.939790725708 seconds to complete one epoch of meta training
Took 165.9589602947235 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468981
Epoch 640
Meta loss on this task batch = 2.6617e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 133.6611, GNorm = 0.2755
Meta loss on this task batch = 2.7332e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 133.6599, GNorm = 0.2777
Meta loss on this task batch = 2.9296e-01, Meta loss averaged over last 500 steps = 2.7874e-01, PNorm = 133.6592, GNorm = 0.2993
Meta loss on this task batch = 2.1273e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 133.6604, GNorm = 0.2246
Meta loss on this task batch = 3.1517e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 133.6638, GNorm = 0.2922
Meta loss on this task batch = 2.8918e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 133.6700, GNorm = 0.3472
Meta loss on this task batch = 2.2569e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 133.6778, GNorm = 0.2387
Meta loss on this task batch = 2.5423e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 133.6875, GNorm = 0.2550
Meta loss on this task batch = 3.0758e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 133.6978, GNorm = 0.3193
Meta loss on this task batch = 2.8931e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 133.7088, GNorm = 0.2587
Meta loss on this task batch = 2.5463e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 133.7181, GNorm = 0.2723
Meta loss on this task batch = 2.4550e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 133.7263, GNorm = 0.2798
Meta loss on this task batch = 2.7814e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 133.7346, GNorm = 0.2808
Meta loss on this task batch = 3.3247e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 133.7404, GNorm = 0.3568
Meta loss on this task batch = 2.5446e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 133.7432, GNorm = 0.3054
Meta loss on this task batch = 3.3902e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 133.7456, GNorm = 0.3354
Meta loss on this task batch = 2.7148e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 133.7474, GNorm = 0.3223
Meta loss on this task batch = 2.7921e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 133.7470, GNorm = 0.3236
Meta loss on this task batch = 2.8148e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 133.7467, GNorm = 0.3518
Took 160.95273876190186 seconds to complete one epoch of meta training
Took 171.27630972862244 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477932
Epoch 641
Meta loss on this task batch = 2.8948e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 133.7489, GNorm = 0.3397
Meta loss on this task batch = 2.6296e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 133.7520, GNorm = 0.3290
Meta loss on this task batch = 3.0404e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 133.7554, GNorm = 0.2816
Meta loss on this task batch = 2.2930e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 133.7585, GNorm = 0.2423
Meta loss on this task batch = 2.4592e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 133.7649, GNorm = 0.3063
Meta loss on this task batch = 2.6057e-01, Meta loss averaged over last 500 steps = 2.7850e-01, PNorm = 133.7726, GNorm = 0.2799
Meta loss on this task batch = 2.4067e-01, Meta loss averaged over last 500 steps = 2.7840e-01, PNorm = 133.7789, GNorm = 0.2666
Meta loss on this task batch = 3.4371e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 133.7822, GNorm = 0.3217
Meta loss on this task batch = 2.5932e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 133.7862, GNorm = 0.2787
Meta loss on this task batch = 2.8223e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 133.7873, GNorm = 0.3248
Meta loss on this task batch = 2.6322e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 133.7875, GNorm = 0.2393
Meta loss on this task batch = 2.9347e-01, Meta loss averaged over last 500 steps = 2.7848e-01, PNorm = 133.7864, GNorm = 0.3236
Meta loss on this task batch = 3.1270e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 133.7870, GNorm = 0.2906
Meta loss on this task batch = 2.9089e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 133.7876, GNorm = 0.2600
Meta loss on this task batch = 2.6728e-01, Meta loss averaged over last 500 steps = 2.7858e-01, PNorm = 133.7869, GNorm = 0.3211
Meta loss on this task batch = 2.2638e-01, Meta loss averaged over last 500 steps = 2.7849e-01, PNorm = 133.7878, GNorm = 0.2584
Meta loss on this task batch = 2.8991e-01, Meta loss averaged over last 500 steps = 2.7859e-01, PNorm = 133.7900, GNorm = 0.2705
Meta loss on this task batch = 3.1448e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 133.7914, GNorm = 0.2758
Meta loss on this task batch = 2.8110e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 133.7926, GNorm = 0.3684
Took 158.95675921440125 seconds to complete one epoch of meta training
Took 169.78525757789612 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515477
Epoch 642
Meta loss on this task batch = 3.0899e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 133.7947, GNorm = 0.2645
Meta loss on this task batch = 2.8672e-01, Meta loss averaged over last 500 steps = 2.7860e-01, PNorm = 133.7994, GNorm = 0.2925
Meta loss on this task batch = 2.6564e-01, Meta loss averaged over last 500 steps = 2.7856e-01, PNorm = 133.8067, GNorm = 0.3051
Meta loss on this task batch = 2.9610e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 133.8136, GNorm = 0.2941
Meta loss on this task batch = 2.3313e-01, Meta loss averaged over last 500 steps = 2.7853e-01, PNorm = 133.8207, GNorm = 0.2509
Meta loss on this task batch = 2.7902e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 133.8269, GNorm = 0.2656
Meta loss on this task batch = 3.0536e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 133.8303, GNorm = 0.3074
Meta loss on this task batch = 2.2794e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 133.8339, GNorm = 0.2562
Meta loss on this task batch = 2.6397e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 133.8390, GNorm = 0.3106
Meta loss on this task batch = 3.8105e-01, Meta loss averaged over last 500 steps = 2.7846e-01, PNorm = 133.8413, GNorm = 0.4053
Meta loss on this task batch = 2.7008e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 133.8442, GNorm = 0.3282
Meta loss on this task batch = 2.6338e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 133.8465, GNorm = 0.2896
Meta loss on this task batch = 2.4195e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 133.8488, GNorm = 0.2375
Meta loss on this task batch = 2.6766e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 133.8510, GNorm = 0.2868
Meta loss on this task batch = 2.9670e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 133.8505, GNorm = 0.3285
Meta loss on this task batch = 3.0370e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 133.8491, GNorm = 0.4252
Meta loss on this task batch = 2.8169e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 133.8475, GNorm = 0.3057
Meta loss on this task batch = 2.6703e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 133.8473, GNorm = 0.2685
Meta loss on this task batch = 2.9600e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 133.8496, GNorm = 0.3736
Took 158.94405698776245 seconds to complete one epoch of meta training
Took 170.24536085128784 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502314
Epoch 643
Meta loss on this task batch = 2.9761e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 133.8521, GNorm = 0.3310
Meta loss on this task batch = 2.4176e-01, Meta loss averaged over last 500 steps = 2.7816e-01, PNorm = 133.8551, GNorm = 0.2585
Meta loss on this task batch = 2.7122e-01, Meta loss averaged over last 500 steps = 2.7817e-01, PNorm = 133.8602, GNorm = 0.2489
Meta loss on this task batch = 2.3805e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 133.8654, GNorm = 0.2269
Meta loss on this task batch = 2.4868e-01, Meta loss averaged over last 500 steps = 2.7808e-01, PNorm = 133.8716, GNorm = 0.3378
Meta loss on this task batch = 2.8092e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 133.8765, GNorm = 0.2789
Meta loss on this task batch = 2.3993e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 133.8794, GNorm = 0.3256
Meta loss on this task batch = 2.9220e-01, Meta loss averaged over last 500 steps = 2.7817e-01, PNorm = 133.8805, GNorm = 0.2993
Meta loss on this task batch = 3.0501e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 133.8797, GNorm = 0.3036
Meta loss on this task batch = 2.6350e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 133.8783, GNorm = 0.2872
Meta loss on this task batch = 2.8184e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 133.8757, GNorm = 0.2972
Meta loss on this task batch = 3.0755e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 133.8732, GNorm = 0.2875
Meta loss on this task batch = 2.6397e-01, Meta loss averaged over last 500 steps = 2.7820e-01, PNorm = 133.8709, GNorm = 0.2644
Meta loss on this task batch = 2.5302e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 133.8713, GNorm = 0.3032
Meta loss on this task batch = 2.9572e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 133.8726, GNorm = 0.2875
Meta loss on this task batch = 2.9937e-01, Meta loss averaged over last 500 steps = 2.7825e-01, PNorm = 133.8733, GNorm = 0.3267
Meta loss on this task batch = 2.4287e-01, Meta loss averaged over last 500 steps = 2.7806e-01, PNorm = 133.8780, GNorm = 0.3149
Meta loss on this task batch = 2.7953e-01, Meta loss averaged over last 500 steps = 2.7793e-01, PNorm = 133.8856, GNorm = 0.3116
Meta loss on this task batch = 2.5912e-01, Meta loss averaged over last 500 steps = 2.7780e-01, PNorm = 133.8943, GNorm = 0.3270
Took 150.00068998336792 seconds to complete one epoch of meta training
Took 161.65283679962158 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488521
Epoch 644
Meta loss on this task batch = 2.7723e-01, Meta loss averaged over last 500 steps = 2.7779e-01, PNorm = 133.9034, GNorm = 0.2855
Meta loss on this task batch = 2.5763e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 133.9134, GNorm = 0.2574
Meta loss on this task batch = 2.5169e-01, Meta loss averaged over last 500 steps = 2.7763e-01, PNorm = 133.9224, GNorm = 0.3243
Meta loss on this task batch = 3.1398e-01, Meta loss averaged over last 500 steps = 2.7780e-01, PNorm = 133.9293, GNorm = 0.3561
Meta loss on this task batch = 2.7643e-01, Meta loss averaged over last 500 steps = 2.7784e-01, PNorm = 133.9340, GNorm = 0.2786
Meta loss on this task batch = 2.5925e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 133.9396, GNorm = 0.3187
Meta loss on this task batch = 2.7852e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 133.9443, GNorm = 0.3179
Meta loss on this task batch = 2.9545e-01, Meta loss averaged over last 500 steps = 2.7785e-01, PNorm = 133.9503, GNorm = 0.2985
Meta loss on this task batch = 3.1577e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 133.9562, GNorm = 0.3581
Meta loss on this task batch = 2.6059e-01, Meta loss averaged over last 500 steps = 2.7788e-01, PNorm = 133.9629, GNorm = 0.2857
Meta loss on this task batch = 2.5506e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 133.9692, GNorm = 0.2769
Meta loss on this task batch = 2.9206e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 133.9761, GNorm = 0.3155
Meta loss on this task batch = 2.6610e-01, Meta loss averaged over last 500 steps = 2.7777e-01, PNorm = 133.9825, GNorm = 0.2827
Meta loss on this task batch = 2.6067e-01, Meta loss averaged over last 500 steps = 2.7774e-01, PNorm = 133.9883, GNorm = 0.2807
Meta loss on this task batch = 2.5585e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 133.9944, GNorm = 0.2830
Meta loss on this task batch = 3.2435e-01, Meta loss averaged over last 500 steps = 2.7777e-01, PNorm = 133.9996, GNorm = 0.3152
Meta loss on this task batch = 2.3736e-01, Meta loss averaged over last 500 steps = 2.7764e-01, PNorm = 134.0039, GNorm = 0.2757
Meta loss on this task batch = 2.8539e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 134.0068, GNorm = 0.3075
Meta loss on this task batch = 2.8744e-01, Meta loss averaged over last 500 steps = 2.7770e-01, PNorm = 134.0092, GNorm = 0.3322
Took 154.50667595863342 seconds to complete one epoch of meta training
Took 165.8145043849945 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499123
Epoch 645
Meta loss on this task batch = 2.4692e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 134.0125, GNorm = 0.2984
Meta loss on this task batch = 2.6045e-01, Meta loss averaged over last 500 steps = 2.7752e-01, PNorm = 134.0162, GNorm = 0.2417
Meta loss on this task batch = 2.3723e-01, Meta loss averaged over last 500 steps = 2.7741e-01, PNorm = 134.0198, GNorm = 0.2654
Meta loss on this task batch = 3.1022e-01, Meta loss averaged over last 500 steps = 2.7763e-01, PNorm = 134.0232, GNorm = 0.3109
Meta loss on this task batch = 2.6410e-01, Meta loss averaged over last 500 steps = 2.7748e-01, PNorm = 134.0260, GNorm = 0.3069
Meta loss on this task batch = 2.8156e-01, Meta loss averaged over last 500 steps = 2.7768e-01, PNorm = 134.0291, GNorm = 0.2840
Meta loss on this task batch = 3.0584e-01, Meta loss averaged over last 500 steps = 2.7770e-01, PNorm = 134.0310, GNorm = 0.2940
Meta loss on this task batch = 3.0667e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 134.0326, GNorm = 0.2980
Meta loss on this task batch = 2.3828e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 134.0351, GNorm = 0.2339
Meta loss on this task batch = 2.4617e-01, Meta loss averaged over last 500 steps = 2.7773e-01, PNorm = 134.0407, GNorm = 0.2725
Meta loss on this task batch = 2.1870e-01, Meta loss averaged over last 500 steps = 2.7761e-01, PNorm = 134.0475, GNorm = 0.2849
Meta loss on this task batch = 2.4485e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 134.0545, GNorm = 0.2603
Meta loss on this task batch = 2.6869e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 134.0630, GNorm = 0.2850
Meta loss on this task batch = 2.7737e-01, Meta loss averaged over last 500 steps = 2.7755e-01, PNorm = 134.0716, GNorm = 0.2678
Meta loss on this task batch = 3.2181e-01, Meta loss averaged over last 500 steps = 2.7760e-01, PNorm = 134.0790, GNorm = 0.2890
Meta loss on this task batch = 3.4261e-01, Meta loss averaged over last 500 steps = 2.7777e-01, PNorm = 134.0845, GNorm = 0.3384
Meta loss on this task batch = 3.3067e-01, Meta loss averaged over last 500 steps = 2.7782e-01, PNorm = 134.0894, GNorm = 0.3102
Meta loss on this task batch = 2.9637e-01, Meta loss averaged over last 500 steps = 2.7787e-01, PNorm = 134.0956, GNorm = 0.2842
Meta loss on this task batch = 3.2892e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 134.1020, GNorm = 0.3718
Took 153.5174560546875 seconds to complete one epoch of meta training
Took 163.23506212234497 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506395
Epoch 646
Meta loss on this task batch = 2.6308e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 134.1083, GNorm = 0.2796
Meta loss on this task batch = 2.8283e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 134.1150, GNorm = 0.2732
Meta loss on this task batch = 3.0438e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 134.1210, GNorm = 0.3094
Meta loss on this task batch = 3.0939e-01, Meta loss averaged over last 500 steps = 2.7821e-01, PNorm = 134.1245, GNorm = 0.2665
Meta loss on this task batch = 2.9864e-01, Meta loss averaged over last 500 steps = 2.7817e-01, PNorm = 134.1279, GNorm = 0.2938
Meta loss on this task batch = 2.7195e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 134.1337, GNorm = 0.3211
Meta loss on this task batch = 2.9365e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 134.1402, GNorm = 0.2346
Meta loss on this task batch = 2.5995e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 134.1447, GNorm = 0.2548
Meta loss on this task batch = 2.6596e-01, Meta loss averaged over last 500 steps = 2.7817e-01, PNorm = 134.1495, GNorm = 0.2715
Meta loss on this task batch = 2.9560e-01, Meta loss averaged over last 500 steps = 2.7812e-01, PNorm = 134.1520, GNorm = 0.3026
Meta loss on this task batch = 2.8749e-01, Meta loss averaged over last 500 steps = 2.7815e-01, PNorm = 134.1547, GNorm = 0.2649
Meta loss on this task batch = 2.8925e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 134.1558, GNorm = 0.2690
Meta loss on this task batch = 2.2908e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 134.1572, GNorm = 0.2743
Meta loss on this task batch = 2.6474e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 134.1584, GNorm = 0.2902
Meta loss on this task batch = 2.5061e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 134.1594, GNorm = 0.3463
Meta loss on this task batch = 2.3525e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 134.1626, GNorm = 0.2886
Meta loss on this task batch = 3.3585e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 134.1653, GNorm = 0.3330
Meta loss on this task batch = 2.7890e-01, Meta loss averaged over last 500 steps = 2.7819e-01, PNorm = 134.1674, GNorm = 0.2928
Meta loss on this task batch = 2.8583e-01, Meta loss averaged over last 500 steps = 2.7826e-01, PNorm = 134.1714, GNorm = 0.3234
Took 155.1014587879181 seconds to complete one epoch of meta training
Took 165.46270036697388 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486829
Epoch 647
Meta loss on this task batch = 2.8130e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 134.1791, GNorm = 0.3136
Meta loss on this task batch = 2.8198e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 134.1877, GNorm = 0.3019
Meta loss on this task batch = 2.9303e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 134.1968, GNorm = 0.2780
Meta loss on this task batch = 2.9040e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 134.2053, GNorm = 0.2880
Meta loss on this task batch = 2.8620e-01, Meta loss averaged over last 500 steps = 2.7832e-01, PNorm = 134.2121, GNorm = 0.3166
Meta loss on this task batch = 2.5309e-01, Meta loss averaged over last 500 steps = 2.7827e-01, PNorm = 134.2192, GNorm = 0.2585
Meta loss on this task batch = 2.9012e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 134.2252, GNorm = 0.3164
Meta loss on this task batch = 2.5835e-01, Meta loss averaged over last 500 steps = 2.7830e-01, PNorm = 134.2311, GNorm = 0.3004
Meta loss on this task batch = 2.2743e-01, Meta loss averaged over last 500 steps = 2.7824e-01, PNorm = 134.2363, GNorm = 0.2835
Meta loss on this task batch = 3.0116e-01, Meta loss averaged over last 500 steps = 2.7822e-01, PNorm = 134.2412, GNorm = 0.3259
Meta loss on this task batch = 3.1778e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 134.2443, GNorm = 0.3631
Meta loss on this task batch = 2.3837e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 134.2462, GNorm = 0.3080
Meta loss on this task batch = 2.3153e-01, Meta loss averaged over last 500 steps = 2.7814e-01, PNorm = 134.2491, GNorm = 0.3046
Meta loss on this task batch = 2.6756e-01, Meta loss averaged over last 500 steps = 2.7819e-01, PNorm = 134.2498, GNorm = 0.3572
Meta loss on this task batch = 2.5862e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 134.2523, GNorm = 0.3458
Meta loss on this task batch = 2.9196e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 134.2553, GNorm = 0.2802
Meta loss on this task batch = 2.8273e-01, Meta loss averaged over last 500 steps = 2.7796e-01, PNorm = 134.2581, GNorm = 0.3300
Meta loss on this task batch = 2.9219e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 134.2597, GNorm = 0.2928
Meta loss on this task batch = 2.3517e-01, Meta loss averaged over last 500 steps = 2.7793e-01, PNorm = 134.2624, GNorm = 0.3131
Took 153.98327040672302 seconds to complete one epoch of meta training
Took 163.55100917816162 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485672
Epoch 648
Meta loss on this task batch = 3.1145e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 134.2639, GNorm = 0.2735
Meta loss on this task batch = 2.5985e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 134.2652, GNorm = 0.3010
Meta loss on this task batch = 2.2455e-01, Meta loss averaged over last 500 steps = 2.7796e-01, PNorm = 134.2679, GNorm = 0.2522
Meta loss on this task batch = 2.6598e-01, Meta loss averaged over last 500 steps = 2.7798e-01, PNorm = 134.2696, GNorm = 0.2502
Meta loss on this task batch = 3.5067e-01, Meta loss averaged over last 500 steps = 2.7819e-01, PNorm = 134.2720, GNorm = 0.2668
Meta loss on this task batch = 2.6632e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 134.2738, GNorm = 0.2512
Meta loss on this task batch = 2.5046e-01, Meta loss averaged over last 500 steps = 2.7817e-01, PNorm = 134.2781, GNorm = 0.3042
Meta loss on this task batch = 2.3657e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 134.2832, GNorm = 0.2601
Meta loss on this task batch = 3.0835e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 134.2879, GNorm = 0.2529
Meta loss on this task batch = 2.4348e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 134.2923, GNorm = 0.2667
Meta loss on this task batch = 2.3715e-01, Meta loss averaged over last 500 steps = 2.7807e-01, PNorm = 134.2972, GNorm = 0.2860
Meta loss on this task batch = 2.6778e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 134.3017, GNorm = 0.2840
Meta loss on this task batch = 2.6370e-01, Meta loss averaged over last 500 steps = 2.7799e-01, PNorm = 134.3039, GNorm = 0.2860
Meta loss on this task batch = 2.8556e-01, Meta loss averaged over last 500 steps = 2.7793e-01, PNorm = 134.3070, GNorm = 0.2988
Meta loss on this task batch = 2.5192e-01, Meta loss averaged over last 500 steps = 2.7790e-01, PNorm = 134.3104, GNorm = 0.2781
Meta loss on this task batch = 2.5874e-01, Meta loss averaged over last 500 steps = 2.7796e-01, PNorm = 134.3124, GNorm = 0.2746
Meta loss on this task batch = 2.7632e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 134.3141, GNorm = 0.2959
Meta loss on this task batch = 2.7613e-01, Meta loss averaged over last 500 steps = 2.7796e-01, PNorm = 134.3164, GNorm = 0.2736
Meta loss on this task batch = 2.3554e-01, Meta loss averaged over last 500 steps = 2.7785e-01, PNorm = 134.3185, GNorm = 0.3482
Took 152.71763038635254 seconds to complete one epoch of meta training
Took 163.19527626037598 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469097
Epoch 649
Meta loss on this task batch = 2.5864e-01, Meta loss averaged over last 500 steps = 2.7776e-01, PNorm = 134.3218, GNorm = 0.2892
Meta loss on this task batch = 2.9556e-01, Meta loss averaged over last 500 steps = 2.7779e-01, PNorm = 134.3271, GNorm = 0.3340
Meta loss on this task batch = 2.3402e-01, Meta loss averaged over last 500 steps = 2.7768e-01, PNorm = 134.3320, GNorm = 0.2915
Meta loss on this task batch = 2.6058e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 134.3366, GNorm = 0.3107
Meta loss on this task batch = 2.1566e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 134.3414, GNorm = 0.2762
Meta loss on this task batch = 2.9254e-01, Meta loss averaged over last 500 steps = 2.7757e-01, PNorm = 134.3449, GNorm = 0.3117
Meta loss on this task batch = 2.9325e-01, Meta loss averaged over last 500 steps = 2.7763e-01, PNorm = 134.3480, GNorm = 0.3217
Meta loss on this task batch = 2.5087e-01, Meta loss averaged over last 500 steps = 2.7747e-01, PNorm = 134.3510, GNorm = 0.3042
Meta loss on this task batch = 3.3999e-01, Meta loss averaged over last 500 steps = 2.7756e-01, PNorm = 134.3508, GNorm = 0.3422
Meta loss on this task batch = 2.4231e-01, Meta loss averaged over last 500 steps = 2.7751e-01, PNorm = 134.3519, GNorm = 0.2925
Meta loss on this task batch = 3.0195e-01, Meta loss averaged over last 500 steps = 2.7763e-01, PNorm = 134.3525, GNorm = 0.3491
Meta loss on this task batch = 2.7538e-01, Meta loss averaged over last 500 steps = 2.7767e-01, PNorm = 134.3523, GNorm = 0.3374
Meta loss on this task batch = 2.6874e-01, Meta loss averaged over last 500 steps = 2.7750e-01, PNorm = 134.3506, GNorm = 0.2880
Meta loss on this task batch = 2.6092e-01, Meta loss averaged over last 500 steps = 2.7740e-01, PNorm = 134.3498, GNorm = 0.2997
Meta loss on this task batch = 2.4754e-01, Meta loss averaged over last 500 steps = 2.7737e-01, PNorm = 134.3478, GNorm = 0.2748
Meta loss on this task batch = 2.7419e-01, Meta loss averaged over last 500 steps = 2.7728e-01, PNorm = 134.3472, GNorm = 0.2722
Meta loss on this task batch = 2.6751e-01, Meta loss averaged over last 500 steps = 2.7729e-01, PNorm = 134.3480, GNorm = 0.3200
Meta loss on this task batch = 2.6141e-01, Meta loss averaged over last 500 steps = 2.7732e-01, PNorm = 134.3518, GNorm = 0.3188
Meta loss on this task batch = 2.7798e-01, Meta loss averaged over last 500 steps = 2.7733e-01, PNorm = 134.3560, GNorm = 0.3500
Took 154.48051929473877 seconds to complete one epoch of meta training
Took 163.87572860717773 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.521501
Epoch 650
Meta loss on this task batch = 2.8624e-01, Meta loss averaged over last 500 steps = 2.7729e-01, PNorm = 134.3588, GNorm = 0.2869
Meta loss on this task batch = 3.0073e-01, Meta loss averaged over last 500 steps = 2.7732e-01, PNorm = 134.3636, GNorm = 0.3288
Meta loss on this task batch = 2.6575e-01, Meta loss averaged over last 500 steps = 2.7734e-01, PNorm = 134.3686, GNorm = 0.2928
Meta loss on this task batch = 2.8039e-01, Meta loss averaged over last 500 steps = 2.7741e-01, PNorm = 134.3743, GNorm = 0.3131
Meta loss on this task batch = 3.3051e-01, Meta loss averaged over last 500 steps = 2.7747e-01, PNorm = 134.3804, GNorm = 0.3171
Meta loss on this task batch = 3.6710e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 134.3875, GNorm = 0.3761
Meta loss on this task batch = 2.6465e-01, Meta loss averaged over last 500 steps = 2.7771e-01, PNorm = 134.3940, GNorm = 0.3500
Meta loss on this task batch = 2.6589e-01, Meta loss averaged over last 500 steps = 2.7769e-01, PNorm = 134.3996, GNorm = 0.2974
Meta loss on this task batch = 2.5723e-01, Meta loss averaged over last 500 steps = 2.7755e-01, PNorm = 134.4081, GNorm = 0.2900
Meta loss on this task batch = 3.0232e-01, Meta loss averaged over last 500 steps = 2.7747e-01, PNorm = 134.4151, GNorm = 0.3094
Meta loss on this task batch = 2.8367e-01, Meta loss averaged over last 500 steps = 2.7746e-01, PNorm = 134.4223, GNorm = 0.3132
Meta loss on this task batch = 2.7130e-01, Meta loss averaged over last 500 steps = 2.7750e-01, PNorm = 134.4278, GNorm = 0.2798
Meta loss on this task batch = 2.3761e-01, Meta loss averaged over last 500 steps = 2.7723e-01, PNorm = 134.4322, GNorm = 0.2502
Meta loss on this task batch = 2.6594e-01, Meta loss averaged over last 500 steps = 2.7729e-01, PNorm = 134.4354, GNorm = 0.3009
Meta loss on this task batch = 2.8675e-01, Meta loss averaged over last 500 steps = 2.7728e-01, PNorm = 134.4383, GNorm = 0.2961
Meta loss on this task batch = 2.9853e-01, Meta loss averaged over last 500 steps = 2.7730e-01, PNorm = 134.4419, GNorm = 0.3193
Meta loss on this task batch = 2.2261e-01, Meta loss averaged over last 500 steps = 2.7717e-01, PNorm = 134.4479, GNorm = 0.2884
Meta loss on this task batch = 2.2711e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 134.4530, GNorm = 0.2726
Meta loss on this task batch = 2.7056e-01, Meta loss averaged over last 500 steps = 2.7705e-01, PNorm = 134.4585, GNorm = 0.3544
Took 157.96110343933105 seconds to complete one epoch of meta training
Took 168.09978199005127 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504944
Epoch 651
Meta loss on this task batch = 2.8299e-01, Meta loss averaged over last 500 steps = 2.7715e-01, PNorm = 134.4653, GNorm = 0.2802
Meta loss on this task batch = 3.1971e-01, Meta loss averaged over last 500 steps = 2.7716e-01, PNorm = 134.4722, GNorm = 0.3323
Meta loss on this task batch = 2.7323e-01, Meta loss averaged over last 500 steps = 2.7717e-01, PNorm = 134.4804, GNorm = 0.2633
Meta loss on this task batch = 2.3932e-01, Meta loss averaged over last 500 steps = 2.7712e-01, PNorm = 134.4886, GNorm = 0.3076
Meta loss on this task batch = 3.3091e-01, Meta loss averaged over last 500 steps = 2.7725e-01, PNorm = 134.4942, GNorm = 0.3129
Meta loss on this task batch = 2.5384e-01, Meta loss averaged over last 500 steps = 2.7711e-01, PNorm = 134.4997, GNorm = 0.2597
Meta loss on this task batch = 2.7257e-01, Meta loss averaged over last 500 steps = 2.7710e-01, PNorm = 134.5036, GNorm = 0.3544
Meta loss on this task batch = 2.8267e-01, Meta loss averaged over last 500 steps = 2.7705e-01, PNorm = 134.5102, GNorm = 0.3364
Meta loss on this task batch = 2.4865e-01, Meta loss averaged over last 500 steps = 2.7702e-01, PNorm = 134.5161, GNorm = 0.2706
Meta loss on this task batch = 3.0141e-01, Meta loss averaged over last 500 steps = 2.7710e-01, PNorm = 134.5216, GNorm = 0.4247
Meta loss on this task batch = 2.4189e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 134.5262, GNorm = 0.3262
Meta loss on this task batch = 2.9179e-01, Meta loss averaged over last 500 steps = 2.7702e-01, PNorm = 134.5281, GNorm = 0.2935
Meta loss on this task batch = 2.6016e-01, Meta loss averaged over last 500 steps = 2.7707e-01, PNorm = 134.5296, GNorm = 0.2929
Meta loss on this task batch = 2.5242e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 134.5307, GNorm = 0.2971
Meta loss on this task batch = 3.2503e-01, Meta loss averaged over last 500 steps = 2.7704e-01, PNorm = 134.5298, GNorm = 0.3166
Meta loss on this task batch = 2.4470e-01, Meta loss averaged over last 500 steps = 2.7706e-01, PNorm = 134.5307, GNorm = 0.2760
Meta loss on this task batch = 2.5608e-01, Meta loss averaged over last 500 steps = 2.7707e-01, PNorm = 134.5326, GNorm = 0.2690
Meta loss on this task batch = 2.7116e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 134.5359, GNorm = 0.2722
Meta loss on this task batch = 2.6991e-01, Meta loss averaged over last 500 steps = 2.7699e-01, PNorm = 134.5373, GNorm = 0.3796
Took 156.57986545562744 seconds to complete one epoch of meta training
Took 166.3926968574524 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470326
Epoch 652
Meta loss on this task batch = 2.7760e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 134.5387, GNorm = 0.3037
Meta loss on this task batch = 3.1658e-01, Meta loss averaged over last 500 steps = 2.7693e-01, PNorm = 134.5389, GNorm = 0.2877
Meta loss on this task batch = 3.0056e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 134.5389, GNorm = 0.3111
Meta loss on this task batch = 2.6352e-01, Meta loss averaged over last 500 steps = 2.7694e-01, PNorm = 134.5394, GNorm = 0.2809
Meta loss on this task batch = 2.9402e-01, Meta loss averaged over last 500 steps = 2.7692e-01, PNorm = 134.5409, GNorm = 0.3288
Meta loss on this task batch = 2.5447e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 134.5429, GNorm = 0.2777
Meta loss on this task batch = 3.1653e-01, Meta loss averaged over last 500 steps = 2.7699e-01, PNorm = 134.5482, GNorm = 0.3107
Meta loss on this task batch = 2.3989e-01, Meta loss averaged over last 500 steps = 2.7693e-01, PNorm = 134.5576, GNorm = 0.2877
Meta loss on this task batch = 3.0737e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 134.5680, GNorm = 0.3002
Meta loss on this task batch = 3.0999e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 134.5789, GNorm = 0.3280
Meta loss on this task batch = 2.7655e-01, Meta loss averaged over last 500 steps = 2.7718e-01, PNorm = 134.5903, GNorm = 0.2974
Meta loss on this task batch = 2.4575e-01, Meta loss averaged over last 500 steps = 2.7720e-01, PNorm = 134.6028, GNorm = 0.2566
Meta loss on this task batch = 3.0010e-01, Meta loss averaged over last 500 steps = 2.7717e-01, PNorm = 134.6138, GNorm = 0.3162
Meta loss on this task batch = 2.9163e-01, Meta loss averaged over last 500 steps = 2.7719e-01, PNorm = 134.6260, GNorm = 0.2969
Meta loss on this task batch = 3.0305e-01, Meta loss averaged over last 500 steps = 2.7734e-01, PNorm = 134.6378, GNorm = 0.2961
Meta loss on this task batch = 3.2422e-01, Meta loss averaged over last 500 steps = 2.7741e-01, PNorm = 134.6491, GNorm = 0.3040
Meta loss on this task batch = 2.2467e-01, Meta loss averaged over last 500 steps = 2.7734e-01, PNorm = 134.6592, GNorm = 0.2668
Meta loss on this task batch = 2.7664e-01, Meta loss averaged over last 500 steps = 2.7736e-01, PNorm = 134.6658, GNorm = 0.4117
Meta loss on this task batch = 2.0578e-01, Meta loss averaged over last 500 steps = 2.7719e-01, PNorm = 134.6697, GNorm = 0.3003
Took 153.9928047657013 seconds to complete one epoch of meta training
Took 163.57537388801575 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466402
Epoch 653
Meta loss on this task batch = 2.4006e-01, Meta loss averaged over last 500 steps = 2.7712e-01, PNorm = 134.6738, GNorm = 0.2445
Meta loss on this task batch = 3.2298e-01, Meta loss averaged over last 500 steps = 2.7717e-01, PNorm = 134.6749, GNorm = 0.2851
Meta loss on this task batch = 2.7193e-01, Meta loss averaged over last 500 steps = 2.7710e-01, PNorm = 134.6735, GNorm = 0.3399
Meta loss on this task batch = 2.6978e-01, Meta loss averaged over last 500 steps = 2.7693e-01, PNorm = 134.6700, GNorm = 0.2962
Meta loss on this task batch = 2.1889e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 134.6673, GNorm = 0.2423
Meta loss on this task batch = 2.9669e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 134.6632, GNorm = 0.2852
Meta loss on this task batch = 3.0294e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 134.6589, GNorm = 0.3643
Meta loss on this task batch = 2.9212e-01, Meta loss averaged over last 500 steps = 2.7694e-01, PNorm = 134.6560, GNorm = 0.2726
Meta loss on this task batch = 2.7126e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 134.6560, GNorm = 0.2453
Meta loss on this task batch = 2.3174e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 134.6588, GNorm = 0.2610
Meta loss on this task batch = 2.4969e-01, Meta loss averaged over last 500 steps = 2.7678e-01, PNorm = 134.6624, GNorm = 0.2450
Meta loss on this task batch = 2.7036e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 134.6687, GNorm = 0.3055
Meta loss on this task batch = 3.1619e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 134.6743, GNorm = 0.3340
Meta loss on this task batch = 2.7318e-01, Meta loss averaged over last 500 steps = 2.7683e-01, PNorm = 134.6823, GNorm = 0.2873
Meta loss on this task batch = 3.5675e-01, Meta loss averaged over last 500 steps = 2.7694e-01, PNorm = 134.6909, GNorm = 0.3054
Meta loss on this task batch = 3.0055e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 134.6992, GNorm = 0.3016
Meta loss on this task batch = 2.5491e-01, Meta loss averaged over last 500 steps = 2.7696e-01, PNorm = 134.7075, GNorm = 0.2473
Meta loss on this task batch = 2.5122e-01, Meta loss averaged over last 500 steps = 2.7693e-01, PNorm = 134.7158, GNorm = 0.2578
Meta loss on this task batch = 2.5679e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 134.7227, GNorm = 0.3085
Took 158.0521376132965 seconds to complete one epoch of meta training
Took 168.4405255317688 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489615
Epoch 654
Meta loss on this task batch = 2.5271e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 134.7291, GNorm = 0.2455
Meta loss on this task batch = 2.8232e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 134.7345, GNorm = 0.3179
Meta loss on this task batch = 2.4631e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 134.7374, GNorm = 0.3176
Meta loss on this task batch = 2.7555e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 134.7386, GNorm = 0.3167
Meta loss on this task batch = 2.7873e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 134.7383, GNorm = 0.3109
Meta loss on this task batch = 2.7929e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 134.7392, GNorm = 0.2854
Meta loss on this task batch = 3.0029e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 134.7397, GNorm = 0.3000
Meta loss on this task batch = 3.2622e-01, Meta loss averaged over last 500 steps = 2.7696e-01, PNorm = 134.7397, GNorm = 0.3521
Meta loss on this task batch = 3.1797e-01, Meta loss averaged over last 500 steps = 2.7710e-01, PNorm = 134.7403, GNorm = 0.3008
Meta loss on this task batch = 2.7155e-01, Meta loss averaged over last 500 steps = 2.7708e-01, PNorm = 134.7420, GNorm = 0.3136
Meta loss on this task batch = 2.7619e-01, Meta loss averaged over last 500 steps = 2.7711e-01, PNorm = 134.7462, GNorm = 0.3196
Meta loss on this task batch = 3.1639e-01, Meta loss averaged over last 500 steps = 2.7704e-01, PNorm = 134.7501, GNorm = 0.3324
Meta loss on this task batch = 2.5197e-01, Meta loss averaged over last 500 steps = 2.7706e-01, PNorm = 134.7568, GNorm = 0.2728
Meta loss on this task batch = 2.7789e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 134.7668, GNorm = 0.2744
Meta loss on this task batch = 2.6313e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 134.7776, GNorm = 0.2639
Meta loss on this task batch = 2.5830e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 134.7896, GNorm = 0.2941
Meta loss on this task batch = 2.6137e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 134.8004, GNorm = 0.2673
Meta loss on this task batch = 3.2733e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 134.8077, GNorm = 0.3509
Meta loss on this task batch = 2.6208e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 134.8117, GNorm = 0.3257
Took 152.39837217330933 seconds to complete one epoch of meta training
Took 162.63037037849426 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487117
Epoch 655
Meta loss on this task batch = 2.5669e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 134.8174, GNorm = 0.3018
Meta loss on this task batch = 3.5084e-01, Meta loss averaged over last 500 steps = 2.7696e-01, PNorm = 134.8214, GNorm = 0.3143
Meta loss on this task batch = 2.6713e-01, Meta loss averaged over last 500 steps = 2.7694e-01, PNorm = 134.8267, GNorm = 0.2653
Meta loss on this task batch = 2.4871e-01, Meta loss averaged over last 500 steps = 2.7699e-01, PNorm = 134.8334, GNorm = 0.2868
Meta loss on this task batch = 3.4102e-01, Meta loss averaged over last 500 steps = 2.7711e-01, PNorm = 134.8377, GNorm = 0.3402
Meta loss on this task batch = 3.0553e-01, Meta loss averaged over last 500 steps = 2.7706e-01, PNorm = 134.8426, GNorm = 0.2902
Meta loss on this task batch = 2.8395e-01, Meta loss averaged over last 500 steps = 2.7712e-01, PNorm = 134.8498, GNorm = 0.3033
Meta loss on this task batch = 2.5818e-01, Meta loss averaged over last 500 steps = 2.7705e-01, PNorm = 134.8583, GNorm = 0.2846
Meta loss on this task batch = 3.0252e-01, Meta loss averaged over last 500 steps = 2.7720e-01, PNorm = 134.8673, GNorm = 0.2791
Meta loss on this task batch = 3.0459e-01, Meta loss averaged over last 500 steps = 2.7736e-01, PNorm = 134.8760, GNorm = 0.3301
Meta loss on this task batch = 2.4940e-01, Meta loss averaged over last 500 steps = 2.7732e-01, PNorm = 134.8844, GNorm = 0.2565
Meta loss on this task batch = 2.4530e-01, Meta loss averaged over last 500 steps = 2.7713e-01, PNorm = 134.8924, GNorm = 0.3013
Meta loss on this task batch = 2.5916e-01, Meta loss averaged over last 500 steps = 2.7713e-01, PNorm = 134.8968, GNorm = 0.2887
Meta loss on this task batch = 2.6488e-01, Meta loss averaged over last 500 steps = 2.7715e-01, PNorm = 134.9020, GNorm = 0.2910
Meta loss on this task batch = 2.6819e-01, Meta loss averaged over last 500 steps = 2.7719e-01, PNorm = 134.9065, GNorm = 0.2650
Meta loss on this task batch = 2.2997e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 134.9118, GNorm = 0.2752
Meta loss on this task batch = 2.9685e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 134.9172, GNorm = 0.3022
Meta loss on this task batch = 2.5969e-01, Meta loss averaged over last 500 steps = 2.7696e-01, PNorm = 134.9212, GNorm = 0.2823
Meta loss on this task batch = 2.7105e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 134.9232, GNorm = 0.3505
Took 151.37682151794434 seconds to complete one epoch of meta training
Took 161.1907012462616 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485169
Epoch 656
Meta loss on this task batch = 2.7740e-01, Meta loss averaged over last 500 steps = 2.7701e-01, PNorm = 134.9242, GNorm = 0.3119
Meta loss on this task batch = 2.3767e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 134.9258, GNorm = 0.3088
Meta loss on this task batch = 3.0734e-01, Meta loss averaged over last 500 steps = 2.7691e-01, PNorm = 134.9266, GNorm = 0.3363
Meta loss on this task batch = 2.7613e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 134.9260, GNorm = 0.3401
Meta loss on this task batch = 2.5347e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 134.9267, GNorm = 0.2689
Meta loss on this task batch = 2.9718e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 134.9279, GNorm = 0.3214
Meta loss on this task batch = 2.1056e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 134.9306, GNorm = 0.2636
Meta loss on this task batch = 2.3766e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 134.9349, GNorm = 0.2674
Meta loss on this task batch = 3.2548e-01, Meta loss averaged over last 500 steps = 2.7680e-01, PNorm = 134.9387, GNorm = 0.3749
Meta loss on this task batch = 2.9248e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 134.9428, GNorm = 0.2873
Meta loss on this task batch = 2.9260e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 134.9449, GNorm = 0.2869
Meta loss on this task batch = 2.6489e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 134.9464, GNorm = 0.2699
Meta loss on this task batch = 2.4253e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 134.9473, GNorm = 0.2550
Meta loss on this task batch = 2.5624e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 134.9510, GNorm = 0.2760
Meta loss on this task batch = 3.3420e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 134.9558, GNorm = 0.3157
Meta loss on this task batch = 2.7415e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 134.9595, GNorm = 0.2681
Meta loss on this task batch = 2.9820e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 134.9620, GNorm = 0.3163
Meta loss on this task batch = 2.7648e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 134.9627, GNorm = 0.2818
Meta loss on this task batch = 3.0409e-01, Meta loss averaged over last 500 steps = 2.7699e-01, PNorm = 134.9636, GNorm = 0.3369
Took 155.88783264160156 seconds to complete one epoch of meta training
Took 164.60730409622192 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490864
Epoch 657
Meta loss on this task batch = 2.5266e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 134.9654, GNorm = 0.2635
Meta loss on this task batch = 2.8837e-01, Meta loss averaged over last 500 steps = 2.7705e-01, PNorm = 134.9652, GNorm = 0.2848
Meta loss on this task batch = 2.3528e-01, Meta loss averaged over last 500 steps = 2.7699e-01, PNorm = 134.9662, GNorm = 0.2709
Meta loss on this task batch = 2.6028e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 134.9696, GNorm = 0.3227
Meta loss on this task batch = 2.6911e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 134.9735, GNorm = 0.2326
Meta loss on this task batch = 2.7191e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 134.9761, GNorm = 0.2662
Meta loss on this task batch = 2.3266e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 134.9806, GNorm = 0.3228
Meta loss on this task batch = 2.9613e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 134.9824, GNorm = 0.3028
Meta loss on this task batch = 2.8130e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 134.9845, GNorm = 0.3601
Meta loss on this task batch = 2.3901e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 134.9876, GNorm = 0.2733
Meta loss on this task batch = 2.5431e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 134.9903, GNorm = 0.2862
Meta loss on this task batch = 2.9226e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 134.9933, GNorm = 0.2985
Meta loss on this task batch = 2.8991e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 134.9974, GNorm = 0.4202
Meta loss on this task batch = 2.6745e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 135.0021, GNorm = 0.3300
Meta loss on this task batch = 2.8796e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 135.0054, GNorm = 0.3235
Meta loss on this task batch = 3.3307e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 135.0094, GNorm = 0.3322
Meta loss on this task batch = 3.3270e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 135.0137, GNorm = 0.3336
Meta loss on this task batch = 2.7251e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 135.0201, GNorm = 0.2862
Meta loss on this task batch = 2.6283e-01, Meta loss averaged over last 500 steps = 2.7678e-01, PNorm = 135.0283, GNorm = 0.3280
Took 154.44487762451172 seconds to complete one epoch of meta training
Took 165.22039604187012 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500124
Epoch 658
Meta loss on this task batch = 2.5298e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 135.0344, GNorm = 0.2795
Meta loss on this task batch = 2.5972e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 135.0404, GNorm = 0.2793
Meta loss on this task batch = 2.5895e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 135.0441, GNorm = 0.2909
Meta loss on this task batch = 2.3223e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 135.0466, GNorm = 0.2804
Meta loss on this task batch = 3.0833e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 135.0484, GNorm = 0.2853
Meta loss on this task batch = 2.8743e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 135.0488, GNorm = 0.2938
Meta loss on this task batch = 3.0284e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 135.0473, GNorm = 0.3171
Meta loss on this task batch = 2.4759e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 135.0461, GNorm = 0.2535
Meta loss on this task batch = 3.0234e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 135.0444, GNorm = 0.3022
Meta loss on this task batch = 2.9919e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 135.0415, GNorm = 0.3044
Meta loss on this task batch = 2.5151e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 135.0393, GNorm = 0.2766
Meta loss on this task batch = 2.6399e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 135.0378, GNorm = 0.2833
Meta loss on this task batch = 2.7874e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 135.0358, GNorm = 0.2602
Meta loss on this task batch = 3.0458e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 135.0392, GNorm = 0.3339
Meta loss on this task batch = 2.6164e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 135.0452, GNorm = 0.2725
Meta loss on this task batch = 3.1151e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 135.0538, GNorm = 0.2902
Meta loss on this task batch = 2.9850e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 135.0638, GNorm = 0.2783
Meta loss on this task batch = 2.6469e-01, Meta loss averaged over last 500 steps = 2.7678e-01, PNorm = 135.0740, GNorm = 0.2725
Meta loss on this task batch = 3.5443e-01, Meta loss averaged over last 500 steps = 2.7694e-01, PNorm = 135.0835, GNorm = 0.3875
Took 153.05461883544922 seconds to complete one epoch of meta training
Took 163.98620200157166 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496549
Epoch 659
Meta loss on this task batch = 2.7172e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 135.0919, GNorm = 0.3049
Meta loss on this task batch = 2.5406e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 135.0997, GNorm = 0.2880
Meta loss on this task batch = 2.4579e-01, Meta loss averaged over last 500 steps = 2.7685e-01, PNorm = 135.1078, GNorm = 0.2663
Meta loss on this task batch = 2.3300e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 135.1166, GNorm = 0.2683
Meta loss on this task batch = 3.0253e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 135.1268, GNorm = 0.3009
Meta loss on this task batch = 2.7406e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 135.1373, GNorm = 0.3608
Meta loss on this task batch = 2.8879e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 135.1485, GNorm = 0.2923
Meta loss on this task batch = 2.9208e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 135.1581, GNorm = 0.3215
Meta loss on this task batch = 3.3004e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 135.1664, GNorm = 0.3626
Meta loss on this task batch = 2.5983e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 135.1727, GNorm = 0.3030
Meta loss on this task batch = 3.1296e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 135.1791, GNorm = 0.3355
Meta loss on this task batch = 3.0684e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 135.1853, GNorm = 0.2946
Meta loss on this task batch = 2.9071e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 135.1881, GNorm = 0.3334
Meta loss on this task batch = 2.6193e-01, Meta loss averaged over last 500 steps = 2.7697e-01, PNorm = 135.1894, GNorm = 0.2682
Meta loss on this task batch = 2.4477e-01, Meta loss averaged over last 500 steps = 2.7690e-01, PNorm = 135.1892, GNorm = 0.2671
Meta loss on this task batch = 3.0608e-01, Meta loss averaged over last 500 steps = 2.7693e-01, PNorm = 135.1892, GNorm = 0.2894
Meta loss on this task batch = 2.8615e-01, Meta loss averaged over last 500 steps = 2.7702e-01, PNorm = 135.1887, GNorm = 0.3032
Meta loss on this task batch = 2.0113e-01, Meta loss averaged over last 500 steps = 2.7691e-01, PNorm = 135.1908, GNorm = 0.2463
Meta loss on this task batch = 2.2850e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 135.1927, GNorm = 0.3267
Took 155.68288493156433 seconds to complete one epoch of meta training
Took 167.87234473228455 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494429
Epoch 660
Meta loss on this task batch = 2.7999e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 135.1944, GNorm = 0.2868
Meta loss on this task batch = 3.3742e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 135.1986, GNorm = 0.2995
Meta loss on this task batch = 2.3412e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 135.2058, GNorm = 0.2775
Meta loss on this task batch = 2.3411e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 135.2160, GNorm = 0.3094
Meta loss on this task batch = 3.1057e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 135.2265, GNorm = 0.2919
Meta loss on this task batch = 3.2010e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 135.2367, GNorm = 0.3061
Meta loss on this task batch = 2.6884e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 135.2459, GNorm = 0.2756
Meta loss on this task batch = 2.4984e-01, Meta loss averaged over last 500 steps = 2.7666e-01, PNorm = 135.2537, GNorm = 0.2730
Meta loss on this task batch = 2.8638e-01, Meta loss averaged over last 500 steps = 2.7666e-01, PNorm = 135.2602, GNorm = 0.2686
Meta loss on this task batch = 2.3568e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 135.2669, GNorm = 0.2792
Meta loss on this task batch = 2.7759e-01, Meta loss averaged over last 500 steps = 2.7666e-01, PNorm = 135.2729, GNorm = 0.2451
Meta loss on this task batch = 2.9111e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 135.2791, GNorm = 0.3376
Meta loss on this task batch = 2.5605e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 135.2836, GNorm = 0.2464
Meta loss on this task batch = 2.3622e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 135.2878, GNorm = 0.2929
Meta loss on this task batch = 2.4384e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 135.2911, GNorm = 0.2832
Meta loss on this task batch = 2.4291e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 135.2925, GNorm = 0.2376
Meta loss on this task batch = 2.6146e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 135.2945, GNorm = 0.2949
Meta loss on this task batch = 3.3116e-01, Meta loss averaged over last 500 steps = 2.7680e-01, PNorm = 135.2949, GNorm = 0.3128
Meta loss on this task batch = 2.4128e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 135.2966, GNorm = 0.3343
Took 158.7506172657013 seconds to complete one epoch of meta training
Took 169.0152325630188 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492495
Epoch 661
Meta loss on this task batch = 3.3772e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 135.2975, GNorm = 0.3518
Meta loss on this task batch = 2.8284e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 135.3003, GNorm = 0.3069
Meta loss on this task batch = 2.4848e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 135.3048, GNorm = 0.2785
Meta loss on this task batch = 2.5047e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 135.3097, GNorm = 0.3198
Meta loss on this task batch = 3.1517e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 135.3164, GNorm = 0.3391
Meta loss on this task batch = 2.7611e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 135.3232, GNorm = 0.2923
Meta loss on this task batch = 2.1348e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 135.3308, GNorm = 0.2396
Meta loss on this task batch = 2.3016e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 135.3394, GNorm = 0.2661
Meta loss on this task batch = 2.5234e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 135.3497, GNorm = 0.2902
Meta loss on this task batch = 2.9156e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 135.3583, GNorm = 0.3307
Meta loss on this task batch = 2.8883e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 135.3643, GNorm = 0.3075
Meta loss on this task batch = 2.8683e-01, Meta loss averaged over last 500 steps = 2.7663e-01, PNorm = 135.3694, GNorm = 0.3016
Meta loss on this task batch = 2.3989e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 135.3752, GNorm = 0.2799
Meta loss on this task batch = 2.6604e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 135.3772, GNorm = 0.3095
Meta loss on this task batch = 2.7625e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 135.3786, GNorm = 0.2990
Meta loss on this task batch = 2.6137e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 135.3796, GNorm = 0.2824
Meta loss on this task batch = 2.6117e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 135.3806, GNorm = 0.2372
Meta loss on this task batch = 3.0748e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 135.3799, GNorm = 0.2901
Meta loss on this task batch = 3.0394e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 135.3799, GNorm = 0.3695
Took 154.5425341129303 seconds to complete one epoch of meta training
Took 164.3543848991394 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471079
Epoch 662
Meta loss on this task batch = 2.4099e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 135.3825, GNorm = 0.2688
Meta loss on this task batch = 3.0941e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 135.3844, GNorm = 0.3062
Meta loss on this task batch = 2.8102e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 135.3862, GNorm = 0.2462
Meta loss on this task batch = 2.9971e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 135.3877, GNorm = 0.2778
Meta loss on this task batch = 2.6100e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 135.3924, GNorm = 0.2976
Meta loss on this task batch = 2.7721e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 135.3983, GNorm = 0.2819
Meta loss on this task batch = 2.6871e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 135.4052, GNorm = 0.2567
Meta loss on this task batch = 3.1579e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 135.4134, GNorm = 0.2771
Meta loss on this task batch = 2.9394e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 135.4209, GNorm = 0.2957
Meta loss on this task batch = 2.8708e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 135.4285, GNorm = 0.3084
Meta loss on this task batch = 2.5824e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 135.4361, GNorm = 0.2502
Meta loss on this task batch = 3.1577e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 135.4417, GNorm = 0.3264
Meta loss on this task batch = 2.3765e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 135.4488, GNorm = 0.2370
Meta loss on this task batch = 2.1936e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 135.4554, GNorm = 0.2452
Meta loss on this task batch = 2.6927e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 135.4618, GNorm = 0.2980
Meta loss on this task batch = 2.4697e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 135.4677, GNorm = 0.3092
Meta loss on this task batch = 2.2322e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 135.4752, GNorm = 0.3128
Meta loss on this task batch = 3.6887e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 135.4803, GNorm = 0.3597
Meta loss on this task batch = 2.9724e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 135.4848, GNorm = 0.3736
Took 158.5546703338623 seconds to complete one epoch of meta training
Took 168.69209671020508 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497528
Epoch 663
Meta loss on this task batch = 3.3533e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 135.4874, GNorm = 0.3436
Meta loss on this task batch = 3.3482e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 135.4890, GNorm = 0.3496
Meta loss on this task batch = 3.0321e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 135.4910, GNorm = 0.3421
Meta loss on this task batch = 2.2536e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 135.4940, GNorm = 0.2869
Meta loss on this task batch = 2.9303e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 135.4961, GNorm = 0.2854
Meta loss on this task batch = 3.0681e-01, Meta loss averaged over last 500 steps = 2.7693e-01, PNorm = 135.4983, GNorm = 0.2978
Meta loss on this task batch = 2.4921e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 135.5015, GNorm = 0.2816
Meta loss on this task batch = 3.0940e-01, Meta loss averaged over last 500 steps = 2.7690e-01, PNorm = 135.5047, GNorm = 0.2947
Meta loss on this task batch = 2.0424e-01, Meta loss averaged over last 500 steps = 2.7679e-01, PNorm = 135.5107, GNorm = 0.2487
Meta loss on this task batch = 2.5320e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 135.5177, GNorm = 0.2458
Meta loss on this task batch = 2.6962e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 135.5244, GNorm = 0.3132
Meta loss on this task batch = 2.3310e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 135.5317, GNorm = 0.2479
Meta loss on this task batch = 2.6092e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 135.5384, GNorm = 0.2734
Meta loss on this task batch = 2.6704e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 135.5408, GNorm = 0.3517
Meta loss on this task batch = 3.4663e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 135.5401, GNorm = 0.3504
Meta loss on this task batch = 2.5671e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 135.5380, GNorm = 0.2877
Meta loss on this task batch = 2.7805e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 135.5356, GNorm = 0.2617
Meta loss on this task batch = 3.0323e-01, Meta loss averaged over last 500 steps = 2.7666e-01, PNorm = 135.5363, GNorm = 0.3186
Meta loss on this task batch = 2.9506e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 135.5378, GNorm = 0.3830
Took 158.96468782424927 seconds to complete one epoch of meta training
Took 169.2102620601654 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494275
Epoch 664
Meta loss on this task batch = 2.7482e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 135.5416, GNorm = 0.2788
Meta loss on this task batch = 2.2759e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 135.5470, GNorm = 0.2633
Meta loss on this task batch = 2.3985e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 135.5525, GNorm = 0.2982
Meta loss on this task batch = 2.5498e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 135.5572, GNorm = 0.2808
Meta loss on this task batch = 3.0510e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 135.5624, GNorm = 0.2789
Meta loss on this task batch = 2.6615e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 135.5698, GNorm = 0.2842
Meta loss on this task batch = 2.7766e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 135.5763, GNorm = 0.2633
Meta loss on this task batch = 3.2117e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 135.5831, GNorm = 0.2753
Meta loss on this task batch = 2.1701e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 135.5901, GNorm = 0.2211
Meta loss on this task batch = 2.5574e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 135.5960, GNorm = 0.3099
Meta loss on this task batch = 2.9123e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 135.6028, GNorm = 0.2935
Meta loss on this task batch = 2.8206e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 135.6095, GNorm = 0.3189
Meta loss on this task batch = 2.6913e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 135.6141, GNorm = 0.3398
Meta loss on this task batch = 3.1857e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 135.6185, GNorm = 0.3066
Meta loss on this task batch = 2.9649e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 135.6217, GNorm = 0.2949
Meta loss on this task batch = 2.9472e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 135.6260, GNorm = 0.3269
Meta loss on this task batch = 2.7003e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 135.6313, GNorm = 0.2907
Meta loss on this task batch = 2.8916e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 135.6353, GNorm = 0.3486
Meta loss on this task batch = 2.6765e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 135.6415, GNorm = 0.3417
Took 154.73395586013794 seconds to complete one epoch of meta training
Took 164.8352792263031 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511241
Epoch 665
Meta loss on this task batch = 2.8967e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 135.6471, GNorm = 0.3373
Meta loss on this task batch = 3.2610e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 135.6512, GNorm = 0.3461
Meta loss on this task batch = 3.0775e-01, Meta loss averaged over last 500 steps = 2.7674e-01, PNorm = 135.6542, GNorm = 0.3021
Meta loss on this task batch = 2.6057e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 135.6566, GNorm = 0.3191
Meta loss on this task batch = 2.8148e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 135.6566, GNorm = 0.3170
Meta loss on this task batch = 2.7417e-01, Meta loss averaged over last 500 steps = 2.7649e-01, PNorm = 135.6572, GNorm = 0.2869
Meta loss on this task batch = 2.1197e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 135.6578, GNorm = 0.2218
Meta loss on this task batch = 3.0010e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 135.6582, GNorm = 0.2532
Meta loss on this task batch = 2.5449e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 135.6603, GNorm = 0.3411
Meta loss on this task batch = 3.0359e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 135.6628, GNorm = 0.3636
Meta loss on this task batch = 2.6289e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 135.6659, GNorm = 0.2728
Meta loss on this task batch = 2.8623e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 135.6701, GNorm = 0.2885
Meta loss on this task batch = 2.9709e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 135.6727, GNorm = 0.3484
Meta loss on this task batch = 2.6072e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 135.6772, GNorm = 0.2978
Meta loss on this task batch = 2.7812e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 135.6814, GNorm = 0.2936
Meta loss on this task batch = 3.1418e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 135.6855, GNorm = 0.3035
Meta loss on this task batch = 3.0993e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 135.6874, GNorm = 0.3030
Meta loss on this task batch = 2.5393e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 135.6895, GNorm = 0.3025
Meta loss on this task batch = 2.5459e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 135.6911, GNorm = 0.2897
Took 150.2823088169098 seconds to complete one epoch of meta training
Took 159.91387391090393 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509341
Epoch 666
Meta loss on this task batch = 2.4664e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 135.6929, GNorm = 0.2539
Meta loss on this task batch = 2.5645e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 135.6943, GNorm = 0.2733
Meta loss on this task batch = 3.1367e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 135.6939, GNorm = 0.3376
Meta loss on this task batch = 3.0027e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 135.6931, GNorm = 0.2961
Meta loss on this task batch = 2.6591e-01, Meta loss averaged over last 500 steps = 2.7609e-01, PNorm = 135.6945, GNorm = 0.3059
Meta loss on this task batch = 2.6431e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 135.6960, GNorm = 0.2616
Meta loss on this task batch = 3.3758e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 135.6970, GNorm = 0.3412
Meta loss on this task batch = 2.4465e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 135.6983, GNorm = 0.2786
Meta loss on this task batch = 2.6652e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 135.6990, GNorm = 0.3186
Meta loss on this task batch = 3.2256e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 135.7008, GNorm = 0.3276
Meta loss on this task batch = 2.8471e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 135.7048, GNorm = 0.3045
Meta loss on this task batch = 2.7911e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 135.7088, GNorm = 0.2780
Meta loss on this task batch = 2.4731e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 135.7147, GNorm = 0.2659
Meta loss on this task batch = 2.9976e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 135.7195, GNorm = 0.3674
Meta loss on this task batch = 2.5014e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 135.7249, GNorm = 0.3016
Meta loss on this task batch = 2.4191e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 135.7296, GNorm = 0.2734
Meta loss on this task batch = 3.2982e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 135.7337, GNorm = 0.3456
Meta loss on this task batch = 2.6034e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 135.7379, GNorm = 0.2553
Meta loss on this task batch = 2.8724e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 135.7415, GNorm = 0.3482
Took 152.18434357643127 seconds to complete one epoch of meta training
Took 161.09202551841736 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486467
Epoch 667
Meta loss on this task batch = 2.7047e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 135.7459, GNorm = 0.2692
Meta loss on this task batch = 2.9337e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 135.7500, GNorm = 0.2492
Meta loss on this task batch = 3.2811e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 135.7545, GNorm = 0.2920
Meta loss on this task batch = 2.6906e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 135.7595, GNorm = 0.2595
Meta loss on this task batch = 2.6025e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 135.7651, GNorm = 0.3027
Meta loss on this task batch = 2.6521e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 135.7715, GNorm = 0.2917
Meta loss on this task batch = 2.8870e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 135.7774, GNorm = 0.3144
Meta loss on this task batch = 3.0740e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 135.7814, GNorm = 0.2825
Meta loss on this task batch = 2.6832e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 135.7861, GNorm = 0.2621
Meta loss on this task batch = 3.1741e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 135.7918, GNorm = 0.3232
Meta loss on this task batch = 2.6551e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 135.7967, GNorm = 0.2700
Meta loss on this task batch = 2.7172e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 135.8002, GNorm = 0.2618
Meta loss on this task batch = 2.9289e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 135.8020, GNorm = 0.2765
Meta loss on this task batch = 2.7909e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 135.8029, GNorm = 0.2648
Meta loss on this task batch = 2.7263e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 135.8026, GNorm = 0.2842
Meta loss on this task batch = 2.6932e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 135.8022, GNorm = 0.2676
Meta loss on this task batch = 2.7124e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 135.8025, GNorm = 0.3146
Meta loss on this task batch = 2.7032e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 135.8011, GNorm = 0.3378
Meta loss on this task batch = 2.6692e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 135.8014, GNorm = 0.3522
Took 154.02037954330444 seconds to complete one epoch of meta training
Took 164.65626692771912 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485525
Epoch 668
Meta loss on this task batch = 2.9534e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 135.8042, GNorm = 0.2730
Meta loss on this task batch = 2.7248e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 135.8063, GNorm = 0.2697
Meta loss on this task batch = 2.1036e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 135.8098, GNorm = 0.2696
Meta loss on this task batch = 2.6108e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 135.8136, GNorm = 0.2486
Meta loss on this task batch = 2.7208e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 135.8175, GNorm = 0.3438
Meta loss on this task batch = 2.9484e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 135.8210, GNorm = 0.2873
Meta loss on this task batch = 2.8357e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 135.8235, GNorm = 0.3191
Meta loss on this task batch = 2.4366e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 135.8273, GNorm = 0.2940
Meta loss on this task batch = 2.7454e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 135.8317, GNorm = 0.3020
Meta loss on this task batch = 3.4306e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 135.8357, GNorm = 0.3095
Meta loss on this task batch = 2.4597e-01, Meta loss averaged over last 500 steps = 2.7626e-01, PNorm = 135.8420, GNorm = 0.3107
Meta loss on this task batch = 2.5680e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 135.8496, GNorm = 0.2808
Meta loss on this task batch = 2.5036e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 135.8564, GNorm = 0.3232
Meta loss on this task batch = 2.5906e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 135.8610, GNorm = 0.3255
Meta loss on this task batch = 3.1601e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 135.8640, GNorm = 0.3401
Meta loss on this task batch = 2.4776e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 135.8693, GNorm = 0.2746
Meta loss on this task batch = 2.2447e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 135.8741, GNorm = 0.3123
Meta loss on this task batch = 3.2849e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 135.8793, GNorm = 0.3493
Meta loss on this task batch = 2.7074e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 135.8840, GNorm = 0.3285
Took 154.5036494731903 seconds to complete one epoch of meta training
Took 165.1932396888733 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493236
Epoch 669
Meta loss on this task batch = 3.1809e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 135.8870, GNorm = 0.2673
Meta loss on this task batch = 3.0954e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 135.8882, GNorm = 0.3730
Meta loss on this task batch = 2.2976e-01, Meta loss averaged over last 500 steps = 2.7609e-01, PNorm = 135.8894, GNorm = 0.2404
Meta loss on this task batch = 3.2339e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 135.8907, GNorm = 0.3257
Meta loss on this task batch = 2.6127e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 135.8920, GNorm = 0.2934
Meta loss on this task batch = 2.8217e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 135.8951, GNorm = 0.3281
Meta loss on this task batch = 2.8107e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 135.8995, GNorm = 0.2497
Meta loss on this task batch = 2.4782e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 135.9070, GNorm = 0.2833
Meta loss on this task batch = 2.4658e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 135.9153, GNorm = 0.2546
Meta loss on this task batch = 2.4176e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 135.9253, GNorm = 0.2775
Meta loss on this task batch = 2.5121e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 135.9358, GNorm = 0.2948
Meta loss on this task batch = 2.7821e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 135.9451, GNorm = 0.2776
Meta loss on this task batch = 2.8214e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 135.9524, GNorm = 0.2901
Meta loss on this task batch = 3.0151e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 135.9568, GNorm = 0.3003
Meta loss on this task batch = 2.8063e-01, Meta loss averaged over last 500 steps = 2.7612e-01, PNorm = 135.9609, GNorm = 0.3107
Meta loss on this task batch = 2.9537e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 135.9638, GNorm = 0.3022
Meta loss on this task batch = 2.6435e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 135.9661, GNorm = 0.2910
Meta loss on this task batch = 2.9156e-01, Meta loss averaged over last 500 steps = 2.7612e-01, PNorm = 135.9702, GNorm = 0.2848
Meta loss on this task batch = 2.9105e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 135.9744, GNorm = 0.3622
Took 159.45852637290955 seconds to complete one epoch of meta training
Took 168.89387154579163 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501164
Epoch 670
Meta loss on this task batch = 2.7172e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 135.9785, GNorm = 0.3061
Meta loss on this task batch = 2.4392e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 135.9833, GNorm = 0.2638
Meta loss on this task batch = 2.5764e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 135.9899, GNorm = 0.2793
Meta loss on this task batch = 2.1489e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 135.9987, GNorm = 0.3047
Meta loss on this task batch = 2.9393e-01, Meta loss averaged over last 500 steps = 2.7599e-01, PNorm = 136.0067, GNorm = 0.3470
Meta loss on this task batch = 2.8310e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 136.0137, GNorm = 0.2893
Meta loss on this task batch = 3.1456e-01, Meta loss averaged over last 500 steps = 2.7612e-01, PNorm = 136.0178, GNorm = 0.3160
Meta loss on this task batch = 2.6859e-01, Meta loss averaged over last 500 steps = 2.7614e-01, PNorm = 136.0233, GNorm = 0.2676
Meta loss on this task batch = 2.6557e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 136.0272, GNorm = 0.2950
Meta loss on this task batch = 3.4254e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 136.0283, GNorm = 0.3608
Meta loss on this task batch = 2.7487e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 136.0298, GNorm = 0.2763
Meta loss on this task batch = 2.5210e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 136.0335, GNorm = 0.2903
Meta loss on this task batch = 3.1075e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 136.0369, GNorm = 0.3337
Meta loss on this task batch = 2.0603e-01, Meta loss averaged over last 500 steps = 2.7609e-01, PNorm = 136.0408, GNorm = 0.2787
Meta loss on this task batch = 2.3488e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 136.0457, GNorm = 0.2746
Meta loss on this task batch = 3.5438e-01, Meta loss averaged over last 500 steps = 2.7612e-01, PNorm = 136.0507, GNorm = 0.3843
Meta loss on this task batch = 2.8540e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 136.0551, GNorm = 0.2882
Meta loss on this task batch = 3.7247e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 136.0598, GNorm = 0.3707
Meta loss on this task batch = 2.9240e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 136.0627, GNorm = 0.3746
Took 160.68762969970703 seconds to complete one epoch of meta training
Took 170.52874636650085 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495676
Epoch 671
Meta loss on this task batch = 2.6760e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 136.0650, GNorm = 0.2837
Meta loss on this task batch = 2.4935e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 136.0672, GNorm = 0.2910
Meta loss on this task batch = 2.9919e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 136.0703, GNorm = 0.3427
Meta loss on this task batch = 3.0623e-01, Meta loss averaged over last 500 steps = 2.7648e-01, PNorm = 136.0760, GNorm = 0.3039
Meta loss on this task batch = 2.7007e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 136.0828, GNorm = 0.2845
Meta loss on this task batch = 2.5668e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 136.0904, GNorm = 0.3189
Meta loss on this task batch = 2.5963e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 136.0987, GNorm = 0.2930
Meta loss on this task batch = 2.2473e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 136.1082, GNorm = 0.2632
Meta loss on this task batch = 2.2523e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 136.1154, GNorm = 0.2902
Meta loss on this task batch = 2.5123e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 136.1212, GNorm = 0.3282
Meta loss on this task batch = 2.4878e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 136.1260, GNorm = 0.3003
Meta loss on this task batch = 2.5102e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 136.1291, GNorm = 0.3566
Meta loss on this task batch = 2.9235e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 136.1294, GNorm = 0.3263
Meta loss on this task batch = 3.0599e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 136.1287, GNorm = 0.3228
Meta loss on this task batch = 3.8591e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 136.1312, GNorm = 0.3121
Meta loss on this task batch = 2.8816e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 136.1340, GNorm = 0.3271
Meta loss on this task batch = 2.6835e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 136.1375, GNorm = 0.2868
Meta loss on this task batch = 3.1561e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 136.1406, GNorm = 0.3296
Meta loss on this task batch = 2.8013e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 136.1438, GNorm = 0.2945
Took 152.88832592964172 seconds to complete one epoch of meta training
Took 162.83781933784485 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479964
Epoch 672
Meta loss on this task batch = 2.4909e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 136.1450, GNorm = 0.2406
Meta loss on this task batch = 2.7273e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 136.1459, GNorm = 0.2839
Meta loss on this task batch = 2.6908e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 136.1474, GNorm = 0.2944
Meta loss on this task batch = 2.8995e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 136.1482, GNorm = 0.3042
Meta loss on this task batch = 2.5341e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 136.1500, GNorm = 0.2444
Meta loss on this task batch = 2.8191e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 136.1522, GNorm = 0.2567
Meta loss on this task batch = 2.7662e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 136.1551, GNorm = 0.2917
Meta loss on this task batch = 2.4657e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 136.1588, GNorm = 0.2586
Meta loss on this task batch = 2.5626e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 136.1649, GNorm = 0.2617
Meta loss on this task batch = 2.7031e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 136.1700, GNorm = 0.2617
Meta loss on this task batch = 2.6972e-01, Meta loss averaged over last 500 steps = 2.7588e-01, PNorm = 136.1760, GNorm = 0.2713
Meta loss on this task batch = 2.9285e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 136.1801, GNorm = 0.2786
Meta loss on this task batch = 3.1952e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 136.1848, GNorm = 0.3075
Meta loss on this task batch = 3.2410e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 136.1862, GNorm = 0.3454
Meta loss on this task batch = 2.4530e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 136.1880, GNorm = 0.2652
Meta loss on this task batch = 2.9028e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 136.1895, GNorm = 0.3484
Meta loss on this task batch = 2.8272e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 136.1903, GNorm = 0.3054
Meta loss on this task batch = 2.5770e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 136.1928, GNorm = 0.2809
Meta loss on this task batch = 2.9026e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 136.1960, GNorm = 0.3242
Took 154.7539234161377 seconds to complete one epoch of meta training
Took 165.50471568107605 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474008
Epoch 673
Meta loss on this task batch = 2.9234e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 136.2012, GNorm = 0.2727
Meta loss on this task batch = 2.2460e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 136.2089, GNorm = 0.2808
Meta loss on this task batch = 2.3563e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 136.2178, GNorm = 0.2683
Meta loss on this task batch = 2.9267e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 136.2272, GNorm = 0.2847
Meta loss on this task batch = 2.7261e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 136.2368, GNorm = 0.2659
Meta loss on this task batch = 2.7501e-01, Meta loss averaged over last 500 steps = 2.7599e-01, PNorm = 136.2457, GNorm = 0.2611
Meta loss on this task batch = 3.0151e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 136.2535, GNorm = 0.2814
Meta loss on this task batch = 2.5127e-01, Meta loss averaged over last 500 steps = 2.7596e-01, PNorm = 136.2596, GNorm = 0.2659
Meta loss on this task batch = 3.4148e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 136.2625, GNorm = 0.3088
Meta loss on this task batch = 2.8112e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 136.2636, GNorm = 0.2941
Meta loss on this task batch = 2.6184e-01, Meta loss averaged over last 500 steps = 2.7599e-01, PNorm = 136.2641, GNorm = 0.3203
Meta loss on this task batch = 2.8476e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 136.2628, GNorm = 0.3088
Meta loss on this task batch = 3.0178e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 136.2623, GNorm = 0.3005
Meta loss on this task batch = 2.6650e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 136.2612, GNorm = 0.2989
Meta loss on this task batch = 2.9213e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 136.2610, GNorm = 0.2873
Meta loss on this task batch = 2.7698e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 136.2643, GNorm = 0.3051
Meta loss on this task batch = 2.4721e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 136.2688, GNorm = 0.2603
Meta loss on this task batch = 2.4988e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 136.2739, GNorm = 0.2918
Meta loss on this task batch = 3.1509e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 136.2803, GNorm = 0.3928
Took 155.1662814617157 seconds to complete one epoch of meta training
Took 164.49024271965027 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479136
Epoch 674
Meta loss on this task batch = 2.3833e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 136.2870, GNorm = 0.2534
Meta loss on this task batch = 3.0180e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 136.2917, GNorm = 0.3062
Meta loss on this task batch = 2.8638e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 136.2974, GNorm = 0.2862
Meta loss on this task batch = 2.8695e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 136.3033, GNorm = 0.2753
Meta loss on this task batch = 2.6884e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 136.3079, GNorm = 0.2973
Meta loss on this task batch = 2.6989e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 136.3133, GNorm = 0.3657
Meta loss on this task batch = 2.4131e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 136.3163, GNorm = 0.2403
Meta loss on this task batch = 3.0175e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 136.3194, GNorm = 0.2909
Meta loss on this task batch = 2.8679e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 136.3213, GNorm = 0.2815
Meta loss on this task batch = 2.8897e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 136.3222, GNorm = 0.3006
Meta loss on this task batch = 2.3813e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 136.3218, GNorm = 0.2772
Meta loss on this task batch = 2.4077e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 136.3220, GNorm = 0.2458
Meta loss on this task batch = 2.5713e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 136.3219, GNorm = 0.2645
Meta loss on this task batch = 2.5528e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 136.3223, GNorm = 0.2646
Meta loss on this task batch = 3.0175e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 136.3219, GNorm = 0.3173
Meta loss on this task batch = 2.2616e-01, Meta loss averaged over last 500 steps = 2.7612e-01, PNorm = 136.3234, GNorm = 0.2236
Meta loss on this task batch = 2.4194e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 136.3267, GNorm = 0.2828
Meta loss on this task batch = 3.0007e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 136.3315, GNorm = 0.2808
Meta loss on this task batch = 2.0839e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 136.3373, GNorm = 0.2976
Took 157.08183336257935 seconds to complete one epoch of meta training
Took 166.02876353263855 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488358
Epoch 675
Meta loss on this task batch = 2.8108e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 136.3447, GNorm = 0.2718
Meta loss on this task batch = 2.2085e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 136.3517, GNorm = 0.2652
Meta loss on this task batch = 2.2128e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 136.3578, GNorm = 0.2553
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 136.3610, GNorm = 0.3324
Meta loss on this task batch = 2.5364e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 136.3631, GNorm = 0.2905
Meta loss on this task batch = 3.3045e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 136.3629, GNorm = 0.3320
Meta loss on this task batch = 2.4893e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 136.3599, GNorm = 0.3292
Meta loss on this task batch = 2.9210e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 136.3571, GNorm = 0.3148
Meta loss on this task batch = 2.8937e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 136.3548, GNorm = 0.3067
Meta loss on this task batch = 3.1292e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 136.3530, GNorm = 0.3124
Meta loss on this task batch = 2.7481e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 136.3519, GNorm = 0.2941
Meta loss on this task batch = 2.9278e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 136.3531, GNorm = 0.3066
Meta loss on this task batch = 2.9999e-01, Meta loss averaged over last 500 steps = 2.7636e-01, PNorm = 136.3574, GNorm = 0.3092
Meta loss on this task batch = 3.5557e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 136.3643, GNorm = 0.3543
Meta loss on this task batch = 2.7614e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 136.3741, GNorm = 0.3204
Meta loss on this task batch = 2.1203e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 136.3832, GNorm = 0.2820
Meta loss on this task batch = 2.2980e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 136.3924, GNorm = 0.2737
Meta loss on this task batch = 3.0740e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 136.4029, GNorm = 0.3057
Meta loss on this task batch = 3.2058e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 136.4150, GNorm = 0.3950
Took 150.94085884094238 seconds to complete one epoch of meta training
Took 159.22460842132568 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477177
Epoch 676
Meta loss on this task batch = 2.7752e-01, Meta loss averaged over last 500 steps = 2.7644e-01, PNorm = 136.4261, GNorm = 0.3524
Meta loss on this task batch = 3.0249e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 136.4365, GNorm = 0.2958
Meta loss on this task batch = 2.9443e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 136.4458, GNorm = 0.3476
Meta loss on this task batch = 2.7052e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 136.4553, GNorm = 0.2622
Meta loss on this task batch = 2.7177e-01, Meta loss averaged over last 500 steps = 2.7662e-01, PNorm = 136.4638, GNorm = 0.2750
Meta loss on this task batch = 2.6104e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 136.4700, GNorm = 0.2626
Meta loss on this task batch = 2.2801e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 136.4756, GNorm = 0.2991
Meta loss on this task batch = 3.0173e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 136.4809, GNorm = 0.3245
Meta loss on this task batch = 2.9617e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 136.4847, GNorm = 0.3301
Meta loss on this task batch = 2.8856e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 136.4875, GNorm = 0.2928
Meta loss on this task batch = 2.1239e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 136.4895, GNorm = 0.2435
Meta loss on this task batch = 2.6395e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 136.4907, GNorm = 0.3095
Meta loss on this task batch = 2.9193e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 136.4903, GNorm = 0.3125
Meta loss on this task batch = 2.8575e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 136.4911, GNorm = 0.3093
Meta loss on this task batch = 3.0665e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 136.4920, GNorm = 0.3091
Meta loss on this task batch = 2.7376e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 136.4947, GNorm = 0.3047
Meta loss on this task batch = 2.4048e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 136.4990, GNorm = 0.2557
Meta loss on this task batch = 2.7107e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 136.5043, GNorm = 0.2560
Meta loss on this task batch = 2.4465e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 136.5109, GNorm = 0.2970
Took 155.65040135383606 seconds to complete one epoch of meta training
Took 164.011714220047 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494285
Epoch 677
Meta loss on this task batch = 2.7147e-01, Meta loss averaged over last 500 steps = 2.7618e-01, PNorm = 136.5168, GNorm = 0.2831
Meta loss on this task batch = 3.1574e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 136.5235, GNorm = 0.3432
Meta loss on this task batch = 2.6246e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 136.5299, GNorm = 0.2964
Meta loss on this task batch = 2.1540e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 136.5363, GNorm = 0.2979
Meta loss on this task batch = 2.7680e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 136.5434, GNorm = 0.3292
Meta loss on this task batch = 2.9361e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 136.5492, GNorm = 0.4022
Meta loss on this task batch = 2.5284e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 136.5536, GNorm = 0.3384
Meta loss on this task batch = 2.8077e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 136.5566, GNorm = 0.3223
Meta loss on this task batch = 2.9226e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 136.5594, GNorm = 0.3715
Meta loss on this task batch = 3.1726e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 136.5638, GNorm = 0.3308
Meta loss on this task batch = 2.8802e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 136.5675, GNorm = 0.3172
Meta loss on this task batch = 2.6742e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 136.5715, GNorm = 0.3027
Meta loss on this task batch = 2.7032e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 136.5798, GNorm = 0.3215
Meta loss on this task batch = 2.4442e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 136.5908, GNorm = 0.2843
Meta loss on this task batch = 3.0341e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 136.6013, GNorm = 0.3070
Meta loss on this task batch = 2.4087e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 136.6112, GNorm = 0.2475
Meta loss on this task batch = 2.3733e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 136.6210, GNorm = 0.2382
Meta loss on this task batch = 2.7437e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 136.6295, GNorm = 0.3075
Meta loss on this task batch = 2.7635e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 136.6382, GNorm = 0.3009
Took 154.4039478302002 seconds to complete one epoch of meta training
Took 163.15654730796814 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451192
Epoch 678
Meta loss on this task batch = 2.7624e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 136.6464, GNorm = 0.3157
Meta loss on this task batch = 2.8488e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 136.6510, GNorm = 0.3841
Meta loss on this task batch = 3.2441e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 136.6529, GNorm = 0.3908
Meta loss on this task batch = 1.7921e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 136.6542, GNorm = 0.3240
Meta loss on this task batch = 2.2826e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 136.6569, GNorm = 0.2609
Meta loss on this task batch = 2.2322e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 136.6575, GNorm = 0.2794
Meta loss on this task batch = 2.8445e-01, Meta loss averaged over last 500 steps = 2.7600e-01, PNorm = 136.6558, GNorm = 0.3217
Meta loss on this task batch = 2.7469e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 136.6532, GNorm = 0.3622
Meta loss on this task batch = 2.6744e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 136.6523, GNorm = 0.2611
Meta loss on this task batch = 2.9638e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 136.6513, GNorm = 0.3307
Meta loss on this task batch = 2.4983e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 136.6551, GNorm = 0.2936
Meta loss on this task batch = 2.5908e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 136.6586, GNorm = 0.2724
Meta loss on this task batch = 2.7619e-01, Meta loss averaged over last 500 steps = 2.7575e-01, PNorm = 136.6626, GNorm = 0.2614
Meta loss on this task batch = 2.8604e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 136.6661, GNorm = 0.2718
Meta loss on this task batch = 2.9142e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 136.6684, GNorm = 0.3040
Meta loss on this task batch = 3.1304e-01, Meta loss averaged over last 500 steps = 2.7582e-01, PNorm = 136.6716, GNorm = 0.2624
Meta loss on this task batch = 2.8993e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 136.6758, GNorm = 0.2880
Meta loss on this task batch = 2.6848e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 136.6808, GNorm = 0.2710
Meta loss on this task batch = 3.2906e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 136.6843, GNorm = 0.3899
Took 153.0696415901184 seconds to complete one epoch of meta training
Took 163.73602414131165 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483011
Epoch 679
Meta loss on this task batch = 2.4583e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 136.6883, GNorm = 0.2389
Meta loss on this task batch = 2.5605e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 136.6949, GNorm = 0.3225
Meta loss on this task batch = 2.5741e-01, Meta loss averaged over last 500 steps = 2.7563e-01, PNorm = 136.7009, GNorm = 0.2862
Meta loss on this task batch = 3.1209e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 136.7074, GNorm = 0.3022
Meta loss on this task batch = 2.3613e-01, Meta loss averaged over last 500 steps = 2.7572e-01, PNorm = 136.7139, GNorm = 0.2512
Meta loss on this task batch = 2.6008e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 136.7184, GNorm = 0.3036
Meta loss on this task batch = 2.8629e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 136.7192, GNorm = 0.3311
Meta loss on this task batch = 2.6902e-01, Meta loss averaged over last 500 steps = 2.7582e-01, PNorm = 136.7191, GNorm = 0.3250
Meta loss on this task batch = 2.8377e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 136.7186, GNorm = 0.3611
Meta loss on this task batch = 2.4964e-01, Meta loss averaged over last 500 steps = 2.7580e-01, PNorm = 136.7171, GNorm = 0.3062
Meta loss on this task batch = 2.8656e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 136.7173, GNorm = 0.2723
Meta loss on this task batch = 2.7525e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 136.7179, GNorm = 0.3037
Meta loss on this task batch = 2.8066e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 136.7212, GNorm = 0.3511
Meta loss on this task batch = 2.7200e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 136.7265, GNorm = 0.3018
Meta loss on this task batch = 2.5665e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 136.7335, GNorm = 0.2875
Meta loss on this task batch = 3.0329e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 136.7390, GNorm = 0.2979
Meta loss on this task batch = 2.5162e-01, Meta loss averaged over last 500 steps = 2.7593e-01, PNorm = 136.7462, GNorm = 0.2681
Meta loss on this task batch = 2.9377e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 136.7539, GNorm = 0.2771
Meta loss on this task batch = 3.4366e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 136.7616, GNorm = 0.3539
Took 152.1005358695984 seconds to complete one epoch of meta training
Took 162.64393854141235 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470779
Epoch 680
Meta loss on this task batch = 2.6780e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 136.7672, GNorm = 0.2855
Meta loss on this task batch = 2.6991e-01, Meta loss averaged over last 500 steps = 2.7584e-01, PNorm = 136.7723, GNorm = 0.2807
Meta loss on this task batch = 1.8323e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 136.7764, GNorm = 0.2211
Meta loss on this task batch = 3.0271e-01, Meta loss averaged over last 500 steps = 2.7571e-01, PNorm = 136.7810, GNorm = 0.2898
Meta loss on this task batch = 3.1581e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 136.7845, GNorm = 0.3348
Meta loss on this task batch = 2.7087e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 136.7868, GNorm = 0.3110
Meta loss on this task batch = 2.0573e-01, Meta loss averaged over last 500 steps = 2.7577e-01, PNorm = 136.7872, GNorm = 0.2490
Meta loss on this task batch = 2.8127e-01, Meta loss averaged over last 500 steps = 2.7577e-01, PNorm = 136.7845, GNorm = 0.3118
Meta loss on this task batch = 2.9356e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 136.7801, GNorm = 0.3466
Meta loss on this task batch = 3.1781e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 136.7760, GNorm = 0.2852
Meta loss on this task batch = 2.8017e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 136.7746, GNorm = 0.3076
Meta loss on this task batch = 2.6331e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 136.7727, GNorm = 0.2663
Meta loss on this task batch = 2.3435e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 136.7734, GNorm = 0.2882
Meta loss on this task batch = 2.9669e-01, Meta loss averaged over last 500 steps = 2.7573e-01, PNorm = 136.7746, GNorm = 0.2586
Meta loss on this task batch = 2.4948e-01, Meta loss averaged over last 500 steps = 2.7559e-01, PNorm = 136.7755, GNorm = 0.2667
Meta loss on this task batch = 2.6336e-01, Meta loss averaged over last 500 steps = 2.7557e-01, PNorm = 136.7768, GNorm = 0.2808
Meta loss on this task batch = 2.8726e-01, Meta loss averaged over last 500 steps = 2.7559e-01, PNorm = 136.7798, GNorm = 0.2978
Meta loss on this task batch = 2.6348e-01, Meta loss averaged over last 500 steps = 2.7549e-01, PNorm = 136.7830, GNorm = 0.2912
Meta loss on this task batch = 2.6712e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 136.7845, GNorm = 0.3244
Took 152.61353969573975 seconds to complete one epoch of meta training
Took 162.2460286617279 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468414
Epoch 681
Meta loss on this task batch = 2.8475e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 136.7855, GNorm = 0.4261
Meta loss on this task batch = 3.0343e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 136.7890, GNorm = 0.3227
Meta loss on this task batch = 2.5972e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 136.7925, GNorm = 0.2782
Meta loss on this task batch = 2.8672e-01, Meta loss averaged over last 500 steps = 2.7567e-01, PNorm = 136.7976, GNorm = 0.2506
Meta loss on this task batch = 2.7190e-01, Meta loss averaged over last 500 steps = 2.7556e-01, PNorm = 136.8024, GNorm = 0.2749
Meta loss on this task batch = 2.5269e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 136.8088, GNorm = 0.2962
Meta loss on this task batch = 2.6930e-01, Meta loss averaged over last 500 steps = 2.7556e-01, PNorm = 136.8163, GNorm = 0.2641
Meta loss on this task batch = 2.5985e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 136.8221, GNorm = 0.3532
Meta loss on this task batch = 2.5921e-01, Meta loss averaged over last 500 steps = 2.7536e-01, PNorm = 136.8285, GNorm = 0.3717
Meta loss on this task batch = 2.8513e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 136.8360, GNorm = 0.3115
Meta loss on this task batch = 3.0387e-01, Meta loss averaged over last 500 steps = 2.7536e-01, PNorm = 136.8421, GNorm = 0.3497
Meta loss on this task batch = 3.3020e-01, Meta loss averaged over last 500 steps = 2.7541e-01, PNorm = 136.8479, GNorm = 0.3057
Meta loss on this task batch = 3.1771e-01, Meta loss averaged over last 500 steps = 2.7548e-01, PNorm = 136.8546, GNorm = 0.2767
Meta loss on this task batch = 2.6652e-01, Meta loss averaged over last 500 steps = 2.7550e-01, PNorm = 136.8605, GNorm = 0.2762
Meta loss on this task batch = 2.6094e-01, Meta loss averaged over last 500 steps = 2.7541e-01, PNorm = 136.8675, GNorm = 0.2570
Meta loss on this task batch = 2.9078e-01, Meta loss averaged over last 500 steps = 2.7539e-01, PNorm = 136.8728, GNorm = 0.3022
Meta loss on this task batch = 2.5319e-01, Meta loss averaged over last 500 steps = 2.7539e-01, PNorm = 136.8795, GNorm = 0.2660
Meta loss on this task batch = 3.0782e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 136.8832, GNorm = 0.3049
Meta loss on this task batch = 2.6903e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 136.8868, GNorm = 0.3336
Took 153.52607107162476 seconds to complete one epoch of meta training
Took 162.99622750282288 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466452
Epoch 682
Meta loss on this task batch = 2.9346e-01, Meta loss averaged over last 500 steps = 2.7560e-01, PNorm = 136.8891, GNorm = 0.3517
Meta loss on this task batch = 2.7662e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 136.8910, GNorm = 0.2916
Meta loss on this task batch = 2.2686e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 136.8956, GNorm = 0.2386
Meta loss on this task batch = 2.1468e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 136.9028, GNorm = 0.2376
Meta loss on this task batch = 1.9312e-01, Meta loss averaged over last 500 steps = 2.7531e-01, PNorm = 136.9105, GNorm = 0.2648
Meta loss on this task batch = 2.1561e-01, Meta loss averaged over last 500 steps = 2.7520e-01, PNorm = 136.9185, GNorm = 0.2251
Meta loss on this task batch = 3.0313e-01, Meta loss averaged over last 500 steps = 2.7525e-01, PNorm = 136.9261, GNorm = 0.3200
Meta loss on this task batch = 2.7171e-01, Meta loss averaged over last 500 steps = 2.7532e-01, PNorm = 136.9351, GNorm = 0.2794
Meta loss on this task batch = 2.7737e-01, Meta loss averaged over last 500 steps = 2.7526e-01, PNorm = 136.9425, GNorm = 0.2965
Meta loss on this task batch = 3.2348e-01, Meta loss averaged over last 500 steps = 2.7535e-01, PNorm = 136.9479, GNorm = 0.2927
Meta loss on this task batch = 3.0691e-01, Meta loss averaged over last 500 steps = 2.7546e-01, PNorm = 136.9512, GNorm = 0.3627
Meta loss on this task batch = 2.9327e-01, Meta loss averaged over last 500 steps = 2.7545e-01, PNorm = 136.9535, GNorm = 0.2922
Meta loss on this task batch = 2.5103e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 136.9562, GNorm = 0.2722
Meta loss on this task batch = 2.9414e-01, Meta loss averaged over last 500 steps = 2.7564e-01, PNorm = 136.9580, GNorm = 0.3110
Meta loss on this task batch = 3.3607e-01, Meta loss averaged over last 500 steps = 2.7567e-01, PNorm = 136.9599, GNorm = 0.2824
Meta loss on this task batch = 2.5901e-01, Meta loss averaged over last 500 steps = 2.7560e-01, PNorm = 136.9630, GNorm = 0.2666
Meta loss on this task batch = 3.2967e-01, Meta loss averaged over last 500 steps = 2.7567e-01, PNorm = 136.9668, GNorm = 0.3092
Meta loss on this task batch = 2.7845e-01, Meta loss averaged over last 500 steps = 2.7570e-01, PNorm = 136.9718, GNorm = 0.3062
Meta loss on this task batch = 2.4578e-01, Meta loss averaged over last 500 steps = 2.7571e-01, PNorm = 136.9769, GNorm = 0.3226
Took 155.7198498249054 seconds to complete one epoch of meta training
Took 165.36374926567078 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495965
Epoch 683
Meta loss on this task batch = 2.5975e-01, Meta loss averaged over last 500 steps = 2.7571e-01, PNorm = 136.9822, GNorm = 0.2555
Meta loss on this task batch = 2.6918e-01, Meta loss averaged over last 500 steps = 2.7558e-01, PNorm = 136.9876, GNorm = 0.2820
Meta loss on this task batch = 2.4526e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 136.9933, GNorm = 0.3000
Meta loss on this task batch = 2.2271e-01, Meta loss averaged over last 500 steps = 2.7537e-01, PNorm = 136.9995, GNorm = 0.2883
Meta loss on this task batch = 2.9795e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 137.0046, GNorm = 0.3152
Meta loss on this task batch = 2.7800e-01, Meta loss averaged over last 500 steps = 2.7537e-01, PNorm = 137.0092, GNorm = 0.2824
Meta loss on this task batch = 2.8155e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 137.0111, GNorm = 0.2958
Meta loss on this task batch = 2.7509e-01, Meta loss averaged over last 500 steps = 2.7540e-01, PNorm = 137.0109, GNorm = 0.2859
Meta loss on this task batch = 2.8602e-01, Meta loss averaged over last 500 steps = 2.7550e-01, PNorm = 137.0120, GNorm = 0.3231
Meta loss on this task batch = 2.7336e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 137.0125, GNorm = 0.2665
Meta loss on this task batch = 2.8758e-01, Meta loss averaged over last 500 steps = 2.7556e-01, PNorm = 137.0135, GNorm = 0.2817
Meta loss on this task batch = 2.6014e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 137.0144, GNorm = 0.2749
Meta loss on this task batch = 3.1703e-01, Meta loss averaged over last 500 steps = 2.7571e-01, PNorm = 137.0166, GNorm = 0.3518
Meta loss on this task batch = 2.7129e-01, Meta loss averaged over last 500 steps = 2.7566e-01, PNorm = 137.0201, GNorm = 0.2955
Meta loss on this task batch = 3.0343e-01, Meta loss averaged over last 500 steps = 2.7570e-01, PNorm = 137.0245, GNorm = 0.3016
Meta loss on this task batch = 2.7938e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 137.0302, GNorm = 0.2908
Meta loss on this task batch = 2.7909e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 137.0371, GNorm = 0.3352
Meta loss on this task batch = 2.8350e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 137.0464, GNorm = 0.3312
Meta loss on this task batch = 2.7246e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 137.0574, GNorm = 0.3936
Took 154.07711458206177 seconds to complete one epoch of meta training
Took 163.89252471923828 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506221
Epoch 684
Meta loss on this task batch = 2.4148e-01, Meta loss averaged over last 500 steps = 2.7573e-01, PNorm = 137.0662, GNorm = 0.2794
Meta loss on this task batch = 2.5784e-01, Meta loss averaged over last 500 steps = 2.7567e-01, PNorm = 137.0733, GNorm = 0.3097
Meta loss on this task batch = 2.7353e-01, Meta loss averaged over last 500 steps = 2.7555e-01, PNorm = 137.0763, GNorm = 0.3152
Meta loss on this task batch = 3.1948e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 137.0784, GNorm = 0.3443
Meta loss on this task batch = 2.3692e-01, Meta loss averaged over last 500 steps = 2.7545e-01, PNorm = 137.0822, GNorm = 0.2768
Meta loss on this task batch = 3.0108e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 137.0858, GNorm = 0.2830
Meta loss on this task batch = 2.6492e-01, Meta loss averaged over last 500 steps = 2.7555e-01, PNorm = 137.0898, GNorm = 0.2809
Meta loss on this task batch = 3.3122e-01, Meta loss averaged over last 500 steps = 2.7569e-01, PNorm = 137.0943, GNorm = 0.2896
Meta loss on this task batch = 3.2315e-01, Meta loss averaged over last 500 steps = 2.7582e-01, PNorm = 137.0995, GNorm = 0.3056
Meta loss on this task batch = 2.5799e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 137.1069, GNorm = 0.3074
Meta loss on this task batch = 2.5963e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 137.1150, GNorm = 0.3548
Meta loss on this task batch = 2.6573e-01, Meta loss averaged over last 500 steps = 2.7573e-01, PNorm = 137.1242, GNorm = 0.3116
Meta loss on this task batch = 3.1212e-01, Meta loss averaged over last 500 steps = 2.7575e-01, PNorm = 137.1337, GNorm = 0.3131
Meta loss on this task batch = 2.7777e-01, Meta loss averaged over last 500 steps = 2.7581e-01, PNorm = 137.1439, GNorm = 0.3192
Meta loss on this task batch = 2.3475e-01, Meta loss averaged over last 500 steps = 2.7568e-01, PNorm = 137.1541, GNorm = 0.2753
Meta loss on this task batch = 3.3509e-01, Meta loss averaged over last 500 steps = 2.7575e-01, PNorm = 137.1617, GNorm = 0.3395
Meta loss on this task batch = 2.7926e-01, Meta loss averaged over last 500 steps = 2.7580e-01, PNorm = 137.1676, GNorm = 0.3400
Meta loss on this task batch = 2.8498e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 137.1710, GNorm = 0.3169
Meta loss on this task batch = 2.8378e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 137.1718, GNorm = 0.3306
Took 155.95519924163818 seconds to complete one epoch of meta training
Took 164.42453789710999 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486194
Epoch 685
Meta loss on this task batch = 2.5495e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 137.1718, GNorm = 0.2790
Meta loss on this task batch = 2.4273e-01, Meta loss averaged over last 500 steps = 2.7572e-01, PNorm = 137.1728, GNorm = 0.2632
Meta loss on this task batch = 2.5969e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 137.1726, GNorm = 0.2814
Meta loss on this task batch = 2.5363e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 137.1726, GNorm = 0.2564
Meta loss on this task batch = 2.8302e-01, Meta loss averaged over last 500 steps = 2.7556e-01, PNorm = 137.1729, GNorm = 0.2619
Meta loss on this task batch = 2.3382e-01, Meta loss averaged over last 500 steps = 2.7532e-01, PNorm = 137.1733, GNorm = 0.2739
Meta loss on this task batch = 2.4751e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 137.1727, GNorm = 0.2911
Meta loss on this task batch = 3.0442e-01, Meta loss averaged over last 500 steps = 2.7537e-01, PNorm = 137.1739, GNorm = 0.3579
Meta loss on this task batch = 2.7220e-01, Meta loss averaged over last 500 steps = 2.7543e-01, PNorm = 137.1743, GNorm = 0.2734
Meta loss on this task batch = 2.8913e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 137.1746, GNorm = 0.3062
Meta loss on this task batch = 2.4573e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 137.1767, GNorm = 0.2824
Meta loss on this task batch = 2.8914e-01, Meta loss averaged over last 500 steps = 2.7545e-01, PNorm = 137.1821, GNorm = 0.3019
Meta loss on this task batch = 2.8452e-01, Meta loss averaged over last 500 steps = 2.7545e-01, PNorm = 137.1880, GNorm = 0.2639
Meta loss on this task batch = 2.8506e-01, Meta loss averaged over last 500 steps = 2.7543e-01, PNorm = 137.1913, GNorm = 0.3814
Meta loss on this task batch = 2.6975e-01, Meta loss averaged over last 500 steps = 2.7531e-01, PNorm = 137.1958, GNorm = 0.2908
Meta loss on this task batch = 3.0114e-01, Meta loss averaged over last 500 steps = 2.7539e-01, PNorm = 137.1993, GNorm = 0.2909
Meta loss on this task batch = 3.0574e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 137.2015, GNorm = 0.2886
Meta loss on this task batch = 3.1090e-01, Meta loss averaged over last 500 steps = 2.7539e-01, PNorm = 137.2036, GNorm = 0.3426
Meta loss on this task batch = 2.7319e-01, Meta loss averaged over last 500 steps = 2.7535e-01, PNorm = 137.2046, GNorm = 0.3259
Took 152.95999240875244 seconds to complete one epoch of meta training
Took 163.17078232765198 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457929
Epoch 686
Meta loss on this task batch = 2.7819e-01, Meta loss averaged over last 500 steps = 2.7539e-01, PNorm = 137.2060, GNorm = 0.3060
Meta loss on this task batch = 2.4299e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 137.2073, GNorm = 0.2741
Meta loss on this task batch = 2.8516e-01, Meta loss averaged over last 500 steps = 2.7534e-01, PNorm = 137.2077, GNorm = 0.2695
Meta loss on this task batch = 2.3063e-01, Meta loss averaged over last 500 steps = 2.7523e-01, PNorm = 137.2073, GNorm = 0.2995
Meta loss on this task batch = 2.7675e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 137.2074, GNorm = 0.3000
Meta loss on this task batch = 2.2742e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 137.2057, GNorm = 0.2830
Meta loss on this task batch = 3.0243e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 137.2048, GNorm = 0.3073
Meta loss on this task batch = 2.8492e-01, Meta loss averaged over last 500 steps = 2.7532e-01, PNorm = 137.2039, GNorm = 0.2844
Meta loss on this task batch = 2.5536e-01, Meta loss averaged over last 500 steps = 2.7536e-01, PNorm = 137.2034, GNorm = 0.2368
Meta loss on this task batch = 2.6461e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 137.2029, GNorm = 0.3837
Meta loss on this task batch = 2.4957e-01, Meta loss averaged over last 500 steps = 2.7530e-01, PNorm = 137.2045, GNorm = 0.2529
Meta loss on this task batch = 2.5592e-01, Meta loss averaged over last 500 steps = 2.7517e-01, PNorm = 137.2066, GNorm = 0.2736
Meta loss on this task batch = 2.7204e-01, Meta loss averaged over last 500 steps = 2.7518e-01, PNorm = 137.2093, GNorm = 0.3569
Meta loss on this task batch = 2.6460e-01, Meta loss averaged over last 500 steps = 2.7521e-01, PNorm = 137.2128, GNorm = 0.2339
Meta loss on this task batch = 3.0547e-01, Meta loss averaged over last 500 steps = 2.7525e-01, PNorm = 137.2178, GNorm = 0.3136
Meta loss on this task batch = 2.9372e-01, Meta loss averaged over last 500 steps = 2.7536e-01, PNorm = 137.2221, GNorm = 0.3118
Meta loss on this task batch = 2.9146e-01, Meta loss averaged over last 500 steps = 2.7539e-01, PNorm = 137.2272, GNorm = 0.2654
Meta loss on this task batch = 2.9015e-01, Meta loss averaged over last 500 steps = 2.7539e-01, PNorm = 137.2333, GNorm = 0.2903
Meta loss on this task batch = 3.1447e-01, Meta loss averaged over last 500 steps = 2.7550e-01, PNorm = 137.2403, GNorm = 0.3778
Took 152.5684838294983 seconds to complete one epoch of meta training
Took 161.5903079509735 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479828
Epoch 687
Meta loss on this task batch = 3.2334e-01, Meta loss averaged over last 500 steps = 2.7568e-01, PNorm = 137.2487, GNorm = 0.3441
Meta loss on this task batch = 2.3707e-01, Meta loss averaged over last 500 steps = 2.7566e-01, PNorm = 137.2567, GNorm = 0.2895
Meta loss on this task batch = 2.7748e-01, Meta loss averaged over last 500 steps = 2.7573e-01, PNorm = 137.2654, GNorm = 0.2744
Meta loss on this task batch = 2.1772e-01, Meta loss averaged over last 500 steps = 2.7565e-01, PNorm = 137.2747, GNorm = 0.2405
Meta loss on this task batch = 2.9011e-01, Meta loss averaged over last 500 steps = 2.7556e-01, PNorm = 137.2837, GNorm = 0.2968
Meta loss on this task batch = 2.8441e-01, Meta loss averaged over last 500 steps = 2.7565e-01, PNorm = 137.2923, GNorm = 0.3039
Meta loss on this task batch = 2.6703e-01, Meta loss averaged over last 500 steps = 2.7551e-01, PNorm = 137.3003, GNorm = 0.3291
Meta loss on this task batch = 2.5853e-01, Meta loss averaged over last 500 steps = 2.7546e-01, PNorm = 137.3076, GNorm = 0.2699
Meta loss on this task batch = 2.6759e-01, Meta loss averaged over last 500 steps = 2.7550e-01, PNorm = 137.3129, GNorm = 0.3299
Meta loss on this task batch = 3.0793e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 137.3150, GNorm = 0.3524
Meta loss on this task batch = 2.5115e-01, Meta loss averaged over last 500 steps = 2.7549e-01, PNorm = 137.3158, GNorm = 0.2739
Meta loss on this task batch = 2.4960e-01, Meta loss averaged over last 500 steps = 2.7543e-01, PNorm = 137.3161, GNorm = 0.3300
Meta loss on this task batch = 2.2005e-01, Meta loss averaged over last 500 steps = 2.7545e-01, PNorm = 137.3145, GNorm = 0.2854
Meta loss on this task batch = 2.9172e-01, Meta loss averaged over last 500 steps = 2.7557e-01, PNorm = 137.3136, GNorm = 0.2863
Meta loss on this task batch = 2.5178e-01, Meta loss averaged over last 500 steps = 2.7557e-01, PNorm = 137.3144, GNorm = 0.2646
Meta loss on this task batch = 2.8843e-01, Meta loss averaged over last 500 steps = 2.7556e-01, PNorm = 137.3172, GNorm = 0.3005
Meta loss on this task batch = 2.7998e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 137.3226, GNorm = 0.2907
Meta loss on this task batch = 2.7136e-01, Meta loss averaged over last 500 steps = 2.7551e-01, PNorm = 137.3296, GNorm = 0.2531
Meta loss on this task batch = 2.8149e-01, Meta loss averaged over last 500 steps = 2.7560e-01, PNorm = 137.3361, GNorm = 0.3881
Took 154.58358478546143 seconds to complete one epoch of meta training
Took 165.66534519195557 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479572
Epoch 688
Meta loss on this task batch = 2.7915e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 137.3446, GNorm = 0.2889
Meta loss on this task batch = 2.3062e-01, Meta loss averaged over last 500 steps = 2.7553e-01, PNorm = 137.3530, GNorm = 0.2460
Meta loss on this task batch = 3.1459e-01, Meta loss averaged over last 500 steps = 2.7564e-01, PNorm = 137.3615, GNorm = 0.3321
Meta loss on this task batch = 2.7208e-01, Meta loss averaged over last 500 steps = 2.7566e-01, PNorm = 137.3680, GNorm = 0.3126
Meta loss on this task batch = 2.3816e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 137.3723, GNorm = 0.3003
Meta loss on this task batch = 2.9046e-01, Meta loss averaged over last 500 steps = 2.7549e-01, PNorm = 137.3770, GNorm = 0.2883
Meta loss on this task batch = 3.0744e-01, Meta loss averaged over last 500 steps = 2.7563e-01, PNorm = 137.3812, GNorm = 0.3198
Meta loss on this task batch = 3.0735e-01, Meta loss averaged over last 500 steps = 2.7562e-01, PNorm = 137.3840, GNorm = 0.3143
Meta loss on this task batch = 2.5970e-01, Meta loss averaged over last 500 steps = 2.7558e-01, PNorm = 137.3866, GNorm = 0.2683
Meta loss on this task batch = 2.4291e-01, Meta loss averaged over last 500 steps = 2.7547e-01, PNorm = 137.3892, GNorm = 0.2490
Meta loss on this task batch = 2.8210e-01, Meta loss averaged over last 500 steps = 2.7551e-01, PNorm = 137.3925, GNorm = 0.2962
Meta loss on this task batch = 2.4490e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 137.3956, GNorm = 0.2480
Meta loss on this task batch = 2.8853e-01, Meta loss averaged over last 500 steps = 2.7548e-01, PNorm = 137.3973, GNorm = 0.3167
Meta loss on this task batch = 2.8802e-01, Meta loss averaged over last 500 steps = 2.7543e-01, PNorm = 137.3975, GNorm = 0.3063
Meta loss on this task batch = 2.5268e-01, Meta loss averaged over last 500 steps = 2.7535e-01, PNorm = 137.3990, GNorm = 0.2414
Meta loss on this task batch = 2.6725e-01, Meta loss averaged over last 500 steps = 2.7531e-01, PNorm = 137.4012, GNorm = 0.2657
Meta loss on this task batch = 3.1672e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 137.4030, GNorm = 0.2814
Meta loss on this task batch = 2.3662e-01, Meta loss averaged over last 500 steps = 2.7526e-01, PNorm = 137.4058, GNorm = 0.2883
Meta loss on this task batch = 2.7247e-01, Meta loss averaged over last 500 steps = 2.7533e-01, PNorm = 137.4102, GNorm = 0.2760
Took 155.8020613193512 seconds to complete one epoch of meta training
Took 164.90979981422424 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481457
Epoch 689
Meta loss on this task batch = 2.9730e-01, Meta loss averaged over last 500 steps = 2.7549e-01, PNorm = 137.4150, GNorm = 0.3016
Meta loss on this task batch = 2.4604e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 137.4193, GNorm = 0.2634
Meta loss on this task batch = 2.1715e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 137.4218, GNorm = 0.2518
Meta loss on this task batch = 2.4677e-01, Meta loss averaged over last 500 steps = 2.7543e-01, PNorm = 137.4242, GNorm = 0.2550
Meta loss on this task batch = 2.8713e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 137.4273, GNorm = 0.2728
Meta loss on this task batch = 3.2124e-01, Meta loss averaged over last 500 steps = 2.7532e-01, PNorm = 137.4297, GNorm = 0.3358
Meta loss on this task batch = 2.3786e-01, Meta loss averaged over last 500 steps = 2.7512e-01, PNorm = 137.4320, GNorm = 0.2555
Meta loss on this task batch = 2.1580e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 137.4355, GNorm = 0.2567
Meta loss on this task batch = 2.7958e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 137.4389, GNorm = 0.2851
Meta loss on this task batch = 2.7925e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 137.4413, GNorm = 0.2729
Meta loss on this task batch = 3.2330e-01, Meta loss averaged over last 500 steps = 2.7500e-01, PNorm = 137.4439, GNorm = 0.3833
Meta loss on this task batch = 2.6631e-01, Meta loss averaged over last 500 steps = 2.7492e-01, PNorm = 137.4472, GNorm = 0.2763
Meta loss on this task batch = 2.1363e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 137.4490, GNorm = 0.2886
Meta loss on this task batch = 2.6773e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 137.4506, GNorm = 0.3137
Meta loss on this task batch = 3.0623e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 137.4543, GNorm = 0.3171
Meta loss on this task batch = 2.9270e-01, Meta loss averaged over last 500 steps = 2.7505e-01, PNorm = 137.4609, GNorm = 0.3399
Meta loss on this task batch = 2.2148e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 137.4688, GNorm = 0.2687
Meta loss on this task batch = 3.3083e-01, Meta loss averaged over last 500 steps = 2.7515e-01, PNorm = 137.4761, GNorm = 0.3444
Meta loss on this task batch = 3.1626e-01, Meta loss averaged over last 500 steps = 2.7526e-01, PNorm = 137.4839, GNorm = 0.3306
Took 160.7056291103363 seconds to complete one epoch of meta training
Took 170.99141669273376 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459909
Epoch 690
Meta loss on this task batch = 2.7097e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 137.4904, GNorm = 0.2800
Meta loss on this task batch = 2.7823e-01, Meta loss averaged over last 500 steps = 2.7513e-01, PNorm = 137.4979, GNorm = 0.2497
Meta loss on this task batch = 2.8355e-01, Meta loss averaged over last 500 steps = 2.7519e-01, PNorm = 137.5039, GNorm = 0.3109
Meta loss on this task batch = 2.5250e-01, Meta loss averaged over last 500 steps = 2.7513e-01, PNorm = 137.5102, GNorm = 0.2921
Meta loss on this task batch = 3.6016e-01, Meta loss averaged over last 500 steps = 2.7525e-01, PNorm = 137.5142, GNorm = 0.3450
Meta loss on this task batch = 2.9342e-01, Meta loss averaged over last 500 steps = 2.7524e-01, PNorm = 137.5173, GNorm = 0.3111
Meta loss on this task batch = 3.3876e-01, Meta loss averaged over last 500 steps = 2.7537e-01, PNorm = 137.5201, GNorm = 0.3255
Meta loss on this task batch = 2.6217e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 137.5217, GNorm = 0.2717
Meta loss on this task batch = 2.5681e-01, Meta loss averaged over last 500 steps = 2.7548e-01, PNorm = 137.5227, GNorm = 0.2869
Meta loss on this task batch = 2.7131e-01, Meta loss averaged over last 500 steps = 2.7551e-01, PNorm = 137.5260, GNorm = 0.2585
Meta loss on this task batch = 2.9182e-01, Meta loss averaged over last 500 steps = 2.7548e-01, PNorm = 137.5305, GNorm = 0.2907
Meta loss on this task batch = 2.3236e-01, Meta loss averaged over last 500 steps = 2.7541e-01, PNorm = 137.5370, GNorm = 0.2692
Meta loss on this task batch = 2.7157e-01, Meta loss averaged over last 500 steps = 2.7540e-01, PNorm = 137.5455, GNorm = 0.3122
Meta loss on this task batch = 2.2984e-01, Meta loss averaged over last 500 steps = 2.7522e-01, PNorm = 137.5546, GNorm = 0.2426
Meta loss on this task batch = 2.7190e-01, Meta loss averaged over last 500 steps = 2.7533e-01, PNorm = 137.5637, GNorm = 0.2829
Meta loss on this task batch = 2.8898e-01, Meta loss averaged over last 500 steps = 2.7540e-01, PNorm = 137.5719, GNorm = 0.3374
Meta loss on this task batch = 3.1282e-01, Meta loss averaged over last 500 steps = 2.7544e-01, PNorm = 137.5787, GNorm = 0.3337
Meta loss on this task batch = 2.5233e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 137.5836, GNorm = 0.2673
Meta loss on this task batch = 2.2161e-01, Meta loss averaged over last 500 steps = 2.7528e-01, PNorm = 137.5901, GNorm = 0.3196
Took 152.6793508529663 seconds to complete one epoch of meta training
Took 162.09656596183777 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480980
Epoch 691
Meta loss on this task batch = 2.7447e-01, Meta loss averaged over last 500 steps = 2.7520e-01, PNorm = 137.5964, GNorm = 0.3199
Meta loss on this task batch = 2.5596e-01, Meta loss averaged over last 500 steps = 2.7512e-01, PNorm = 137.6007, GNorm = 0.2663
Meta loss on this task batch = 3.0356e-01, Meta loss averaged over last 500 steps = 2.7513e-01, PNorm = 137.6051, GNorm = 0.3402
Meta loss on this task batch = 2.4601e-01, Meta loss averaged over last 500 steps = 2.7508e-01, PNorm = 137.6093, GNorm = 0.3087
Meta loss on this task batch = 2.4425e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 137.6133, GNorm = 0.2985
Meta loss on this task batch = 2.4506e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 137.6190, GNorm = 0.2812
Meta loss on this task batch = 2.7248e-01, Meta loss averaged over last 500 steps = 2.7492e-01, PNorm = 137.6251, GNorm = 0.2766
Meta loss on this task batch = 2.5745e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 137.6331, GNorm = 0.2858
Meta loss on this task batch = 2.9270e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 137.6411, GNorm = 0.3700
Meta loss on this task batch = 2.7025e-01, Meta loss averaged over last 500 steps = 2.7477e-01, PNorm = 137.6509, GNorm = 0.3303
Meta loss on this task batch = 3.2476e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 137.6613, GNorm = 0.3421
Meta loss on this task batch = 2.0324e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 137.6710, GNorm = 0.2714
Meta loss on this task batch = 3.4333e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 137.6795, GNorm = 0.3204
Meta loss on this task batch = 3.1264e-01, Meta loss averaged over last 500 steps = 2.7500e-01, PNorm = 137.6868, GNorm = 0.3342
Meta loss on this task batch = 2.5127e-01, Meta loss averaged over last 500 steps = 2.7499e-01, PNorm = 137.6923, GNorm = 0.2953
Meta loss on this task batch = 2.3632e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 137.6963, GNorm = 0.2847
Meta loss on this task batch = 2.4945e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 137.6986, GNorm = 0.2783
Meta loss on this task batch = 2.8852e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 137.6998, GNorm = 0.2983
Meta loss on this task batch = 2.7996e-01, Meta loss averaged over last 500 steps = 2.7480e-01, PNorm = 137.7023, GNorm = 0.3343
Took 153.2299463748932 seconds to complete one epoch of meta training
Took 164.0805892944336 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482507
Epoch 692
Meta loss on this task batch = 3.2642e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 137.7062, GNorm = 0.3068
Meta loss on this task batch = 2.7830e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 137.7110, GNorm = 0.2709
Meta loss on this task batch = 2.7628e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 137.7144, GNorm = 0.3167
Meta loss on this task batch = 2.3273e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 137.7180, GNorm = 0.2489
Meta loss on this task batch = 2.5186e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 137.7223, GNorm = 0.2702
Meta loss on this task batch = 2.7150e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 137.7264, GNorm = 0.2715
Meta loss on this task batch = 2.6087e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 137.7322, GNorm = 0.2562
Meta loss on this task batch = 2.8918e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 137.7363, GNorm = 0.3226
Meta loss on this task batch = 2.7923e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 137.7409, GNorm = 0.2481
Meta loss on this task batch = 2.7794e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 137.7448, GNorm = 0.3167
Meta loss on this task batch = 2.8358e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 137.7484, GNorm = 0.2888
Meta loss on this task batch = 2.5981e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 137.7534, GNorm = 0.2838
Meta loss on this task batch = 3.3692e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 137.7562, GNorm = 0.3088
Meta loss on this task batch = 2.3009e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 137.7600, GNorm = 0.2636
Meta loss on this task batch = 2.8686e-01, Meta loss averaged over last 500 steps = 2.7475e-01, PNorm = 137.7633, GNorm = 0.3014
Meta loss on this task batch = 2.8982e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 137.7653, GNorm = 0.3476
Meta loss on this task batch = 2.8876e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 137.7677, GNorm = 0.2866
Meta loss on this task batch = 2.3878e-01, Meta loss averaged over last 500 steps = 2.7461e-01, PNorm = 137.7709, GNorm = 0.2549
Meta loss on this task batch = 2.5695e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 137.7748, GNorm = 0.3877
Took 154.14129829406738 seconds to complete one epoch of meta training
Took 164.60547137260437 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483995
Epoch 693
Meta loss on this task batch = 2.8465e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 137.7790, GNorm = 0.2997
Meta loss on this task batch = 3.1146e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 137.7839, GNorm = 0.3080
Meta loss on this task batch = 2.5208e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 137.7903, GNorm = 0.2810
Meta loss on this task batch = 2.0715e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 137.7949, GNorm = 0.2704
Meta loss on this task batch = 2.5458e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 137.7990, GNorm = 0.3090
Meta loss on this task batch = 2.9606e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 137.8044, GNorm = 0.3074
Meta loss on this task batch = 2.6227e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 137.8098, GNorm = 0.3068
Meta loss on this task batch = 2.9351e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 137.8153, GNorm = 0.3096
Meta loss on this task batch = 3.0621e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 137.8183, GNorm = 0.3090
Meta loss on this task batch = 2.4153e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 137.8200, GNorm = 0.3097
Meta loss on this task batch = 2.3719e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 137.8216, GNorm = 0.3079
Meta loss on this task batch = 2.9218e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 137.8242, GNorm = 0.3281
Meta loss on this task batch = 2.6823e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 137.8274, GNorm = 0.2713
Meta loss on this task batch = 2.6596e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 137.8322, GNorm = 0.3085
Meta loss on this task batch = 2.4108e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 137.8376, GNorm = 0.2718
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 137.8453, GNorm = 0.3275
Meta loss on this task batch = 2.8672e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 137.8526, GNorm = 0.3248
Meta loss on this task batch = 2.3434e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 137.8580, GNorm = 0.3299
Meta loss on this task batch = 2.8903e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 137.8632, GNorm = 0.3557
Took 153.05479192733765 seconds to complete one epoch of meta training
Took 164.04572820663452 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483747
Epoch 694
Meta loss on this task batch = 2.5781e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 137.8671, GNorm = 0.3251
Meta loss on this task batch = 3.2027e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 137.8701, GNorm = 0.3215
Meta loss on this task batch = 2.0018e-01, Meta loss averaged over last 500 steps = 2.7405e-01, PNorm = 137.8728, GNorm = 0.2617
Meta loss on this task batch = 2.4681e-01, Meta loss averaged over last 500 steps = 2.7401e-01, PNorm = 137.8772, GNorm = 0.2380
Meta loss on this task batch = 3.1578e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 137.8808, GNorm = 0.3024
Meta loss on this task batch = 2.6663e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 137.8847, GNorm = 0.3205
Meta loss on this task batch = 3.3198e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 137.8872, GNorm = 0.3081
Meta loss on this task batch = 2.5453e-01, Meta loss averaged over last 500 steps = 2.7413e-01, PNorm = 137.8894, GNorm = 0.2513
Meta loss on this task batch = 2.1995e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 137.8927, GNorm = 0.2759
Meta loss on this task batch = 2.8264e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 137.8920, GNorm = 0.2802
Meta loss on this task batch = 2.3553e-01, Meta loss averaged over last 500 steps = 2.7412e-01, PNorm = 137.8916, GNorm = 0.2817
Meta loss on this task batch = 3.2340e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 137.8902, GNorm = 0.3284
Meta loss on this task batch = 2.2412e-01, Meta loss averaged over last 500 steps = 2.7406e-01, PNorm = 137.8909, GNorm = 0.2576
Meta loss on this task batch = 3.2481e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 137.8928, GNorm = 0.5022
Meta loss on this task batch = 2.7332e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 137.8966, GNorm = 0.3115
Meta loss on this task batch = 2.9959e-01, Meta loss averaged over last 500 steps = 2.7413e-01, PNorm = 137.8984, GNorm = 0.3353
Meta loss on this task batch = 2.5404e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 137.9008, GNorm = 0.3278
Meta loss on this task batch = 2.7427e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 137.9041, GNorm = 0.2971
Meta loss on this task batch = 2.4806e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 137.9090, GNorm = 0.3021
Took 159.97820663452148 seconds to complete one epoch of meta training
Took 171.19687342643738 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451584
Epoch 695
Meta loss on this task batch = 2.8571e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 137.9156, GNorm = 0.2787
Meta loss on this task batch = 2.1990e-01, Meta loss averaged over last 500 steps = 2.7404e-01, PNorm = 137.9228, GNorm = 0.2611
Meta loss on this task batch = 2.9822e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 137.9275, GNorm = 0.2752
Meta loss on this task batch = 3.1340e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 137.9327, GNorm = 0.4106
Meta loss on this task batch = 2.9896e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 137.9391, GNorm = 0.2871
Meta loss on this task batch = 2.7181e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 137.9472, GNorm = 0.3002
Meta loss on this task batch = 2.2785e-01, Meta loss averaged over last 500 steps = 2.7408e-01, PNorm = 137.9541, GNorm = 0.3005
Meta loss on this task batch = 2.5604e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 137.9604, GNorm = 0.2773
Meta loss on this task batch = 2.8030e-01, Meta loss averaged over last 500 steps = 2.7408e-01, PNorm = 137.9659, GNorm = 0.3161
Meta loss on this task batch = 2.8398e-01, Meta loss averaged over last 500 steps = 2.7400e-01, PNorm = 137.9695, GNorm = 0.3324
Meta loss on this task batch = 2.5123e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 137.9736, GNorm = 0.2880
Meta loss on this task batch = 3.2045e-01, Meta loss averaged over last 500 steps = 2.7405e-01, PNorm = 137.9779, GNorm = 0.3371
Meta loss on this task batch = 2.6778e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 137.9817, GNorm = 0.2998
Meta loss on this task batch = 2.7591e-01, Meta loss averaged over last 500 steps = 2.7408e-01, PNorm = 137.9862, GNorm = 0.2699
Meta loss on this task batch = 2.9691e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 137.9889, GNorm = 0.3268
Meta loss on this task batch = 2.5355e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 137.9905, GNorm = 0.2751
Meta loss on this task batch = 3.0679e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 137.9921, GNorm = 0.3092
Meta loss on this task batch = 2.2370e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 137.9935, GNorm = 0.2554
Meta loss on this task batch = 2.8023e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 137.9931, GNorm = 0.3836
Took 153.4703848361969 seconds to complete one epoch of meta training
Took 163.56048345565796 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462529
Epoch 696
Meta loss on this task batch = 2.8120e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 137.9901, GNorm = 0.2799
Meta loss on this task batch = 3.2922e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 137.9862, GNorm = 0.3041
Meta loss on this task batch = 2.8488e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 137.9860, GNorm = 0.2926
Meta loss on this task batch = 2.7693e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 137.9881, GNorm = 0.3114
Meta loss on this task batch = 3.0232e-01, Meta loss averaged over last 500 steps = 2.7429e-01, PNorm = 137.9901, GNorm = 0.3690
Meta loss on this task batch = 2.7712e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 137.9939, GNorm = 0.2950
Meta loss on this task batch = 2.2997e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 138.0002, GNorm = 0.2740
Meta loss on this task batch = 2.2497e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 138.0071, GNorm = 0.2478
Meta loss on this task batch = 2.9752e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 138.0144, GNorm = 0.2855
Meta loss on this task batch = 2.4266e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 138.0220, GNorm = 0.2698
Meta loss on this task batch = 2.7893e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 138.0284, GNorm = 0.3038
Meta loss on this task batch = 2.7430e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 138.0337, GNorm = 0.2595
Meta loss on this task batch = 2.5318e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 138.0391, GNorm = 0.2926
Meta loss on this task batch = 2.9231e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 138.0433, GNorm = 0.3110
Meta loss on this task batch = 2.0820e-01, Meta loss averaged over last 500 steps = 2.7404e-01, PNorm = 138.0479, GNorm = 0.2819
Meta loss on this task batch = 2.8379e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 138.0511, GNorm = 0.2853
Meta loss on this task batch = 2.7611e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 138.0538, GNorm = 0.2580
Meta loss on this task batch = 2.4102e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 138.0569, GNorm = 0.2942
Meta loss on this task batch = 2.1640e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 138.0594, GNorm = 0.3717
Took 155.0605764389038 seconds to complete one epoch of meta training
Took 164.53611707687378 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507995
Epoch 697
Meta loss on this task batch = 2.7727e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 138.0612, GNorm = 0.3139
Meta loss on this task batch = 2.7305e-01, Meta loss averaged over last 500 steps = 2.7393e-01, PNorm = 138.0634, GNorm = 0.3169
Meta loss on this task batch = 2.6215e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 138.0666, GNorm = 0.2681
Meta loss on this task batch = 1.9962e-01, Meta loss averaged over last 500 steps = 2.7357e-01, PNorm = 138.0706, GNorm = 0.2717
Meta loss on this task batch = 2.6296e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 138.0752, GNorm = 0.2789
Meta loss on this task batch = 2.7584e-01, Meta loss averaged over last 500 steps = 2.7332e-01, PNorm = 138.0790, GNorm = 0.2837
Meta loss on this task batch = 2.8378e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 138.0824, GNorm = 0.3143
Meta loss on this task batch = 2.6208e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 138.0866, GNorm = 0.2862
Meta loss on this task batch = 1.9006e-01, Meta loss averaged over last 500 steps = 2.7316e-01, PNorm = 138.0914, GNorm = 0.2507
Meta loss on this task batch = 2.9366e-01, Meta loss averaged over last 500 steps = 2.7314e-01, PNorm = 138.0974, GNorm = 0.3099
Meta loss on this task batch = 2.5783e-01, Meta loss averaged over last 500 steps = 2.7311e-01, PNorm = 138.1046, GNorm = 0.2629
Meta loss on this task batch = 3.2054e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 138.1130, GNorm = 0.2861
Meta loss on this task batch = 2.6874e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 138.1189, GNorm = 0.3193
Meta loss on this task batch = 2.4273e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 138.1243, GNorm = 0.2861
Meta loss on this task batch = 2.8664e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 138.1299, GNorm = 0.3048
Meta loss on this task batch = 2.2658e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 138.1357, GNorm = 0.3098
Meta loss on this task batch = 3.2278e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 138.1401, GNorm = 0.2908
Meta loss on this task batch = 2.9983e-01, Meta loss averaged over last 500 steps = 2.7361e-01, PNorm = 138.1448, GNorm = 0.3394
Meta loss on this task batch = 3.0218e-01, Meta loss averaged over last 500 steps = 2.7363e-01, PNorm = 138.1493, GNorm = 0.3633
Took 155.50025844573975 seconds to complete one epoch of meta training
Took 166.14572763442993 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472877
Epoch 698
Meta loss on this task batch = 2.5421e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 138.1533, GNorm = 0.3475
Meta loss on this task batch = 2.8505e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 138.1594, GNorm = 0.3282
Meta loss on this task batch = 2.9920e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 138.1661, GNorm = 0.3179
Meta loss on this task batch = 3.1456e-01, Meta loss averaged over last 500 steps = 2.7344e-01, PNorm = 138.1727, GNorm = 0.3384
Meta loss on this task batch = 3.2934e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 138.1782, GNorm = 0.3205
Meta loss on this task batch = 2.9919e-01, Meta loss averaged over last 500 steps = 2.7351e-01, PNorm = 138.1845, GNorm = 0.4007
Meta loss on this task batch = 2.0402e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 138.1924, GNorm = 0.2904
Meta loss on this task batch = 2.9055e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 138.2019, GNorm = 0.2961
Meta loss on this task batch = 3.2816e-01, Meta loss averaged over last 500 steps = 2.7357e-01, PNorm = 138.2116, GNorm = 0.2933
Meta loss on this task batch = 3.5467e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 138.2203, GNorm = 0.3150
Meta loss on this task batch = 2.6707e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 138.2284, GNorm = 0.2898
Meta loss on this task batch = 3.4499e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 138.2374, GNorm = 0.3287
Meta loss on this task batch = 2.5529e-01, Meta loss averaged over last 500 steps = 2.7381e-01, PNorm = 138.2462, GNorm = 0.3016
Meta loss on this task batch = 2.8562e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 138.2559, GNorm = 0.3094
Meta loss on this task batch = 2.4992e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 138.2648, GNorm = 0.2867
Meta loss on this task batch = 1.9829e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 138.2726, GNorm = 0.2516
Meta loss on this task batch = 2.9744e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 138.2763, GNorm = 0.3352
Meta loss on this task batch = 2.6838e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 138.2764, GNorm = 0.2800
Meta loss on this task batch = 2.5236e-01, Meta loss averaged over last 500 steps = 2.7360e-01, PNorm = 138.2751, GNorm = 0.3294
Took 156.65141797065735 seconds to complete one epoch of meta training
Took 166.47912311553955 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478638
Epoch 699
Meta loss on this task batch = 2.6751e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 138.2741, GNorm = 0.2810
Meta loss on this task batch = 2.1630e-01, Meta loss averaged over last 500 steps = 2.7343e-01, PNorm = 138.2728, GNorm = 0.2571
Meta loss on this task batch = 2.3071e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 138.2719, GNorm = 0.2479
Meta loss on this task batch = 3.3162e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 138.2716, GNorm = 0.3343
Meta loss on this task batch = 2.5365e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 138.2705, GNorm = 0.2821
Meta loss on this task batch = 2.5039e-01, Meta loss averaged over last 500 steps = 2.7332e-01, PNorm = 138.2701, GNorm = 0.2703
Meta loss on this task batch = 3.0285e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 138.2697, GNorm = 0.3169
Meta loss on this task batch = 2.7725e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 138.2715, GNorm = 0.3401
Meta loss on this task batch = 2.8938e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 138.2757, GNorm = 0.3133
Meta loss on this task batch = 2.7418e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 138.2803, GNorm = 0.2730
Meta loss on this task batch = 2.5567e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 138.2888, GNorm = 0.2650
Meta loss on this task batch = 2.4187e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 138.2972, GNorm = 0.2460
Meta loss on this task batch = 2.2623e-01, Meta loss averaged over last 500 steps = 2.7327e-01, PNorm = 138.3060, GNorm = 0.2835
Meta loss on this task batch = 3.8483e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 138.3145, GNorm = 0.4092
Meta loss on this task batch = 2.5856e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 138.3228, GNorm = 0.3207
Meta loss on this task batch = 2.5583e-01, Meta loss averaged over last 500 steps = 2.7332e-01, PNorm = 138.3308, GNorm = 0.2559
Meta loss on this task batch = 2.7306e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 138.3386, GNorm = 0.2984
Meta loss on this task batch = 2.8700e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 138.3423, GNorm = 0.3626
Meta loss on this task batch = 3.3057e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 138.3453, GNorm = 0.3882
Took 162.8708906173706 seconds to complete one epoch of meta training
Took 172.03751063346863 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488993
Epoch 700
Meta loss on this task batch = 2.2556e-01, Meta loss averaged over last 500 steps = 2.7332e-01, PNorm = 138.3489, GNorm = 0.2685
Meta loss on this task batch = 2.4093e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 138.3512, GNorm = 0.3160
Meta loss on this task batch = 2.9255e-01, Meta loss averaged over last 500 steps = 2.7325e-01, PNorm = 138.3511, GNorm = 0.3387
Meta loss on this task batch = 2.4141e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 138.3506, GNorm = 0.2961
Meta loss on this task batch = 2.5612e-01, Meta loss averaged over last 500 steps = 2.7325e-01, PNorm = 138.3479, GNorm = 0.3395
Meta loss on this task batch = 3.1003e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 138.3444, GNorm = 0.3065
Meta loss on this task batch = 2.3765e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 138.3404, GNorm = 0.2947
Meta loss on this task batch = 3.6150e-01, Meta loss averaged over last 500 steps = 2.7336e-01, PNorm = 138.3365, GNorm = 0.3698
Meta loss on this task batch = 2.7210e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 138.3324, GNorm = 0.2686
Meta loss on this task batch = 2.8958e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 138.3310, GNorm = 0.2904
Meta loss on this task batch = 2.6352e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 138.3307, GNorm = 0.2786
Meta loss on this task batch = 2.6629e-01, Meta loss averaged over last 500 steps = 2.7332e-01, PNorm = 138.3329, GNorm = 0.2940
Meta loss on this task batch = 3.0316e-01, Meta loss averaged over last 500 steps = 2.7344e-01, PNorm = 138.3367, GNorm = 0.2929
Meta loss on this task batch = 2.4686e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 138.3408, GNorm = 0.2756
Meta loss on this task batch = 2.9549e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 138.3436, GNorm = 0.2951
Meta loss on this task batch = 3.0327e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 138.3434, GNorm = 0.2875
Meta loss on this task batch = 2.3140e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 138.3459, GNorm = 0.2568
Meta loss on this task batch = 3.0538e-01, Meta loss averaged over last 500 steps = 2.7350e-01, PNorm = 138.3492, GNorm = 0.2935
Meta loss on this task batch = 3.4409e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 138.3489, GNorm = 0.3970
Took 159.6420226097107 seconds to complete one epoch of meta training
Took 169.7342026233673 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461777
Epoch 701
Meta loss on this task batch = 2.8720e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 138.3495, GNorm = 0.2506
Meta loss on this task batch = 3.0468e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 138.3526, GNorm = 0.3055
Meta loss on this task batch = 3.0027e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 138.3565, GNorm = 0.3141
Meta loss on this task batch = 2.6162e-01, Meta loss averaged over last 500 steps = 2.7393e-01, PNorm = 138.3609, GNorm = 0.2460
Meta loss on this task batch = 2.7928e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 138.3648, GNorm = 0.3570
Meta loss on this task batch = 2.5052e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 138.3690, GNorm = 0.2904
Meta loss on this task batch = 2.7766e-01, Meta loss averaged over last 500 steps = 2.7396e-01, PNorm = 138.3754, GNorm = 0.3009
Meta loss on this task batch = 2.2649e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 138.3821, GNorm = 0.2767
Meta loss on this task batch = 2.6521e-01, Meta loss averaged over last 500 steps = 2.7406e-01, PNorm = 138.3897, GNorm = 0.2913
Meta loss on this task batch = 2.7853e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 138.3959, GNorm = 0.3354
Meta loss on this task batch = 2.3718e-01, Meta loss averaged over last 500 steps = 2.7407e-01, PNorm = 138.4010, GNorm = 0.3271
Meta loss on this task batch = 2.7203e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 138.4040, GNorm = 0.3065
Meta loss on this task batch = 2.2019e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 138.4073, GNorm = 0.2669
Meta loss on this task batch = 2.5262e-01, Meta loss averaged over last 500 steps = 2.7381e-01, PNorm = 138.4109, GNorm = 0.2668
Meta loss on this task batch = 2.4194e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 138.4131, GNorm = 0.2919
Meta loss on this task batch = 2.7072e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 138.4151, GNorm = 0.2973
Meta loss on this task batch = 2.7926e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 138.4174, GNorm = 0.2912
Meta loss on this task batch = 2.5979e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 138.4208, GNorm = 0.2819
Meta loss on this task batch = 3.5970e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 138.4223, GNorm = 0.4039
Took 147.94232511520386 seconds to complete one epoch of meta training
Took 157.90896320343018 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461295
Epoch 702
Meta loss on this task batch = 2.7053e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 138.4266, GNorm = 0.3015
Meta loss on this task batch = 2.8743e-01, Meta loss averaged over last 500 steps = 2.7355e-01, PNorm = 138.4309, GNorm = 0.3396
Meta loss on this task batch = 2.8492e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 138.4360, GNorm = 0.3116
Meta loss on this task batch = 2.9388e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 138.4426, GNorm = 0.3180
Meta loss on this task batch = 2.7999e-01, Meta loss averaged over last 500 steps = 2.7377e-01, PNorm = 138.4489, GNorm = 0.3034
Meta loss on this task batch = 2.6341e-01, Meta loss averaged over last 500 steps = 2.7366e-01, PNorm = 138.4557, GNorm = 0.3460
Meta loss on this task batch = 2.9865e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 138.4609, GNorm = 0.3191
Meta loss on this task batch = 3.0960e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 138.4671, GNorm = 0.3078
Meta loss on this task batch = 2.5765e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 138.4742, GNorm = 0.2713
Meta loss on this task batch = 2.6075e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 138.4809, GNorm = 0.2633
Meta loss on this task batch = 2.4072e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 138.4888, GNorm = 0.2928
Meta loss on this task batch = 3.0659e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 138.4976, GNorm = 0.2666
Meta loss on this task batch = 2.5401e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 138.5071, GNorm = 0.3176
Meta loss on this task batch = 3.4650e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 138.5152, GNorm = 0.3212
Meta loss on this task batch = 2.7017e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 138.5238, GNorm = 0.2979
Meta loss on this task batch = 2.0492e-01, Meta loss averaged over last 500 steps = 2.7357e-01, PNorm = 138.5331, GNorm = 0.2752
Meta loss on this task batch = 2.6484e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 138.5412, GNorm = 0.3143
Meta loss on this task batch = 3.0404e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 138.5462, GNorm = 0.3029
Meta loss on this task batch = 3.0199e-01, Meta loss averaged over last 500 steps = 2.7377e-01, PNorm = 138.5512, GNorm = 0.3565
Took 182.46272683143616 seconds to complete one epoch of meta training
Took 192.70708012580872 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.512040
Epoch 703
Meta loss on this task batch = 3.1174e-01, Meta loss averaged over last 500 steps = 2.7383e-01, PNorm = 138.5554, GNorm = 0.2857
Meta loss on this task batch = 2.5513e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 138.5586, GNorm = 0.3038
Meta loss on this task batch = 3.0242e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 138.5594, GNorm = 0.2991
Meta loss on this task batch = 2.5111e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 138.5603, GNorm = 0.2812
Meta loss on this task batch = 2.7225e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 138.5627, GNorm = 0.2950
Meta loss on this task batch = 2.6009e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 138.5659, GNorm = 0.2723
Meta loss on this task batch = 2.8657e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 138.5667, GNorm = 0.2844
Meta loss on this task batch = 2.8754e-01, Meta loss averaged over last 500 steps = 2.7381e-01, PNorm = 138.5652, GNorm = 0.3518
Meta loss on this task batch = 2.7074e-01, Meta loss averaged over last 500 steps = 2.7383e-01, PNorm = 138.5626, GNorm = 0.3154
Meta loss on this task batch = 2.6158e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 138.5608, GNorm = 0.2717
Meta loss on this task batch = 2.5707e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 138.5604, GNorm = 0.2470
Meta loss on this task batch = 2.9088e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 138.5601, GNorm = 0.3179
Meta loss on this task batch = 2.8738e-01, Meta loss averaged over last 500 steps = 2.7394e-01, PNorm = 138.5617, GNorm = 0.3037
Meta loss on this task batch = 2.8803e-01, Meta loss averaged over last 500 steps = 2.7396e-01, PNorm = 138.5639, GNorm = 0.3083
Meta loss on this task batch = 2.3908e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 138.5682, GNorm = 0.2969
Meta loss on this task batch = 2.8647e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 138.5742, GNorm = 0.2754
Meta loss on this task batch = 2.6813e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 138.5789, GNorm = 0.2668
Meta loss on this task batch = 3.4641e-01, Meta loss averaged over last 500 steps = 2.7391e-01, PNorm = 138.5839, GNorm = 0.3246
Meta loss on this task batch = 2.3419e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 138.5907, GNorm = 0.3303
Took 153.14899015426636 seconds to complete one epoch of meta training
Took 163.06707096099854 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474486
Epoch 704
Meta loss on this task batch = 2.3510e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 138.5960, GNorm = 0.2949
Meta loss on this task batch = 2.8885e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 138.6019, GNorm = 0.3182
Meta loss on this task batch = 2.6178e-01, Meta loss averaged over last 500 steps = 2.7383e-01, PNorm = 138.6032, GNorm = 0.3109
Meta loss on this task batch = 2.9269e-01, Meta loss averaged over last 500 steps = 2.7394e-01, PNorm = 138.6027, GNorm = 0.3308
Meta loss on this task batch = 2.2698e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 138.6038, GNorm = 0.2660
Meta loss on this task batch = 2.8277e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 138.6028, GNorm = 0.3034
Meta loss on this task batch = 2.8008e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 138.6044, GNorm = 0.3021
Meta loss on this task batch = 2.7912e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 138.6083, GNorm = 0.3148
Meta loss on this task batch = 2.7844e-01, Meta loss averaged over last 500 steps = 2.7376e-01, PNorm = 138.6129, GNorm = 0.2787
Meta loss on this task batch = 2.4274e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 138.6175, GNorm = 0.3350
Meta loss on this task batch = 3.4260e-01, Meta loss averaged over last 500 steps = 2.7412e-01, PNorm = 138.6218, GNorm = 0.3021
Meta loss on this task batch = 2.6749e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 138.6280, GNorm = 0.2908
Meta loss on this task batch = 3.0422e-01, Meta loss averaged over last 500 steps = 2.7425e-01, PNorm = 138.6361, GNorm = 0.3321
Meta loss on this task batch = 2.9779e-01, Meta loss averaged over last 500 steps = 2.7429e-01, PNorm = 138.6451, GNorm = 0.2872
Meta loss on this task batch = 2.6157e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 138.6539, GNorm = 0.2620
Meta loss on this task batch = 2.5355e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 138.6634, GNorm = 0.3057
Meta loss on this task batch = 3.1057e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 138.6720, GNorm = 0.3273
Meta loss on this task batch = 2.2614e-01, Meta loss averaged over last 500 steps = 2.7425e-01, PNorm = 138.6795, GNorm = 0.2486
Meta loss on this task batch = 2.5944e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 138.6859, GNorm = 0.3334
Took 147.9092719554901 seconds to complete one epoch of meta training
Took 155.59574842453003 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472412
Epoch 705
Meta loss on this task batch = 3.1130e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 138.6915, GNorm = 0.3156
Meta loss on this task batch = 2.7181e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 138.6952, GNorm = 0.2658
Meta loss on this task batch = 2.1411e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 138.6978, GNorm = 0.2553
Meta loss on this task batch = 2.8376e-01, Meta loss averaged over last 500 steps = 2.7402e-01, PNorm = 138.7003, GNorm = 0.2853
Meta loss on this task batch = 3.1355e-01, Meta loss averaged over last 500 steps = 2.7411e-01, PNorm = 138.6991, GNorm = 0.3490
Meta loss on this task batch = 2.5095e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 138.6978, GNorm = 0.2969
Meta loss on this task batch = 2.6659e-01, Meta loss averaged over last 500 steps = 2.7399e-01, PNorm = 138.6957, GNorm = 0.2820
Meta loss on this task batch = 2.4468e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 138.6944, GNorm = 0.2891
Meta loss on this task batch = 2.7545e-01, Meta loss averaged over last 500 steps = 2.7401e-01, PNorm = 138.6949, GNorm = 0.3149
Meta loss on this task batch = 2.9391e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 138.6965, GNorm = 0.2990
Meta loss on this task batch = 2.3866e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 138.6995, GNorm = 0.2871
Meta loss on this task batch = 2.9106e-01, Meta loss averaged over last 500 steps = 2.7404e-01, PNorm = 138.7038, GNorm = 0.3354
Meta loss on this task batch = 2.8313e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 138.7088, GNorm = 0.2548
Meta loss on this task batch = 3.2591e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 138.7179, GNorm = 0.3391
Meta loss on this task batch = 2.4496e-01, Meta loss averaged over last 500 steps = 2.7407e-01, PNorm = 138.7287, GNorm = 0.3042
Meta loss on this task batch = 3.1596e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 138.7403, GNorm = 0.3437
Meta loss on this task batch = 2.7496e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 138.7514, GNorm = 0.2772
Meta loss on this task batch = 2.9187e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 138.7614, GNorm = 0.3065
Meta loss on this task batch = 2.3890e-01, Meta loss averaged over last 500 steps = 2.7413e-01, PNorm = 138.7716, GNorm = 0.3086
Took 135.62570238113403 seconds to complete one epoch of meta training
Took 144.0252788066864 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498114
Epoch 706
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 138.7795, GNorm = 0.2691
Meta loss on this task batch = 2.4049e-01, Meta loss averaged over last 500 steps = 2.7411e-01, PNorm = 138.7846, GNorm = 0.2785
Meta loss on this task batch = 2.8250e-01, Meta loss averaged over last 500 steps = 2.7407e-01, PNorm = 138.7882, GNorm = 0.2670
Meta loss on this task batch = 2.6141e-01, Meta loss averaged over last 500 steps = 2.7409e-01, PNorm = 138.7897, GNorm = 0.2836
Meta loss on this task batch = 2.3221e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 138.7931, GNorm = 0.2518
Meta loss on this task batch = 2.8115e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 138.7948, GNorm = 0.3163
Meta loss on this task batch = 2.8516e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 138.7952, GNorm = 0.3115
Meta loss on this task batch = 2.4280e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 138.7947, GNorm = 0.2957
Meta loss on this task batch = 3.0957e-01, Meta loss averaged over last 500 steps = 2.7408e-01, PNorm = 138.7961, GNorm = 0.3185
Meta loss on this task batch = 2.8513e-01, Meta loss averaged over last 500 steps = 2.7404e-01, PNorm = 138.7976, GNorm = 0.3155
Meta loss on this task batch = 2.5612e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 138.8011, GNorm = 0.2812
Meta loss on this task batch = 2.8436e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 138.8048, GNorm = 0.3464
Meta loss on this task batch = 3.3803e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 138.8089, GNorm = 0.3047
Meta loss on this task batch = 2.7011e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 138.8131, GNorm = 0.2902
Meta loss on this task batch = 2.5520e-01, Meta loss averaged over last 500 steps = 2.7411e-01, PNorm = 138.8176, GNorm = 0.2744
Meta loss on this task batch = 2.6018e-01, Meta loss averaged over last 500 steps = 2.7400e-01, PNorm = 138.8225, GNorm = 0.2629
Meta loss on this task batch = 2.8162e-01, Meta loss averaged over last 500 steps = 2.7400e-01, PNorm = 138.8274, GNorm = 0.3072
Meta loss on this task batch = 2.5167e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 138.8319, GNorm = 0.2746
Meta loss on this task batch = 2.4862e-01, Meta loss averaged over last 500 steps = 2.7401e-01, PNorm = 138.8359, GNorm = 0.3243
Took 131.70580792427063 seconds to complete one epoch of meta training
Took 140.02469420433044 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498701
Epoch 707
Meta loss on this task batch = 3.0650e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 138.8394, GNorm = 0.2598
Meta loss on this task batch = 2.6979e-01, Meta loss averaged over last 500 steps = 2.7407e-01, PNorm = 138.8418, GNorm = 0.2690
Meta loss on this task batch = 3.1020e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 138.8418, GNorm = 0.3012
Meta loss on this task batch = 3.0349e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 138.8415, GNorm = 0.2905
Meta loss on this task batch = 2.3185e-01, Meta loss averaged over last 500 steps = 2.7413e-01, PNorm = 138.8416, GNorm = 0.2508
Meta loss on this task batch = 3.0164e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 138.8414, GNorm = 0.3123
Meta loss on this task batch = 2.8830e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 138.8417, GNorm = 0.3078
Meta loss on this task batch = 2.5559e-01, Meta loss averaged over last 500 steps = 2.7411e-01, PNorm = 138.8446, GNorm = 0.2822
Meta loss on this task batch = 2.4980e-01, Meta loss averaged over last 500 steps = 2.7409e-01, PNorm = 138.8489, GNorm = 0.3026
Meta loss on this task batch = 2.9302e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 138.8546, GNorm = 0.2980
Meta loss on this task batch = 2.4505e-01, Meta loss averaged over last 500 steps = 2.7405e-01, PNorm = 138.8633, GNorm = 0.2939
Meta loss on this task batch = 3.6372e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 138.8710, GNorm = 0.3383
Meta loss on this task batch = 2.4422e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 138.8787, GNorm = 0.2890
Meta loss on this task batch = 2.5516e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 138.8844, GNorm = 0.2512
Meta loss on this task batch = 2.6075e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 138.8883, GNorm = 0.3182
Meta loss on this task batch = 2.7846e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 138.8915, GNorm = 0.2765
Meta loss on this task batch = 2.4136e-01, Meta loss averaged over last 500 steps = 2.7408e-01, PNorm = 138.8951, GNorm = 0.2742
Meta loss on this task batch = 3.0946e-01, Meta loss averaged over last 500 steps = 2.7404e-01, PNorm = 138.8968, GNorm = 0.4132
Meta loss on this task batch = 2.1613e-01, Meta loss averaged over last 500 steps = 2.7383e-01, PNorm = 138.8996, GNorm = 0.2738
Took 134.81788325309753 seconds to complete one epoch of meta training
Took 142.32143092155457 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474442
Epoch 708
Meta loss on this task batch = 2.6345e-01, Meta loss averaged over last 500 steps = 2.7383e-01, PNorm = 138.9006, GNorm = 0.3302
Meta loss on this task batch = 2.5882e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 138.9016, GNorm = 0.2822
Meta loss on this task batch = 3.1596e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 138.9020, GNorm = 0.2904
Meta loss on this task batch = 2.5710e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 138.9039, GNorm = 0.3253
Meta loss on this task batch = 2.5037e-01, Meta loss averaged over last 500 steps = 2.7377e-01, PNorm = 138.9071, GNorm = 0.2541
Meta loss on this task batch = 2.4701e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 138.9121, GNorm = 0.2693
Meta loss on this task batch = 2.6987e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 138.9182, GNorm = 0.3593
Meta loss on this task batch = 2.7839e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 138.9253, GNorm = 0.2906
Meta loss on this task batch = 2.8137e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 138.9332, GNorm = 0.2810
Meta loss on this task batch = 3.5073e-01, Meta loss averaged over last 500 steps = 2.7406e-01, PNorm = 138.9399, GNorm = 0.3033
Meta loss on this task batch = 2.5115e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 138.9473, GNorm = 0.4675
Meta loss on this task batch = 3.0026e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 138.9541, GNorm = 0.3031
Meta loss on this task batch = 2.3872e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 138.9616, GNorm = 0.2960
Meta loss on this task batch = 2.8250e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 138.9683, GNorm = 0.3042
Meta loss on this task batch = 2.7491e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 138.9752, GNorm = 0.3007
Meta loss on this task batch = 2.9030e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 138.9814, GNorm = 0.3255
Meta loss on this task batch = 2.4993e-01, Meta loss averaged over last 500 steps = 2.7405e-01, PNorm = 138.9873, GNorm = 0.2924
Meta loss on this task batch = 3.1008e-01, Meta loss averaged over last 500 steps = 2.7409e-01, PNorm = 138.9920, GNorm = 0.3720
Meta loss on this task batch = 2.6548e-01, Meta loss averaged over last 500 steps = 2.7411e-01, PNorm = 138.9976, GNorm = 0.3369
Took 160.88917708396912 seconds to complete one epoch of meta training
Took 169.76840996742249 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494058
Epoch 709
Meta loss on this task batch = 2.1831e-01, Meta loss averaged over last 500 steps = 2.7396e-01, PNorm = 139.0010, GNorm = 0.3750
Meta loss on this task batch = 3.1587e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 139.0062, GNorm = 0.3844
Meta loss on this task batch = 2.8568e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 139.0098, GNorm = 0.3138
Meta loss on this task batch = 1.9307e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 139.0136, GNorm = 0.2645
Meta loss on this task batch = 2.2877e-01, Meta loss averaged over last 500 steps = 2.7360e-01, PNorm = 139.0186, GNorm = 0.3107
Meta loss on this task batch = 2.5348e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 139.0242, GNorm = 0.3208
Meta loss on this task batch = 3.2243e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 139.0286, GNorm = 0.3725
Meta loss on this task batch = 3.1362e-01, Meta loss averaged over last 500 steps = 2.7383e-01, PNorm = 139.0351, GNorm = 0.3496
Meta loss on this task batch = 2.8443e-01, Meta loss averaged over last 500 steps = 2.7391e-01, PNorm = 139.0403, GNorm = 0.3114
Meta loss on this task batch = 2.9446e-01, Meta loss averaged over last 500 steps = 2.7405e-01, PNorm = 139.0455, GNorm = 0.3421
Meta loss on this task batch = 2.5857e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 139.0488, GNorm = 0.3571
Meta loss on this task batch = 3.1701e-01, Meta loss averaged over last 500 steps = 2.7405e-01, PNorm = 139.0513, GNorm = 0.2982
Meta loss on this task batch = 2.8812e-01, Meta loss averaged over last 500 steps = 2.7407e-01, PNorm = 139.0547, GNorm = 0.2868
Meta loss on this task batch = 3.2077e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 139.0572, GNorm = 0.3001
Meta loss on this task batch = 2.9804e-01, Meta loss averaged over last 500 steps = 2.7418e-01, PNorm = 139.0606, GNorm = 0.3246
Meta loss on this task batch = 3.1695e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 139.0624, GNorm = 0.3281
Meta loss on this task batch = 3.1262e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 139.0650, GNorm = 0.2964
Meta loss on this task batch = 2.3928e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 139.0689, GNorm = 0.2743
Meta loss on this task batch = 2.5245e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 139.0730, GNorm = 0.3545
Took 165.11991000175476 seconds to complete one epoch of meta training
Took 173.71305966377258 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460477
Epoch 710
Meta loss on this task batch = 2.7814e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 139.0766, GNorm = 0.2796
Meta loss on this task batch = 2.4994e-01, Meta loss averaged over last 500 steps = 2.7406e-01, PNorm = 139.0819, GNorm = 0.2562
Meta loss on this task batch = 2.9773e-01, Meta loss averaged over last 500 steps = 2.7409e-01, PNorm = 139.0871, GNorm = 0.2599
Meta loss on this task batch = 3.0428e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 139.0924, GNorm = 0.2975
Meta loss on this task batch = 3.1801e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 139.0961, GNorm = 0.3689
Meta loss on this task batch = 3.2416e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 139.0996, GNorm = 0.3293
Meta loss on this task batch = 2.7112e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 139.1033, GNorm = 0.2987
Meta loss on this task batch = 2.5267e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 139.1066, GNorm = 0.3016
Meta loss on this task batch = 2.7035e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 139.1094, GNorm = 0.2796
Meta loss on this task batch = 2.7890e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 139.1137, GNorm = 0.3098
Meta loss on this task batch = 2.8254e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 139.1169, GNorm = 0.2782
Meta loss on this task batch = 2.8396e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 139.1197, GNorm = 0.3157
Meta loss on this task batch = 2.9948e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 139.1213, GNorm = 0.3284
Meta loss on this task batch = 3.4983e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 139.1246, GNorm = 0.3625
Meta loss on this task batch = 2.8546e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 139.1288, GNorm = 0.2776
Meta loss on this task batch = 2.8481e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 139.1357, GNorm = 0.2954
Meta loss on this task batch = 2.5027e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 139.1443, GNorm = 0.2343
Meta loss on this task batch = 2.3790e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 139.1546, GNorm = 0.2727
Meta loss on this task batch = 2.4565e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 139.1662, GNorm = 0.2853
Took 134.66313314437866 seconds to complete one epoch of meta training
Took 143.25611019134521 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487806
Epoch 711
Meta loss on this task batch = 3.2678e-01, Meta loss averaged over last 500 steps = 2.7431e-01, PNorm = 139.1768, GNorm = 0.3031
Meta loss on this task batch = 2.2996e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 139.1863, GNorm = 0.2805
Meta loss on this task batch = 3.0845e-01, Meta loss averaged over last 500 steps = 2.7425e-01, PNorm = 139.1949, GNorm = 0.3029
Meta loss on this task batch = 3.5196e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 139.2019, GNorm = 0.3598
Meta loss on this task batch = 3.0004e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 139.2053, GNorm = 0.3737
Meta loss on this task batch = 3.2571e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 139.2089, GNorm = 0.3703
Meta loss on this task batch = 2.8491e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 139.2126, GNorm = 0.3352
Meta loss on this task batch = 2.5507e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 139.2175, GNorm = 0.2741
Meta loss on this task batch = 2.1370e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 139.2245, GNorm = 0.2320
Meta loss on this task batch = 2.5898e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 139.2312, GNorm = 0.2607
Meta loss on this task batch = 2.4761e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 139.2372, GNorm = 0.2826
Meta loss on this task batch = 2.3039e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 139.2433, GNorm = 0.2359
Meta loss on this task batch = 3.3315e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 139.2501, GNorm = 0.3810
Meta loss on this task batch = 2.1359e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 139.2577, GNorm = 0.2749
Meta loss on this task batch = 2.8195e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 139.2652, GNorm = 0.3465
Meta loss on this task batch = 2.9979e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 139.2725, GNorm = 0.2989
Meta loss on this task batch = 2.7987e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 139.2797, GNorm = 0.2796
Meta loss on this task batch = 2.4636e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 139.2867, GNorm = 0.2641
Meta loss on this task batch = 2.6278e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 139.2905, GNorm = 0.3074
Took 141.16349911689758 seconds to complete one epoch of meta training
Took 149.4145164489746 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492282
Epoch 712
Meta loss on this task batch = 2.8646e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 139.2918, GNorm = 0.3174
Meta loss on this task batch = 3.0124e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 139.2907, GNorm = 0.3080
Meta loss on this task batch = 2.8009e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 139.2892, GNorm = 0.3125
Meta loss on this task batch = 2.5263e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 139.2887, GNorm = 0.2780
Meta loss on this task batch = 2.7150e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 139.2894, GNorm = 0.2866
Meta loss on this task batch = 3.3447e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 139.2928, GNorm = 0.3492
Meta loss on this task batch = 2.6389e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 139.2969, GNorm = 0.2761
Meta loss on this task batch = 2.7631e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 139.3026, GNorm = 0.2855
Meta loss on this task batch = 2.7078e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 139.3080, GNorm = 0.3010
Meta loss on this task batch = 2.8804e-01, Meta loss averaged over last 500 steps = 2.7449e-01, PNorm = 139.3137, GNorm = 0.3324
Meta loss on this task batch = 2.4852e-01, Meta loss averaged over last 500 steps = 2.7443e-01, PNorm = 139.3197, GNorm = 0.2853
Meta loss on this task batch = 2.9384e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 139.3270, GNorm = 0.3545
Meta loss on this task batch = 2.5359e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 139.3342, GNorm = 0.2753
Meta loss on this task batch = 2.9092e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 139.3409, GNorm = 0.2713
Meta loss on this task batch = 2.2501e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 139.3484, GNorm = 0.2661
Meta loss on this task batch = 2.5101e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 139.3553, GNorm = 0.2637
Meta loss on this task batch = 3.3856e-01, Meta loss averaged over last 500 steps = 2.7457e-01, PNorm = 139.3610, GNorm = 0.3152
Meta loss on this task batch = 2.3696e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 139.3660, GNorm = 0.2639
Meta loss on this task batch = 2.3374e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 139.3700, GNorm = 0.3231
Took 135.88003778457642 seconds to complete one epoch of meta training
Took 143.92294788360596 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476583
Epoch 713
Meta loss on this task batch = 2.2479e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 139.3735, GNorm = 0.2608
Meta loss on this task batch = 2.5827e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 139.3771, GNorm = 0.2740
Meta loss on this task batch = 2.5218e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 139.3812, GNorm = 0.2571
Meta loss on this task batch = 2.8002e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 139.3855, GNorm = 0.2947
Meta loss on this task batch = 2.5035e-01, Meta loss averaged over last 500 steps = 2.7409e-01, PNorm = 139.3889, GNorm = 0.2802
Meta loss on this task batch = 2.1900e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 139.3913, GNorm = 0.2839
Meta loss on this task batch = 2.6730e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 139.3936, GNorm = 0.2962
Meta loss on this task batch = 2.6623e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 139.3941, GNorm = 0.3440
Meta loss on this task batch = 2.9323e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 139.3943, GNorm = 0.2899
Meta loss on this task batch = 2.6271e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 139.3942, GNorm = 0.3198
Meta loss on this task batch = 2.9277e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 139.3926, GNorm = 0.3194
Meta loss on this task batch = 2.4152e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 139.3909, GNorm = 0.2971
Meta loss on this task batch = 2.8161e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 139.3913, GNorm = 0.2702
Meta loss on this task batch = 2.3798e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 139.3939, GNorm = 0.3043
Meta loss on this task batch = 2.2337e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 139.3979, GNorm = 0.2579
Meta loss on this task batch = 2.7097e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 139.4009, GNorm = 0.3083
Meta loss on this task batch = 2.4726e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 139.4063, GNorm = 0.3108
Meta loss on this task batch = 2.8666e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 139.4141, GNorm = 0.3091
Meta loss on this task batch = 3.0173e-01, Meta loss averaged over last 500 steps = 2.7394e-01, PNorm = 139.4230, GNorm = 0.3449
Took 175.62797093391418 seconds to complete one epoch of meta training
Took 184.2067472934723 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465161
Epoch 714
Meta loss on this task batch = 2.8860e-01, Meta loss averaged over last 500 steps = 2.7394e-01, PNorm = 139.4322, GNorm = 0.3020
Meta loss on this task batch = 2.4882e-01, Meta loss averaged over last 500 steps = 2.7393e-01, PNorm = 139.4416, GNorm = 0.2804
Meta loss on this task batch = 2.6892e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 139.4519, GNorm = 0.3314
Meta loss on this task batch = 2.4333e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 139.4598, GNorm = 0.2933
Meta loss on this task batch = 2.9965e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 139.4640, GNorm = 0.2932
Meta loss on this task batch = 2.6440e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 139.4671, GNorm = 0.3488
Meta loss on this task batch = 3.4217e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 139.4667, GNorm = 0.3733
Meta loss on this task batch = 2.9157e-01, Meta loss averaged over last 500 steps = 2.7409e-01, PNorm = 139.4651, GNorm = 0.3092
Meta loss on this task batch = 2.8666e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 139.4641, GNorm = 0.2990
Meta loss on this task batch = 2.6796e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 139.4637, GNorm = 0.2994
Meta loss on this task batch = 2.5922e-01, Meta loss averaged over last 500 steps = 2.7407e-01, PNorm = 139.4633, GNorm = 0.2907
Meta loss on this task batch = 2.3167e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 139.4645, GNorm = 0.2668
Meta loss on this task batch = 2.6060e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 139.4662, GNorm = 0.2848
Meta loss on this task batch = 2.4558e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 139.4680, GNorm = 0.2640
Meta loss on this task batch = 2.6753e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 139.4702, GNorm = 0.2580
Meta loss on this task batch = 3.1606e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 139.4715, GNorm = 0.2979
Meta loss on this task batch = 2.8213e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 139.4719, GNorm = 0.3476
Meta loss on this task batch = 2.3713e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 139.4702, GNorm = 0.2697
Meta loss on this task batch = 2.8784e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 139.4677, GNorm = 0.3318
Took 131.98673033714294 seconds to complete one epoch of meta training
Took 140.45511555671692 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488811
Epoch 715
Meta loss on this task batch = 2.4689e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 139.4656, GNorm = 0.2357
Meta loss on this task batch = 3.0499e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 139.4660, GNorm = 0.2717
Meta loss on this task batch = 2.7778e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 139.4671, GNorm = 0.3118
Meta loss on this task batch = 2.9088e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 139.4703, GNorm = 0.2975
Meta loss on this task batch = 2.3294e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 139.4762, GNorm = 0.2521
Meta loss on this task batch = 2.8022e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 139.4836, GNorm = 0.2973
Meta loss on this task batch = 2.2703e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 139.4912, GNorm = 0.2682
Meta loss on this task batch = 2.7175e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 139.4992, GNorm = 0.3167
Meta loss on this task batch = 2.1113e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 139.5074, GNorm = 0.2662
Meta loss on this task batch = 2.8591e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 139.5146, GNorm = 0.3364
Meta loss on this task batch = 3.1710e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 139.5219, GNorm = 0.2969
Meta loss on this task batch = 2.4777e-01, Meta loss averaged over last 500 steps = 2.7377e-01, PNorm = 139.5278, GNorm = 0.3201
Meta loss on this task batch = 2.6601e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 139.5348, GNorm = 0.3238
Meta loss on this task batch = 2.8675e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 139.5424, GNorm = 0.2754
Meta loss on this task batch = 2.5149e-01, Meta loss averaged over last 500 steps = 2.7391e-01, PNorm = 139.5496, GNorm = 0.2660
Meta loss on this task batch = 3.0354e-01, Meta loss averaged over last 500 steps = 2.7396e-01, PNorm = 139.5534, GNorm = 0.3614
Meta loss on this task batch = 2.8778e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 139.5570, GNorm = 0.3089
Meta loss on this task batch = 2.7436e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 139.5597, GNorm = 0.2848
Meta loss on this task batch = 3.3139e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 139.5595, GNorm = 0.4065
Took 130.62933611869812 seconds to complete one epoch of meta training
Took 139.3251976966858 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482318
Epoch 716
Meta loss on this task batch = 2.5189e-01, Meta loss averaged over last 500 steps = 2.7411e-01, PNorm = 139.5602, GNorm = 0.2814
Meta loss on this task batch = 2.6273e-01, Meta loss averaged over last 500 steps = 2.7402e-01, PNorm = 139.5647, GNorm = 0.3352
Meta loss on this task batch = 2.7731e-01, Meta loss averaged over last 500 steps = 2.7399e-01, PNorm = 139.5690, GNorm = 0.3342
Meta loss on this task batch = 2.2642e-01, Meta loss averaged over last 500 steps = 2.7400e-01, PNorm = 139.5740, GNorm = 0.2483
Meta loss on this task batch = 2.2800e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 139.5796, GNorm = 0.2538
Meta loss on this task batch = 2.7321e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 139.5843, GNorm = 0.2801
Meta loss on this task batch = 2.9657e-01, Meta loss averaged over last 500 steps = 2.7376e-01, PNorm = 139.5878, GNorm = 0.3098
Meta loss on this task batch = 2.3469e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 139.5896, GNorm = 0.3056
Meta loss on this task batch = 2.3198e-01, Meta loss averaged over last 500 steps = 2.7357e-01, PNorm = 139.5905, GNorm = 0.2556
Meta loss on this task batch = 3.0354e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 139.5913, GNorm = 0.3129
Meta loss on this task batch = 2.6923e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 139.5920, GNorm = 0.2697
Meta loss on this task batch = 2.4143e-01, Meta loss averaged over last 500 steps = 2.7339e-01, PNorm = 139.5937, GNorm = 0.2504
Meta loss on this task batch = 2.3265e-01, Meta loss averaged over last 500 steps = 2.7317e-01, PNorm = 139.5971, GNorm = 0.2665
Meta loss on this task batch = 2.7542e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 139.5976, GNorm = 0.4541
Meta loss on this task batch = 3.2311e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 139.5988, GNorm = 0.3095
Meta loss on this task batch = 2.8594e-01, Meta loss averaged over last 500 steps = 2.7336e-01, PNorm = 139.6005, GNorm = 0.3425
Meta loss on this task batch = 2.8625e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 139.6036, GNorm = 0.3061
Meta loss on this task batch = 2.8657e-01, Meta loss averaged over last 500 steps = 2.7346e-01, PNorm = 139.6088, GNorm = 0.3137
Meta loss on this task batch = 3.0049e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 139.6151, GNorm = 0.4615
Took 137.43461322784424 seconds to complete one epoch of meta training
Took 145.0338671207428 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474014
Epoch 717
Meta loss on this task batch = 2.2061e-01, Meta loss averaged over last 500 steps = 2.7350e-01, PNorm = 139.6234, GNorm = 0.3304
Meta loss on this task batch = 2.6614e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 139.6316, GNorm = 0.3480
Meta loss on this task batch = 2.4452e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 139.6404, GNorm = 0.2465
Meta loss on this task batch = 2.8177e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 139.6489, GNorm = 0.2819
Meta loss on this task batch = 2.3908e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 139.6571, GNorm = 0.3365
Meta loss on this task batch = 2.5110e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 139.6634, GNorm = 0.2839
Meta loss on this task batch = 2.6647e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 139.6673, GNorm = 0.3345
Meta loss on this task batch = 3.3174e-01, Meta loss averaged over last 500 steps = 2.7350e-01, PNorm = 139.6705, GNorm = 0.3767
Meta loss on this task batch = 3.4561e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 139.6709, GNorm = 0.3752
Meta loss on this task batch = 2.0982e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 139.6702, GNorm = 0.2732
Meta loss on this task batch = 2.6421e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 139.6675, GNorm = 0.2902
Meta loss on this task batch = 3.0680e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 139.6632, GNorm = 0.3610
Meta loss on this task batch = 2.9959e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 139.6594, GNorm = 0.3332
Meta loss on this task batch = 2.8246e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 139.6577, GNorm = 0.2712
Meta loss on this task batch = 2.6541e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 139.6573, GNorm = 0.2776
Meta loss on this task batch = 2.8769e-01, Meta loss averaged over last 500 steps = 2.7376e-01, PNorm = 139.6584, GNorm = 0.2288
Meta loss on this task batch = 3.0840e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 139.6587, GNorm = 0.3243
Meta loss on this task batch = 2.7664e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 139.6591, GNorm = 0.2920
Meta loss on this task batch = 2.6063e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 139.6598, GNorm = 0.3279
Took 136.48593854904175 seconds to complete one epoch of meta training
Took 144.1814067363739 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467655
Epoch 718
Meta loss on this task batch = 2.5711e-01, Meta loss averaged over last 500 steps = 2.7360e-01, PNorm = 139.6612, GNorm = 0.2938
Meta loss on this task batch = 3.0420e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 139.6659, GNorm = 0.2891
Meta loss on this task batch = 2.5918e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 139.6724, GNorm = 0.2731
Meta loss on this task batch = 2.8214e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 139.6786, GNorm = 0.2972
Meta loss on this task batch = 2.6326e-01, Meta loss averaged over last 500 steps = 2.7377e-01, PNorm = 139.6855, GNorm = 0.2686
Meta loss on this task batch = 3.1686e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 139.6932, GNorm = 0.3355
Meta loss on this task batch = 2.4738e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 139.6999, GNorm = 0.2706
Meta loss on this task batch = 2.4371e-01, Meta loss averaged over last 500 steps = 2.7361e-01, PNorm = 139.7075, GNorm = 0.2916
Meta loss on this task batch = 2.6654e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 139.7153, GNorm = 0.2888
Meta loss on this task batch = 3.2431e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 139.7226, GNorm = 0.3073
Meta loss on this task batch = 2.5920e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 139.7301, GNorm = 0.3281
Meta loss on this task batch = 2.3812e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 139.7372, GNorm = 0.2805
Meta loss on this task batch = 2.4845e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 139.7442, GNorm = 0.3282
Meta loss on this task batch = 3.0375e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 139.7498, GNorm = 0.4067
Meta loss on this task batch = 2.8220e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 139.7546, GNorm = 0.3030
Meta loss on this task batch = 2.7789e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 139.7579, GNorm = 0.3289
Meta loss on this task batch = 2.5205e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 139.7596, GNorm = 0.3395
Meta loss on this task batch = 2.6216e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 139.7593, GNorm = 0.2736
Meta loss on this task batch = 2.7923e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 139.7575, GNorm = 0.3820
Took 135.1256103515625 seconds to complete one epoch of meta training
Took 142.63147163391113 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460904
Epoch 719
Meta loss on this task batch = 2.5770e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 139.7564, GNorm = 0.3028
Meta loss on this task batch = 2.3498e-01, Meta loss averaged over last 500 steps = 2.7351e-01, PNorm = 139.7554, GNorm = 0.3660
Meta loss on this task batch = 2.5600e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 139.7556, GNorm = 0.2609
Meta loss on this task batch = 3.1188e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 139.7545, GNorm = 0.3136
Meta loss on this task batch = 2.9099e-01, Meta loss averaged over last 500 steps = 2.7360e-01, PNorm = 139.7555, GNorm = 0.2815
Meta loss on this task batch = 3.0130e-01, Meta loss averaged over last 500 steps = 2.7369e-01, PNorm = 139.7594, GNorm = 0.3380
Meta loss on this task batch = 2.1335e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 139.7648, GNorm = 0.2473
Meta loss on this task batch = 2.7999e-01, Meta loss averaged over last 500 steps = 2.7348e-01, PNorm = 139.7726, GNorm = 0.2785
Meta loss on this task batch = 2.4804e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 139.7799, GNorm = 0.2661
Meta loss on this task batch = 2.4511e-01, Meta loss averaged over last 500 steps = 2.7355e-01, PNorm = 139.7877, GNorm = 0.2890
Meta loss on this task batch = 2.7633e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 139.7947, GNorm = 0.2902
Meta loss on this task batch = 2.6584e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 139.8028, GNorm = 0.2808
Meta loss on this task batch = 2.9906e-01, Meta loss averaged over last 500 steps = 2.7360e-01, PNorm = 139.8130, GNorm = 0.2923
Meta loss on this task batch = 2.1874e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 139.8226, GNorm = 0.2911
Meta loss on this task batch = 2.8319e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 139.8321, GNorm = 0.2984
Meta loss on this task batch = 2.7707e-01, Meta loss averaged over last 500 steps = 2.7348e-01, PNorm = 139.8387, GNorm = 0.2881
Meta loss on this task batch = 2.8864e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 139.8447, GNorm = 0.3174
Meta loss on this task batch = 3.2510e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 139.8506, GNorm = 0.3291
Meta loss on this task batch = 2.9014e-01, Meta loss averaged over last 500 steps = 2.7369e-01, PNorm = 139.8573, GNorm = 0.3688
Took 136.02708435058594 seconds to complete one epoch of meta training
Took 144.75457954406738 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492856
Epoch 720
Meta loss on this task batch = 2.6086e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 139.8613, GNorm = 0.3218
Meta loss on this task batch = 2.8194e-01, Meta loss averaged over last 500 steps = 2.7376e-01, PNorm = 139.8655, GNorm = 0.3062
Meta loss on this task batch = 2.4353e-01, Meta loss averaged over last 500 steps = 2.7366e-01, PNorm = 139.8693, GNorm = 0.3151
Meta loss on this task batch = 2.4920e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 139.8703, GNorm = 0.3463
Meta loss on this task batch = 2.3118e-01, Meta loss averaged over last 500 steps = 2.7357e-01, PNorm = 139.8729, GNorm = 0.2473
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 139.8763, GNorm = 0.2835
Meta loss on this task batch = 2.5146e-01, Meta loss averaged over last 500 steps = 2.7351e-01, PNorm = 139.8792, GNorm = 0.3170
Meta loss on this task batch = 2.7443e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 139.8842, GNorm = 0.3096
Meta loss on this task batch = 3.5328e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 139.8860, GNorm = 0.3645
Meta loss on this task batch = 2.5133e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 139.8890, GNorm = 0.2739
Meta loss on this task batch = 2.7300e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 139.8931, GNorm = 0.2964
Meta loss on this task batch = 2.5127e-01, Meta loss averaged over last 500 steps = 2.7361e-01, PNorm = 139.8977, GNorm = 0.2716
Meta loss on this task batch = 2.5589e-01, Meta loss averaged over last 500 steps = 2.7346e-01, PNorm = 139.9017, GNorm = 0.2985
Meta loss on this task batch = 3.0375e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 139.9049, GNorm = 0.3079
Meta loss on this task batch = 2.3691e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 139.9078, GNorm = 0.2527
Meta loss on this task batch = 2.8681e-01, Meta loss averaged over last 500 steps = 2.7360e-01, PNorm = 139.9106, GNorm = 0.2427
Meta loss on this task batch = 3.3183e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 139.9118, GNorm = 0.3326
Meta loss on this task batch = 3.2569e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 139.9126, GNorm = 0.2529
Meta loss on this task batch = 2.7095e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 139.9141, GNorm = 0.3508
Took 142.50818610191345 seconds to complete one epoch of meta training
Took 151.11489939689636 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484063
Epoch 721
Meta loss on this task batch = 2.8276e-01, Meta loss averaged over last 500 steps = 2.7381e-01, PNorm = 139.9198, GNorm = 0.3209
Meta loss on this task batch = 2.6195e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 139.9268, GNorm = 0.2959
Meta loss on this task batch = 3.1841e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 139.9353, GNorm = 0.3677
Meta loss on this task batch = 2.8030e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 139.9453, GNorm = 0.2598
Meta loss on this task batch = 2.2701e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 139.9584, GNorm = 0.3049
Meta loss on this task batch = 2.8690e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 139.9718, GNorm = 0.3065
Meta loss on this task batch = 2.3814e-01, Meta loss averaged over last 500 steps = 2.7376e-01, PNorm = 139.9864, GNorm = 0.2678
Meta loss on this task batch = 2.1904e-01, Meta loss averaged over last 500 steps = 2.7376e-01, PNorm = 139.9987, GNorm = 0.2667
Meta loss on this task batch = 2.1188e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 140.0098, GNorm = 0.2565
Meta loss on this task batch = 2.7847e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 140.0165, GNorm = 0.3116
Meta loss on this task batch = 2.9159e-01, Meta loss averaged over last 500 steps = 2.7351e-01, PNorm = 140.0180, GNorm = 0.3686
Meta loss on this task batch = 2.4895e-01, Meta loss averaged over last 500 steps = 2.7346e-01, PNorm = 140.0160, GNorm = 0.2981
Meta loss on this task batch = 3.1625e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 140.0123, GNorm = 0.2738
Meta loss on this task batch = 3.3769e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 140.0090, GNorm = 0.3037
Meta loss on this task batch = 2.7669e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 140.0055, GNorm = 0.5980
Meta loss on this task batch = 2.6145e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 140.0022, GNorm = 0.3798
Meta loss on this task batch = 3.2399e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 140.0033, GNorm = 0.3149
Meta loss on this task batch = 2.9988e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 140.0087, GNorm = 0.3255
Meta loss on this task batch = 2.5078e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 140.0155, GNorm = 0.3051
Took 154.27875471115112 seconds to complete one epoch of meta training
Took 162.7166669368744 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469468
Epoch 722
Meta loss on this task batch = 2.1712e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 140.0253, GNorm = 0.2628
Meta loss on this task batch = 2.2790e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 140.0348, GNorm = 0.2707
Meta loss on this task batch = 3.0373e-01, Meta loss averaged over last 500 steps = 2.7366e-01, PNorm = 140.0454, GNorm = 0.3000
Meta loss on this task batch = 2.6655e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 140.0567, GNorm = 0.3110
Meta loss on this task batch = 3.0695e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 140.0700, GNorm = 0.2855
Meta loss on this task batch = 2.3948e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 140.0843, GNorm = 0.2524
Meta loss on this task batch = 2.7153e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 140.0966, GNorm = 0.3073
Meta loss on this task batch = 3.1504e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 140.1075, GNorm = 0.3210
Meta loss on this task batch = 2.8677e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 140.1133, GNorm = 0.3064
Meta loss on this task batch = 2.9152e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 140.1175, GNorm = 0.2941
Meta loss on this task batch = 3.1503e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 140.1195, GNorm = 0.3712
Meta loss on this task batch = 2.7823e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 140.1216, GNorm = 0.3319
Meta loss on this task batch = 2.6535e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 140.1235, GNorm = 0.2600
Meta loss on this task batch = 2.5383e-01, Meta loss averaged over last 500 steps = 2.7381e-01, PNorm = 140.1263, GNorm = 0.2847
Meta loss on this task batch = 2.8933e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 140.1281, GNorm = 0.2849
Meta loss on this task batch = 2.6987e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 140.1314, GNorm = 0.2523
Meta loss on this task batch = 2.8124e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 140.1352, GNorm = 0.3101
Meta loss on this task batch = 2.2206e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 140.1401, GNorm = 0.2513
Meta loss on this task batch = 2.3300e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 140.1458, GNorm = 0.2789
Took 142.42470240592957 seconds to complete one epoch of meta training
Took 150.76620030403137 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501548
Epoch 723
Meta loss on this task batch = 2.6567e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 140.1523, GNorm = 0.2628
Meta loss on this task batch = 3.4041e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 140.1585, GNorm = 0.2879
Meta loss on this task batch = 3.3431e-01, Meta loss averaged over last 500 steps = 2.7402e-01, PNorm = 140.1639, GNorm = 0.3065
Meta loss on this task batch = 2.7310e-01, Meta loss averaged over last 500 steps = 2.7401e-01, PNorm = 140.1686, GNorm = 0.3097
Meta loss on this task batch = 2.6894e-01, Meta loss averaged over last 500 steps = 2.7407e-01, PNorm = 140.1733, GNorm = 0.3446
Meta loss on this task batch = 2.5914e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 140.1778, GNorm = 0.2792
Meta loss on this task batch = 2.9745e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 140.1807, GNorm = 0.3168
Meta loss on this task batch = 3.0109e-01, Meta loss averaged over last 500 steps = 2.7425e-01, PNorm = 140.1832, GNorm = 0.3067
Meta loss on this task batch = 2.7608e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 140.1861, GNorm = 0.3105
Meta loss on this task batch = 2.5816e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 140.1887, GNorm = 0.2875
Meta loss on this task batch = 3.1623e-01, Meta loss averaged over last 500 steps = 2.7450e-01, PNorm = 140.1926, GNorm = 0.2980
Meta loss on this task batch = 2.8072e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 140.1966, GNorm = 0.3038
Meta loss on this task batch = 2.8189e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 140.1995, GNorm = 0.3787
Meta loss on this task batch = 2.7003e-01, Meta loss averaged over last 500 steps = 2.7452e-01, PNorm = 140.2037, GNorm = 0.2770
Meta loss on this task batch = 2.7273e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 140.2090, GNorm = 0.2850
Meta loss on this task batch = 2.9335e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 140.2127, GNorm = 0.3401
Meta loss on this task batch = 2.7784e-01, Meta loss averaged over last 500 steps = 2.7473e-01, PNorm = 140.2168, GNorm = 0.3112
Meta loss on this task batch = 3.1806e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 140.2213, GNorm = 0.3224
Meta loss on this task batch = 1.8893e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 140.2251, GNorm = 0.2416
Took 159.5932207107544 seconds to complete one epoch of meta training
Took 168.4336531162262 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454869
Epoch 724
Meta loss on this task batch = 2.9971e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 140.2285, GNorm = 0.3687
Meta loss on this task batch = 2.9726e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 140.2289, GNorm = 0.3089
Meta loss on this task batch = 3.3981e-01, Meta loss averaged over last 500 steps = 2.7493e-01, PNorm = 140.2279, GNorm = 0.3042
Meta loss on this task batch = 2.7585e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 140.2255, GNorm = 0.3249
Meta loss on this task batch = 2.3332e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 140.2249, GNorm = 0.2870
Meta loss on this task batch = 2.4570e-01, Meta loss averaged over last 500 steps = 2.7459e-01, PNorm = 140.2241, GNorm = 0.2821
Meta loss on this task batch = 2.4266e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 140.2257, GNorm = 0.3147
Meta loss on this task batch = 2.9192e-01, Meta loss averaged over last 500 steps = 2.7458e-01, PNorm = 140.2280, GNorm = 0.2707
Meta loss on this task batch = 3.1174e-01, Meta loss averaged over last 500 steps = 2.7460e-01, PNorm = 140.2306, GNorm = 0.3168
Meta loss on this task batch = 2.4042e-01, Meta loss averaged over last 500 steps = 2.7445e-01, PNorm = 140.2339, GNorm = 0.2814
Meta loss on this task batch = 2.2333e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 140.2377, GNorm = 0.2675
Meta loss on this task batch = 2.6121e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 140.2435, GNorm = 0.2986
Meta loss on this task batch = 2.7171e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 140.2489, GNorm = 0.2751
Meta loss on this task batch = 2.8532e-01, Meta loss averaged over last 500 steps = 2.7429e-01, PNorm = 140.2534, GNorm = 0.2720
Meta loss on this task batch = 2.7748e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 140.2597, GNorm = 0.2928
Meta loss on this task batch = 3.2639e-01, Meta loss averaged over last 500 steps = 2.7413e-01, PNorm = 140.2620, GNorm = 0.3884
Meta loss on this task batch = 2.7713e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 140.2593, GNorm = 0.3003
Meta loss on this task batch = 2.1826e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 140.2582, GNorm = 0.2363
Meta loss on this task batch = 3.3658e-01, Meta loss averaged over last 500 steps = 2.7406e-01, PNorm = 140.2568, GNorm = 0.3564
Took 145.66299676895142 seconds to complete one epoch of meta training
Took 154.22125458717346 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469323
Epoch 725
Meta loss on this task batch = 2.8761e-01, Meta loss averaged over last 500 steps = 2.7407e-01, PNorm = 140.2574, GNorm = 0.2950
Meta loss on this task batch = 3.1852e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 140.2555, GNorm = 0.3402
Meta loss on this task batch = 3.6414e-01, Meta loss averaged over last 500 steps = 2.7453e-01, PNorm = 140.2514, GNorm = 0.4174
Meta loss on this task batch = 2.6195e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 140.2478, GNorm = 0.2619
Meta loss on this task batch = 3.0796e-01, Meta loss averaged over last 500 steps = 2.7454e-01, PNorm = 140.2438, GNorm = 0.2790
Meta loss on this task batch = 3.2410e-01, Meta loss averaged over last 500 steps = 2.7469e-01, PNorm = 140.2415, GNorm = 0.2761
Meta loss on this task batch = 2.5616e-01, Meta loss averaged over last 500 steps = 2.7466e-01, PNorm = 140.2410, GNorm = 0.2275
Meta loss on this task batch = 2.3844e-01, Meta loss averaged over last 500 steps = 2.7471e-01, PNorm = 140.2436, GNorm = 0.2356
Meta loss on this task batch = 3.0756e-01, Meta loss averaged over last 500 steps = 2.7486e-01, PNorm = 140.2473, GNorm = 0.2471
Meta loss on this task batch = 2.4877e-01, Meta loss averaged over last 500 steps = 2.7470e-01, PNorm = 140.2537, GNorm = 0.2662
Meta loss on this task batch = 2.7671e-01, Meta loss averaged over last 500 steps = 2.7474e-01, PNorm = 140.2617, GNorm = 0.2948
Meta loss on this task batch = 2.5829e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 140.2698, GNorm = 0.2763
Meta loss on this task batch = 2.5898e-01, Meta loss averaged over last 500 steps = 2.7467e-01, PNorm = 140.2801, GNorm = 0.2851
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 2.7463e-01, PNorm = 140.2917, GNorm = 0.2529
Meta loss on this task batch = 2.4738e-01, Meta loss averaged over last 500 steps = 2.7455e-01, PNorm = 140.3025, GNorm = 0.2698
Meta loss on this task batch = 2.8033e-01, Meta loss averaged over last 500 steps = 2.7456e-01, PNorm = 140.3118, GNorm = 0.2638
Meta loss on this task batch = 2.0833e-01, Meta loss averaged over last 500 steps = 2.7446e-01, PNorm = 140.3202, GNorm = 0.2917
Meta loss on this task batch = 2.0964e-01, Meta loss averaged over last 500 steps = 2.7440e-01, PNorm = 140.3269, GNorm = 0.3378
Meta loss on this task batch = 2.6207e-01, Meta loss averaged over last 500 steps = 2.7447e-01, PNorm = 140.3326, GNorm = 0.3527
Took 137.57112979888916 seconds to complete one epoch of meta training
Took 146.16847276687622 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476202
Epoch 726
Meta loss on this task batch = 2.6352e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 140.3376, GNorm = 0.2529
Meta loss on this task batch = 2.4192e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 140.3434, GNorm = 0.2834
Meta loss on this task batch = 3.6709e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 140.3454, GNorm = 0.3357
Meta loss on this task batch = 2.8369e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 140.3465, GNorm = 0.2936
Meta loss on this task batch = 2.8671e-01, Meta loss averaged over last 500 steps = 2.7444e-01, PNorm = 140.3477, GNorm = 0.3111
Meta loss on this task batch = 2.3971e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 140.3483, GNorm = 0.2741
Meta loss on this task batch = 2.6318e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 140.3502, GNorm = 0.2583
Meta loss on this task batch = 2.2154e-01, Meta loss averaged over last 500 steps = 2.7429e-01, PNorm = 140.3507, GNorm = 0.2964
Meta loss on this task batch = 2.6155e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 140.3521, GNorm = 0.3042
Meta loss on this task batch = 2.8658e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 140.3524, GNorm = 0.3260
Meta loss on this task batch = 2.7248e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 140.3549, GNorm = 0.3072
Meta loss on this task batch = 2.9674e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 140.3575, GNorm = 0.3004
Meta loss on this task batch = 3.2903e-01, Meta loss averaged over last 500 steps = 2.7451e-01, PNorm = 140.3593, GNorm = 0.3208
Meta loss on this task batch = 2.4368e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 140.3635, GNorm = 0.3063
Meta loss on this task batch = 2.6907e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 140.3688, GNorm = 0.2999
Meta loss on this task batch = 2.4125e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 140.3745, GNorm = 0.2689
Meta loss on this task batch = 2.4933e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 140.3809, GNorm = 0.2609
Meta loss on this task batch = 3.0265e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 140.3878, GNorm = 0.3258
Meta loss on this task batch = 2.5472e-01, Meta loss averaged over last 500 steps = 2.7412e-01, PNorm = 140.3934, GNorm = 0.3717
Took 171.52253484725952 seconds to complete one epoch of meta training
Took 180.310800075531 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474490
Epoch 727
Meta loss on this task batch = 2.7420e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 140.3994, GNorm = 0.2587
Meta loss on this task batch = 2.8365e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 140.4051, GNorm = 0.2917
Meta loss on this task batch = 3.2841e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 140.4096, GNorm = 0.3028
Meta loss on this task batch = 3.0604e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 140.4144, GNorm = 0.2823
Meta loss on this task batch = 2.6301e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 140.4195, GNorm = 0.2770
Meta loss on this task batch = 2.1966e-01, Meta loss averaged over last 500 steps = 2.7402e-01, PNorm = 140.4227, GNorm = 0.2726
Meta loss on this task batch = 2.8659e-01, Meta loss averaged over last 500 steps = 2.7401e-01, PNorm = 140.4241, GNorm = 0.3174
Meta loss on this task batch = 3.0157e-01, Meta loss averaged over last 500 steps = 2.7401e-01, PNorm = 140.4229, GNorm = 0.2703
Meta loss on this task batch = 2.8518e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 140.4203, GNorm = 0.3084
Meta loss on this task batch = 2.9435e-01, Meta loss averaged over last 500 steps = 2.7404e-01, PNorm = 140.4161, GNorm = 0.2879
Meta loss on this task batch = 2.4466e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 140.4125, GNorm = 0.2711
Meta loss on this task batch = 2.2250e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 140.4106, GNorm = 0.2795
Meta loss on this task batch = 3.1880e-01, Meta loss averaged over last 500 steps = 2.7400e-01, PNorm = 140.4080, GNorm = 0.2955
Meta loss on this task batch = 2.3667e-01, Meta loss averaged over last 500 steps = 2.7402e-01, PNorm = 140.4076, GNorm = 0.2617
Meta loss on this task batch = 2.1888e-01, Meta loss averaged over last 500 steps = 2.7393e-01, PNorm = 140.4094, GNorm = 0.2425
Meta loss on this task batch = 2.4930e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 140.4114, GNorm = 0.2656
Meta loss on this task batch = 2.3994e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 140.4147, GNorm = 0.2899
Meta loss on this task batch = 2.6454e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 140.4182, GNorm = 0.2845
Meta loss on this task batch = 3.1247e-01, Meta loss averaged over last 500 steps = 2.7405e-01, PNorm = 140.4210, GNorm = 0.4439
Took 132.08668184280396 seconds to complete one epoch of meta training
Took 140.35889840126038 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465688
Epoch 728
Meta loss on this task batch = 2.3749e-01, Meta loss averaged over last 500 steps = 2.7402e-01, PNorm = 140.4252, GNorm = 0.2763
Meta loss on this task batch = 2.9520e-01, Meta loss averaged over last 500 steps = 2.7412e-01, PNorm = 140.4257, GNorm = 0.2988
Meta loss on this task batch = 2.9347e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 140.4261, GNorm = 0.3022
Meta loss on this task batch = 2.6635e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 140.4265, GNorm = 0.2662
Meta loss on this task batch = 2.8805e-01, Meta loss averaged over last 500 steps = 2.7420e-01, PNorm = 140.4256, GNorm = 0.3310
Meta loss on this task batch = 3.3620e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 140.4236, GNorm = 0.3862
Meta loss on this task batch = 3.3265e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 140.4205, GNorm = 0.3201
Meta loss on this task batch = 2.5637e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 140.4180, GNorm = 0.2999
Meta loss on this task batch = 2.7289e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 140.4147, GNorm = 0.3098
Meta loss on this task batch = 2.4789e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 140.4148, GNorm = 0.2910
Meta loss on this task batch = 3.2460e-01, Meta loss averaged over last 500 steps = 2.7419e-01, PNorm = 140.4161, GNorm = 0.3589
Meta loss on this task batch = 3.0485e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 140.4178, GNorm = 0.2791
Meta loss on this task batch = 2.7844e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 140.4216, GNorm = 0.3031
Meta loss on this task batch = 2.7599e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 140.4288, GNorm = 0.3438
Meta loss on this task batch = 2.5530e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 140.4387, GNorm = 0.3256
Meta loss on this task batch = 2.6231e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 140.4496, GNorm = 0.2716
Meta loss on this task batch = 2.7113e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 140.4611, GNorm = 0.2720
Meta loss on this task batch = 2.7050e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 140.4728, GNorm = 0.2584
Meta loss on this task batch = 2.3206e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 140.4819, GNorm = 0.3746
Took 129.6033775806427 seconds to complete one epoch of meta training
Took 137.03244972229004 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510256
Epoch 729
Meta loss on this task batch = 2.7678e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 140.4877, GNorm = 0.3341
Meta loss on this task batch = 2.6803e-01, Meta loss averaged over last 500 steps = 2.7396e-01, PNorm = 140.4931, GNorm = 0.3593
Meta loss on this task batch = 2.3998e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 140.4983, GNorm = 0.2793
Meta loss on this task batch = 2.4784e-01, Meta loss averaged over last 500 steps = 2.7400e-01, PNorm = 140.5035, GNorm = 0.3459
Meta loss on this task batch = 2.5561e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 140.5071, GNorm = 0.2867
Meta loss on this task batch = 3.1299e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 140.5099, GNorm = 0.3402
Meta loss on this task batch = 2.5171e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 140.5139, GNorm = 0.2603
Meta loss on this task batch = 2.8567e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 140.5182, GNorm = 0.2704
Meta loss on this task batch = 2.9603e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 140.5229, GNorm = 0.2630
Meta loss on this task batch = 3.2104e-01, Meta loss averaged over last 500 steps = 2.7399e-01, PNorm = 140.5280, GNorm = 0.2763
Meta loss on this task batch = 2.9895e-01, Meta loss averaged over last 500 steps = 2.7404e-01, PNorm = 140.5329, GNorm = 0.3084
Meta loss on this task batch = 2.7267e-01, Meta loss averaged over last 500 steps = 2.7407e-01, PNorm = 140.5385, GNorm = 0.2978
Meta loss on this task batch = 2.3801e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 140.5443, GNorm = 0.2957
Meta loss on this task batch = 2.6120e-01, Meta loss averaged over last 500 steps = 2.7392e-01, PNorm = 140.5503, GNorm = 0.2640
Meta loss on this task batch = 2.4055e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 140.5575, GNorm = 0.2696
Meta loss on this task batch = 2.7188e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 140.5642, GNorm = 0.3299
Meta loss on this task batch = 2.5599e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 140.5708, GNorm = 0.2751
Meta loss on this task batch = 3.0463e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 140.5772, GNorm = 0.3649
Meta loss on this task batch = 2.6568e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 140.5831, GNorm = 0.3276
Took 132.15140461921692 seconds to complete one epoch of meta training
Took 141.0553936958313 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482016
Epoch 730
Meta loss on this task batch = 2.8759e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 140.5887, GNorm = 0.3363
Meta loss on this task batch = 2.9808e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 140.5943, GNorm = 0.3173
Meta loss on this task batch = 3.0372e-01, Meta loss averaged over last 500 steps = 2.7401e-01, PNorm = 140.5982, GNorm = 0.3497
Meta loss on this task batch = 3.4908e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 140.5995, GNorm = 0.3504
Meta loss on this task batch = 2.7581e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 140.6011, GNorm = 0.3146
Meta loss on this task batch = 2.8716e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 140.5999, GNorm = 0.3570
Meta loss on this task batch = 2.9018e-01, Meta loss averaged over last 500 steps = 2.7425e-01, PNorm = 140.5998, GNorm = 0.2829
Meta loss on this task batch = 2.4052e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 140.6015, GNorm = 0.2644
Meta loss on this task batch = 2.5607e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 140.6048, GNorm = 0.2629
Meta loss on this task batch = 2.7062e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 140.6093, GNorm = 0.2512
Meta loss on this task batch = 2.2241e-01, Meta loss averaged over last 500 steps = 2.7409e-01, PNorm = 140.6145, GNorm = 0.2802
Meta loss on this task batch = 2.6380e-01, Meta loss averaged over last 500 steps = 2.7405e-01, PNorm = 140.6208, GNorm = 0.2611
Meta loss on this task batch = 3.3931e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 140.6254, GNorm = 0.2624
Meta loss on this task batch = 2.4264e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 140.6290, GNorm = 0.2549
Meta loss on this task batch = 2.5947e-01, Meta loss averaged over last 500 steps = 2.7406e-01, PNorm = 140.6320, GNorm = 0.3144
Meta loss on this task batch = 2.6590e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 140.6350, GNorm = 0.2523
Meta loss on this task batch = 2.1352e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 140.6362, GNorm = 0.2536
Meta loss on this task batch = 2.7716e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 140.6374, GNorm = 0.3036
Meta loss on this task batch = 2.8627e-01, Meta loss averaged over last 500 steps = 2.7383e-01, PNorm = 140.6397, GNorm = 0.3787
Took 130.29480814933777 seconds to complete one epoch of meta training
Took 138.63581204414368 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476509
Epoch 731
Meta loss on this task batch = 3.0577e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 140.6421, GNorm = 0.3250
Meta loss on this task batch = 2.3487e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 140.6459, GNorm = 0.2781
Meta loss on this task batch = 2.9916e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 140.6504, GNorm = 0.2935
Meta loss on this task batch = 2.8405e-01, Meta loss averaged over last 500 steps = 2.7383e-01, PNorm = 140.6520, GNorm = 0.3067
Meta loss on this task batch = 2.8800e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 140.6500, GNorm = 0.3372
Meta loss on this task batch = 2.2905e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 140.6490, GNorm = 0.2857
Meta loss on this task batch = 2.6267e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 140.6501, GNorm = 0.3357
Meta loss on this task batch = 2.8234e-01, Meta loss averaged over last 500 steps = 2.7382e-01, PNorm = 140.6514, GNorm = 0.2904
Meta loss on this task batch = 2.4705e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 140.6540, GNorm = 0.2952
Meta loss on this task batch = 2.9444e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 140.6599, GNorm = 0.3284
Meta loss on this task batch = 2.8987e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 140.6646, GNorm = 0.3013
Meta loss on this task batch = 2.7743e-01, Meta loss averaged over last 500 steps = 2.7391e-01, PNorm = 140.6692, GNorm = 0.3137
Meta loss on this task batch = 3.3198e-01, Meta loss averaged over last 500 steps = 2.7404e-01, PNorm = 140.6729, GNorm = 0.3178
Meta loss on this task batch = 2.3844e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 140.6766, GNorm = 0.2694
Meta loss on this task batch = 2.5630e-01, Meta loss averaged over last 500 steps = 2.7399e-01, PNorm = 140.6796, GNorm = 0.2727
Meta loss on this task batch = 2.6745e-01, Meta loss averaged over last 500 steps = 2.7394e-01, PNorm = 140.6818, GNorm = 0.2835
Meta loss on this task batch = 3.0252e-01, Meta loss averaged over last 500 steps = 2.7406e-01, PNorm = 140.6845, GNorm = 0.3293
Meta loss on this task batch = 2.4873e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 140.6886, GNorm = 0.2541
Meta loss on this task batch = 2.7902e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 140.6929, GNorm = 0.3147
Took 126.89337539672852 seconds to complete one epoch of meta training
Took 135.74445104599 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471147
Epoch 732
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 140.6972, GNorm = 0.2692
Meta loss on this task batch = 3.1229e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 140.7015, GNorm = 0.3068
Meta loss on this task batch = 2.4708e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 140.7067, GNorm = 0.2724
Meta loss on this task batch = 2.5715e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 140.7126, GNorm = 0.3079
Meta loss on this task batch = 2.3869e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 140.7200, GNorm = 0.2690
Meta loss on this task batch = 2.9675e-01, Meta loss averaged over last 500 steps = 2.7381e-01, PNorm = 140.7258, GNorm = 0.2963
Meta loss on this task batch = 2.7404e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 140.7327, GNorm = 0.3191
Meta loss on this task batch = 2.6265e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 140.7405, GNorm = 0.2869
Meta loss on this task batch = 2.6695e-01, Meta loss averaged over last 500 steps = 2.7381e-01, PNorm = 140.7489, GNorm = 0.2996
Meta loss on this task batch = 2.2952e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 140.7574, GNorm = 0.2384
Meta loss on this task batch = 2.6479e-01, Meta loss averaged over last 500 steps = 2.7381e-01, PNorm = 140.7682, GNorm = 0.3244
Meta loss on this task batch = 2.7077e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 140.7790, GNorm = 0.3190
Meta loss on this task batch = 3.1432e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 140.7907, GNorm = 0.3108
Meta loss on this task batch = 2.8736e-01, Meta loss averaged over last 500 steps = 2.7394e-01, PNorm = 140.8008, GNorm = 0.3129
Meta loss on this task batch = 2.8818e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 140.8095, GNorm = 0.3202
Meta loss on this task batch = 2.8710e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 140.8161, GNorm = 0.3138
Meta loss on this task batch = 2.4797e-01, Meta loss averaged over last 500 steps = 2.7388e-01, PNorm = 140.8205, GNorm = 0.2899
Meta loss on this task batch = 2.4014e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 140.8223, GNorm = 0.3321
Meta loss on this task batch = 2.7760e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 140.8254, GNorm = 0.3091
Took 129.79422974586487 seconds to complete one epoch of meta training
Took 138.14162588119507 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464266
Epoch 733
Meta loss on this task batch = 2.7301e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 140.8275, GNorm = 0.2964
Meta loss on this task batch = 2.5371e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 140.8287, GNorm = 0.2796
Meta loss on this task batch = 2.8057e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 140.8306, GNorm = 0.2947
Meta loss on this task batch = 2.5303e-01, Meta loss averaged over last 500 steps = 2.7366e-01, PNorm = 140.8296, GNorm = 0.2946
Meta loss on this task batch = 2.4320e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 140.8286, GNorm = 0.3165
Meta loss on this task batch = 2.8469e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 140.8293, GNorm = 0.3560
Meta loss on this task batch = 2.6153e-01, Meta loss averaged over last 500 steps = 2.7363e-01, PNorm = 140.8300, GNorm = 0.2523
Meta loss on this task batch = 2.7264e-01, Meta loss averaged over last 500 steps = 2.7363e-01, PNorm = 140.8318, GNorm = 0.2814
Meta loss on this task batch = 2.9003e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 140.8327, GNorm = 0.2853
Meta loss on this task batch = 2.8742e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 140.8318, GNorm = 0.2793
Meta loss on this task batch = 2.7885e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 140.8304, GNorm = 0.2724
Meta loss on this task batch = 2.5688e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 140.8307, GNorm = 0.3366
Meta loss on this task batch = 2.6458e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 140.8313, GNorm = 0.2559
Meta loss on this task batch = 2.8048e-01, Meta loss averaged over last 500 steps = 2.7357e-01, PNorm = 140.8311, GNorm = 0.2747
Meta loss on this task batch = 2.7418e-01, Meta loss averaged over last 500 steps = 2.7361e-01, PNorm = 140.8317, GNorm = 0.2807
Meta loss on this task batch = 3.1474e-01, Meta loss averaged over last 500 steps = 2.7366e-01, PNorm = 140.8328, GNorm = 0.3821
Meta loss on this task batch = 2.7367e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 140.8355, GNorm = 0.2799
Meta loss on this task batch = 2.8865e-01, Meta loss averaged over last 500 steps = 2.7357e-01, PNorm = 140.8387, GNorm = 0.3295
Meta loss on this task batch = 2.3388e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 140.8406, GNorm = 0.2884
Took 131.4527280330658 seconds to complete one epoch of meta training
Took 139.77968859672546 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469627
Epoch 734
Meta loss on this task batch = 2.8780e-01, Meta loss averaged over last 500 steps = 2.7361e-01, PNorm = 140.8410, GNorm = 0.3064
Meta loss on this task batch = 2.1730e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 140.8425, GNorm = 0.2474
Meta loss on this task batch = 2.7419e-01, Meta loss averaged over last 500 steps = 2.7351e-01, PNorm = 140.8450, GNorm = 0.2940
Meta loss on this task batch = 3.4719e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 140.8456, GNorm = 0.3548
Meta loss on this task batch = 2.1739e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 140.8489, GNorm = 0.2675
Meta loss on this task batch = 2.4223e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 140.8532, GNorm = 0.2946
Meta loss on this task batch = 2.8125e-01, Meta loss averaged over last 500 steps = 2.7363e-01, PNorm = 140.8578, GNorm = 0.3159
Meta loss on this task batch = 2.9004e-01, Meta loss averaged over last 500 steps = 2.7369e-01, PNorm = 140.8635, GNorm = 0.2803
Meta loss on this task batch = 2.3327e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 140.8693, GNorm = 0.2712
Meta loss on this task batch = 2.5701e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 140.8754, GNorm = 0.2634
Meta loss on this task batch = 1.9466e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 140.8829, GNorm = 0.2500
Meta loss on this task batch = 3.2507e-01, Meta loss averaged over last 500 steps = 2.7357e-01, PNorm = 140.8879, GNorm = 0.3106
Meta loss on this task batch = 2.2874e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 140.8917, GNorm = 0.2540
Meta loss on this task batch = 3.1912e-01, Meta loss averaged over last 500 steps = 2.7357e-01, PNorm = 140.8938, GNorm = 0.3435
Meta loss on this task batch = 2.9849e-01, Meta loss averaged over last 500 steps = 2.7360e-01, PNorm = 140.8932, GNorm = 0.3635
Meta loss on this task batch = 3.1355e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 140.8892, GNorm = 0.3786
Meta loss on this task batch = 2.5415e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 140.8849, GNorm = 0.2958
Meta loss on this task batch = 2.5807e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 140.8801, GNorm = 0.3112
Meta loss on this task batch = 3.0967e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 140.8760, GNorm = 0.3534
Took 129.53774237632751 seconds to complete one epoch of meta training
Took 137.66102027893066 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462604
Epoch 735
Meta loss on this task batch = 2.7667e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 140.8733, GNorm = 0.2820
Meta loss on this task batch = 2.4309e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 140.8711, GNorm = 0.2840
Meta loss on this task batch = 2.6738e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 140.8702, GNorm = 0.2646
Meta loss on this task batch = 2.1391e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 140.8715, GNorm = 0.2630
Meta loss on this task batch = 2.2212e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 140.8755, GNorm = 0.2950
Meta loss on this task batch = 2.7494e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 140.8808, GNorm = 0.2765
Meta loss on this task batch = 2.4827e-01, Meta loss averaged over last 500 steps = 2.7330e-01, PNorm = 140.8883, GNorm = 0.2675
Meta loss on this task batch = 3.2121e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 140.8965, GNorm = 0.3090
Meta loss on this task batch = 2.6482e-01, Meta loss averaged over last 500 steps = 2.7327e-01, PNorm = 140.9053, GNorm = 0.3911
Meta loss on this task batch = 2.8734e-01, Meta loss averaged over last 500 steps = 2.7346e-01, PNorm = 140.9129, GNorm = 0.2983
Meta loss on this task batch = 2.8965e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 140.9189, GNorm = 0.3384
Meta loss on this task batch = 2.8147e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 140.9251, GNorm = 0.2878
Meta loss on this task batch = 2.5476e-01, Meta loss averaged over last 500 steps = 2.7350e-01, PNorm = 140.9316, GNorm = 0.2631
Meta loss on this task batch = 2.4130e-01, Meta loss averaged over last 500 steps = 2.7336e-01, PNorm = 140.9361, GNorm = 0.3128
Meta loss on this task batch = 2.4228e-01, Meta loss averaged over last 500 steps = 2.7327e-01, PNorm = 140.9399, GNorm = 0.2964
Meta loss on this task batch = 2.6346e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 140.9417, GNorm = 0.3081
Meta loss on this task batch = 2.9650e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 140.9412, GNorm = 0.3468
Meta loss on this task batch = 2.7447e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 140.9402, GNorm = 0.2720
Meta loss on this task batch = 3.3444e-01, Meta loss averaged over last 500 steps = 2.7330e-01, PNorm = 140.9377, GNorm = 0.3771
Took 139.43554091453552 seconds to complete one epoch of meta training
Took 147.49671339988708 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494757
Epoch 736
Meta loss on this task batch = 2.3948e-01, Meta loss averaged over last 500 steps = 2.7313e-01, PNorm = 140.9351, GNorm = 0.2992
Meta loss on this task batch = 3.2563e-01, Meta loss averaged over last 500 steps = 2.7319e-01, PNorm = 140.9343, GNorm = 0.3465
Meta loss on this task batch = 2.1196e-01, Meta loss averaged over last 500 steps = 2.7298e-01, PNorm = 140.9351, GNorm = 0.2881
Meta loss on this task batch = 3.1030e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 140.9358, GNorm = 0.3023
Meta loss on this task batch = 3.2261e-01, Meta loss averaged over last 500 steps = 2.7314e-01, PNorm = 140.9372, GNorm = 0.2798
Meta loss on this task batch = 2.6490e-01, Meta loss averaged over last 500 steps = 2.7317e-01, PNorm = 140.9386, GNorm = 0.2944
Meta loss on this task batch = 2.5212e-01, Meta loss averaged over last 500 steps = 2.7311e-01, PNorm = 140.9438, GNorm = 0.3024
Meta loss on this task batch = 2.7103e-01, Meta loss averaged over last 500 steps = 2.7316e-01, PNorm = 140.9504, GNorm = 0.2796
Meta loss on this task batch = 2.8350e-01, Meta loss averaged over last 500 steps = 2.7313e-01, PNorm = 140.9602, GNorm = 0.3040
Meta loss on this task batch = 2.8857e-01, Meta loss averaged over last 500 steps = 2.7310e-01, PNorm = 140.9704, GNorm = 0.2714
Meta loss on this task batch = 2.6611e-01, Meta loss averaged over last 500 steps = 2.7299e-01, PNorm = 140.9805, GNorm = 0.2668
Meta loss on this task batch = 2.4744e-01, Meta loss averaged over last 500 steps = 2.7284e-01, PNorm = 140.9903, GNorm = 0.2567
Meta loss on this task batch = 2.5139e-01, Meta loss averaged over last 500 steps = 2.7280e-01, PNorm = 140.9984, GNorm = 0.2741
Meta loss on this task batch = 2.9890e-01, Meta loss averaged over last 500 steps = 2.7289e-01, PNorm = 141.0079, GNorm = 0.2694
Meta loss on this task batch = 2.4383e-01, Meta loss averaged over last 500 steps = 2.7284e-01, PNorm = 141.0170, GNorm = 0.2787
Meta loss on this task batch = 3.0912e-01, Meta loss averaged over last 500 steps = 2.7290e-01, PNorm = 141.0220, GNorm = 0.3358
Meta loss on this task batch = 2.6046e-01, Meta loss averaged over last 500 steps = 2.7285e-01, PNorm = 141.0262, GNorm = 0.2895
Meta loss on this task batch = 2.0088e-01, Meta loss averaged over last 500 steps = 2.7269e-01, PNorm = 141.0304, GNorm = 0.3076
Meta loss on this task batch = 2.9527e-01, Meta loss averaged over last 500 steps = 2.7268e-01, PNorm = 141.0338, GNorm = 0.3871
Took 169.59792566299438 seconds to complete one epoch of meta training
Took 177.69500970840454 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493174
Epoch 737
Meta loss on this task batch = 3.0369e-01, Meta loss averaged over last 500 steps = 2.7259e-01, PNorm = 141.0357, GNorm = 0.2989
Meta loss on this task batch = 3.0930e-01, Meta loss averaged over last 500 steps = 2.7264e-01, PNorm = 141.0351, GNorm = 0.3435
Meta loss on this task batch = 2.2888e-01, Meta loss averaged over last 500 steps = 2.7252e-01, PNorm = 141.0331, GNorm = 0.2750
Meta loss on this task batch = 2.6152e-01, Meta loss averaged over last 500 steps = 2.7255e-01, PNorm = 141.0300, GNorm = 0.2731
Meta loss on this task batch = 2.9208e-01, Meta loss averaged over last 500 steps = 2.7265e-01, PNorm = 141.0282, GNorm = 0.3107
Meta loss on this task batch = 2.6170e-01, Meta loss averaged over last 500 steps = 2.7269e-01, PNorm = 141.0251, GNorm = 0.3112
Meta loss on this task batch = 2.3862e-01, Meta loss averaged over last 500 steps = 2.7251e-01, PNorm = 141.0250, GNorm = 0.2415
Meta loss on this task batch = 3.6587e-01, Meta loss averaged over last 500 steps = 2.7278e-01, PNorm = 141.0246, GNorm = 0.3492
Meta loss on this task batch = 3.2163e-01, Meta loss averaged over last 500 steps = 2.7281e-01, PNorm = 141.0275, GNorm = 0.3215
Meta loss on this task batch = 2.4044e-01, Meta loss averaged over last 500 steps = 2.7259e-01, PNorm = 141.0336, GNorm = 0.2656
Meta loss on this task batch = 2.6034e-01, Meta loss averaged over last 500 steps = 2.7251e-01, PNorm = 141.0404, GNorm = 0.2798
Meta loss on this task batch = 2.5996e-01, Meta loss averaged over last 500 steps = 2.7237e-01, PNorm = 141.0469, GNorm = 0.2869
Meta loss on this task batch = 2.4221e-01, Meta loss averaged over last 500 steps = 2.7229e-01, PNorm = 141.0523, GNorm = 0.2667
Meta loss on this task batch = 2.4828e-01, Meta loss averaged over last 500 steps = 2.7228e-01, PNorm = 141.0576, GNorm = 0.2589
Meta loss on this task batch = 3.5892e-01, Meta loss averaged over last 500 steps = 2.7257e-01, PNorm = 141.0612, GNorm = 0.3026
Meta loss on this task batch = 1.9736e-01, Meta loss averaged over last 500 steps = 2.7244e-01, PNorm = 141.0678, GNorm = 0.2529
Meta loss on this task batch = 2.3927e-01, Meta loss averaged over last 500 steps = 2.7243e-01, PNorm = 141.0737, GNorm = 0.2675
Meta loss on this task batch = 2.4865e-01, Meta loss averaged over last 500 steps = 2.7246e-01, PNorm = 141.0794, GNorm = 0.2849
Meta loss on this task batch = 1.8661e-01, Meta loss averaged over last 500 steps = 2.7217e-01, PNorm = 141.0862, GNorm = 0.2830
Took 169.07621216773987 seconds to complete one epoch of meta training
Took 177.42862343788147 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484468
Epoch 738
Meta loss on this task batch = 1.9739e-01, Meta loss averaged over last 500 steps = 2.7214e-01, PNorm = 141.0935, GNorm = 0.2577
Meta loss on this task batch = 3.0658e-01, Meta loss averaged over last 500 steps = 2.7219e-01, PNorm = 141.0981, GNorm = 0.3156
Meta loss on this task batch = 2.9928e-01, Meta loss averaged over last 500 steps = 2.7219e-01, PNorm = 141.0980, GNorm = 0.3490
Meta loss on this task batch = 3.1636e-01, Meta loss averaged over last 500 steps = 2.7226e-01, PNorm = 141.0986, GNorm = 0.3120
Meta loss on this task batch = 2.5942e-01, Meta loss averaged over last 500 steps = 2.7228e-01, PNorm = 141.0987, GNorm = 0.3164
Meta loss on this task batch = 2.5972e-01, Meta loss averaged over last 500 steps = 2.7228e-01, PNorm = 141.0972, GNorm = 0.3154
Meta loss on this task batch = 2.5188e-01, Meta loss averaged over last 500 steps = 2.7221e-01, PNorm = 141.0953, GNorm = 0.2969
Meta loss on this task batch = 2.8060e-01, Meta loss averaged over last 500 steps = 2.7217e-01, PNorm = 141.0926, GNorm = 0.3657
Meta loss on this task batch = 2.9213e-01, Meta loss averaged over last 500 steps = 2.7219e-01, PNorm = 141.0909, GNorm = 0.3181
Meta loss on this task batch = 2.8674e-01, Meta loss averaged over last 500 steps = 2.7226e-01, PNorm = 141.0900, GNorm = 0.3103
Meta loss on this task batch = 2.8654e-01, Meta loss averaged over last 500 steps = 2.7229e-01, PNorm = 141.0915, GNorm = 0.2857
Meta loss on this task batch = 3.2266e-01, Meta loss averaged over last 500 steps = 2.7227e-01, PNorm = 141.0945, GNorm = 0.3223
Meta loss on this task batch = 2.6447e-01, Meta loss averaged over last 500 steps = 2.7227e-01, PNorm = 141.1020, GNorm = 0.3148
Meta loss on this task batch = 2.6274e-01, Meta loss averaged over last 500 steps = 2.7224e-01, PNorm = 141.1100, GNorm = 0.2729
Meta loss on this task batch = 2.3282e-01, Meta loss averaged over last 500 steps = 2.7216e-01, PNorm = 141.1202, GNorm = 0.2781
Meta loss on this task batch = 3.1720e-01, Meta loss averaged over last 500 steps = 2.7222e-01, PNorm = 141.1293, GNorm = 0.3076
Meta loss on this task batch = 2.5476e-01, Meta loss averaged over last 500 steps = 2.7224e-01, PNorm = 141.1391, GNorm = 0.3097
Meta loss on this task batch = 3.0105e-01, Meta loss averaged over last 500 steps = 2.7225e-01, PNorm = 141.1477, GNorm = 0.3438
Meta loss on this task batch = 2.4150e-01, Meta loss averaged over last 500 steps = 2.7223e-01, PNorm = 141.1541, GNorm = 0.3205
Took 149.62952733039856 seconds to complete one epoch of meta training
Took 157.89859795570374 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483791
Epoch 739
Meta loss on this task batch = 2.2477e-01, Meta loss averaged over last 500 steps = 2.7209e-01, PNorm = 141.1598, GNorm = 0.2611
Meta loss on this task batch = 2.3449e-01, Meta loss averaged over last 500 steps = 2.7211e-01, PNorm = 141.1645, GNorm = 0.2780
Meta loss on this task batch = 2.9912e-01, Meta loss averaged over last 500 steps = 2.7221e-01, PNorm = 141.1683, GNorm = 0.2936
Meta loss on this task batch = 2.5517e-01, Meta loss averaged over last 500 steps = 2.7204e-01, PNorm = 141.1708, GNorm = 0.3330
Meta loss on this task batch = 2.8328e-01, Meta loss averaged over last 500 steps = 2.7213e-01, PNorm = 141.1727, GNorm = 0.3027
Meta loss on this task batch = 2.7388e-01, Meta loss averaged over last 500 steps = 2.7221e-01, PNorm = 141.1731, GNorm = 0.2741
Meta loss on this task batch = 3.0876e-01, Meta loss averaged over last 500 steps = 2.7238e-01, PNorm = 141.1729, GNorm = 0.3299
Meta loss on this task batch = 2.9115e-01, Meta loss averaged over last 500 steps = 2.7245e-01, PNorm = 141.1727, GNorm = 0.2772
Meta loss on this task batch = 3.2996e-01, Meta loss averaged over last 500 steps = 2.7260e-01, PNorm = 141.1726, GNorm = 0.3006
Meta loss on this task batch = 2.7419e-01, Meta loss averaged over last 500 steps = 2.7259e-01, PNorm = 141.1734, GNorm = 0.3228
Meta loss on this task batch = 2.4033e-01, Meta loss averaged over last 500 steps = 2.7257e-01, PNorm = 141.1742, GNorm = 0.2892
Meta loss on this task batch = 3.3002e-01, Meta loss averaged over last 500 steps = 2.7279e-01, PNorm = 141.1782, GNorm = 0.3666
Meta loss on this task batch = 2.7337e-01, Meta loss averaged over last 500 steps = 2.7281e-01, PNorm = 141.1842, GNorm = 0.2756
Meta loss on this task batch = 2.7665e-01, Meta loss averaged over last 500 steps = 2.7283e-01, PNorm = 141.1913, GNorm = 0.3215
Meta loss on this task batch = 2.3690e-01, Meta loss averaged over last 500 steps = 2.7271e-01, PNorm = 141.1995, GNorm = 0.2341
Meta loss on this task batch = 3.1314e-01, Meta loss averaged over last 500 steps = 2.7282e-01, PNorm = 141.2055, GNorm = 0.3172
Meta loss on this task batch = 2.8010e-01, Meta loss averaged over last 500 steps = 2.7279e-01, PNorm = 141.2134, GNorm = 0.2908
Meta loss on this task batch = 2.8454e-01, Meta loss averaged over last 500 steps = 2.7288e-01, PNorm = 141.2197, GNorm = 0.3155
Meta loss on this task batch = 2.1948e-01, Meta loss averaged over last 500 steps = 2.7275e-01, PNorm = 141.2244, GNorm = 0.2917
Took 151.47167038917542 seconds to complete one epoch of meta training
Took 159.50812244415283 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467898
Epoch 740
Meta loss on this task batch = 2.3488e-01, Meta loss averaged over last 500 steps = 2.7275e-01, PNorm = 141.2296, GNorm = 0.2552
Meta loss on this task batch = 2.4278e-01, Meta loss averaged over last 500 steps = 2.7278e-01, PNorm = 141.2342, GNorm = 0.2918
Meta loss on this task batch = 2.8541e-01, Meta loss averaged over last 500 steps = 2.7281e-01, PNorm = 141.2376, GNorm = 0.2987
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.7284e-01, PNorm = 141.2414, GNorm = 0.3017
Meta loss on this task batch = 2.9445e-01, Meta loss averaged over last 500 steps = 2.7286e-01, PNorm = 141.2443, GNorm = 0.3848
Meta loss on this task batch = 2.4331e-01, Meta loss averaged over last 500 steps = 2.7274e-01, PNorm = 141.2476, GNorm = 0.3092
Meta loss on this task batch = 3.0046e-01, Meta loss averaged over last 500 steps = 2.7276e-01, PNorm = 141.2488, GNorm = 0.3329
Meta loss on this task batch = 2.2075e-01, Meta loss averaged over last 500 steps = 2.7271e-01, PNorm = 141.2506, GNorm = 0.2768
Meta loss on this task batch = 2.6388e-01, Meta loss averaged over last 500 steps = 2.7270e-01, PNorm = 141.2524, GNorm = 0.2770
Meta loss on this task batch = 2.9584e-01, Meta loss averaged over last 500 steps = 2.7280e-01, PNorm = 141.2554, GNorm = 0.3135
Meta loss on this task batch = 3.0088e-01, Meta loss averaged over last 500 steps = 2.7281e-01, PNorm = 141.2579, GNorm = 0.2714
Meta loss on this task batch = 2.6539e-01, Meta loss averaged over last 500 steps = 2.7281e-01, PNorm = 141.2614, GNorm = 0.2894
Meta loss on this task batch = 2.3531e-01, Meta loss averaged over last 500 steps = 2.7259e-01, PNorm = 141.2667, GNorm = 0.2326
Meta loss on this task batch = 2.8959e-01, Meta loss averaged over last 500 steps = 2.7259e-01, PNorm = 141.2729, GNorm = 0.3048
Meta loss on this task batch = 2.5299e-01, Meta loss averaged over last 500 steps = 2.7252e-01, PNorm = 141.2785, GNorm = 0.3172
Meta loss on this task batch = 3.0171e-01, Meta loss averaged over last 500 steps = 2.7259e-01, PNorm = 141.2854, GNorm = 0.3214
Meta loss on this task batch = 2.3731e-01, Meta loss averaged over last 500 steps = 2.7255e-01, PNorm = 141.2917, GNorm = 0.2545
Meta loss on this task batch = 2.7065e-01, Meta loss averaged over last 500 steps = 2.7262e-01, PNorm = 141.3001, GNorm = 0.3001
Meta loss on this task batch = 3.1826e-01, Meta loss averaged over last 500 steps = 2.7274e-01, PNorm = 141.3072, GNorm = 0.3689
Took 172.27504014968872 seconds to complete one epoch of meta training
Took 180.80489945411682 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481379
Epoch 741
Meta loss on this task batch = 2.7332e-01, Meta loss averaged over last 500 steps = 2.7279e-01, PNorm = 141.3131, GNorm = 0.2519
Meta loss on this task batch = 2.6055e-01, Meta loss averaged over last 500 steps = 2.7278e-01, PNorm = 141.3178, GNorm = 0.2747
Meta loss on this task batch = 3.3395e-01, Meta loss averaged over last 500 steps = 2.7282e-01, PNorm = 141.3200, GNorm = 0.3300
Meta loss on this task batch = 3.6029e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 141.3063, GNorm = 1.0000
Meta loss on this task batch = 2.8107e-01, Meta loss averaged over last 500 steps = 2.7306e-01, PNorm = 141.2954, GNorm = 0.2592
Meta loss on this task batch = 2.6493e-01, Meta loss averaged over last 500 steps = 2.7301e-01, PNorm = 141.2871, GNorm = 0.2715
Meta loss on this task batch = 2.8731e-01, Meta loss averaged over last 500 steps = 2.7310e-01, PNorm = 141.2828, GNorm = 0.3191
Meta loss on this task batch = 2.6562e-01, Meta loss averaged over last 500 steps = 2.7302e-01, PNorm = 141.2800, GNorm = 0.2656
Meta loss on this task batch = 2.4447e-01, Meta loss averaged over last 500 steps = 2.7295e-01, PNorm = 141.2805, GNorm = 0.2967
Meta loss on this task batch = 2.5016e-01, Meta loss averaged over last 500 steps = 2.7287e-01, PNorm = 141.2828, GNorm = 0.2786
Meta loss on this task batch = 2.5969e-01, Meta loss averaged over last 500 steps = 2.7292e-01, PNorm = 141.2865, GNorm = 0.3965
Meta loss on this task batch = 2.9597e-01, Meta loss averaged over last 500 steps = 2.7295e-01, PNorm = 141.2915, GNorm = 0.2748
Meta loss on this task batch = 2.4390e-01, Meta loss averaged over last 500 steps = 2.7299e-01, PNorm = 141.2955, GNorm = 0.2748
Meta loss on this task batch = 2.3788e-01, Meta loss averaged over last 500 steps = 2.7292e-01, PNorm = 141.2999, GNorm = 0.2566
Meta loss on this task batch = 2.8688e-01, Meta loss averaged over last 500 steps = 2.7307e-01, PNorm = 141.3021, GNorm = 0.3162
Meta loss on this task batch = 2.7165e-01, Meta loss averaged over last 500 steps = 2.7304e-01, PNorm = 141.3046, GNorm = 0.3158
Meta loss on this task batch = 2.6964e-01, Meta loss averaged over last 500 steps = 2.7295e-01, PNorm = 141.3062, GNorm = 0.2944
Meta loss on this task batch = 2.7263e-01, Meta loss averaged over last 500 steps = 2.7300e-01, PNorm = 141.3079, GNorm = 0.3004
Meta loss on this task batch = 2.8520e-01, Meta loss averaged over last 500 steps = 2.7304e-01, PNorm = 141.3094, GNorm = 0.3267
Took 142.89217352867126 seconds to complete one epoch of meta training
Took 151.04613852500916 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486241
Epoch 742
Meta loss on this task batch = 2.3495e-01, Meta loss averaged over last 500 steps = 2.7293e-01, PNorm = 141.3103, GNorm = 0.2847
Meta loss on this task batch = 2.9148e-01, Meta loss averaged over last 500 steps = 2.7301e-01, PNorm = 141.3097, GNorm = 0.3119
Meta loss on this task batch = 2.5907e-01, Meta loss averaged over last 500 steps = 2.7292e-01, PNorm = 141.3103, GNorm = 0.3128
Meta loss on this task batch = 2.4516e-01, Meta loss averaged over last 500 steps = 2.7284e-01, PNorm = 141.3115, GNorm = 0.2529
Meta loss on this task batch = 2.7882e-01, Meta loss averaged over last 500 steps = 2.7285e-01, PNorm = 141.3137, GNorm = 0.2856
Meta loss on this task batch = 3.0490e-01, Meta loss averaged over last 500 steps = 2.7279e-01, PNorm = 141.3144, GNorm = 0.3283
Meta loss on this task batch = 2.6011e-01, Meta loss averaged over last 500 steps = 2.7281e-01, PNorm = 141.3165, GNorm = 0.2765
Meta loss on this task batch = 2.6889e-01, Meta loss averaged over last 500 steps = 2.7282e-01, PNorm = 141.3199, GNorm = 0.3018
Meta loss on this task batch = 2.8357e-01, Meta loss averaged over last 500 steps = 2.7284e-01, PNorm = 141.3226, GNorm = 0.3129
Meta loss on this task batch = 2.9573e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 141.3276, GNorm = 0.3268
Meta loss on this task batch = 2.3152e-01, Meta loss averaged over last 500 steps = 2.7298e-01, PNorm = 141.3334, GNorm = 0.2725
Meta loss on this task batch = 2.7906e-01, Meta loss averaged over last 500 steps = 2.7299e-01, PNorm = 141.3393, GNorm = 0.2894
Meta loss on this task batch = 2.1000e-01, Meta loss averaged over last 500 steps = 2.7282e-01, PNorm = 141.3457, GNorm = 0.2433
Meta loss on this task batch = 2.8827e-01, Meta loss averaged over last 500 steps = 2.7293e-01, PNorm = 141.3527, GNorm = 0.3111
Meta loss on this task batch = 2.7081e-01, Meta loss averaged over last 500 steps = 2.7300e-01, PNorm = 141.3593, GNorm = 0.3106
Meta loss on this task batch = 3.4066e-01, Meta loss averaged over last 500 steps = 2.7308e-01, PNorm = 141.3661, GNorm = 0.2965
Meta loss on this task batch = 2.6756e-01, Meta loss averaged over last 500 steps = 2.7308e-01, PNorm = 141.3732, GNorm = 0.2735
Meta loss on this task batch = 2.4438e-01, Meta loss averaged over last 500 steps = 2.7308e-01, PNorm = 141.3809, GNorm = 0.2636
Meta loss on this task batch = 3.1609e-01, Meta loss averaged over last 500 steps = 2.7325e-01, PNorm = 141.3857, GNorm = 0.4141
Took 153.54776453971863 seconds to complete one epoch of meta training
Took 161.9315779209137 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491469
Epoch 743
Meta loss on this task batch = 2.4013e-01, Meta loss averaged over last 500 steps = 2.7318e-01, PNorm = 141.3897, GNorm = 0.2655
Meta loss on this task batch = 2.7103e-01, Meta loss averaged over last 500 steps = 2.7307e-01, PNorm = 141.3933, GNorm = 0.3135
Meta loss on this task batch = 3.3364e-01, Meta loss averaged over last 500 steps = 2.7317e-01, PNorm = 141.3968, GNorm = 0.3253
Meta loss on this task batch = 3.0296e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 141.4022, GNorm = 0.2821
Meta loss on this task batch = 2.3783e-01, Meta loss averaged over last 500 steps = 2.7310e-01, PNorm = 141.4075, GNorm = 0.2793
Meta loss on this task batch = 2.5639e-01, Meta loss averaged over last 500 steps = 2.7302e-01, PNorm = 141.4118, GNorm = 0.2804
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 2.7309e-01, PNorm = 141.4159, GNorm = 0.3279
Meta loss on this task batch = 2.5898e-01, Meta loss averaged over last 500 steps = 2.7308e-01, PNorm = 141.4198, GNorm = 0.2793
Meta loss on this task batch = 2.6353e-01, Meta loss averaged over last 500 steps = 2.7311e-01, PNorm = 141.4230, GNorm = 0.2903
Meta loss on this task batch = 2.6709e-01, Meta loss averaged over last 500 steps = 2.7308e-01, PNorm = 141.4255, GNorm = 0.2516
Meta loss on this task batch = 2.3295e-01, Meta loss averaged over last 500 steps = 2.7307e-01, PNorm = 141.4294, GNorm = 0.2716
Meta loss on this task batch = 2.6145e-01, Meta loss averaged over last 500 steps = 2.7309e-01, PNorm = 141.4307, GNorm = 0.3062
Meta loss on this task batch = 2.8244e-01, Meta loss averaged over last 500 steps = 2.7312e-01, PNorm = 141.4308, GNorm = 0.2989
Meta loss on this task batch = 2.7474e-01, Meta loss averaged over last 500 steps = 2.7301e-01, PNorm = 141.4292, GNorm = 0.3394
Meta loss on this task batch = 2.6486e-01, Meta loss averaged over last 500 steps = 2.7285e-01, PNorm = 141.4267, GNorm = 0.2638
Meta loss on this task batch = 2.8753e-01, Meta loss averaged over last 500 steps = 2.7300e-01, PNorm = 141.4235, GNorm = 0.3638
Meta loss on this task batch = 2.6221e-01, Meta loss averaged over last 500 steps = 2.7300e-01, PNorm = 141.4212, GNorm = 0.2686
Meta loss on this task batch = 2.8449e-01, Meta loss averaged over last 500 steps = 2.7296e-01, PNorm = 141.4199, GNorm = 0.3332
Meta loss on this task batch = 2.3829e-01, Meta loss averaged over last 500 steps = 2.7283e-01, PNorm = 141.4211, GNorm = 0.3226
Took 153.08466696739197 seconds to complete one epoch of meta training
Took 161.39170670509338 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471916
Epoch 744
Meta loss on this task batch = 3.1452e-01, Meta loss averaged over last 500 steps = 2.7290e-01, PNorm = 141.4230, GNorm = 0.3091
Meta loss on this task batch = 3.0134e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 141.4262, GNorm = 0.2987
Meta loss on this task batch = 2.6746e-01, Meta loss averaged over last 500 steps = 2.7293e-01, PNorm = 141.4307, GNorm = 0.2539
Meta loss on this task batch = 2.5818e-01, Meta loss averaged over last 500 steps = 2.7283e-01, PNorm = 141.4350, GNorm = 0.2914
Meta loss on this task batch = 2.9217e-01, Meta loss averaged over last 500 steps = 2.7286e-01, PNorm = 141.4385, GNorm = 0.2749
Meta loss on this task batch = 2.8055e-01, Meta loss averaged over last 500 steps = 2.7290e-01, PNorm = 141.4434, GNorm = 0.3020
Meta loss on this task batch = 2.5998e-01, Meta loss averaged over last 500 steps = 2.7291e-01, PNorm = 141.4495, GNorm = 0.2889
Meta loss on this task batch = 2.8915e-01, Meta loss averaged over last 500 steps = 2.7288e-01, PNorm = 141.4547, GNorm = 0.3651
Meta loss on this task batch = 2.5784e-01, Meta loss averaged over last 500 steps = 2.7287e-01, PNorm = 141.4587, GNorm = 0.2790
Meta loss on this task batch = 2.3369e-01, Meta loss averaged over last 500 steps = 2.7278e-01, PNorm = 141.4636, GNorm = 0.2554
Meta loss on this task batch = 2.5608e-01, Meta loss averaged over last 500 steps = 2.7276e-01, PNorm = 141.4699, GNorm = 0.3275
Meta loss on this task batch = 2.4764e-01, Meta loss averaged over last 500 steps = 2.7262e-01, PNorm = 141.4763, GNorm = 0.2336
Meta loss on this task batch = 2.5554e-01, Meta loss averaged over last 500 steps = 2.7264e-01, PNorm = 141.4817, GNorm = 0.2566
Meta loss on this task batch = 2.9895e-01, Meta loss averaged over last 500 steps = 2.7275e-01, PNorm = 141.4861, GNorm = 0.3130
Meta loss on this task batch = 3.0426e-01, Meta loss averaged over last 500 steps = 2.7283e-01, PNorm = 141.4877, GNorm = 0.3034
Meta loss on this task batch = 3.2221e-01, Meta loss averaged over last 500 steps = 2.7282e-01, PNorm = 141.4894, GNorm = 0.2894
Meta loss on this task batch = 2.9807e-01, Meta loss averaged over last 500 steps = 2.7290e-01, PNorm = 141.4916, GNorm = 0.3518
Meta loss on this task batch = 2.3469e-01, Meta loss averaged over last 500 steps = 2.7289e-01, PNorm = 141.4932, GNorm = 0.2927
Meta loss on this task batch = 2.4810e-01, Meta loss averaged over last 500 steps = 2.7289e-01, PNorm = 141.4944, GNorm = 0.3604
Took 144.48485851287842 seconds to complete one epoch of meta training
Took 152.7477090358734 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493609
Epoch 745
Meta loss on this task batch = 2.2787e-01, Meta loss averaged over last 500 steps = 2.7274e-01, PNorm = 141.4962, GNorm = 0.2512
Meta loss on this task batch = 2.5972e-01, Meta loss averaged over last 500 steps = 2.7269e-01, PNorm = 141.4986, GNorm = 0.2478
Meta loss on this task batch = 2.4784e-01, Meta loss averaged over last 500 steps = 2.7263e-01, PNorm = 141.5024, GNorm = 0.2777
Meta loss on this task batch = 2.6310e-01, Meta loss averaged over last 500 steps = 2.7266e-01, PNorm = 141.5055, GNorm = 0.2964
Meta loss on this task batch = 2.2471e-01, Meta loss averaged over last 500 steps = 2.7258e-01, PNorm = 141.5092, GNorm = 0.2775
Meta loss on this task batch = 2.4483e-01, Meta loss averaged over last 500 steps = 2.7251e-01, PNorm = 141.5150, GNorm = 0.2593
Meta loss on this task batch = 3.2344e-01, Meta loss averaged over last 500 steps = 2.7264e-01, PNorm = 141.5217, GNorm = 0.3792
Meta loss on this task batch = 2.9957e-01, Meta loss averaged over last 500 steps = 2.7277e-01, PNorm = 141.5293, GNorm = 0.2987
Meta loss on this task batch = 2.5495e-01, Meta loss averaged over last 500 steps = 2.7277e-01, PNorm = 141.5373, GNorm = 0.2591
Meta loss on this task batch = 2.8971e-01, Meta loss averaged over last 500 steps = 2.7273e-01, PNorm = 141.5450, GNorm = 0.2828
Meta loss on this task batch = 2.3464e-01, Meta loss averaged over last 500 steps = 2.7261e-01, PNorm = 141.5547, GNorm = 0.3281
Meta loss on this task batch = 3.0232e-01, Meta loss averaged over last 500 steps = 2.7262e-01, PNorm = 141.5630, GNorm = 0.3025
Meta loss on this task batch = 2.6585e-01, Meta loss averaged over last 500 steps = 2.7272e-01, PNorm = 141.5698, GNorm = 0.2778
Meta loss on this task batch = 2.5307e-01, Meta loss averaged over last 500 steps = 2.7267e-01, PNorm = 141.5745, GNorm = 0.3058
Meta loss on this task batch = 2.6637e-01, Meta loss averaged over last 500 steps = 2.7270e-01, PNorm = 141.5795, GNorm = 0.2839
Meta loss on this task batch = 2.0306e-01, Meta loss averaged over last 500 steps = 2.7262e-01, PNorm = 141.5833, GNorm = 0.2533
Meta loss on this task batch = 2.4027e-01, Meta loss averaged over last 500 steps = 2.7255e-01, PNorm = 141.5869, GNorm = 0.2613
Meta loss on this task batch = 3.3801e-01, Meta loss averaged over last 500 steps = 2.7269e-01, PNorm = 141.5895, GNorm = 0.3114
Meta loss on this task batch = 2.9909e-01, Meta loss averaged over last 500 steps = 2.7269e-01, PNorm = 141.5905, GNorm = 0.3938
Took 130.24045729637146 seconds to complete one epoch of meta training
Took 138.27344489097595 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503678
Epoch 746
Meta loss on this task batch = 2.7771e-01, Meta loss averaged over last 500 steps = 2.7281e-01, PNorm = 141.5928, GNorm = 0.2816
Meta loss on this task batch = 2.7932e-01, Meta loss averaged over last 500 steps = 2.7280e-01, PNorm = 141.5958, GNorm = 0.2963
Meta loss on this task batch = 2.7815e-01, Meta loss averaged over last 500 steps = 2.7280e-01, PNorm = 141.5985, GNorm = 0.3501
Meta loss on this task batch = 2.6596e-01, Meta loss averaged over last 500 steps = 2.7276e-01, PNorm = 141.6026, GNorm = 0.3173
Meta loss on this task batch = 3.0193e-01, Meta loss averaged over last 500 steps = 2.7271e-01, PNorm = 141.6082, GNorm = 0.2957
Meta loss on this task batch = 2.4259e-01, Meta loss averaged over last 500 steps = 2.7262e-01, PNorm = 141.6130, GNorm = 0.2969
Meta loss on this task batch = 2.8173e-01, Meta loss averaged over last 500 steps = 2.7266e-01, PNorm = 141.6156, GNorm = 0.3347
Meta loss on this task batch = 2.5534e-01, Meta loss averaged over last 500 steps = 2.7261e-01, PNorm = 141.6192, GNorm = 0.3039
Meta loss on this task batch = 2.1642e-01, Meta loss averaged over last 500 steps = 2.7255e-01, PNorm = 141.6222, GNorm = 0.2541
Meta loss on this task batch = 2.7285e-01, Meta loss averaged over last 500 steps = 2.7260e-01, PNorm = 141.6246, GNorm = 0.3011
Meta loss on this task batch = 2.6067e-01, Meta loss averaged over last 500 steps = 2.7266e-01, PNorm = 141.6250, GNorm = 0.2752
Meta loss on this task batch = 2.4484e-01, Meta loss averaged over last 500 steps = 2.7263e-01, PNorm = 141.6264, GNorm = 0.2509
Meta loss on this task batch = 3.2848e-01, Meta loss averaged over last 500 steps = 2.7278e-01, PNorm = 141.6273, GNorm = 0.3540
Meta loss on this task batch = 3.1310e-01, Meta loss averaged over last 500 steps = 2.7286e-01, PNorm = 141.6292, GNorm = 0.3384
Meta loss on this task batch = 2.7560e-01, Meta loss averaged over last 500 steps = 2.7270e-01, PNorm = 141.6328, GNorm = 0.2954
Meta loss on this task batch = 2.4272e-01, Meta loss averaged over last 500 steps = 2.7268e-01, PNorm = 141.6375, GNorm = 0.2585
Meta loss on this task batch = 2.6697e-01, Meta loss averaged over last 500 steps = 2.7267e-01, PNorm = 141.6426, GNorm = 0.2789
Meta loss on this task batch = 2.6086e-01, Meta loss averaged over last 500 steps = 2.7269e-01, PNorm = 141.6480, GNorm = 0.2990
Meta loss on this task batch = 2.4682e-01, Meta loss averaged over last 500 steps = 2.7267e-01, PNorm = 141.6539, GNorm = 0.3939
Took 129.37213444709778 seconds to complete one epoch of meta training
Took 137.69081330299377 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493118
Epoch 747
Meta loss on this task batch = 2.4316e-01, Meta loss averaged over last 500 steps = 2.7255e-01, PNorm = 141.6610, GNorm = 0.3386
Meta loss on this task batch = 2.7625e-01, Meta loss averaged over last 500 steps = 2.7263e-01, PNorm = 141.6686, GNorm = 0.2879
Meta loss on this task batch = 3.1192e-01, Meta loss averaged over last 500 steps = 2.7268e-01, PNorm = 141.6777, GNorm = 0.3220
Meta loss on this task batch = 2.6314e-01, Meta loss averaged over last 500 steps = 2.7254e-01, PNorm = 141.6869, GNorm = 0.2716
Meta loss on this task batch = 2.6944e-01, Meta loss averaged over last 500 steps = 2.7243e-01, PNorm = 141.6940, GNorm = 0.4245
Meta loss on this task batch = 2.4878e-01, Meta loss averaged over last 500 steps = 2.7239e-01, PNorm = 141.6993, GNorm = 0.2817
Meta loss on this task batch = 2.5457e-01, Meta loss averaged over last 500 steps = 2.7233e-01, PNorm = 141.7040, GNorm = 0.2797
Meta loss on this task batch = 2.7087e-01, Meta loss averaged over last 500 steps = 2.7235e-01, PNorm = 141.7060, GNorm = 0.3112
Meta loss on this task batch = 2.5696e-01, Meta loss averaged over last 500 steps = 2.7223e-01, PNorm = 141.7087, GNorm = 0.2738
Meta loss on this task batch = 2.7249e-01, Meta loss averaged over last 500 steps = 2.7221e-01, PNorm = 141.7111, GNorm = 0.2744
Meta loss on this task batch = 2.7107e-01, Meta loss averaged over last 500 steps = 2.7230e-01, PNorm = 141.7137, GNorm = 0.2692
Meta loss on this task batch = 2.9988e-01, Meta loss averaged over last 500 steps = 2.7232e-01, PNorm = 141.7174, GNorm = 0.3536
Meta loss on this task batch = 2.5120e-01, Meta loss averaged over last 500 steps = 2.7235e-01, PNorm = 141.7207, GNorm = 0.3050
Meta loss on this task batch = 2.7438e-01, Meta loss averaged over last 500 steps = 2.7246e-01, PNorm = 141.7250, GNorm = 0.2954
Meta loss on this task batch = 2.6448e-01, Meta loss averaged over last 500 steps = 2.7257e-01, PNorm = 141.7286, GNorm = 0.2827
Meta loss on this task batch = 2.1482e-01, Meta loss averaged over last 500 steps = 2.7244e-01, PNorm = 141.7318, GNorm = 0.2803
Meta loss on this task batch = 2.6990e-01, Meta loss averaged over last 500 steps = 2.7240e-01, PNorm = 141.7325, GNorm = 0.3427
Meta loss on this task batch = 2.8300e-01, Meta loss averaged over last 500 steps = 2.7246e-01, PNorm = 141.7341, GNorm = 0.2901
Meta loss on this task batch = 1.9028e-01, Meta loss averaged over last 500 steps = 2.7221e-01, PNorm = 141.7361, GNorm = 0.3184
Took 129.42030239105225 seconds to complete one epoch of meta training
Took 137.8105320930481 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.516252
Epoch 748
Meta loss on this task batch = 2.7730e-01, Meta loss averaged over last 500 steps = 2.7209e-01, PNorm = 141.7389, GNorm = 0.3280
Meta loss on this task batch = 2.7985e-01, Meta loss averaged over last 500 steps = 2.7210e-01, PNorm = 141.7423, GNorm = 0.3160
Meta loss on this task batch = 2.7586e-01, Meta loss averaged over last 500 steps = 2.7213e-01, PNorm = 141.7453, GNorm = 0.3083
Meta loss on this task batch = 2.7210e-01, Meta loss averaged over last 500 steps = 2.7202e-01, PNorm = 141.7477, GNorm = 0.3028
Meta loss on this task batch = 2.5183e-01, Meta loss averaged over last 500 steps = 2.7193e-01, PNorm = 141.7515, GNorm = 0.3301
Meta loss on this task batch = 2.4644e-01, Meta loss averaged over last 500 steps = 2.7192e-01, PNorm = 141.7555, GNorm = 0.2998
Meta loss on this task batch = 2.0028e-01, Meta loss averaged over last 500 steps = 2.7188e-01, PNorm = 141.7605, GNorm = 0.2530
Meta loss on this task batch = 2.3555e-01, Meta loss averaged over last 500 steps = 2.7190e-01, PNorm = 141.7666, GNorm = 0.2731
Meta loss on this task batch = 2.2001e-01, Meta loss averaged over last 500 steps = 2.7173e-01, PNorm = 141.7726, GNorm = 0.2936
Meta loss on this task batch = 2.8810e-01, Meta loss averaged over last 500 steps = 2.7177e-01, PNorm = 141.7800, GNorm = 0.3084
Meta loss on this task batch = 2.5687e-01, Meta loss averaged over last 500 steps = 2.7167e-01, PNorm = 141.7872, GNorm = 0.2976
Meta loss on this task batch = 2.6495e-01, Meta loss averaged over last 500 steps = 2.7173e-01, PNorm = 141.7935, GNorm = 0.2830
Meta loss on this task batch = 2.9168e-01, Meta loss averaged over last 500 steps = 2.7177e-01, PNorm = 141.8006, GNorm = 0.3242
Meta loss on this task batch = 3.2115e-01, Meta loss averaged over last 500 steps = 2.7178e-01, PNorm = 141.8068, GNorm = 0.3599
Meta loss on this task batch = 3.1959e-01, Meta loss averaged over last 500 steps = 2.7184e-01, PNorm = 141.8121, GNorm = 0.3185
Meta loss on this task batch = 2.4055e-01, Meta loss averaged over last 500 steps = 2.7174e-01, PNorm = 141.8169, GNorm = 0.2995
Meta loss on this task batch = 3.4967e-01, Meta loss averaged over last 500 steps = 2.7181e-01, PNorm = 141.8208, GNorm = 0.3409
Meta loss on this task batch = 2.3548e-01, Meta loss averaged over last 500 steps = 2.7173e-01, PNorm = 141.8270, GNorm = 0.2613
Meta loss on this task batch = 3.1098e-01, Meta loss averaged over last 500 steps = 2.7182e-01, PNorm = 141.8313, GNorm = 0.3641
Took 128.42978930473328 seconds to complete one epoch of meta training
Took 136.670476436615 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503241
Epoch 749
Meta loss on this task batch = 2.8777e-01, Meta loss averaged over last 500 steps = 2.7188e-01, PNorm = 141.8339, GNorm = 0.2827
Meta loss on this task batch = 2.3613e-01, Meta loss averaged over last 500 steps = 2.7178e-01, PNorm = 141.8349, GNorm = 0.2762
Meta loss on this task batch = 2.9950e-01, Meta loss averaged over last 500 steps = 2.7184e-01, PNorm = 141.8371, GNorm = 0.3070
Meta loss on this task batch = 3.0563e-01, Meta loss averaged over last 500 steps = 2.7189e-01, PNorm = 141.8403, GNorm = 0.3288
Meta loss on this task batch = 2.1334e-01, Meta loss averaged over last 500 steps = 2.7187e-01, PNorm = 141.8443, GNorm = 0.2536
Meta loss on this task batch = 2.6688e-01, Meta loss averaged over last 500 steps = 2.7194e-01, PNorm = 141.8490, GNorm = 0.2862
Meta loss on this task batch = 2.7113e-01, Meta loss averaged over last 500 steps = 2.7195e-01, PNorm = 141.8545, GNorm = 0.2825
Meta loss on this task batch = 2.2563e-01, Meta loss averaged over last 500 steps = 2.7172e-01, PNorm = 141.8593, GNorm = 0.2889
Meta loss on this task batch = 3.3350e-01, Meta loss averaged over last 500 steps = 2.7172e-01, PNorm = 141.8622, GNorm = 0.2924
Meta loss on this task batch = 3.0354e-01, Meta loss averaged over last 500 steps = 2.7178e-01, PNorm = 141.8647, GNorm = 0.3023
Meta loss on this task batch = 2.4177e-01, Meta loss averaged over last 500 steps = 2.7172e-01, PNorm = 141.8688, GNorm = 0.2900
Meta loss on this task batch = 2.7165e-01, Meta loss averaged over last 500 steps = 2.7175e-01, PNorm = 141.8715, GNorm = 0.3450
Meta loss on this task batch = 3.0990e-01, Meta loss averaged over last 500 steps = 2.7177e-01, PNorm = 141.8737, GNorm = 0.3049
Meta loss on this task batch = 2.1345e-01, Meta loss averaged over last 500 steps = 2.7160e-01, PNorm = 141.8780, GNorm = 0.2512
Meta loss on this task batch = 2.6027e-01, Meta loss averaged over last 500 steps = 2.7157e-01, PNorm = 141.8832, GNorm = 0.2830
Meta loss on this task batch = 3.0048e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 141.8882, GNorm = 0.2788
Meta loss on this task batch = 1.9855e-01, Meta loss averaged over last 500 steps = 2.7141e-01, PNorm = 141.8942, GNorm = 0.2505
Meta loss on this task batch = 3.3720e-01, Meta loss averaged over last 500 steps = 2.7153e-01, PNorm = 141.9001, GNorm = 0.3701
Meta loss on this task batch = 2.9648e-01, Meta loss averaged over last 500 steps = 2.7156e-01, PNorm = 141.9061, GNorm = 0.3519
Took 127.71502041816711 seconds to complete one epoch of meta training
Took 136.19472765922546 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463032
Epoch 750
Meta loss on this task batch = 2.2208e-01, Meta loss averaged over last 500 steps = 2.7146e-01, PNorm = 141.9112, GNorm = 0.2779
Meta loss on this task batch = 2.8583e-01, Meta loss averaged over last 500 steps = 2.7149e-01, PNorm = 141.9128, GNorm = 0.3166
Meta loss on this task batch = 2.8530e-01, Meta loss averaged over last 500 steps = 2.7147e-01, PNorm = 141.9167, GNorm = 0.3121
Meta loss on this task batch = 2.6151e-01, Meta loss averaged over last 500 steps = 2.7144e-01, PNorm = 141.9210, GNorm = 0.2871
Meta loss on this task batch = 2.8250e-01, Meta loss averaged over last 500 steps = 2.7137e-01, PNorm = 141.9237, GNorm = 0.3375
Meta loss on this task batch = 2.9588e-01, Meta loss averaged over last 500 steps = 2.7158e-01, PNorm = 141.9283, GNorm = 0.2732
Meta loss on this task batch = 3.0073e-01, Meta loss averaged over last 500 steps = 2.7158e-01, PNorm = 141.9332, GNorm = 0.2897
Meta loss on this task batch = 3.1208e-01, Meta loss averaged over last 500 steps = 2.7161e-01, PNorm = 141.9369, GNorm = 0.2706
Meta loss on this task batch = 2.6735e-01, Meta loss averaged over last 500 steps = 2.7147e-01, PNorm = 141.9410, GNorm = 0.2584
Meta loss on this task batch = 3.1210e-01, Meta loss averaged over last 500 steps = 2.7154e-01, PNorm = 141.9440, GNorm = 0.2915
Meta loss on this task batch = 2.7007e-01, Meta loss averaged over last 500 steps = 2.7161e-01, PNorm = 141.9468, GNorm = 0.2918
Meta loss on this task batch = 2.9211e-01, Meta loss averaged over last 500 steps = 2.7171e-01, PNorm = 141.9489, GNorm = 0.3027
Meta loss on this task batch = 2.4950e-01, Meta loss averaged over last 500 steps = 2.7172e-01, PNorm = 141.9511, GNorm = 0.2757
Meta loss on this task batch = 2.6555e-01, Meta loss averaged over last 500 steps = 2.7167e-01, PNorm = 141.9538, GNorm = 0.3121
Meta loss on this task batch = 2.9908e-01, Meta loss averaged over last 500 steps = 2.7164e-01, PNorm = 141.9583, GNorm = 0.2850
Meta loss on this task batch = 2.9661e-01, Meta loss averaged over last 500 steps = 2.7175e-01, PNorm = 141.9632, GNorm = 0.2972
Meta loss on this task batch = 2.2038e-01, Meta loss averaged over last 500 steps = 2.7175e-01, PNorm = 141.9681, GNorm = 0.2834
Meta loss on this task batch = 2.6469e-01, Meta loss averaged over last 500 steps = 2.7176e-01, PNorm = 141.9734, GNorm = 0.2677
Meta loss on this task batch = 2.1640e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 141.9784, GNorm = 0.3568
Took 127.40300631523132 seconds to complete one epoch of meta training
Took 135.3662121295929 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478248
Epoch 751
Meta loss on this task batch = 2.4373e-01, Meta loss averaged over last 500 steps = 2.7156e-01, PNorm = 141.9823, GNorm = 0.2876
Meta loss on this task batch = 3.0499e-01, Meta loss averaged over last 500 steps = 2.7162e-01, PNorm = 141.9868, GNorm = 0.2890
Meta loss on this task batch = 2.0767e-01, Meta loss averaged over last 500 steps = 2.7138e-01, PNorm = 141.9922, GNorm = 0.2866
Meta loss on this task batch = 2.9851e-01, Meta loss averaged over last 500 steps = 2.7142e-01, PNorm = 141.9969, GNorm = 0.3128
Meta loss on this task batch = 2.5620e-01, Meta loss averaged over last 500 steps = 2.7150e-01, PNorm = 142.0020, GNorm = 0.3083
Meta loss on this task batch = 3.1395e-01, Meta loss averaged over last 500 steps = 2.7145e-01, PNorm = 142.0061, GNorm = 0.3343
Meta loss on this task batch = 2.4196e-01, Meta loss averaged over last 500 steps = 2.7136e-01, PNorm = 142.0100, GNorm = 0.3047
Meta loss on this task batch = 2.6590e-01, Meta loss averaged over last 500 steps = 2.7126e-01, PNorm = 142.0121, GNorm = 0.3490
Meta loss on this task batch = 2.6436e-01, Meta loss averaged over last 500 steps = 2.7106e-01, PNorm = 142.0121, GNorm = 0.2904
Meta loss on this task batch = 2.6114e-01, Meta loss averaged over last 500 steps = 2.7106e-01, PNorm = 142.0124, GNorm = 0.2610
Meta loss on this task batch = 2.6968e-01, Meta loss averaged over last 500 steps = 2.7098e-01, PNorm = 142.0132, GNorm = 0.3173
Meta loss on this task batch = 3.3220e-01, Meta loss averaged over last 500 steps = 2.7100e-01, PNorm = 142.0157, GNorm = 0.3485
Meta loss on this task batch = 2.9297e-01, Meta loss averaged over last 500 steps = 2.7107e-01, PNorm = 142.0203, GNorm = 0.2948
Meta loss on this task batch = 2.7500e-01, Meta loss averaged over last 500 steps = 2.7114e-01, PNorm = 142.0265, GNorm = 0.3763
Meta loss on this task batch = 2.4937e-01, Meta loss averaged over last 500 steps = 2.7103e-01, PNorm = 142.0354, GNorm = 0.2676
Meta loss on this task batch = 2.7774e-01, Meta loss averaged over last 500 steps = 2.7108e-01, PNorm = 142.0437, GNorm = 0.2637
Meta loss on this task batch = 2.2914e-01, Meta loss averaged over last 500 steps = 2.7099e-01, PNorm = 142.0522, GNorm = 0.2744
Meta loss on this task batch = 3.2784e-01, Meta loss averaged over last 500 steps = 2.7113e-01, PNorm = 142.0593, GNorm = 0.3148
Meta loss on this task batch = 2.3552e-01, Meta loss averaged over last 500 steps = 2.7108e-01, PNorm = 142.0658, GNorm = 0.2912
Took 134.84664869308472 seconds to complete one epoch of meta training
Took 143.26819825172424 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499478
Epoch 752
Meta loss on this task batch = 1.9365e-01, Meta loss averaged over last 500 steps = 2.7095e-01, PNorm = 142.0716, GNorm = 0.2252
Meta loss on this task batch = 3.0391e-01, Meta loss averaged over last 500 steps = 2.7107e-01, PNorm = 142.0753, GNorm = 0.2952
Meta loss on this task batch = 2.7665e-01, Meta loss averaged over last 500 steps = 2.7106e-01, PNorm = 142.0782, GNorm = 0.2681
Meta loss on this task batch = 2.3801e-01, Meta loss averaged over last 500 steps = 2.7112e-01, PNorm = 142.0803, GNorm = 0.2610
Meta loss on this task batch = 2.5755e-01, Meta loss averaged over last 500 steps = 2.7121e-01, PNorm = 142.0812, GNorm = 0.3053
Meta loss on this task batch = 2.7224e-01, Meta loss averaged over last 500 steps = 2.7123e-01, PNorm = 142.0813, GNorm = 0.2897
Meta loss on this task batch = 2.9486e-01, Meta loss averaged over last 500 steps = 2.7130e-01, PNorm = 142.0802, GNorm = 0.3361
Meta loss on this task batch = 2.8483e-01, Meta loss averaged over last 500 steps = 2.7138e-01, PNorm = 142.0792, GNorm = 0.2898
Meta loss on this task batch = 2.7790e-01, Meta loss averaged over last 500 steps = 2.7120e-01, PNorm = 142.0791, GNorm = 0.3046
Meta loss on this task batch = 2.5418e-01, Meta loss averaged over last 500 steps = 2.7115e-01, PNorm = 142.0801, GNorm = 0.2719
Meta loss on this task batch = 2.4478e-01, Meta loss averaged over last 500 steps = 2.7106e-01, PNorm = 142.0811, GNorm = 0.2959
Meta loss on this task batch = 3.1251e-01, Meta loss averaged over last 500 steps = 2.7121e-01, PNorm = 142.0817, GNorm = 0.3650
Meta loss on this task batch = 2.5537e-01, Meta loss averaged over last 500 steps = 2.7119e-01, PNorm = 142.0841, GNorm = 0.2833
Meta loss on this task batch = 2.7479e-01, Meta loss averaged over last 500 steps = 2.7130e-01, PNorm = 142.0873, GNorm = 0.2796
Meta loss on this task batch = 2.4414e-01, Meta loss averaged over last 500 steps = 2.7126e-01, PNorm = 142.0911, GNorm = 0.2621
Meta loss on this task batch = 3.1403e-01, Meta loss averaged over last 500 steps = 2.7132e-01, PNorm = 142.0956, GNorm = 0.3134
Meta loss on this task batch = 2.8863e-01, Meta loss averaged over last 500 steps = 2.7135e-01, PNorm = 142.1002, GNorm = 0.2842
Meta loss on this task batch = 3.0028e-01, Meta loss averaged over last 500 steps = 2.7136e-01, PNorm = 142.1046, GNorm = 0.3628
Meta loss on this task batch = 2.3133e-01, Meta loss averaged over last 500 steps = 2.7116e-01, PNorm = 142.1097, GNorm = 0.2915
Took 127.11762475967407 seconds to complete one epoch of meta training
Took 135.34259748458862 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477175
Epoch 753
Meta loss on this task batch = 2.9547e-01, Meta loss averaged over last 500 steps = 2.7127e-01, PNorm = 142.1146, GNorm = 0.3074
Meta loss on this task batch = 2.5081e-01, Meta loss averaged over last 500 steps = 2.7123e-01, PNorm = 142.1207, GNorm = 0.2751
Meta loss on this task batch = 2.5361e-01, Meta loss averaged over last 500 steps = 2.7125e-01, PNorm = 142.1253, GNorm = 0.2949
Meta loss on this task batch = 3.3701e-01, Meta loss averaged over last 500 steps = 2.7143e-01, PNorm = 142.1287, GNorm = 0.3675
Meta loss on this task batch = 2.4639e-01, Meta loss averaged over last 500 steps = 2.7132e-01, PNorm = 142.1327, GNorm = 0.2857
Meta loss on this task batch = 2.7705e-01, Meta loss averaged over last 500 steps = 2.7136e-01, PNorm = 142.1359, GNorm = 0.2811
Meta loss on this task batch = 2.5529e-01, Meta loss averaged over last 500 steps = 2.7132e-01, PNorm = 142.1404, GNorm = 0.2547
Meta loss on this task batch = 2.5366e-01, Meta loss averaged over last 500 steps = 2.7126e-01, PNorm = 142.1467, GNorm = 0.2617
Meta loss on this task batch = 2.4827e-01, Meta loss averaged over last 500 steps = 2.7110e-01, PNorm = 142.1538, GNorm = 0.2553
Meta loss on this task batch = 2.3042e-01, Meta loss averaged over last 500 steps = 2.7095e-01, PNorm = 142.1594, GNorm = 0.2217
Meta loss on this task batch = 2.6282e-01, Meta loss averaged over last 500 steps = 2.7095e-01, PNorm = 142.1637, GNorm = 0.2587
Meta loss on this task batch = 2.8856e-01, Meta loss averaged over last 500 steps = 2.7109e-01, PNorm = 142.1681, GNorm = 0.2799
Meta loss on this task batch = 2.4032e-01, Meta loss averaged over last 500 steps = 2.7100e-01, PNorm = 142.1714, GNorm = 0.2753
Meta loss on this task batch = 2.7908e-01, Meta loss averaged over last 500 steps = 2.7095e-01, PNorm = 142.1745, GNorm = 0.2642
Meta loss on this task batch = 2.7161e-01, Meta loss averaged over last 500 steps = 2.7092e-01, PNorm = 142.1773, GNorm = 0.2971
Meta loss on this task batch = 2.7533e-01, Meta loss averaged over last 500 steps = 2.7089e-01, PNorm = 142.1796, GNorm = 0.3059
Meta loss on this task batch = 2.8904e-01, Meta loss averaged over last 500 steps = 2.7098e-01, PNorm = 142.1823, GNorm = 0.3835
Meta loss on this task batch = 2.2821e-01, Meta loss averaged over last 500 steps = 2.7099e-01, PNorm = 142.1860, GNorm = 0.2949
Meta loss on this task batch = 2.7909e-01, Meta loss averaged over last 500 steps = 2.7091e-01, PNorm = 142.1887, GNorm = 0.3751
Took 155.36183667182922 seconds to complete one epoch of meta training
Took 163.86188960075378 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472683
Epoch 754
Meta loss on this task batch = 2.6493e-01, Meta loss averaged over last 500 steps = 2.7096e-01, PNorm = 142.1911, GNorm = 0.2991
Meta loss on this task batch = 2.5304e-01, Meta loss averaged over last 500 steps = 2.7103e-01, PNorm = 142.1948, GNorm = 0.2624
Meta loss on this task batch = 2.4534e-01, Meta loss averaged over last 500 steps = 2.7102e-01, PNorm = 142.1987, GNorm = 0.3136
Meta loss on this task batch = 2.2275e-01, Meta loss averaged over last 500 steps = 2.7099e-01, PNorm = 142.2006, GNorm = 0.2866
Meta loss on this task batch = 2.8706e-01, Meta loss averaged over last 500 steps = 2.7103e-01, PNorm = 142.2042, GNorm = 0.3620
Meta loss on this task batch = 2.4329e-01, Meta loss averaged over last 500 steps = 2.7090e-01, PNorm = 142.2061, GNorm = 0.3124
Meta loss on this task batch = 2.8896e-01, Meta loss averaged over last 500 steps = 2.7100e-01, PNorm = 142.2051, GNorm = 0.3193
Meta loss on this task batch = 2.7749e-01, Meta loss averaged over last 500 steps = 2.7096e-01, PNorm = 142.2044, GNorm = 0.3344
Meta loss on this task batch = 2.6786e-01, Meta loss averaged over last 500 steps = 2.7091e-01, PNorm = 142.2032, GNorm = 0.3735
Meta loss on this task batch = 2.7694e-01, Meta loss averaged over last 500 steps = 2.7093e-01, PNorm = 142.2025, GNorm = 0.2900
Meta loss on this task batch = 2.7175e-01, Meta loss averaged over last 500 steps = 2.7090e-01, PNorm = 142.2026, GNorm = 0.3140
Meta loss on this task batch = 2.9118e-01, Meta loss averaged over last 500 steps = 2.7081e-01, PNorm = 142.2021, GNorm = 0.2915
Meta loss on this task batch = 2.6893e-01, Meta loss averaged over last 500 steps = 2.7068e-01, PNorm = 142.2015, GNorm = 0.3013
Meta loss on this task batch = 2.3818e-01, Meta loss averaged over last 500 steps = 2.7065e-01, PNorm = 142.2003, GNorm = 0.2647
Meta loss on this task batch = 2.8586e-01, Meta loss averaged over last 500 steps = 2.7067e-01, PNorm = 142.2002, GNorm = 0.2950
Meta loss on this task batch = 2.3313e-01, Meta loss averaged over last 500 steps = 2.7064e-01, PNorm = 142.2025, GNorm = 0.2755
Meta loss on this task batch = 2.6888e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 142.2067, GNorm = 0.2659
Meta loss on this task batch = 3.1108e-01, Meta loss averaged over last 500 steps = 2.7054e-01, PNorm = 142.2115, GNorm = 0.3015
Meta loss on this task batch = 2.6031e-01, Meta loss averaged over last 500 steps = 2.7051e-01, PNorm = 142.2170, GNorm = 0.3157
Took 130.75061202049255 seconds to complete one epoch of meta training
Took 138.9287166595459 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501299
Epoch 755
Meta loss on this task batch = 2.8892e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 142.2243, GNorm = 0.2875
Meta loss on this task batch = 2.3823e-01, Meta loss averaged over last 500 steps = 2.7050e-01, PNorm = 142.2314, GNorm = 0.2894
Meta loss on this task batch = 2.6802e-01, Meta loss averaged over last 500 steps = 2.7051e-01, PNorm = 142.2389, GNorm = 0.2792
Meta loss on this task batch = 2.8877e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 142.2459, GNorm = 0.2602
Meta loss on this task batch = 3.0204e-01, Meta loss averaged over last 500 steps = 2.7061e-01, PNorm = 142.2518, GNorm = 0.3060
Meta loss on this task batch = 2.7695e-01, Meta loss averaged over last 500 steps = 2.7070e-01, PNorm = 142.2576, GNorm = 0.3132
Meta loss on this task batch = 3.0066e-01, Meta loss averaged over last 500 steps = 2.7075e-01, PNorm = 142.2630, GNorm = 0.3181
Meta loss on this task batch = 2.9530e-01, Meta loss averaged over last 500 steps = 2.7080e-01, PNorm = 142.2673, GNorm = 0.3387
Meta loss on this task batch = 3.0619e-01, Meta loss averaged over last 500 steps = 2.7093e-01, PNorm = 142.2711, GNorm = 0.3000
Meta loss on this task batch = 2.5150e-01, Meta loss averaged over last 500 steps = 2.7094e-01, PNorm = 142.2751, GNorm = 0.2873
Meta loss on this task batch = 2.5474e-01, Meta loss averaged over last 500 steps = 2.7094e-01, PNorm = 142.2801, GNorm = 0.3415
Meta loss on this task batch = 2.8061e-01, Meta loss averaged over last 500 steps = 2.7088e-01, PNorm = 142.2850, GNorm = 0.2689
Meta loss on this task batch = 2.7895e-01, Meta loss averaged over last 500 steps = 2.7093e-01, PNorm = 142.2891, GNorm = 0.2984
Meta loss on this task batch = 2.6528e-01, Meta loss averaged over last 500 steps = 2.7089e-01, PNorm = 142.2934, GNorm = 0.2736
Meta loss on this task batch = 2.4580e-01, Meta loss averaged over last 500 steps = 2.7079e-01, PNorm = 142.2992, GNorm = 0.2708
Meta loss on this task batch = 2.7914e-01, Meta loss averaged over last 500 steps = 2.7071e-01, PNorm = 142.3060, GNorm = 0.3066
Meta loss on this task batch = 2.3514e-01, Meta loss averaged over last 500 steps = 2.7058e-01, PNorm = 142.3151, GNorm = 0.2776
Meta loss on this task batch = 2.7994e-01, Meta loss averaged over last 500 steps = 2.7059e-01, PNorm = 142.3245, GNorm = 0.2733
Meta loss on this task batch = 2.9349e-01, Meta loss averaged over last 500 steps = 2.7070e-01, PNorm = 142.3330, GNorm = 0.3428
Took 161.08497428894043 seconds to complete one epoch of meta training
Took 169.37601613998413 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.516951
Epoch 756
Meta loss on this task batch = 2.5107e-01, Meta loss averaged over last 500 steps = 2.7068e-01, PNorm = 142.3393, GNorm = 0.2831
Meta loss on this task batch = 3.1170e-01, Meta loss averaged over last 500 steps = 2.7082e-01, PNorm = 142.3440, GNorm = 0.3069
Meta loss on this task batch = 2.8451e-01, Meta loss averaged over last 500 steps = 2.7085e-01, PNorm = 142.3487, GNorm = 0.3483
Meta loss on this task batch = 2.4299e-01, Meta loss averaged over last 500 steps = 2.7082e-01, PNorm = 142.3515, GNorm = 0.3410
Meta loss on this task batch = 2.5711e-01, Meta loss averaged over last 500 steps = 2.7073e-01, PNorm = 142.3540, GNorm = 0.3008
Meta loss on this task batch = 2.4251e-01, Meta loss averaged over last 500 steps = 2.7068e-01, PNorm = 142.3575, GNorm = 0.2512
Meta loss on this task batch = 2.4886e-01, Meta loss averaged over last 500 steps = 2.7061e-01, PNorm = 142.3605, GNorm = 0.2559
Meta loss on this task batch = 2.5591e-01, Meta loss averaged over last 500 steps = 2.7052e-01, PNorm = 142.3636, GNorm = 0.2684
Meta loss on this task batch = 2.9937e-01, Meta loss averaged over last 500 steps = 2.7051e-01, PNorm = 142.3665, GNorm = 0.2900
Meta loss on this task batch = 2.5954e-01, Meta loss averaged over last 500 steps = 2.7033e-01, PNorm = 142.3706, GNorm = 0.2876
Meta loss on this task batch = 2.6915e-01, Meta loss averaged over last 500 steps = 2.7032e-01, PNorm = 142.3745, GNorm = 0.2673
Meta loss on this task batch = 2.7560e-01, Meta loss averaged over last 500 steps = 2.7030e-01, PNorm = 142.3778, GNorm = 0.2975
Meta loss on this task batch = 2.8127e-01, Meta loss averaged over last 500 steps = 2.7028e-01, PNorm = 142.3839, GNorm = 0.3522
Meta loss on this task batch = 2.7149e-01, Meta loss averaged over last 500 steps = 2.7034e-01, PNorm = 142.3904, GNorm = 0.2941
Meta loss on this task batch = 2.7544e-01, Meta loss averaged over last 500 steps = 2.7038e-01, PNorm = 142.3978, GNorm = 0.3067
Meta loss on this task batch = 2.4695e-01, Meta loss averaged over last 500 steps = 2.7033e-01, PNorm = 142.4058, GNorm = 0.3125
Meta loss on this task batch = 2.9082e-01, Meta loss averaged over last 500 steps = 2.7047e-01, PNorm = 142.4133, GNorm = 0.3228
Meta loss on this task batch = 2.9097e-01, Meta loss averaged over last 500 steps = 2.7052e-01, PNorm = 142.4197, GNorm = 0.3297
Meta loss on this task batch = 2.5723e-01, Meta loss averaged over last 500 steps = 2.7036e-01, PNorm = 142.4267, GNorm = 0.4128
Took 127.16366481781006 seconds to complete one epoch of meta training
Took 135.2737147808075 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481662
Epoch 757
Meta loss on this task batch = 2.1149e-01, Meta loss averaged over last 500 steps = 2.7030e-01, PNorm = 142.4338, GNorm = 0.2851
Meta loss on this task batch = 3.0891e-01, Meta loss averaged over last 500 steps = 2.7040e-01, PNorm = 142.4395, GNorm = 0.3720
Meta loss on this task batch = 2.9316e-01, Meta loss averaged over last 500 steps = 2.7045e-01, PNorm = 142.4436, GNorm = 0.3345
Meta loss on this task batch = 2.6410e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 142.4457, GNorm = 0.2764
Meta loss on this task batch = 2.3049e-01, Meta loss averaged over last 500 steps = 2.7046e-01, PNorm = 142.4469, GNorm = 0.2743
Meta loss on this task batch = 2.6757e-01, Meta loss averaged over last 500 steps = 2.7042e-01, PNorm = 142.4478, GNorm = 0.2846
Meta loss on this task batch = 2.2232e-01, Meta loss averaged over last 500 steps = 2.7025e-01, PNorm = 142.4499, GNorm = 0.2204
Meta loss on this task batch = 2.6000e-01, Meta loss averaged over last 500 steps = 2.7030e-01, PNorm = 142.4503, GNorm = 0.3071
Meta loss on this task batch = 2.7585e-01, Meta loss averaged over last 500 steps = 2.7026e-01, PNorm = 142.4504, GNorm = 0.2829
Meta loss on this task batch = 3.1309e-01, Meta loss averaged over last 500 steps = 2.7032e-01, PNorm = 142.4493, GNorm = 0.4240
Meta loss on this task batch = 2.3942e-01, Meta loss averaged over last 500 steps = 2.7022e-01, PNorm = 142.4501, GNorm = 0.2403
Meta loss on this task batch = 2.7078e-01, Meta loss averaged over last 500 steps = 2.7030e-01, PNorm = 142.4515, GNorm = 0.3166
Meta loss on this task batch = 2.7784e-01, Meta loss averaged over last 500 steps = 2.7033e-01, PNorm = 142.4516, GNorm = 0.2916
Meta loss on this task batch = 3.0782e-01, Meta loss averaged over last 500 steps = 2.7038e-01, PNorm = 142.4521, GNorm = 0.3239
Meta loss on this task batch = 2.8652e-01, Meta loss averaged over last 500 steps = 2.7046e-01, PNorm = 142.4534, GNorm = 0.3095
Meta loss on this task batch = 3.2248e-01, Meta loss averaged over last 500 steps = 2.7052e-01, PNorm = 142.4561, GNorm = 0.3375
Meta loss on this task batch = 2.4726e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 142.4591, GNorm = 0.2904
Meta loss on this task batch = 2.9928e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 142.4621, GNorm = 0.2827
Meta loss on this task batch = 2.8369e-01, Meta loss averaged over last 500 steps = 2.7038e-01, PNorm = 142.4633, GNorm = 0.3714
Took 132.95978546142578 seconds to complete one epoch of meta training
Took 141.44881963729858 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475163
Epoch 758
Meta loss on this task batch = 2.7216e-01, Meta loss averaged over last 500 steps = 2.7045e-01, PNorm = 142.4633, GNorm = 0.2600
Meta loss on this task batch = 2.4992e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 142.4631, GNorm = 0.2577
Meta loss on this task batch = 2.9796e-01, Meta loss averaged over last 500 steps = 2.7050e-01, PNorm = 142.4631, GNorm = 0.2724
Meta loss on this task batch = 2.8557e-01, Meta loss averaged over last 500 steps = 2.7046e-01, PNorm = 142.4649, GNorm = 0.2776
Meta loss on this task batch = 2.2877e-01, Meta loss averaged over last 500 steps = 2.7042e-01, PNorm = 142.4657, GNorm = 0.2746
Meta loss on this task batch = 3.0362e-01, Meta loss averaged over last 500 steps = 2.7047e-01, PNorm = 142.4655, GNorm = 0.3116
Meta loss on this task batch = 2.3915e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 142.4656, GNorm = 0.3112
Meta loss on this task batch = 2.9817e-01, Meta loss averaged over last 500 steps = 2.7040e-01, PNorm = 142.4663, GNorm = 0.2897
Meta loss on this task batch = 2.6136e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 142.4682, GNorm = 0.2911
Meta loss on this task batch = 2.7601e-01, Meta loss averaged over last 500 steps = 2.7047e-01, PNorm = 142.4724, GNorm = 0.3088
Meta loss on this task batch = 2.4043e-01, Meta loss averaged over last 500 steps = 2.7047e-01, PNorm = 142.4766, GNorm = 0.3149
Meta loss on this task batch = 2.5670e-01, Meta loss averaged over last 500 steps = 2.7039e-01, PNorm = 142.4810, GNorm = 0.3540
Meta loss on this task batch = 2.1219e-01, Meta loss averaged over last 500 steps = 2.7026e-01, PNorm = 142.4858, GNorm = 0.2513
Meta loss on this task batch = 2.4918e-01, Meta loss averaged over last 500 steps = 2.7024e-01, PNorm = 142.4914, GNorm = 0.3279
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 2.7032e-01, PNorm = 142.4971, GNorm = 0.3413
Meta loss on this task batch = 2.7252e-01, Meta loss averaged over last 500 steps = 2.7040e-01, PNorm = 142.5028, GNorm = 0.2812
Meta loss on this task batch = 3.1432e-01, Meta loss averaged over last 500 steps = 2.7050e-01, PNorm = 142.5078, GNorm = 0.3095
Meta loss on this task batch = 2.5018e-01, Meta loss averaged over last 500 steps = 2.7046e-01, PNorm = 142.5133, GNorm = 0.3164
Meta loss on this task batch = 3.0357e-01, Meta loss averaged over last 500 steps = 2.7044e-01, PNorm = 142.5171, GNorm = 0.3559
Took 130.98505187034607 seconds to complete one epoch of meta training
Took 139.19275283813477 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496799
Epoch 759
Meta loss on this task batch = 3.0493e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 142.5212, GNorm = 0.3392
Meta loss on this task batch = 2.5989e-01, Meta loss averaged over last 500 steps = 2.7042e-01, PNorm = 142.5259, GNorm = 0.2781
Meta loss on this task batch = 3.2788e-01, Meta loss averaged over last 500 steps = 2.7050e-01, PNorm = 142.5319, GNorm = 0.3255
Meta loss on this task batch = 2.4852e-01, Meta loss averaged over last 500 steps = 2.7050e-01, PNorm = 142.5372, GNorm = 0.2954
Meta loss on this task batch = 2.1590e-01, Meta loss averaged over last 500 steps = 2.7045e-01, PNorm = 142.5434, GNorm = 0.2738
Meta loss on this task batch = 2.9005e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 142.5483, GNorm = 0.2846
Meta loss on this task batch = 2.9514e-01, Meta loss averaged over last 500 steps = 2.7052e-01, PNorm = 142.5533, GNorm = 0.2938
Meta loss on this task batch = 2.9714e-01, Meta loss averaged over last 500 steps = 2.7061e-01, PNorm = 142.5590, GNorm = 0.3120
Meta loss on this task batch = 2.5877e-01, Meta loss averaged over last 500 steps = 2.7057e-01, PNorm = 142.5644, GNorm = 0.2768
Meta loss on this task batch = 3.0364e-01, Meta loss averaged over last 500 steps = 2.7067e-01, PNorm = 142.5694, GNorm = 0.2981
Meta loss on this task batch = 2.9081e-01, Meta loss averaged over last 500 steps = 2.7076e-01, PNorm = 142.5744, GNorm = 0.3030
Meta loss on this task batch = 2.9146e-01, Meta loss averaged over last 500 steps = 2.7078e-01, PNorm = 142.5789, GNorm = 0.2890
Meta loss on this task batch = 1.9599e-01, Meta loss averaged over last 500 steps = 2.7064e-01, PNorm = 142.5834, GNorm = 0.2608
Meta loss on this task batch = 2.6068e-01, Meta loss averaged over last 500 steps = 2.7062e-01, PNorm = 142.5902, GNorm = 0.2772
Meta loss on this task batch = 2.3031e-01, Meta loss averaged over last 500 steps = 2.7050e-01, PNorm = 142.5964, GNorm = 0.2524
Meta loss on this task batch = 2.7672e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 142.6016, GNorm = 0.3130
Meta loss on this task batch = 2.6245e-01, Meta loss averaged over last 500 steps = 2.7045e-01, PNorm = 142.6062, GNorm = 0.2844
Meta loss on this task batch = 2.5584e-01, Meta loss averaged over last 500 steps = 2.7045e-01, PNorm = 142.6113, GNorm = 0.3223
Meta loss on this task batch = 1.8623e-01, Meta loss averaged over last 500 steps = 2.7029e-01, PNorm = 142.6157, GNorm = 0.3009
Took 130.70451164245605 seconds to complete one epoch of meta training
Took 138.92422747612 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470699
Epoch 760
Meta loss on this task batch = 2.0227e-01, Meta loss averaged over last 500 steps = 2.7013e-01, PNorm = 142.6207, GNorm = 0.2174
Meta loss on this task batch = 2.3733e-01, Meta loss averaged over last 500 steps = 2.7006e-01, PNorm = 142.6262, GNorm = 0.2782
Meta loss on this task batch = 3.0877e-01, Meta loss averaged over last 500 steps = 2.7005e-01, PNorm = 142.6312, GNorm = 0.3063
Meta loss on this task batch = 2.3682e-01, Meta loss averaged over last 500 steps = 2.6997e-01, PNorm = 142.6355, GNorm = 0.2641
Meta loss on this task batch = 2.4454e-01, Meta loss averaged over last 500 steps = 2.6988e-01, PNorm = 142.6392, GNorm = 0.2880
Meta loss on this task batch = 2.2186e-01, Meta loss averaged over last 500 steps = 2.6986e-01, PNorm = 142.6439, GNorm = 0.2477
Meta loss on this task batch = 3.0461e-01, Meta loss averaged over last 500 steps = 2.6989e-01, PNorm = 142.6480, GNorm = 0.3157
Meta loss on this task batch = 2.2202e-01, Meta loss averaged over last 500 steps = 2.6990e-01, PNorm = 142.6521, GNorm = 0.2675
Meta loss on this task batch = 2.7063e-01, Meta loss averaged over last 500 steps = 2.6990e-01, PNorm = 142.6561, GNorm = 0.3044
Meta loss on this task batch = 2.2475e-01, Meta loss averaged over last 500 steps = 2.6965e-01, PNorm = 142.6599, GNorm = 0.3007
Meta loss on this task batch = 3.1173e-01, Meta loss averaged over last 500 steps = 2.6984e-01, PNorm = 142.6634, GNorm = 0.2898
Meta loss on this task batch = 2.7059e-01, Meta loss averaged over last 500 steps = 2.6990e-01, PNorm = 142.6677, GNorm = 0.3312
Meta loss on this task batch = 2.7188e-01, Meta loss averaged over last 500 steps = 2.6988e-01, PNorm = 142.6732, GNorm = 0.3084
Meta loss on this task batch = 2.5303e-01, Meta loss averaged over last 500 steps = 2.6980e-01, PNorm = 142.6805, GNorm = 0.2742
Meta loss on this task batch = 2.8958e-01, Meta loss averaged over last 500 steps = 2.6992e-01, PNorm = 142.6857, GNorm = 0.3099
Meta loss on this task batch = 3.2002e-01, Meta loss averaged over last 500 steps = 2.7004e-01, PNorm = 142.6896, GNorm = 0.3745
Meta loss on this task batch = 2.4842e-01, Meta loss averaged over last 500 steps = 2.7015e-01, PNorm = 142.6918, GNorm = 0.2967
Meta loss on this task batch = 2.9452e-01, Meta loss averaged over last 500 steps = 2.7009e-01, PNorm = 142.6942, GNorm = 0.2957
Meta loss on this task batch = 2.3180e-01, Meta loss averaged over last 500 steps = 2.7010e-01, PNorm = 142.6983, GNorm = 0.2896
Took 130.23068284988403 seconds to complete one epoch of meta training
Took 138.658127784729 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487817
Epoch 761
Meta loss on this task batch = 2.7214e-01, Meta loss averaged over last 500 steps = 2.7000e-01, PNorm = 142.7044, GNorm = 0.2737
Meta loss on this task batch = 2.7768e-01, Meta loss averaged over last 500 steps = 2.6996e-01, PNorm = 142.7108, GNorm = 0.2914
Meta loss on this task batch = 2.2700e-01, Meta loss averaged over last 500 steps = 2.6979e-01, PNorm = 142.7197, GNorm = 0.2570
Meta loss on this task batch = 2.6813e-01, Meta loss averaged over last 500 steps = 2.6981e-01, PNorm = 142.7282, GNorm = 0.3706
Meta loss on this task batch = 3.1442e-01, Meta loss averaged over last 500 steps = 2.6993e-01, PNorm = 142.7352, GNorm = 0.3028
Meta loss on this task batch = 2.4109e-01, Meta loss averaged over last 500 steps = 2.6979e-01, PNorm = 142.7425, GNorm = 0.2468
Meta loss on this task batch = 2.5830e-01, Meta loss averaged over last 500 steps = 2.6975e-01, PNorm = 142.7502, GNorm = 0.2497
Meta loss on this task batch = 2.7891e-01, Meta loss averaged over last 500 steps = 2.6982e-01, PNorm = 142.7595, GNorm = 0.2667
Meta loss on this task batch = 2.4222e-01, Meta loss averaged over last 500 steps = 2.6977e-01, PNorm = 142.7680, GNorm = 0.2692
Meta loss on this task batch = 2.1084e-01, Meta loss averaged over last 500 steps = 2.6977e-01, PNorm = 142.7761, GNorm = 0.2471
Meta loss on this task batch = 3.4018e-01, Meta loss averaged over last 500 steps = 2.7000e-01, PNorm = 142.7818, GNorm = 0.3060
Meta loss on this task batch = 2.9250e-01, Meta loss averaged over last 500 steps = 2.7004e-01, PNorm = 142.7869, GNorm = 0.2863
Meta loss on this task batch = 2.2128e-01, Meta loss averaged over last 500 steps = 2.6999e-01, PNorm = 142.7945, GNorm = 0.2958
Meta loss on this task batch = 3.1945e-01, Meta loss averaged over last 500 steps = 2.6998e-01, PNorm = 142.7999, GNorm = 0.3255
Meta loss on this task batch = 2.6266e-01, Meta loss averaged over last 500 steps = 2.6998e-01, PNorm = 142.8040, GNorm = 0.3357
Meta loss on this task batch = 2.4498e-01, Meta loss averaged over last 500 steps = 2.6989e-01, PNorm = 142.8074, GNorm = 0.3435
Meta loss on this task batch = 2.2983e-01, Meta loss averaged over last 500 steps = 2.6977e-01, PNorm = 142.8119, GNorm = 0.3054
Meta loss on this task batch = 2.6021e-01, Meta loss averaged over last 500 steps = 2.6973e-01, PNorm = 142.8167, GNorm = 0.3691
Meta loss on this task batch = 3.0983e-01, Meta loss averaged over last 500 steps = 2.6984e-01, PNorm = 142.8227, GNorm = 0.4449
Took 129.64600539207458 seconds to complete one epoch of meta training
Took 137.9808738231659 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505634
Epoch 762
Meta loss on this task batch = 2.8289e-01, Meta loss averaged over last 500 steps = 2.6992e-01, PNorm = 142.8287, GNorm = 0.3072
Meta loss on this task batch = 2.8715e-01, Meta loss averaged over last 500 steps = 2.7001e-01, PNorm = 142.8327, GNorm = 0.3231
Meta loss on this task batch = 2.8337e-01, Meta loss averaged over last 500 steps = 2.7005e-01, PNorm = 142.8372, GNorm = 0.2866
Meta loss on this task batch = 2.8576e-01, Meta loss averaged over last 500 steps = 2.7003e-01, PNorm = 142.8409, GNorm = 0.3062
Meta loss on this task batch = 2.9360e-01, Meta loss averaged over last 500 steps = 2.7007e-01, PNorm = 142.8445, GNorm = 0.2968
Meta loss on this task batch = 3.2774e-01, Meta loss averaged over last 500 steps = 2.7006e-01, PNorm = 142.8473, GNorm = 0.3263
Meta loss on this task batch = 3.3103e-01, Meta loss averaged over last 500 steps = 2.7024e-01, PNorm = 142.8478, GNorm = 0.3984
Meta loss on this task batch = 2.5729e-01, Meta loss averaged over last 500 steps = 2.7010e-01, PNorm = 142.8486, GNorm = 0.2821
Meta loss on this task batch = 2.2434e-01, Meta loss averaged over last 500 steps = 2.7013e-01, PNorm = 142.8480, GNorm = 0.2696
Meta loss on this task batch = 2.3296e-01, Meta loss averaged over last 500 steps = 2.6997e-01, PNorm = 142.8488, GNorm = 0.2544
Meta loss on this task batch = 2.3175e-01, Meta loss averaged over last 500 steps = 2.6979e-01, PNorm = 142.8514, GNorm = 0.2536
Meta loss on this task batch = 2.4551e-01, Meta loss averaged over last 500 steps = 2.6975e-01, PNorm = 142.8564, GNorm = 0.2684
Meta loss on this task batch = 3.0961e-01, Meta loss averaged over last 500 steps = 2.6987e-01, PNorm = 142.8621, GNorm = 0.2773
Meta loss on this task batch = 2.9356e-01, Meta loss averaged over last 500 steps = 2.6991e-01, PNorm = 142.8688, GNorm = 0.2825
Meta loss on this task batch = 2.8116e-01, Meta loss averaged over last 500 steps = 2.6991e-01, PNorm = 142.8750, GNorm = 0.3158
Meta loss on this task batch = 2.5407e-01, Meta loss averaged over last 500 steps = 2.6984e-01, PNorm = 142.8809, GNorm = 0.2289
Meta loss on this task batch = 2.5659e-01, Meta loss averaged over last 500 steps = 2.6982e-01, PNorm = 142.8865, GNorm = 0.3102
Meta loss on this task batch = 2.2372e-01, Meta loss averaged over last 500 steps = 2.6977e-01, PNorm = 142.8935, GNorm = 0.2569
Meta loss on this task batch = 2.4733e-01, Meta loss averaged over last 500 steps = 2.6976e-01, PNorm = 142.8953, GNorm = 0.4282
Took 129.42304825782776 seconds to complete one epoch of meta training
Took 137.76937699317932 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508031
Epoch 763
Meta loss on this task batch = 3.0766e-01, Meta loss averaged over last 500 steps = 2.6978e-01, PNorm = 142.8941, GNorm = 0.3472
Meta loss on this task batch = 2.4941e-01, Meta loss averaged over last 500 steps = 2.6979e-01, PNorm = 142.8920, GNorm = 0.2423
Meta loss on this task batch = 3.1123e-01, Meta loss averaged over last 500 steps = 2.6980e-01, PNorm = 142.8898, GNorm = 0.3018
Meta loss on this task batch = 2.9465e-01, Meta loss averaged over last 500 steps = 2.6987e-01, PNorm = 142.8885, GNorm = 0.2926
Meta loss on this task batch = 2.6661e-01, Meta loss averaged over last 500 steps = 2.7000e-01, PNorm = 142.8885, GNorm = 0.2944
Meta loss on this task batch = 3.0568e-01, Meta loss averaged over last 500 steps = 2.7002e-01, PNorm = 142.8892, GNorm = 0.3348
Meta loss on this task batch = 2.7779e-01, Meta loss averaged over last 500 steps = 2.6997e-01, PNorm = 142.8918, GNorm = 0.2969
Meta loss on this task batch = 2.7437e-01, Meta loss averaged over last 500 steps = 2.6990e-01, PNorm = 142.8966, GNorm = 0.2790
Meta loss on this task batch = 2.7155e-01, Meta loss averaged over last 500 steps = 2.6998e-01, PNorm = 142.9001, GNorm = 0.2670
Meta loss on this task batch = 2.4564e-01, Meta loss averaged over last 500 steps = 2.6995e-01, PNorm = 142.9042, GNorm = 0.3071
Meta loss on this task batch = 2.9770e-01, Meta loss averaged over last 500 steps = 2.6996e-01, PNorm = 142.9079, GNorm = 0.2970
Meta loss on this task batch = 2.5197e-01, Meta loss averaged over last 500 steps = 2.6994e-01, PNorm = 142.9131, GNorm = 0.2706
Meta loss on this task batch = 2.6220e-01, Meta loss averaged over last 500 steps = 2.6999e-01, PNorm = 142.9200, GNorm = 0.2915
Meta loss on this task batch = 2.4864e-01, Meta loss averaged over last 500 steps = 2.6975e-01, PNorm = 142.9272, GNorm = 0.2582
Meta loss on this task batch = 2.5858e-01, Meta loss averaged over last 500 steps = 2.6963e-01, PNorm = 142.9341, GNorm = 0.3051
Meta loss on this task batch = 2.9727e-01, Meta loss averaged over last 500 steps = 2.6974e-01, PNorm = 142.9398, GNorm = 0.3076
Meta loss on this task batch = 2.3216e-01, Meta loss averaged over last 500 steps = 2.6969e-01, PNorm = 142.9446, GNorm = 0.2775
Meta loss on this task batch = 2.5445e-01, Meta loss averaged over last 500 steps = 2.6968e-01, PNorm = 142.9493, GNorm = 0.2767
Meta loss on this task batch = 2.2700e-01, Meta loss averaged over last 500 steps = 2.6964e-01, PNorm = 142.9550, GNorm = 0.3803
Took 187.1378755569458 seconds to complete one epoch of meta training
Took 195.75758838653564 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491836
Epoch 764
Meta loss on this task batch = 3.0555e-01, Meta loss averaged over last 500 steps = 2.6976e-01, PNorm = 142.9597, GNorm = 0.3382
Meta loss on this task batch = 2.6238e-01, Meta loss averaged over last 500 steps = 2.6957e-01, PNorm = 142.9637, GNorm = 0.2908
Meta loss on this task batch = 2.8616e-01, Meta loss averaged over last 500 steps = 2.6974e-01, PNorm = 142.9676, GNorm = 0.2944
Meta loss on this task batch = 2.6315e-01, Meta loss averaged over last 500 steps = 2.6979e-01, PNorm = 142.9700, GNorm = 0.2871
Meta loss on this task batch = 2.1475e-01, Meta loss averaged over last 500 steps = 2.6972e-01, PNorm = 142.9703, GNorm = 0.2588
Meta loss on this task batch = 2.9097e-01, Meta loss averaged over last 500 steps = 2.6993e-01, PNorm = 142.9701, GNorm = 0.3341
Meta loss on this task batch = 2.2698e-01, Meta loss averaged over last 500 steps = 2.6999e-01, PNorm = 142.9704, GNorm = 0.2530
Meta loss on this task batch = 3.1500e-01, Meta loss averaged over last 500 steps = 2.7001e-01, PNorm = 142.9698, GNorm = 0.3006
Meta loss on this task batch = 2.4811e-01, Meta loss averaged over last 500 steps = 2.6991e-01, PNorm = 142.9695, GNorm = 0.2514
Meta loss on this task batch = 2.5506e-01, Meta loss averaged over last 500 steps = 2.6978e-01, PNorm = 142.9712, GNorm = 0.2654
Meta loss on this task batch = 2.6201e-01, Meta loss averaged over last 500 steps = 2.6979e-01, PNorm = 142.9721, GNorm = 0.2762
Meta loss on this task batch = 2.7637e-01, Meta loss averaged over last 500 steps = 2.6982e-01, PNorm = 142.9718, GNorm = 0.2823
Meta loss on this task batch = 3.4092e-01, Meta loss averaged over last 500 steps = 2.7000e-01, PNorm = 142.9712, GNorm = 0.3552
Meta loss on this task batch = 3.1714e-01, Meta loss averaged over last 500 steps = 2.7007e-01, PNorm = 142.9689, GNorm = 0.4239
Meta loss on this task batch = 2.6749e-01, Meta loss averaged over last 500 steps = 2.7002e-01, PNorm = 142.9699, GNorm = 0.3029
Meta loss on this task batch = 3.4593e-01, Meta loss averaged over last 500 steps = 2.7014e-01, PNorm = 142.9715, GNorm = 0.3059
Meta loss on this task batch = 2.2028e-01, Meta loss averaged over last 500 steps = 2.7001e-01, PNorm = 142.9767, GNorm = 0.2816
Meta loss on this task batch = 2.6895e-01, Meta loss averaged over last 500 steps = 2.6990e-01, PNorm = 142.9829, GNorm = 0.3056
Meta loss on this task batch = 2.3623e-01, Meta loss averaged over last 500 steps = 2.6985e-01, PNorm = 142.9902, GNorm = 0.2784
Took 140.97967195510864 seconds to complete one epoch of meta training
Took 149.10326147079468 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495221
Epoch 765
Meta loss on this task batch = 2.3325e-01, Meta loss averaged over last 500 steps = 2.6979e-01, PNorm = 142.9990, GNorm = 0.2779
Meta loss on this task batch = 2.8335e-01, Meta loss averaged over last 500 steps = 2.6989e-01, PNorm = 143.0080, GNorm = 0.2535
Meta loss on this task batch = 2.5975e-01, Meta loss averaged over last 500 steps = 2.6977e-01, PNorm = 143.0175, GNorm = 0.2803
Meta loss on this task batch = 3.0795e-01, Meta loss averaged over last 500 steps = 2.6988e-01, PNorm = 143.0254, GNorm = 0.2856
Meta loss on this task batch = 2.1114e-01, Meta loss averaged over last 500 steps = 2.6970e-01, PNorm = 143.0317, GNorm = 0.2378
Meta loss on this task batch = 2.1872e-01, Meta loss averaged over last 500 steps = 2.6965e-01, PNorm = 143.0381, GNorm = 0.2355
Meta loss on this task batch = 2.9633e-01, Meta loss averaged over last 500 steps = 2.6980e-01, PNorm = 143.0427, GNorm = 0.2967
Meta loss on this task batch = 3.0937e-01, Meta loss averaged over last 500 steps = 2.6995e-01, PNorm = 143.0443, GNorm = 0.3385
Meta loss on this task batch = 3.0690e-01, Meta loss averaged over last 500 steps = 2.6996e-01, PNorm = 143.0442, GNorm = 0.2829
Meta loss on this task batch = 2.7178e-01, Meta loss averaged over last 500 steps = 2.7000e-01, PNorm = 143.0436, GNorm = 0.3050
Meta loss on this task batch = 2.3407e-01, Meta loss averaged over last 500 steps = 2.6990e-01, PNorm = 143.0423, GNorm = 0.2586
Meta loss on this task batch = 2.9912e-01, Meta loss averaged over last 500 steps = 2.6995e-01, PNorm = 143.0394, GNorm = 0.3093
Meta loss on this task batch = 2.5282e-01, Meta loss averaged over last 500 steps = 2.6984e-01, PNorm = 143.0374, GNorm = 0.3150
Meta loss on this task batch = 2.8893e-01, Meta loss averaged over last 500 steps = 2.6983e-01, PNorm = 143.0363, GNorm = 0.2970
Meta loss on this task batch = 2.5654e-01, Meta loss averaged over last 500 steps = 2.6968e-01, PNorm = 143.0373, GNorm = 0.2755
Meta loss on this task batch = 2.8365e-01, Meta loss averaged over last 500 steps = 2.6970e-01, PNorm = 143.0399, GNorm = 0.3089
Meta loss on this task batch = 2.6624e-01, Meta loss averaged over last 500 steps = 2.6976e-01, PNorm = 143.0442, GNorm = 0.2949
Meta loss on this task batch = 2.9187e-01, Meta loss averaged over last 500 steps = 2.6968e-01, PNorm = 143.0512, GNorm = 0.3530
Meta loss on this task batch = 3.0084e-01, Meta loss averaged over last 500 steps = 2.6973e-01, PNorm = 143.0594, GNorm = 0.3927
Took 141.6773190498352 seconds to complete one epoch of meta training
Took 150.14628672599792 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497361
Epoch 766
Meta loss on this task batch = 2.3960e-01, Meta loss averaged over last 500 steps = 2.6966e-01, PNorm = 143.0666, GNorm = 0.2590
Meta loss on this task batch = 2.7909e-01, Meta loss averaged over last 500 steps = 2.6974e-01, PNorm = 143.0744, GNorm = 0.3041
Meta loss on this task batch = 2.6285e-01, Meta loss averaged over last 500 steps = 2.6964e-01, PNorm = 143.0834, GNorm = 0.2979
Meta loss on this task batch = 2.2932e-01, Meta loss averaged over last 500 steps = 2.6954e-01, PNorm = 143.0930, GNorm = 0.2669
Meta loss on this task batch = 3.5639e-01, Meta loss averaged over last 500 steps = 2.6969e-01, PNorm = 143.1034, GNorm = 0.3183
Meta loss on this task batch = 2.5223e-01, Meta loss averaged over last 500 steps = 2.6975e-01, PNorm = 143.1151, GNorm = 0.2597
Meta loss on this task batch = 2.7908e-01, Meta loss averaged over last 500 steps = 2.6984e-01, PNorm = 143.1277, GNorm = 0.2789
Meta loss on this task batch = 2.5019e-01, Meta loss averaged over last 500 steps = 2.6985e-01, PNorm = 143.1431, GNorm = 0.3033
Meta loss on this task batch = 2.4585e-01, Meta loss averaged over last 500 steps = 2.6978e-01, PNorm = 143.1578, GNorm = 0.2562
Meta loss on this task batch = 3.1830e-01, Meta loss averaged over last 500 steps = 2.6989e-01, PNorm = 143.1686, GNorm = 0.3164
Meta loss on this task batch = 2.6386e-01, Meta loss averaged over last 500 steps = 2.6983e-01, PNorm = 143.1766, GNorm = 0.3427
Meta loss on this task batch = 2.9984e-01, Meta loss averaged over last 500 steps = 2.6994e-01, PNorm = 143.1835, GNorm = 0.2840
Meta loss on this task batch = 2.1799e-01, Meta loss averaged over last 500 steps = 2.6978e-01, PNorm = 143.1888, GNorm = 0.2794
Meta loss on this task batch = 2.5680e-01, Meta loss averaged over last 500 steps = 2.6985e-01, PNorm = 143.1929, GNorm = 0.2981
Meta loss on this task batch = 2.5072e-01, Meta loss averaged over last 500 steps = 2.6982e-01, PNorm = 143.1953, GNorm = 0.2938
Meta loss on this task batch = 2.8230e-01, Meta loss averaged over last 500 steps = 2.6980e-01, PNorm = 143.1975, GNorm = 0.2972
Meta loss on this task batch = 2.5311e-01, Meta loss averaged over last 500 steps = 2.6970e-01, PNorm = 143.2021, GNorm = 0.3163
Meta loss on this task batch = 2.6252e-01, Meta loss averaged over last 500 steps = 2.6969e-01, PNorm = 143.2074, GNorm = 0.2810
Meta loss on this task batch = 3.1288e-01, Meta loss averaged over last 500 steps = 2.6985e-01, PNorm = 143.2122, GNorm = 0.4521
Took 131.83177947998047 seconds to complete one epoch of meta training
Took 140.32118487358093 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504989
Epoch 767
Meta loss on this task batch = 2.5222e-01, Meta loss averaged over last 500 steps = 2.6977e-01, PNorm = 143.2168, GNorm = 0.2836
Meta loss on this task batch = 2.6059e-01, Meta loss averaged over last 500 steps = 2.6979e-01, PNorm = 143.2218, GNorm = 0.2931
Meta loss on this task batch = 2.7093e-01, Meta loss averaged over last 500 steps = 2.6973e-01, PNorm = 143.2261, GNorm = 0.3605
Meta loss on this task batch = 3.0103e-01, Meta loss averaged over last 500 steps = 2.6986e-01, PNorm = 143.2289, GNorm = 0.2932
Meta loss on this task batch = 2.4268e-01, Meta loss averaged over last 500 steps = 2.6980e-01, PNorm = 143.2314, GNorm = 0.2598
Meta loss on this task batch = 2.4591e-01, Meta loss averaged over last 500 steps = 2.6965e-01, PNorm = 143.2349, GNorm = 0.2827
Meta loss on this task batch = 2.9846e-01, Meta loss averaged over last 500 steps = 2.6971e-01, PNorm = 143.2386, GNorm = 0.3408
Meta loss on this task batch = 3.2766e-01, Meta loss averaged over last 500 steps = 2.6984e-01, PNorm = 143.2414, GNorm = 0.3608
Meta loss on this task batch = 2.7982e-01, Meta loss averaged over last 500 steps = 2.6973e-01, PNorm = 143.2442, GNorm = 0.3400
Meta loss on this task batch = 2.4151e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 143.2466, GNorm = 0.2630
Meta loss on this task batch = 2.4292e-01, Meta loss averaged over last 500 steps = 2.6942e-01, PNorm = 143.2495, GNorm = 0.2698
Meta loss on this task batch = 2.8780e-01, Meta loss averaged over last 500 steps = 2.6946e-01, PNorm = 143.2523, GNorm = 0.2936
Meta loss on this task batch = 2.6085e-01, Meta loss averaged over last 500 steps = 2.6941e-01, PNorm = 143.2566, GNorm = 0.3048
Meta loss on this task batch = 3.0667e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 143.2596, GNorm = 0.3686
Meta loss on this task batch = 2.5677e-01, Meta loss averaged over last 500 steps = 2.6952e-01, PNorm = 143.2615, GNorm = 0.2744
Meta loss on this task batch = 2.8865e-01, Meta loss averaged over last 500 steps = 2.6959e-01, PNorm = 143.2624, GNorm = 0.3678
Meta loss on this task batch = 2.1523e-01, Meta loss averaged over last 500 steps = 2.6950e-01, PNorm = 143.2643, GNorm = 0.2589
Meta loss on this task batch = 2.9459e-01, Meta loss averaged over last 500 steps = 2.6950e-01, PNorm = 143.2654, GNorm = 0.3578
Meta loss on this task batch = 1.8534e-01, Meta loss averaged over last 500 steps = 2.6938e-01, PNorm = 143.2670, GNorm = 0.3161
Took 130.48833346366882 seconds to complete one epoch of meta training
Took 138.78342962265015 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486787
Epoch 768
Meta loss on this task batch = 2.5701e-01, Meta loss averaged over last 500 steps = 2.6942e-01, PNorm = 143.2709, GNorm = 0.3234
Meta loss on this task batch = 2.7737e-01, Meta loss averaged over last 500 steps = 2.6940e-01, PNorm = 143.2753, GNorm = 0.2688
Meta loss on this task batch = 3.1446e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 143.2786, GNorm = 0.3215
Meta loss on this task batch = 3.1239e-01, Meta loss averaged over last 500 steps = 2.6958e-01, PNorm = 143.2825, GNorm = 0.2983
Meta loss on this task batch = 2.8164e-01, Meta loss averaged over last 500 steps = 2.6959e-01, PNorm = 143.2871, GNorm = 0.2666
Meta loss on this task batch = 2.4344e-01, Meta loss averaged over last 500 steps = 2.6951e-01, PNorm = 143.2949, GNorm = 0.3238
Meta loss on this task batch = 2.8756e-01, Meta loss averaged over last 500 steps = 2.6962e-01, PNorm = 143.3052, GNorm = 0.2775
Meta loss on this task batch = 2.5678e-01, Meta loss averaged over last 500 steps = 2.6955e-01, PNorm = 143.3129, GNorm = 0.2904
Meta loss on this task batch = 3.1791e-01, Meta loss averaged over last 500 steps = 2.6966e-01, PNorm = 143.3195, GNorm = 0.2950
Meta loss on this task batch = 2.4965e-01, Meta loss averaged over last 500 steps = 2.6967e-01, PNorm = 143.3249, GNorm = 0.3019
Meta loss on this task batch = 2.9537e-01, Meta loss averaged over last 500 steps = 2.6971e-01, PNorm = 143.3307, GNorm = 0.2962
Meta loss on this task batch = 2.0124e-01, Meta loss averaged over last 500 steps = 2.6950e-01, PNorm = 143.3364, GNorm = 0.2416
Meta loss on this task batch = 3.3581e-01, Meta loss averaged over last 500 steps = 2.6965e-01, PNorm = 143.3389, GNorm = 0.4002
Meta loss on this task batch = 2.5279e-01, Meta loss averaged over last 500 steps = 2.6962e-01, PNorm = 143.3409, GNorm = 0.3074
Meta loss on this task batch = 2.7769e-01, Meta loss averaged over last 500 steps = 2.6961e-01, PNorm = 143.3433, GNorm = 0.2604
Meta loss on this task batch = 2.8234e-01, Meta loss averaged over last 500 steps = 2.6958e-01, PNorm = 143.3463, GNorm = 0.3036
Meta loss on this task batch = 2.4394e-01, Meta loss averaged over last 500 steps = 2.6960e-01, PNorm = 143.3478, GNorm = 0.2941
Meta loss on this task batch = 2.5498e-01, Meta loss averaged over last 500 steps = 2.6956e-01, PNorm = 143.3500, GNorm = 0.2898
Meta loss on this task batch = 2.5090e-01, Meta loss averaged over last 500 steps = 2.6964e-01, PNorm = 143.3518, GNorm = 0.2854
Took 145.50064253807068 seconds to complete one epoch of meta training
Took 154.39740180969238 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502522
Epoch 769
Meta loss on this task batch = 2.4684e-01, Meta loss averaged over last 500 steps = 2.6955e-01, PNorm = 143.3527, GNorm = 0.2527
Meta loss on this task batch = 2.4289e-01, Meta loss averaged over last 500 steps = 2.6950e-01, PNorm = 143.3542, GNorm = 0.2671
Meta loss on this task batch = 2.6937e-01, Meta loss averaged over last 500 steps = 2.6936e-01, PNorm = 143.3556, GNorm = 0.2873
Meta loss on this task batch = 2.4013e-01, Meta loss averaged over last 500 steps = 2.6930e-01, PNorm = 143.3571, GNorm = 0.2460
Meta loss on this task batch = 3.0933e-01, Meta loss averaged over last 500 steps = 2.6943e-01, PNorm = 143.3576, GNorm = 0.2769
Meta loss on this task batch = 2.8243e-01, Meta loss averaged over last 500 steps = 2.6936e-01, PNorm = 143.3571, GNorm = 0.3752
Meta loss on this task batch = 2.6445e-01, Meta loss averaged over last 500 steps = 2.6941e-01, PNorm = 143.3554, GNorm = 0.2956
Meta loss on this task batch = 2.8463e-01, Meta loss averaged over last 500 steps = 2.6944e-01, PNorm = 143.3524, GNorm = 0.3271
Meta loss on this task batch = 3.1377e-01, Meta loss averaged over last 500 steps = 2.6940e-01, PNorm = 143.3494, GNorm = 0.2791
Meta loss on this task batch = 2.5448e-01, Meta loss averaged over last 500 steps = 2.6930e-01, PNorm = 143.3467, GNorm = 0.2865
Meta loss on this task batch = 2.2505e-01, Meta loss averaged over last 500 steps = 2.6928e-01, PNorm = 143.3436, GNorm = 0.2589
Meta loss on this task batch = 3.0093e-01, Meta loss averaged over last 500 steps = 2.6937e-01, PNorm = 143.3393, GNorm = 0.3833
Meta loss on this task batch = 2.6051e-01, Meta loss averaged over last 500 steps = 2.6937e-01, PNorm = 143.3376, GNorm = 0.2872
Meta loss on this task batch = 3.0110e-01, Meta loss averaged over last 500 steps = 2.6946e-01, PNorm = 143.3375, GNorm = 0.3535
Meta loss on this task batch = 2.7385e-01, Meta loss averaged over last 500 steps = 2.6948e-01, PNorm = 143.3380, GNorm = 0.2938
Meta loss on this task batch = 2.5850e-01, Meta loss averaged over last 500 steps = 2.6946e-01, PNorm = 143.3409, GNorm = 0.3096
Meta loss on this task batch = 2.5346e-01, Meta loss averaged over last 500 steps = 2.6950e-01, PNorm = 143.3449, GNorm = 0.2971
Meta loss on this task batch = 2.9399e-01, Meta loss averaged over last 500 steps = 2.6957e-01, PNorm = 143.3517, GNorm = 0.3697
Meta loss on this task batch = 2.2400e-01, Meta loss averaged over last 500 steps = 2.6945e-01, PNorm = 143.3584, GNorm = 0.3912
Took 145.7502317428589 seconds to complete one epoch of meta training
Took 153.0752432346344 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478676
Epoch 770
Meta loss on this task batch = 2.2802e-01, Meta loss averaged over last 500 steps = 2.6936e-01, PNorm = 143.3668, GNorm = 0.2831
Meta loss on this task batch = 2.5734e-01, Meta loss averaged over last 500 steps = 2.6934e-01, PNorm = 143.3760, GNorm = 0.2540
Meta loss on this task batch = 2.5330e-01, Meta loss averaged over last 500 steps = 2.6927e-01, PNorm = 143.3863, GNorm = 0.3099
Meta loss on this task batch = 2.8346e-01, Meta loss averaged over last 500 steps = 2.6931e-01, PNorm = 143.3959, GNorm = 0.3302
Meta loss on this task batch = 2.4352e-01, Meta loss averaged over last 500 steps = 2.6923e-01, PNorm = 143.4064, GNorm = 0.2576
Meta loss on this task batch = 2.7014e-01, Meta loss averaged over last 500 steps = 2.6930e-01, PNorm = 143.4153, GNorm = 0.3057
Meta loss on this task batch = 2.0974e-01, Meta loss averaged over last 500 steps = 2.6909e-01, PNorm = 143.4245, GNorm = 0.2503
Meta loss on this task batch = 3.6024e-01, Meta loss averaged over last 500 steps = 2.6920e-01, PNorm = 143.4273, GNorm = 0.4502
Meta loss on this task batch = 2.9425e-01, Meta loss averaged over last 500 steps = 2.6926e-01, PNorm = 143.4293, GNorm = 0.3129
Meta loss on this task batch = 2.6031e-01, Meta loss averaged over last 500 steps = 2.6926e-01, PNorm = 143.4314, GNorm = 0.2864
Meta loss on this task batch = 3.0519e-01, Meta loss averaged over last 500 steps = 2.6929e-01, PNorm = 143.4310, GNorm = 0.3209
Meta loss on this task batch = 2.3375e-01, Meta loss averaged over last 500 steps = 2.6920e-01, PNorm = 143.4313, GNorm = 0.2559
Meta loss on this task batch = 2.4199e-01, Meta loss averaged over last 500 steps = 2.6916e-01, PNorm = 143.4343, GNorm = 0.3252
Meta loss on this task batch = 2.6524e-01, Meta loss averaged over last 500 steps = 2.6911e-01, PNorm = 143.4368, GNorm = 0.2889
Meta loss on this task batch = 2.9125e-01, Meta loss averaged over last 500 steps = 2.6918e-01, PNorm = 143.4395, GNorm = 0.3349
Meta loss on this task batch = 3.0408e-01, Meta loss averaged over last 500 steps = 2.6932e-01, PNorm = 143.4381, GNorm = 0.3146
Meta loss on this task batch = 2.6319e-01, Meta loss averaged over last 500 steps = 2.6933e-01, PNorm = 143.4396, GNorm = 0.2717
Meta loss on this task batch = 2.6251e-01, Meta loss averaged over last 500 steps = 2.6936e-01, PNorm = 143.4424, GNorm = 0.2955
Meta loss on this task batch = 2.9717e-01, Meta loss averaged over last 500 steps = 2.6945e-01, PNorm = 143.4474, GNorm = 0.3273
Took 133.1987042427063 seconds to complete one epoch of meta training
Took 141.80394864082336 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484935
Epoch 771
Meta loss on this task batch = 2.4701e-01, Meta loss averaged over last 500 steps = 2.6934e-01, PNorm = 143.4516, GNorm = 0.2529
Meta loss on this task batch = 2.9465e-01, Meta loss averaged over last 500 steps = 2.6932e-01, PNorm = 143.4575, GNorm = 0.2518
Meta loss on this task batch = 2.7082e-01, Meta loss averaged over last 500 steps = 2.6922e-01, PNorm = 143.4624, GNorm = 0.2729
Meta loss on this task batch = 3.0893e-01, Meta loss averaged over last 500 steps = 2.6924e-01, PNorm = 143.4658, GNorm = 0.2844
Meta loss on this task batch = 2.0291e-01, Meta loss averaged over last 500 steps = 2.6918e-01, PNorm = 143.4694, GNorm = 0.2115
Meta loss on this task batch = 2.6544e-01, Meta loss averaged over last 500 steps = 2.6921e-01, PNorm = 143.4728, GNorm = 0.2804
Meta loss on this task batch = 2.8720e-01, Meta loss averaged over last 500 steps = 2.6933e-01, PNorm = 143.4772, GNorm = 0.3008
Meta loss on this task batch = 2.5544e-01, Meta loss averaged over last 500 steps = 2.6932e-01, PNorm = 143.4814, GNorm = 0.3312
Meta loss on this task batch = 2.9491e-01, Meta loss averaged over last 500 steps = 2.6942e-01, PNorm = 143.4849, GNorm = 0.3123
Meta loss on this task batch = 2.7334e-01, Meta loss averaged over last 500 steps = 2.6944e-01, PNorm = 143.4868, GNorm = 0.3038
Meta loss on this task batch = 3.3493e-01, Meta loss averaged over last 500 steps = 2.6966e-01, PNorm = 143.4879, GNorm = 0.3772
Meta loss on this task batch = 2.9355e-01, Meta loss averaged over last 500 steps = 2.6976e-01, PNorm = 143.4887, GNorm = 0.3265
Meta loss on this task batch = 2.4464e-01, Meta loss averaged over last 500 steps = 2.6960e-01, PNorm = 143.4901, GNorm = 0.2692
Meta loss on this task batch = 2.3686e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 143.4932, GNorm = 0.2457
Meta loss on this task batch = 2.5322e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 143.4951, GNorm = 0.2714
Meta loss on this task batch = 2.4566e-01, Meta loss averaged over last 500 steps = 2.6938e-01, PNorm = 143.4981, GNorm = 0.2945
Meta loss on this task batch = 2.5623e-01, Meta loss averaged over last 500 steps = 2.6942e-01, PNorm = 143.5027, GNorm = 0.2965
Meta loss on this task batch = 2.5627e-01, Meta loss averaged over last 500 steps = 2.6933e-01, PNorm = 143.5079, GNorm = 0.2715
Meta loss on this task batch = 2.7075e-01, Meta loss averaged over last 500 steps = 2.6934e-01, PNorm = 143.5116, GNorm = 0.3571
Took 138.82330060005188 seconds to complete one epoch of meta training
Took 147.2583363056183 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499309
Epoch 772
Meta loss on this task batch = 2.4445e-01, Meta loss averaged over last 500 steps = 2.6932e-01, PNorm = 143.5161, GNorm = 0.2470
Meta loss on this task batch = 2.3851e-01, Meta loss averaged over last 500 steps = 2.6927e-01, PNorm = 143.5217, GNorm = 0.2649
Meta loss on this task batch = 2.7731e-01, Meta loss averaged over last 500 steps = 2.6942e-01, PNorm = 143.5283, GNorm = 0.2944
Meta loss on this task batch = 2.6226e-01, Meta loss averaged over last 500 steps = 2.6946e-01, PNorm = 143.5341, GNorm = 0.2787
Meta loss on this task batch = 2.3707e-01, Meta loss averaged over last 500 steps = 2.6926e-01, PNorm = 143.5402, GNorm = 0.2741
Meta loss on this task batch = 2.6764e-01, Meta loss averaged over last 500 steps = 2.6920e-01, PNorm = 143.5474, GNorm = 0.2875
Meta loss on this task batch = 2.6671e-01, Meta loss averaged over last 500 steps = 2.6917e-01, PNorm = 143.5544, GNorm = 0.2521
Meta loss on this task batch = 2.1862e-01, Meta loss averaged over last 500 steps = 2.6905e-01, PNorm = 143.5607, GNorm = 0.2493
Meta loss on this task batch = 2.8241e-01, Meta loss averaged over last 500 steps = 2.6906e-01, PNorm = 143.5659, GNorm = 0.3084
Meta loss on this task batch = 3.5263e-01, Meta loss averaged over last 500 steps = 2.6924e-01, PNorm = 143.5701, GNorm = 0.4223
Meta loss on this task batch = 2.5428e-01, Meta loss averaged over last 500 steps = 2.6914e-01, PNorm = 143.5748, GNorm = 0.3074
Meta loss on this task batch = 2.4796e-01, Meta loss averaged over last 500 steps = 2.6915e-01, PNorm = 143.5797, GNorm = 0.2670
Meta loss on this task batch = 2.3251e-01, Meta loss averaged over last 500 steps = 2.6905e-01, PNorm = 143.5853, GNorm = 0.2928
Meta loss on this task batch = 3.1110e-01, Meta loss averaged over last 500 steps = 2.6916e-01, PNorm = 143.5899, GNorm = 0.3440
Meta loss on this task batch = 2.5016e-01, Meta loss averaged over last 500 steps = 2.6923e-01, PNorm = 143.5948, GNorm = 0.2866
Meta loss on this task batch = 2.4110e-01, Meta loss averaged over last 500 steps = 2.6917e-01, PNorm = 143.6019, GNorm = 0.3647
Meta loss on this task batch = 2.1549e-01, Meta loss averaged over last 500 steps = 2.6908e-01, PNorm = 143.6097, GNorm = 0.2660
Meta loss on this task batch = 2.6202e-01, Meta loss averaged over last 500 steps = 2.6911e-01, PNorm = 143.6169, GNorm = 0.2593
Meta loss on this task batch = 2.6791e-01, Meta loss averaged over last 500 steps = 2.6899e-01, PNorm = 143.6239, GNorm = 0.3378
Took 129.63413548469543 seconds to complete one epoch of meta training
Took 138.10122513771057 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478521
Epoch 773
Meta loss on this task batch = 2.7991e-01, Meta loss averaged over last 500 steps = 2.6892e-01, PNorm = 143.6296, GNorm = 0.2907
Meta loss on this task batch = 2.5999e-01, Meta loss averaged over last 500 steps = 2.6889e-01, PNorm = 143.6341, GNorm = 0.3109
Meta loss on this task batch = 2.5567e-01, Meta loss averaged over last 500 steps = 2.6892e-01, PNorm = 143.6378, GNorm = 0.2924
Meta loss on this task batch = 2.9402e-01, Meta loss averaged over last 500 steps = 2.6897e-01, PNorm = 143.6400, GNorm = 0.2843
Meta loss on this task batch = 2.0388e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 143.6410, GNorm = 0.2630
Meta loss on this task batch = 2.4785e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 143.6404, GNorm = 0.3352
Meta loss on this task batch = 2.4299e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 143.6396, GNorm = 0.2975
Meta loss on this task batch = 2.5134e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 143.6395, GNorm = 0.2769
Meta loss on this task batch = 2.5496e-01, Meta loss averaged over last 500 steps = 2.6870e-01, PNorm = 143.6379, GNorm = 0.3917
Meta loss on this task batch = 2.6301e-01, Meta loss averaged over last 500 steps = 2.6870e-01, PNorm = 143.6326, GNorm = 0.3287
Meta loss on this task batch = 3.1074e-01, Meta loss averaged over last 500 steps = 2.6878e-01, PNorm = 143.6270, GNorm = 0.3291
Meta loss on this task batch = 2.8633e-01, Meta loss averaged over last 500 steps = 2.6885e-01, PNorm = 143.6186, GNorm = 0.3476
Meta loss on this task batch = 2.4191e-01, Meta loss averaged over last 500 steps = 2.6883e-01, PNorm = 143.6129, GNorm = 0.2755
Meta loss on this task batch = 2.8629e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 143.6078, GNorm = 0.3163
Meta loss on this task batch = 3.2211e-01, Meta loss averaged over last 500 steps = 2.6899e-01, PNorm = 143.6034, GNorm = 0.3147
Meta loss on this task batch = 2.5408e-01, Meta loss averaged over last 500 steps = 2.6895e-01, PNorm = 143.6002, GNorm = 0.2965
Meta loss on this task batch = 2.3394e-01, Meta loss averaged over last 500 steps = 2.6888e-01, PNorm = 143.5981, GNorm = 0.2521
Meta loss on this task batch = 2.8124e-01, Meta loss averaged over last 500 steps = 2.6884e-01, PNorm = 143.5983, GNorm = 0.3019
Meta loss on this task batch = 2.6607e-01, Meta loss averaged over last 500 steps = 2.6887e-01, PNorm = 143.6000, GNorm = 0.3407
Took 151.90261507034302 seconds to complete one epoch of meta training
Took 160.3652777671814 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484304
Epoch 774
Meta loss on this task batch = 2.7189e-01, Meta loss averaged over last 500 steps = 2.6887e-01, PNorm = 143.6007, GNorm = 0.2625
Meta loss on this task batch = 2.5896e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 143.6019, GNorm = 0.2523
Meta loss on this task batch = 2.3213e-01, Meta loss averaged over last 500 steps = 2.6889e-01, PNorm = 143.6054, GNorm = 0.2543
Meta loss on this task batch = 2.4144e-01, Meta loss averaged over last 500 steps = 2.6883e-01, PNorm = 143.6084, GNorm = 0.2564
Meta loss on this task batch = 2.4913e-01, Meta loss averaged over last 500 steps = 2.6877e-01, PNorm = 143.6129, GNorm = 0.2620
Meta loss on this task batch = 3.2990e-01, Meta loss averaged over last 500 steps = 2.6904e-01, PNorm = 143.6162, GNorm = 0.3717
Meta loss on this task batch = 3.4048e-01, Meta loss averaged over last 500 steps = 2.6917e-01, PNorm = 143.6198, GNorm = 0.3799
Meta loss on this task batch = 2.8906e-01, Meta loss averaged over last 500 steps = 2.6919e-01, PNorm = 143.6240, GNorm = 0.3118
Meta loss on this task batch = 2.5899e-01, Meta loss averaged over last 500 steps = 2.6916e-01, PNorm = 143.6271, GNorm = 0.3294
Meta loss on this task batch = 2.7075e-01, Meta loss averaged over last 500 steps = 2.6915e-01, PNorm = 143.6288, GNorm = 0.3417
Meta loss on this task batch = 2.6398e-01, Meta loss averaged over last 500 steps = 2.6918e-01, PNorm = 143.6307, GNorm = 0.2987
Meta loss on this task batch = 3.0187e-01, Meta loss averaged over last 500 steps = 2.6929e-01, PNorm = 143.6315, GNorm = 0.3381
Meta loss on this task batch = 2.5850e-01, Meta loss averaged over last 500 steps = 2.6940e-01, PNorm = 143.6316, GNorm = 0.3380
Meta loss on this task batch = 2.6587e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 143.6328, GNorm = 0.3572
Meta loss on this task batch = 2.7166e-01, Meta loss averaged over last 500 steps = 2.6957e-01, PNorm = 143.6376, GNorm = 0.3312
Meta loss on this task batch = 3.2302e-01, Meta loss averaged over last 500 steps = 2.6964e-01, PNorm = 143.6416, GNorm = 0.3223
Meta loss on this task batch = 2.4028e-01, Meta loss averaged over last 500 steps = 2.6961e-01, PNorm = 143.6476, GNorm = 0.2618
Meta loss on this task batch = 2.4296e-01, Meta loss averaged over last 500 steps = 2.6956e-01, PNorm = 143.6560, GNorm = 0.2627
Meta loss on this task batch = 2.5081e-01, Meta loss averaged over last 500 steps = 2.6948e-01, PNorm = 143.6643, GNorm = 0.3162
Took 130.25280594825745 seconds to complete one epoch of meta training
Took 138.91987895965576 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491282
Epoch 775
Meta loss on this task batch = 3.2514e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 143.6718, GNorm = 0.2799
Meta loss on this task batch = 2.6795e-01, Meta loss averaged over last 500 steps = 2.6938e-01, PNorm = 143.6765, GNorm = 0.2958
Meta loss on this task batch = 2.4924e-01, Meta loss averaged over last 500 steps = 2.6940e-01, PNorm = 143.6808, GNorm = 0.2578
Meta loss on this task batch = 2.4533e-01, Meta loss averaged over last 500 steps = 2.6919e-01, PNorm = 143.6831, GNorm = 0.2864
Meta loss on this task batch = 2.8896e-01, Meta loss averaged over last 500 steps = 2.6930e-01, PNorm = 143.6856, GNorm = 0.2751
Meta loss on this task batch = 2.7535e-01, Meta loss averaged over last 500 steps = 2.6923e-01, PNorm = 143.6883, GNorm = 0.2476
Meta loss on this task batch = 2.8636e-01, Meta loss averaged over last 500 steps = 2.6923e-01, PNorm = 143.6914, GNorm = 0.2587
Meta loss on this task batch = 2.8791e-01, Meta loss averaged over last 500 steps = 2.6933e-01, PNorm = 143.6942, GNorm = 0.3250
Meta loss on this task batch = 2.4015e-01, Meta loss averaged over last 500 steps = 2.6921e-01, PNorm = 143.6969, GNorm = 0.2672
Meta loss on this task batch = 2.9765e-01, Meta loss averaged over last 500 steps = 2.6919e-01, PNorm = 143.7003, GNorm = 0.2849
Meta loss on this task batch = 2.4901e-01, Meta loss averaged over last 500 steps = 2.6927e-01, PNorm = 143.7045, GNorm = 0.2557
Meta loss on this task batch = 3.0185e-01, Meta loss averaged over last 500 steps = 2.6934e-01, PNorm = 143.7119, GNorm = 0.3463
Meta loss on this task batch = 2.6578e-01, Meta loss averaged over last 500 steps = 2.6933e-01, PNorm = 143.7209, GNorm = 0.2893
Meta loss on this task batch = 2.6961e-01, Meta loss averaged over last 500 steps = 2.6941e-01, PNorm = 143.7317, GNorm = 0.2889
Meta loss on this task batch = 2.8861e-01, Meta loss averaged over last 500 steps = 2.6932e-01, PNorm = 143.7417, GNorm = 0.2871
Meta loss on this task batch = 2.9037e-01, Meta loss averaged over last 500 steps = 2.6930e-01, PNorm = 143.7498, GNorm = 0.2862
Meta loss on this task batch = 2.8801e-01, Meta loss averaged over last 500 steps = 2.6939e-01, PNorm = 143.7572, GNorm = 0.2882
Meta loss on this task batch = 2.4092e-01, Meta loss averaged over last 500 steps = 2.6933e-01, PNorm = 143.7642, GNorm = 0.2830
Meta loss on this task batch = 2.9853e-01, Meta loss averaged over last 500 steps = 2.6931e-01, PNorm = 143.7682, GNorm = 0.3606
Took 130.77030658721924 seconds to complete one epoch of meta training
Took 137.83177590370178 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474120
Epoch 776
Meta loss on this task batch = 2.6767e-01, Meta loss averaged over last 500 steps = 2.6941e-01, PNorm = 143.7722, GNorm = 0.2879
Meta loss on this task batch = 2.5730e-01, Meta loss averaged over last 500 steps = 2.6941e-01, PNorm = 143.7773, GNorm = 0.3176
Meta loss on this task batch = 3.4167e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 143.7815, GNorm = 0.4095
Meta loss on this task batch = 2.7672e-01, Meta loss averaged over last 500 steps = 2.6965e-01, PNorm = 143.7848, GNorm = 0.3433
Meta loss on this task batch = 2.6155e-01, Meta loss averaged over last 500 steps = 2.6950e-01, PNorm = 143.7872, GNorm = 0.2796
Meta loss on this task batch = 2.4528e-01, Meta loss averaged over last 500 steps = 2.6939e-01, PNorm = 143.7908, GNorm = 0.2909
Meta loss on this task batch = 2.7834e-01, Meta loss averaged over last 500 steps = 2.6951e-01, PNorm = 143.7941, GNorm = 0.2830
Meta loss on this task batch = 2.4782e-01, Meta loss averaged over last 500 steps = 2.6943e-01, PNorm = 143.7973, GNorm = 0.2742
Meta loss on this task batch = 2.9002e-01, Meta loss averaged over last 500 steps = 2.6944e-01, PNorm = 143.7997, GNorm = 0.3082
Meta loss on this task batch = 2.8622e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 143.8031, GNorm = 0.2784
Meta loss on this task batch = 2.2537e-01, Meta loss averaged over last 500 steps = 2.6937e-01, PNorm = 143.8069, GNorm = 0.3044
Meta loss on this task batch = 2.1141e-01, Meta loss averaged over last 500 steps = 2.6921e-01, PNorm = 143.8116, GNorm = 0.2515
Meta loss on this task batch = 2.6947e-01, Meta loss averaged over last 500 steps = 2.6914e-01, PNorm = 143.8151, GNorm = 0.2986
Meta loss on this task batch = 2.3002e-01, Meta loss averaged over last 500 steps = 2.6898e-01, PNorm = 143.8195, GNorm = 0.2561
Meta loss on this task batch = 2.6991e-01, Meta loss averaged over last 500 steps = 2.6898e-01, PNorm = 143.8256, GNorm = 0.3173
Meta loss on this task batch = 2.4585e-01, Meta loss averaged over last 500 steps = 2.6885e-01, PNorm = 143.8321, GNorm = 0.2552
Meta loss on this task batch = 2.5250e-01, Meta loss averaged over last 500 steps = 2.6882e-01, PNorm = 143.8395, GNorm = 0.2424
Meta loss on this task batch = 3.2355e-01, Meta loss averaged over last 500 steps = 2.6888e-01, PNorm = 143.8491, GNorm = 0.3145
Meta loss on this task batch = 2.2649e-01, Meta loss averaged over last 500 steps = 2.6883e-01, PNorm = 143.8611, GNorm = 0.3047
Took 130.33889389038086 seconds to complete one epoch of meta training
Took 138.68918800354004 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475786
Epoch 777
Meta loss on this task batch = 2.5253e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 143.8718, GNorm = 0.2608
Meta loss on this task batch = 3.0795e-01, Meta loss averaged over last 500 steps = 2.6882e-01, PNorm = 143.8791, GNorm = 0.2812
Meta loss on this task batch = 2.6218e-01, Meta loss averaged over last 500 steps = 2.6876e-01, PNorm = 143.8859, GNorm = 0.2600
Meta loss on this task batch = 3.1765e-01, Meta loss averaged over last 500 steps = 2.6895e-01, PNorm = 143.8914, GNorm = 0.2763
Meta loss on this task batch = 2.5200e-01, Meta loss averaged over last 500 steps = 2.6893e-01, PNorm = 143.8959, GNorm = 0.2610
Meta loss on this task batch = 2.7355e-01, Meta loss averaged over last 500 steps = 2.6904e-01, PNorm = 143.8996, GNorm = 0.3291
Meta loss on this task batch = 2.4458e-01, Meta loss averaged over last 500 steps = 2.6904e-01, PNorm = 143.9025, GNorm = 0.2596
Meta loss on this task batch = 2.7940e-01, Meta loss averaged over last 500 steps = 2.6899e-01, PNorm = 143.9038, GNorm = 0.3112
Meta loss on this task batch = 2.6492e-01, Meta loss averaged over last 500 steps = 2.6910e-01, PNorm = 143.9041, GNorm = 0.3072
Meta loss on this task batch = 2.8139e-01, Meta loss averaged over last 500 steps = 2.6907e-01, PNorm = 143.9036, GNorm = 0.3179
Meta loss on this task batch = 2.4017e-01, Meta loss averaged over last 500 steps = 2.6904e-01, PNorm = 143.9037, GNorm = 0.3530
Meta loss on this task batch = 2.7900e-01, Meta loss averaged over last 500 steps = 2.6897e-01, PNorm = 143.9035, GNorm = 0.2851
Meta loss on this task batch = 2.6856e-01, Meta loss averaged over last 500 steps = 2.6902e-01, PNorm = 143.9040, GNorm = 0.2709
Meta loss on this task batch = 2.4811e-01, Meta loss averaged over last 500 steps = 2.6899e-01, PNorm = 143.9064, GNorm = 0.2935
Meta loss on this task batch = 2.3927e-01, Meta loss averaged over last 500 steps = 2.6894e-01, PNorm = 143.9110, GNorm = 0.2485
Meta loss on this task batch = 2.3007e-01, Meta loss averaged over last 500 steps = 2.6887e-01, PNorm = 143.9159, GNorm = 0.2698
Meta loss on this task batch = 2.1471e-01, Meta loss averaged over last 500 steps = 2.6876e-01, PNorm = 143.9212, GNorm = 0.2724
Meta loss on this task batch = 2.5424e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 143.9253, GNorm = 0.2858
Meta loss on this task batch = 2.5917e-01, Meta loss averaged over last 500 steps = 2.6854e-01, PNorm = 143.9281, GNorm = 0.3380
Took 129.8486921787262 seconds to complete one epoch of meta training
Took 138.22433280944824 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507573
Epoch 778
Meta loss on this task batch = 3.1047e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 143.9328, GNorm = 0.3228
Meta loss on this task batch = 2.6084e-01, Meta loss averaged over last 500 steps = 2.6863e-01, PNorm = 143.9392, GNorm = 0.2945
Meta loss on this task batch = 3.0118e-01, Meta loss averaged over last 500 steps = 2.6868e-01, PNorm = 143.9436, GNorm = 0.3089
Meta loss on this task batch = 2.4157e-01, Meta loss averaged over last 500 steps = 2.6871e-01, PNorm = 143.9480, GNorm = 0.3158
Meta loss on this task batch = 2.6385e-01, Meta loss averaged over last 500 steps = 2.6858e-01, PNorm = 143.9528, GNorm = 0.2930
Meta loss on this task batch = 2.2602e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 143.9582, GNorm = 0.2484
Meta loss on this task batch = 2.7672e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 143.9624, GNorm = 0.2878
Meta loss on this task batch = 1.8785e-01, Meta loss averaged over last 500 steps = 2.6849e-01, PNorm = 143.9672, GNorm = 0.2442
Meta loss on this task batch = 2.6354e-01, Meta loss averaged over last 500 steps = 2.6847e-01, PNorm = 143.9706, GNorm = 0.2648
Meta loss on this task batch = 2.5396e-01, Meta loss averaged over last 500 steps = 2.6850e-01, PNorm = 143.9736, GNorm = 0.2836
Meta loss on this task batch = 2.8462e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 143.9772, GNorm = 0.3231
Meta loss on this task batch = 3.2634e-01, Meta loss averaged over last 500 steps = 2.6866e-01, PNorm = 143.9796, GNorm = 0.3114
Meta loss on this task batch = 3.4452e-01, Meta loss averaged over last 500 steps = 2.6876e-01, PNorm = 143.9790, GNorm = 0.4268
Meta loss on this task batch = 2.7890e-01, Meta loss averaged over last 500 steps = 2.6875e-01, PNorm = 143.9789, GNorm = 0.3150
Meta loss on this task batch = 2.7081e-01, Meta loss averaged over last 500 steps = 2.6873e-01, PNorm = 143.9767, GNorm = 0.2889
Meta loss on this task batch = 2.7024e-01, Meta loss averaged over last 500 steps = 2.6877e-01, PNorm = 143.9764, GNorm = 0.2915
Meta loss on this task batch = 2.4053e-01, Meta loss averaged over last 500 steps = 2.6876e-01, PNorm = 143.9760, GNorm = 0.2999
Meta loss on this task batch = 2.6483e-01, Meta loss averaged over last 500 steps = 2.6866e-01, PNorm = 143.9765, GNorm = 0.2798
Meta loss on this task batch = 2.2125e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 143.9803, GNorm = 0.3412
Took 131.74708127975464 seconds to complete one epoch of meta training
Took 140.60221529006958 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500590
Epoch 779
Meta loss on this task batch = 2.7289e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 143.9847, GNorm = 0.3459
Meta loss on this task batch = 2.7811e-01, Meta loss averaged over last 500 steps = 2.6866e-01, PNorm = 143.9876, GNorm = 0.2804
Meta loss on this task batch = 3.1277e-01, Meta loss averaged over last 500 steps = 2.6866e-01, PNorm = 143.9907, GNorm = 0.2785
Meta loss on this task batch = 2.6560e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 143.9949, GNorm = 0.2657
Meta loss on this task batch = 2.3437e-01, Meta loss averaged over last 500 steps = 2.6848e-01, PNorm = 143.9998, GNorm = 0.2760
Meta loss on this task batch = 2.8598e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 144.0047, GNorm = 0.2968
Meta loss on this task batch = 2.7634e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 144.0090, GNorm = 0.2864
Meta loss on this task batch = 2.7349e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 144.0134, GNorm = 0.2923
Meta loss on this task batch = 1.6017e-01, Meta loss averaged over last 500 steps = 2.6841e-01, PNorm = 144.0180, GNorm = 0.2067
Meta loss on this task batch = 2.3272e-01, Meta loss averaged over last 500 steps = 2.6820e-01, PNorm = 144.0229, GNorm = 0.2567
Meta loss on this task batch = 2.6434e-01, Meta loss averaged over last 500 steps = 2.6823e-01, PNorm = 144.0259, GNorm = 0.2526
Meta loss on this task batch = 2.0513e-01, Meta loss averaged over last 500 steps = 2.6809e-01, PNorm = 144.0303, GNorm = 0.2323
Meta loss on this task batch = 3.3031e-01, Meta loss averaged over last 500 steps = 2.6824e-01, PNorm = 144.0323, GNorm = 0.3474
Meta loss on this task batch = 2.4951e-01, Meta loss averaged over last 500 steps = 2.6823e-01, PNorm = 144.0317, GNorm = 0.2885
Meta loss on this task batch = 2.6741e-01, Meta loss averaged over last 500 steps = 2.6827e-01, PNorm = 144.0318, GNorm = 0.2708
Meta loss on this task batch = 2.9337e-01, Meta loss averaged over last 500 steps = 2.6840e-01, PNorm = 144.0334, GNorm = 0.3375
Meta loss on this task batch = 2.4021e-01, Meta loss averaged over last 500 steps = 2.6835e-01, PNorm = 144.0336, GNorm = 0.2937
Meta loss on this task batch = 2.8257e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 144.0343, GNorm = 0.3221
Meta loss on this task batch = 2.8981e-01, Meta loss averaged over last 500 steps = 2.6844e-01, PNorm = 144.0372, GNorm = 0.3347
Took 130.08383536338806 seconds to complete one epoch of meta training
Took 138.3975009918213 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503038
Epoch 780
Meta loss on this task batch = 2.8338e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 144.0416, GNorm = 0.3138
Meta loss on this task batch = 2.8214e-01, Meta loss averaged over last 500 steps = 2.6847e-01, PNorm = 144.0446, GNorm = 0.3067
Meta loss on this task batch = 2.6275e-01, Meta loss averaged over last 500 steps = 2.6844e-01, PNorm = 144.0467, GNorm = 0.3226
Meta loss on this task batch = 2.6298e-01, Meta loss averaged over last 500 steps = 2.6839e-01, PNorm = 144.0488, GNorm = 0.2886
Meta loss on this task batch = 3.6586e-01, Meta loss averaged over last 500 steps = 2.6867e-01, PNorm = 144.0498, GNorm = 0.3703
Meta loss on this task batch = 2.4480e-01, Meta loss averaged over last 500 steps = 2.6860e-01, PNorm = 144.0529, GNorm = 0.2727
Meta loss on this task batch = 3.0959e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 144.0543, GNorm = 0.2950
Meta loss on this task batch = 2.5378e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 144.0540, GNorm = 0.2978
Meta loss on this task batch = 2.8641e-01, Meta loss averaged over last 500 steps = 2.6877e-01, PNorm = 144.0547, GNorm = 0.3318
Meta loss on this task batch = 2.5763e-01, Meta loss averaged over last 500 steps = 2.6884e-01, PNorm = 144.0572, GNorm = 0.2680
Meta loss on this task batch = 3.0325e-01, Meta loss averaged over last 500 steps = 2.6887e-01, PNorm = 144.0605, GNorm = 0.2961
Meta loss on this task batch = 2.5802e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 144.0644, GNorm = 0.2545
Meta loss on this task batch = 3.1976e-01, Meta loss averaged over last 500 steps = 2.6896e-01, PNorm = 144.0702, GNorm = 0.3663
Meta loss on this task batch = 2.6465e-01, Meta loss averaged over last 500 steps = 2.6894e-01, PNorm = 144.0764, GNorm = 0.2755
Meta loss on this task batch = 2.4960e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 144.0838, GNorm = 0.2578
Meta loss on this task batch = 2.3030e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 144.0926, GNorm = 0.2516
Meta loss on this task batch = 2.7138e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 144.1017, GNorm = 0.2610
Meta loss on this task batch = 2.5913e-01, Meta loss averaged over last 500 steps = 2.6874e-01, PNorm = 144.1107, GNorm = 0.2778
Meta loss on this task batch = 3.0015e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 144.1194, GNorm = 0.3288
Took 130.25237917900085 seconds to complete one epoch of meta training
Took 139.01223945617676 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503311
Epoch 781
Meta loss on this task batch = 2.6769e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 144.1259, GNorm = 0.2529
Meta loss on this task batch = 2.7727e-01, Meta loss averaged over last 500 steps = 2.6885e-01, PNorm = 144.1305, GNorm = 0.3090
Meta loss on this task batch = 3.0378e-01, Meta loss averaged over last 500 steps = 2.6899e-01, PNorm = 144.1325, GNorm = 0.3105
Meta loss on this task batch = 3.3762e-01, Meta loss averaged over last 500 steps = 2.6913e-01, PNorm = 144.1318, GNorm = 0.3731
Meta loss on this task batch = 2.0260e-01, Meta loss averaged over last 500 steps = 2.6891e-01, PNorm = 144.1327, GNorm = 0.2608
Meta loss on this task batch = 2.2768e-01, Meta loss averaged over last 500 steps = 2.6884e-01, PNorm = 144.1348, GNorm = 0.2796
Meta loss on this task batch = 2.4998e-01, Meta loss averaged over last 500 steps = 2.6877e-01, PNorm = 144.1382, GNorm = 0.2594
Meta loss on this task batch = 2.1436e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 144.1440, GNorm = 0.2996
Meta loss on this task batch = 2.0598e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 144.1510, GNorm = 0.2470
Meta loss on this task batch = 2.7664e-01, Meta loss averaged over last 500 steps = 2.6857e-01, PNorm = 144.1576, GNorm = 0.2760
Meta loss on this task batch = 2.7668e-01, Meta loss averaged over last 500 steps = 2.6852e-01, PNorm = 144.1657, GNorm = 0.3303
Meta loss on this task batch = 2.5895e-01, Meta loss averaged over last 500 steps = 2.6848e-01, PNorm = 144.1745, GNorm = 0.3064
Meta loss on this task batch = 3.1806e-01, Meta loss averaged over last 500 steps = 2.6852e-01, PNorm = 144.1847, GNorm = 0.3126
Meta loss on this task batch = 2.9020e-01, Meta loss averaged over last 500 steps = 2.6851e-01, PNorm = 144.1951, GNorm = 0.2855
Meta loss on this task batch = 2.7524e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 144.2046, GNorm = 0.3051
Meta loss on this task batch = 2.8143e-01, Meta loss averaged over last 500 steps = 2.6851e-01, PNorm = 144.2134, GNorm = 0.3725
Meta loss on this task batch = 2.9904e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 144.2203, GNorm = 0.3215
Meta loss on this task batch = 3.2916e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 144.2236, GNorm = 0.3387
Meta loss on this task batch = 3.1010e-01, Meta loss averaged over last 500 steps = 2.6875e-01, PNorm = 144.2257, GNorm = 0.3744
Took 131.66507148742676 seconds to complete one epoch of meta training
Took 140.27129364013672 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507311
Epoch 782
Meta loss on this task batch = 2.6803e-01, Meta loss averaged over last 500 steps = 2.6876e-01, PNorm = 144.2271, GNorm = 0.2936
Meta loss on this task batch = 3.0462e-01, Meta loss averaged over last 500 steps = 2.6888e-01, PNorm = 144.2287, GNorm = 0.2825
Meta loss on this task batch = 2.9032e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 144.2299, GNorm = 0.3032
Meta loss on this task batch = 2.9300e-01, Meta loss averaged over last 500 steps = 2.6902e-01, PNorm = 144.2314, GNorm = 0.3276
Meta loss on this task batch = 2.6535e-01, Meta loss averaged over last 500 steps = 2.6899e-01, PNorm = 144.2327, GNorm = 0.2884
Meta loss on this task batch = 2.5867e-01, Meta loss averaged over last 500 steps = 2.6892e-01, PNorm = 144.2346, GNorm = 0.2860
Meta loss on this task batch = 2.6733e-01, Meta loss averaged over last 500 steps = 2.6895e-01, PNorm = 144.2375, GNorm = 0.2815
Meta loss on this task batch = 2.7549e-01, Meta loss averaged over last 500 steps = 2.6888e-01, PNorm = 144.2410, GNorm = 0.2858
Meta loss on this task batch = 2.6215e-01, Meta loss averaged over last 500 steps = 2.6883e-01, PNorm = 144.2432, GNorm = 0.2720
Meta loss on this task batch = 2.4451e-01, Meta loss averaged over last 500 steps = 2.6883e-01, PNorm = 144.2459, GNorm = 0.2452
Meta loss on this task batch = 3.1500e-01, Meta loss averaged over last 500 steps = 2.6895e-01, PNorm = 144.2489, GNorm = 0.3317
Meta loss on this task batch = 1.9941e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 144.2538, GNorm = 0.2417
Meta loss on this task batch = 2.6323e-01, Meta loss averaged over last 500 steps = 2.6889e-01, PNorm = 144.2599, GNorm = 0.3062
Meta loss on this task batch = 2.6122e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 144.2676, GNorm = 0.2738
Meta loss on this task batch = 2.4113e-01, Meta loss averaged over last 500 steps = 2.6879e-01, PNorm = 144.2758, GNorm = 0.2723
Meta loss on this task batch = 2.5857e-01, Meta loss averaged over last 500 steps = 2.6879e-01, PNorm = 144.2838, GNorm = 0.3071
Meta loss on this task batch = 2.4904e-01, Meta loss averaged over last 500 steps = 2.6875e-01, PNorm = 144.2927, GNorm = 0.2836
Meta loss on this task batch = 2.6566e-01, Meta loss averaged over last 500 steps = 2.6873e-01, PNorm = 144.3015, GNorm = 0.2863
Meta loss on this task batch = 3.2439e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 144.3075, GNorm = 0.3996
Took 131.9373424053192 seconds to complete one epoch of meta training
Took 140.3779273033142 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476229
Epoch 783
Meta loss on this task batch = 2.0016e-01, Meta loss averaged over last 500 steps = 2.6867e-01, PNorm = 144.3128, GNorm = 0.2749
Meta loss on this task batch = 2.4647e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 144.3188, GNorm = 0.2368
Meta loss on this task batch = 2.3612e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 144.3260, GNorm = 0.2821
Meta loss on this task batch = 2.5084e-01, Meta loss averaged over last 500 steps = 2.6851e-01, PNorm = 144.3331, GNorm = 0.2974
Meta loss on this task batch = 3.4846e-01, Meta loss averaged over last 500 steps = 2.6862e-01, PNorm = 144.3381, GNorm = 0.3324
Meta loss on this task batch = 2.8749e-01, Meta loss averaged over last 500 steps = 2.6868e-01, PNorm = 144.3409, GNorm = 0.3244
Meta loss on this task batch = 2.4646e-01, Meta loss averaged over last 500 steps = 2.6875e-01, PNorm = 144.3415, GNorm = 0.3091
Meta loss on this task batch = 3.1045e-01, Meta loss averaged over last 500 steps = 2.6876e-01, PNorm = 144.3418, GNorm = 0.3143
Meta loss on this task batch = 2.7741e-01, Meta loss averaged over last 500 steps = 2.6873e-01, PNorm = 144.3421, GNorm = 0.3075
Meta loss on this task batch = 2.5515e-01, Meta loss averaged over last 500 steps = 2.6871e-01, PNorm = 144.3414, GNorm = 0.2655
Meta loss on this task batch = 2.7598e-01, Meta loss averaged over last 500 steps = 2.6880e-01, PNorm = 144.3410, GNorm = 0.2882
Meta loss on this task batch = 3.1063e-01, Meta loss averaged over last 500 steps = 2.6889e-01, PNorm = 144.3409, GNorm = 0.3293
Meta loss on this task batch = 2.8390e-01, Meta loss averaged over last 500 steps = 2.6901e-01, PNorm = 144.3411, GNorm = 0.3110
Meta loss on this task batch = 2.3713e-01, Meta loss averaged over last 500 steps = 2.6896e-01, PNorm = 144.3425, GNorm = 0.2612
Meta loss on this task batch = 2.4373e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 144.3448, GNorm = 0.2643
Meta loss on this task batch = 2.9683e-01, Meta loss averaged over last 500 steps = 2.6887e-01, PNorm = 144.3458, GNorm = 0.3797
Meta loss on this task batch = 2.7436e-01, Meta loss averaged over last 500 steps = 2.6894e-01, PNorm = 144.3469, GNorm = 0.2636
Meta loss on this task batch = 2.5086e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 144.3478, GNorm = 0.2753
Meta loss on this task batch = 2.7436e-01, Meta loss averaged over last 500 steps = 2.6889e-01, PNorm = 144.3486, GNorm = 0.3380
Took 128.4613265991211 seconds to complete one epoch of meta training
Took 136.665629863739 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511827
Epoch 784
Meta loss on this task batch = 2.4217e-01, Meta loss averaged over last 500 steps = 2.6876e-01, PNorm = 144.3479, GNorm = 0.2826
Meta loss on this task batch = 2.5589e-01, Meta loss averaged over last 500 steps = 2.6870e-01, PNorm = 144.3471, GNorm = 0.2730
Meta loss on this task batch = 3.3766e-01, Meta loss averaged over last 500 steps = 2.6873e-01, PNorm = 144.3431, GNorm = 0.3617
Meta loss on this task batch = 2.4540e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 144.3399, GNorm = 0.2879
Meta loss on this task batch = 2.3706e-01, Meta loss averaged over last 500 steps = 2.6860e-01, PNorm = 144.3391, GNorm = 0.2376
Meta loss on this task batch = 2.6016e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 144.3392, GNorm = 0.2568
Meta loss on this task batch = 2.7091e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 144.3387, GNorm = 0.3000
Meta loss on this task batch = 2.6249e-01, Meta loss averaged over last 500 steps = 2.6857e-01, PNorm = 144.3396, GNorm = 0.2833
Meta loss on this task batch = 2.4112e-01, Meta loss averaged over last 500 steps = 2.6846e-01, PNorm = 144.3422, GNorm = 0.2896
Meta loss on this task batch = 2.5386e-01, Meta loss averaged over last 500 steps = 2.6840e-01, PNorm = 144.3448, GNorm = 0.2771
Meta loss on this task batch = 3.2022e-01, Meta loss averaged over last 500 steps = 2.6858e-01, PNorm = 144.3488, GNorm = 0.3455
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 2.6849e-01, PNorm = 144.3539, GNorm = 0.2823
Meta loss on this task batch = 3.0524e-01, Meta loss averaged over last 500 steps = 2.6862e-01, PNorm = 144.3611, GNorm = 0.3253
Meta loss on this task batch = 2.8242e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 144.3671, GNorm = 0.2866
Meta loss on this task batch = 3.1114e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 144.3747, GNorm = 0.3145
Meta loss on this task batch = 2.6658e-01, Meta loss averaged over last 500 steps = 2.6867e-01, PNorm = 144.3828, GNorm = 0.3168
Meta loss on this task batch = 2.7029e-01, Meta loss averaged over last 500 steps = 2.6873e-01, PNorm = 144.3893, GNorm = 0.3029
Meta loss on this task batch = 2.3041e-01, Meta loss averaged over last 500 steps = 2.6868e-01, PNorm = 144.3966, GNorm = 0.2517
Meta loss on this task batch = 3.0531e-01, Meta loss averaged over last 500 steps = 2.6887e-01, PNorm = 144.4035, GNorm = 0.3414
Took 132.2480652332306 seconds to complete one epoch of meta training
Took 140.71878361701965 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467244
Epoch 785
Meta loss on this task batch = 2.7788e-01, Meta loss averaged over last 500 steps = 2.6892e-01, PNorm = 144.4070, GNorm = 0.3120
Meta loss on this task batch = 2.7665e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 144.4101, GNorm = 0.3177
Meta loss on this task batch = 2.9063e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 144.4128, GNorm = 0.2764
Meta loss on this task batch = 2.8712e-01, Meta loss averaged over last 500 steps = 2.6884e-01, PNorm = 144.4137, GNorm = 0.3142
Meta loss on this task batch = 2.2789e-01, Meta loss averaged over last 500 steps = 2.6880e-01, PNorm = 144.4136, GNorm = 0.3193
Meta loss on this task batch = 2.6338e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 144.4127, GNorm = 0.2974
Meta loss on this task batch = 2.8668e-01, Meta loss averaged over last 500 steps = 2.6868e-01, PNorm = 144.4098, GNorm = 0.3108
Meta loss on this task batch = 1.9641e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 144.4091, GNorm = 0.2037
Meta loss on this task batch = 2.2125e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 144.4096, GNorm = 0.2594
Meta loss on this task batch = 2.6556e-01, Meta loss averaged over last 500 steps = 2.6838e-01, PNorm = 144.4105, GNorm = 0.3056
Meta loss on this task batch = 2.2807e-01, Meta loss averaged over last 500 steps = 2.6840e-01, PNorm = 144.4120, GNorm = 0.2563
Meta loss on this task batch = 2.5992e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 144.4136, GNorm = 0.2769
Meta loss on this task batch = 2.9424e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 144.4154, GNorm = 0.2797
Meta loss on this task batch = 2.0556e-01, Meta loss averaged over last 500 steps = 2.6816e-01, PNorm = 144.4176, GNorm = 0.2575
Meta loss on this task batch = 2.9296e-01, Meta loss averaged over last 500 steps = 2.6822e-01, PNorm = 144.4192, GNorm = 0.2700
Meta loss on this task batch = 2.2827e-01, Meta loss averaged over last 500 steps = 2.6807e-01, PNorm = 144.4205, GNorm = 0.2896
Meta loss on this task batch = 2.8406e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 144.4218, GNorm = 0.3451
Meta loss on this task batch = 3.1409e-01, Meta loss averaged over last 500 steps = 2.6811e-01, PNorm = 144.4227, GNorm = 0.3958
Meta loss on this task batch = 3.2592e-01, Meta loss averaged over last 500 steps = 2.6837e-01, PNorm = 144.4249, GNorm = 0.3587
Took 131.45048475265503 seconds to complete one epoch of meta training
Took 140.3813247680664 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481811
Epoch 786
Meta loss on this task batch = 2.1753e-01, Meta loss averaged over last 500 steps = 2.6828e-01, PNorm = 144.4280, GNorm = 0.2418
Meta loss on this task batch = 2.5956e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 144.4315, GNorm = 0.2832
Meta loss on this task batch = 2.6388e-01, Meta loss averaged over last 500 steps = 2.6831e-01, PNorm = 144.4339, GNorm = 0.2929
Meta loss on this task batch = 2.4884e-01, Meta loss averaged over last 500 steps = 2.6828e-01, PNorm = 144.4373, GNorm = 0.2728
Meta loss on this task batch = 2.2705e-01, Meta loss averaged over last 500 steps = 2.6823e-01, PNorm = 144.4419, GNorm = 0.2667
Meta loss on this task batch = 2.7005e-01, Meta loss averaged over last 500 steps = 2.6839e-01, PNorm = 144.4469, GNorm = 0.2594
Meta loss on this task batch = 2.7660e-01, Meta loss averaged over last 500 steps = 2.6854e-01, PNorm = 144.4525, GNorm = 0.2654
Meta loss on this task batch = 2.2303e-01, Meta loss averaged over last 500 steps = 2.6851e-01, PNorm = 144.4573, GNorm = 0.2670
Meta loss on this task batch = 3.1462e-01, Meta loss averaged over last 500 steps = 2.6853e-01, PNorm = 144.4623, GNorm = 0.2866
Meta loss on this task batch = 2.7260e-01, Meta loss averaged over last 500 steps = 2.6860e-01, PNorm = 144.4666, GNorm = 0.2973
Meta loss on this task batch = 2.8989e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 144.4713, GNorm = 0.3253
Meta loss on this task batch = 2.8799e-01, Meta loss averaged over last 500 steps = 2.6882e-01, PNorm = 144.4766, GNorm = 0.2965
Meta loss on this task batch = 2.6395e-01, Meta loss averaged over last 500 steps = 2.6874e-01, PNorm = 144.4810, GNorm = 0.3039
Meta loss on this task batch = 3.4285e-01, Meta loss averaged over last 500 steps = 2.6898e-01, PNorm = 144.4861, GNorm = 0.3350
Meta loss on this task batch = 1.9456e-01, Meta loss averaged over last 500 steps = 2.6883e-01, PNorm = 144.4902, GNorm = 0.2362
Meta loss on this task batch = 3.1613e-01, Meta loss averaged over last 500 steps = 2.6901e-01, PNorm = 144.4931, GNorm = 0.3345
Meta loss on this task batch = 2.5020e-01, Meta loss averaged over last 500 steps = 2.6889e-01, PNorm = 144.4966, GNorm = 0.2814
Meta loss on this task batch = 2.1307e-01, Meta loss averaged over last 500 steps = 2.6877e-01, PNorm = 144.4982, GNorm = 0.2622
Meta loss on this task batch = 2.4332e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 144.5009, GNorm = 0.4186
Took 130.50681376457214 seconds to complete one epoch of meta training
Took 138.77156257629395 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481785
Epoch 787
Meta loss on this task batch = 2.9125e-01, Meta loss averaged over last 500 steps = 2.6879e-01, PNorm = 144.5059, GNorm = 0.3103
Meta loss on this task batch = 2.8060e-01, Meta loss averaged over last 500 steps = 2.6877e-01, PNorm = 144.5105, GNorm = 0.2810
Meta loss on this task batch = 2.8373e-01, Meta loss averaged over last 500 steps = 2.6870e-01, PNorm = 144.5144, GNorm = 0.3123
Meta loss on this task batch = 2.5371e-01, Meta loss averaged over last 500 steps = 2.6871e-01, PNorm = 144.5178, GNorm = 0.2949
Meta loss on this task batch = 3.2014e-01, Meta loss averaged over last 500 steps = 2.6876e-01, PNorm = 144.5201, GNorm = 0.3193
Meta loss on this task batch = 2.1880e-01, Meta loss averaged over last 500 steps = 2.6874e-01, PNorm = 144.5213, GNorm = 0.2422
Meta loss on this task batch = 2.2941e-01, Meta loss averaged over last 500 steps = 2.6865e-01, PNorm = 144.5224, GNorm = 0.2889
Meta loss on this task batch = 2.9740e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 144.5230, GNorm = 0.3053
Meta loss on this task batch = 3.0810e-01, Meta loss averaged over last 500 steps = 2.6885e-01, PNorm = 144.5214, GNorm = 0.3516
Meta loss on this task batch = 2.9721e-01, Meta loss averaged over last 500 steps = 2.6891e-01, PNorm = 144.5218, GNorm = 0.3360
Meta loss on this task batch = 2.3277e-01, Meta loss averaged over last 500 steps = 2.6875e-01, PNorm = 144.5243, GNorm = 0.2804
Meta loss on this task batch = 3.4491e-01, Meta loss averaged over last 500 steps = 2.6896e-01, PNorm = 144.5280, GNorm = 0.3429
Meta loss on this task batch = 2.3491e-01, Meta loss averaged over last 500 steps = 2.6891e-01, PNorm = 144.5328, GNorm = 0.2720
Meta loss on this task batch = 3.0499e-01, Meta loss averaged over last 500 steps = 2.6896e-01, PNorm = 144.5427, GNorm = 0.3443
Meta loss on this task batch = 2.4762e-01, Meta loss averaged over last 500 steps = 2.6897e-01, PNorm = 144.5536, GNorm = 0.2919
Meta loss on this task batch = 3.1896e-01, Meta loss averaged over last 500 steps = 2.6919e-01, PNorm = 144.5656, GNorm = 0.3000
Meta loss on this task batch = 2.9605e-01, Meta loss averaged over last 500 steps = 2.6910e-01, PNorm = 144.5778, GNorm = 0.3005
Meta loss on this task batch = 2.0252e-01, Meta loss averaged over last 500 steps = 2.6892e-01, PNorm = 144.5890, GNorm = 0.2524
Meta loss on this task batch = 2.8528e-01, Meta loss averaged over last 500 steps = 2.6905e-01, PNorm = 144.5987, GNorm = 0.3454
Took 260.56869888305664 seconds to complete one epoch of meta training
Took 268.82617449760437 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489636
Epoch 788
Meta loss on this task batch = 3.1864e-01, Meta loss averaged over last 500 steps = 2.6905e-01, PNorm = 144.6080, GNorm = 0.3148
Meta loss on this task batch = 2.8698e-01, Meta loss averaged over last 500 steps = 2.6910e-01, PNorm = 144.6152, GNorm = 0.3094
Meta loss on this task batch = 2.5171e-01, Meta loss averaged over last 500 steps = 2.6911e-01, PNorm = 144.6207, GNorm = 0.3014
Meta loss on this task batch = 2.7230e-01, Meta loss averaged over last 500 steps = 2.6919e-01, PNorm = 144.6239, GNorm = 0.3446
Meta loss on this task batch = 2.5249e-01, Meta loss averaged over last 500 steps = 2.6918e-01, PNorm = 144.6268, GNorm = 0.3528
Meta loss on this task batch = 2.2991e-01, Meta loss averaged over last 500 steps = 2.6902e-01, PNorm = 144.6306, GNorm = 0.2521
Meta loss on this task batch = 2.8626e-01, Meta loss averaged over last 500 steps = 2.6903e-01, PNorm = 144.6355, GNorm = 0.2968
Meta loss on this task batch = 2.4825e-01, Meta loss averaged over last 500 steps = 2.6895e-01, PNorm = 144.6400, GNorm = 0.2686
Meta loss on this task batch = 2.9883e-01, Meta loss averaged over last 500 steps = 2.6898e-01, PNorm = 144.6436, GNorm = 0.3266
Meta loss on this task batch = 2.7020e-01, Meta loss averaged over last 500 steps = 2.6895e-01, PNorm = 144.6485, GNorm = 0.3380
Meta loss on this task batch = 2.8555e-01, Meta loss averaged over last 500 steps = 2.6893e-01, PNorm = 144.6529, GNorm = 0.2819
Meta loss on this task batch = 2.3718e-01, Meta loss averaged over last 500 steps = 2.6875e-01, PNorm = 144.6558, GNorm = 0.2806
Meta loss on this task batch = 2.7201e-01, Meta loss averaged over last 500 steps = 2.6863e-01, PNorm = 144.6591, GNorm = 0.2581
Meta loss on this task batch = 2.1692e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 144.6639, GNorm = 0.2325
Meta loss on this task batch = 3.0258e-01, Meta loss averaged over last 500 steps = 2.6871e-01, PNorm = 144.6684, GNorm = 0.3003
Meta loss on this task batch = 2.5767e-01, Meta loss averaged over last 500 steps = 2.6876e-01, PNorm = 144.6731, GNorm = 0.2644
Meta loss on this task batch = 2.8047e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 144.6794, GNorm = 0.2692
Meta loss on this task batch = 2.4718e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 144.6864, GNorm = 0.3124
Meta loss on this task batch = 2.9418e-01, Meta loss averaged over last 500 steps = 2.6883e-01, PNorm = 144.6931, GNorm = 0.4095
Took 281.6987087726593 seconds to complete one epoch of meta training
Took 290.6701591014862 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492006
Epoch 789
Meta loss on this task batch = 2.8677e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 144.6989, GNorm = 0.3178
Meta loss on this task batch = 3.3670e-01, Meta loss averaged over last 500 steps = 2.6893e-01, PNorm = 144.7035, GNorm = 0.2909
Meta loss on this task batch = 2.1886e-01, Meta loss averaged over last 500 steps = 2.6885e-01, PNorm = 144.7091, GNorm = 0.2910
Meta loss on this task batch = 2.5081e-01, Meta loss averaged over last 500 steps = 2.6884e-01, PNorm = 144.7148, GNorm = 0.3097
Meta loss on this task batch = 2.3888e-01, Meta loss averaged over last 500 steps = 2.6887e-01, PNorm = 144.7204, GNorm = 0.2486
Meta loss on this task batch = 2.6623e-01, Meta loss averaged over last 500 steps = 2.6891e-01, PNorm = 144.7258, GNorm = 0.3019
Meta loss on this task batch = 2.4648e-01, Meta loss averaged over last 500 steps = 2.6879e-01, PNorm = 144.7313, GNorm = 0.2660
Meta loss on this task batch = 2.5944e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 144.7367, GNorm = 0.2715
Meta loss on this task batch = 2.8311e-01, Meta loss averaged over last 500 steps = 2.6875e-01, PNorm = 144.7402, GNorm = 0.2731
Meta loss on this task batch = 2.1540e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 144.7433, GNorm = 0.2508
Meta loss on this task batch = 2.5877e-01, Meta loss averaged over last 500 steps = 2.6858e-01, PNorm = 144.7460, GNorm = 0.2998
Meta loss on this task batch = 2.3991e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 144.7489, GNorm = 0.2533
Meta loss on this task batch = 2.5807e-01, Meta loss averaged over last 500 steps = 2.6841e-01, PNorm = 144.7503, GNorm = 0.2992
Meta loss on this task batch = 2.4692e-01, Meta loss averaged over last 500 steps = 2.6835e-01, PNorm = 144.7512, GNorm = 0.2970
Meta loss on this task batch = 2.5248e-01, Meta loss averaged over last 500 steps = 2.6831e-01, PNorm = 144.7526, GNorm = 0.2814
Meta loss on this task batch = 2.8815e-01, Meta loss averaged over last 500 steps = 2.6840e-01, PNorm = 144.7546, GNorm = 0.2825
Meta loss on this task batch = 3.1146e-01, Meta loss averaged over last 500 steps = 2.6843e-01, PNorm = 144.7563, GNorm = 0.2625
Meta loss on this task batch = 2.4221e-01, Meta loss averaged over last 500 steps = 2.6841e-01, PNorm = 144.7585, GNorm = 0.3212
Meta loss on this task batch = 2.7959e-01, Meta loss averaged over last 500 steps = 2.6844e-01, PNorm = 144.7600, GNorm = 0.3059
Took 133.70210909843445 seconds to complete one epoch of meta training
Took 142.8706865310669 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.456531
Epoch 790
Meta loss on this task batch = 2.3654e-01, Meta loss averaged over last 500 steps = 2.6842e-01, PNorm = 144.7607, GNorm = 0.2617
Meta loss on this task batch = 2.2029e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 144.7623, GNorm = 0.2611
Meta loss on this task batch = 2.9646e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 144.7641, GNorm = 0.4207
Meta loss on this task batch = 2.5627e-01, Meta loss averaged over last 500 steps = 2.6839e-01, PNorm = 144.7656, GNorm = 0.2811
Meta loss on this task batch = 2.5360e-01, Meta loss averaged over last 500 steps = 2.6839e-01, PNorm = 144.7685, GNorm = 0.2586
Meta loss on this task batch = 3.5110e-01, Meta loss averaged over last 500 steps = 2.6863e-01, PNorm = 144.7687, GNorm = 0.3291
Meta loss on this task batch = 2.9254e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 144.7685, GNorm = 0.2902
Meta loss on this task batch = 2.6076e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 144.7664, GNorm = 0.2766
Meta loss on this task batch = 2.6140e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 144.7653, GNorm = 0.2648
Meta loss on this task batch = 2.5632e-01, Meta loss averaged over last 500 steps = 2.6854e-01, PNorm = 144.7653, GNorm = 0.2435
Meta loss on this task batch = 2.9313e-01, Meta loss averaged over last 500 steps = 2.6870e-01, PNorm = 144.7661, GNorm = 0.2807
Meta loss on this task batch = 2.6534e-01, Meta loss averaged over last 500 steps = 2.6865e-01, PNorm = 144.7667, GNorm = 0.3353
Meta loss on this task batch = 2.5562e-01, Meta loss averaged over last 500 steps = 2.6871e-01, PNorm = 144.7686, GNorm = 0.2786
Meta loss on this task batch = 2.6601e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 144.7733, GNorm = 0.2630
Meta loss on this task batch = 2.6530e-01, Meta loss averaged over last 500 steps = 2.6864e-01, PNorm = 144.7787, GNorm = 0.2702
Meta loss on this task batch = 2.3101e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 144.7841, GNorm = 0.2967
Meta loss on this task batch = 2.7450e-01, Meta loss averaged over last 500 steps = 2.6862e-01, PNorm = 144.7888, GNorm = 0.3263
Meta loss on this task batch = 2.7367e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 144.7939, GNorm = 0.3373
Meta loss on this task batch = 2.9963e-01, Meta loss averaged over last 500 steps = 2.6853e-01, PNorm = 144.7993, GNorm = 0.4384
Took 138.3942940235138 seconds to complete one epoch of meta training
Took 147.70323371887207 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501902
Epoch 791
Meta loss on this task batch = 2.9972e-01, Meta loss averaged over last 500 steps = 2.6850e-01, PNorm = 144.8059, GNorm = 0.3285
Meta loss on this task batch = 2.3936e-01, Meta loss averaged over last 500 steps = 2.6844e-01, PNorm = 144.8134, GNorm = 0.2960
Meta loss on this task batch = 2.8438e-01, Meta loss averaged over last 500 steps = 2.6832e-01, PNorm = 144.8215, GNorm = 0.3291
Meta loss on this task batch = 2.9519e-01, Meta loss averaged over last 500 steps = 2.6847e-01, PNorm = 144.8267, GNorm = 0.3469
Meta loss on this task batch = 2.6691e-01, Meta loss averaged over last 500 steps = 2.6846e-01, PNorm = 144.8305, GNorm = 0.2734
Meta loss on this task batch = 2.6510e-01, Meta loss averaged over last 500 steps = 2.6852e-01, PNorm = 144.8337, GNorm = 0.3106
Meta loss on this task batch = 2.6809e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 144.8383, GNorm = 0.3082
Meta loss on this task batch = 3.1387e-01, Meta loss averaged over last 500 steps = 2.6865e-01, PNorm = 144.8429, GNorm = 0.3848
Meta loss on this task batch = 2.7467e-01, Meta loss averaged over last 500 steps = 2.6868e-01, PNorm = 144.8480, GNorm = 0.3044
Meta loss on this task batch = 2.7271e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 144.8533, GNorm = 0.3022
Meta loss on this task batch = 2.9925e-01, Meta loss averaged over last 500 steps = 2.6879e-01, PNorm = 144.8578, GNorm = 0.3528
Meta loss on this task batch = 2.5969e-01, Meta loss averaged over last 500 steps = 2.6887e-01, PNorm = 144.8645, GNorm = 0.2885
Meta loss on this task batch = 2.6150e-01, Meta loss averaged over last 500 steps = 2.6880e-01, PNorm = 144.8727, GNorm = 0.4044
Meta loss on this task batch = 2.9812e-01, Meta loss averaged over last 500 steps = 2.6878e-01, PNorm = 144.8796, GNorm = 0.2864
Meta loss on this task batch = 2.6001e-01, Meta loss averaged over last 500 steps = 2.6868e-01, PNorm = 144.8853, GNorm = 0.2661
Meta loss on this task batch = 2.4318e-01, Meta loss averaged over last 500 steps = 2.6862e-01, PNorm = 144.8905, GNorm = 0.2632
Meta loss on this task batch = 2.2654e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 144.8955, GNorm = 0.2553
Meta loss on this task batch = 2.3230e-01, Meta loss averaged over last 500 steps = 2.6848e-01, PNorm = 144.9006, GNorm = 0.2343
Meta loss on this task batch = 2.9640e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 144.9039, GNorm = 0.3555
Took 139.07834148406982 seconds to complete one epoch of meta training
Took 148.13411021232605 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487329
Epoch 792
Meta loss on this task batch = 2.8109e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 144.9064, GNorm = 0.2932
Meta loss on this task batch = 2.8562e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 144.9096, GNorm = 0.2778
Meta loss on this task batch = 3.0637e-01, Meta loss averaged over last 500 steps = 2.6865e-01, PNorm = 144.9111, GNorm = 0.3249
Meta loss on this task batch = 2.5889e-01, Meta loss averaged over last 500 steps = 2.6864e-01, PNorm = 144.9142, GNorm = 0.2509
Meta loss on this task batch = 3.4405e-01, Meta loss averaged over last 500 steps = 2.6874e-01, PNorm = 144.9150, GNorm = 0.3477
Meta loss on this task batch = 2.7370e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 144.9156, GNorm = 0.2601
Meta loss on this task batch = 2.9675e-01, Meta loss averaged over last 500 steps = 2.6880e-01, PNorm = 144.9174, GNorm = 0.2797
Meta loss on this task batch = 2.6227e-01, Meta loss averaged over last 500 steps = 2.6877e-01, PNorm = 144.9191, GNorm = 0.2927
Meta loss on this task batch = 2.8317e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 144.9216, GNorm = 0.2598
Meta loss on this task batch = 2.3019e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 144.9243, GNorm = 0.2634
Meta loss on this task batch = 2.7106e-01, Meta loss averaged over last 500 steps = 2.6864e-01, PNorm = 144.9270, GNorm = 0.3088
Meta loss on this task batch = 2.2237e-01, Meta loss averaged over last 500 steps = 2.6858e-01, PNorm = 144.9309, GNorm = 0.2526
Meta loss on this task batch = 3.1512e-01, Meta loss averaged over last 500 steps = 2.6865e-01, PNorm = 144.9344, GNorm = 0.3245
Meta loss on this task batch = 2.2116e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 144.9388, GNorm = 0.2938
Meta loss on this task batch = 2.6388e-01, Meta loss averaged over last 500 steps = 2.6863e-01, PNorm = 144.9440, GNorm = 0.3237
Meta loss on this task batch = 2.7068e-01, Meta loss averaged over last 500 steps = 2.6853e-01, PNorm = 144.9494, GNorm = 0.2556
Meta loss on this task batch = 2.2172e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 144.9565, GNorm = 0.2765
Meta loss on this task batch = 2.7553e-01, Meta loss averaged over last 500 steps = 2.6840e-01, PNorm = 144.9650, GNorm = 0.3230
Meta loss on this task batch = 2.3163e-01, Meta loss averaged over last 500 steps = 2.6843e-01, PNorm = 144.9720, GNorm = 0.3325
Took 136.19274711608887 seconds to complete one epoch of meta training
Took 145.62862992286682 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457078
Epoch 793
Meta loss on this task batch = 3.2327e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 144.9791, GNorm = 0.2897
Meta loss on this task batch = 2.2796e-01, Meta loss averaged over last 500 steps = 2.6852e-01, PNorm = 144.9851, GNorm = 0.2573
Meta loss on this task batch = 2.5610e-01, Meta loss averaged over last 500 steps = 2.6846e-01, PNorm = 144.9898, GNorm = 0.3238
Meta loss on this task batch = 2.7028e-01, Meta loss averaged over last 500 steps = 2.6850e-01, PNorm = 144.9922, GNorm = 0.2977
Meta loss on this task batch = 2.9133e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 144.9939, GNorm = 0.3033
Meta loss on this task batch = 2.4919e-01, Meta loss averaged over last 500 steps = 2.6843e-01, PNorm = 144.9952, GNorm = 0.3002
Meta loss on this task batch = 2.4398e-01, Meta loss averaged over last 500 steps = 2.6841e-01, PNorm = 144.9957, GNorm = 0.2882
Meta loss on this task batch = 2.5656e-01, Meta loss averaged over last 500 steps = 2.6840e-01, PNorm = 144.9972, GNorm = 0.2958
Meta loss on this task batch = 2.3510e-01, Meta loss averaged over last 500 steps = 2.6833e-01, PNorm = 144.9992, GNorm = 0.2545
Meta loss on this task batch = 3.0243e-01, Meta loss averaged over last 500 steps = 2.6833e-01, PNorm = 145.0012, GNorm = 0.3164
Meta loss on this task batch = 2.3131e-01, Meta loss averaged over last 500 steps = 2.6831e-01, PNorm = 145.0029, GNorm = 0.2409
Meta loss on this task batch = 2.6059e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 145.0068, GNorm = 0.2654
Meta loss on this task batch = 2.8643e-01, Meta loss averaged over last 500 steps = 2.6832e-01, PNorm = 145.0109, GNorm = 0.2825
Meta loss on this task batch = 2.8215e-01, Meta loss averaged over last 500 steps = 2.6823e-01, PNorm = 145.0183, GNorm = 0.2668
Meta loss on this task batch = 2.5676e-01, Meta loss averaged over last 500 steps = 2.6818e-01, PNorm = 145.0257, GNorm = 0.3116
Meta loss on this task batch = 3.4084e-01, Meta loss averaged over last 500 steps = 2.6838e-01, PNorm = 145.0327, GNorm = 0.3022
Meta loss on this task batch = 2.8335e-01, Meta loss averaged over last 500 steps = 2.6846e-01, PNorm = 145.0407, GNorm = 0.3171
Meta loss on this task batch = 2.9003e-01, Meta loss averaged over last 500 steps = 2.6846e-01, PNorm = 145.0496, GNorm = 0.3015
Meta loss on this task batch = 2.4719e-01, Meta loss averaged over last 500 steps = 2.6844e-01, PNorm = 145.0577, GNorm = 0.3240
Took 134.9766969680786 seconds to complete one epoch of meta training
Took 144.2959005832672 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486461
Epoch 794
Meta loss on this task batch = 2.2682e-01, Meta loss averaged over last 500 steps = 2.6828e-01, PNorm = 145.0660, GNorm = 0.2811
Meta loss on this task batch = 2.7009e-01, Meta loss averaged over last 500 steps = 2.6830e-01, PNorm = 145.0746, GNorm = 0.2794
Meta loss on this task batch = 2.5463e-01, Meta loss averaged over last 500 steps = 2.6824e-01, PNorm = 145.0836, GNorm = 0.2630
Meta loss on this task batch = 3.1582e-01, Meta loss averaged over last 500 steps = 2.6844e-01, PNorm = 145.0926, GNorm = 0.3187
Meta loss on this task batch = 2.5206e-01, Meta loss averaged over last 500 steps = 2.6835e-01, PNorm = 145.0995, GNorm = 0.2723
Meta loss on this task batch = 2.8994e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 145.1046, GNorm = 0.2703
Meta loss on this task batch = 2.8047e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 145.1085, GNorm = 0.3210
Meta loss on this task batch = 2.6592e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 145.1126, GNorm = 0.3295
Meta loss on this task batch = 2.0416e-01, Meta loss averaged over last 500 steps = 2.6836e-01, PNorm = 145.1157, GNorm = 0.2558
Meta loss on this task batch = 2.8041e-01, Meta loss averaged over last 500 steps = 2.6830e-01, PNorm = 145.1191, GNorm = 0.2514
Meta loss on this task batch = 2.7816e-01, Meta loss averaged over last 500 steps = 2.6829e-01, PNorm = 145.1233, GNorm = 0.2968
Meta loss on this task batch = 2.2821e-01, Meta loss averaged over last 500 steps = 2.6826e-01, PNorm = 145.1284, GNorm = 0.3282
Meta loss on this task batch = 3.3671e-01, Meta loss averaged over last 500 steps = 2.6836e-01, PNorm = 145.1342, GNorm = 0.2894
Meta loss on this task batch = 3.4282e-01, Meta loss averaged over last 500 steps = 2.6853e-01, PNorm = 145.1416, GNorm = 0.3186
Meta loss on this task batch = 2.9270e-01, Meta loss averaged over last 500 steps = 2.6848e-01, PNorm = 145.1489, GNorm = 0.3470
Meta loss on this task batch = 2.6046e-01, Meta loss averaged over last 500 steps = 2.6850e-01, PNorm = 145.1567, GNorm = 0.2819
Meta loss on this task batch = 2.2267e-01, Meta loss averaged over last 500 steps = 2.6836e-01, PNorm = 145.1636, GNorm = 0.2615
Meta loss on this task batch = 2.5473e-01, Meta loss averaged over last 500 steps = 2.6847e-01, PNorm = 145.1696, GNorm = 0.2710
Meta loss on this task batch = 2.3410e-01, Meta loss averaged over last 500 steps = 2.6826e-01, PNorm = 145.1736, GNorm = 0.3161
Took 134.2471799850464 seconds to complete one epoch of meta training
Took 143.68012857437134 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483442
Epoch 795
Meta loss on this task batch = 2.8240e-01, Meta loss averaged over last 500 steps = 2.6832e-01, PNorm = 145.1767, GNorm = 0.2685
Meta loss on this task batch = 2.1928e-01, Meta loss averaged over last 500 steps = 2.6821e-01, PNorm = 145.1792, GNorm = 0.2655
Meta loss on this task batch = 2.5504e-01, Meta loss averaged over last 500 steps = 2.6815e-01, PNorm = 145.1818, GNorm = 0.2941
Meta loss on this task batch = 2.4504e-01, Meta loss averaged over last 500 steps = 2.6815e-01, PNorm = 145.1838, GNorm = 0.3166
Meta loss on this task batch = 2.9456e-01, Meta loss averaged over last 500 steps = 2.6823e-01, PNorm = 145.1827, GNorm = 0.3029
Meta loss on this task batch = 2.0248e-01, Meta loss averaged over last 500 steps = 2.6814e-01, PNorm = 145.1821, GNorm = 0.2531
Meta loss on this task batch = 2.6429e-01, Meta loss averaged over last 500 steps = 2.6817e-01, PNorm = 145.1798, GNorm = 0.2886
Meta loss on this task batch = 2.4794e-01, Meta loss averaged over last 500 steps = 2.6818e-01, PNorm = 145.1787, GNorm = 0.2624
Meta loss on this task batch = 2.8863e-01, Meta loss averaged over last 500 steps = 2.6822e-01, PNorm = 145.1787, GNorm = 0.3107
Meta loss on this task batch = 2.7031e-01, Meta loss averaged over last 500 steps = 2.6828e-01, PNorm = 145.1786, GNorm = 0.3065
Meta loss on this task batch = 2.2003e-01, Meta loss averaged over last 500 steps = 2.6810e-01, PNorm = 145.1785, GNorm = 0.2588
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 2.6805e-01, PNorm = 145.1789, GNorm = 0.2797
Meta loss on this task batch = 2.4523e-01, Meta loss averaged over last 500 steps = 2.6801e-01, PNorm = 145.1797, GNorm = 0.2498
Meta loss on this task batch = 3.0880e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 145.1801, GNorm = 0.2867
Meta loss on this task batch = 2.5316e-01, Meta loss averaged over last 500 steps = 2.6794e-01, PNorm = 145.1810, GNorm = 0.2476
Meta loss on this task batch = 2.8189e-01, Meta loss averaged over last 500 steps = 2.6799e-01, PNorm = 145.1826, GNorm = 0.2786
Meta loss on this task batch = 2.9212e-01, Meta loss averaged over last 500 steps = 2.6813e-01, PNorm = 145.1839, GNorm = 0.3293
Meta loss on this task batch = 2.8521e-01, Meta loss averaged over last 500 steps = 2.6810e-01, PNorm = 145.1851, GNorm = 0.3420
Meta loss on this task batch = 3.1005e-01, Meta loss averaged over last 500 steps = 2.6820e-01, PNorm = 145.1836, GNorm = 0.3911
Took 135.8286874294281 seconds to complete one epoch of meta training
Took 144.68622136116028 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467367
Epoch 796
Meta loss on this task batch = 3.1768e-01, Meta loss averaged over last 500 steps = 2.6823e-01, PNorm = 145.1812, GNorm = 0.3493
Meta loss on this task batch = 2.4855e-01, Meta loss averaged over last 500 steps = 2.6818e-01, PNorm = 145.1821, GNorm = 0.3133
Meta loss on this task batch = 2.3178e-01, Meta loss averaged over last 500 steps = 2.6812e-01, PNorm = 145.1842, GNorm = 0.2746
Meta loss on this task batch = 2.2173e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 145.1873, GNorm = 0.2586
Meta loss on this task batch = 2.9374e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 145.1898, GNorm = 0.2831
Meta loss on this task batch = 2.7916e-01, Meta loss averaged over last 500 steps = 2.6817e-01, PNorm = 145.1919, GNorm = 0.2974
Meta loss on this task batch = 3.2515e-01, Meta loss averaged over last 500 steps = 2.6837e-01, PNorm = 145.1947, GNorm = 0.3051
Meta loss on this task batch = 2.2280e-01, Meta loss averaged over last 500 steps = 2.6830e-01, PNorm = 145.1981, GNorm = 0.2390
Meta loss on this task batch = 3.3947e-01, Meta loss averaged over last 500 steps = 2.6847e-01, PNorm = 145.1997, GNorm = 0.3086
Meta loss on this task batch = 2.5992e-01, Meta loss averaged over last 500 steps = 2.6842e-01, PNorm = 145.2034, GNorm = 0.2794
Meta loss on this task batch = 2.1982e-01, Meta loss averaged over last 500 steps = 2.6837e-01, PNorm = 145.2075, GNorm = 0.2678
Meta loss on this task batch = 2.4740e-01, Meta loss averaged over last 500 steps = 2.6833e-01, PNorm = 145.2108, GNorm = 0.2572
Meta loss on this task batch = 2.6826e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 145.2138, GNorm = 0.2606
Meta loss on this task batch = 2.4752e-01, Meta loss averaged over last 500 steps = 2.6822e-01, PNorm = 145.2151, GNorm = 0.2960
Meta loss on this task batch = 2.6055e-01, Meta loss averaged over last 500 steps = 2.6815e-01, PNorm = 145.2162, GNorm = 0.2976
Meta loss on this task batch = 2.7035e-01, Meta loss averaged over last 500 steps = 2.6817e-01, PNorm = 145.2167, GNorm = 0.2757
Meta loss on this task batch = 2.4881e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 145.2193, GNorm = 0.2529
Meta loss on this task batch = 2.4606e-01, Meta loss averaged over last 500 steps = 2.6808e-01, PNorm = 145.2213, GNorm = 0.2848
Meta loss on this task batch = 2.7908e-01, Meta loss averaged over last 500 steps = 2.6816e-01, PNorm = 145.2236, GNorm = 0.3171
Took 133.65684580802917 seconds to complete one epoch of meta training
Took 142.09763884544373 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498208
Epoch 797
Meta loss on this task batch = 2.7384e-01, Meta loss averaged over last 500 steps = 2.6818e-01, PNorm = 145.2295, GNorm = 0.3055
Meta loss on this task batch = 2.9597e-01, Meta loss averaged over last 500 steps = 2.6819e-01, PNorm = 145.2343, GNorm = 0.3300
Meta loss on this task batch = 2.6816e-01, Meta loss averaged over last 500 steps = 2.6811e-01, PNorm = 145.2407, GNorm = 0.3078
Meta loss on this task batch = 3.0747e-01, Meta loss averaged over last 500 steps = 2.6820e-01, PNorm = 145.2476, GNorm = 0.3034
Meta loss on this task batch = 3.2148e-01, Meta loss averaged over last 500 steps = 2.6832e-01, PNorm = 145.2528, GNorm = 0.3499
Meta loss on this task batch = 2.6361e-01, Meta loss averaged over last 500 steps = 2.6825e-01, PNorm = 145.2580, GNorm = 0.2830
Meta loss on this task batch = 2.5588e-01, Meta loss averaged over last 500 steps = 2.6827e-01, PNorm = 145.2631, GNorm = 0.2778
Meta loss on this task batch = 2.5850e-01, Meta loss averaged over last 500 steps = 2.6820e-01, PNorm = 145.2684, GNorm = 0.2673
Meta loss on this task batch = 2.6336e-01, Meta loss averaged over last 500 steps = 2.6818e-01, PNorm = 145.2724, GNorm = 0.3029
Meta loss on this task batch = 2.6578e-01, Meta loss averaged over last 500 steps = 2.6810e-01, PNorm = 145.2724, GNorm = 0.2895
Meta loss on this task batch = 2.3844e-01, Meta loss averaged over last 500 steps = 2.6817e-01, PNorm = 145.2730, GNorm = 0.2986
Meta loss on this task batch = 2.6699e-01, Meta loss averaged over last 500 steps = 2.6817e-01, PNorm = 145.2761, GNorm = 0.2931
Meta loss on this task batch = 2.7364e-01, Meta loss averaged over last 500 steps = 2.6814e-01, PNorm = 145.2813, GNorm = 0.3329
Meta loss on this task batch = 2.5019e-01, Meta loss averaged over last 500 steps = 2.6813e-01, PNorm = 145.2873, GNorm = 0.2732
Meta loss on this task batch = 2.1744e-01, Meta loss averaged over last 500 steps = 2.6798e-01, PNorm = 145.2935, GNorm = 0.2723
Meta loss on this task batch = 2.6741e-01, Meta loss averaged over last 500 steps = 2.6797e-01, PNorm = 145.3005, GNorm = 0.2984
Meta loss on this task batch = 2.9440e-01, Meta loss averaged over last 500 steps = 2.6789e-01, PNorm = 145.3066, GNorm = 0.3393
Meta loss on this task batch = 3.2605e-01, Meta loss averaged over last 500 steps = 2.6795e-01, PNorm = 145.3105, GNorm = 0.3270
Meta loss on this task batch = 2.9014e-01, Meta loss averaged over last 500 steps = 2.6804e-01, PNorm = 145.3132, GNorm = 0.3327
Took 136.59401726722717 seconds to complete one epoch of meta training
Took 145.28550124168396 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503667
Epoch 798
Meta loss on this task batch = 3.3427e-01, Meta loss averaged over last 500 steps = 2.6824e-01, PNorm = 145.3153, GNorm = 0.4013
Meta loss on this task batch = 2.4358e-01, Meta loss averaged over last 500 steps = 2.6822e-01, PNorm = 145.3173, GNorm = 0.3542
Meta loss on this task batch = 3.1214e-01, Meta loss averaged over last 500 steps = 2.6835e-01, PNorm = 145.3184, GNorm = 0.3475
Meta loss on this task batch = 2.5253e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 145.3198, GNorm = 0.2653
Meta loss on this task batch = 2.9214e-01, Meta loss averaged over last 500 steps = 2.6842e-01, PNorm = 145.3233, GNorm = 0.3333
Meta loss on this task batch = 2.5176e-01, Meta loss averaged over last 500 steps = 2.6838e-01, PNorm = 145.3278, GNorm = 0.3170
Meta loss on this task batch = 2.8108e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 145.3324, GNorm = 0.2796
Meta loss on this task batch = 2.9176e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 145.3375, GNorm = 0.3355
Meta loss on this task batch = 2.7720e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 145.3432, GNorm = 0.2729
Meta loss on this task batch = 2.4752e-01, Meta loss averaged over last 500 steps = 2.6853e-01, PNorm = 145.3495, GNorm = 0.2647
Meta loss on this task batch = 2.8582e-01, Meta loss averaged over last 500 steps = 2.6862e-01, PNorm = 145.3553, GNorm = 0.2890
Meta loss on this task batch = 2.0879e-01, Meta loss averaged over last 500 steps = 2.6851e-01, PNorm = 145.3603, GNorm = 0.2689
Meta loss on this task batch = 2.7522e-01, Meta loss averaged over last 500 steps = 2.6852e-01, PNorm = 145.3640, GNorm = 0.2912
Meta loss on this task batch = 2.2114e-01, Meta loss averaged over last 500 steps = 2.6853e-01, PNorm = 145.3667, GNorm = 0.2663
Meta loss on this task batch = 2.7982e-01, Meta loss averaged over last 500 steps = 2.6852e-01, PNorm = 145.3689, GNorm = 0.2903
Meta loss on this task batch = 2.5923e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 145.3687, GNorm = 0.3348
Meta loss on this task batch = 2.5230e-01, Meta loss averaged over last 500 steps = 2.6833e-01, PNorm = 145.3674, GNorm = 0.2749
Meta loss on this task batch = 2.6798e-01, Meta loss averaged over last 500 steps = 2.6837e-01, PNorm = 145.3661, GNorm = 0.2814
Meta loss on this task batch = 2.5032e-01, Meta loss averaged over last 500 steps = 2.6841e-01, PNorm = 145.3621, GNorm = 0.3753
Took 134.10619187355042 seconds to complete one epoch of meta training
Took 142.83715915679932 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480142
Epoch 799
Meta loss on this task batch = 2.6864e-01, Meta loss averaged over last 500 steps = 2.6832e-01, PNorm = 145.3596, GNorm = 0.2403
Meta loss on this task batch = 3.1267e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 145.3586, GNorm = 0.3215
Meta loss on this task batch = 3.0398e-01, Meta loss averaged over last 500 steps = 2.6857e-01, PNorm = 145.3582, GNorm = 0.2979
Meta loss on this task batch = 2.6351e-01, Meta loss averaged over last 500 steps = 2.6867e-01, PNorm = 145.3582, GNorm = 0.3024
Meta loss on this task batch = 2.4976e-01, Meta loss averaged over last 500 steps = 2.6865e-01, PNorm = 145.3605, GNorm = 0.3630
Meta loss on this task batch = 2.4719e-01, Meta loss averaged over last 500 steps = 2.6860e-01, PNorm = 145.3630, GNorm = 0.2519
Meta loss on this task batch = 2.8969e-01, Meta loss averaged over last 500 steps = 2.6862e-01, PNorm = 145.3646, GNorm = 0.2945
Meta loss on this task batch = 1.8832e-01, Meta loss averaged over last 500 steps = 2.6848e-01, PNorm = 145.3684, GNorm = 0.2717
Meta loss on this task batch = 2.2557e-01, Meta loss averaged over last 500 steps = 2.6842e-01, PNorm = 145.3736, GNorm = 0.2863
Meta loss on this task batch = 2.5590e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 145.3784, GNorm = 0.3056
Meta loss on this task batch = 2.8257e-01, Meta loss averaged over last 500 steps = 2.6850e-01, PNorm = 145.3826, GNorm = 0.3252
Meta loss on this task batch = 2.9464e-01, Meta loss averaged over last 500 steps = 2.6860e-01, PNorm = 145.3858, GNorm = 0.3061
Meta loss on this task batch = 3.0269e-01, Meta loss averaged over last 500 steps = 2.6871e-01, PNorm = 145.3878, GNorm = 0.2998
Meta loss on this task batch = 3.0570e-01, Meta loss averaged over last 500 steps = 2.6882e-01, PNorm = 145.3884, GNorm = 0.3635
Meta loss on this task batch = 3.2325e-01, Meta loss averaged over last 500 steps = 2.6896e-01, PNorm = 145.3887, GNorm = 0.3802
Meta loss on this task batch = 2.3117e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 145.3878, GNorm = 0.2994
Meta loss on this task batch = 2.6875e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 145.3845, GNorm = 0.3183
Meta loss on this task batch = 2.8579e-01, Meta loss averaged over last 500 steps = 2.6881e-01, PNorm = 145.3838, GNorm = 0.2917
Meta loss on this task batch = 2.9304e-01, Meta loss averaged over last 500 steps = 2.6891e-01, PNorm = 145.3842, GNorm = 0.3794
Took 136.75904154777527 seconds to complete one epoch of meta training
Took 146.10753345489502 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464052
Epoch 800
Meta loss on this task batch = 3.1130e-01, Meta loss averaged over last 500 steps = 2.6896e-01, PNorm = 145.3856, GNorm = 0.3014
Meta loss on this task batch = 2.4090e-01, Meta loss averaged over last 500 steps = 2.6880e-01, PNorm = 145.3881, GNorm = 0.2537
Meta loss on this task batch = 2.3755e-01, Meta loss averaged over last 500 steps = 2.6877e-01, PNorm = 145.3911, GNorm = 0.2818
Meta loss on this task batch = 2.7355e-01, Meta loss averaged over last 500 steps = 2.6885e-01, PNorm = 145.3938, GNorm = 0.2712
Meta loss on this task batch = 2.8538e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 145.3976, GNorm = 0.2996
Meta loss on this task batch = 2.7751e-01, Meta loss averaged over last 500 steps = 2.6888e-01, PNorm = 145.4031, GNorm = 0.2617
Meta loss on this task batch = 2.7907e-01, Meta loss averaged over last 500 steps = 2.6889e-01, PNorm = 145.4104, GNorm = 0.2791
Meta loss on this task batch = 2.5284e-01, Meta loss averaged over last 500 steps = 2.6888e-01, PNorm = 145.4167, GNorm = 0.3103
Meta loss on this task batch = 2.7316e-01, Meta loss averaged over last 500 steps = 2.6896e-01, PNorm = 145.4230, GNorm = 0.2991
Meta loss on this task batch = 2.8869e-01, Meta loss averaged over last 500 steps = 2.6906e-01, PNorm = 145.4297, GNorm = 0.2992
Meta loss on this task batch = 2.3297e-01, Meta loss averaged over last 500 steps = 2.6902e-01, PNorm = 145.4341, GNorm = 0.2740
Meta loss on this task batch = 2.4861e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 145.4385, GNorm = 0.2657
Meta loss on this task batch = 2.9964e-01, Meta loss averaged over last 500 steps = 2.6878e-01, PNorm = 145.4437, GNorm = 0.2604
Meta loss on this task batch = 2.6288e-01, Meta loss averaged over last 500 steps = 2.6873e-01, PNorm = 145.4507, GNorm = 0.2573
Meta loss on this task batch = 3.4142e-01, Meta loss averaged over last 500 steps = 2.6889e-01, PNorm = 145.4574, GNorm = 0.3724
Meta loss on this task batch = 2.4897e-01, Meta loss averaged over last 500 steps = 2.6885e-01, PNorm = 145.4631, GNorm = 0.3023
Meta loss on this task batch = 2.6192e-01, Meta loss averaged over last 500 steps = 2.6885e-01, PNorm = 145.4699, GNorm = 0.2640
Meta loss on this task batch = 2.3500e-01, Meta loss averaged over last 500 steps = 2.6871e-01, PNorm = 145.4771, GNorm = 0.2456
Meta loss on this task batch = 2.6214e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 145.4835, GNorm = 0.3095
Took 135.5011215209961 seconds to complete one epoch of meta training
Took 144.6621208190918 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494142
Epoch 801
Meta loss on this task batch = 2.3221e-01, Meta loss averaged over last 500 steps = 2.6865e-01, PNorm = 145.4879, GNorm = 0.3062
Meta loss on this task batch = 2.9823e-01, Meta loss averaged over last 500 steps = 2.6870e-01, PNorm = 145.4895, GNorm = 0.3112
Meta loss on this task batch = 2.8858e-01, Meta loss averaged over last 500 steps = 2.6864e-01, PNorm = 145.4902, GNorm = 0.3547
Meta loss on this task batch = 2.8732e-01, Meta loss averaged over last 500 steps = 2.6873e-01, PNorm = 145.4911, GNorm = 0.2823
Meta loss on this task batch = 3.0506e-01, Meta loss averaged over last 500 steps = 2.6885e-01, PNorm = 145.4920, GNorm = 0.3207
Meta loss on this task batch = 2.6508e-01, Meta loss averaged over last 500 steps = 2.6888e-01, PNorm = 145.4950, GNorm = 0.2717
Meta loss on this task batch = 2.7124e-01, Meta loss averaged over last 500 steps = 2.6877e-01, PNorm = 145.4986, GNorm = 0.3329
Meta loss on this task batch = 2.4033e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 145.5032, GNorm = 0.2655
Meta loss on this task batch = 2.6159e-01, Meta loss averaged over last 500 steps = 2.6874e-01, PNorm = 145.5109, GNorm = 0.3384
Meta loss on this task batch = 2.3552e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 145.5185, GNorm = 0.2972
Meta loss on this task batch = 2.7153e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 145.5248, GNorm = 0.2965
Meta loss on this task batch = 2.6380e-01, Meta loss averaged over last 500 steps = 2.6867e-01, PNorm = 145.5298, GNorm = 0.2884
Meta loss on this task batch = 2.2933e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 145.5344, GNorm = 0.2502
Meta loss on this task batch = 2.9757e-01, Meta loss averaged over last 500 steps = 2.6857e-01, PNorm = 145.5370, GNorm = 0.3687
Meta loss on this task batch = 2.5457e-01, Meta loss averaged over last 500 steps = 2.6860e-01, PNorm = 145.5381, GNorm = 0.2879
Meta loss on this task batch = 2.3510e-01, Meta loss averaged over last 500 steps = 2.6848e-01, PNorm = 145.5403, GNorm = 0.2717
Meta loss on this task batch = 2.0550e-01, Meta loss averaged over last 500 steps = 2.6839e-01, PNorm = 145.5405, GNorm = 0.6257
Meta loss on this task batch = 2.5300e-01, Meta loss averaged over last 500 steps = 2.6829e-01, PNorm = 145.5431, GNorm = 0.2673
Meta loss on this task batch = 3.1765e-01, Meta loss averaged over last 500 steps = 2.6839e-01, PNorm = 145.5451, GNorm = 0.3852
Took 135.2784080505371 seconds to complete one epoch of meta training
Took 143.56139159202576 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471599
Epoch 802
Meta loss on this task batch = 3.0310e-01, Meta loss averaged over last 500 steps = 2.6846e-01, PNorm = 145.5472, GNorm = 0.2929
Meta loss on this task batch = 2.8068e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 145.5472, GNorm = 0.3115
Meta loss on this task batch = 2.6559e-01, Meta loss averaged over last 500 steps = 2.6840e-01, PNorm = 145.5477, GNorm = 0.2653
Meta loss on this task batch = 2.7763e-01, Meta loss averaged over last 500 steps = 2.6838e-01, PNorm = 145.5492, GNorm = 0.3147
Meta loss on this task batch = 2.5077e-01, Meta loss averaged over last 500 steps = 2.6840e-01, PNorm = 145.5516, GNorm = 0.2598
Meta loss on this task batch = 2.3581e-01, Meta loss averaged over last 500 steps = 2.6827e-01, PNorm = 145.5563, GNorm = 0.2438
Meta loss on this task batch = 2.7756e-01, Meta loss averaged over last 500 steps = 2.6829e-01, PNorm = 145.5624, GNorm = 0.2629
Meta loss on this task batch = 3.3742e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 145.5684, GNorm = 0.2921
Meta loss on this task batch = 2.4019e-01, Meta loss averaged over last 500 steps = 2.6825e-01, PNorm = 145.5757, GNorm = 0.2519
Meta loss on this task batch = 2.5607e-01, Meta loss averaged over last 500 steps = 2.6821e-01, PNorm = 145.5840, GNorm = 0.3081
Meta loss on this task batch = 2.3713e-01, Meta loss averaged over last 500 steps = 2.6816e-01, PNorm = 145.5918, GNorm = 0.2699
Meta loss on this task batch = 2.8016e-01, Meta loss averaged over last 500 steps = 2.6823e-01, PNorm = 145.5995, GNorm = 0.3099
Meta loss on this task batch = 2.9559e-01, Meta loss averaged over last 500 steps = 2.6826e-01, PNorm = 145.6051, GNorm = 0.3164
Meta loss on this task batch = 3.3589e-01, Meta loss averaged over last 500 steps = 2.6844e-01, PNorm = 145.6111, GNorm = 0.3313
Meta loss on this task batch = 2.5683e-01, Meta loss averaged over last 500 steps = 2.6837e-01, PNorm = 145.6162, GNorm = 0.3140
Meta loss on this task batch = 2.3914e-01, Meta loss averaged over last 500 steps = 2.6828e-01, PNorm = 145.6188, GNorm = 0.2790
Meta loss on this task batch = 2.5596e-01, Meta loss averaged over last 500 steps = 2.6834e-01, PNorm = 145.6192, GNorm = 0.3016
Meta loss on this task batch = 1.9343e-01, Meta loss averaged over last 500 steps = 2.6830e-01, PNorm = 145.6201, GNorm = 0.2420
Meta loss on this task batch = 3.0455e-01, Meta loss averaged over last 500 steps = 2.6837e-01, PNorm = 145.6231, GNorm = 0.3640
Took 133.54014205932617 seconds to complete one epoch of meta training
Took 142.60133147239685 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471018
Epoch 803
Meta loss on this task batch = 2.9276e-01, Meta loss averaged over last 500 steps = 2.6850e-01, PNorm = 145.6253, GNorm = 0.3103
Meta loss on this task batch = 2.4811e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 145.6270, GNorm = 0.2925
Meta loss on this task batch = 2.3076e-01, Meta loss averaged over last 500 steps = 2.6842e-01, PNorm = 145.6285, GNorm = 0.2666
Meta loss on this task batch = 2.8288e-01, Meta loss averaged over last 500 steps = 2.6848e-01, PNorm = 145.6305, GNorm = 0.2805
Meta loss on this task batch = 2.9168e-01, Meta loss averaged over last 500 steps = 2.6842e-01, PNorm = 145.6314, GNorm = 0.2907
Meta loss on this task batch = 2.9076e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 145.6325, GNorm = 0.2598
Meta loss on this task batch = 2.5587e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 145.6339, GNorm = 0.2998
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 2.6850e-01, PNorm = 145.6370, GNorm = 0.3103
Meta loss on this task batch = 2.8228e-01, Meta loss averaged over last 500 steps = 2.6854e-01, PNorm = 145.6410, GNorm = 0.2399
Meta loss on this task batch = 2.4583e-01, Meta loss averaged over last 500 steps = 2.6840e-01, PNorm = 145.6474, GNorm = 0.2855
Meta loss on this task batch = 2.8079e-01, Meta loss averaged over last 500 steps = 2.6846e-01, PNorm = 145.6568, GNorm = 0.3516
Meta loss on this task batch = 2.2260e-01, Meta loss averaged over last 500 steps = 2.6836e-01, PNorm = 145.6667, GNorm = 0.2601
Meta loss on this task batch = 2.5505e-01, Meta loss averaged over last 500 steps = 2.6838e-01, PNorm = 145.6756, GNorm = 0.2930
Meta loss on this task batch = 2.7182e-01, Meta loss averaged over last 500 steps = 2.6836e-01, PNorm = 145.6828, GNorm = 0.2700
Meta loss on this task batch = 2.7691e-01, Meta loss averaged over last 500 steps = 2.6839e-01, PNorm = 145.6877, GNorm = 0.3324
Meta loss on this task batch = 3.0763e-01, Meta loss averaged over last 500 steps = 2.6844e-01, PNorm = 145.6908, GNorm = 0.3065
Meta loss on this task batch = 2.6217e-01, Meta loss averaged over last 500 steps = 2.6848e-01, PNorm = 145.6913, GNorm = 0.2917
Meta loss on this task batch = 3.1353e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 145.6921, GNorm = 0.3087
Meta loss on this task batch = 2.5439e-01, Meta loss averaged over last 500 steps = 2.6852e-01, PNorm = 145.6915, GNorm = 0.3976
Took 136.215970993042 seconds to complete one epoch of meta training
Took 145.34174585342407 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457430
Epoch 804
Meta loss on this task batch = 2.8373e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 145.6923, GNorm = 0.2951
Meta loss on this task batch = 2.7235e-01, Meta loss averaged over last 500 steps = 2.6866e-01, PNorm = 145.6956, GNorm = 0.2948
Meta loss on this task batch = 2.6152e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 145.7003, GNorm = 0.3000
Meta loss on this task batch = 2.2679e-01, Meta loss averaged over last 500 steps = 2.6875e-01, PNorm = 145.7057, GNorm = 0.2560
Meta loss on this task batch = 3.0928e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 145.7104, GNorm = 0.3080
Meta loss on this task batch = 2.4326e-01, Meta loss averaged over last 500 steps = 2.6883e-01, PNorm = 145.7160, GNorm = 0.2745
Meta loss on this task batch = 2.6248e-01, Meta loss averaged over last 500 steps = 2.6873e-01, PNorm = 145.7207, GNorm = 0.2842
Meta loss on this task batch = 2.5401e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 145.7288, GNorm = 0.2731
Meta loss on this task batch = 3.0215e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 145.7364, GNorm = 0.3661
Meta loss on this task batch = 2.6592e-01, Meta loss averaged over last 500 steps = 2.6877e-01, PNorm = 145.7442, GNorm = 0.2666
Meta loss on this task batch = 2.9104e-01, Meta loss averaged over last 500 steps = 2.6882e-01, PNorm = 145.7522, GNorm = 0.3011
Meta loss on this task batch = 2.6206e-01, Meta loss averaged over last 500 steps = 2.6889e-01, PNorm = 145.7572, GNorm = 0.2598
Meta loss on this task batch = 2.8538e-01, Meta loss averaged over last 500 steps = 2.6891e-01, PNorm = 145.7620, GNorm = 0.3014
Meta loss on this task batch = 2.6549e-01, Meta loss averaged over last 500 steps = 2.6907e-01, PNorm = 145.7657, GNorm = 0.3251
Meta loss on this task batch = 2.8069e-01, Meta loss averaged over last 500 steps = 2.6910e-01, PNorm = 145.7704, GNorm = 0.2695
Meta loss on this task batch = 2.6889e-01, Meta loss averaged over last 500 steps = 2.6913e-01, PNorm = 145.7752, GNorm = 0.2984
Meta loss on this task batch = 2.1803e-01, Meta loss averaged over last 500 steps = 2.6900e-01, PNorm = 145.7796, GNorm = 0.2766
Meta loss on this task batch = 2.7926e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 145.7851, GNorm = 0.2999
Meta loss on this task batch = 3.1525e-01, Meta loss averaged over last 500 steps = 2.6884e-01, PNorm = 145.7927, GNorm = 0.3645
Took 135.3459188938141 seconds to complete one epoch of meta training
Took 144.15599370002747 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472530
Epoch 805
Meta loss on this task batch = 3.0520e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 145.8001, GNorm = 0.2935
Meta loss on this task batch = 2.7489e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 145.8070, GNorm = 0.2795
Meta loss on this task batch = 2.6977e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 145.8124, GNorm = 0.2638
Meta loss on this task batch = 2.7175e-01, Meta loss averaged over last 500 steps = 2.6897e-01, PNorm = 145.8178, GNorm = 0.2650
Meta loss on this task batch = 2.5238e-01, Meta loss averaged over last 500 steps = 2.6894e-01, PNorm = 145.8221, GNorm = 0.3085
Meta loss on this task batch = 2.1570e-01, Meta loss averaged over last 500 steps = 2.6893e-01, PNorm = 145.8262, GNorm = 0.2988
Meta loss on this task batch = 2.9078e-01, Meta loss averaged over last 500 steps = 2.6897e-01, PNorm = 145.8290, GNorm = 0.3787
Meta loss on this task batch = 2.9479e-01, Meta loss averaged over last 500 steps = 2.6900e-01, PNorm = 145.8311, GNorm = 0.2985
Meta loss on this task batch = 2.5485e-01, Meta loss averaged over last 500 steps = 2.6888e-01, PNorm = 145.8335, GNorm = 0.2919
Meta loss on this task batch = 2.7370e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 145.8381, GNorm = 0.2809
Meta loss on this task batch = 2.7989e-01, Meta loss averaged over last 500 steps = 2.6899e-01, PNorm = 145.8438, GNorm = 0.2951
Meta loss on this task batch = 2.6507e-01, Meta loss averaged over last 500 steps = 2.6895e-01, PNorm = 145.8505, GNorm = 0.2622
Meta loss on this task batch = 2.7463e-01, Meta loss averaged over last 500 steps = 2.6895e-01, PNorm = 145.8584, GNorm = 0.2900
Meta loss on this task batch = 2.4302e-01, Meta loss averaged over last 500 steps = 2.6888e-01, PNorm = 145.8676, GNorm = 0.2861
Meta loss on this task batch = 2.4142e-01, Meta loss averaged over last 500 steps = 2.6905e-01, PNorm = 145.8767, GNorm = 0.2782
Meta loss on this task batch = 2.3899e-01, Meta loss averaged over last 500 steps = 2.6906e-01, PNorm = 145.8845, GNorm = 0.2629
Meta loss on this task batch = 2.1437e-01, Meta loss averaged over last 500 steps = 2.6896e-01, PNorm = 145.8904, GNorm = 0.2954
Meta loss on this task batch = 2.5092e-01, Meta loss averaged over last 500 steps = 2.6905e-01, PNorm = 145.8957, GNorm = 0.2350
Meta loss on this task batch = 2.5057e-01, Meta loss averaged over last 500 steps = 2.6889e-01, PNorm = 145.8993, GNorm = 0.3732
Took 138.38267993927002 seconds to complete one epoch of meta training
Took 146.17100977897644 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471718
Epoch 806
Meta loss on this task batch = 2.8093e-01, Meta loss averaged over last 500 steps = 2.6895e-01, PNorm = 145.8997, GNorm = 0.3067
Meta loss on this task batch = 2.9203e-01, Meta loss averaged over last 500 steps = 2.6900e-01, PNorm = 145.8978, GNorm = 0.3278
Meta loss on this task batch = 2.4319e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 145.8961, GNorm = 0.2757
Meta loss on this task batch = 2.5449e-01, Meta loss averaged over last 500 steps = 2.6893e-01, PNorm = 145.8928, GNorm = 0.2939
Meta loss on this task batch = 2.8713e-01, Meta loss averaged over last 500 steps = 2.6894e-01, PNorm = 145.8870, GNorm = 0.3340
Meta loss on this task batch = 2.8700e-01, Meta loss averaged over last 500 steps = 2.6894e-01, PNorm = 145.8817, GNorm = 0.3141
Meta loss on this task batch = 2.4412e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 145.8771, GNorm = 0.2584
Meta loss on this task batch = 2.9988e-01, Meta loss averaged over last 500 steps = 2.6889e-01, PNorm = 145.8757, GNorm = 0.2874
Meta loss on this task batch = 2.7042e-01, Meta loss averaged over last 500 steps = 2.6891e-01, PNorm = 145.8755, GNorm = 0.2363
Meta loss on this task batch = 2.8142e-01, Meta loss averaged over last 500 steps = 2.6894e-01, PNorm = 145.8781, GNorm = 0.2840
Meta loss on this task batch = 3.2704e-01, Meta loss averaged over last 500 steps = 2.6887e-01, PNorm = 145.8828, GNorm = 0.3282
Meta loss on this task batch = 2.4624e-01, Meta loss averaged over last 500 steps = 2.6887e-01, PNorm = 145.8890, GNorm = 0.2840
Meta loss on this task batch = 3.0518e-01, Meta loss averaged over last 500 steps = 2.6886e-01, PNorm = 145.8963, GNorm = 0.3228
Meta loss on this task batch = 2.5819e-01, Meta loss averaged over last 500 steps = 2.6887e-01, PNorm = 145.9056, GNorm = 0.2959
Meta loss on this task batch = 2.6376e-01, Meta loss averaged over last 500 steps = 2.6882e-01, PNorm = 145.9166, GNorm = 0.3072
Meta loss on this task batch = 2.3847e-01, Meta loss averaged over last 500 steps = 2.6879e-01, PNorm = 145.9269, GNorm = 0.2998
Meta loss on this task batch = 2.4664e-01, Meta loss averaged over last 500 steps = 2.6867e-01, PNorm = 145.9368, GNorm = 0.2837
Meta loss on this task batch = 2.6425e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 145.9474, GNorm = 0.2550
Meta loss on this task batch = 2.5297e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 145.9568, GNorm = 0.3550
Took 136.901442527771 seconds to complete one epoch of meta training
Took 144.56639862060547 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478820
Epoch 807
Meta loss on this task batch = 2.6075e-01, Meta loss averaged over last 500 steps = 2.6854e-01, PNorm = 145.9655, GNorm = 0.2799
Meta loss on this task batch = 2.5226e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 145.9735, GNorm = 0.2583
Meta loss on this task batch = 2.3386e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 145.9811, GNorm = 0.2979
Meta loss on this task batch = 2.7671e-01, Meta loss averaged over last 500 steps = 2.6857e-01, PNorm = 145.9871, GNorm = 0.2701
Meta loss on this task batch = 3.3669e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 145.9903, GNorm = 0.3827
Meta loss on this task batch = 2.6686e-01, Meta loss averaged over last 500 steps = 2.6866e-01, PNorm = 145.9920, GNorm = 0.3163
Meta loss on this task batch = 2.7445e-01, Meta loss averaged over last 500 steps = 2.6867e-01, PNorm = 145.9953, GNorm = 0.2640
Meta loss on this task batch = 2.2777e-01, Meta loss averaged over last 500 steps = 2.6857e-01, PNorm = 145.9993, GNorm = 0.2790
Meta loss on this task batch = 2.1377e-01, Meta loss averaged over last 500 steps = 2.6839e-01, PNorm = 146.0033, GNorm = 0.2403
Meta loss on this task batch = 2.4307e-01, Meta loss averaged over last 500 steps = 2.6820e-01, PNorm = 146.0084, GNorm = 0.2799
Meta loss on this task batch = 2.5286e-01, Meta loss averaged over last 500 steps = 2.6830e-01, PNorm = 146.0146, GNorm = 0.2687
Meta loss on this task batch = 3.0355e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 146.0197, GNorm = 0.2816
Meta loss on this task batch = 2.4929e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 146.0242, GNorm = 0.2641
Meta loss on this task batch = 2.9440e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 146.0301, GNorm = 0.2993
Meta loss on this task batch = 3.2289e-01, Meta loss averaged over last 500 steps = 2.6885e-01, PNorm = 146.0372, GNorm = 0.3824
Meta loss on this task batch = 2.5519e-01, Meta loss averaged over last 500 steps = 2.6880e-01, PNorm = 146.0444, GNorm = 0.3262
Meta loss on this task batch = 1.8594e-01, Meta loss averaged over last 500 steps = 2.6862e-01, PNorm = 146.0509, GNorm = 0.2358
Meta loss on this task batch = 2.6009e-01, Meta loss averaged over last 500 steps = 2.6862e-01, PNorm = 146.0553, GNorm = 0.3003
Meta loss on this task batch = 2.7965e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 146.0601, GNorm = 0.4076
Took 136.45825624465942 seconds to complete one epoch of meta training
Took 145.0468647480011 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.512052
Epoch 808
Meta loss on this task batch = 2.8023e-01, Meta loss averaged over last 500 steps = 2.6853e-01, PNorm = 146.0641, GNorm = 0.3172
Meta loss on this task batch = 2.5756e-01, Meta loss averaged over last 500 steps = 2.6849e-01, PNorm = 146.0680, GNorm = 0.2901
Meta loss on this task batch = 2.5998e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 146.0733, GNorm = 0.3074
Meta loss on this task batch = 2.6138e-01, Meta loss averaged over last 500 steps = 2.6837e-01, PNorm = 146.0795, GNorm = 0.2670
Meta loss on this task batch = 2.6158e-01, Meta loss averaged over last 500 steps = 2.6824e-01, PNorm = 146.0851, GNorm = 0.3178
Meta loss on this task batch = 2.9606e-01, Meta loss averaged over last 500 steps = 2.6821e-01, PNorm = 146.0882, GNorm = 0.3197
Meta loss on this task batch = 2.6866e-01, Meta loss averaged over last 500 steps = 2.6821e-01, PNorm = 146.0909, GNorm = 0.2472
Meta loss on this task batch = 2.2635e-01, Meta loss averaged over last 500 steps = 2.6805e-01, PNorm = 146.0934, GNorm = 0.2708
Meta loss on this task batch = 2.7812e-01, Meta loss averaged over last 500 steps = 2.6803e-01, PNorm = 146.0963, GNorm = 0.2954
Meta loss on this task batch = 2.1959e-01, Meta loss averaged over last 500 steps = 2.6788e-01, PNorm = 146.0987, GNorm = 0.2527
Meta loss on this task batch = 2.5688e-01, Meta loss averaged over last 500 steps = 2.6787e-01, PNorm = 146.1013, GNorm = 0.2811
Meta loss on this task batch = 2.4594e-01, Meta loss averaged over last 500 steps = 2.6784e-01, PNorm = 146.1029, GNorm = 0.2638
Meta loss on this task batch = 2.6779e-01, Meta loss averaged over last 500 steps = 2.6784e-01, PNorm = 146.1048, GNorm = 0.3176
Meta loss on this task batch = 2.7063e-01, Meta loss averaged over last 500 steps = 2.6783e-01, PNorm = 146.1085, GNorm = 0.3095
Meta loss on this task batch = 2.4313e-01, Meta loss averaged over last 500 steps = 2.6779e-01, PNorm = 146.1123, GNorm = 0.2865
Meta loss on this task batch = 3.1380e-01, Meta loss averaged over last 500 steps = 2.6793e-01, PNorm = 146.1145, GNorm = 0.3042
Meta loss on this task batch = 2.5875e-01, Meta loss averaged over last 500 steps = 2.6782e-01, PNorm = 146.1162, GNorm = 0.3224
Meta loss on this task batch = 2.6432e-01, Meta loss averaged over last 500 steps = 2.6795e-01, PNorm = 146.1191, GNorm = 0.2760
Meta loss on this task batch = 2.6262e-01, Meta loss averaged over last 500 steps = 2.6795e-01, PNorm = 146.1223, GNorm = 0.3255
Took 145.99105191230774 seconds to complete one epoch of meta training
Took 154.9546823501587 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474187
Epoch 809
Meta loss on this task batch = 2.4566e-01, Meta loss averaged over last 500 steps = 2.6792e-01, PNorm = 146.1262, GNorm = 0.2859
Meta loss on this task batch = 2.5634e-01, Meta loss averaged over last 500 steps = 2.6795e-01, PNorm = 146.1313, GNorm = 0.2530
Meta loss on this task batch = 2.9215e-01, Meta loss averaged over last 500 steps = 2.6802e-01, PNorm = 146.1365, GNorm = 0.3373
Meta loss on this task batch = 2.8496e-01, Meta loss averaged over last 500 steps = 2.6809e-01, PNorm = 146.1424, GNorm = 0.2893
Meta loss on this task batch = 2.4230e-01, Meta loss averaged over last 500 steps = 2.6804e-01, PNorm = 146.1490, GNorm = 0.2990
Meta loss on this task batch = 2.4379e-01, Meta loss averaged over last 500 steps = 2.6788e-01, PNorm = 146.1540, GNorm = 0.2814
Meta loss on this task batch = 2.6175e-01, Meta loss averaged over last 500 steps = 2.6800e-01, PNorm = 146.1595, GNorm = 0.2729
Meta loss on this task batch = 2.5348e-01, Meta loss averaged over last 500 steps = 2.6802e-01, PNorm = 146.1656, GNorm = 0.2386
Meta loss on this task batch = 2.7875e-01, Meta loss averaged over last 500 steps = 2.6810e-01, PNorm = 146.1702, GNorm = 0.2740
Meta loss on this task batch = 2.3385e-01, Meta loss averaged over last 500 steps = 2.6807e-01, PNorm = 146.1739, GNorm = 0.3188
Meta loss on this task batch = 2.4248e-01, Meta loss averaged over last 500 steps = 2.6786e-01, PNorm = 146.1767, GNorm = 0.2684
Meta loss on this task batch = 2.1143e-01, Meta loss averaged over last 500 steps = 2.6770e-01, PNorm = 146.1798, GNorm = 0.2593
Meta loss on this task batch = 2.7579e-01, Meta loss averaged over last 500 steps = 2.6776e-01, PNorm = 146.1822, GNorm = 0.2614
Meta loss on this task batch = 2.6762e-01, Meta loss averaged over last 500 steps = 2.6768e-01, PNorm = 146.1836, GNorm = 0.2835
Meta loss on this task batch = 3.1130e-01, Meta loss averaged over last 500 steps = 2.6774e-01, PNorm = 146.1849, GNorm = 0.3149
Meta loss on this task batch = 2.9602e-01, Meta loss averaged over last 500 steps = 2.6783e-01, PNorm = 146.1860, GNorm = 0.3197
Meta loss on this task batch = 2.4179e-01, Meta loss averaged over last 500 steps = 2.6776e-01, PNorm = 146.1891, GNorm = 0.2687
Meta loss on this task batch = 2.8861e-01, Meta loss averaged over last 500 steps = 2.6771e-01, PNorm = 146.1941, GNorm = 0.2983
Meta loss on this task batch = 2.2492e-01, Meta loss averaged over last 500 steps = 2.6760e-01, PNorm = 146.2009, GNorm = 0.3297
Took 136.64988088607788 seconds to complete one epoch of meta training
Took 145.0663139820099 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487321
Epoch 810
Meta loss on this task batch = 2.7942e-01, Meta loss averaged over last 500 steps = 2.6768e-01, PNorm = 146.2090, GNorm = 0.2884
Meta loss on this task batch = 3.0361e-01, Meta loss averaged over last 500 steps = 2.6780e-01, PNorm = 146.2188, GNorm = 0.3119
Meta loss on this task batch = 2.2363e-01, Meta loss averaged over last 500 steps = 2.6765e-01, PNorm = 146.2274, GNorm = 0.2541
Meta loss on this task batch = 3.1152e-01, Meta loss averaged over last 500 steps = 2.6773e-01, PNorm = 146.2335, GNorm = 0.3426
Meta loss on this task batch = 2.1676e-01, Meta loss averaged over last 500 steps = 2.6766e-01, PNorm = 146.2374, GNorm = 0.2721
Meta loss on this task batch = 2.3215e-01, Meta loss averaged over last 500 steps = 2.6758e-01, PNorm = 146.2416, GNorm = 0.2777
Meta loss on this task batch = 2.4244e-01, Meta loss averaged over last 500 steps = 2.6758e-01, PNorm = 146.2446, GNorm = 0.2668
Meta loss on this task batch = 2.2378e-01, Meta loss averaged over last 500 steps = 2.6751e-01, PNorm = 146.2466, GNorm = 0.3033
Meta loss on this task batch = 2.9431e-01, Meta loss averaged over last 500 steps = 2.6743e-01, PNorm = 146.2471, GNorm = 0.3424
Meta loss on this task batch = 2.8565e-01, Meta loss averaged over last 500 steps = 2.6751e-01, PNorm = 146.2467, GNorm = 0.3100
Meta loss on this task batch = 2.7550e-01, Meta loss averaged over last 500 steps = 2.6758e-01, PNorm = 146.2465, GNorm = 0.2998
Meta loss on this task batch = 2.8886e-01, Meta loss averaged over last 500 steps = 2.6764e-01, PNorm = 146.2498, GNorm = 0.3066
Meta loss on this task batch = 2.2339e-01, Meta loss averaged over last 500 steps = 2.6754e-01, PNorm = 146.2564, GNorm = 0.2953
Meta loss on this task batch = 2.7989e-01, Meta loss averaged over last 500 steps = 2.6758e-01, PNorm = 146.2662, GNorm = 0.3514
Meta loss on this task batch = 2.3973e-01, Meta loss averaged over last 500 steps = 2.6758e-01, PNorm = 146.2769, GNorm = 0.2591
Meta loss on this task batch = 2.9982e-01, Meta loss averaged over last 500 steps = 2.6767e-01, PNorm = 146.2895, GNorm = 0.2961
Meta loss on this task batch = 2.6119e-01, Meta loss averaged over last 500 steps = 2.6755e-01, PNorm = 146.3019, GNorm = 0.2980
Meta loss on this task batch = 3.3745e-01, Meta loss averaged over last 500 steps = 2.6771e-01, PNorm = 146.3137, GNorm = 0.2717
Meta loss on this task batch = 2.7411e-01, Meta loss averaged over last 500 steps = 2.6765e-01, PNorm = 146.3253, GNorm = 0.4200
Took 147.71038460731506 seconds to complete one epoch of meta training
Took 156.8495535850525 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509020
Epoch 811
Meta loss on this task batch = 2.8961e-01, Meta loss averaged over last 500 steps = 2.6766e-01, PNorm = 146.3351, GNorm = 0.3116
Meta loss on this task batch = 2.6401e-01, Meta loss averaged over last 500 steps = 2.6757e-01, PNorm = 146.3435, GNorm = 0.2806
Meta loss on this task batch = 2.3034e-01, Meta loss averaged over last 500 steps = 2.6749e-01, PNorm = 146.3510, GNorm = 0.3108
Meta loss on this task batch = 3.2184e-01, Meta loss averaged over last 500 steps = 2.6760e-01, PNorm = 146.3572, GNorm = 0.3369
Meta loss on this task batch = 2.9010e-01, Meta loss averaged over last 500 steps = 2.6772e-01, PNorm = 146.3628, GNorm = 0.2956
Meta loss on this task batch = 2.9323e-01, Meta loss averaged over last 500 steps = 2.6769e-01, PNorm = 146.3667, GNorm = 0.3409
Meta loss on this task batch = 2.5075e-01, Meta loss averaged over last 500 steps = 2.6764e-01, PNorm = 146.3699, GNorm = 0.2855
Meta loss on this task batch = 2.4318e-01, Meta loss averaged over last 500 steps = 2.6757e-01, PNorm = 146.3722, GNorm = 0.2736
Meta loss on this task batch = 2.8029e-01, Meta loss averaged over last 500 steps = 2.6755e-01, PNorm = 146.3704, GNorm = 0.3484
Meta loss on this task batch = 2.8352e-01, Meta loss averaged over last 500 steps = 2.6754e-01, PNorm = 146.3671, GNorm = 0.2920
Meta loss on this task batch = 2.7339e-01, Meta loss averaged over last 500 steps = 2.6763e-01, PNorm = 146.3647, GNorm = 0.2432
Meta loss on this task batch = 2.9205e-01, Meta loss averaged over last 500 steps = 2.6769e-01, PNorm = 146.3612, GNorm = 0.3177
Meta loss on this task batch = 2.8459e-01, Meta loss averaged over last 500 steps = 2.6769e-01, PNorm = 146.3581, GNorm = 0.3002
Meta loss on this task batch = 2.7401e-01, Meta loss averaged over last 500 steps = 2.6784e-01, PNorm = 146.3572, GNorm = 0.2841
Meta loss on this task batch = 2.1823e-01, Meta loss averaged over last 500 steps = 2.6784e-01, PNorm = 146.3570, GNorm = 0.2743
Meta loss on this task batch = 3.2693e-01, Meta loss averaged over last 500 steps = 2.6796e-01, PNorm = 146.3555, GNorm = 0.3276
Meta loss on this task batch = 2.2049e-01, Meta loss averaged over last 500 steps = 2.6794e-01, PNorm = 146.3543, GNorm = 0.2277
Meta loss on this task batch = 2.6566e-01, Meta loss averaged over last 500 steps = 2.6795e-01, PNorm = 146.3557, GNorm = 0.2848
Meta loss on this task batch = 2.6485e-01, Meta loss averaged over last 500 steps = 2.6790e-01, PNorm = 146.3580, GNorm = 0.3192
Took 134.7287061214447 seconds to complete one epoch of meta training
Took 143.42082595825195 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492577
Epoch 812
Meta loss on this task batch = 2.3908e-01, Meta loss averaged over last 500 steps = 2.6796e-01, PNorm = 146.3617, GNorm = 0.2942
Meta loss on this task batch = 2.5241e-01, Meta loss averaged over last 500 steps = 2.6788e-01, PNorm = 146.3660, GNorm = 0.2660
Meta loss on this task batch = 3.0852e-01, Meta loss averaged over last 500 steps = 2.6804e-01, PNorm = 146.3713, GNorm = 0.3407
Meta loss on this task batch = 2.6070e-01, Meta loss averaged over last 500 steps = 2.6800e-01, PNorm = 146.3761, GNorm = 0.2962
Meta loss on this task batch = 2.5861e-01, Meta loss averaged over last 500 steps = 2.6788e-01, PNorm = 146.3805, GNorm = 0.3048
Meta loss on this task batch = 2.5102e-01, Meta loss averaged over last 500 steps = 2.6773e-01, PNorm = 146.3861, GNorm = 0.2678
Meta loss on this task batch = 2.3516e-01, Meta loss averaged over last 500 steps = 2.6777e-01, PNorm = 146.3930, GNorm = 0.2552
Meta loss on this task batch = 2.5149e-01, Meta loss averaged over last 500 steps = 2.6775e-01, PNorm = 146.3992, GNorm = 0.2518
Meta loss on this task batch = 2.1851e-01, Meta loss averaged over last 500 steps = 2.6766e-01, PNorm = 146.4037, GNorm = 0.2575
Meta loss on this task batch = 2.8558e-01, Meta loss averaged over last 500 steps = 2.6774e-01, PNorm = 146.4072, GNorm = 0.3047
Meta loss on this task batch = 2.5783e-01, Meta loss averaged over last 500 steps = 2.6780e-01, PNorm = 146.4073, GNorm = 0.2813
Meta loss on this task batch = 3.1656e-01, Meta loss averaged over last 500 steps = 2.6789e-01, PNorm = 146.4054, GNorm = 0.3602
Meta loss on this task batch = 2.9902e-01, Meta loss averaged over last 500 steps = 2.6794e-01, PNorm = 146.4028, GNorm = 0.3226
Meta loss on this task batch = 2.7971e-01, Meta loss averaged over last 500 steps = 2.6805e-01, PNorm = 146.4018, GNorm = 0.2867
Meta loss on this task batch = 2.8888e-01, Meta loss averaged over last 500 steps = 2.6800e-01, PNorm = 146.4017, GNorm = 0.3367
Meta loss on this task batch = 2.5661e-01, Meta loss averaged over last 500 steps = 2.6797e-01, PNorm = 146.4033, GNorm = 0.2784
Meta loss on this task batch = 2.4633e-01, Meta loss averaged over last 500 steps = 2.6788e-01, PNorm = 146.4066, GNorm = 0.2577
Meta loss on this task batch = 2.8718e-01, Meta loss averaged over last 500 steps = 2.6788e-01, PNorm = 146.4101, GNorm = 0.2935
Meta loss on this task batch = 2.6766e-01, Meta loss averaged over last 500 steps = 2.6788e-01, PNorm = 146.4158, GNorm = 0.3784
Took 139.44313859939575 seconds to complete one epoch of meta training
Took 148.75660729408264 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503578
Epoch 813
Meta loss on this task batch = 2.9136e-01, Meta loss averaged over last 500 steps = 2.6778e-01, PNorm = 146.4246, GNorm = 0.3301
Meta loss on this task batch = 2.7347e-01, Meta loss averaged over last 500 steps = 2.6794e-01, PNorm = 146.4328, GNorm = 0.2747
Meta loss on this task batch = 2.7514e-01, Meta loss averaged over last 500 steps = 2.6786e-01, PNorm = 146.4415, GNorm = 0.3094
Meta loss on this task batch = 2.5416e-01, Meta loss averaged over last 500 steps = 2.6787e-01, PNorm = 146.4507, GNorm = 0.2868
Meta loss on this task batch = 2.0915e-01, Meta loss averaged over last 500 steps = 2.6786e-01, PNorm = 146.4594, GNorm = 0.2429
Meta loss on this task batch = 2.7555e-01, Meta loss averaged over last 500 steps = 2.6792e-01, PNorm = 146.4686, GNorm = 0.2755
Meta loss on this task batch = 2.7556e-01, Meta loss averaged over last 500 steps = 2.6789e-01, PNorm = 146.4786, GNorm = 0.3394
Meta loss on this task batch = 2.5922e-01, Meta loss averaged over last 500 steps = 2.6785e-01, PNorm = 146.4875, GNorm = 0.2780
Meta loss on this task batch = 2.7013e-01, Meta loss averaged over last 500 steps = 2.6782e-01, PNorm = 146.4954, GNorm = 0.3115
Meta loss on this task batch = 2.3567e-01, Meta loss averaged over last 500 steps = 2.6778e-01, PNorm = 146.5033, GNorm = 0.2641
Meta loss on this task batch = 2.8206e-01, Meta loss averaged over last 500 steps = 2.6771e-01, PNorm = 146.5109, GNorm = 0.3120
Meta loss on this task batch = 2.7346e-01, Meta loss averaged over last 500 steps = 2.6782e-01, PNorm = 146.5174, GNorm = 0.2878
Meta loss on this task batch = 2.5110e-01, Meta loss averaged over last 500 steps = 2.6786e-01, PNorm = 146.5234, GNorm = 0.2587
Meta loss on this task batch = 2.4196e-01, Meta loss averaged over last 500 steps = 2.6775e-01, PNorm = 146.5278, GNorm = 0.3264
Meta loss on this task batch = 2.7371e-01, Meta loss averaged over last 500 steps = 2.6768e-01, PNorm = 146.5307, GNorm = 0.2907
Meta loss on this task batch = 3.1295e-01, Meta loss averaged over last 500 steps = 2.6771e-01, PNorm = 146.5326, GNorm = 0.3123
Meta loss on this task batch = 2.5698e-01, Meta loss averaged over last 500 steps = 2.6776e-01, PNorm = 146.5334, GNorm = 0.3044
Meta loss on this task batch = 2.4360e-01, Meta loss averaged over last 500 steps = 2.6756e-01, PNorm = 146.5338, GNorm = 0.3252
Meta loss on this task batch = 2.7009e-01, Meta loss averaged over last 500 steps = 2.6763e-01, PNorm = 146.5340, GNorm = 0.3341
Took 138.90171551704407 seconds to complete one epoch of meta training
Took 146.71838068962097 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496604
Epoch 814
Meta loss on this task batch = 2.4425e-01, Meta loss averaged over last 500 steps = 2.6751e-01, PNorm = 146.5354, GNorm = 0.2730
Meta loss on this task batch = 2.8375e-01, Meta loss averaged over last 500 steps = 2.6758e-01, PNorm = 146.5380, GNorm = 0.3025
Meta loss on this task batch = 2.8567e-01, Meta loss averaged over last 500 steps = 2.6751e-01, PNorm = 146.5396, GNorm = 0.3168
Meta loss on this task batch = 2.4653e-01, Meta loss averaged over last 500 steps = 2.6741e-01, PNorm = 146.5417, GNorm = 0.3313
Meta loss on this task batch = 2.9408e-01, Meta loss averaged over last 500 steps = 2.6760e-01, PNorm = 146.5429, GNorm = 0.3030
Meta loss on this task batch = 2.3287e-01, Meta loss averaged over last 500 steps = 2.6749e-01, PNorm = 146.5444, GNorm = 0.2982
Meta loss on this task batch = 2.9327e-01, Meta loss averaged over last 500 steps = 2.6744e-01, PNorm = 146.5446, GNorm = 0.2776
Meta loss on this task batch = 2.7187e-01, Meta loss averaged over last 500 steps = 2.6741e-01, PNorm = 146.5431, GNorm = 0.3411
Meta loss on this task batch = 2.3579e-01, Meta loss averaged over last 500 steps = 2.6738e-01, PNorm = 146.5438, GNorm = 0.2859
Meta loss on this task batch = 2.4662e-01, Meta loss averaged over last 500 steps = 2.6733e-01, PNorm = 146.5435, GNorm = 0.2665
Meta loss on this task batch = 2.3089e-01, Meta loss averaged over last 500 steps = 2.6729e-01, PNorm = 146.5429, GNorm = 0.2818
Meta loss on this task batch = 2.4964e-01, Meta loss averaged over last 500 steps = 2.6732e-01, PNorm = 146.5408, GNorm = 0.3031
Meta loss on this task batch = 2.4336e-01, Meta loss averaged over last 500 steps = 2.6724e-01, PNorm = 146.5385, GNorm = 0.2783
Meta loss on this task batch = 2.6871e-01, Meta loss averaged over last 500 steps = 2.6728e-01, PNorm = 146.5375, GNorm = 0.2697
Meta loss on this task batch = 3.0977e-01, Meta loss averaged over last 500 steps = 2.6730e-01, PNorm = 146.5378, GNorm = 0.3272
Meta loss on this task batch = 3.0131e-01, Meta loss averaged over last 500 steps = 2.6736e-01, PNorm = 146.5400, GNorm = 0.3188
Meta loss on this task batch = 2.1239e-01, Meta loss averaged over last 500 steps = 2.6722e-01, PNorm = 146.5420, GNorm = 0.2616
Meta loss on this task batch = 2.7168e-01, Meta loss averaged over last 500 steps = 2.6729e-01, PNorm = 146.5433, GNorm = 0.2917
Meta loss on this task batch = 2.7507e-01, Meta loss averaged over last 500 steps = 2.6729e-01, PNorm = 146.5449, GNorm = 0.3770
Took 136.6812298297882 seconds to complete one epoch of meta training
Took 145.6521782875061 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510408
Epoch 815
Meta loss on this task batch = 2.3671e-01, Meta loss averaged over last 500 steps = 2.6733e-01, PNorm = 146.5487, GNorm = 0.2442
Meta loss on this task batch = 2.5773e-01, Meta loss averaged over last 500 steps = 2.6724e-01, PNorm = 146.5526, GNorm = 0.2728
Meta loss on this task batch = 2.5649e-01, Meta loss averaged over last 500 steps = 2.6724e-01, PNorm = 146.5563, GNorm = 0.2811
Meta loss on this task batch = 2.6498e-01, Meta loss averaged over last 500 steps = 2.6721e-01, PNorm = 146.5599, GNorm = 0.2652
Meta loss on this task batch = 2.9435e-01, Meta loss averaged over last 500 steps = 2.6730e-01, PNorm = 146.5620, GNorm = 0.2910
Meta loss on this task batch = 2.8449e-01, Meta loss averaged over last 500 steps = 2.6728e-01, PNorm = 146.5632, GNorm = 0.2797
Meta loss on this task batch = 2.8218e-01, Meta loss averaged over last 500 steps = 2.6727e-01, PNorm = 146.5658, GNorm = 0.3547
Meta loss on this task batch = 2.4272e-01, Meta loss averaged over last 500 steps = 2.6709e-01, PNorm = 146.5686, GNorm = 0.2584
Meta loss on this task batch = 2.6148e-01, Meta loss averaged over last 500 steps = 2.6717e-01, PNorm = 146.5712, GNorm = 0.3059
Meta loss on this task batch = 2.7595e-01, Meta loss averaged over last 500 steps = 2.6722e-01, PNorm = 146.5728, GNorm = 0.2755
Meta loss on this task batch = 2.9180e-01, Meta loss averaged over last 500 steps = 2.6733e-01, PNorm = 146.5734, GNorm = 0.2642
Meta loss on this task batch = 2.6139e-01, Meta loss averaged over last 500 steps = 2.6732e-01, PNorm = 146.5745, GNorm = 0.3360
Meta loss on this task batch = 3.0534e-01, Meta loss averaged over last 500 steps = 2.6744e-01, PNorm = 146.5748, GNorm = 0.3479
Meta loss on this task batch = 2.3275e-01, Meta loss averaged over last 500 steps = 2.6738e-01, PNorm = 146.5750, GNorm = 0.2674
Meta loss on this task batch = 2.8143e-01, Meta loss averaged over last 500 steps = 2.6738e-01, PNorm = 146.5760, GNorm = 0.2913
Meta loss on this task batch = 2.6324e-01, Meta loss averaged over last 500 steps = 2.6748e-01, PNorm = 146.5787, GNorm = 0.3221
Meta loss on this task batch = 2.5014e-01, Meta loss averaged over last 500 steps = 2.6746e-01, PNorm = 146.5804, GNorm = 0.3321
Meta loss on this task batch = 2.0619e-01, Meta loss averaged over last 500 steps = 2.6739e-01, PNorm = 146.5845, GNorm = 0.2401
Meta loss on this task batch = 2.3049e-01, Meta loss averaged over last 500 steps = 2.6734e-01, PNorm = 146.5900, GNorm = 0.3667
Took 134.9404194355011 seconds to complete one epoch of meta training
Took 142.31916737556458 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492221
Epoch 816
Meta loss on this task batch = 2.5592e-01, Meta loss averaged over last 500 steps = 2.6735e-01, PNorm = 146.5959, GNorm = 0.2750
Meta loss on this task batch = 3.2578e-01, Meta loss averaged over last 500 steps = 2.6750e-01, PNorm = 146.6031, GNorm = 0.4393
Meta loss on this task batch = 3.0011e-01, Meta loss averaged over last 500 steps = 2.6752e-01, PNorm = 146.6088, GNorm = 0.2874
Meta loss on this task batch = 2.6685e-01, Meta loss averaged over last 500 steps = 2.6743e-01, PNorm = 146.6135, GNorm = 0.3209
Meta loss on this task batch = 2.5806e-01, Meta loss averaged over last 500 steps = 2.6747e-01, PNorm = 146.6184, GNorm = 0.2599
Meta loss on this task batch = 2.3724e-01, Meta loss averaged over last 500 steps = 2.6738e-01, PNorm = 146.6227, GNorm = 0.2716
Meta loss on this task batch = 2.7076e-01, Meta loss averaged over last 500 steps = 2.6745e-01, PNorm = 146.6288, GNorm = 0.3080
Meta loss on this task batch = 2.6230e-01, Meta loss averaged over last 500 steps = 2.6753e-01, PNorm = 146.6361, GNorm = 0.3097
Meta loss on this task batch = 2.7876e-01, Meta loss averaged over last 500 steps = 2.6750e-01, PNorm = 146.6429, GNorm = 0.2705
Meta loss on this task batch = 2.7847e-01, Meta loss averaged over last 500 steps = 2.6754e-01, PNorm = 146.6495, GNorm = 0.2233
Meta loss on this task batch = 2.6708e-01, Meta loss averaged over last 500 steps = 2.6757e-01, PNorm = 146.6549, GNorm = 0.3043
Meta loss on this task batch = 2.7006e-01, Meta loss averaged over last 500 steps = 2.6741e-01, PNorm = 146.6582, GNorm = 0.3072
Meta loss on this task batch = 2.3781e-01, Meta loss averaged over last 500 steps = 2.6730e-01, PNorm = 146.6598, GNorm = 0.2885
Meta loss on this task batch = 2.4238e-01, Meta loss averaged over last 500 steps = 2.6726e-01, PNorm = 146.6602, GNorm = 0.2704
Meta loss on this task batch = 2.9309e-01, Meta loss averaged over last 500 steps = 2.6733e-01, PNorm = 146.6594, GNorm = 0.2993
Meta loss on this task batch = 2.3124e-01, Meta loss averaged over last 500 steps = 2.6728e-01, PNorm = 146.6564, GNorm = 0.3049
Meta loss on this task batch = 2.6352e-01, Meta loss averaged over last 500 steps = 2.6722e-01, PNorm = 146.6550, GNorm = 0.2576
Meta loss on this task batch = 2.6844e-01, Meta loss averaged over last 500 steps = 2.6722e-01, PNorm = 146.6538, GNorm = 0.2881
Meta loss on this task batch = 2.3809e-01, Meta loss averaged over last 500 steps = 2.6719e-01, PNorm = 146.6550, GNorm = 0.3067
Took 137.6309266090393 seconds to complete one epoch of meta training
Took 146.69865345954895 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486156
Epoch 817
Meta loss on this task batch = 2.5468e-01, Meta loss averaged over last 500 steps = 2.6716e-01, PNorm = 146.6559, GNorm = 0.2964
Meta loss on this task batch = 2.7233e-01, Meta loss averaged over last 500 steps = 2.6718e-01, PNorm = 146.6567, GNorm = 0.3234
Meta loss on this task batch = 3.1360e-01, Meta loss averaged over last 500 steps = 2.6734e-01, PNorm = 146.6579, GNorm = 0.3417
Meta loss on this task batch = 3.0150e-01, Meta loss averaged over last 500 steps = 2.6740e-01, PNorm = 146.6604, GNorm = 0.3225
Meta loss on this task batch = 2.7858e-01, Meta loss averaged over last 500 steps = 2.6741e-01, PNorm = 146.6633, GNorm = 0.3066
Meta loss on this task batch = 2.9978e-01, Meta loss averaged over last 500 steps = 2.6741e-01, PNorm = 146.6686, GNorm = 0.3005
Meta loss on this task batch = 1.6779e-01, Meta loss averaged over last 500 steps = 2.6714e-01, PNorm = 146.6756, GNorm = 0.2572
Meta loss on this task batch = 3.1794e-01, Meta loss averaged over last 500 steps = 2.6730e-01, PNorm = 146.6833, GNorm = 0.3032
Meta loss on this task batch = 2.5110e-01, Meta loss averaged over last 500 steps = 2.6723e-01, PNorm = 146.6920, GNorm = 0.2915
Meta loss on this task batch = 3.2590e-01, Meta loss averaged over last 500 steps = 2.6730e-01, PNorm = 146.6992, GNorm = 0.3234
Meta loss on this task batch = 2.5174e-01, Meta loss averaged over last 500 steps = 2.6727e-01, PNorm = 146.7045, GNorm = 0.2988
Meta loss on this task batch = 2.4567e-01, Meta loss averaged over last 500 steps = 2.6723e-01, PNorm = 146.7090, GNorm = 0.2508
Meta loss on this task batch = 2.5757e-01, Meta loss averaged over last 500 steps = 2.6721e-01, PNorm = 146.7129, GNorm = 0.2594
Meta loss on this task batch = 2.7223e-01, Meta loss averaged over last 500 steps = 2.6712e-01, PNorm = 146.7168, GNorm = 0.2692
Meta loss on this task batch = 2.8026e-01, Meta loss averaged over last 500 steps = 2.6713e-01, PNorm = 146.7203, GNorm = 0.3045
Meta loss on this task batch = 2.0454e-01, Meta loss averaged over last 500 steps = 2.6700e-01, PNorm = 146.7254, GNorm = 0.2616
Meta loss on this task batch = 2.7805e-01, Meta loss averaged over last 500 steps = 2.6695e-01, PNorm = 146.7287, GNorm = 0.2587
Meta loss on this task batch = 3.0777e-01, Meta loss averaged over last 500 steps = 2.6705e-01, PNorm = 146.7299, GNorm = 0.2818
Meta loss on this task batch = 2.4843e-01, Meta loss averaged over last 500 steps = 2.6702e-01, PNorm = 146.7322, GNorm = 0.2962
Took 137.1594524383545 seconds to complete one epoch of meta training
Took 146.10650277137756 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498787
Epoch 818
Meta loss on this task batch = 2.5884e-01, Meta loss averaged over last 500 steps = 2.6695e-01, PNorm = 146.7345, GNorm = 0.3097
Meta loss on this task batch = 2.8534e-01, Meta loss averaged over last 500 steps = 2.6700e-01, PNorm = 146.7362, GNorm = 0.3330
Meta loss on this task batch = 2.4460e-01, Meta loss averaged over last 500 steps = 2.6700e-01, PNorm = 146.7389, GNorm = 0.2994
Meta loss on this task batch = 2.4225e-01, Meta loss averaged over last 500 steps = 2.6703e-01, PNorm = 146.7413, GNorm = 0.2732
Meta loss on this task batch = 2.1287e-01, Meta loss averaged over last 500 steps = 2.6699e-01, PNorm = 146.7458, GNorm = 0.2627
Meta loss on this task batch = 2.6326e-01, Meta loss averaged over last 500 steps = 2.6693e-01, PNorm = 146.7507, GNorm = 0.2926
Meta loss on this task batch = 3.1803e-01, Meta loss averaged over last 500 steps = 2.6700e-01, PNorm = 146.7539, GNorm = 0.3255
Meta loss on this task batch = 2.1488e-01, Meta loss averaged over last 500 steps = 2.6686e-01, PNorm = 146.7567, GNorm = 0.2679
Meta loss on this task batch = 2.5263e-01, Meta loss averaged over last 500 steps = 2.6675e-01, PNorm = 146.7596, GNorm = 0.2662
Meta loss on this task batch = 2.3381e-01, Meta loss averaged over last 500 steps = 2.6670e-01, PNorm = 146.7630, GNorm = 0.2244
Meta loss on this task batch = 3.1542e-01, Meta loss averaged over last 500 steps = 2.6664e-01, PNorm = 146.7641, GNorm = 0.3229
Meta loss on this task batch = 2.5934e-01, Meta loss averaged over last 500 steps = 2.6661e-01, PNorm = 146.7656, GNorm = 0.2722
Meta loss on this task batch = 2.6976e-01, Meta loss averaged over last 500 steps = 2.6656e-01, PNorm = 146.7692, GNorm = 0.2650
Meta loss on this task batch = 2.9668e-01, Meta loss averaged over last 500 steps = 2.6663e-01, PNorm = 146.7717, GNorm = 0.2951
Meta loss on this task batch = 3.0225e-01, Meta loss averaged over last 500 steps = 2.6667e-01, PNorm = 146.7749, GNorm = 0.2767
Meta loss on this task batch = 2.9691e-01, Meta loss averaged over last 500 steps = 2.6680e-01, PNorm = 146.7806, GNorm = 0.3186
Meta loss on this task batch = 2.6636e-01, Meta loss averaged over last 500 steps = 2.6679e-01, PNorm = 146.7868, GNorm = 0.3079
Meta loss on this task batch = 3.2013e-01, Meta loss averaged over last 500 steps = 2.6699e-01, PNorm = 146.7938, GNorm = 0.3205
Meta loss on this task batch = 2.8712e-01, Meta loss averaged over last 500 steps = 2.6693e-01, PNorm = 146.8023, GNorm = 0.3742
Took 137.77395510673523 seconds to complete one epoch of meta training
Took 146.40251564979553 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509580
Epoch 819
Meta loss on this task batch = 2.9881e-01, Meta loss averaged over last 500 steps = 2.6709e-01, PNorm = 146.8117, GNorm = 0.2955
Meta loss on this task batch = 3.2651e-01, Meta loss averaged over last 500 steps = 2.6721e-01, PNorm = 146.8222, GNorm = 0.3701
Meta loss on this task batch = 2.4135e-01, Meta loss averaged over last 500 steps = 2.6715e-01, PNorm = 146.8357, GNorm = 0.3273
Meta loss on this task batch = 2.9473e-01, Meta loss averaged over last 500 steps = 2.6730e-01, PNorm = 146.8489, GNorm = 0.2773
Meta loss on this task batch = 1.8065e-01, Meta loss averaged over last 500 steps = 2.6711e-01, PNorm = 146.8617, GNorm = 0.2356
Meta loss on this task batch = 2.5062e-01, Meta loss averaged over last 500 steps = 2.6715e-01, PNorm = 146.8741, GNorm = 0.2679
Meta loss on this task batch = 2.0686e-01, Meta loss averaged over last 500 steps = 2.6691e-01, PNorm = 146.8866, GNorm = 0.2772
Meta loss on this task batch = 2.6115e-01, Meta loss averaged over last 500 steps = 2.6698e-01, PNorm = 146.8962, GNorm = 0.4015
Meta loss on this task batch = 3.0002e-01, Meta loss averaged over last 500 steps = 2.6707e-01, PNorm = 146.9042, GNorm = 0.2845
Meta loss on this task batch = 2.6912e-01, Meta loss averaged over last 500 steps = 2.6707e-01, PNorm = 146.9121, GNorm = 0.2777
Meta loss on this task batch = 2.6969e-01, Meta loss averaged over last 500 steps = 2.6702e-01, PNorm = 146.9195, GNorm = 0.2545
Meta loss on this task batch = 2.7625e-01, Meta loss averaged over last 500 steps = 2.6708e-01, PNorm = 146.9248, GNorm = 0.3309
Meta loss on this task batch = 3.2246e-01, Meta loss averaged over last 500 steps = 2.6723e-01, PNorm = 146.9284, GNorm = 0.3173
Meta loss on this task batch = 2.3499e-01, Meta loss averaged over last 500 steps = 2.6719e-01, PNorm = 146.9313, GNorm = 0.2735
Meta loss on this task batch = 2.4480e-01, Meta loss averaged over last 500 steps = 2.6721e-01, PNorm = 146.9338, GNorm = 0.2836
Meta loss on this task batch = 2.8046e-01, Meta loss averaged over last 500 steps = 2.6717e-01, PNorm = 146.9356, GNorm = 0.3035
Meta loss on this task batch = 2.9171e-01, Meta loss averaged over last 500 steps = 2.6729e-01, PNorm = 146.9378, GNorm = 0.3116
Meta loss on this task batch = 2.2860e-01, Meta loss averaged over last 500 steps = 2.6722e-01, PNorm = 146.9403, GNorm = 0.2537
Meta loss on this task batch = 2.5147e-01, Meta loss averaged over last 500 steps = 2.6715e-01, PNorm = 146.9448, GNorm = 0.2900
Took 138.06953620910645 seconds to complete one epoch of meta training
Took 147.78985953330994 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508885
Epoch 820
Meta loss on this task batch = 2.6531e-01, Meta loss averaged over last 500 steps = 2.6712e-01, PNorm = 146.9458, GNorm = 0.4546
Meta loss on this task batch = 2.2154e-01, Meta loss averaged over last 500 steps = 2.6705e-01, PNorm = 146.9483, GNorm = 0.2280
Meta loss on this task batch = 3.1745e-01, Meta loss averaged over last 500 steps = 2.6700e-01, PNorm = 146.9511, GNorm = 0.2957
Meta loss on this task batch = 2.5093e-01, Meta loss averaged over last 500 steps = 2.6694e-01, PNorm = 146.9540, GNorm = 0.2618
Meta loss on this task batch = 2.5475e-01, Meta loss averaged over last 500 steps = 2.6687e-01, PNorm = 146.9594, GNorm = 0.3154
Meta loss on this task batch = 2.8800e-01, Meta loss averaged over last 500 steps = 2.6695e-01, PNorm = 146.9660, GNorm = 0.3117
Meta loss on this task batch = 2.9236e-01, Meta loss averaged over last 500 steps = 2.6708e-01, PNorm = 146.9741, GNorm = 0.3197
Meta loss on this task batch = 2.3690e-01, Meta loss averaged over last 500 steps = 2.6701e-01, PNorm = 146.9810, GNorm = 0.2666
Meta loss on this task batch = 3.2231e-01, Meta loss averaged over last 500 steps = 2.6715e-01, PNorm = 146.9868, GNorm = 0.4643
Meta loss on this task batch = 2.4058e-01, Meta loss averaged over last 500 steps = 2.6700e-01, PNorm = 146.9922, GNorm = 0.2749
Meta loss on this task batch = 2.2155e-01, Meta loss averaged over last 500 steps = 2.6694e-01, PNorm = 146.9977, GNorm = 0.2646
Meta loss on this task batch = 2.6526e-01, Meta loss averaged over last 500 steps = 2.6689e-01, PNorm = 147.0030, GNorm = 0.2842
Meta loss on this task batch = 2.7956e-01, Meta loss averaged over last 500 steps = 2.6689e-01, PNorm = 147.0060, GNorm = 0.3046
Meta loss on this task batch = 2.7534e-01, Meta loss averaged over last 500 steps = 2.6691e-01, PNorm = 147.0072, GNorm = 0.3439
Meta loss on this task batch = 3.0092e-01, Meta loss averaged over last 500 steps = 2.6710e-01, PNorm = 147.0072, GNorm = 0.3028
Meta loss on this task batch = 2.7916e-01, Meta loss averaged over last 500 steps = 2.6710e-01, PNorm = 147.0068, GNorm = 0.3016
Meta loss on this task batch = 3.4068e-01, Meta loss averaged over last 500 steps = 2.6722e-01, PNorm = 147.0044, GNorm = 0.3627
Meta loss on this task batch = 2.2823e-01, Meta loss averaged over last 500 steps = 2.6722e-01, PNorm = 147.0033, GNorm = 0.2863
Meta loss on this task batch = 2.5579e-01, Meta loss averaged over last 500 steps = 2.6706e-01, PNorm = 147.0031, GNorm = 0.3591
Took 138.27113556861877 seconds to complete one epoch of meta training
Took 147.59831547737122 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505541
Epoch 821
Meta loss on this task batch = 2.4852e-01, Meta loss averaged over last 500 steps = 2.6687e-01, PNorm = 147.0040, GNorm = 0.2912
Meta loss on this task batch = 2.5098e-01, Meta loss averaged over last 500 steps = 2.6679e-01, PNorm = 147.0054, GNorm = 0.2728
Meta loss on this task batch = 2.9162e-01, Meta loss averaged over last 500 steps = 2.6685e-01, PNorm = 147.0075, GNorm = 0.3230
Meta loss on this task batch = 2.9354e-01, Meta loss averaged over last 500 steps = 2.6699e-01, PNorm = 147.0110, GNorm = 0.2597
Meta loss on this task batch = 2.5546e-01, Meta loss averaged over last 500 steps = 2.6699e-01, PNorm = 147.0172, GNorm = 0.3072
Meta loss on this task batch = 2.2564e-01, Meta loss averaged over last 500 steps = 2.6698e-01, PNorm = 147.0259, GNorm = 0.2865
Meta loss on this task batch = 2.2807e-01, Meta loss averaged over last 500 steps = 2.6687e-01, PNorm = 147.0356, GNorm = 0.2905
Meta loss on this task batch = 2.5516e-01, Meta loss averaged over last 500 steps = 2.6694e-01, PNorm = 147.0469, GNorm = 0.2763
Meta loss on this task batch = 2.8271e-01, Meta loss averaged over last 500 steps = 2.6699e-01, PNorm = 147.0575, GNorm = 0.3261
Meta loss on this task batch = 3.3577e-01, Meta loss averaged over last 500 steps = 2.6718e-01, PNorm = 147.0670, GNorm = 0.3211
Meta loss on this task batch = 2.8790e-01, Meta loss averaged over last 500 steps = 2.6716e-01, PNorm = 147.0731, GNorm = 0.2948
Meta loss on this task batch = 2.5626e-01, Meta loss averaged over last 500 steps = 2.6727e-01, PNorm = 147.0786, GNorm = 0.2633
Meta loss on this task batch = 2.5760e-01, Meta loss averaged over last 500 steps = 2.6726e-01, PNorm = 147.0838, GNorm = 0.2665
Meta loss on this task batch = 2.8557e-01, Meta loss averaged over last 500 steps = 2.6733e-01, PNorm = 147.0882, GNorm = 0.2892
Meta loss on this task batch = 2.4575e-01, Meta loss averaged over last 500 steps = 2.6725e-01, PNorm = 147.0908, GNorm = 0.2941
Meta loss on this task batch = 2.5508e-01, Meta loss averaged over last 500 steps = 2.6722e-01, PNorm = 147.0919, GNorm = 0.2669
Meta loss on this task batch = 2.8768e-01, Meta loss averaged over last 500 steps = 2.6735e-01, PNorm = 147.0942, GNorm = 0.3240
Meta loss on this task batch = 2.1223e-01, Meta loss averaged over last 500 steps = 2.6726e-01, PNorm = 147.0970, GNorm = 0.2358
Meta loss on this task batch = 3.0326e-01, Meta loss averaged over last 500 steps = 2.6738e-01, PNorm = 147.0984, GNorm = 0.3412
Took 138.98017644882202 seconds to complete one epoch of meta training
Took 148.5129759311676 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503703
Epoch 822
Meta loss on this task batch = 2.6462e-01, Meta loss averaged over last 500 steps = 2.6729e-01, PNorm = 147.0987, GNorm = 0.2576
Meta loss on this task batch = 2.5189e-01, Meta loss averaged over last 500 steps = 2.6729e-01, PNorm = 147.0985, GNorm = 0.2526
Meta loss on this task batch = 3.2416e-01, Meta loss averaged over last 500 steps = 2.6737e-01, PNorm = 147.1000, GNorm = 0.3364
Meta loss on this task batch = 3.0336e-01, Meta loss averaged over last 500 steps = 2.6739e-01, PNorm = 147.1016, GNorm = 0.2966
Meta loss on this task batch = 2.7936e-01, Meta loss averaged over last 500 steps = 2.6738e-01, PNorm = 147.1048, GNorm = 0.2568
Meta loss on this task batch = 2.5846e-01, Meta loss averaged over last 500 steps = 2.6728e-01, PNorm = 147.1093, GNorm = 0.3069
Meta loss on this task batch = 3.2585e-01, Meta loss averaged over last 500 steps = 2.6729e-01, PNorm = 147.1137, GNorm = 0.3051
Meta loss on this task batch = 2.4531e-01, Meta loss averaged over last 500 steps = 2.6729e-01, PNorm = 147.1179, GNorm = 0.2750
Meta loss on this task batch = 2.5656e-01, Meta loss averaged over last 500 steps = 2.6734e-01, PNorm = 147.1225, GNorm = 0.2927
Meta loss on this task batch = 2.8977e-01, Meta loss averaged over last 500 steps = 2.6747e-01, PNorm = 147.1257, GNorm = 0.3160
Meta loss on this task batch = 2.1484e-01, Meta loss averaged over last 500 steps = 2.6732e-01, PNorm = 147.1290, GNorm = 0.3040
Meta loss on this task batch = 2.3796e-01, Meta loss averaged over last 500 steps = 2.6723e-01, PNorm = 147.1308, GNorm = 0.3247
Meta loss on this task batch = 2.3140e-01, Meta loss averaged over last 500 steps = 2.6705e-01, PNorm = 147.1318, GNorm = 0.2779
Meta loss on this task batch = 2.5602e-01, Meta loss averaged over last 500 steps = 2.6711e-01, PNorm = 147.1327, GNorm = 0.2988
Meta loss on this task batch = 2.7784e-01, Meta loss averaged over last 500 steps = 2.6699e-01, PNorm = 147.1328, GNorm = 0.2936
Meta loss on this task batch = 2.1049e-01, Meta loss averaged over last 500 steps = 2.6689e-01, PNorm = 147.1337, GNorm = 0.2556
Meta loss on this task batch = 2.7638e-01, Meta loss averaged over last 500 steps = 2.6700e-01, PNorm = 147.1350, GNorm = 0.2695
Meta loss on this task batch = 2.1095e-01, Meta loss averaged over last 500 steps = 2.6693e-01, PNorm = 147.1358, GNorm = 0.2954
Meta loss on this task batch = 2.6398e-01, Meta loss averaged over last 500 steps = 2.6692e-01, PNorm = 147.1373, GNorm = 0.3674
Took 138.38528966903687 seconds to complete one epoch of meta training
Took 147.4474003314972 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478502
Epoch 823
Meta loss on this task batch = 2.8192e-01, Meta loss averaged over last 500 steps = 2.6699e-01, PNorm = 147.1389, GNorm = 0.2623
Meta loss on this task batch = 2.0660e-01, Meta loss averaged over last 500 steps = 2.6688e-01, PNorm = 147.1428, GNorm = 0.2745
Meta loss on this task batch = 2.4182e-01, Meta loss averaged over last 500 steps = 2.6683e-01, PNorm = 147.1450, GNorm = 0.2621
Meta loss on this task batch = 3.0650e-01, Meta loss averaged over last 500 steps = 2.6694e-01, PNorm = 147.1460, GNorm = 0.3144
Meta loss on this task batch = 2.7833e-01, Meta loss averaged over last 500 steps = 2.6701e-01, PNorm = 147.1458, GNorm = 0.3363
Meta loss on this task batch = 2.5967e-01, Meta loss averaged over last 500 steps = 2.6697e-01, PNorm = 147.1466, GNorm = 0.3311
Meta loss on this task batch = 2.0482e-01, Meta loss averaged over last 500 steps = 2.6683e-01, PNorm = 147.1480, GNorm = 0.3548
Meta loss on this task batch = 2.6934e-01, Meta loss averaged over last 500 steps = 2.6678e-01, PNorm = 147.1476, GNorm = 0.3732
Meta loss on this task batch = 2.7982e-01, Meta loss averaged over last 500 steps = 2.6680e-01, PNorm = 147.1475, GNorm = 0.2880
Meta loss on this task batch = 2.3473e-01, Meta loss averaged over last 500 steps = 2.6665e-01, PNorm = 147.1474, GNorm = 0.2624
Meta loss on this task batch = 2.3981e-01, Meta loss averaged over last 500 steps = 2.6649e-01, PNorm = 147.1475, GNorm = 0.2253
Meta loss on this task batch = 3.4433e-01, Meta loss averaged over last 500 steps = 2.6665e-01, PNorm = 147.1470, GNorm = 0.3158
Meta loss on this task batch = 2.1652e-01, Meta loss averaged over last 500 steps = 2.6657e-01, PNorm = 147.1477, GNorm = 0.2291
Meta loss on this task batch = 2.3656e-01, Meta loss averaged over last 500 steps = 2.6653e-01, PNorm = 147.1499, GNorm = 0.2784
Meta loss on this task batch = 3.0186e-01, Meta loss averaged over last 500 steps = 2.6661e-01, PNorm = 147.1514, GNorm = 0.3577
Meta loss on this task batch = 2.9118e-01, Meta loss averaged over last 500 steps = 2.6666e-01, PNorm = 147.1534, GNorm = 0.2937
Meta loss on this task batch = 2.6893e-01, Meta loss averaged over last 500 steps = 2.6672e-01, PNorm = 147.1567, GNorm = 0.3191
Meta loss on this task batch = 2.8589e-01, Meta loss averaged over last 500 steps = 2.6676e-01, PNorm = 147.1628, GNorm = 0.3166
Meta loss on this task batch = 3.0470e-01, Meta loss averaged over last 500 steps = 2.6682e-01, PNorm = 147.1666, GNorm = 0.4111
Took 134.45912551879883 seconds to complete one epoch of meta training
Took 143.0702681541443 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481507
Epoch 824
Meta loss on this task batch = 2.5902e-01, Meta loss averaged over last 500 steps = 2.6684e-01, PNorm = 147.1728, GNorm = 0.3118
Meta loss on this task batch = 2.3631e-01, Meta loss averaged over last 500 steps = 2.6687e-01, PNorm = 147.1793, GNorm = 0.2885
Meta loss on this task batch = 2.3997e-01, Meta loss averaged over last 500 steps = 2.6682e-01, PNorm = 147.1886, GNorm = 0.2682
Meta loss on this task batch = 2.8824e-01, Meta loss averaged over last 500 steps = 2.6681e-01, PNorm = 147.1992, GNorm = 0.2936
Meta loss on this task batch = 2.5267e-01, Meta loss averaged over last 500 steps = 2.6666e-01, PNorm = 147.2108, GNorm = 0.2692
Meta loss on this task batch = 2.8847e-01, Meta loss averaged over last 500 steps = 2.6666e-01, PNorm = 147.2224, GNorm = 0.2556
Meta loss on this task batch = 2.3403e-01, Meta loss averaged over last 500 steps = 2.6646e-01, PNorm = 147.2326, GNorm = 0.2754
Meta loss on this task batch = 2.8023e-01, Meta loss averaged over last 500 steps = 2.6653e-01, PNorm = 147.2403, GNorm = 0.2846
Meta loss on this task batch = 2.7020e-01, Meta loss averaged over last 500 steps = 2.6644e-01, PNorm = 147.2464, GNorm = 0.2773
Meta loss on this task batch = 2.8385e-01, Meta loss averaged over last 500 steps = 2.6651e-01, PNorm = 147.2511, GNorm = 0.2716
Meta loss on this task batch = 2.6532e-01, Meta loss averaged over last 500 steps = 2.6645e-01, PNorm = 147.2532, GNorm = 0.2973
Meta loss on this task batch = 2.9348e-01, Meta loss averaged over last 500 steps = 2.6654e-01, PNorm = 147.2557, GNorm = 0.2801
Meta loss on this task batch = 2.8947e-01, Meta loss averaged over last 500 steps = 2.6655e-01, PNorm = 147.2568, GNorm = 0.3147
Meta loss on this task batch = 3.1914e-01, Meta loss averaged over last 500 steps = 2.6661e-01, PNorm = 147.2579, GNorm = 0.3608
Meta loss on this task batch = 2.2042e-01, Meta loss averaged over last 500 steps = 2.6649e-01, PNorm = 147.2596, GNorm = 0.2795
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 2.6657e-01, PNorm = 147.2616, GNorm = 0.4089
Meta loss on this task batch = 2.7699e-01, Meta loss averaged over last 500 steps = 2.6656e-01, PNorm = 147.2631, GNorm = 0.2826
Meta loss on this task batch = 2.6092e-01, Meta loss averaged over last 500 steps = 2.6666e-01, PNorm = 147.2654, GNorm = 0.3062
Meta loss on this task batch = 2.6833e-01, Meta loss averaged over last 500 steps = 2.6665e-01, PNorm = 147.2715, GNorm = 0.3605
Took 134.70444130897522 seconds to complete one epoch of meta training
Took 143.39295053482056 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502335
Epoch 825
Meta loss on this task batch = 2.1409e-01, Meta loss averaged over last 500 steps = 2.6663e-01, PNorm = 147.2783, GNorm = 0.2427
Meta loss on this task batch = 2.5545e-01, Meta loss averaged over last 500 steps = 2.6658e-01, PNorm = 147.2868, GNorm = 0.2896
Meta loss on this task batch = 2.1815e-01, Meta loss averaged over last 500 steps = 2.6650e-01, PNorm = 147.2966, GNorm = 0.2734
Meta loss on this task batch = 3.1672e-01, Meta loss averaged over last 500 steps = 2.6663e-01, PNorm = 147.3052, GNorm = 0.3286
Meta loss on this task batch = 2.5195e-01, Meta loss averaged over last 500 steps = 2.6660e-01, PNorm = 147.3129, GNorm = 0.2759
Meta loss on this task batch = 2.8603e-01, Meta loss averaged over last 500 steps = 2.6667e-01, PNorm = 147.3196, GNorm = 0.3018
Meta loss on this task batch = 3.1167e-01, Meta loss averaged over last 500 steps = 2.6676e-01, PNorm = 147.3247, GNorm = 0.3260
Meta loss on this task batch = 2.3330e-01, Meta loss averaged over last 500 steps = 2.6660e-01, PNorm = 147.3307, GNorm = 0.2523
Meta loss on this task batch = 2.2304e-01, Meta loss averaged over last 500 steps = 2.6644e-01, PNorm = 147.3371, GNorm = 0.2424
Meta loss on this task batch = 3.4017e-01, Meta loss averaged over last 500 steps = 2.6659e-01, PNorm = 147.3415, GNorm = 0.3289
Meta loss on this task batch = 2.4740e-01, Meta loss averaged over last 500 steps = 2.6658e-01, PNorm = 147.3457, GNorm = 0.2982
Meta loss on this task batch = 2.6835e-01, Meta loss averaged over last 500 steps = 2.6663e-01, PNorm = 147.3499, GNorm = 0.2640
Meta loss on this task batch = 2.9309e-01, Meta loss averaged over last 500 steps = 2.6663e-01, PNorm = 147.3548, GNorm = 0.2939
Meta loss on this task batch = 2.5098e-01, Meta loss averaged over last 500 steps = 2.6676e-01, PNorm = 147.3589, GNorm = 0.2741
Meta loss on this task batch = 2.8085e-01, Meta loss averaged over last 500 steps = 2.6687e-01, PNorm = 147.3615, GNorm = 0.2657
Meta loss on this task batch = 2.6793e-01, Meta loss averaged over last 500 steps = 2.6689e-01, PNorm = 147.3626, GNorm = 0.3705
Meta loss on this task batch = 2.5362e-01, Meta loss averaged over last 500 steps = 2.6684e-01, PNorm = 147.3634, GNorm = 0.2575
Meta loss on this task batch = 3.2078e-01, Meta loss averaged over last 500 steps = 2.6689e-01, PNorm = 147.3647, GNorm = 0.3504
Meta loss on this task batch = 2.9766e-01, Meta loss averaged over last 500 steps = 2.6688e-01, PNorm = 147.3675, GNorm = 0.2870
Took 133.8167588710785 seconds to complete one epoch of meta training
Took 142.92627906799316 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502387
Epoch 826
Meta loss on this task batch = 2.8410e-01, Meta loss averaged over last 500 steps = 2.6683e-01, PNorm = 147.3713, GNorm = 0.2938
Meta loss on this task batch = 2.6419e-01, Meta loss averaged over last 500 steps = 2.6672e-01, PNorm = 147.3738, GNorm = 0.3186
Meta loss on this task batch = 2.3209e-01, Meta loss averaged over last 500 steps = 2.6672e-01, PNorm = 147.3792, GNorm = 0.2616
Meta loss on this task batch = 2.5320e-01, Meta loss averaged over last 500 steps = 2.6669e-01, PNorm = 147.3868, GNorm = 0.2922
Meta loss on this task batch = 2.4652e-01, Meta loss averaged over last 500 steps = 2.6661e-01, PNorm = 147.3953, GNorm = 0.2679
Meta loss on this task batch = 2.8552e-01, Meta loss averaged over last 500 steps = 2.6659e-01, PNorm = 147.4057, GNorm = 0.3097
Meta loss on this task batch = 2.2271e-01, Meta loss averaged over last 500 steps = 2.6642e-01, PNorm = 147.4157, GNorm = 0.2909
Meta loss on this task batch = 2.7082e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 147.4242, GNorm = 0.2419
Meta loss on this task batch = 2.7453e-01, Meta loss averaged over last 500 steps = 2.6655e-01, PNorm = 147.4311, GNorm = 0.2943
Meta loss on this task batch = 3.0142e-01, Meta loss averaged over last 500 steps = 2.6661e-01, PNorm = 147.4378, GNorm = 0.3316
Meta loss on this task batch = 2.5670e-01, Meta loss averaged over last 500 steps = 2.6655e-01, PNorm = 147.4418, GNorm = 0.3210
Meta loss on this task batch = 2.4795e-01, Meta loss averaged over last 500 steps = 2.6649e-01, PNorm = 147.4434, GNorm = 0.3078
Meta loss on this task batch = 1.9884e-01, Meta loss averaged over last 500 steps = 2.6633e-01, PNorm = 147.4444, GNorm = 0.3148
Meta loss on this task batch = 2.6035e-01, Meta loss averaged over last 500 steps = 2.6634e-01, PNorm = 147.4439, GNorm = 0.3116
Meta loss on this task batch = 3.2535e-01, Meta loss averaged over last 500 steps = 2.6645e-01, PNorm = 147.4430, GNorm = 0.3108
Meta loss on this task batch = 2.4661e-01, Meta loss averaged over last 500 steps = 2.6636e-01, PNorm = 147.4401, GNorm = 0.3035
Meta loss on this task batch = 2.9002e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 147.4356, GNorm = 0.3427
Meta loss on this task batch = 2.3794e-01, Meta loss averaged over last 500 steps = 2.6646e-01, PNorm = 147.4319, GNorm = 0.3074
Meta loss on this task batch = 2.5919e-01, Meta loss averaged over last 500 steps = 2.6638e-01, PNorm = 147.4306, GNorm = 0.3273
Took 185.18573927879333 seconds to complete one epoch of meta training
Took 194.08465385437012 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496521
Epoch 827
Meta loss on this task batch = 2.5956e-01, Meta loss averaged over last 500 steps = 2.6637e-01, PNorm = 147.4318, GNorm = 0.2835
Meta loss on this task batch = 2.4959e-01, Meta loss averaged over last 500 steps = 2.6619e-01, PNorm = 147.4339, GNorm = 0.2992
Meta loss on this task batch = 2.5253e-01, Meta loss averaged over last 500 steps = 2.6619e-01, PNorm = 147.4365, GNorm = 0.2842
Meta loss on this task batch = 2.8520e-01, Meta loss averaged over last 500 steps = 2.6624e-01, PNorm = 147.4409, GNorm = 0.3125
Meta loss on this task batch = 2.7633e-01, Meta loss averaged over last 500 steps = 2.6632e-01, PNorm = 147.4467, GNorm = 0.3012
Meta loss on this task batch = 2.0031e-01, Meta loss averaged over last 500 steps = 2.6620e-01, PNorm = 147.4537, GNorm = 0.2400
Meta loss on this task batch = 1.8000e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 147.4601, GNorm = 0.2458
Meta loss on this task batch = 3.3250e-01, Meta loss averaged over last 500 steps = 2.6616e-01, PNorm = 147.4632, GNorm = 0.3183
Meta loss on this task batch = 2.9070e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 147.4640, GNorm = 0.3568
Meta loss on this task batch = 2.3570e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 147.4648, GNorm = 0.2840
Meta loss on this task batch = 3.1163e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 147.4620, GNorm = 0.3003
Meta loss on this task batch = 2.4095e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 147.4586, GNorm = 0.2799
Meta loss on this task batch = 2.8108e-01, Meta loss averaged over last 500 steps = 2.6605e-01, PNorm = 147.4539, GNorm = 0.3109
Meta loss on this task batch = 2.5697e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 147.4503, GNorm = 0.3042
Meta loss on this task batch = 2.8673e-01, Meta loss averaged over last 500 steps = 2.6613e-01, PNorm = 147.4482, GNorm = 0.3247
Meta loss on this task batch = 2.8151e-01, Meta loss averaged over last 500 steps = 2.6622e-01, PNorm = 147.4468, GNorm = 0.3142
Meta loss on this task batch = 2.4965e-01, Meta loss averaged over last 500 steps = 2.6618e-01, PNorm = 147.4478, GNorm = 0.2750
Meta loss on this task batch = 2.9406e-01, Meta loss averaged over last 500 steps = 2.6624e-01, PNorm = 147.4507, GNorm = 0.3030
Meta loss on this task batch = 3.0519e-01, Meta loss averaged over last 500 steps = 2.6639e-01, PNorm = 147.4534, GNorm = 0.3306
Took 182.08611226081848 seconds to complete one epoch of meta training
Took 190.08537220954895 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494724
Epoch 828
Meta loss on this task batch = 2.6036e-01, Meta loss averaged over last 500 steps = 2.6632e-01, PNorm = 147.4561, GNorm = 0.2767
Meta loss on this task batch = 2.5556e-01, Meta loss averaged over last 500 steps = 2.6632e-01, PNorm = 147.4583, GNorm = 0.2431
Meta loss on this task batch = 2.2725e-01, Meta loss averaged over last 500 steps = 2.6630e-01, PNorm = 147.4604, GNorm = 0.2536
Meta loss on this task batch = 2.6303e-01, Meta loss averaged over last 500 steps = 2.6642e-01, PNorm = 147.4619, GNorm = 0.2479
Meta loss on this task batch = 3.1484e-01, Meta loss averaged over last 500 steps = 2.6654e-01, PNorm = 147.4634, GNorm = 0.3192
Meta loss on this task batch = 2.6041e-01, Meta loss averaged over last 500 steps = 2.6643e-01, PNorm = 147.4639, GNorm = 0.3038
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.6634e-01, PNorm = 147.4652, GNorm = 0.3653
Meta loss on this task batch = 2.4889e-01, Meta loss averaged over last 500 steps = 2.6628e-01, PNorm = 147.4653, GNorm = 0.2989
Meta loss on this task batch = 2.0223e-01, Meta loss averaged over last 500 steps = 2.6615e-01, PNorm = 147.4661, GNorm = 0.2747
Meta loss on this task batch = 2.4983e-01, Meta loss averaged over last 500 steps = 2.6610e-01, PNorm = 147.4663, GNorm = 0.2779
Meta loss on this task batch = 2.8530e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 147.4674, GNorm = 0.3022
Meta loss on this task batch = 2.2814e-01, Meta loss averaged over last 500 steps = 2.6615e-01, PNorm = 147.4697, GNorm = 0.2853
Meta loss on this task batch = 2.1536e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 147.4748, GNorm = 0.2582
Meta loss on this task batch = 3.0074e-01, Meta loss averaged over last 500 steps = 2.6595e-01, PNorm = 147.4795, GNorm = 0.2954
Meta loss on this task batch = 3.2926e-01, Meta loss averaged over last 500 steps = 2.6613e-01, PNorm = 147.4823, GNorm = 0.3835
Meta loss on this task batch = 2.1896e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 147.4846, GNorm = 0.3046
Meta loss on this task batch = 3.1090e-01, Meta loss averaged over last 500 steps = 2.6621e-01, PNorm = 147.4854, GNorm = 0.3376
Meta loss on this task batch = 2.5541e-01, Meta loss averaged over last 500 steps = 2.6616e-01, PNorm = 147.4853, GNorm = 0.3072
Meta loss on this task batch = 2.4450e-01, Meta loss averaged over last 500 steps = 2.6605e-01, PNorm = 147.4854, GNorm = 0.3286
Took 172.38368701934814 seconds to complete one epoch of meta training
Took 181.33724737167358 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476001
Epoch 829
Meta loss on this task batch = 3.2573e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 147.4841, GNorm = 0.3130
Meta loss on this task batch = 2.2799e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 147.4829, GNorm = 0.2796
Meta loss on this task batch = 2.7115e-01, Meta loss averaged over last 500 steps = 2.6604e-01, PNorm = 147.4813, GNorm = 0.3010
Meta loss on this task batch = 2.7488e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 147.4812, GNorm = 0.2865
Meta loss on this task batch = 2.9539e-01, Meta loss averaged over last 500 steps = 2.6628e-01, PNorm = 147.4808, GNorm = 0.3378
Meta loss on this task batch = 2.4910e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 147.4825, GNorm = 0.2501
Meta loss on this task batch = 2.8248e-01, Meta loss averaged over last 500 steps = 2.6615e-01, PNorm = 147.4851, GNorm = 0.3193
Meta loss on this task batch = 2.7948e-01, Meta loss averaged over last 500 steps = 2.6621e-01, PNorm = 147.4913, GNorm = 0.2665
Meta loss on this task batch = 3.0325e-01, Meta loss averaged over last 500 steps = 2.6636e-01, PNorm = 147.4992, GNorm = 0.3454
Meta loss on this task batch = 2.7861e-01, Meta loss averaged over last 500 steps = 2.6635e-01, PNorm = 147.5079, GNorm = 0.2731
Meta loss on this task batch = 2.3599e-01, Meta loss averaged over last 500 steps = 2.6624e-01, PNorm = 147.5160, GNorm = 0.2583
Meta loss on this task batch = 2.8738e-01, Meta loss averaged over last 500 steps = 2.6623e-01, PNorm = 147.5233, GNorm = 0.2736
Meta loss on this task batch = 2.4439e-01, Meta loss averaged over last 500 steps = 2.6621e-01, PNorm = 147.5304, GNorm = 0.2559
Meta loss on this task batch = 3.0986e-01, Meta loss averaged over last 500 steps = 2.6627e-01, PNorm = 147.5349, GNorm = 0.3424
Meta loss on this task batch = 2.4949e-01, Meta loss averaged over last 500 steps = 2.6620e-01, PNorm = 147.5406, GNorm = 0.3608
Meta loss on this task batch = 2.5637e-01, Meta loss averaged over last 500 steps = 2.6622e-01, PNorm = 147.5462, GNorm = 0.3009
Meta loss on this task batch = 1.8657e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 147.5526, GNorm = 0.2473
Meta loss on this task batch = 2.7862e-01, Meta loss averaged over last 500 steps = 2.6614e-01, PNorm = 147.5560, GNorm = 0.3296
Meta loss on this task batch = 2.3209e-01, Meta loss averaged over last 500 steps = 2.6610e-01, PNorm = 147.5573, GNorm = 0.3250
Took 139.59853100776672 seconds to complete one epoch of meta training
Took 148.8072738647461 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497067
Epoch 830
Meta loss on this task batch = 2.5626e-01, Meta loss averaged over last 500 steps = 2.6607e-01, PNorm = 147.5584, GNorm = 0.2956
Meta loss on this task batch = 3.4683e-01, Meta loss averaged over last 500 steps = 2.6621e-01, PNorm = 147.5576, GNorm = 0.3149
Meta loss on this task batch = 2.7459e-01, Meta loss averaged over last 500 steps = 2.6614e-01, PNorm = 147.5587, GNorm = 0.3405
Meta loss on this task batch = 2.2895e-01, Meta loss averaged over last 500 steps = 2.6607e-01, PNorm = 147.5604, GNorm = 0.3181
Meta loss on this task batch = 2.5100e-01, Meta loss averaged over last 500 steps = 2.6595e-01, PNorm = 147.5628, GNorm = 0.2479
Meta loss on this task batch = 2.8696e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 147.5656, GNorm = 0.3275
Meta loss on this task batch = 2.3933e-01, Meta loss averaged over last 500 steps = 2.6593e-01, PNorm = 147.5696, GNorm = 0.2774
Meta loss on this task batch = 2.1939e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 147.5761, GNorm = 0.2937
Meta loss on this task batch = 2.7095e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 147.5834, GNorm = 0.3033
Meta loss on this task batch = 3.0116e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 147.5917, GNorm = 0.3222
Meta loss on this task batch = 2.2477e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 147.6023, GNorm = 0.2895
Meta loss on this task batch = 2.8161e-01, Meta loss averaged over last 500 steps = 2.6590e-01, PNorm = 147.6121, GNorm = 0.3237
Meta loss on this task batch = 2.2303e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 147.6194, GNorm = 0.2618
Meta loss on this task batch = 2.9722e-01, Meta loss averaged over last 500 steps = 2.6590e-01, PNorm = 147.6268, GNorm = 0.3263
Meta loss on this task batch = 2.5618e-01, Meta loss averaged over last 500 steps = 2.6581e-01, PNorm = 147.6328, GNorm = 0.2885
Meta loss on this task batch = 3.0268e-01, Meta loss averaged over last 500 steps = 2.6588e-01, PNorm = 147.6392, GNorm = 0.3701
Meta loss on this task batch = 3.5380e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 147.6428, GNorm = 0.3573
Meta loss on this task batch = 2.4153e-01, Meta loss averaged over last 500 steps = 2.6597e-01, PNorm = 147.6449, GNorm = 0.3113
Meta loss on this task batch = 1.8413e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 147.6490, GNorm = 0.3392
Took 138.71264624595642 seconds to complete one epoch of meta training
Took 146.79738211631775 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513516
Epoch 831
Meta loss on this task batch = 2.7112e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 147.6509, GNorm = 0.2847
Meta loss on this task batch = 2.1544e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 147.6531, GNorm = 0.2361
Meta loss on this task batch = 2.3215e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 147.6557, GNorm = 0.2489
Meta loss on this task batch = 2.7093e-01, Meta loss averaged over last 500 steps = 2.6568e-01, PNorm = 147.6598, GNorm = 0.2922
Meta loss on this task batch = 2.2394e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 147.6637, GNorm = 0.2259
Meta loss on this task batch = 2.5324e-01, Meta loss averaged over last 500 steps = 2.6544e-01, PNorm = 147.6680, GNorm = 0.3062
Meta loss on this task batch = 2.7587e-01, Meta loss averaged over last 500 steps = 2.6539e-01, PNorm = 147.6740, GNorm = 0.3220
Meta loss on this task batch = 3.5026e-01, Meta loss averaged over last 500 steps = 2.6554e-01, PNorm = 147.6816, GNorm = 0.3453
Meta loss on this task batch = 2.6199e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 147.6890, GNorm = 0.2787
Meta loss on this task batch = 2.3631e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 147.6990, GNorm = 0.3540
Meta loss on this task batch = 3.4269e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 147.7086, GNorm = 0.3309
Meta loss on this task batch = 3.0249e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 147.7179, GNorm = 0.3264
Meta loss on this task batch = 2.3271e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 147.7273, GNorm = 0.2539
Meta loss on this task batch = 2.4132e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 147.7383, GNorm = 0.2951
Meta loss on this task batch = 2.4727e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 147.7503, GNorm = 0.2496
Meta loss on this task batch = 2.8357e-01, Meta loss averaged over last 500 steps = 2.6559e-01, PNorm = 147.7619, GNorm = 0.2913
Meta loss on this task batch = 2.7978e-01, Meta loss averaged over last 500 steps = 2.6559e-01, PNorm = 147.7730, GNorm = 0.3108
Meta loss on this task batch = 2.4842e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 147.7814, GNorm = 0.3143
Meta loss on this task batch = 2.6187e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 147.7857, GNorm = 0.3515
Took 133.77780747413635 seconds to complete one epoch of meta training
Took 142.79984664916992 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517241
Epoch 832
Meta loss on this task batch = 2.0702e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 147.7892, GNorm = 0.2457
Meta loss on this task batch = 2.7337e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 147.7898, GNorm = 0.3821
Meta loss on this task batch = 2.6404e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 147.7892, GNorm = 0.3697
Meta loss on this task batch = 2.9523e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 147.7869, GNorm = 0.3425
Meta loss on this task batch = 2.8034e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 147.7843, GNorm = 0.2947
Meta loss on this task batch = 2.4709e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 147.7827, GNorm = 0.2878
Meta loss on this task batch = 2.7006e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 147.7814, GNorm = 0.2805
Meta loss on this task batch = 2.8312e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 147.7828, GNorm = 0.3040
Meta loss on this task batch = 2.4326e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 147.7863, GNorm = 0.3090
Meta loss on this task batch = 2.7924e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 147.7907, GNorm = 0.2758
Meta loss on this task batch = 2.9609e-01, Meta loss averaged over last 500 steps = 2.6581e-01, PNorm = 147.7967, GNorm = 0.3308
Meta loss on this task batch = 2.8977e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 147.8025, GNorm = 0.3379
Meta loss on this task batch = 2.5459e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 147.8082, GNorm = 0.3072
Meta loss on this task batch = 2.6998e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 147.8164, GNorm = 0.2839
Meta loss on this task batch = 2.5235e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 147.8261, GNorm = 0.2372
Meta loss on this task batch = 2.2082e-01, Meta loss averaged over last 500 steps = 2.6562e-01, PNorm = 147.8358, GNorm = 0.2706
Meta loss on this task batch = 2.2072e-01, Meta loss averaged over last 500 steps = 2.6541e-01, PNorm = 147.8453, GNorm = 0.2222
Meta loss on this task batch = 3.1628e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 147.8530, GNorm = 0.2882
Meta loss on this task batch = 2.6967e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 147.8605, GNorm = 0.3244
Took 133.95327973365784 seconds to complete one epoch of meta training
Took 142.4829671382904 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497703
Epoch 833
Meta loss on this task batch = 2.7121e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 147.8673, GNorm = 0.2603
Meta loss on this task batch = 2.1148e-01, Meta loss averaged over last 500 steps = 2.6540e-01, PNorm = 147.8737, GNorm = 0.2773
Meta loss on this task batch = 2.5162e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 147.8770, GNorm = 0.2860
Meta loss on this task batch = 2.4821e-01, Meta loss averaged over last 500 steps = 2.6543e-01, PNorm = 147.8790, GNorm = 0.2877
Meta loss on this task batch = 2.4047e-01, Meta loss averaged over last 500 steps = 2.6538e-01, PNorm = 147.8798, GNorm = 0.2986
Meta loss on this task batch = 2.1474e-01, Meta loss averaged over last 500 steps = 2.6530e-01, PNorm = 147.8797, GNorm = 0.2785
Meta loss on this task batch = 3.1965e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 147.8773, GNorm = 0.3417
Meta loss on this task batch = 2.1024e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 147.8747, GNorm = 0.2744
Meta loss on this task batch = 3.2820e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 147.8721, GNorm = 0.3574
Meta loss on this task batch = 2.3980e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 147.8721, GNorm = 0.3264
Meta loss on this task batch = 2.3951e-01, Meta loss averaged over last 500 steps = 2.6526e-01, PNorm = 147.8711, GNorm = 0.2528
Meta loss on this task batch = 3.1173e-01, Meta loss averaged over last 500 steps = 2.6535e-01, PNorm = 147.8711, GNorm = 0.3000
Meta loss on this task batch = 3.1733e-01, Meta loss averaged over last 500 steps = 2.6543e-01, PNorm = 147.8715, GNorm = 0.3118
Meta loss on this task batch = 2.2359e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 147.8728, GNorm = 0.2791
Meta loss on this task batch = 2.7067e-01, Meta loss averaged over last 500 steps = 2.6554e-01, PNorm = 147.8738, GNorm = 0.2951
Meta loss on this task batch = 2.4323e-01, Meta loss averaged over last 500 steps = 2.6554e-01, PNorm = 147.8760, GNorm = 0.2500
Meta loss on this task batch = 2.7352e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 147.8797, GNorm = 0.2823
Meta loss on this task batch = 2.7470e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 147.8846, GNorm = 0.2842
Meta loss on this task batch = 3.0683e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 147.8894, GNorm = 0.3588
Took 134.90651106834412 seconds to complete one epoch of meta training
Took 143.65395045280457 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483168
Epoch 834
Meta loss on this task batch = 2.3342e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 147.8959, GNorm = 0.2606
Meta loss on this task batch = 2.6933e-01, Meta loss averaged over last 500 steps = 2.6541e-01, PNorm = 147.9036, GNorm = 0.2934
Meta loss on this task batch = 2.6033e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 147.9095, GNorm = 0.3274
Meta loss on this task batch = 2.6760e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 147.9138, GNorm = 0.3372
Meta loss on this task batch = 2.6024e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 147.9183, GNorm = 0.2611
Meta loss on this task batch = 2.7691e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 147.9208, GNorm = 0.3547
Meta loss on this task batch = 2.5479e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 147.9236, GNorm = 0.2683
Meta loss on this task batch = 2.7051e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 147.9264, GNorm = 0.3238
Meta loss on this task batch = 2.6165e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 147.9281, GNorm = 0.2743
Meta loss on this task batch = 2.9086e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 147.9297, GNorm = 0.3566
Meta loss on this task batch = 2.3548e-01, Meta loss averaged over last 500 steps = 2.6556e-01, PNorm = 147.9321, GNorm = 0.2708
Meta loss on this task batch = 2.9369e-01, Meta loss averaged over last 500 steps = 2.6556e-01, PNorm = 147.9333, GNorm = 0.3404
Meta loss on this task batch = 2.5678e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 147.9356, GNorm = 0.2699
Meta loss on this task batch = 2.7268e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 147.9385, GNorm = 0.2710
Meta loss on this task batch = 2.4240e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 147.9421, GNorm = 0.2546
Meta loss on this task batch = 3.1783e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 147.9473, GNorm = 0.3275
Meta loss on this task batch = 3.1323e-01, Meta loss averaged over last 500 steps = 2.6586e-01, PNorm = 147.9540, GNorm = 0.3131
Meta loss on this task batch = 3.0694e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 147.9606, GNorm = 0.2662
Meta loss on this task batch = 2.0752e-01, Meta loss averaged over last 500 steps = 2.6587e-01, PNorm = 147.9677, GNorm = 0.2868
Took 135.51162457466125 seconds to complete one epoch of meta training
Took 144.61311078071594 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510454
Epoch 835
Meta loss on this task batch = 1.9829e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 147.9755, GNorm = 0.2423
Meta loss on this task batch = 2.8403e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 147.9836, GNorm = 0.2593
Meta loss on this task batch = 2.4225e-01, Meta loss averaged over last 500 steps = 2.6566e-01, PNorm = 147.9923, GNorm = 0.2476
Meta loss on this task batch = 2.7388e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 147.9993, GNorm = 0.3214
Meta loss on this task batch = 3.0206e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 148.0033, GNorm = 0.3488
Meta loss on this task batch = 2.7177e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 148.0053, GNorm = 0.3438
Meta loss on this task batch = 3.1442e-01, Meta loss averaged over last 500 steps = 2.6592e-01, PNorm = 148.0059, GNorm = 0.3121
Meta loss on this task batch = 3.0023e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 148.0053, GNorm = 0.3003
Meta loss on this task batch = 2.5492e-01, Meta loss averaged over last 500 steps = 2.6593e-01, PNorm = 148.0035, GNorm = 0.3223
Meta loss on this task batch = 2.2548e-01, Meta loss averaged over last 500 steps = 2.6581e-01, PNorm = 148.0009, GNorm = 0.2634
Meta loss on this task batch = 2.6420e-01, Meta loss averaged over last 500 steps = 2.6586e-01, PNorm = 147.9977, GNorm = 0.2947
Meta loss on this task batch = 2.9877e-01, Meta loss averaged over last 500 steps = 2.6597e-01, PNorm = 147.9957, GNorm = 0.2720
Meta loss on this task batch = 2.1185e-01, Meta loss averaged over last 500 steps = 2.6587e-01, PNorm = 147.9958, GNorm = 0.2470
Meta loss on this task batch = 2.4208e-01, Meta loss averaged over last 500 steps = 2.6585e-01, PNorm = 147.9970, GNorm = 0.2669
Meta loss on this task batch = 2.9916e-01, Meta loss averaged over last 500 steps = 2.6589e-01, PNorm = 147.9987, GNorm = 0.2703
Meta loss on this task batch = 2.8650e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 148.0008, GNorm = 0.2914
Meta loss on this task batch = 2.2839e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 148.0034, GNorm = 0.2551
Meta loss on this task batch = 3.0855e-01, Meta loss averaged over last 500 steps = 2.6616e-01, PNorm = 148.0066, GNorm = 0.3501
Meta loss on this task batch = 1.7071e-01, Meta loss averaged over last 500 steps = 2.6595e-01, PNorm = 148.0116, GNorm = 0.2285
Took 138.05568480491638 seconds to complete one epoch of meta training
Took 146.91160821914673 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482248
Epoch 836
Meta loss on this task batch = 1.9526e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 148.0175, GNorm = 0.2473
Meta loss on this task batch = 2.8570e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 148.0241, GNorm = 0.3091
Meta loss on this task batch = 2.5591e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 148.0309, GNorm = 0.2450
Meta loss on this task batch = 2.5294e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 148.0378, GNorm = 0.2716
Meta loss on this task batch = 3.0676e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 148.0456, GNorm = 0.3495
Meta loss on this task batch = 2.4050e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 148.0536, GNorm = 0.2437
Meta loss on this task batch = 2.7781e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 148.0604, GNorm = 0.3059
Meta loss on this task batch = 2.9818e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 148.0662, GNorm = 0.3166
Meta loss on this task batch = 2.5870e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 148.0712, GNorm = 0.2818
Meta loss on this task batch = 2.6883e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 148.0751, GNorm = 0.3105
Meta loss on this task batch = 2.7002e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 148.0782, GNorm = 0.3479
Meta loss on this task batch = 3.0272e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 148.0799, GNorm = 0.3580
Meta loss on this task batch = 2.7380e-01, Meta loss averaged over last 500 steps = 2.6604e-01, PNorm = 148.0795, GNorm = 0.3000
Meta loss on this task batch = 2.7255e-01, Meta loss averaged over last 500 steps = 2.6614e-01, PNorm = 148.0783, GNorm = 0.3150
Meta loss on this task batch = 2.5046e-01, Meta loss averaged over last 500 steps = 2.6605e-01, PNorm = 148.0767, GNorm = 0.3364
Meta loss on this task batch = 2.5344e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 148.0729, GNorm = 0.2768
Meta loss on this task batch = 2.5087e-01, Meta loss averaged over last 500 steps = 2.6594e-01, PNorm = 148.0696, GNorm = 0.2957
Meta loss on this task batch = 2.8385e-01, Meta loss averaged over last 500 steps = 2.6593e-01, PNorm = 148.0669, GNorm = 0.3255
Meta loss on this task batch = 2.2571e-01, Meta loss averaged over last 500 steps = 2.6593e-01, PNorm = 148.0675, GNorm = 0.3122
Took 134.4203746318817 seconds to complete one epoch of meta training
Took 142.66211771965027 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484774
Epoch 837
Meta loss on this task batch = 2.0714e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 148.0700, GNorm = 0.2509
Meta loss on this task batch = 2.9012e-01, Meta loss averaged over last 500 steps = 2.6589e-01, PNorm = 148.0738, GNorm = 0.2381
Meta loss on this task batch = 2.8967e-01, Meta loss averaged over last 500 steps = 2.6587e-01, PNorm = 148.0777, GNorm = 0.3196
Meta loss on this task batch = 3.2856e-01, Meta loss averaged over last 500 steps = 2.6600e-01, PNorm = 148.0816, GNorm = 0.3064
Meta loss on this task batch = 2.5586e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 148.0862, GNorm = 0.2564
Meta loss on this task batch = 2.5882e-01, Meta loss averaged over last 500 steps = 2.6581e-01, PNorm = 148.0916, GNorm = 0.2800
Meta loss on this task batch = 3.0767e-01, Meta loss averaged over last 500 steps = 2.6585e-01, PNorm = 148.0976, GNorm = 0.3330
Meta loss on this task batch = 3.0996e-01, Meta loss averaged over last 500 steps = 2.6594e-01, PNorm = 148.1056, GNorm = 0.3121
Meta loss on this task batch = 2.2925e-01, Meta loss averaged over last 500 steps = 2.6594e-01, PNorm = 148.1136, GNorm = 0.2452
Meta loss on this task batch = 2.9308e-01, Meta loss averaged over last 500 steps = 2.6588e-01, PNorm = 148.1214, GNorm = 0.3015
Meta loss on this task batch = 2.5981e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 148.1296, GNorm = 0.2673
Meta loss on this task batch = 2.4469e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 148.1375, GNorm = 0.2663
Meta loss on this task batch = 2.7281e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 148.1464, GNorm = 0.2709
Meta loss on this task batch = 2.6811e-01, Meta loss averaged over last 500 steps = 2.6581e-01, PNorm = 148.1555, GNorm = 0.2801
Meta loss on this task batch = 2.4946e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 148.1633, GNorm = 0.2641
Meta loss on this task batch = 2.6208e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 148.1708, GNorm = 0.2924
Meta loss on this task batch = 2.5804e-01, Meta loss averaged over last 500 steps = 2.6568e-01, PNorm = 148.1778, GNorm = 0.2668
Meta loss on this task batch = 2.0255e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 148.1853, GNorm = 0.2755
Meta loss on this task batch = 2.5226e-01, Meta loss averaged over last 500 steps = 2.6544e-01, PNorm = 148.1920, GNorm = 0.3803
Took 136.44389939308167 seconds to complete one epoch of meta training
Took 144.7701337337494 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477906
Epoch 838
Meta loss on this task batch = 2.6031e-01, Meta loss averaged over last 500 steps = 2.6541e-01, PNorm = 148.1972, GNorm = 0.2893
Meta loss on this task batch = 2.7455e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 148.2017, GNorm = 0.2850
Meta loss on this task batch = 2.7772e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 148.2081, GNorm = 0.3347
Meta loss on this task batch = 2.4321e-01, Meta loss averaged over last 500 steps = 2.6547e-01, PNorm = 148.2135, GNorm = 0.3181
Meta loss on this task batch = 2.8301e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 148.2181, GNorm = 0.3407
Meta loss on this task batch = 2.8631e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 148.2211, GNorm = 0.3290
Meta loss on this task batch = 2.7893e-01, Meta loss averaged over last 500 steps = 2.6562e-01, PNorm = 148.2238, GNorm = 0.3053
Meta loss on this task batch = 2.2848e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 148.2259, GNorm = 0.2655
Meta loss on this task batch = 3.0639e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 148.2268, GNorm = 0.3005
Meta loss on this task batch = 2.7316e-01, Meta loss averaged over last 500 steps = 2.6560e-01, PNorm = 148.2254, GNorm = 0.3020
Meta loss on this task batch = 2.6307e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 148.2265, GNorm = 0.2910
Meta loss on this task batch = 2.8917e-01, Meta loss averaged over last 500 steps = 2.6568e-01, PNorm = 148.2280, GNorm = 0.2815
Meta loss on this task batch = 2.1319e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 148.2302, GNorm = 0.2290
Meta loss on this task batch = 2.5608e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 148.2329, GNorm = 0.3049
Meta loss on this task batch = 2.5864e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 148.2363, GNorm = 0.2754
Meta loss on this task batch = 2.6735e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 148.2428, GNorm = 0.3337
Meta loss on this task batch = 3.0418e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 148.2485, GNorm = 0.3095
Meta loss on this task batch = 2.0803e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 148.2547, GNorm = 0.2703
Meta loss on this task batch = 2.8410e-01, Meta loss averaged over last 500 steps = 2.6554e-01, PNorm = 148.2608, GNorm = 0.3379
Took 132.74606561660767 seconds to complete one epoch of meta training
Took 141.58669805526733 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480174
Epoch 839
Meta loss on this task batch = 3.2248e-01, Meta loss averaged over last 500 steps = 2.6562e-01, PNorm = 148.2657, GNorm = 0.3067
Meta loss on this task batch = 2.7704e-01, Meta loss averaged over last 500 steps = 2.6560e-01, PNorm = 148.2696, GNorm = 0.2665
Meta loss on this task batch = 2.7253e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 148.2729, GNorm = 0.2649
Meta loss on this task batch = 2.3314e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 148.2770, GNorm = 0.2732
Meta loss on this task batch = 2.8101e-01, Meta loss averaged over last 500 steps = 2.6559e-01, PNorm = 148.2793, GNorm = 0.2926
Meta loss on this task batch = 2.9005e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 148.2825, GNorm = 0.2697
Meta loss on this task batch = 2.9870e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 148.2842, GNorm = 0.2917
Meta loss on this task batch = 2.9158e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 148.2869, GNorm = 0.2792
Meta loss on this task batch = 2.7201e-01, Meta loss averaged over last 500 steps = 2.6568e-01, PNorm = 148.2898, GNorm = 0.2716
Meta loss on this task batch = 2.3062e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 148.2919, GNorm = 0.2889
Meta loss on this task batch = 2.6402e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 148.2943, GNorm = 0.2824
Meta loss on this task batch = 2.3639e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 148.2968, GNorm = 0.2564
Meta loss on this task batch = 2.4543e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 148.2992, GNorm = 0.2891
Meta loss on this task batch = 2.7761e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 148.3021, GNorm = 0.2897
Meta loss on this task batch = 2.6302e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 148.3042, GNorm = 0.2790
Meta loss on this task batch = 3.0146e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 148.3063, GNorm = 0.3288
Meta loss on this task batch = 2.8760e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 148.3081, GNorm = 0.2864
Meta loss on this task batch = 2.4715e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 148.3113, GNorm = 0.2815
Meta loss on this task batch = 2.2081e-01, Meta loss averaged over last 500 steps = 2.6566e-01, PNorm = 148.3153, GNorm = 0.3542
Took 134.64517641067505 seconds to complete one epoch of meta training
Took 144.06139850616455 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479383
Epoch 840
Meta loss on this task batch = 1.9284e-01, Meta loss averaged over last 500 steps = 2.6556e-01, PNorm = 148.3202, GNorm = 0.2363
Meta loss on this task batch = 2.8949e-01, Meta loss averaged over last 500 steps = 2.6559e-01, PNorm = 148.3265, GNorm = 0.3180
Meta loss on this task batch = 2.8658e-01, Meta loss averaged over last 500 steps = 2.6554e-01, PNorm = 148.3320, GNorm = 0.2863
Meta loss on this task batch = 1.9142e-01, Meta loss averaged over last 500 steps = 2.6541e-01, PNorm = 148.3380, GNorm = 0.2673
Meta loss on this task batch = 2.4621e-01, Meta loss averaged over last 500 steps = 2.6541e-01, PNorm = 148.3429, GNorm = 0.2600
Meta loss on this task batch = 2.3113e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 148.3479, GNorm = 0.2754
Meta loss on this task batch = 2.7283e-01, Meta loss averaged over last 500 steps = 2.6539e-01, PNorm = 148.3527, GNorm = 0.2888
Meta loss on this task batch = 2.8614e-01, Meta loss averaged over last 500 steps = 2.6540e-01, PNorm = 148.3553, GNorm = 0.2996
Meta loss on this task batch = 2.7622e-01, Meta loss averaged over last 500 steps = 2.6538e-01, PNorm = 148.3595, GNorm = 0.2974
Meta loss on this task batch = 3.4101e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 148.3621, GNorm = 0.3556
Meta loss on this task batch = 2.3468e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 148.3633, GNorm = 0.2799
Meta loss on this task batch = 2.8697e-01, Meta loss averaged over last 500 steps = 2.6556e-01, PNorm = 148.3631, GNorm = 0.3380
Meta loss on this task batch = 2.4899e-01, Meta loss averaged over last 500 steps = 2.6547e-01, PNorm = 148.3617, GNorm = 0.3187
Meta loss on this task batch = 2.5012e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 148.3619, GNorm = 0.3255
Meta loss on this task batch = 2.4730e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 148.3628, GNorm = 0.2607
Meta loss on this task batch = 2.3251e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 148.3632, GNorm = 0.2652
Meta loss on this task batch = 2.8136e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 148.3642, GNorm = 0.2799
Meta loss on this task batch = 3.2255e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 148.3649, GNorm = 0.3288
Meta loss on this task batch = 2.2875e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 148.3674, GNorm = 0.3124
Took 132.07969284057617 seconds to complete one epoch of meta training
Took 141.60627579689026 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506610
Epoch 841
Meta loss on this task batch = 2.2458e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 148.3687, GNorm = 0.2691
Meta loss on this task batch = 2.2943e-01, Meta loss averaged over last 500 steps = 2.6539e-01, PNorm = 148.3710, GNorm = 0.2778
Meta loss on this task batch = 2.9478e-01, Meta loss averaged over last 500 steps = 2.6537e-01, PNorm = 148.3756, GNorm = 0.3389
Meta loss on this task batch = 2.5531e-01, Meta loss averaged over last 500 steps = 2.6546e-01, PNorm = 148.3808, GNorm = 0.3931
Meta loss on this task batch = 2.3234e-01, Meta loss averaged over last 500 steps = 2.6538e-01, PNorm = 148.3874, GNorm = 0.2687
Meta loss on this task batch = 2.6635e-01, Meta loss averaged over last 500 steps = 2.6536e-01, PNorm = 148.3951, GNorm = 0.2843
Meta loss on this task batch = 3.0155e-01, Meta loss averaged over last 500 steps = 2.6549e-01, PNorm = 148.4031, GNorm = 0.2943
Meta loss on this task batch = 2.0798e-01, Meta loss averaged over last 500 steps = 2.6539e-01, PNorm = 148.4111, GNorm = 0.2429
Meta loss on this task batch = 2.5287e-01, Meta loss averaged over last 500 steps = 2.6539e-01, PNorm = 148.4179, GNorm = 0.2715
Meta loss on this task batch = 2.5332e-01, Meta loss averaged over last 500 steps = 2.6536e-01, PNorm = 148.4245, GNorm = 0.3001
Meta loss on this task batch = 2.8619e-01, Meta loss averaged over last 500 steps = 2.6535e-01, PNorm = 148.4325, GNorm = 0.3155
Meta loss on this task batch = 2.5562e-01, Meta loss averaged over last 500 steps = 2.6529e-01, PNorm = 148.4390, GNorm = 0.3129
Meta loss on this task batch = 3.0979e-01, Meta loss averaged over last 500 steps = 2.6535e-01, PNorm = 148.4454, GNorm = 0.3013
Meta loss on this task batch = 3.1284e-01, Meta loss averaged over last 500 steps = 2.6549e-01, PNorm = 148.4525, GNorm = 0.2973
Meta loss on this task batch = 2.7742e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 148.4595, GNorm = 0.3144
Meta loss on this task batch = 2.4885e-01, Meta loss averaged over last 500 steps = 2.6546e-01, PNorm = 148.4656, GNorm = 0.3240
Meta loss on this task batch = 2.6475e-01, Meta loss averaged over last 500 steps = 2.6541e-01, PNorm = 148.4717, GNorm = 0.3000
Meta loss on this task batch = 2.5458e-01, Meta loss averaged over last 500 steps = 2.6540e-01, PNorm = 148.4773, GNorm = 0.2764
Meta loss on this task batch = 2.6362e-01, Meta loss averaged over last 500 steps = 2.6531e-01, PNorm = 148.4832, GNorm = 0.3879
Took 133.29767060279846 seconds to complete one epoch of meta training
Took 142.46662974357605 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478362
Epoch 842
Meta loss on this task batch = 2.9507e-01, Meta loss averaged over last 500 steps = 2.6544e-01, PNorm = 148.4886, GNorm = 0.3023
Meta loss on this task batch = 2.3743e-01, Meta loss averaged over last 500 steps = 2.6535e-01, PNorm = 148.4939, GNorm = 0.2762
Meta loss on this task batch = 2.9286e-01, Meta loss averaged over last 500 steps = 2.6541e-01, PNorm = 148.4984, GNorm = 0.3334
Meta loss on this task batch = 2.9652e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 148.5041, GNorm = 0.3207
Meta loss on this task batch = 2.5755e-01, Meta loss averaged over last 500 steps = 2.6560e-01, PNorm = 148.5100, GNorm = 0.2726
Meta loss on this task batch = 2.5447e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 148.5152, GNorm = 0.2582
Meta loss on this task batch = 2.3552e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 148.5211, GNorm = 0.2733
Meta loss on this task batch = 2.8408e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 148.5250, GNorm = 0.3561
Meta loss on this task batch = 2.8751e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 148.5280, GNorm = 0.3307
Meta loss on this task batch = 2.4967e-01, Meta loss averaged over last 500 steps = 2.6547e-01, PNorm = 148.5285, GNorm = 0.2936
Meta loss on this task batch = 2.5131e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 148.5286, GNorm = 0.2976
Meta loss on this task batch = 2.8303e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 148.5280, GNorm = 0.3318
Meta loss on this task batch = 2.5670e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 148.5268, GNorm = 0.2765
Meta loss on this task batch = 2.6085e-01, Meta loss averaged over last 500 steps = 2.6551e-01, PNorm = 148.5254, GNorm = 0.2791
Meta loss on this task batch = 2.9359e-01, Meta loss averaged over last 500 steps = 2.6554e-01, PNorm = 148.5240, GNorm = 0.2920
Meta loss on this task batch = 2.9144e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 148.5241, GNorm = 0.3078
Meta loss on this task batch = 2.6673e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 148.5260, GNorm = 0.2594
Meta loss on this task batch = 2.3686e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 148.5306, GNorm = 0.2778
Meta loss on this task batch = 3.0140e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 148.5373, GNorm = 0.3514
Took 135.28804230690002 seconds to complete one epoch of meta training
Took 143.3712363243103 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495870
Epoch 843
Meta loss on this task batch = 2.1626e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 148.5461, GNorm = 0.2391
Meta loss on this task batch = 2.7540e-01, Meta loss averaged over last 500 steps = 2.6554e-01, PNorm = 148.5543, GNorm = 0.2923
Meta loss on this task batch = 3.0326e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 148.5619, GNorm = 0.3201
Meta loss on this task batch = 2.9002e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 148.5689, GNorm = 0.3588
Meta loss on this task batch = 2.2839e-01, Meta loss averaged over last 500 steps = 2.6566e-01, PNorm = 148.5751, GNorm = 0.2799
Meta loss on this task batch = 2.9954e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 148.5786, GNorm = 0.2818
Meta loss on this task batch = 2.5001e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 148.5820, GNorm = 0.2801
Meta loss on this task batch = 3.0068e-01, Meta loss averaged over last 500 steps = 2.6583e-01, PNorm = 148.5825, GNorm = 0.3224
Meta loss on this task batch = 2.6226e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 148.5829, GNorm = 0.2705
Meta loss on this task batch = 2.4864e-01, Meta loss averaged over last 500 steps = 2.6562e-01, PNorm = 148.5826, GNorm = 0.2982
Meta loss on this task batch = 2.7598e-01, Meta loss averaged over last 500 steps = 2.6562e-01, PNorm = 148.5832, GNorm = 0.3067
Meta loss on this task batch = 2.4063e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 148.5835, GNorm = 0.2760
Meta loss on this task batch = 2.1142e-01, Meta loss averaged over last 500 steps = 2.6559e-01, PNorm = 148.5858, GNorm = 0.2377
Meta loss on this task batch = 2.7327e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 148.5884, GNorm = 0.2562
Meta loss on this task batch = 2.4405e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 148.5921, GNorm = 0.2899
Meta loss on this task batch = 2.5680e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 148.5961, GNorm = 0.2972
Meta loss on this task batch = 2.9242e-01, Meta loss averaged over last 500 steps = 2.6543e-01, PNorm = 148.5993, GNorm = 0.2920
Meta loss on this task batch = 2.7972e-01, Meta loss averaged over last 500 steps = 2.6549e-01, PNorm = 148.6036, GNorm = 0.3099
Meta loss on this task batch = 2.1468e-01, Meta loss averaged over last 500 steps = 2.6541e-01, PNorm = 148.6071, GNorm = 0.2865
Took 132.31734466552734 seconds to complete one epoch of meta training
Took 140.7543966770172 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494442
Epoch 844
Meta loss on this task batch = 2.7133e-01, Meta loss averaged over last 500 steps = 2.6541e-01, PNorm = 148.6104, GNorm = 0.3505
Meta loss on this task batch = 2.8670e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 148.6119, GNorm = 0.3090
Meta loss on this task batch = 2.6940e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 148.6124, GNorm = 0.2765
Meta loss on this task batch = 2.0547e-01, Meta loss averaged over last 500 steps = 2.6540e-01, PNorm = 148.6132, GNorm = 0.2847
Meta loss on this task batch = 2.5567e-01, Meta loss averaged over last 500 steps = 2.6530e-01, PNorm = 148.6143, GNorm = 0.2766
Meta loss on this task batch = 2.9074e-01, Meta loss averaged over last 500 steps = 2.6538e-01, PNorm = 148.6151, GNorm = 0.3223
Meta loss on this task batch = 2.8117e-01, Meta loss averaged over last 500 steps = 2.6543e-01, PNorm = 148.6189, GNorm = 0.2903
Meta loss on this task batch = 1.9647e-01, Meta loss averaged over last 500 steps = 2.6525e-01, PNorm = 148.6240, GNorm = 0.2317
Meta loss on this task batch = 2.8572e-01, Meta loss averaged over last 500 steps = 2.6533e-01, PNorm = 148.6271, GNorm = 0.2692
Meta loss on this task batch = 2.7402e-01, Meta loss averaged over last 500 steps = 2.6540e-01, PNorm = 148.6337, GNorm = 0.3365
Meta loss on this task batch = 1.9328e-01, Meta loss averaged over last 500 steps = 2.6536e-01, PNorm = 148.6400, GNorm = 0.2194
Meta loss on this task batch = 3.0121e-01, Meta loss averaged over last 500 steps = 2.6543e-01, PNorm = 148.6444, GNorm = 0.3116
Meta loss on this task batch = 2.7639e-01, Meta loss averaged over last 500 steps = 2.6535e-01, PNorm = 148.6472, GNorm = 0.4313
Meta loss on this task batch = 3.4868e-01, Meta loss averaged over last 500 steps = 2.6562e-01, PNorm = 148.6507, GNorm = 0.3342
Meta loss on this task batch = 2.7793e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 148.6550, GNorm = 0.3322
Meta loss on this task batch = 2.7754e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 148.6595, GNorm = 0.3156
Meta loss on this task batch = 2.7223e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 148.6656, GNorm = 0.2801
Meta loss on this task batch = 2.5860e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 148.6724, GNorm = 0.2932
Meta loss on this task batch = 2.7842e-01, Meta loss averaged over last 500 steps = 2.6568e-01, PNorm = 148.6798, GNorm = 0.3742
Took 133.51357245445251 seconds to complete one epoch of meta training
Took 143.0151481628418 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497052
Epoch 845
Meta loss on this task batch = 2.6040e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 148.6884, GNorm = 0.2761
Meta loss on this task batch = 3.2390e-01, Meta loss averaged over last 500 steps = 2.6566e-01, PNorm = 148.6975, GNorm = 0.2838
Meta loss on this task batch = 2.3850e-01, Meta loss averaged over last 500 steps = 2.6554e-01, PNorm = 148.7060, GNorm = 0.2430
Meta loss on this task batch = 2.2218e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 148.7143, GNorm = 0.2376
Meta loss on this task batch = 2.9670e-01, Meta loss averaged over last 500 steps = 2.6540e-01, PNorm = 148.7217, GNorm = 0.3140
Meta loss on this task batch = 2.7154e-01, Meta loss averaged over last 500 steps = 2.6537e-01, PNorm = 148.7282, GNorm = 0.3193
Meta loss on this task batch = 2.4383e-01, Meta loss averaged over last 500 steps = 2.6526e-01, PNorm = 148.7338, GNorm = 0.2979
Meta loss on this task batch = 2.8485e-01, Meta loss averaged over last 500 steps = 2.6518e-01, PNorm = 148.7373, GNorm = 0.2956
Meta loss on this task batch = 2.7687e-01, Meta loss averaged over last 500 steps = 2.6525e-01, PNorm = 148.7425, GNorm = 0.2881
Meta loss on this task batch = 2.6394e-01, Meta loss averaged over last 500 steps = 2.6519e-01, PNorm = 148.7464, GNorm = 0.3285
Meta loss on this task batch = 2.5567e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 148.7508, GNorm = 0.2996
Meta loss on this task batch = 2.5349e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 148.7561, GNorm = 0.2719
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 148.7604, GNorm = 0.3077
Meta loss on this task batch = 2.7410e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 148.7647, GNorm = 0.2618
Meta loss on this task batch = 2.3278e-01, Meta loss averaged over last 500 steps = 2.6539e-01, PNorm = 148.7691, GNorm = 0.3265
Meta loss on this task batch = 2.3332e-01, Meta loss averaged over last 500 steps = 2.6532e-01, PNorm = 148.7733, GNorm = 0.2642
Meta loss on this task batch = 2.8540e-01, Meta loss averaged over last 500 steps = 2.6535e-01, PNorm = 148.7775, GNorm = 0.3810
Meta loss on this task batch = 3.2194e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 148.7818, GNorm = 0.3705
Meta loss on this task batch = 3.4712e-01, Meta loss averaged over last 500 steps = 2.6549e-01, PNorm = 148.7848, GNorm = 0.4624
Took 131.79271936416626 seconds to complete one epoch of meta training
Took 140.23434925079346 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515282
Epoch 846
Meta loss on this task batch = 2.7380e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 148.7877, GNorm = 0.3167
Meta loss on this task batch = 2.9031e-01, Meta loss averaged over last 500 steps = 2.6566e-01, PNorm = 148.7887, GNorm = 0.3193
Meta loss on this task batch = 2.4675e-01, Meta loss averaged over last 500 steps = 2.6560e-01, PNorm = 148.7901, GNorm = 0.2566
Meta loss on this task batch = 3.1322e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 148.7924, GNorm = 0.3173
Meta loss on this task batch = 2.4489e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 148.7939, GNorm = 0.2764
Meta loss on this task batch = 2.9983e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 148.7957, GNorm = 0.2900
Meta loss on this task batch = 2.4833e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 148.7968, GNorm = 0.2867
Meta loss on this task batch = 2.3518e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 148.7981, GNorm = 0.2735
Meta loss on this task batch = 2.5969e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 148.7997, GNorm = 0.2411
Meta loss on this task batch = 3.2376e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 148.7988, GNorm = 0.3144
Meta loss on this task batch = 2.7537e-01, Meta loss averaged over last 500 steps = 2.6583e-01, PNorm = 148.7973, GNorm = 0.2832
Meta loss on this task batch = 2.9198e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 148.7966, GNorm = 0.3843
Meta loss on this task batch = 2.2063e-01, Meta loss averaged over last 500 steps = 2.6570e-01, PNorm = 148.7960, GNorm = 0.2745
Meta loss on this task batch = 3.1299e-01, Meta loss averaged over last 500 steps = 2.6585e-01, PNorm = 148.7961, GNorm = 0.2710
Meta loss on this task batch = 2.3720e-01, Meta loss averaged over last 500 steps = 2.6568e-01, PNorm = 148.7955, GNorm = 0.2822
Meta loss on this task batch = 2.8918e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 148.7946, GNorm = 0.2982
Meta loss on this task batch = 2.5301e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 148.7930, GNorm = 0.3168
Meta loss on this task batch = 2.0756e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 148.7915, GNorm = 0.2644
Meta loss on this task batch = 3.0684e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 148.7911, GNorm = 0.4557
Took 133.8269612789154 seconds to complete one epoch of meta training
Took 142.81268620491028 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467983
Epoch 847
Meta loss on this task batch = 2.8614e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 148.7928, GNorm = 0.2949
Meta loss on this task batch = 2.1003e-01, Meta loss averaged over last 500 steps = 2.6562e-01, PNorm = 148.7949, GNorm = 0.2351
Meta loss on this task batch = 2.4087e-01, Meta loss averaged over last 500 steps = 2.6554e-01, PNorm = 148.7993, GNorm = 0.2389
Meta loss on this task batch = 2.2709e-01, Meta loss averaged over last 500 steps = 2.6531e-01, PNorm = 148.8051, GNorm = 0.2478
Meta loss on this task batch = 2.5804e-01, Meta loss averaged over last 500 steps = 2.6537e-01, PNorm = 148.8129, GNorm = 0.2584
Meta loss on this task batch = 2.4899e-01, Meta loss averaged over last 500 steps = 2.6536e-01, PNorm = 148.8212, GNorm = 0.2793
Meta loss on this task batch = 2.9304e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 148.8293, GNorm = 0.2719
Meta loss on this task batch = 2.9104e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 148.8374, GNorm = 0.2966
Meta loss on this task batch = 3.2267e-01, Meta loss averaged over last 500 steps = 2.6559e-01, PNorm = 148.8440, GNorm = 0.3593
Meta loss on this task batch = 2.4864e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 148.8511, GNorm = 0.2383
Meta loss on this task batch = 2.3550e-01, Meta loss averaged over last 500 steps = 2.6546e-01, PNorm = 148.8560, GNorm = 0.2936
Meta loss on this task batch = 2.9114e-01, Meta loss averaged over last 500 steps = 2.6559e-01, PNorm = 148.8607, GNorm = 0.2594
Meta loss on this task batch = 2.3332e-01, Meta loss averaged over last 500 steps = 2.6560e-01, PNorm = 148.8654, GNorm = 0.2648
Meta loss on this task batch = 2.9097e-01, Meta loss averaged over last 500 steps = 2.6568e-01, PNorm = 148.8705, GNorm = 0.3333
Meta loss on this task batch = 2.5674e-01, Meta loss averaged over last 500 steps = 2.6562e-01, PNorm = 148.8752, GNorm = 0.2988
Meta loss on this task batch = 2.7150e-01, Meta loss averaged over last 500 steps = 2.6549e-01, PNorm = 148.8806, GNorm = 0.2754
Meta loss on this task batch = 2.1903e-01, Meta loss averaged over last 500 steps = 2.6536e-01, PNorm = 148.8853, GNorm = 0.3042
Meta loss on this task batch = 3.1954e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 148.8889, GNorm = 0.3479
Meta loss on this task batch = 3.0178e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 148.8900, GNorm = 0.3822
Took 136.99291443824768 seconds to complete one epoch of meta training
Took 146.59548497200012 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495050
Epoch 848
Meta loss on this task batch = 2.1922e-01, Meta loss averaged over last 500 steps = 2.6544e-01, PNorm = 148.8904, GNorm = 0.2901
Meta loss on this task batch = 2.3388e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 148.8917, GNorm = 0.2904
Meta loss on this task batch = 2.7716e-01, Meta loss averaged over last 500 steps = 2.6546e-01, PNorm = 148.8931, GNorm = 0.3153
Meta loss on this task batch = 2.6000e-01, Meta loss averaged over last 500 steps = 2.6540e-01, PNorm = 148.8943, GNorm = 0.2785
Meta loss on this task batch = 2.6119e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 148.8939, GNorm = 0.2685
Meta loss on this task batch = 2.6811e-01, Meta loss averaged over last 500 steps = 2.6543e-01, PNorm = 148.8937, GNorm = 0.2928
Meta loss on this task batch = 3.3493e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 148.8910, GNorm = 0.3598
Meta loss on this task batch = 2.7052e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 148.8889, GNorm = 0.2975
Meta loss on this task batch = 2.8826e-01, Meta loss averaged over last 500 steps = 2.6554e-01, PNorm = 148.8886, GNorm = 0.3139
Meta loss on this task batch = 2.6067e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 148.8898, GNorm = 0.3100
Meta loss on this task batch = 2.6203e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 148.8916, GNorm = 0.2612
Meta loss on this task batch = 2.4056e-01, Meta loss averaged over last 500 steps = 2.6538e-01, PNorm = 148.8947, GNorm = 0.2640
Meta loss on this task batch = 2.4169e-01, Meta loss averaged over last 500 steps = 2.6521e-01, PNorm = 148.8995, GNorm = 0.2848
Meta loss on this task batch = 2.4916e-01, Meta loss averaged over last 500 steps = 2.6522e-01, PNorm = 148.9052, GNorm = 0.2976
Meta loss on this task batch = 2.5500e-01, Meta loss averaged over last 500 steps = 2.6522e-01, PNorm = 148.9121, GNorm = 0.3030
Meta loss on this task batch = 2.7956e-01, Meta loss averaged over last 500 steps = 2.6520e-01, PNorm = 148.9195, GNorm = 0.2692
Meta loss on this task batch = 2.8812e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 148.9277, GNorm = 0.2907
Meta loss on this task batch = 2.5387e-01, Meta loss averaged over last 500 steps = 2.6538e-01, PNorm = 148.9366, GNorm = 0.2972
Meta loss on this task batch = 2.8472e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 148.9441, GNorm = 0.3200
Took 133.7191731929779 seconds to complete one epoch of meta training
Took 142.8231897354126 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482492
Epoch 849
Meta loss on this task batch = 2.5547e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 148.9494, GNorm = 0.2716
Meta loss on this task batch = 2.3397e-01, Meta loss averaged over last 500 steps = 2.6539e-01, PNorm = 148.9532, GNorm = 0.2761
Meta loss on this task batch = 3.1506e-01, Meta loss averaged over last 500 steps = 2.6560e-01, PNorm = 148.9565, GNorm = 0.3951
Meta loss on this task batch = 2.7134e-01, Meta loss averaged over last 500 steps = 2.6559e-01, PNorm = 148.9597, GNorm = 0.2802
Meta loss on this task batch = 2.4853e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 148.9637, GNorm = 0.2804
Meta loss on this task batch = 2.6081e-01, Meta loss averaged over last 500 steps = 2.6566e-01, PNorm = 148.9675, GNorm = 0.2681
Meta loss on this task batch = 2.9979e-01, Meta loss averaged over last 500 steps = 2.6570e-01, PNorm = 148.9707, GNorm = 0.3171
Meta loss on this task batch = 2.3382e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 148.9734, GNorm = 0.2424
Meta loss on this task batch = 2.8718e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 148.9768, GNorm = 0.2960
Meta loss on this task batch = 2.8185e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 148.9785, GNorm = 0.3112
Meta loss on this task batch = 2.3639e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 148.9815, GNorm = 0.2428
Meta loss on this task batch = 3.0689e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 148.9834, GNorm = 0.2608
Meta loss on this task batch = 2.7215e-01, Meta loss averaged over last 500 steps = 2.6594e-01, PNorm = 148.9852, GNorm = 0.2778
Meta loss on this task batch = 2.2226e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 148.9876, GNorm = 0.2979
Meta loss on this task batch = 2.4953e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 148.9904, GNorm = 0.3013
Meta loss on this task batch = 2.2573e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 148.9943, GNorm = 0.2746
Meta loss on this task batch = 2.1984e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 148.9976, GNorm = 0.2584
Meta loss on this task batch = 3.0372e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 149.0006, GNorm = 0.3034
Meta loss on this task batch = 2.8614e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 149.0047, GNorm = 0.3657
Took 136.01867723464966 seconds to complete one epoch of meta training
Took 144.82787775993347 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481482
Epoch 850
Meta loss on this task batch = 2.2572e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 149.0081, GNorm = 0.2622
Meta loss on this task batch = 2.5195e-01, Meta loss averaged over last 500 steps = 2.6566e-01, PNorm = 149.0113, GNorm = 0.2653
Meta loss on this task batch = 2.9314e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 149.0122, GNorm = 0.2998
Meta loss on this task batch = 3.4658e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 149.0114, GNorm = 0.3464
Meta loss on this task batch = 2.5316e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 149.0113, GNorm = 0.2475
Meta loss on this task batch = 2.2839e-01, Meta loss averaged over last 500 steps = 2.6560e-01, PNorm = 149.0101, GNorm = 0.2445
Meta loss on this task batch = 2.5442e-01, Meta loss averaged over last 500 steps = 2.6559e-01, PNorm = 149.0097, GNorm = 0.2712
Meta loss on this task batch = 2.4176e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 149.0104, GNorm = 0.2737
Meta loss on this task batch = 2.8187e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 149.0100, GNorm = 0.3132
Meta loss on this task batch = 2.8968e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 149.0121, GNorm = 0.2995
Meta loss on this task batch = 2.1703e-01, Meta loss averaged over last 500 steps = 2.6562e-01, PNorm = 149.0152, GNorm = 0.2515
Meta loss on this task batch = 2.5118e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 149.0193, GNorm = 0.3122
Meta loss on this task batch = 2.7009e-01, Meta loss averaged over last 500 steps = 2.6562e-01, PNorm = 149.0245, GNorm = 0.2769
Meta loss on this task batch = 2.9124e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 149.0292, GNorm = 0.2910
Meta loss on this task batch = 1.9290e-01, Meta loss averaged over last 500 steps = 2.6549e-01, PNorm = 149.0339, GNorm = 0.2431
Meta loss on this task batch = 2.2982e-01, Meta loss averaged over last 500 steps = 2.6538e-01, PNorm = 149.0376, GNorm = 0.2822
Meta loss on this task batch = 2.2631e-01, Meta loss averaged over last 500 steps = 2.6530e-01, PNorm = 149.0418, GNorm = 0.2390
Meta loss on this task batch = 3.2822e-01, Meta loss averaged over last 500 steps = 2.6537e-01, PNorm = 149.0453, GNorm = 0.3240
Meta loss on this task batch = 2.4240e-01, Meta loss averaged over last 500 steps = 2.6528e-01, PNorm = 149.0478, GNorm = 0.2959
Took 135.8749885559082 seconds to complete one epoch of meta training
Took 145.04495072364807 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.514912
Epoch 851
Meta loss on this task batch = 2.1414e-01, Meta loss averaged over last 500 steps = 2.6507e-01, PNorm = 149.0513, GNorm = 0.2390
Meta loss on this task batch = 2.7379e-01, Meta loss averaged over last 500 steps = 2.6517e-01, PNorm = 149.0543, GNorm = 0.3217
Meta loss on this task batch = 2.6188e-01, Meta loss averaged over last 500 steps = 2.6512e-01, PNorm = 149.0565, GNorm = 0.2775
Meta loss on this task batch = 2.7192e-01, Meta loss averaged over last 500 steps = 2.6511e-01, PNorm = 149.0579, GNorm = 0.2962
Meta loss on this task batch = 2.5835e-01, Meta loss averaged over last 500 steps = 2.6511e-01, PNorm = 149.0602, GNorm = 0.2520
Meta loss on this task batch = 2.8643e-01, Meta loss averaged over last 500 steps = 2.6514e-01, PNorm = 149.0632, GNorm = 0.2810
Meta loss on this task batch = 1.9883e-01, Meta loss averaged over last 500 steps = 2.6511e-01, PNorm = 149.0679, GNorm = 0.2720
Meta loss on this task batch = 2.4381e-01, Meta loss averaged over last 500 steps = 2.6509e-01, PNorm = 149.0740, GNorm = 0.3089
Meta loss on this task batch = 2.6643e-01, Meta loss averaged over last 500 steps = 2.6518e-01, PNorm = 149.0804, GNorm = 0.3073
Meta loss on this task batch = 2.6884e-01, Meta loss averaged over last 500 steps = 2.6509e-01, PNorm = 149.0860, GNorm = 0.3293
Meta loss on this task batch = 2.5947e-01, Meta loss averaged over last 500 steps = 2.6510e-01, PNorm = 149.0912, GNorm = 0.2711
Meta loss on this task batch = 2.6330e-01, Meta loss averaged over last 500 steps = 2.6506e-01, PNorm = 149.0958, GNorm = 0.3103
Meta loss on this task batch = 3.4237e-01, Meta loss averaged over last 500 steps = 2.6512e-01, PNorm = 149.0976, GNorm = 0.3620
Meta loss on this task batch = 2.8734e-01, Meta loss averaged over last 500 steps = 2.6523e-01, PNorm = 149.0970, GNorm = 0.3110
Meta loss on this task batch = 2.4310e-01, Meta loss averaged over last 500 steps = 2.6527e-01, PNorm = 149.0980, GNorm = 0.3240
Meta loss on this task batch = 2.3864e-01, Meta loss averaged over last 500 steps = 2.6506e-01, PNorm = 149.0997, GNorm = 0.2729
Meta loss on this task batch = 2.4196e-01, Meta loss averaged over last 500 steps = 2.6505e-01, PNorm = 149.1045, GNorm = 0.2826
Meta loss on this task batch = 2.3885e-01, Meta loss averaged over last 500 steps = 2.6500e-01, PNorm = 149.1093, GNorm = 0.2642
Meta loss on this task batch = 3.5614e-01, Meta loss averaged over last 500 steps = 2.6512e-01, PNorm = 149.1116, GNorm = 0.4371
Took 132.39496660232544 seconds to complete one epoch of meta training
Took 141.41737818717957 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503464
Epoch 852
Meta loss on this task batch = 2.5109e-01, Meta loss averaged over last 500 steps = 2.6512e-01, PNorm = 149.1136, GNorm = 0.2824
Meta loss on this task batch = 2.6642e-01, Meta loss averaged over last 500 steps = 2.6509e-01, PNorm = 149.1173, GNorm = 0.2463
Meta loss on this task batch = 2.6203e-01, Meta loss averaged over last 500 steps = 2.6508e-01, PNorm = 149.1224, GNorm = 0.2785
Meta loss on this task batch = 2.8721e-01, Meta loss averaged over last 500 steps = 2.6515e-01, PNorm = 149.1281, GNorm = 0.2461
Meta loss on this task batch = 2.6447e-01, Meta loss averaged over last 500 steps = 2.6504e-01, PNorm = 149.1349, GNorm = 0.2770
Meta loss on this task batch = 2.5586e-01, Meta loss averaged over last 500 steps = 2.6495e-01, PNorm = 149.1427, GNorm = 0.2764
Meta loss on this task batch = 2.0833e-01, Meta loss averaged over last 500 steps = 2.6480e-01, PNorm = 149.1506, GNorm = 0.2599
Meta loss on this task batch = 2.6986e-01, Meta loss averaged over last 500 steps = 2.6481e-01, PNorm = 149.1577, GNorm = 0.2674
Meta loss on this task batch = 2.6588e-01, Meta loss averaged over last 500 steps = 2.6488e-01, PNorm = 149.1639, GNorm = 0.2868
Meta loss on this task batch = 2.3830e-01, Meta loss averaged over last 500 steps = 2.6485e-01, PNorm = 149.1705, GNorm = 0.2803
Meta loss on this task batch = 2.6919e-01, Meta loss averaged over last 500 steps = 2.6489e-01, PNorm = 149.1762, GNorm = 0.2941
Meta loss on this task batch = 2.3786e-01, Meta loss averaged over last 500 steps = 2.6480e-01, PNorm = 149.1814, GNorm = 0.3113
Meta loss on this task batch = 3.0046e-01, Meta loss averaged over last 500 steps = 2.6495e-01, PNorm = 149.1864, GNorm = 0.3025
Meta loss on this task batch = 2.5062e-01, Meta loss averaged over last 500 steps = 2.6491e-01, PNorm = 149.1903, GNorm = 0.2421
Meta loss on this task batch = 2.6920e-01, Meta loss averaged over last 500 steps = 2.6490e-01, PNorm = 149.1946, GNorm = 0.3294
Meta loss on this task batch = 1.9664e-01, Meta loss averaged over last 500 steps = 2.6469e-01, PNorm = 149.1968, GNorm = 0.2495
Meta loss on this task batch = 3.1071e-01, Meta loss averaged over last 500 steps = 2.6480e-01, PNorm = 149.1994, GNorm = 0.3286
Meta loss on this task batch = 3.4629e-01, Meta loss averaged over last 500 steps = 2.6500e-01, PNorm = 149.1998, GNorm = 0.3434
Meta loss on this task batch = 2.9311e-01, Meta loss averaged over last 500 steps = 2.6519e-01, PNorm = 149.2018, GNorm = 0.3489
Took 134.49254369735718 seconds to complete one epoch of meta training
Took 142.79609942436218 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480290
Epoch 853
Meta loss on this task batch = 2.6536e-01, Meta loss averaged over last 500 steps = 2.6520e-01, PNorm = 149.2045, GNorm = 0.2950
Meta loss on this task batch = 2.7976e-01, Meta loss averaged over last 500 steps = 2.6511e-01, PNorm = 149.2067, GNorm = 0.3076
Meta loss on this task batch = 2.4804e-01, Meta loss averaged over last 500 steps = 2.6511e-01, PNorm = 149.2084, GNorm = 0.2388
Meta loss on this task batch = 2.0414e-01, Meta loss averaged over last 500 steps = 2.6494e-01, PNorm = 149.2132, GNorm = 0.2549
Meta loss on this task batch = 2.4834e-01, Meta loss averaged over last 500 steps = 2.6496e-01, PNorm = 149.2201, GNorm = 0.2607
Meta loss on this task batch = 2.9591e-01, Meta loss averaged over last 500 steps = 2.6503e-01, PNorm = 149.2279, GNorm = 0.2654
Meta loss on this task batch = 2.7170e-01, Meta loss averaged over last 500 steps = 2.6506e-01, PNorm = 149.2363, GNorm = 0.2671
Meta loss on this task batch = 2.9553e-01, Meta loss averaged over last 500 steps = 2.6515e-01, PNorm = 149.2457, GNorm = 0.3270
Meta loss on this task batch = 3.1209e-01, Meta loss averaged over last 500 steps = 2.6527e-01, PNorm = 149.2552, GNorm = 0.2928
Meta loss on this task batch = 2.6266e-01, Meta loss averaged over last 500 steps = 2.6522e-01, PNorm = 149.2654, GNorm = 0.3383
Meta loss on this task batch = 2.8098e-01, Meta loss averaged over last 500 steps = 2.6523e-01, PNorm = 149.2736, GNorm = 0.5822
Meta loss on this task batch = 1.9560e-01, Meta loss averaged over last 500 steps = 2.6522e-01, PNorm = 149.2820, GNorm = 0.2596
Meta loss on this task batch = 2.5449e-01, Meta loss averaged over last 500 steps = 2.6537e-01, PNorm = 149.2909, GNorm = 0.3047
Meta loss on this task batch = 2.6080e-01, Meta loss averaged over last 500 steps = 2.6523e-01, PNorm = 149.2987, GNorm = 0.3254
Meta loss on this task batch = 2.4935e-01, Meta loss averaged over last 500 steps = 2.6514e-01, PNorm = 149.3070, GNorm = 0.2678
Meta loss on this task batch = 2.8380e-01, Meta loss averaged over last 500 steps = 2.6524e-01, PNorm = 149.3151, GNorm = 0.3007
Meta loss on this task batch = 2.4004e-01, Meta loss averaged over last 500 steps = 2.6510e-01, PNorm = 149.3219, GNorm = 0.2877
Meta loss on this task batch = 2.6352e-01, Meta loss averaged over last 500 steps = 2.6514e-01, PNorm = 149.3280, GNorm = 0.3131
Meta loss on this task batch = 2.6097e-01, Meta loss averaged over last 500 steps = 2.6510e-01, PNorm = 149.3342, GNorm = 0.3935
Took 137.98132801055908 seconds to complete one epoch of meta training
Took 146.65320086479187 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494447
Epoch 854
Meta loss on this task batch = 2.4959e-01, Meta loss averaged over last 500 steps = 2.6509e-01, PNorm = 149.3397, GNorm = 0.3011
Meta loss on this task batch = 2.5222e-01, Meta loss averaged over last 500 steps = 2.6502e-01, PNorm = 149.3447, GNorm = 0.2952
Meta loss on this task batch = 2.8200e-01, Meta loss averaged over last 500 steps = 2.6502e-01, PNorm = 149.3461, GNorm = 0.3387
Meta loss on this task batch = 2.9100e-01, Meta loss averaged over last 500 steps = 2.6510e-01, PNorm = 149.3446, GNorm = 0.3593
Meta loss on this task batch = 2.6264e-01, Meta loss averaged over last 500 steps = 2.6504e-01, PNorm = 149.3420, GNorm = 0.2987
Meta loss on this task batch = 2.4887e-01, Meta loss averaged over last 500 steps = 2.6493e-01, PNorm = 149.3403, GNorm = 0.2864
Meta loss on this task batch = 2.0517e-01, Meta loss averaged over last 500 steps = 2.6482e-01, PNorm = 149.3406, GNorm = 0.2582
Meta loss on this task batch = 2.4490e-01, Meta loss averaged over last 500 steps = 2.6480e-01, PNorm = 149.3412, GNorm = 0.2867
Meta loss on this task batch = 2.1976e-01, Meta loss averaged over last 500 steps = 2.6478e-01, PNorm = 149.3422, GNorm = 0.2761
Meta loss on this task batch = 2.8937e-01, Meta loss averaged over last 500 steps = 2.6483e-01, PNorm = 149.3443, GNorm = 0.2797
Meta loss on this task batch = 2.5413e-01, Meta loss averaged over last 500 steps = 2.6471e-01, PNorm = 149.3486, GNorm = 0.3031
Meta loss on this task batch = 3.1301e-01, Meta loss averaged over last 500 steps = 2.6482e-01, PNorm = 149.3533, GNorm = 0.3248
Meta loss on this task batch = 2.1476e-01, Meta loss averaged over last 500 steps = 2.6472e-01, PNorm = 149.3580, GNorm = 0.2441
Meta loss on this task batch = 2.6540e-01, Meta loss averaged over last 500 steps = 2.6476e-01, PNorm = 149.3625, GNorm = 0.3168
Meta loss on this task batch = 3.3529e-01, Meta loss averaged over last 500 steps = 2.6502e-01, PNorm = 149.3639, GNorm = 0.3823
Meta loss on this task batch = 2.8124e-01, Meta loss averaged over last 500 steps = 2.6509e-01, PNorm = 149.3645, GNorm = 0.3073
Meta loss on this task batch = 2.8577e-01, Meta loss averaged over last 500 steps = 2.6509e-01, PNorm = 149.3643, GNorm = 0.3513
Meta loss on this task batch = 2.4988e-01, Meta loss averaged over last 500 steps = 2.6513e-01, PNorm = 149.3635, GNorm = 0.3486
Meta loss on this task batch = 2.9777e-01, Meta loss averaged over last 500 steps = 2.6530e-01, PNorm = 149.3615, GNorm = 0.4798
Took 133.51105451583862 seconds to complete one epoch of meta training
Took 142.44099974632263 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468112
Epoch 855
Meta loss on this task batch = 2.6144e-01, Meta loss averaged over last 500 steps = 2.6522e-01, PNorm = 149.3607, GNorm = 0.2686
Meta loss on this task batch = 2.9426e-01, Meta loss averaged over last 500 steps = 2.6515e-01, PNorm = 149.3605, GNorm = 0.3283
Meta loss on this task batch = 2.6660e-01, Meta loss averaged over last 500 steps = 2.6524e-01, PNorm = 149.3628, GNorm = 0.3193
Meta loss on this task batch = 2.6640e-01, Meta loss averaged over last 500 steps = 2.6515e-01, PNorm = 149.3655, GNorm = 0.3269
Meta loss on this task batch = 2.3472e-01, Meta loss averaged over last 500 steps = 2.6511e-01, PNorm = 149.3703, GNorm = 0.2627
Meta loss on this task batch = 2.3306e-01, Meta loss averaged over last 500 steps = 2.6509e-01, PNorm = 149.3758, GNorm = 0.2642
Meta loss on this task batch = 2.5665e-01, Meta loss averaged over last 500 steps = 2.6495e-01, PNorm = 149.3815, GNorm = 0.2582
Meta loss on this task batch = 2.6661e-01, Meta loss averaged over last 500 steps = 2.6503e-01, PNorm = 149.3879, GNorm = 0.3181
Meta loss on this task batch = 2.3286e-01, Meta loss averaged over last 500 steps = 2.6495e-01, PNorm = 149.3944, GNorm = 0.3711
Meta loss on this task batch = 2.4810e-01, Meta loss averaged over last 500 steps = 2.6490e-01, PNorm = 149.4007, GNorm = 0.2826
Meta loss on this task batch = 2.4752e-01, Meta loss averaged over last 500 steps = 2.6480e-01, PNorm = 149.4073, GNorm = 0.3010
Meta loss on this task batch = 3.1793e-01, Meta loss averaged over last 500 steps = 2.6494e-01, PNorm = 149.4118, GNorm = 0.3198
Meta loss on this task batch = 2.8844e-01, Meta loss averaged over last 500 steps = 2.6495e-01, PNorm = 149.4163, GNorm = 0.2933
Meta loss on this task batch = 3.0153e-01, Meta loss averaged over last 500 steps = 2.6500e-01, PNorm = 149.4198, GNorm = 0.3699
Meta loss on this task batch = 2.7709e-01, Meta loss averaged over last 500 steps = 2.6494e-01, PNorm = 149.4232, GNorm = 0.2689
Meta loss on this task batch = 2.8223e-01, Meta loss averaged over last 500 steps = 2.6495e-01, PNorm = 149.4263, GNorm = 0.2983
Meta loss on this task batch = 2.8678e-01, Meta loss averaged over last 500 steps = 2.6505e-01, PNorm = 149.4287, GNorm = 0.2840
Meta loss on this task batch = 2.8204e-01, Meta loss averaged over last 500 steps = 2.6504e-01, PNorm = 149.4316, GNorm = 0.2931
Meta loss on this task batch = 2.9634e-01, Meta loss averaged over last 500 steps = 2.6515e-01, PNorm = 149.4345, GNorm = 0.3459
Took 136.07746577262878 seconds to complete one epoch of meta training
Took 144.01548027992249 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474224
Epoch 856
Meta loss on this task batch = 2.7966e-01, Meta loss averaged over last 500 steps = 2.6508e-01, PNorm = 149.4387, GNorm = 0.3229
Meta loss on this task batch = 2.5963e-01, Meta loss averaged over last 500 steps = 2.6511e-01, PNorm = 149.4434, GNorm = 0.2652
Meta loss on this task batch = 2.7571e-01, Meta loss averaged over last 500 steps = 2.6514e-01, PNorm = 149.4494, GNorm = 0.2892
Meta loss on this task batch = 2.8399e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 149.4550, GNorm = 0.3022
Meta loss on this task batch = 2.4164e-01, Meta loss averaged over last 500 steps = 2.6526e-01, PNorm = 149.4608, GNorm = 0.2758
Meta loss on this task batch = 2.8196e-01, Meta loss averaged over last 500 steps = 2.6536e-01, PNorm = 149.4658, GNorm = 0.3106
Meta loss on this task batch = 3.1824e-01, Meta loss averaged over last 500 steps = 2.6549e-01, PNorm = 149.4706, GNorm = 0.2662
Meta loss on this task batch = 2.8956e-01, Meta loss averaged over last 500 steps = 2.6537e-01, PNorm = 149.4765, GNorm = 0.3295
Meta loss on this task batch = 2.4203e-01, Meta loss averaged over last 500 steps = 2.6531e-01, PNorm = 149.4838, GNorm = 0.2510
Meta loss on this task batch = 2.0586e-01, Meta loss averaged over last 500 steps = 2.6526e-01, PNorm = 149.4912, GNorm = 0.2545
Meta loss on this task batch = 2.3495e-01, Meta loss averaged over last 500 steps = 2.6523e-01, PNorm = 149.4994, GNorm = 0.2807
Meta loss on this task batch = 2.7084e-01, Meta loss averaged over last 500 steps = 2.6520e-01, PNorm = 149.5049, GNorm = 0.3421
Meta loss on this task batch = 2.8510e-01, Meta loss averaged over last 500 steps = 2.6529e-01, PNorm = 149.5107, GNorm = 0.3693
Meta loss on this task batch = 2.4745e-01, Meta loss averaged over last 500 steps = 2.6535e-01, PNorm = 149.5162, GNorm = 0.3040
Meta loss on this task batch = 3.3297e-01, Meta loss averaged over last 500 steps = 2.6547e-01, PNorm = 149.5232, GNorm = 0.2686
Meta loss on this task batch = 2.4871e-01, Meta loss averaged over last 500 steps = 2.6537e-01, PNorm = 149.5284, GNorm = 0.2968
Meta loss on this task batch = 2.1200e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 149.5333, GNorm = 0.3392
Meta loss on this task batch = 2.7883e-01, Meta loss averaged over last 500 steps = 2.6533e-01, PNorm = 149.5359, GNorm = 0.3164
Meta loss on this task batch = 3.0278e-01, Meta loss averaged over last 500 steps = 2.6549e-01, PNorm = 149.5374, GNorm = 0.3205
Took 136.2808620929718 seconds to complete one epoch of meta training
Took 144.95971941947937 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473281
Epoch 857
Meta loss on this task batch = 2.9083e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 149.5358, GNorm = 0.3281
Meta loss on this task batch = 2.4275e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 149.5344, GNorm = 0.3148
Meta loss on this task batch = 2.5734e-01, Meta loss averaged over last 500 steps = 2.6536e-01, PNorm = 149.5334, GNorm = 0.2620
Meta loss on this task batch = 2.2377e-01, Meta loss averaged over last 500 steps = 2.6510e-01, PNorm = 149.5313, GNorm = 0.3040
Meta loss on this task batch = 2.1451e-01, Meta loss averaged over last 500 steps = 2.6505e-01, PNorm = 149.5310, GNorm = 0.2352
Meta loss on this task batch = 2.8289e-01, Meta loss averaged over last 500 steps = 2.6525e-01, PNorm = 149.5314, GNorm = 0.2827
Meta loss on this task batch = 2.8343e-01, Meta loss averaged over last 500 steps = 2.6527e-01, PNorm = 149.5318, GNorm = 0.3263
Meta loss on this task batch = 3.3416e-01, Meta loss averaged over last 500 steps = 2.6551e-01, PNorm = 149.5319, GNorm = 0.3534
Meta loss on this task batch = 2.2168e-01, Meta loss averaged over last 500 steps = 2.6549e-01, PNorm = 149.5324, GNorm = 0.3420
Meta loss on this task batch = 2.6101e-01, Meta loss averaged over last 500 steps = 2.6547e-01, PNorm = 149.5321, GNorm = 0.3007
Meta loss on this task batch = 2.5267e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 149.5324, GNorm = 0.2632
Meta loss on this task batch = 2.2087e-01, Meta loss averaged over last 500 steps = 2.6546e-01, PNorm = 149.5330, GNorm = 0.2530
Meta loss on this task batch = 2.6682e-01, Meta loss averaged over last 500 steps = 2.6544e-01, PNorm = 149.5350, GNorm = 0.3026
Meta loss on this task batch = 2.5231e-01, Meta loss averaged over last 500 steps = 2.6525e-01, PNorm = 149.5373, GNorm = 0.2807
Meta loss on this task batch = 3.0667e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 149.5408, GNorm = 0.3105
Meta loss on this task batch = 2.6738e-01, Meta loss averaged over last 500 steps = 2.6540e-01, PNorm = 149.5438, GNorm = 0.2773
Meta loss on this task batch = 3.0783e-01, Meta loss averaged over last 500 steps = 2.6533e-01, PNorm = 149.5481, GNorm = 0.2899
Meta loss on this task batch = 2.5543e-01, Meta loss averaged over last 500 steps = 2.6523e-01, PNorm = 149.5535, GNorm = 0.2798
Meta loss on this task batch = 2.8481e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 149.5599, GNorm = 0.3442
Took 134.64977526664734 seconds to complete one epoch of meta training
Took 143.35791397094727 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496159
Epoch 858
Meta loss on this task batch = 2.9492e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 149.5659, GNorm = 0.3006
Meta loss on this task batch = 2.4609e-01, Meta loss averaged over last 500 steps = 2.6544e-01, PNorm = 149.5733, GNorm = 0.2463
Meta loss on this task batch = 2.7357e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 149.5804, GNorm = 0.2865
Meta loss on this task batch = 2.5006e-01, Meta loss averaged over last 500 steps = 2.6536e-01, PNorm = 149.5875, GNorm = 0.2904
Meta loss on this task batch = 2.3835e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 149.5939, GNorm = 0.2795
Meta loss on this task batch = 3.0028e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 149.5975, GNorm = 0.3181
Meta loss on this task batch = 2.7932e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 149.6004, GNorm = 0.3080
Meta loss on this task batch = 2.5602e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 149.6036, GNorm = 0.3130
Meta loss on this task batch = 2.9665e-01, Meta loss averaged over last 500 steps = 2.6560e-01, PNorm = 149.6068, GNorm = 0.3271
Meta loss on this task batch = 3.1484e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 149.6100, GNorm = 0.2907
Meta loss on this task batch = 2.3737e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 149.6120, GNorm = 0.2788
Meta loss on this task batch = 2.3772e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 149.6139, GNorm = 0.2649
Meta loss on this task batch = 2.5373e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 149.6139, GNorm = 0.2816
Meta loss on this task batch = 2.4515e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 149.6145, GNorm = 0.2935
Meta loss on this task batch = 2.9911e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 149.6162, GNorm = 0.3101
Meta loss on this task batch = 2.5124e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 149.6191, GNorm = 0.2632
Meta loss on this task batch = 2.2657e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 149.6211, GNorm = 0.2644
Meta loss on this task batch = 2.5845e-01, Meta loss averaged over last 500 steps = 2.6528e-01, PNorm = 149.6243, GNorm = 0.2539
Meta loss on this task batch = 2.9621e-01, Meta loss averaged over last 500 steps = 2.6536e-01, PNorm = 149.6249, GNorm = 0.3481
Took 136.21732115745544 seconds to complete one epoch of meta training
Took 143.57586240768433 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475676
Epoch 859
Meta loss on this task batch = 2.2786e-01, Meta loss averaged over last 500 steps = 2.6527e-01, PNorm = 149.6250, GNorm = 0.2920
Meta loss on this task batch = 2.6997e-01, Meta loss averaged over last 500 steps = 2.6531e-01, PNorm = 149.6265, GNorm = 0.3112
Meta loss on this task batch = 2.9875e-01, Meta loss averaged over last 500 steps = 2.6547e-01, PNorm = 149.6291, GNorm = 0.3386
Meta loss on this task batch = 2.8207e-01, Meta loss averaged over last 500 steps = 2.6559e-01, PNorm = 149.6326, GNorm = 0.2735
Meta loss on this task batch = 2.1209e-01, Meta loss averaged over last 500 steps = 2.6538e-01, PNorm = 149.6398, GNorm = 0.2623
Meta loss on this task batch = 2.5101e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 149.6476, GNorm = 0.2875
Meta loss on this task batch = 2.8437e-01, Meta loss averaged over last 500 steps = 2.6537e-01, PNorm = 149.6555, GNorm = 0.2834
Meta loss on this task batch = 2.7680e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 149.6632, GNorm = 0.3059
Meta loss on this task batch = 2.4175e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 149.6723, GNorm = 0.2898
Meta loss on this task batch = 2.9763e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 149.6800, GNorm = 0.2633
Meta loss on this task batch = 2.8569e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 149.6867, GNorm = 0.2630
Meta loss on this task batch = 2.3293e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 149.6921, GNorm = 0.2545
Meta loss on this task batch = 2.2615e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 149.6972, GNorm = 0.2399
Meta loss on this task batch = 2.6859e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 149.7019, GNorm = 0.2476
Meta loss on this task batch = 2.6841e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 149.7058, GNorm = 0.3203
Meta loss on this task batch = 3.0904e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 149.7099, GNorm = 0.3025
Meta loss on this task batch = 2.6983e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 149.7140, GNorm = 0.2730
Meta loss on this task batch = 2.6823e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 149.7179, GNorm = 0.2556
Meta loss on this task batch = 3.1910e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 149.7214, GNorm = 0.3536
Took 136.80088472366333 seconds to complete one epoch of meta training
Took 145.91992497444153 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483803
Epoch 860
Meta loss on this task batch = 2.7137e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 149.7229, GNorm = 0.3536
Meta loss on this task batch = 2.3255e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 149.7255, GNorm = 0.2529
Meta loss on this task batch = 2.8531e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 149.7286, GNorm = 0.2649
Meta loss on this task batch = 2.6932e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 149.7319, GNorm = 0.2915
Meta loss on this task batch = 1.9792e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 149.7367, GNorm = 0.2512
Meta loss on this task batch = 2.6032e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 149.7424, GNorm = 0.2698
Meta loss on this task batch = 2.8323e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 149.7471, GNorm = 0.2744
Meta loss on this task batch = 3.0522e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 149.7503, GNorm = 0.3005
Meta loss on this task batch = 2.6982e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 149.7535, GNorm = 0.2889
Meta loss on this task batch = 2.3362e-01, Meta loss averaged over last 500 steps = 2.6560e-01, PNorm = 149.7560, GNorm = 0.2539
Meta loss on this task batch = 2.9770e-01, Meta loss averaged over last 500 steps = 2.6568e-01, PNorm = 149.7579, GNorm = 0.3179
Meta loss on this task batch = 3.0622e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 149.7593, GNorm = 0.3001
Meta loss on this task batch = 2.4351e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 149.7611, GNorm = 0.2650
Meta loss on this task batch = 3.0622e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 149.7648, GNorm = 0.2805
Meta loss on this task batch = 2.1184e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 149.7704, GNorm = 0.2777
Meta loss on this task batch = 2.8118e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 149.7745, GNorm = 0.3079
Meta loss on this task batch = 2.6244e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 149.7783, GNorm = 0.2779
Meta loss on this task batch = 2.3993e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 149.7839, GNorm = 0.2621
Meta loss on this task batch = 1.9155e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 149.7907, GNorm = 0.2831
Took 134.62913751602173 seconds to complete one epoch of meta training
Took 143.95356488227844 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502733
Epoch 861
Meta loss on this task batch = 2.8356e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 149.7982, GNorm = 0.2981
Meta loss on this task batch = 2.7994e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 149.8068, GNorm = 0.3135
Meta loss on this task batch = 2.6957e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 149.8167, GNorm = 0.2714
Meta loss on this task batch = 2.6641e-01, Meta loss averaged over last 500 steps = 2.6539e-01, PNorm = 149.8282, GNorm = 0.3016
Meta loss on this task batch = 2.5549e-01, Meta loss averaged over last 500 steps = 2.6529e-01, PNorm = 149.8379, GNorm = 0.3172
Meta loss on this task batch = 2.7483e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 149.8471, GNorm = 0.2865
Meta loss on this task batch = 2.5542e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 149.8557, GNorm = 0.2729
Meta loss on this task batch = 2.6175e-01, Meta loss averaged over last 500 steps = 2.6549e-01, PNorm = 149.8634, GNorm = 0.3179
Meta loss on this task batch = 2.4542e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 149.8687, GNorm = 0.2589
Meta loss on this task batch = 2.4225e-01, Meta loss averaged over last 500 steps = 2.6543e-01, PNorm = 149.8715, GNorm = 0.2936
Meta loss on this task batch = 2.4378e-01, Meta loss averaged over last 500 steps = 2.6532e-01, PNorm = 149.8738, GNorm = 0.2509
Meta loss on this task batch = 2.6524e-01, Meta loss averaged over last 500 steps = 2.6530e-01, PNorm = 149.8764, GNorm = 0.2623
Meta loss on this task batch = 2.3401e-01, Meta loss averaged over last 500 steps = 2.6514e-01, PNorm = 149.8785, GNorm = 0.2675
Meta loss on this task batch = 2.9647e-01, Meta loss averaged over last 500 steps = 2.6513e-01, PNorm = 149.8815, GNorm = 0.2976
Meta loss on this task batch = 3.2899e-01, Meta loss averaged over last 500 steps = 2.6528e-01, PNorm = 149.8831, GNorm = 0.3019
Meta loss on this task batch = 2.5303e-01, Meta loss averaged over last 500 steps = 2.6534e-01, PNorm = 149.8847, GNorm = 0.2557
Meta loss on this task batch = 2.1878e-01, Meta loss averaged over last 500 steps = 2.6525e-01, PNorm = 149.8881, GNorm = 0.2604
Meta loss on this task batch = 2.4250e-01, Meta loss averaged over last 500 steps = 2.6513e-01, PNorm = 149.8917, GNorm = 0.2674
Meta loss on this task batch = 1.8429e-01, Meta loss averaged over last 500 steps = 2.6508e-01, PNorm = 149.8951, GNorm = 0.2987
Took 134.97033381462097 seconds to complete one epoch of meta training
Took 143.95042085647583 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470272
Epoch 862
Meta loss on this task batch = 2.8991e-01, Meta loss averaged over last 500 steps = 2.6518e-01, PNorm = 149.9004, GNorm = 0.2847
Meta loss on this task batch = 2.9968e-01, Meta loss averaged over last 500 steps = 2.6518e-01, PNorm = 149.9056, GNorm = 0.2883
Meta loss on this task batch = 2.8109e-01, Meta loss averaged over last 500 steps = 2.6517e-01, PNorm = 149.9093, GNorm = 0.3004
Meta loss on this task batch = 2.5636e-01, Meta loss averaged over last 500 steps = 2.6522e-01, PNorm = 149.9134, GNorm = 0.2831
Meta loss on this task batch = 2.4365e-01, Meta loss averaged over last 500 steps = 2.6509e-01, PNorm = 149.9180, GNorm = 0.2750
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 2.6532e-01, PNorm = 149.9225, GNorm = 0.2826
Meta loss on this task batch = 2.3705e-01, Meta loss averaged over last 500 steps = 2.6541e-01, PNorm = 149.9274, GNorm = 0.2694
Meta loss on this task batch = 2.9478e-01, Meta loss averaged over last 500 steps = 2.6543e-01, PNorm = 149.9310, GNorm = 0.3430
Meta loss on this task batch = 2.6249e-01, Meta loss averaged over last 500 steps = 2.6544e-01, PNorm = 149.9361, GNorm = 0.3146
Meta loss on this task batch = 2.5718e-01, Meta loss averaged over last 500 steps = 2.6545e-01, PNorm = 149.9412, GNorm = 0.2702
Meta loss on this task batch = 2.3378e-01, Meta loss averaged over last 500 steps = 2.6530e-01, PNorm = 149.9462, GNorm = 0.2965
Meta loss on this task batch = 2.3593e-01, Meta loss averaged over last 500 steps = 2.6529e-01, PNorm = 149.9525, GNorm = 0.2261
Meta loss on this task batch = 2.0258e-01, Meta loss averaged over last 500 steps = 2.6514e-01, PNorm = 149.9599, GNorm = 0.2708
Meta loss on this task batch = 2.4589e-01, Meta loss averaged over last 500 steps = 2.6504e-01, PNorm = 149.9666, GNorm = 0.2670
Meta loss on this task batch = 2.4995e-01, Meta loss averaged over last 500 steps = 2.6502e-01, PNorm = 149.9724, GNorm = 0.3144
Meta loss on this task batch = 2.8491e-01, Meta loss averaged over last 500 steps = 2.6505e-01, PNorm = 149.9772, GNorm = 0.3366
Meta loss on this task batch = 2.7528e-01, Meta loss averaged over last 500 steps = 2.6506e-01, PNorm = 149.9817, GNorm = 0.2538
Meta loss on this task batch = 3.2497e-01, Meta loss averaged over last 500 steps = 2.6511e-01, PNorm = 149.9870, GNorm = 0.2765
Meta loss on this task batch = 3.0225e-01, Meta loss averaged over last 500 steps = 2.6516e-01, PNorm = 149.9914, GNorm = 0.3074
Took 135.33426761627197 seconds to complete one epoch of meta training
Took 144.04394435882568 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494024
Epoch 863
Meta loss on this task batch = 2.2242e-01, Meta loss averaged over last 500 steps = 2.6506e-01, PNorm = 149.9967, GNorm = 0.2409
Meta loss on this task batch = 3.4101e-01, Meta loss averaged over last 500 steps = 2.6525e-01, PNorm = 149.9992, GNorm = 0.3372
Meta loss on this task batch = 2.3978e-01, Meta loss averaged over last 500 steps = 2.6522e-01, PNorm = 150.0031, GNorm = 0.2907
Meta loss on this task batch = 2.1299e-01, Meta loss averaged over last 500 steps = 2.6514e-01, PNorm = 150.0060, GNorm = 0.2988
Meta loss on this task batch = 2.6299e-01, Meta loss averaged over last 500 steps = 2.6510e-01, PNorm = 150.0083, GNorm = 0.2817
Meta loss on this task batch = 2.6547e-01, Meta loss averaged over last 500 steps = 2.6518e-01, PNorm = 150.0087, GNorm = 0.3249
Meta loss on this task batch = 2.3792e-01, Meta loss averaged over last 500 steps = 2.6524e-01, PNorm = 150.0095, GNorm = 0.2524
Meta loss on this task batch = 2.2507e-01, Meta loss averaged over last 500 steps = 2.6511e-01, PNorm = 150.0116, GNorm = 0.2506
Meta loss on this task batch = 2.5201e-01, Meta loss averaged over last 500 steps = 2.6504e-01, PNorm = 150.0163, GNorm = 0.3305
Meta loss on this task batch = 2.4924e-01, Meta loss averaged over last 500 steps = 2.6488e-01, PNorm = 150.0227, GNorm = 0.3086
Meta loss on this task batch = 2.8282e-01, Meta loss averaged over last 500 steps = 2.6493e-01, PNorm = 150.0310, GNorm = 0.2886
Meta loss on this task batch = 2.7377e-01, Meta loss averaged over last 500 steps = 2.6496e-01, PNorm = 150.0375, GNorm = 0.3041
Meta loss on this task batch = 2.5688e-01, Meta loss averaged over last 500 steps = 2.6486e-01, PNorm = 150.0457, GNorm = 0.4021
Meta loss on this task batch = 2.4127e-01, Meta loss averaged over last 500 steps = 2.6472e-01, PNorm = 150.0548, GNorm = 0.2863
Meta loss on this task batch = 2.4538e-01, Meta loss averaged over last 500 steps = 2.6475e-01, PNorm = 150.0632, GNorm = 0.2822
Meta loss on this task batch = 2.4171e-01, Meta loss averaged over last 500 steps = 2.6465e-01, PNorm = 150.0711, GNorm = 0.2743
Meta loss on this task batch = 3.0125e-01, Meta loss averaged over last 500 steps = 2.6473e-01, PNorm = 150.0771, GNorm = 0.3194
Meta loss on this task batch = 2.3606e-01, Meta loss averaged over last 500 steps = 2.6472e-01, PNorm = 150.0816, GNorm = 0.2366
Meta loss on this task batch = 2.7539e-01, Meta loss averaged over last 500 steps = 2.6472e-01, PNorm = 150.0828, GNorm = 0.3156
Took 136.59195494651794 seconds to complete one epoch of meta training
Took 144.17377591133118 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517555
Epoch 864
Meta loss on this task batch = 2.4214e-01, Meta loss averaged over last 500 steps = 2.6467e-01, PNorm = 150.0847, GNorm = 0.2698
Meta loss on this task batch = 2.4183e-01, Meta loss averaged over last 500 steps = 2.6466e-01, PNorm = 150.0862, GNorm = 0.2776
Meta loss on this task batch = 2.3193e-01, Meta loss averaged over last 500 steps = 2.6460e-01, PNorm = 150.0883, GNorm = 0.2643
Meta loss on this task batch = 2.6089e-01, Meta loss averaged over last 500 steps = 2.6460e-01, PNorm = 150.0908, GNorm = 0.3041
Meta loss on this task batch = 2.4703e-01, Meta loss averaged over last 500 steps = 2.6469e-01, PNorm = 150.0934, GNorm = 0.2708
Meta loss on this task batch = 2.1648e-01, Meta loss averaged over last 500 steps = 2.6462e-01, PNorm = 150.0960, GNorm = 0.2253
Meta loss on this task batch = 2.8776e-01, Meta loss averaged over last 500 steps = 2.6467e-01, PNorm = 150.1002, GNorm = 0.3476
Meta loss on this task batch = 3.0517e-01, Meta loss averaged over last 500 steps = 2.6473e-01, PNorm = 150.1029, GNorm = 0.3447
Meta loss on this task batch = 2.6423e-01, Meta loss averaged over last 500 steps = 2.6471e-01, PNorm = 150.1045, GNorm = 0.3356
Meta loss on this task batch = 2.7055e-01, Meta loss averaged over last 500 steps = 2.6476e-01, PNorm = 150.1064, GNorm = 0.3251
Meta loss on this task batch = 2.2968e-01, Meta loss averaged over last 500 steps = 2.6466e-01, PNorm = 150.1104, GNorm = 0.2970
Meta loss on this task batch = 2.3766e-01, Meta loss averaged over last 500 steps = 2.6456e-01, PNorm = 150.1135, GNorm = 0.2873
Meta loss on this task batch = 3.2002e-01, Meta loss averaged over last 500 steps = 2.6464e-01, PNorm = 150.1167, GNorm = 0.3644
Meta loss on this task batch = 2.1333e-01, Meta loss averaged over last 500 steps = 2.6461e-01, PNorm = 150.1206, GNorm = 0.2836
Meta loss on this task batch = 3.1488e-01, Meta loss averaged over last 500 steps = 2.6463e-01, PNorm = 150.1248, GNorm = 0.2864
Meta loss on this task batch = 3.0222e-01, Meta loss averaged over last 500 steps = 2.6469e-01, PNorm = 150.1279, GNorm = 0.2954
Meta loss on this task batch = 2.4636e-01, Meta loss averaged over last 500 steps = 2.6465e-01, PNorm = 150.1291, GNorm = 0.2767
Meta loss on this task batch = 3.1887e-01, Meta loss averaged over last 500 steps = 2.6471e-01, PNorm = 150.1318, GNorm = 0.3148
Meta loss on this task batch = 2.6613e-01, Meta loss averaged over last 500 steps = 2.6482e-01, PNorm = 150.1338, GNorm = 0.3697
Took 134.28352308273315 seconds to complete one epoch of meta training
Took 142.9695291519165 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500166
Epoch 865
Meta loss on this task batch = 3.0407e-01, Meta loss averaged over last 500 steps = 2.6491e-01, PNorm = 150.1336, GNorm = 0.3152
Meta loss on this task batch = 2.5035e-01, Meta loss averaged over last 500 steps = 2.6490e-01, PNorm = 150.1340, GNorm = 0.2625
Meta loss on this task batch = 1.8867e-01, Meta loss averaged over last 500 steps = 2.6474e-01, PNorm = 150.1356, GNorm = 0.2075
Meta loss on this task batch = 2.6883e-01, Meta loss averaged over last 500 steps = 2.6467e-01, PNorm = 150.1398, GNorm = 0.3062
Meta loss on this task batch = 2.3462e-01, Meta loss averaged over last 500 steps = 2.6472e-01, PNorm = 150.1432, GNorm = 0.2907
Meta loss on this task batch = 2.6189e-01, Meta loss averaged over last 500 steps = 2.6468e-01, PNorm = 150.1474, GNorm = 0.2529
Meta loss on this task batch = 2.3939e-01, Meta loss averaged over last 500 steps = 2.6451e-01, PNorm = 150.1503, GNorm = 0.3089
Meta loss on this task batch = 2.6493e-01, Meta loss averaged over last 500 steps = 2.6449e-01, PNorm = 150.1526, GNorm = 0.2574
Meta loss on this task batch = 2.8213e-01, Meta loss averaged over last 500 steps = 2.6451e-01, PNorm = 150.1555, GNorm = 0.2684
Meta loss on this task batch = 2.8020e-01, Meta loss averaged over last 500 steps = 2.6460e-01, PNorm = 150.1578, GNorm = 0.2551
Meta loss on this task batch = 2.7104e-01, Meta loss averaged over last 500 steps = 2.6458e-01, PNorm = 150.1606, GNorm = 0.2812
Meta loss on this task batch = 3.0838e-01, Meta loss averaged over last 500 steps = 2.6462e-01, PNorm = 150.1644, GNorm = 0.2581
Meta loss on this task batch = 2.5177e-01, Meta loss averaged over last 500 steps = 2.6452e-01, PNorm = 150.1692, GNorm = 0.2913
Meta loss on this task batch = 2.4919e-01, Meta loss averaged over last 500 steps = 2.6444e-01, PNorm = 150.1735, GNorm = 0.3124
Meta loss on this task batch = 2.6217e-01, Meta loss averaged over last 500 steps = 2.6442e-01, PNorm = 150.1783, GNorm = 0.2716
Meta loss on this task batch = 2.9997e-01, Meta loss averaged over last 500 steps = 2.6456e-01, PNorm = 150.1828, GNorm = 0.2708
Meta loss on this task batch = 3.0052e-01, Meta loss averaged over last 500 steps = 2.6463e-01, PNorm = 150.1867, GNorm = 0.2904
Meta loss on this task batch = 2.2051e-01, Meta loss averaged over last 500 steps = 2.6460e-01, PNorm = 150.1907, GNorm = 0.2783
Meta loss on this task batch = 2.4417e-01, Meta loss averaged over last 500 steps = 2.6460e-01, PNorm = 150.1941, GNorm = 0.3437
Took 131.1445209980011 seconds to complete one epoch of meta training
Took 139.60037112236023 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480378
Epoch 866
Meta loss on this task batch = 2.1891e-01, Meta loss averaged over last 500 steps = 2.6448e-01, PNorm = 150.1962, GNorm = 0.2850
Meta loss on this task batch = 2.4649e-01, Meta loss averaged over last 500 steps = 2.6445e-01, PNorm = 150.1978, GNorm = 0.2790
Meta loss on this task batch = 2.4041e-01, Meta loss averaged over last 500 steps = 2.6432e-01, PNorm = 150.1988, GNorm = 0.2526
Meta loss on this task batch = 3.0074e-01, Meta loss averaged over last 500 steps = 2.6435e-01, PNorm = 150.1992, GNorm = 0.2888
Meta loss on this task batch = 2.5565e-01, Meta loss averaged over last 500 steps = 2.6437e-01, PNorm = 150.2007, GNorm = 0.2617
Meta loss on this task batch = 2.5345e-01, Meta loss averaged over last 500 steps = 2.6443e-01, PNorm = 150.2018, GNorm = 0.3188
Meta loss on this task batch = 2.1985e-01, Meta loss averaged over last 500 steps = 2.6449e-01, PNorm = 150.2059, GNorm = 0.2709
Meta loss on this task batch = 2.6144e-01, Meta loss averaged over last 500 steps = 2.6443e-01, PNorm = 150.2107, GNorm = 0.3246
Meta loss on this task batch = 2.4901e-01, Meta loss averaged over last 500 steps = 2.6435e-01, PNorm = 150.2158, GNorm = 0.2949
Meta loss on this task batch = 3.1332e-01, Meta loss averaged over last 500 steps = 2.6460e-01, PNorm = 150.2221, GNorm = 0.3703
Meta loss on this task batch = 2.4734e-01, Meta loss averaged over last 500 steps = 2.6460e-01, PNorm = 150.2294, GNorm = 0.2993
Meta loss on this task batch = 2.9723e-01, Meta loss averaged over last 500 steps = 2.6473e-01, PNorm = 150.2340, GNorm = 0.3481
Meta loss on this task batch = 2.7367e-01, Meta loss averaged over last 500 steps = 2.6473e-01, PNorm = 150.2396, GNorm = 0.3064
Meta loss on this task batch = 2.4326e-01, Meta loss averaged over last 500 steps = 2.6465e-01, PNorm = 150.2460, GNorm = 0.2922
Meta loss on this task batch = 2.9612e-01, Meta loss averaged over last 500 steps = 2.6469e-01, PNorm = 150.2508, GNorm = 0.3238
Meta loss on this task batch = 3.3222e-01, Meta loss averaged over last 500 steps = 2.6467e-01, PNorm = 150.2519, GNorm = 0.4496
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.6477e-01, PNorm = 150.2550, GNorm = 0.3367
Meta loss on this task batch = 2.8623e-01, Meta loss averaged over last 500 steps = 2.6477e-01, PNorm = 150.2561, GNorm = 0.3207
Meta loss on this task batch = 1.8524e-01, Meta loss averaged over last 500 steps = 2.6464e-01, PNorm = 150.2565, GNorm = 0.3010
Took 132.87625360488892 seconds to complete one epoch of meta training
Took 141.70558309555054 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481363
Epoch 867
Meta loss on this task batch = 2.8647e-01, Meta loss averaged over last 500 steps = 2.6472e-01, PNorm = 150.2576, GNorm = 0.2838
Meta loss on this task batch = 2.1301e-01, Meta loss averaged over last 500 steps = 2.6465e-01, PNorm = 150.2602, GNorm = 0.2756
Meta loss on this task batch = 2.8880e-01, Meta loss averaged over last 500 steps = 2.6476e-01, PNorm = 150.2623, GNorm = 0.2918
Meta loss on this task batch = 2.4412e-01, Meta loss averaged over last 500 steps = 2.6469e-01, PNorm = 150.2655, GNorm = 0.2873
Meta loss on this task batch = 3.0004e-01, Meta loss averaged over last 500 steps = 2.6464e-01, PNorm = 150.2704, GNorm = 0.3010
Meta loss on this task batch = 2.7265e-01, Meta loss averaged over last 500 steps = 2.6473e-01, PNorm = 150.2760, GNorm = 0.2884
Meta loss on this task batch = 3.0891e-01, Meta loss averaged over last 500 steps = 2.6490e-01, PNorm = 150.2792, GNorm = 0.2805
Meta loss on this task batch = 2.9458e-01, Meta loss averaged over last 500 steps = 2.6503e-01, PNorm = 150.2829, GNorm = 0.3202
Meta loss on this task batch = 2.4349e-01, Meta loss averaged over last 500 steps = 2.6493e-01, PNorm = 150.2863, GNorm = 0.2864
Meta loss on this task batch = 3.1161e-01, Meta loss averaged over last 500 steps = 2.6504e-01, PNorm = 150.2895, GNorm = 0.3036
Meta loss on this task batch = 2.4015e-01, Meta loss averaged over last 500 steps = 2.6505e-01, PNorm = 150.2937, GNorm = 0.2775
Meta loss on this task batch = 2.7066e-01, Meta loss averaged over last 500 steps = 2.6506e-01, PNorm = 150.2980, GNorm = 0.2800
Meta loss on this task batch = 2.7090e-01, Meta loss averaged over last 500 steps = 2.6500e-01, PNorm = 150.3029, GNorm = 0.3254
Meta loss on this task batch = 2.6719e-01, Meta loss averaged over last 500 steps = 2.6512e-01, PNorm = 150.3089, GNorm = 0.2407
Meta loss on this task batch = 2.4696e-01, Meta loss averaged over last 500 steps = 2.6511e-01, PNorm = 150.3148, GNorm = 0.2572
Meta loss on this task batch = 1.6620e-01, Meta loss averaged over last 500 steps = 2.6493e-01, PNorm = 150.3211, GNorm = 0.2324
Meta loss on this task batch = 2.3787e-01, Meta loss averaged over last 500 steps = 2.6484e-01, PNorm = 150.3267, GNorm = 0.2927
Meta loss on this task batch = 2.6093e-01, Meta loss averaged over last 500 steps = 2.6485e-01, PNorm = 150.3323, GNorm = 0.2688
Meta loss on this task batch = 2.5981e-01, Meta loss averaged over last 500 steps = 2.6475e-01, PNorm = 150.3384, GNorm = 0.3858
Took 130.98918461799622 seconds to complete one epoch of meta training
Took 139.31034517288208 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485227
Epoch 868
Meta loss on this task batch = 2.5324e-01, Meta loss averaged over last 500 steps = 2.6463e-01, PNorm = 150.3417, GNorm = 0.3848
Meta loss on this task batch = 3.0623e-01, Meta loss averaged over last 500 steps = 2.6469e-01, PNorm = 150.3414, GNorm = 0.3110
Meta loss on this task batch = 2.7163e-01, Meta loss averaged over last 500 steps = 2.6473e-01, PNorm = 150.3410, GNorm = 0.3001
Meta loss on this task batch = 2.3352e-01, Meta loss averaged over last 500 steps = 2.6467e-01, PNorm = 150.3398, GNorm = 0.2304
Meta loss on this task batch = 2.3423e-01, Meta loss averaged over last 500 steps = 2.6463e-01, PNorm = 150.3391, GNorm = 0.2726
Meta loss on this task batch = 2.3264e-01, Meta loss averaged over last 500 steps = 2.6457e-01, PNorm = 150.3369, GNorm = 0.2814
Meta loss on this task batch = 2.9008e-01, Meta loss averaged over last 500 steps = 2.6456e-01, PNorm = 150.3367, GNorm = 0.3392
Meta loss on this task batch = 2.5566e-01, Meta loss averaged over last 500 steps = 2.6459e-01, PNorm = 150.3379, GNorm = 0.2753
Meta loss on this task batch = 2.9383e-01, Meta loss averaged over last 500 steps = 2.6460e-01, PNorm = 150.3404, GNorm = 0.3582
Meta loss on this task batch = 2.2872e-01, Meta loss averaged over last 500 steps = 2.6446e-01, PNorm = 150.3434, GNorm = 0.2653
Meta loss on this task batch = 3.3010e-01, Meta loss averaged over last 500 steps = 2.6461e-01, PNorm = 150.3457, GNorm = 0.3432
Meta loss on this task batch = 2.4615e-01, Meta loss averaged over last 500 steps = 2.6459e-01, PNorm = 150.3501, GNorm = 0.3045
Meta loss on this task batch = 2.7237e-01, Meta loss averaged over last 500 steps = 2.6466e-01, PNorm = 150.3555, GNorm = 0.2692
Meta loss on this task batch = 3.0015e-01, Meta loss averaged over last 500 steps = 2.6469e-01, PNorm = 150.3609, GNorm = 0.3156
Meta loss on this task batch = 2.4623e-01, Meta loss averaged over last 500 steps = 2.6461e-01, PNorm = 150.3665, GNorm = 0.2888
Meta loss on this task batch = 2.6494e-01, Meta loss averaged over last 500 steps = 2.6464e-01, PNorm = 150.3710, GNorm = 0.3338
Meta loss on this task batch = 2.2355e-01, Meta loss averaged over last 500 steps = 2.6459e-01, PNorm = 150.3760, GNorm = 0.2786
Meta loss on this task batch = 2.6685e-01, Meta loss averaged over last 500 steps = 2.6455e-01, PNorm = 150.3811, GNorm = 0.3170
Meta loss on this task batch = 2.7034e-01, Meta loss averaged over last 500 steps = 2.6458e-01, PNorm = 150.3880, GNorm = 0.4351
Took 132.283926486969 seconds to complete one epoch of meta training
Took 141.9370288848877 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491279
Epoch 869
Meta loss on this task batch = 2.7687e-01, Meta loss averaged over last 500 steps = 2.6461e-01, PNorm = 150.3945, GNorm = 0.3395
Meta loss on this task batch = 2.6229e-01, Meta loss averaged over last 500 steps = 2.6455e-01, PNorm = 150.4008, GNorm = 0.3010
Meta loss on this task batch = 2.6389e-01, Meta loss averaged over last 500 steps = 2.6450e-01, PNorm = 150.4061, GNorm = 0.3211
Meta loss on this task batch = 2.4491e-01, Meta loss averaged over last 500 steps = 2.6445e-01, PNorm = 150.4122, GNorm = 0.2674
Meta loss on this task batch = 2.1617e-01, Meta loss averaged over last 500 steps = 2.6441e-01, PNorm = 150.4192, GNorm = 0.2515
Meta loss on this task batch = 2.5449e-01, Meta loss averaged over last 500 steps = 2.6432e-01, PNorm = 150.4264, GNorm = 0.2537
Meta loss on this task batch = 2.1421e-01, Meta loss averaged over last 500 steps = 2.6431e-01, PNorm = 150.4322, GNorm = 0.2590
Meta loss on this task batch = 2.4566e-01, Meta loss averaged over last 500 steps = 2.6425e-01, PNorm = 150.4378, GNorm = 0.3009
Meta loss on this task batch = 2.5561e-01, Meta loss averaged over last 500 steps = 2.6416e-01, PNorm = 150.4439, GNorm = 0.2737
Meta loss on this task batch = 2.9606e-01, Meta loss averaged over last 500 steps = 2.6417e-01, PNorm = 150.4520, GNorm = 0.3464
Meta loss on this task batch = 2.6784e-01, Meta loss averaged over last 500 steps = 2.6425e-01, PNorm = 150.4590, GNorm = 0.3196
Meta loss on this task batch = 2.9234e-01, Meta loss averaged over last 500 steps = 2.6423e-01, PNorm = 150.4645, GNorm = 0.2917
Meta loss on this task batch = 3.2743e-01, Meta loss averaged over last 500 steps = 2.6439e-01, PNorm = 150.4700, GNorm = 0.3150
Meta loss on this task batch = 3.0686e-01, Meta loss averaged over last 500 steps = 2.6440e-01, PNorm = 150.4743, GNorm = 0.3853
Meta loss on this task batch = 3.0860e-01, Meta loss averaged over last 500 steps = 2.6449e-01, PNorm = 150.4778, GNorm = 0.3144
Meta loss on this task batch = 2.6722e-01, Meta loss averaged over last 500 steps = 2.6453e-01, PNorm = 150.4826, GNorm = 0.2749
Meta loss on this task batch = 2.5999e-01, Meta loss averaged over last 500 steps = 2.6450e-01, PNorm = 150.4855, GNorm = 0.2763
Meta loss on this task batch = 2.4054e-01, Meta loss averaged over last 500 steps = 2.6450e-01, PNorm = 150.4891, GNorm = 0.2684
Meta loss on this task batch = 2.7323e-01, Meta loss averaged over last 500 steps = 2.6462e-01, PNorm = 150.4925, GNorm = 0.3244
Took 130.63855648040771 seconds to complete one epoch of meta training
Took 139.38835310935974 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469235
Epoch 870
Meta loss on this task batch = 2.3611e-01, Meta loss averaged over last 500 steps = 2.6455e-01, PNorm = 150.4958, GNorm = 0.2890
Meta loss on this task batch = 2.7775e-01, Meta loss averaged over last 500 steps = 2.6462e-01, PNorm = 150.4993, GNorm = 0.2753
Meta loss on this task batch = 2.3799e-01, Meta loss averaged over last 500 steps = 2.6458e-01, PNorm = 150.5043, GNorm = 0.3225
Meta loss on this task batch = 2.2774e-01, Meta loss averaged over last 500 steps = 2.6445e-01, PNorm = 150.5101, GNorm = 0.2497
Meta loss on this task batch = 2.4690e-01, Meta loss averaged over last 500 steps = 2.6438e-01, PNorm = 150.5147, GNorm = 0.3048
Meta loss on this task batch = 3.3167e-01, Meta loss averaged over last 500 steps = 2.6462e-01, PNorm = 150.5188, GNorm = 0.2917
Meta loss on this task batch = 2.0280e-01, Meta loss averaged over last 500 steps = 2.6448e-01, PNorm = 150.5221, GNorm = 0.2712
Meta loss on this task batch = 2.6108e-01, Meta loss averaged over last 500 steps = 2.6443e-01, PNorm = 150.5251, GNorm = 0.2683
Meta loss on this task batch = 3.2650e-01, Meta loss averaged over last 500 steps = 2.6454e-01, PNorm = 150.5270, GNorm = 0.2686
Meta loss on this task batch = 2.4768e-01, Meta loss averaged over last 500 steps = 2.6463e-01, PNorm = 150.5293, GNorm = 0.2663
Meta loss on this task batch = 2.3661e-01, Meta loss averaged over last 500 steps = 2.6459e-01, PNorm = 150.5310, GNorm = 0.2753
Meta loss on this task batch = 3.0255e-01, Meta loss averaged over last 500 steps = 2.6461e-01, PNorm = 150.5328, GNorm = 0.3388
Meta loss on this task batch = 2.5585e-01, Meta loss averaged over last 500 steps = 2.6456e-01, PNorm = 150.5351, GNorm = 0.2501
Meta loss on this task batch = 2.4998e-01, Meta loss averaged over last 500 steps = 2.6467e-01, PNorm = 150.5377, GNorm = 0.3259
Meta loss on this task batch = 1.8737e-01, Meta loss averaged over last 500 steps = 2.6447e-01, PNorm = 150.5384, GNorm = 0.2561
Meta loss on this task batch = 2.7214e-01, Meta loss averaged over last 500 steps = 2.6447e-01, PNorm = 150.5393, GNorm = 0.2921
Meta loss on this task batch = 2.9357e-01, Meta loss averaged over last 500 steps = 2.6467e-01, PNorm = 150.5405, GNorm = 0.3167
Meta loss on this task batch = 2.9172e-01, Meta loss averaged over last 500 steps = 2.6465e-01, PNorm = 150.5409, GNorm = 0.2708
Meta loss on this task batch = 3.0000e-01, Meta loss averaged over last 500 steps = 2.6470e-01, PNorm = 150.5428, GNorm = 0.3487
Took 132.31384706497192 seconds to complete one epoch of meta training
Took 140.7267291545868 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481184
Epoch 871
Meta loss on this task batch = 2.1791e-01, Meta loss averaged over last 500 steps = 2.6444e-01, PNorm = 150.5462, GNorm = 0.2875
Meta loss on this task batch = 2.4653e-01, Meta loss averaged over last 500 steps = 2.6437e-01, PNorm = 150.5508, GNorm = 0.2942
Meta loss on this task batch = 2.7382e-01, Meta loss averaged over last 500 steps = 2.6437e-01, PNorm = 150.5567, GNorm = 0.2792
Meta loss on this task batch = 2.3996e-01, Meta loss averaged over last 500 steps = 2.6430e-01, PNorm = 150.5637, GNorm = 0.3136
Meta loss on this task batch = 2.3612e-01, Meta loss averaged over last 500 steps = 2.6426e-01, PNorm = 150.5709, GNorm = 0.2721
Meta loss on this task batch = 2.5149e-01, Meta loss averaged over last 500 steps = 2.6420e-01, PNorm = 150.5772, GNorm = 0.2602
Meta loss on this task batch = 2.1157e-01, Meta loss averaged over last 500 steps = 2.6411e-01, PNorm = 150.5837, GNorm = 0.3051
Meta loss on this task batch = 2.5770e-01, Meta loss averaged over last 500 steps = 2.6397e-01, PNorm = 150.5899, GNorm = 0.2809
Meta loss on this task batch = 1.9766e-01, Meta loss averaged over last 500 steps = 2.6389e-01, PNorm = 150.5949, GNorm = 0.2353
Meta loss on this task batch = 3.2669e-01, Meta loss averaged over last 500 steps = 2.6410e-01, PNorm = 150.5996, GNorm = 0.3699
Meta loss on this task batch = 2.8828e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 150.6048, GNorm = 0.3322
Meta loss on this task batch = 2.6671e-01, Meta loss averaged over last 500 steps = 2.6407e-01, PNorm = 150.6087, GNorm = 0.2482
Meta loss on this task batch = 3.0080e-01, Meta loss averaged over last 500 steps = 2.6419e-01, PNorm = 150.6100, GNorm = 0.3123
Meta loss on this task batch = 2.5858e-01, Meta loss averaged over last 500 steps = 2.6414e-01, PNorm = 150.6098, GNorm = 0.2906
Meta loss on this task batch = 2.9503e-01, Meta loss averaged over last 500 steps = 2.6417e-01, PNorm = 150.6109, GNorm = 0.3097
Meta loss on this task batch = 3.2254e-01, Meta loss averaged over last 500 steps = 2.6429e-01, PNorm = 150.6119, GNorm = 0.2933
Meta loss on this task batch = 2.9134e-01, Meta loss averaged over last 500 steps = 2.6436e-01, PNorm = 150.6108, GNorm = 0.3401
Meta loss on this task batch = 2.5970e-01, Meta loss averaged over last 500 steps = 2.6437e-01, PNorm = 150.6113, GNorm = 0.2920
Meta loss on this task batch = 2.7018e-01, Meta loss averaged over last 500 steps = 2.6434e-01, PNorm = 150.6112, GNorm = 0.3522
Took 132.5343723297119 seconds to complete one epoch of meta training
Took 141.1656436920166 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481431
Epoch 872
Meta loss on this task batch = 2.6325e-01, Meta loss averaged over last 500 steps = 2.6432e-01, PNorm = 150.6107, GNorm = 0.2668
Meta loss on this task batch = 2.6417e-01, Meta loss averaged over last 500 steps = 2.6438e-01, PNorm = 150.6104, GNorm = 0.2872
Meta loss on this task batch = 2.4749e-01, Meta loss averaged over last 500 steps = 2.6441e-01, PNorm = 150.6095, GNorm = 0.2911
Meta loss on this task batch = 1.9831e-01, Meta loss averaged over last 500 steps = 2.6424e-01, PNorm = 150.6095, GNorm = 0.2340
Meta loss on this task batch = 2.3667e-01, Meta loss averaged over last 500 steps = 2.6407e-01, PNorm = 150.6099, GNorm = 0.2847
Meta loss on this task batch = 2.8440e-01, Meta loss averaged over last 500 steps = 2.6394e-01, PNorm = 150.6109, GNorm = 0.2561
Meta loss on this task batch = 3.1687e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 150.6128, GNorm = 0.4979
Meta loss on this task batch = 2.9000e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 150.6143, GNorm = 0.3182
Meta loss on this task batch = 2.2295e-01, Meta loss averaged over last 500 steps = 2.6398e-01, PNorm = 150.6160, GNorm = 0.2769
Meta loss on this task batch = 2.6979e-01, Meta loss averaged over last 500 steps = 2.6389e-01, PNorm = 150.6183, GNorm = 0.2742
Meta loss on this task batch = 3.0198e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 150.6212, GNorm = 0.3235
Meta loss on this task batch = 3.1862e-01, Meta loss averaged over last 500 steps = 2.6404e-01, PNorm = 150.6224, GNorm = 0.3507
Meta loss on this task batch = 2.1462e-01, Meta loss averaged over last 500 steps = 2.6398e-01, PNorm = 150.6243, GNorm = 0.3063
Meta loss on this task batch = 2.3796e-01, Meta loss averaged over last 500 steps = 2.6398e-01, PNorm = 150.6281, GNorm = 0.2849
Meta loss on this task batch = 2.9446e-01, Meta loss averaged over last 500 steps = 2.6405e-01, PNorm = 150.6318, GNorm = 0.2922
Meta loss on this task batch = 2.6980e-01, Meta loss averaged over last 500 steps = 2.6394e-01, PNorm = 150.6340, GNorm = 0.3022
Meta loss on this task batch = 3.0671e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 150.6364, GNorm = 0.3335
Meta loss on this task batch = 2.2663e-01, Meta loss averaged over last 500 steps = 2.6387e-01, PNorm = 150.6382, GNorm = 0.2618
Meta loss on this task batch = 2.3892e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 150.6374, GNorm = 0.3339
Took 132.3250274658203 seconds to complete one epoch of meta training
Took 142.29068732261658 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444358
Epoch 873
Meta loss on this task batch = 2.4778e-01, Meta loss averaged over last 500 steps = 2.6378e-01, PNorm = 150.6365, GNorm = 0.3216
Meta loss on this task batch = 2.1035e-01, Meta loss averaged over last 500 steps = 2.6373e-01, PNorm = 150.6356, GNorm = 0.2458
Meta loss on this task batch = 3.2327e-01, Meta loss averaged over last 500 steps = 2.6380e-01, PNorm = 150.6329, GNorm = 0.3365
Meta loss on this task batch = 2.6325e-01, Meta loss averaged over last 500 steps = 2.6382e-01, PNorm = 150.6297, GNorm = 0.2979
Meta loss on this task batch = 3.3095e-01, Meta loss averaged over last 500 steps = 2.6406e-01, PNorm = 150.6273, GNorm = 0.3109
Meta loss on this task batch = 2.6185e-01, Meta loss averaged over last 500 steps = 2.6397e-01, PNorm = 150.6273, GNorm = 0.3199
Meta loss on this task batch = 2.9433e-01, Meta loss averaged over last 500 steps = 2.6399e-01, PNorm = 150.6297, GNorm = 0.3637
Meta loss on this task batch = 2.5665e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 150.6327, GNorm = 0.2867
Meta loss on this task batch = 2.4971e-01, Meta loss averaged over last 500 steps = 2.6410e-01, PNorm = 150.6368, GNorm = 0.2879
Meta loss on this task batch = 2.7385e-01, Meta loss averaged over last 500 steps = 2.6419e-01, PNorm = 150.6437, GNorm = 0.2675
Meta loss on this task batch = 2.9292e-01, Meta loss averaged over last 500 steps = 2.6426e-01, PNorm = 150.6527, GNorm = 0.3093
Meta loss on this task batch = 2.3369e-01, Meta loss averaged over last 500 steps = 2.6423e-01, PNorm = 150.6624, GNorm = 0.2552
Meta loss on this task batch = 2.2455e-01, Meta loss averaged over last 500 steps = 2.6410e-01, PNorm = 150.6742, GNorm = 0.3043
Meta loss on this task batch = 2.4459e-01, Meta loss averaged over last 500 steps = 2.6400e-01, PNorm = 150.6869, GNorm = 0.2551
Meta loss on this task batch = 2.3320e-01, Meta loss averaged over last 500 steps = 2.6382e-01, PNorm = 150.6986, GNorm = 0.2933
Meta loss on this task batch = 3.0757e-01, Meta loss averaged over last 500 steps = 2.6394e-01, PNorm = 150.7097, GNorm = 0.2808
Meta loss on this task batch = 2.5570e-01, Meta loss averaged over last 500 steps = 2.6398e-01, PNorm = 150.7178, GNorm = 0.3024
Meta loss on this task batch = 3.3607e-01, Meta loss averaged over last 500 steps = 2.6407e-01, PNorm = 150.7238, GNorm = 0.3538
Meta loss on this task batch = 2.6070e-01, Meta loss averaged over last 500 steps = 2.6413e-01, PNorm = 150.7289, GNorm = 0.3541
Took 131.56889176368713 seconds to complete one epoch of meta training
Took 139.01228737831116 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451372
Epoch 874
Meta loss on this task batch = 2.4112e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 150.7332, GNorm = 0.2612
Meta loss on this task batch = 3.1972e-01, Meta loss averaged over last 500 steps = 2.6415e-01, PNorm = 150.7368, GNorm = 0.3257
Meta loss on this task batch = 2.4460e-01, Meta loss averaged over last 500 steps = 2.6410e-01, PNorm = 150.7401, GNorm = 0.2903
Meta loss on this task batch = 2.6424e-01, Meta loss averaged over last 500 steps = 2.6419e-01, PNorm = 150.7436, GNorm = 0.2469
Meta loss on this task batch = 2.6362e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 150.7460, GNorm = 0.2798
Meta loss on this task batch = 2.6461e-01, Meta loss averaged over last 500 steps = 2.6400e-01, PNorm = 150.7481, GNorm = 0.2897
Meta loss on this task batch = 2.4880e-01, Meta loss averaged over last 500 steps = 2.6406e-01, PNorm = 150.7514, GNorm = 0.2806
Meta loss on this task batch = 2.2808e-01, Meta loss averaged over last 500 steps = 2.6405e-01, PNorm = 150.7541, GNorm = 0.2645
Meta loss on this task batch = 2.5477e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 150.7576, GNorm = 0.2692
Meta loss on this task batch = 2.3555e-01, Meta loss averaged over last 500 steps = 2.6396e-01, PNorm = 150.7619, GNorm = 0.2889
Meta loss on this task batch = 2.2291e-01, Meta loss averaged over last 500 steps = 2.6388e-01, PNorm = 150.7656, GNorm = 0.2574
Meta loss on this task batch = 2.9036e-01, Meta loss averaged over last 500 steps = 2.6393e-01, PNorm = 150.7680, GNorm = 0.2926
Meta loss on this task batch = 2.2951e-01, Meta loss averaged over last 500 steps = 2.6371e-01, PNorm = 150.7696, GNorm = 0.2610
Meta loss on this task batch = 2.8194e-01, Meta loss averaged over last 500 steps = 2.6374e-01, PNorm = 150.7719, GNorm = 0.3152
Meta loss on this task batch = 2.7967e-01, Meta loss averaged over last 500 steps = 2.6372e-01, PNorm = 150.7743, GNorm = 0.2745
Meta loss on this task batch = 1.7939e-01, Meta loss averaged over last 500 steps = 2.6356e-01, PNorm = 150.7780, GNorm = 0.2630
Meta loss on this task batch = 2.7574e-01, Meta loss averaged over last 500 steps = 2.6358e-01, PNorm = 150.7813, GNorm = 0.3163
Meta loss on this task batch = 2.7641e-01, Meta loss averaged over last 500 steps = 2.6366e-01, PNorm = 150.7844, GNorm = 0.3137
Meta loss on this task batch = 2.5640e-01, Meta loss averaged over last 500 steps = 2.6369e-01, PNorm = 150.7872, GNorm = 0.4149
Took 134.32945561408997 seconds to complete one epoch of meta training
Took 142.93784737586975 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490405
Epoch 875
Meta loss on this task batch = 2.2797e-01, Meta loss averaged over last 500 steps = 2.6364e-01, PNorm = 150.7891, GNorm = 0.2948
Meta loss on this task batch = 2.7258e-01, Meta loss averaged over last 500 steps = 2.6368e-01, PNorm = 150.7912, GNorm = 0.2997
Meta loss on this task batch = 2.5053e-01, Meta loss averaged over last 500 steps = 2.6362e-01, PNorm = 150.7931, GNorm = 0.3138
Meta loss on this task batch = 2.1612e-01, Meta loss averaged over last 500 steps = 2.6348e-01, PNorm = 150.7952, GNorm = 0.2869
Meta loss on this task batch = 3.1769e-01, Meta loss averaged over last 500 steps = 2.6360e-01, PNorm = 150.7966, GNorm = 0.3327
Meta loss on this task batch = 2.8652e-01, Meta loss averaged over last 500 steps = 2.6361e-01, PNorm = 150.7969, GNorm = 0.3629
Meta loss on this task batch = 2.8848e-01, Meta loss averaged over last 500 steps = 2.6367e-01, PNorm = 150.7991, GNorm = 0.3189
Meta loss on this task batch = 2.6593e-01, Meta loss averaged over last 500 steps = 2.6374e-01, PNorm = 150.8036, GNorm = 0.2940
Meta loss on this task batch = 3.0145e-01, Meta loss averaged over last 500 steps = 2.6371e-01, PNorm = 150.8070, GNorm = 0.3134
Meta loss on this task batch = 2.7372e-01, Meta loss averaged over last 500 steps = 2.6372e-01, PNorm = 150.8104, GNorm = 0.2827
Meta loss on this task batch = 2.3883e-01, Meta loss averaged over last 500 steps = 2.6370e-01, PNorm = 150.8158, GNorm = 0.2931
Meta loss on this task batch = 2.8074e-01, Meta loss averaged over last 500 steps = 2.6374e-01, PNorm = 150.8202, GNorm = 0.2821
Meta loss on this task batch = 2.6945e-01, Meta loss averaged over last 500 steps = 2.6368e-01, PNorm = 150.8242, GNorm = 0.2537
Meta loss on this task batch = 2.6890e-01, Meta loss averaged over last 500 steps = 2.6375e-01, PNorm = 150.8280, GNorm = 0.2918
Meta loss on this task batch = 2.8682e-01, Meta loss averaged over last 500 steps = 2.6374e-01, PNorm = 150.8312, GNorm = 0.2795
Meta loss on this task batch = 2.5700e-01, Meta loss averaged over last 500 steps = 2.6369e-01, PNorm = 150.8326, GNorm = 0.3023
Meta loss on this task batch = 2.8605e-01, Meta loss averaged over last 500 steps = 2.6379e-01, PNorm = 150.8338, GNorm = 0.3092
Meta loss on this task batch = 2.9011e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 150.8369, GNorm = 0.3148
Meta loss on this task batch = 2.1801e-01, Meta loss averaged over last 500 steps = 2.6365e-01, PNorm = 150.8393, GNorm = 0.3102
Took 132.91475677490234 seconds to complete one epoch of meta training
Took 141.13600206375122 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505321
Epoch 876
Meta loss on this task batch = 2.4133e-01, Meta loss averaged over last 500 steps = 2.6369e-01, PNorm = 150.8414, GNorm = 0.2495
Meta loss on this task batch = 2.4493e-01, Meta loss averaged over last 500 steps = 2.6368e-01, PNorm = 150.8435, GNorm = 0.2350
Meta loss on this task batch = 2.2076e-01, Meta loss averaged over last 500 steps = 2.6367e-01, PNorm = 150.8461, GNorm = 0.2259
Meta loss on this task batch = 2.8715e-01, Meta loss averaged over last 500 steps = 2.6381e-01, PNorm = 150.8490, GNorm = 0.3185
Meta loss on this task batch = 2.8669e-01, Meta loss averaged over last 500 steps = 2.6377e-01, PNorm = 150.8503, GNorm = 0.3232
Meta loss on this task batch = 2.8063e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 150.8524, GNorm = 0.3057
Meta loss on this task batch = 2.6206e-01, Meta loss averaged over last 500 steps = 2.6383e-01, PNorm = 150.8563, GNorm = 0.2979
Meta loss on this task batch = 2.7172e-01, Meta loss averaged over last 500 steps = 2.6387e-01, PNorm = 150.8611, GNorm = 0.2686
Meta loss on this task batch = 2.4453e-01, Meta loss averaged over last 500 steps = 2.6378e-01, PNorm = 150.8649, GNorm = 0.2962
Meta loss on this task batch = 3.0542e-01, Meta loss averaged over last 500 steps = 2.6369e-01, PNorm = 150.8694, GNorm = 0.3106
Meta loss on this task batch = 2.6990e-01, Meta loss averaged over last 500 steps = 2.6373e-01, PNorm = 150.8741, GNorm = 0.3622
Meta loss on this task batch = 2.6172e-01, Meta loss averaged over last 500 steps = 2.6379e-01, PNorm = 150.8788, GNorm = 0.2582
Meta loss on this task batch = 2.6039e-01, Meta loss averaged over last 500 steps = 2.6381e-01, PNorm = 150.8840, GNorm = 0.2895
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 2.6389e-01, PNorm = 150.8890, GNorm = 0.2992
Meta loss on this task batch = 3.2231e-01, Meta loss averaged over last 500 steps = 2.6397e-01, PNorm = 150.8942, GNorm = 0.3536
Meta loss on this task batch = 2.8388e-01, Meta loss averaged over last 500 steps = 2.6396e-01, PNorm = 150.9000, GNorm = 0.2851
Meta loss on this task batch = 2.6078e-01, Meta loss averaged over last 500 steps = 2.6405e-01, PNorm = 150.9080, GNorm = 0.2959
Meta loss on this task batch = 2.1677e-01, Meta loss averaged over last 500 steps = 2.6398e-01, PNorm = 150.9153, GNorm = 0.2598
Meta loss on this task batch = 2.2498e-01, Meta loss averaged over last 500 steps = 2.6389e-01, PNorm = 150.9214, GNorm = 0.3179
Took 131.84305953979492 seconds to complete one epoch of meta training
Took 140.89685463905334 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486909
Epoch 877
Meta loss on this task batch = 3.0063e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 150.9266, GNorm = 0.3311
Meta loss on this task batch = 2.4795e-01, Meta loss averaged over last 500 steps = 2.6402e-01, PNorm = 150.9313, GNorm = 0.2878
Meta loss on this task batch = 2.7911e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 150.9349, GNorm = 0.2932
Meta loss on this task batch = 2.4073e-01, Meta loss averaged over last 500 steps = 2.6415e-01, PNorm = 150.9379, GNorm = 0.2514
Meta loss on this task batch = 2.7173e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 150.9399, GNorm = 0.2938
Meta loss on this task batch = 2.3044e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 150.9416, GNorm = 0.3109
Meta loss on this task batch = 2.0909e-01, Meta loss averaged over last 500 steps = 2.6400e-01, PNorm = 150.9427, GNorm = 0.2474
Meta loss on this task batch = 2.3636e-01, Meta loss averaged over last 500 steps = 2.6392e-01, PNorm = 150.9427, GNorm = 0.2887
Meta loss on this task batch = 2.0592e-01, Meta loss averaged over last 500 steps = 2.6381e-01, PNorm = 150.9438, GNorm = 0.3182
Meta loss on this task batch = 2.5227e-01, Meta loss averaged over last 500 steps = 2.6377e-01, PNorm = 150.9446, GNorm = 0.2728
Meta loss on this task batch = 2.7120e-01, Meta loss averaged over last 500 steps = 2.6380e-01, PNorm = 150.9454, GNorm = 0.2672
Meta loss on this task batch = 3.0345e-01, Meta loss averaged over last 500 steps = 2.6383e-01, PNorm = 150.9453, GNorm = 0.3319
Meta loss on this task batch = 2.6759e-01, Meta loss averaged over last 500 steps = 2.6397e-01, PNorm = 150.9439, GNorm = 0.2929
Meta loss on this task batch = 3.1139e-01, Meta loss averaged over last 500 steps = 2.6411e-01, PNorm = 150.9427, GNorm = 0.2941
Meta loss on this task batch = 2.9777e-01, Meta loss averaged over last 500 steps = 2.6417e-01, PNorm = 150.9437, GNorm = 0.3537
Meta loss on this task batch = 2.5836e-01, Meta loss averaged over last 500 steps = 2.6415e-01, PNorm = 150.9472, GNorm = 0.2710
Meta loss on this task batch = 2.8610e-01, Meta loss averaged over last 500 steps = 2.6420e-01, PNorm = 150.9509, GNorm = 0.3016
Meta loss on this task batch = 2.8934e-01, Meta loss averaged over last 500 steps = 2.6425e-01, PNorm = 150.9545, GNorm = 0.2908
Meta loss on this task batch = 2.5194e-01, Meta loss averaged over last 500 steps = 2.6407e-01, PNorm = 150.9586, GNorm = 0.3534
Took 144.07625675201416 seconds to complete one epoch of meta training
Took 152.67308449745178 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489030
Epoch 878
Meta loss on this task batch = 2.4564e-01, Meta loss averaged over last 500 steps = 2.6399e-01, PNorm = 150.9648, GNorm = 0.2559
Meta loss on this task batch = 2.3366e-01, Meta loss averaged over last 500 steps = 2.6397e-01, PNorm = 150.9711, GNorm = 0.2588
Meta loss on this task batch = 3.0745e-01, Meta loss averaged over last 500 steps = 2.6411e-01, PNorm = 150.9751, GNorm = 0.2795
Meta loss on this task batch = 2.6772e-01, Meta loss averaged over last 500 steps = 2.6416e-01, PNorm = 150.9787, GNorm = 0.3098
Meta loss on this task batch = 2.6835e-01, Meta loss averaged over last 500 steps = 2.6422e-01, PNorm = 150.9818, GNorm = 0.3044
Meta loss on this task batch = 2.7031e-01, Meta loss averaged over last 500 steps = 2.6405e-01, PNorm = 150.9824, GNorm = 0.3205
Meta loss on this task batch = 2.9256e-01, Meta loss averaged over last 500 steps = 2.6413e-01, PNorm = 150.9815, GNorm = 0.3732
Meta loss on this task batch = 2.7803e-01, Meta loss averaged over last 500 steps = 2.6415e-01, PNorm = 150.9807, GNorm = 0.3474
Meta loss on this task batch = 2.6933e-01, Meta loss averaged over last 500 steps = 2.6417e-01, PNorm = 150.9805, GNorm = 0.2583
Meta loss on this task batch = 2.7128e-01, Meta loss averaged over last 500 steps = 2.6413e-01, PNorm = 150.9807, GNorm = 0.2764
Meta loss on this task batch = 2.8408e-01, Meta loss averaged over last 500 steps = 2.6417e-01, PNorm = 150.9822, GNorm = 0.3162
Meta loss on this task batch = 2.2717e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 150.9831, GNorm = 0.2928
Meta loss on this task batch = 2.4652e-01, Meta loss averaged over last 500 steps = 2.6419e-01, PNorm = 150.9853, GNorm = 0.2665
Meta loss on this task batch = 2.5387e-01, Meta loss averaged over last 500 steps = 2.6416e-01, PNorm = 150.9894, GNorm = 0.3164
Meta loss on this task batch = 2.7816e-01, Meta loss averaged over last 500 steps = 2.6419e-01, PNorm = 150.9942, GNorm = 0.2926
Meta loss on this task batch = 2.3188e-01, Meta loss averaged over last 500 steps = 2.6417e-01, PNorm = 151.0003, GNorm = 0.2776
Meta loss on this task batch = 2.7132e-01, Meta loss averaged over last 500 steps = 2.6418e-01, PNorm = 151.0088, GNorm = 0.3224
Meta loss on this task batch = 2.6421e-01, Meta loss averaged over last 500 steps = 2.6423e-01, PNorm = 151.0191, GNorm = 0.2967
Meta loss on this task batch = 3.2012e-01, Meta loss averaged over last 500 steps = 2.6427e-01, PNorm = 151.0269, GNorm = 0.3793
Took 147.94941687583923 seconds to complete one epoch of meta training
Took 156.8838610649109 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509482
Epoch 879
Meta loss on this task batch = 2.5112e-01, Meta loss averaged over last 500 steps = 2.6427e-01, PNorm = 151.0353, GNorm = 0.2743
Meta loss on this task batch = 2.5852e-01, Meta loss averaged over last 500 steps = 2.6425e-01, PNorm = 151.0418, GNorm = 0.3361
Meta loss on this task batch = 2.6346e-01, Meta loss averaged over last 500 steps = 2.6438e-01, PNorm = 151.0476, GNorm = 0.2508
Meta loss on this task batch = 2.8608e-01, Meta loss averaged over last 500 steps = 2.6433e-01, PNorm = 151.0515, GNorm = 0.3332
Meta loss on this task batch = 2.5269e-01, Meta loss averaged over last 500 steps = 2.6415e-01, PNorm = 151.0557, GNorm = 0.2774
Meta loss on this task batch = 2.8046e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 151.0606, GNorm = 0.3802
Meta loss on this task batch = 2.5693e-01, Meta loss averaged over last 500 steps = 2.6410e-01, PNorm = 151.0665, GNorm = 0.3107
Meta loss on this task batch = 2.4400e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 151.0715, GNorm = 0.2764
Meta loss on this task batch = 2.7003e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 151.0771, GNorm = 0.2810
Meta loss on this task batch = 2.3553e-01, Meta loss averaged over last 500 steps = 2.6414e-01, PNorm = 151.0838, GNorm = 0.2755
Meta loss on this task batch = 2.3356e-01, Meta loss averaged over last 500 steps = 2.6411e-01, PNorm = 151.0916, GNorm = 0.2530
Meta loss on this task batch = 2.3778e-01, Meta loss averaged over last 500 steps = 2.6399e-01, PNorm = 151.0979, GNorm = 0.2997
Meta loss on this task batch = 2.3721e-01, Meta loss averaged over last 500 steps = 2.6392e-01, PNorm = 151.1031, GNorm = 0.2859
Meta loss on this task batch = 3.1042e-01, Meta loss averaged over last 500 steps = 2.6395e-01, PNorm = 151.1071, GNorm = 0.2814
Meta loss on this task batch = 1.9454e-01, Meta loss averaged over last 500 steps = 2.6372e-01, PNorm = 151.1102, GNorm = 0.2140
Meta loss on this task batch = 2.9267e-01, Meta loss averaged over last 500 steps = 2.6378e-01, PNorm = 151.1115, GNorm = 0.2599
Meta loss on this task batch = 2.9055e-01, Meta loss averaged over last 500 steps = 2.6380e-01, PNorm = 151.1120, GNorm = 0.3065
Meta loss on this task batch = 2.3709e-01, Meta loss averaged over last 500 steps = 2.6388e-01, PNorm = 151.1116, GNorm = 0.2995
Meta loss on this task batch = 2.2330e-01, Meta loss averaged over last 500 steps = 2.6382e-01, PNorm = 151.1114, GNorm = 0.3381
Took 136.72938895225525 seconds to complete one epoch of meta training
Took 144.9459810256958 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502435
Epoch 880
Meta loss on this task batch = 2.9232e-01, Meta loss averaged over last 500 steps = 2.6388e-01, PNorm = 151.1114, GNorm = 0.3979
Meta loss on this task batch = 2.6369e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 151.1086, GNorm = 0.3420
Meta loss on this task batch = 2.6936e-01, Meta loss averaged over last 500 steps = 2.6388e-01, PNorm = 151.1085, GNorm = 0.2743
Meta loss on this task batch = 2.7487e-01, Meta loss averaged over last 500 steps = 2.6395e-01, PNorm = 151.1083, GNorm = 0.3306
Meta loss on this task batch = 2.9269e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 151.1083, GNorm = 0.3444
Meta loss on this task batch = 2.7849e-01, Meta loss averaged over last 500 steps = 2.6404e-01, PNorm = 151.1081, GNorm = 0.3165
Meta loss on this task batch = 3.2442e-01, Meta loss averaged over last 500 steps = 2.6419e-01, PNorm = 151.1082, GNorm = 0.3002
Meta loss on this task batch = 2.5333e-01, Meta loss averaged over last 500 steps = 2.6420e-01, PNorm = 151.1097, GNorm = 0.2505
Meta loss on this task batch = 2.4324e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 151.1134, GNorm = 0.2710
Meta loss on this task batch = 3.2127e-01, Meta loss averaged over last 500 steps = 2.6418e-01, PNorm = 151.1196, GNorm = 0.2816
Meta loss on this task batch = 2.3434e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 151.1266, GNorm = 0.2543
Meta loss on this task batch = 2.8184e-01, Meta loss averaged over last 500 steps = 2.6419e-01, PNorm = 151.1340, GNorm = 0.2911
Meta loss on this task batch = 2.6880e-01, Meta loss averaged over last 500 steps = 2.6432e-01, PNorm = 151.1411, GNorm = 0.3015
Meta loss on this task batch = 2.4182e-01, Meta loss averaged over last 500 steps = 2.6431e-01, PNorm = 151.1477, GNorm = 0.2420
Meta loss on this task batch = 2.3258e-01, Meta loss averaged over last 500 steps = 2.6434e-01, PNorm = 151.1512, GNorm = 0.2973
Meta loss on this task batch = 2.3653e-01, Meta loss averaged over last 500 steps = 2.6423e-01, PNorm = 151.1533, GNorm = 0.2920
Meta loss on this task batch = 2.3705e-01, Meta loss averaged over last 500 steps = 2.6420e-01, PNorm = 151.1549, GNorm = 0.2688
Meta loss on this task batch = 3.2430e-01, Meta loss averaged over last 500 steps = 2.6422e-01, PNorm = 151.1545, GNorm = 0.3482
Meta loss on this task batch = 2.4368e-01, Meta loss averaged over last 500 steps = 2.6428e-01, PNorm = 151.1540, GNorm = 0.3797
Took 135.82716584205627 seconds to complete one epoch of meta training
Took 144.91683554649353 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494930
Epoch 881
Meta loss on this task batch = 3.1795e-01, Meta loss averaged over last 500 steps = 2.6438e-01, PNorm = 151.1538, GNorm = 0.3958
Meta loss on this task batch = 2.2223e-01, Meta loss averaged over last 500 steps = 2.6415e-01, PNorm = 151.1550, GNorm = 0.2741
Meta loss on this task batch = 2.6260e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 151.1571, GNorm = 0.2837
Meta loss on this task batch = 2.6054e-01, Meta loss averaged over last 500 steps = 2.6407e-01, PNorm = 151.1616, GNorm = 0.3029
Meta loss on this task batch = 2.8951e-01, Meta loss averaged over last 500 steps = 2.6415e-01, PNorm = 151.1667, GNorm = 0.2916
Meta loss on this task batch = 3.0081e-01, Meta loss averaged over last 500 steps = 2.6415e-01, PNorm = 151.1728, GNorm = 0.2707
Meta loss on this task batch = 2.8773e-01, Meta loss averaged over last 500 steps = 2.6420e-01, PNorm = 151.1796, GNorm = 0.2725
Meta loss on this task batch = 2.3961e-01, Meta loss averaged over last 500 steps = 2.6410e-01, PNorm = 151.1872, GNorm = 0.2758
Meta loss on this task batch = 2.2315e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 151.1942, GNorm = 0.2823
Meta loss on this task batch = 2.5093e-01, Meta loss averaged over last 500 steps = 2.6398e-01, PNorm = 151.2000, GNorm = 0.2853
Meta loss on this task batch = 2.1390e-01, Meta loss averaged over last 500 steps = 2.6394e-01, PNorm = 151.2050, GNorm = 0.2479
Meta loss on this task batch = 3.0329e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 151.2087, GNorm = 0.2848
Meta loss on this task batch = 2.3213e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 151.2122, GNorm = 0.2669
Meta loss on this task batch = 2.9167e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 151.2122, GNorm = 0.3528
Meta loss on this task batch = 2.3323e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 151.2135, GNorm = 0.2657
Meta loss on this task batch = 2.6860e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 151.2152, GNorm = 0.2699
Meta loss on this task batch = 2.8278e-01, Meta loss averaged over last 500 steps = 2.6419e-01, PNorm = 151.2160, GNorm = 0.3223
Meta loss on this task batch = 2.6382e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 151.2175, GNorm = 0.2582
Meta loss on this task batch = 2.6444e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 151.2202, GNorm = 0.3172
Took 130.5487678050995 seconds to complete one epoch of meta training
Took 138.55555725097656 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513677
Epoch 882
Meta loss on this task batch = 2.6020e-01, Meta loss averaged over last 500 steps = 2.6395e-01, PNorm = 151.2242, GNorm = 0.2614
Meta loss on this task batch = 2.2651e-01, Meta loss averaged over last 500 steps = 2.6385e-01, PNorm = 151.2299, GNorm = 0.2626
Meta loss on this task batch = 2.3879e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 151.2374, GNorm = 0.2963
Meta loss on this task batch = 2.2324e-01, Meta loss averaged over last 500 steps = 2.6364e-01, PNorm = 151.2442, GNorm = 0.2599
Meta loss on this task batch = 2.6917e-01, Meta loss averaged over last 500 steps = 2.6361e-01, PNorm = 151.2506, GNorm = 0.2811
Meta loss on this task batch = 2.2996e-01, Meta loss averaged over last 500 steps = 2.6348e-01, PNorm = 151.2564, GNorm = 0.2799
Meta loss on this task batch = 2.7784e-01, Meta loss averaged over last 500 steps = 2.6347e-01, PNorm = 151.2627, GNorm = 0.2490
Meta loss on this task batch = 2.5136e-01, Meta loss averaged over last 500 steps = 2.6346e-01, PNorm = 151.2682, GNorm = 0.2603
Meta loss on this task batch = 3.2694e-01, Meta loss averaged over last 500 steps = 2.6356e-01, PNorm = 151.2713, GNorm = 0.4057
Meta loss on this task batch = 2.2735e-01, Meta loss averaged over last 500 steps = 2.6345e-01, PNorm = 151.2725, GNorm = 0.3145
Meta loss on this task batch = 2.9577e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 151.2710, GNorm = 0.3656
Meta loss on this task batch = 2.3521e-01, Meta loss averaged over last 500 steps = 2.6346e-01, PNorm = 151.2699, GNorm = 0.2924
Meta loss on this task batch = 2.5791e-01, Meta loss averaged over last 500 steps = 2.6334e-01, PNorm = 151.2697, GNorm = 0.2812
Meta loss on this task batch = 3.1105e-01, Meta loss averaged over last 500 steps = 2.6338e-01, PNorm = 151.2689, GNorm = 0.3119
Meta loss on this task batch = 2.5826e-01, Meta loss averaged over last 500 steps = 2.6342e-01, PNorm = 151.2678, GNorm = 0.2763
Meta loss on this task batch = 2.8601e-01, Meta loss averaged over last 500 steps = 2.6358e-01, PNorm = 151.2665, GNorm = 0.3055
Meta loss on this task batch = 2.3450e-01, Meta loss averaged over last 500 steps = 2.6358e-01, PNorm = 151.2675, GNorm = 0.2749
Meta loss on this task batch = 2.4816e-01, Meta loss averaged over last 500 steps = 2.6353e-01, PNorm = 151.2713, GNorm = 0.2760
Meta loss on this task batch = 3.0175e-01, Meta loss averaged over last 500 steps = 2.6356e-01, PNorm = 151.2771, GNorm = 0.3399
Took 133.63445043563843 seconds to complete one epoch of meta training
Took 142.37560057640076 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.520400
Epoch 883
Meta loss on this task batch = 2.9992e-01, Meta loss averaged over last 500 steps = 2.6367e-01, PNorm = 151.2840, GNorm = 0.2544
Meta loss on this task batch = 3.0383e-01, Meta loss averaged over last 500 steps = 2.6361e-01, PNorm = 151.2917, GNorm = 0.3020
Meta loss on this task batch = 2.4086e-01, Meta loss averaged over last 500 steps = 2.6359e-01, PNorm = 151.3009, GNorm = 0.3072
Meta loss on this task batch = 2.2708e-01, Meta loss averaged over last 500 steps = 2.6362e-01, PNorm = 151.3101, GNorm = 0.3004
Meta loss on this task batch = 2.6686e-01, Meta loss averaged over last 500 steps = 2.6360e-01, PNorm = 151.3182, GNorm = 0.2782
Meta loss on this task batch = 2.6056e-01, Meta loss averaged over last 500 steps = 2.6352e-01, PNorm = 151.3240, GNorm = 0.3400
Meta loss on this task batch = 2.1825e-01, Meta loss averaged over last 500 steps = 2.6337e-01, PNorm = 151.3297, GNorm = 0.2712
Meta loss on this task batch = 2.3129e-01, Meta loss averaged over last 500 steps = 2.6335e-01, PNorm = 151.3343, GNorm = 0.3470
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 2.6340e-01, PNorm = 151.3379, GNorm = 0.2992
Meta loss on this task batch = 2.9634e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 151.3395, GNorm = 0.3166
Meta loss on this task batch = 2.6727e-01, Meta loss averaged over last 500 steps = 2.6365e-01, PNorm = 151.3387, GNorm = 0.3121
Meta loss on this task batch = 2.8391e-01, Meta loss averaged over last 500 steps = 2.6366e-01, PNorm = 151.3380, GNorm = 0.3044
Meta loss on this task batch = 2.6271e-01, Meta loss averaged over last 500 steps = 2.6362e-01, PNorm = 151.3377, GNorm = 0.3206
Meta loss on this task batch = 2.5174e-01, Meta loss averaged over last 500 steps = 2.6345e-01, PNorm = 151.3389, GNorm = 0.3057
Meta loss on this task batch = 2.8187e-01, Meta loss averaged over last 500 steps = 2.6357e-01, PNorm = 151.3413, GNorm = 0.2691
Meta loss on this task batch = 2.0570e-01, Meta loss averaged over last 500 steps = 2.6346e-01, PNorm = 151.3446, GNorm = 0.2422
Meta loss on this task batch = 2.5963e-01, Meta loss averaged over last 500 steps = 2.6347e-01, PNorm = 151.3499, GNorm = 0.2783
Meta loss on this task batch = 2.6083e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 151.3566, GNorm = 0.2976
Meta loss on this task batch = 3.0590e-01, Meta loss averaged over last 500 steps = 2.6363e-01, PNorm = 151.3642, GNorm = 0.4548
Took 133.2524163722992 seconds to complete one epoch of meta training
Took 141.96903824806213 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493038
Epoch 884
Meta loss on this task batch = 2.6332e-01, Meta loss averaged over last 500 steps = 2.6365e-01, PNorm = 151.3721, GNorm = 0.3127
Meta loss on this task batch = 2.6700e-01, Meta loss averaged over last 500 steps = 2.6358e-01, PNorm = 151.3797, GNorm = 0.2812
Meta loss on this task batch = 2.5003e-01, Meta loss averaged over last 500 steps = 2.6354e-01, PNorm = 151.3855, GNorm = 0.2676
Meta loss on this task batch = 2.9100e-01, Meta loss averaged over last 500 steps = 2.6351e-01, PNorm = 151.3886, GNorm = 0.3300
Meta loss on this task batch = 2.4248e-01, Meta loss averaged over last 500 steps = 2.6348e-01, PNorm = 151.3904, GNorm = 0.2597
Meta loss on this task batch = 2.4106e-01, Meta loss averaged over last 500 steps = 2.6339e-01, PNorm = 151.3923, GNorm = 0.2764
Meta loss on this task batch = 2.6121e-01, Meta loss averaged over last 500 steps = 2.6333e-01, PNorm = 151.3944, GNorm = 0.2907
Meta loss on this task batch = 2.0091e-01, Meta loss averaged over last 500 steps = 2.6324e-01, PNorm = 151.3969, GNorm = 0.3147
Meta loss on this task batch = 2.7875e-01, Meta loss averaged over last 500 steps = 2.6325e-01, PNorm = 151.3977, GNorm = 0.2707
Meta loss on this task batch = 3.0105e-01, Meta loss averaged over last 500 steps = 2.6335e-01, PNorm = 151.3977, GNorm = 0.3337
Meta loss on this task batch = 2.5578e-01, Meta loss averaged over last 500 steps = 2.6338e-01, PNorm = 151.3976, GNorm = 0.2804
Meta loss on this task batch = 2.7236e-01, Meta loss averaged over last 500 steps = 2.6333e-01, PNorm = 151.3988, GNorm = 0.3032
Meta loss on this task batch = 2.8752e-01, Meta loss averaged over last 500 steps = 2.6334e-01, PNorm = 151.4010, GNorm = 0.2909
Meta loss on this task batch = 2.9587e-01, Meta loss averaged over last 500 steps = 2.6342e-01, PNorm = 151.4030, GNorm = 0.3239
Meta loss on this task batch = 2.8600e-01, Meta loss averaged over last 500 steps = 2.6340e-01, PNorm = 151.4050, GNorm = 0.3648
Meta loss on this task batch = 1.8534e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 151.4070, GNorm = 0.2616
Meta loss on this task batch = 2.0725e-01, Meta loss averaged over last 500 steps = 2.6308e-01, PNorm = 151.4101, GNorm = 0.2560
Meta loss on this task batch = 2.5984e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 151.4145, GNorm = 0.3066
Meta loss on this task batch = 2.3466e-01, Meta loss averaged over last 500 steps = 2.6309e-01, PNorm = 151.4210, GNorm = 0.4726
Took 131.74082946777344 seconds to complete one epoch of meta training
Took 140.21962642669678 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503061
Epoch 885
Meta loss on this task batch = 3.5077e-01, Meta loss averaged over last 500 steps = 2.6330e-01, PNorm = 151.4260, GNorm = 0.3478
Meta loss on this task batch = 2.4313e-01, Meta loss averaged over last 500 steps = 2.6319e-01, PNorm = 151.4301, GNorm = 0.2637
Meta loss on this task batch = 1.9993e-01, Meta loss averaged over last 500 steps = 2.6309e-01, PNorm = 151.4344, GNorm = 0.2839
Meta loss on this task batch = 2.8118e-01, Meta loss averaged over last 500 steps = 2.6319e-01, PNorm = 151.4391, GNorm = 0.3036
Meta loss on this task batch = 2.5390e-01, Meta loss averaged over last 500 steps = 2.6319e-01, PNorm = 151.4425, GNorm = 0.2946
Meta loss on this task batch = 2.1600e-01, Meta loss averaged over last 500 steps = 2.6302e-01, PNorm = 151.4457, GNorm = 0.2782
Meta loss on this task batch = 2.6908e-01, Meta loss averaged over last 500 steps = 2.6311e-01, PNorm = 151.4484, GNorm = 0.2796
Meta loss on this task batch = 2.5403e-01, Meta loss averaged over last 500 steps = 2.6308e-01, PNorm = 151.4524, GNorm = 0.2815
Meta loss on this task batch = 2.7626e-01, Meta loss averaged over last 500 steps = 2.6303e-01, PNorm = 151.4558, GNorm = 0.3155
Meta loss on this task batch = 2.4675e-01, Meta loss averaged over last 500 steps = 2.6296e-01, PNorm = 151.4603, GNorm = 0.2497
Meta loss on this task batch = 2.1983e-01, Meta loss averaged over last 500 steps = 2.6298e-01, PNorm = 151.4656, GNorm = 0.2848
Meta loss on this task batch = 2.4599e-01, Meta loss averaged over last 500 steps = 2.6297e-01, PNorm = 151.4724, GNorm = 0.2653
Meta loss on this task batch = 2.3048e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 151.4782, GNorm = 0.2262
Meta loss on this task batch = 3.3747e-01, Meta loss averaged over last 500 steps = 2.6298e-01, PNorm = 151.4811, GNorm = 0.3245
Meta loss on this task batch = 2.8484e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 151.4838, GNorm = 0.3057
Meta loss on this task batch = 2.8852e-01, Meta loss averaged over last 500 steps = 2.6305e-01, PNorm = 151.4866, GNorm = 0.2894
Meta loss on this task batch = 3.0220e-01, Meta loss averaged over last 500 steps = 2.6308e-01, PNorm = 151.4874, GNorm = 0.2926
Meta loss on this task batch = 2.6544e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 151.4858, GNorm = 0.3411
Meta loss on this task batch = 2.2115e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 151.4858, GNorm = 0.2761
Took 134.04672574996948 seconds to complete one epoch of meta training
Took 143.08578777313232 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496987
Epoch 886
Meta loss on this task batch = 2.5009e-01, Meta loss averaged over last 500 steps = 2.6310e-01, PNorm = 151.4861, GNorm = 0.3110
Meta loss on this task batch = 2.4050e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 151.4864, GNorm = 0.2534
Meta loss on this task batch = 2.7180e-01, Meta loss averaged over last 500 steps = 2.6297e-01, PNorm = 151.4869, GNorm = 0.2917
Meta loss on this task batch = 2.7649e-01, Meta loss averaged over last 500 steps = 2.6298e-01, PNorm = 151.4890, GNorm = 0.3089
Meta loss on this task batch = 2.0656e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 151.4914, GNorm = 0.2701
Meta loss on this task batch = 3.2392e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 151.4959, GNorm = 0.3178
Meta loss on this task batch = 2.4545e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 151.4998, GNorm = 0.2580
Meta loss on this task batch = 2.5974e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 151.5039, GNorm = 0.3080
Meta loss on this task batch = 2.3368e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 151.5098, GNorm = 0.2583
Meta loss on this task batch = 1.9151e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 151.5166, GNorm = 0.2424
Meta loss on this task batch = 2.4804e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 151.5234, GNorm = 0.2609
Meta loss on this task batch = 2.4443e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 151.5308, GNorm = 0.2912
Meta loss on this task batch = 3.0429e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 151.5370, GNorm = 0.2937
Meta loss on this task batch = 2.5742e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 151.5419, GNorm = 0.2943
Meta loss on this task batch = 2.7394e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 151.5471, GNorm = 0.3001
Meta loss on this task batch = 3.1256e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 151.5523, GNorm = 0.3547
Meta loss on this task batch = 2.5199e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 151.5557, GNorm = 0.2824
Meta loss on this task batch = 3.1158e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 151.5562, GNorm = 0.3519
Meta loss on this task batch = 2.8152e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 151.5561, GNorm = 0.3761
Took 172.07835793495178 seconds to complete one epoch of meta training
Took 181.0722119808197 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502538
Epoch 887
Meta loss on this task batch = 2.6161e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 151.5559, GNorm = 0.2582
Meta loss on this task batch = 2.1319e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 151.5556, GNorm = 0.2373
Meta loss on this task batch = 2.8959e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 151.5557, GNorm = 0.2872
Meta loss on this task batch = 2.1381e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 151.5570, GNorm = 0.2348
Meta loss on this task batch = 1.8805e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 151.5607, GNorm = 0.2752
Meta loss on this task batch = 2.3943e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 151.5643, GNorm = 0.2951
Meta loss on this task batch = 2.8821e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 151.5675, GNorm = 0.2912
Meta loss on this task batch = 2.4158e-01, Meta loss averaged over last 500 steps = 2.6254e-01, PNorm = 151.5711, GNorm = 0.3022
Meta loss on this task batch = 2.8255e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 151.5745, GNorm = 0.3385
Meta loss on this task batch = 2.8400e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 151.5793, GNorm = 0.2787
Meta loss on this task batch = 3.0459e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 151.5867, GNorm = 0.3069
Meta loss on this task batch = 2.8868e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 151.5952, GNorm = 0.2961
Meta loss on this task batch = 2.4577e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 151.6051, GNorm = 0.3364
Meta loss on this task batch = 3.0920e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 151.6175, GNorm = 0.3277
Meta loss on this task batch = 2.5145e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 151.6296, GNorm = 0.2852
Meta loss on this task batch = 2.4944e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 151.6399, GNorm = 0.2923
Meta loss on this task batch = 2.5892e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 151.6503, GNorm = 0.3372
Meta loss on this task batch = 2.5457e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 151.6598, GNorm = 0.3794
Meta loss on this task batch = 2.2612e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 151.6703, GNorm = 0.3931
Took 301.09753799438477 seconds to complete one epoch of meta training
Took 309.76025772094727 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499036
Epoch 888
Meta loss on this task batch = 2.1984e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 151.6799, GNorm = 0.3156
Meta loss on this task batch = 2.7801e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 151.6869, GNorm = 0.3182
Meta loss on this task batch = 2.7311e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 151.6900, GNorm = 0.3332
Meta loss on this task batch = 2.4455e-01, Meta loss averaged over last 500 steps = 2.6266e-01, PNorm = 151.6906, GNorm = 0.3197
Meta loss on this task batch = 2.9595e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 151.6867, GNorm = 0.4423
Meta loss on this task batch = 2.9039e-01, Meta loss averaged over last 500 steps = 2.6298e-01, PNorm = 151.6803, GNorm = 0.3317
Meta loss on this task batch = 2.0646e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 151.6741, GNorm = 0.2268
Meta loss on this task batch = 2.9038e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 151.6677, GNorm = 0.3117
Meta loss on this task batch = 2.1424e-01, Meta loss averaged over last 500 steps = 2.6266e-01, PNorm = 151.6618, GNorm = 0.2254
Meta loss on this task batch = 2.7500e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 151.6571, GNorm = 0.3185
Meta loss on this task batch = 2.6338e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 151.6546, GNorm = 0.2531
Meta loss on this task batch = 3.0644e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 151.6520, GNorm = 0.2905
Meta loss on this task batch = 2.7477e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 151.6523, GNorm = 0.3096
Meta loss on this task batch = 2.7883e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 151.6543, GNorm = 0.2568
Meta loss on this task batch = 2.3435e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 151.6572, GNorm = 0.2902
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 151.6613, GNorm = 0.3186
Meta loss on this task batch = 2.9302e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 151.6663, GNorm = 0.3046
Meta loss on this task batch = 3.1944e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 151.6711, GNorm = 0.3003
Meta loss on this task batch = 2.6337e-01, Meta loss averaged over last 500 steps = 2.6318e-01, PNorm = 151.6754, GNorm = 0.3735
Took 132.10037517547607 seconds to complete one epoch of meta training
Took 140.3602945804596 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513378
Epoch 889
Meta loss on this task batch = 2.4551e-01, Meta loss averaged over last 500 steps = 2.6318e-01, PNorm = 151.6809, GNorm = 0.2480
Meta loss on this task batch = 2.1633e-01, Meta loss averaged over last 500 steps = 2.6311e-01, PNorm = 151.6867, GNorm = 0.3708
Meta loss on this task batch = 2.6233e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 151.6927, GNorm = 0.2704
Meta loss on this task batch = 3.2032e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 151.6983, GNorm = 0.3136
Meta loss on this task batch = 2.7130e-01, Meta loss averaged over last 500 steps = 2.6305e-01, PNorm = 151.7036, GNorm = 0.2866
Meta loss on this task batch = 2.6883e-01, Meta loss averaged over last 500 steps = 2.6298e-01, PNorm = 151.7092, GNorm = 0.3457
Meta loss on this task batch = 2.5618e-01, Meta loss averaged over last 500 steps = 2.6305e-01, PNorm = 151.7125, GNorm = 0.2893
Meta loss on this task batch = 2.2742e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 151.7142, GNorm = 0.2599
Meta loss on this task batch = 2.6350e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 151.7165, GNorm = 0.2712
Meta loss on this task batch = 2.4524e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 151.7186, GNorm = 0.3020
Meta loss on this task batch = 2.5029e-01, Meta loss averaged over last 500 steps = 2.6291e-01, PNorm = 151.7203, GNorm = 0.3061
Meta loss on this task batch = 3.0655e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 151.7213, GNorm = 0.2979
Meta loss on this task batch = 2.6261e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 151.7223, GNorm = 0.2968
Meta loss on this task batch = 2.3606e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 151.7226, GNorm = 0.2787
Meta loss on this task batch = 2.6867e-01, Meta loss averaged over last 500 steps = 2.6309e-01, PNorm = 151.7219, GNorm = 0.3390
Meta loss on this task batch = 2.7660e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 151.7208, GNorm = 0.2923
Meta loss on this task batch = 2.6431e-01, Meta loss averaged over last 500 steps = 2.6311e-01, PNorm = 151.7219, GNorm = 0.3050
Meta loss on this task batch = 2.6992e-01, Meta loss averaged over last 500 steps = 2.6310e-01, PNorm = 151.7220, GNorm = 0.2833
Meta loss on this task batch = 2.5173e-01, Meta loss averaged over last 500 steps = 2.6309e-01, PNorm = 151.7224, GNorm = 0.3391
Took 178.66691398620605 seconds to complete one epoch of meta training
Took 186.05866026878357 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501196
Epoch 890
Meta loss on this task batch = 2.5733e-01, Meta loss averaged over last 500 steps = 2.6312e-01, PNorm = 151.7213, GNorm = 0.3430
Meta loss on this task batch = 2.5426e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 151.7212, GNorm = 0.3105
Meta loss on this task batch = 2.4342e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 151.7197, GNorm = 0.3145
Meta loss on this task batch = 2.5234e-01, Meta loss averaged over last 500 steps = 2.6305e-01, PNorm = 151.7186, GNorm = 0.2928
Meta loss on this task batch = 2.7935e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 151.7180, GNorm = 0.2625
Meta loss on this task batch = 2.6068e-01, Meta loss averaged over last 500 steps = 2.6311e-01, PNorm = 151.7176, GNorm = 0.3286
Meta loss on this task batch = 2.2647e-01, Meta loss averaged over last 500 steps = 2.6307e-01, PNorm = 151.7165, GNorm = 0.2670
Meta loss on this task batch = 2.5071e-01, Meta loss averaged over last 500 steps = 2.6309e-01, PNorm = 151.7160, GNorm = 0.2547
Meta loss on this task batch = 2.6659e-01, Meta loss averaged over last 500 steps = 2.6316e-01, PNorm = 151.7163, GNorm = 0.3079
Meta loss on this task batch = 2.4882e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 151.7176, GNorm = 0.2394
Meta loss on this task batch = 2.7658e-01, Meta loss averaged over last 500 steps = 2.6320e-01, PNorm = 151.7202, GNorm = 0.2887
Meta loss on this task batch = 2.8724e-01, Meta loss averaged over last 500 steps = 2.6334e-01, PNorm = 151.7230, GNorm = 0.2927
Meta loss on this task batch = 3.2763e-01, Meta loss averaged over last 500 steps = 2.6342e-01, PNorm = 151.7248, GNorm = 0.3150
Meta loss on this task batch = 2.5115e-01, Meta loss averaged over last 500 steps = 2.6331e-01, PNorm = 151.7274, GNorm = 0.2631
Meta loss on this task batch = 3.2083e-01, Meta loss averaged over last 500 steps = 2.6342e-01, PNorm = 151.7327, GNorm = 0.2984
Meta loss on this task batch = 2.9646e-01, Meta loss averaged over last 500 steps = 2.6347e-01, PNorm = 151.7397, GNorm = 0.3166
Meta loss on this task batch = 2.5220e-01, Meta loss averaged over last 500 steps = 2.6352e-01, PNorm = 151.7479, GNorm = 0.2466
Meta loss on this task batch = 2.5350e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 151.7551, GNorm = 0.3088
Meta loss on this task batch = 3.2117e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 151.7624, GNorm = 0.3700
Took 148.2404589653015 seconds to complete one epoch of meta training
Took 156.68209624290466 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.514207
Epoch 891
Meta loss on this task batch = 2.4451e-01, Meta loss averaged over last 500 steps = 2.6362e-01, PNorm = 151.7699, GNorm = 0.2491
Meta loss on this task batch = 2.8213e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 151.7786, GNorm = 0.2713
Meta loss on this task batch = 2.1814e-01, Meta loss averaged over last 500 steps = 2.6338e-01, PNorm = 151.7868, GNorm = 0.2053
Meta loss on this task batch = 2.6183e-01, Meta loss averaged over last 500 steps = 2.6341e-01, PNorm = 151.7940, GNorm = 0.2872
Meta loss on this task batch = 2.7908e-01, Meta loss averaged over last 500 steps = 2.6333e-01, PNorm = 151.7999, GNorm = 0.3269
Meta loss on this task batch = 2.1346e-01, Meta loss averaged over last 500 steps = 2.6323e-01, PNorm = 151.8045, GNorm = 0.3145
Meta loss on this task batch = 2.2227e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 151.8070, GNorm = 0.2961
Meta loss on this task batch = 2.6661e-01, Meta loss averaged over last 500 steps = 2.6310e-01, PNorm = 151.8078, GNorm = 0.3319
Meta loss on this task batch = 2.5544e-01, Meta loss averaged over last 500 steps = 2.6323e-01, PNorm = 151.8081, GNorm = 0.2915
Meta loss on this task batch = 2.2461e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 151.8067, GNorm = 0.2633
Meta loss on this task batch = 2.7986e-01, Meta loss averaged over last 500 steps = 2.6323e-01, PNorm = 151.8053, GNorm = 0.2885
Meta loss on this task batch = 2.4212e-01, Meta loss averaged over last 500 steps = 2.6319e-01, PNorm = 151.8057, GNorm = 0.2738
Meta loss on this task batch = 2.7347e-01, Meta loss averaged over last 500 steps = 2.6326e-01, PNorm = 151.8083, GNorm = 0.3001
Meta loss on this task batch = 3.1597e-01, Meta loss averaged over last 500 steps = 2.6336e-01, PNorm = 151.8119, GNorm = 0.3101
Meta loss on this task batch = 3.0593e-01, Meta loss averaged over last 500 steps = 2.6341e-01, PNorm = 151.8163, GNorm = 0.3186
Meta loss on this task batch = 2.6045e-01, Meta loss averaged over last 500 steps = 2.6337e-01, PNorm = 151.8217, GNorm = 0.3262
Meta loss on this task batch = 2.7974e-01, Meta loss averaged over last 500 steps = 2.6339e-01, PNorm = 151.8272, GNorm = 0.3113
Meta loss on this task batch = 2.6412e-01, Meta loss averaged over last 500 steps = 2.6330e-01, PNorm = 151.8326, GNorm = 0.2865
Meta loss on this task batch = 2.5724e-01, Meta loss averaged over last 500 steps = 2.6331e-01, PNorm = 151.8365, GNorm = 0.4519
Took 134.26820468902588 seconds to complete one epoch of meta training
Took 142.83567333221436 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501772
Epoch 892
Meta loss on this task batch = 2.5247e-01, Meta loss averaged over last 500 steps = 2.6332e-01, PNorm = 151.8407, GNorm = 0.2942
Meta loss on this task batch = 2.3550e-01, Meta loss averaged over last 500 steps = 2.6326e-01, PNorm = 151.8458, GNorm = 0.2507
Meta loss on this task batch = 2.4101e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 151.8516, GNorm = 0.2904
Meta loss on this task batch = 2.1269e-01, Meta loss averaged over last 500 steps = 2.6297e-01, PNorm = 151.8578, GNorm = 0.2585
Meta loss on this task batch = 3.3470e-01, Meta loss averaged over last 500 steps = 2.6320e-01, PNorm = 151.8629, GNorm = 0.2615
Meta loss on this task batch = 2.2154e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 151.8685, GNorm = 0.2538
Meta loss on this task batch = 2.3950e-01, Meta loss averaged over last 500 steps = 2.6320e-01, PNorm = 151.8742, GNorm = 0.3043
Meta loss on this task batch = 1.9045e-01, Meta loss averaged over last 500 steps = 2.6308e-01, PNorm = 151.8811, GNorm = 0.2259
Meta loss on this task batch = 2.2836e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 151.8878, GNorm = 0.3081
Meta loss on this task batch = 2.8933e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 151.8920, GNorm = 0.2977
Meta loss on this task batch = 2.7111e-01, Meta loss averaged over last 500 steps = 2.6307e-01, PNorm = 151.8964, GNorm = 0.3253
Meta loss on this task batch = 2.7768e-01, Meta loss averaged over last 500 steps = 2.6312e-01, PNorm = 151.8987, GNorm = 0.2986
Meta loss on this task batch = 2.3642e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 151.9005, GNorm = 0.3014
Meta loss on this task batch = 2.6360e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 151.9028, GNorm = 0.3066
Meta loss on this task batch = 2.4803e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 151.9037, GNorm = 0.2732
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.6310e-01, PNorm = 151.9043, GNorm = 0.3212
Meta loss on this task batch = 2.8137e-01, Meta loss averaged over last 500 steps = 2.6316e-01, PNorm = 151.9058, GNorm = 0.3300
Meta loss on this task batch = 2.4957e-01, Meta loss averaged over last 500 steps = 2.6307e-01, PNorm = 151.9068, GNorm = 0.2676
Meta loss on this task batch = 2.8582e-01, Meta loss averaged over last 500 steps = 2.6309e-01, PNorm = 151.9078, GNorm = 0.3574
Took 135.32211661338806 seconds to complete one epoch of meta training
Took 143.6823124885559 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513710
Epoch 893
Meta loss on this task batch = 2.9908e-01, Meta loss averaged over last 500 steps = 2.6321e-01, PNorm = 151.9085, GNorm = 0.2885
Meta loss on this task batch = 2.4882e-01, Meta loss averaged over last 500 steps = 2.6311e-01, PNorm = 151.9104, GNorm = 0.2732
Meta loss on this task batch = 2.5147e-01, Meta loss averaged over last 500 steps = 2.6295e-01, PNorm = 151.9127, GNorm = 0.2780
Meta loss on this task batch = 2.3784e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 151.9152, GNorm = 0.3355
Meta loss on this task batch = 2.5309e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 151.9178, GNorm = 0.2792
Meta loss on this task batch = 2.9153e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 151.9222, GNorm = 0.3518
Meta loss on this task batch = 2.8432e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 151.9259, GNorm = 0.2947
Meta loss on this task batch = 3.0498e-01, Meta loss averaged over last 500 steps = 2.6318e-01, PNorm = 151.9275, GNorm = 0.2950
Meta loss on this task batch = 1.9620e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 151.9305, GNorm = 0.2027
Meta loss on this task batch = 2.4287e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 151.9335, GNorm = 0.3227
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 2.6295e-01, PNorm = 151.9350, GNorm = 0.2871
Meta loss on this task batch = 3.1076e-01, Meta loss averaged over last 500 steps = 2.6303e-01, PNorm = 151.9370, GNorm = 0.2759
Meta loss on this task batch = 2.8332e-01, Meta loss averaged over last 500 steps = 2.6298e-01, PNorm = 151.9398, GNorm = 0.2888
Meta loss on this task batch = 1.9590e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 151.9427, GNorm = 0.2802
Meta loss on this task batch = 2.8858e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 151.9463, GNorm = 0.2987
Meta loss on this task batch = 2.7073e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 151.9498, GNorm = 0.2882
Meta loss on this task batch = 2.2773e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 151.9559, GNorm = 0.2826
Meta loss on this task batch = 1.9909e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 151.9623, GNorm = 0.2556
Meta loss on this task batch = 2.7306e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 151.9676, GNorm = 0.3341
Took 132.7812774181366 seconds to complete one epoch of meta training
Took 140.32549858093262 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487676
Epoch 894
Meta loss on this task batch = 2.2580e-01, Meta loss averaged over last 500 steps = 2.6254e-01, PNorm = 151.9708, GNorm = 0.3193
Meta loss on this task batch = 2.4199e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 151.9738, GNorm = 0.2907
Meta loss on this task batch = 2.8715e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 151.9765, GNorm = 0.3138
Meta loss on this task batch = 2.5689e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 151.9801, GNorm = 0.2696
Meta loss on this task batch = 3.1254e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 151.9831, GNorm = 0.3166
Meta loss on this task batch = 2.7910e-01, Meta loss averaged over last 500 steps = 2.6296e-01, PNorm = 151.9867, GNorm = 0.2783
Meta loss on this task batch = 2.7806e-01, Meta loss averaged over last 500 steps = 2.6301e-01, PNorm = 151.9896, GNorm = 0.3026
Meta loss on this task batch = 2.5101e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 151.9904, GNorm = 0.3467
Meta loss on this task batch = 3.2592e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 151.9941, GNorm = 0.3384
Meta loss on this task batch = 2.6522e-01, Meta loss averaged over last 500 steps = 2.6307e-01, PNorm = 152.0014, GNorm = 0.2860
Meta loss on this task batch = 2.1555e-01, Meta loss averaged over last 500 steps = 2.6303e-01, PNorm = 152.0115, GNorm = 0.3018
Meta loss on this task batch = 2.5293e-01, Meta loss averaged over last 500 steps = 2.6307e-01, PNorm = 152.0227, GNorm = 0.2965
Meta loss on this task batch = 2.8796e-01, Meta loss averaged over last 500 steps = 2.6307e-01, PNorm = 152.0338, GNorm = 0.3402
Meta loss on this task batch = 3.3960e-01, Meta loss averaged over last 500 steps = 2.6323e-01, PNorm = 152.0461, GNorm = 0.3439
Meta loss on this task batch = 2.4577e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 152.0600, GNorm = 0.2463
Meta loss on this task batch = 2.6841e-01, Meta loss averaged over last 500 steps = 2.6322e-01, PNorm = 152.0740, GNorm = 0.3904
Meta loss on this task batch = 2.9205e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 152.0852, GNorm = 0.3304
Meta loss on this task batch = 2.6444e-01, Meta loss averaged over last 500 steps = 2.6318e-01, PNorm = 152.0952, GNorm = 0.2784
Meta loss on this task batch = 3.3846e-01, Meta loss averaged over last 500 steps = 2.6331e-01, PNorm = 152.1033, GNorm = 0.4097
Took 132.5559265613556 seconds to complete one epoch of meta training
Took 141.03943753242493 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495757
Epoch 895
Meta loss on this task batch = 2.6983e-01, Meta loss averaged over last 500 steps = 2.6325e-01, PNorm = 152.1088, GNorm = 0.2701
Meta loss on this task batch = 2.7569e-01, Meta loss averaged over last 500 steps = 2.6331e-01, PNorm = 152.1123, GNorm = 0.2954
Meta loss on this task batch = 2.2705e-01, Meta loss averaged over last 500 steps = 2.6323e-01, PNorm = 152.1150, GNorm = 0.2743
Meta loss on this task batch = 2.8601e-01, Meta loss averaged over last 500 steps = 2.6336e-01, PNorm = 152.1176, GNorm = 0.2773
Meta loss on this task batch = 3.1150e-01, Meta loss averaged over last 500 steps = 2.6345e-01, PNorm = 152.1191, GNorm = 0.3097
Meta loss on this task batch = 3.0849e-01, Meta loss averaged over last 500 steps = 2.6352e-01, PNorm = 152.1211, GNorm = 0.3401
Meta loss on this task batch = 3.1619e-01, Meta loss averaged over last 500 steps = 2.6360e-01, PNorm = 152.1218, GNorm = 0.3306
Meta loss on this task batch = 2.8715e-01, Meta loss averaged over last 500 steps = 2.6365e-01, PNorm = 152.1215, GNorm = 0.2701
Meta loss on this task batch = 2.1739e-01, Meta loss averaged over last 500 steps = 2.6356e-01, PNorm = 152.1221, GNorm = 0.2496
Meta loss on this task batch = 2.8799e-01, Meta loss averaged over last 500 steps = 2.6364e-01, PNorm = 152.1232, GNorm = 0.3192
Meta loss on this task batch = 2.6281e-01, Meta loss averaged over last 500 steps = 2.6374e-01, PNorm = 152.1253, GNorm = 0.2905
Meta loss on this task batch = 2.3345e-01, Meta loss averaged over last 500 steps = 2.6370e-01, PNorm = 152.1291, GNorm = 0.2549
Meta loss on this task batch = 2.2300e-01, Meta loss averaged over last 500 steps = 2.6371e-01, PNorm = 152.1324, GNorm = 0.2417
Meta loss on this task batch = 2.3055e-01, Meta loss averaged over last 500 steps = 2.6368e-01, PNorm = 152.1373, GNorm = 0.3445
Meta loss on this task batch = 3.1526e-01, Meta loss averaged over last 500 steps = 2.6380e-01, PNorm = 152.1420, GNorm = 0.2634
Meta loss on this task batch = 2.4975e-01, Meta loss averaged over last 500 steps = 2.6371e-01, PNorm = 152.1468, GNorm = 0.2949
Meta loss on this task batch = 2.7869e-01, Meta loss averaged over last 500 steps = 2.6373e-01, PNorm = 152.1526, GNorm = 0.3228
Meta loss on this task batch = 3.0501e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 152.1584, GNorm = 0.3540
Meta loss on this task batch = 2.5421e-01, Meta loss averaged over last 500 steps = 2.6361e-01, PNorm = 152.1645, GNorm = 0.2829
Took 134.48325538635254 seconds to complete one epoch of meta training
Took 143.09580516815186 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.535931
Epoch 896
Meta loss on this task batch = 2.7585e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 152.1701, GNorm = 0.2776
Meta loss on this task batch = 2.2860e-01, Meta loss averaged over last 500 steps = 2.6339e-01, PNorm = 152.1759, GNorm = 0.2430
Meta loss on this task batch = 2.6038e-01, Meta loss averaged over last 500 steps = 2.6337e-01, PNorm = 152.1808, GNorm = 0.3282
Meta loss on this task batch = 2.0591e-01, Meta loss averaged over last 500 steps = 2.6327e-01, PNorm = 152.1861, GNorm = 0.2910
Meta loss on this task batch = 2.9567e-01, Meta loss averaged over last 500 steps = 2.6338e-01, PNorm = 152.1915, GNorm = 0.3127
Meta loss on this task batch = 2.9779e-01, Meta loss averaged over last 500 steps = 2.6343e-01, PNorm = 152.1951, GNorm = 0.4007
Meta loss on this task batch = 2.3005e-01, Meta loss averaged over last 500 steps = 2.6341e-01, PNorm = 152.1977, GNorm = 0.3279
Meta loss on this task batch = 2.4415e-01, Meta loss averaged over last 500 steps = 2.6335e-01, PNorm = 152.1990, GNorm = 0.3167
Meta loss on this task batch = 3.0914e-01, Meta loss averaged over last 500 steps = 2.6349e-01, PNorm = 152.1995, GNorm = 0.2690
Meta loss on this task batch = 2.2757e-01, Meta loss averaged over last 500 steps = 2.6349e-01, PNorm = 152.2018, GNorm = 0.2535
Meta loss on this task batch = 2.5023e-01, Meta loss averaged over last 500 steps = 2.6349e-01, PNorm = 152.2056, GNorm = 0.2726
Meta loss on this task batch = 3.1223e-01, Meta loss averaged over last 500 steps = 2.6346e-01, PNorm = 152.2099, GNorm = 0.3012
Meta loss on this task batch = 2.6096e-01, Meta loss averaged over last 500 steps = 2.6357e-01, PNorm = 152.2154, GNorm = 0.2978
Meta loss on this task batch = 2.0560e-01, Meta loss averaged over last 500 steps = 2.6346e-01, PNorm = 152.2221, GNorm = 0.2657
Meta loss on this task batch = 2.4289e-01, Meta loss averaged over last 500 steps = 2.6329e-01, PNorm = 152.2289, GNorm = 0.2839
Meta loss on this task batch = 2.5881e-01, Meta loss averaged over last 500 steps = 2.6332e-01, PNorm = 152.2363, GNorm = 0.2718
Meta loss on this task batch = 2.2084e-01, Meta loss averaged over last 500 steps = 2.6328e-01, PNorm = 152.2448, GNorm = 0.2914
Meta loss on this task batch = 2.7848e-01, Meta loss averaged over last 500 steps = 2.6324e-01, PNorm = 152.2518, GNorm = 0.2907
Meta loss on this task batch = 2.9631e-01, Meta loss averaged over last 500 steps = 2.6332e-01, PNorm = 152.2567, GNorm = 0.3514
Took 130.69864082336426 seconds to complete one epoch of meta training
Took 139.766295671463 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476076
Epoch 897
Meta loss on this task batch = 2.2801e-01, Meta loss averaged over last 500 steps = 2.6327e-01, PNorm = 152.2603, GNorm = 0.2723
Meta loss on this task batch = 2.7145e-01, Meta loss averaged over last 500 steps = 2.6344e-01, PNorm = 152.2627, GNorm = 0.3132
Meta loss on this task batch = 2.5284e-01, Meta loss averaged over last 500 steps = 2.6340e-01, PNorm = 152.2656, GNorm = 0.3123
Meta loss on this task batch = 2.8176e-01, Meta loss averaged over last 500 steps = 2.6338e-01, PNorm = 152.2687, GNorm = 0.2973
Meta loss on this task batch = 2.9086e-01, Meta loss averaged over last 500 steps = 2.6338e-01, PNorm = 152.2711, GNorm = 0.3373
Meta loss on this task batch = 2.6505e-01, Meta loss averaged over last 500 steps = 2.6331e-01, PNorm = 152.2709, GNorm = 0.3046
Meta loss on this task batch = 3.2510e-01, Meta loss averaged over last 500 steps = 2.6352e-01, PNorm = 152.2703, GNorm = 0.3026
Meta loss on this task batch = 2.5913e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 152.2698, GNorm = 0.2641
Meta loss on this task batch = 2.7607e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 152.2692, GNorm = 0.3377
Meta loss on this task batch = 3.0431e-01, Meta loss averaged over last 500 steps = 2.6368e-01, PNorm = 152.2697, GNorm = 0.2913
Meta loss on this task batch = 2.7679e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 152.2706, GNorm = 0.3320
Meta loss on this task batch = 2.4519e-01, Meta loss averaged over last 500 steps = 2.6375e-01, PNorm = 152.2730, GNorm = 0.2522
Meta loss on this task batch = 2.4356e-01, Meta loss averaged over last 500 steps = 2.6381e-01, PNorm = 152.2774, GNorm = 0.2730
Meta loss on this task batch = 2.3462e-01, Meta loss averaged over last 500 steps = 2.6377e-01, PNorm = 152.2826, GNorm = 0.3005
Meta loss on this task batch = 2.3011e-01, Meta loss averaged over last 500 steps = 2.6383e-01, PNorm = 152.2904, GNorm = 0.3075
Meta loss on this task batch = 2.4199e-01, Meta loss averaged over last 500 steps = 2.6366e-01, PNorm = 152.2995, GNorm = 0.2800
Meta loss on this task batch = 2.6527e-01, Meta loss averaged over last 500 steps = 2.6362e-01, PNorm = 152.3095, GNorm = 0.3352
Meta loss on this task batch = 2.4699e-01, Meta loss averaged over last 500 steps = 2.6358e-01, PNorm = 152.3197, GNorm = 0.2585
Meta loss on this task batch = 2.3563e-01, Meta loss averaged over last 500 steps = 2.6345e-01, PNorm = 152.3288, GNorm = 0.3279
Took 132.73693323135376 seconds to complete one epoch of meta training
Took 141.63135695457458 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481682
Epoch 898
Meta loss on this task batch = 2.7455e-01, Meta loss averaged over last 500 steps = 2.6348e-01, PNorm = 152.3383, GNorm = 0.2853
Meta loss on this task batch = 2.6743e-01, Meta loss averaged over last 500 steps = 2.6342e-01, PNorm = 152.3468, GNorm = 0.2883
Meta loss on this task batch = 2.5374e-01, Meta loss averaged over last 500 steps = 2.6329e-01, PNorm = 152.3551, GNorm = 0.2931
Meta loss on this task batch = 2.6693e-01, Meta loss averaged over last 500 steps = 2.6324e-01, PNorm = 152.3624, GNorm = 0.2719
Meta loss on this task batch = 2.7910e-01, Meta loss averaged over last 500 steps = 2.6328e-01, PNorm = 152.3679, GNorm = 0.2915
Meta loss on this task batch = 3.1407e-01, Meta loss averaged over last 500 steps = 2.6336e-01, PNorm = 152.3727, GNorm = 0.3361
Meta loss on this task batch = 2.1590e-01, Meta loss averaged over last 500 steps = 2.6327e-01, PNorm = 152.3766, GNorm = 0.2941
Meta loss on this task batch = 2.5357e-01, Meta loss averaged over last 500 steps = 2.6325e-01, PNorm = 152.3798, GNorm = 0.2889
Meta loss on this task batch = 3.3138e-01, Meta loss averaged over last 500 steps = 2.6342e-01, PNorm = 152.3810, GNorm = 0.3744
Meta loss on this task batch = 2.3476e-01, Meta loss averaged over last 500 steps = 2.6349e-01, PNorm = 152.3818, GNorm = 0.2538
Meta loss on this task batch = 2.4786e-01, Meta loss averaged over last 500 steps = 2.6351e-01, PNorm = 152.3836, GNorm = 0.2615
Meta loss on this task batch = 2.4071e-01, Meta loss averaged over last 500 steps = 2.6342e-01, PNorm = 152.3850, GNorm = 0.2571
Meta loss on this task batch = 2.5050e-01, Meta loss averaged over last 500 steps = 2.6329e-01, PNorm = 152.3869, GNorm = 0.2778
Meta loss on this task batch = 2.7319e-01, Meta loss averaged over last 500 steps = 2.6326e-01, PNorm = 152.3889, GNorm = 0.3760
Meta loss on this task batch = 2.5485e-01, Meta loss averaged over last 500 steps = 2.6332e-01, PNorm = 152.3921, GNorm = 0.2919
Meta loss on this task batch = 2.8150e-01, Meta loss averaged over last 500 steps = 2.6334e-01, PNorm = 152.3943, GNorm = 0.3139
Meta loss on this task batch = 2.7019e-01, Meta loss averaged over last 500 steps = 2.6328e-01, PNorm = 152.3957, GNorm = 0.2776
Meta loss on this task batch = 2.8334e-01, Meta loss averaged over last 500 steps = 2.6321e-01, PNorm = 152.3969, GNorm = 0.2980
Meta loss on this task batch = 2.5142e-01, Meta loss averaged over last 500 steps = 2.6328e-01, PNorm = 152.3970, GNorm = 0.3333
Took 131.3108615875244 seconds to complete one epoch of meta training
Took 139.02023029327393 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474979
Epoch 899
Meta loss on this task batch = 2.9058e-01, Meta loss averaged over last 500 steps = 2.6339e-01, PNorm = 152.3974, GNorm = 0.2795
Meta loss on this task batch = 2.3267e-01, Meta loss averaged over last 500 steps = 2.6327e-01, PNorm = 152.3986, GNorm = 0.2283
Meta loss on this task batch = 2.9450e-01, Meta loss averaged over last 500 steps = 2.6332e-01, PNorm = 152.3996, GNorm = 0.3052
Meta loss on this task batch = 2.3749e-01, Meta loss averaged over last 500 steps = 2.6318e-01, PNorm = 152.4018, GNorm = 0.2178
Meta loss on this task batch = 2.2588e-01, Meta loss averaged over last 500 steps = 2.6318e-01, PNorm = 152.4051, GNorm = 0.2716
Meta loss on this task batch = 2.8330e-01, Meta loss averaged over last 500 steps = 2.6326e-01, PNorm = 152.4093, GNorm = 0.2779
Meta loss on this task batch = 2.4479e-01, Meta loss averaged over last 500 steps = 2.6326e-01, PNorm = 152.4119, GNorm = 0.2925
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.6336e-01, PNorm = 152.4164, GNorm = 0.2932
Meta loss on this task batch = 2.9571e-01, Meta loss averaged over last 500 steps = 2.6330e-01, PNorm = 152.4198, GNorm = 0.3496
Meta loss on this task batch = 2.8828e-01, Meta loss averaged over last 500 steps = 2.6335e-01, PNorm = 152.4233, GNorm = 0.3160
Meta loss on this task batch = 2.6643e-01, Meta loss averaged over last 500 steps = 2.6323e-01, PNorm = 152.4274, GNorm = 0.2677
Meta loss on this task batch = 2.2317e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 152.4311, GNorm = 0.2516
Meta loss on this task batch = 2.7533e-01, Meta loss averaged over last 500 steps = 2.6311e-01, PNorm = 152.4345, GNorm = 0.3105
Meta loss on this task batch = 2.5531e-01, Meta loss averaged over last 500 steps = 2.6311e-01, PNorm = 152.4370, GNorm = 0.3108
Meta loss on this task batch = 2.0637e-01, Meta loss averaged over last 500 steps = 2.6302e-01, PNorm = 152.4394, GNorm = 0.2626
Meta loss on this task batch = 2.6306e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 152.4421, GNorm = 0.2833
Meta loss on this task batch = 2.7232e-01, Meta loss averaged over last 500 steps = 2.6296e-01, PNorm = 152.4435, GNorm = 0.2788
Meta loss on this task batch = 2.8830e-01, Meta loss averaged over last 500 steps = 2.6307e-01, PNorm = 152.4441, GNorm = 0.3299
Meta loss on this task batch = 2.9353e-01, Meta loss averaged over last 500 steps = 2.6321e-01, PNorm = 152.4452, GNorm = 0.3485
Took 133.75993633270264 seconds to complete one epoch of meta training
Took 142.24112129211426 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469314
Epoch 900
Meta loss on this task batch = 2.5735e-01, Meta loss averaged over last 500 steps = 2.6323e-01, PNorm = 152.4468, GNorm = 0.3429
Meta loss on this task batch = 2.7761e-01, Meta loss averaged over last 500 steps = 2.6332e-01, PNorm = 152.4496, GNorm = 0.3466
Meta loss on this task batch = 2.5683e-01, Meta loss averaged over last 500 steps = 2.6322e-01, PNorm = 152.4527, GNorm = 0.3156
Meta loss on this task batch = 3.0278e-01, Meta loss averaged over last 500 steps = 2.6331e-01, PNorm = 152.4575, GNorm = 0.3135
Meta loss on this task batch = 1.9426e-01, Meta loss averaged over last 500 steps = 2.6303e-01, PNorm = 152.4635, GNorm = 0.2621
Meta loss on this task batch = 2.4326e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 152.4678, GNorm = 0.2837
Meta loss on this task batch = 2.4520e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 152.4724, GNorm = 0.2536
Meta loss on this task batch = 2.8765e-01, Meta loss averaged over last 500 steps = 2.6294e-01, PNorm = 152.4771, GNorm = 0.2673
Meta loss on this task batch = 2.0113e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 152.4815, GNorm = 0.2462
Meta loss on this task batch = 2.7569e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 152.4833, GNorm = 0.3452
Meta loss on this task batch = 2.1769e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 152.4849, GNorm = 0.2312
Meta loss on this task batch = 2.4371e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 152.4875, GNorm = 0.2779
Meta loss on this task batch = 2.6659e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 152.4911, GNorm = 0.3315
Meta loss on this task batch = 3.3607e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 152.4953, GNorm = 0.3498
Meta loss on this task batch = 2.9774e-01, Meta loss averaged over last 500 steps = 2.6308e-01, PNorm = 152.4991, GNorm = 0.3502
Meta loss on this task batch = 2.6082e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 152.5039, GNorm = 0.3110
Meta loss on this task batch = 3.0530e-01, Meta loss averaged over last 500 steps = 2.6329e-01, PNorm = 152.5086, GNorm = 0.3391
Meta loss on this task batch = 3.0843e-01, Meta loss averaged over last 500 steps = 2.6333e-01, PNorm = 152.5152, GNorm = 0.3107
Meta loss on this task batch = 2.1251e-01, Meta loss averaged over last 500 steps = 2.6330e-01, PNorm = 152.5220, GNorm = 0.3454
Took 133.72241306304932 seconds to complete one epoch of meta training
Took 141.30693936347961 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470802
Epoch 901
Meta loss on this task batch = 3.2547e-01, Meta loss averaged over last 500 steps = 2.6338e-01, PNorm = 152.5293, GNorm = 0.3581
Meta loss on this task batch = 2.3388e-01, Meta loss averaged over last 500 steps = 2.6329e-01, PNorm = 152.5371, GNorm = 0.3037
Meta loss on this task batch = 2.6977e-01, Meta loss averaged over last 500 steps = 2.6347e-01, PNorm = 152.5442, GNorm = 0.3133
Meta loss on this task batch = 2.1375e-01, Meta loss averaged over last 500 steps = 2.6335e-01, PNorm = 152.5517, GNorm = 0.2539
Meta loss on this task batch = 2.6793e-01, Meta loss averaged over last 500 steps = 2.6333e-01, PNorm = 152.5588, GNorm = 0.2416
Meta loss on this task batch = 2.3957e-01, Meta loss averaged over last 500 steps = 2.6330e-01, PNorm = 152.5651, GNorm = 0.3165
Meta loss on this task batch = 2.3898e-01, Meta loss averaged over last 500 steps = 2.6332e-01, PNorm = 152.5704, GNorm = 0.2611
Meta loss on this task batch = 2.7259e-01, Meta loss averaged over last 500 steps = 2.6332e-01, PNorm = 152.5729, GNorm = 0.3030
Meta loss on this task batch = 2.5865e-01, Meta loss averaged over last 500 steps = 2.6333e-01, PNorm = 152.5732, GNorm = 0.3147
Meta loss on this task batch = 2.6147e-01, Meta loss averaged over last 500 steps = 2.6343e-01, PNorm = 152.5727, GNorm = 0.2897
Meta loss on this task batch = 2.3382e-01, Meta loss averaged over last 500 steps = 2.6326e-01, PNorm = 152.5722, GNorm = 0.2497
Meta loss on this task batch = 2.8593e-01, Meta loss averaged over last 500 steps = 2.6326e-01, PNorm = 152.5719, GNorm = 0.2957
Meta loss on this task batch = 3.0005e-01, Meta loss averaged over last 500 steps = 2.6328e-01, PNorm = 152.5706, GNorm = 0.2979
Meta loss on this task batch = 3.2021e-01, Meta loss averaged over last 500 steps = 2.6339e-01, PNorm = 152.5682, GNorm = 0.3774
Meta loss on this task batch = 2.7379e-01, Meta loss averaged over last 500 steps = 2.6333e-01, PNorm = 152.5668, GNorm = 0.2568
Meta loss on this task batch = 2.4602e-01, Meta loss averaged over last 500 steps = 2.6328e-01, PNorm = 152.5666, GNorm = 0.2553
Meta loss on this task batch = 2.4296e-01, Meta loss averaged over last 500 steps = 2.6329e-01, PNorm = 152.5686, GNorm = 0.2731
Meta loss on this task batch = 2.4009e-01, Meta loss averaged over last 500 steps = 2.6320e-01, PNorm = 152.5703, GNorm = 0.2688
Meta loss on this task batch = 2.7452e-01, Meta loss averaged over last 500 steps = 2.6321e-01, PNorm = 152.5750, GNorm = 0.3036
Took 134.86507177352905 seconds to complete one epoch of meta training
Took 142.30727171897888 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495831
Epoch 902
Meta loss on this task batch = 2.6210e-01, Meta loss averaged over last 500 steps = 2.6320e-01, PNorm = 152.5806, GNorm = 0.2666
Meta loss on this task batch = 2.5611e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 152.5863, GNorm = 0.2662
Meta loss on this task batch = 2.3748e-01, Meta loss averaged over last 500 steps = 2.6310e-01, PNorm = 152.5908, GNorm = 0.2768
Meta loss on this task batch = 3.3079e-01, Meta loss averaged over last 500 steps = 2.6319e-01, PNorm = 152.5966, GNorm = 0.3178
Meta loss on this task batch = 2.2767e-01, Meta loss averaged over last 500 steps = 2.6307e-01, PNorm = 152.6038, GNorm = 0.2407
Meta loss on this task batch = 2.1621e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 152.6102, GNorm = 0.2828
Meta loss on this task batch = 2.3559e-01, Meta loss averaged over last 500 steps = 2.6305e-01, PNorm = 152.6150, GNorm = 0.2619
Meta loss on this task batch = 2.9865e-01, Meta loss averaged over last 500 steps = 2.6316e-01, PNorm = 152.6194, GNorm = 0.2999
Meta loss on this task batch = 2.7794e-01, Meta loss averaged over last 500 steps = 2.6327e-01, PNorm = 152.6227, GNorm = 0.2808
Meta loss on this task batch = 2.6112e-01, Meta loss averaged over last 500 steps = 2.6322e-01, PNorm = 152.6266, GNorm = 0.2938
Meta loss on this task batch = 2.5025e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 152.6294, GNorm = 0.3192
Meta loss on this task batch = 2.3803e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 152.6324, GNorm = 0.2802
Meta loss on this task batch = 2.3063e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 152.6342, GNorm = 0.2827
Meta loss on this task batch = 2.7983e-01, Meta loss averaged over last 500 steps = 2.6302e-01, PNorm = 152.6364, GNorm = 0.3048
Meta loss on this task batch = 3.0320e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 152.6375, GNorm = 0.3752
Meta loss on this task batch = 2.6063e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 152.6379, GNorm = 0.2726
Meta loss on this task batch = 2.6087e-01, Meta loss averaged over last 500 steps = 2.6303e-01, PNorm = 152.6369, GNorm = 0.3556
Meta loss on this task batch = 2.7360e-01, Meta loss averaged over last 500 steps = 2.6305e-01, PNorm = 152.6361, GNorm = 0.2835
Meta loss on this task batch = 3.4339e-01, Meta loss averaged over last 500 steps = 2.6321e-01, PNorm = 152.6363, GNorm = 0.3871
Took 133.28119444847107 seconds to complete one epoch of meta training
Took 140.87291932106018 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475111
Epoch 903
Meta loss on this task batch = 2.1939e-01, Meta loss averaged over last 500 steps = 2.6308e-01, PNorm = 152.6382, GNorm = 0.2836
Meta loss on this task batch = 2.7660e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 152.6407, GNorm = 0.3007
Meta loss on this task batch = 3.4126e-01, Meta loss averaged over last 500 steps = 2.6311e-01, PNorm = 152.6452, GNorm = 0.2864
Meta loss on this task batch = 2.9522e-01, Meta loss averaged over last 500 steps = 2.6318e-01, PNorm = 152.6508, GNorm = 0.3078
Meta loss on this task batch = 2.3239e-01, Meta loss averaged over last 500 steps = 2.6321e-01, PNorm = 152.6563, GNorm = 0.2730
Meta loss on this task batch = 2.7201e-01, Meta loss averaged over last 500 steps = 2.6330e-01, PNorm = 152.6607, GNorm = 0.2583
Meta loss on this task batch = 2.8631e-01, Meta loss averaged over last 500 steps = 2.6327e-01, PNorm = 152.6668, GNorm = 0.3041
Meta loss on this task batch = 2.7734e-01, Meta loss averaged over last 500 steps = 2.6333e-01, PNorm = 152.6716, GNorm = 0.2698
Meta loss on this task batch = 2.4654e-01, Meta loss averaged over last 500 steps = 2.6327e-01, PNorm = 152.6768, GNorm = 0.2688
Meta loss on this task batch = 2.4408e-01, Meta loss averaged over last 500 steps = 2.6327e-01, PNorm = 152.6821, GNorm = 0.2705
Meta loss on this task batch = 2.7722e-01, Meta loss averaged over last 500 steps = 2.6328e-01, PNorm = 152.6873, GNorm = 0.2598
Meta loss on this task batch = 3.0718e-01, Meta loss averaged over last 500 steps = 2.6344e-01, PNorm = 152.6914, GNorm = 0.3033
Meta loss on this task batch = 2.7886e-01, Meta loss averaged over last 500 steps = 2.6358e-01, PNorm = 152.6953, GNorm = 0.2841
Meta loss on this task batch = 2.4024e-01, Meta loss averaged over last 500 steps = 2.6359e-01, PNorm = 152.7002, GNorm = 0.2287
Meta loss on this task batch = 2.4537e-01, Meta loss averaged over last 500 steps = 2.6366e-01, PNorm = 152.7049, GNorm = 0.2696
Meta loss on this task batch = 2.5164e-01, Meta loss averaged over last 500 steps = 2.6366e-01, PNorm = 152.7091, GNorm = 0.2940
Meta loss on this task batch = 2.8790e-01, Meta loss averaged over last 500 steps = 2.6370e-01, PNorm = 152.7126, GNorm = 0.3063
Meta loss on this task batch = 2.6203e-01, Meta loss averaged over last 500 steps = 2.6361e-01, PNorm = 152.7163, GNorm = 0.2805
Meta loss on this task batch = 2.1838e-01, Meta loss averaged over last 500 steps = 2.6351e-01, PNorm = 152.7209, GNorm = 0.3120
Took 133.17392802238464 seconds to complete one epoch of meta training
Took 141.6312747001648 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473389
Epoch 904
Meta loss on this task batch = 2.8194e-01, Meta loss averaged over last 500 steps = 2.6346e-01, PNorm = 152.7253, GNorm = 0.2731
Meta loss on this task batch = 2.7702e-01, Meta loss averaged over last 500 steps = 2.6341e-01, PNorm = 152.7294, GNorm = 0.2763
Meta loss on this task batch = 2.3845e-01, Meta loss averaged over last 500 steps = 2.6337e-01, PNorm = 152.7324, GNorm = 0.2998
Meta loss on this task batch = 2.5592e-01, Meta loss averaged over last 500 steps = 2.6331e-01, PNorm = 152.7361, GNorm = 0.2720
Meta loss on this task batch = 2.5590e-01, Meta loss averaged over last 500 steps = 2.6325e-01, PNorm = 152.7403, GNorm = 0.2544
Meta loss on this task batch = 2.6322e-01, Meta loss averaged over last 500 steps = 2.6327e-01, PNorm = 152.7461, GNorm = 0.2839
Meta loss on this task batch = 3.1017e-01, Meta loss averaged over last 500 steps = 2.6340e-01, PNorm = 152.7499, GNorm = 0.3181
Meta loss on this task batch = 2.0050e-01, Meta loss averaged over last 500 steps = 2.6333e-01, PNorm = 152.7547, GNorm = 0.2288
Meta loss on this task batch = 2.3357e-01, Meta loss averaged over last 500 steps = 2.6318e-01, PNorm = 152.7607, GNorm = 0.2911
Meta loss on this task batch = 2.5854e-01, Meta loss averaged over last 500 steps = 2.6317e-01, PNorm = 152.7671, GNorm = 0.3027
Meta loss on this task batch = 2.7777e-01, Meta loss averaged over last 500 steps = 2.6319e-01, PNorm = 152.7708, GNorm = 0.2902
Meta loss on this task batch = 3.0093e-01, Meta loss averaged over last 500 steps = 2.6325e-01, PNorm = 152.7744, GNorm = 0.3118
Meta loss on this task batch = 2.6702e-01, Meta loss averaged over last 500 steps = 2.6320e-01, PNorm = 152.7767, GNorm = 0.3018
Meta loss on this task batch = 2.3751e-01, Meta loss averaged over last 500 steps = 2.6311e-01, PNorm = 152.7795, GNorm = 0.2634
Meta loss on this task batch = 2.7415e-01, Meta loss averaged over last 500 steps = 2.6312e-01, PNorm = 152.7820, GNorm = 0.2757
Meta loss on this task batch = 2.8437e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 152.7858, GNorm = 0.3427
Meta loss on this task batch = 2.0334e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 152.7918, GNorm = 0.3165
Meta loss on this task batch = 2.7159e-01, Meta loss averaged over last 500 steps = 2.6308e-01, PNorm = 152.7969, GNorm = 0.3974
Meta loss on this task batch = 1.9749e-01, Meta loss averaged over last 500 steps = 2.6298e-01, PNorm = 152.8024, GNorm = 0.2993
Took 130.63228034973145 seconds to complete one epoch of meta training
Took 139.85198950767517 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497975
Epoch 905
Meta loss on this task batch = 3.2271e-01, Meta loss averaged over last 500 steps = 2.6312e-01, PNorm = 152.8085, GNorm = 0.3084
Meta loss on this task batch = 2.2843e-01, Meta loss averaged over last 500 steps = 2.6302e-01, PNorm = 152.8148, GNorm = 0.2769
Meta loss on this task batch = 2.8877e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 152.8184, GNorm = 0.3379
Meta loss on this task batch = 2.4013e-01, Meta loss averaged over last 500 steps = 2.6307e-01, PNorm = 152.8229, GNorm = 0.2272
Meta loss on this task batch = 2.5872e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 152.8275, GNorm = 0.2929
Meta loss on this task batch = 2.2519e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 152.8321, GNorm = 0.2446
Meta loss on this task batch = 2.7540e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 152.8365, GNorm = 0.2878
Meta loss on this task batch = 2.3054e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 152.8402, GNorm = 0.2644
Meta loss on this task batch = 2.9169e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 152.8438, GNorm = 0.2948
Meta loss on this task batch = 3.1294e-01, Meta loss averaged over last 500 steps = 2.6297e-01, PNorm = 152.8469, GNorm = 0.2910
Meta loss on this task batch = 2.1233e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 152.8514, GNorm = 0.2738
Meta loss on this task batch = 2.8994e-01, Meta loss averaged over last 500 steps = 2.6291e-01, PNorm = 152.8564, GNorm = 0.3057
Meta loss on this task batch = 2.4271e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 152.8621, GNorm = 0.3143
Meta loss on this task batch = 2.7424e-01, Meta loss averaged over last 500 steps = 2.6294e-01, PNorm = 152.8678, GNorm = 0.2834
Meta loss on this task batch = 2.3902e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 152.8725, GNorm = 0.2961
Meta loss on this task batch = 2.2537e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 152.8768, GNorm = 0.2629
Meta loss on this task batch = 3.0844e-01, Meta loss averaged over last 500 steps = 2.6301e-01, PNorm = 152.8793, GNorm = 0.3272
Meta loss on this task batch = 2.4358e-01, Meta loss averaged over last 500 steps = 2.6302e-01, PNorm = 152.8810, GNorm = 0.2658
Meta loss on this task batch = 2.4525e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 152.8830, GNorm = 0.3322
Took 131.04210305213928 seconds to complete one epoch of meta training
Took 139.6139097213745 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501353
Epoch 906
Meta loss on this task batch = 2.4546e-01, Meta loss averaged over last 500 steps = 2.6291e-01, PNorm = 152.8849, GNorm = 0.2855
Meta loss on this task batch = 2.9124e-01, Meta loss averaged over last 500 steps = 2.6310e-01, PNorm = 152.8872, GNorm = 0.2944
Meta loss on this task batch = 2.2656e-01, Meta loss averaged over last 500 steps = 2.6297e-01, PNorm = 152.8914, GNorm = 0.2721
Meta loss on this task batch = 2.5805e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 152.8989, GNorm = 0.4255
Meta loss on this task batch = 2.7977e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 152.9078, GNorm = 0.2455
Meta loss on this task batch = 2.5154e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 152.9170, GNorm = 0.2520
Meta loss on this task batch = 2.8124e-01, Meta loss averaged over last 500 steps = 2.6302e-01, PNorm = 152.9247, GNorm = 0.3017
Meta loss on this task batch = 2.4873e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 152.9299, GNorm = 0.3665
Meta loss on this task batch = 2.7869e-01, Meta loss averaged over last 500 steps = 2.6301e-01, PNorm = 152.9331, GNorm = 0.3043
Meta loss on this task batch = 2.7603e-01, Meta loss averaged over last 500 steps = 2.6301e-01, PNorm = 152.9356, GNorm = 0.3609
Meta loss on this task batch = 2.5330e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 152.9386, GNorm = 0.2920
Meta loss on this task batch = 2.7750e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 152.9420, GNorm = 0.2683
Meta loss on this task batch = 2.1681e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 152.9462, GNorm = 0.2772
Meta loss on this task batch = 2.8785e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 152.9494, GNorm = 0.3194
Meta loss on this task batch = 2.4977e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 152.9516, GNorm = 0.2732
Meta loss on this task batch = 2.6222e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 152.9546, GNorm = 0.2699
Meta loss on this task batch = 3.1675e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 152.9573, GNorm = 0.2951
Meta loss on this task batch = 2.8518e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 152.9588, GNorm = 0.2742
Meta loss on this task batch = 2.5874e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 152.9579, GNorm = 0.4211
Took 133.34005427360535 seconds to complete one epoch of meta training
Took 142.00174260139465 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470906
Epoch 907
Meta loss on this task batch = 2.7493e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 152.9555, GNorm = 0.2858
Meta loss on this task batch = 2.4919e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 152.9536, GNorm = 0.2709
Meta loss on this task batch = 2.9674e-01, Meta loss averaged over last 500 steps = 2.6305e-01, PNorm = 152.9520, GNorm = 0.2859
Meta loss on this task batch = 2.9378e-01, Meta loss averaged over last 500 steps = 2.6317e-01, PNorm = 152.9520, GNorm = 0.4043
Meta loss on this task batch = 2.2021e-01, Meta loss averaged over last 500 steps = 2.6296e-01, PNorm = 152.9550, GNorm = 0.2763
Meta loss on this task batch = 2.2916e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 152.9586, GNorm = 0.2586
Meta loss on this task batch = 2.5814e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 152.9627, GNorm = 0.2599
Meta loss on this task batch = 3.0413e-01, Meta loss averaged over last 500 steps = 2.6297e-01, PNorm = 152.9671, GNorm = 0.3398
Meta loss on this task batch = 2.6785e-01, Meta loss averaged over last 500 steps = 2.6298e-01, PNorm = 152.9731, GNorm = 0.2632
Meta loss on this task batch = 2.4394e-01, Meta loss averaged over last 500 steps = 2.6295e-01, PNorm = 152.9807, GNorm = 0.3054
Meta loss on this task batch = 2.0660e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 152.9879, GNorm = 0.2534
Meta loss on this task batch = 2.4245e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 152.9956, GNorm = 0.3125
Meta loss on this task batch = 2.1574e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 153.0020, GNorm = 0.2800
Meta loss on this task batch = 2.7242e-01, Meta loss averaged over last 500 steps = 2.6259e-01, PNorm = 153.0081, GNorm = 0.3031
Meta loss on this task batch = 3.1342e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 153.0143, GNorm = 0.3040
Meta loss on this task batch = 2.7107e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 153.0211, GNorm = 0.2917
Meta loss on this task batch = 2.9591e-01, Meta loss averaged over last 500 steps = 2.6297e-01, PNorm = 153.0261, GNorm = 0.3569
Meta loss on this task batch = 2.5075e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 153.0311, GNorm = 0.2890
Meta loss on this task batch = 2.9376e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 153.0344, GNorm = 0.3677
Took 135.6178412437439 seconds to complete one epoch of meta training
Took 143.84184503555298 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502019
Epoch 908
Meta loss on this task batch = 2.5922e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 153.0365, GNorm = 0.2971
Meta loss on this task batch = 2.3605e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 153.0393, GNorm = 0.2978
Meta loss on this task batch = 2.5417e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 153.0410, GNorm = 0.2796
Meta loss on this task batch = 2.0390e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 153.0425, GNorm = 0.2577
Meta loss on this task batch = 2.3036e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 153.0448, GNorm = 0.2477
Meta loss on this task batch = 2.2001e-01, Meta loss averaged over last 500 steps = 2.6259e-01, PNorm = 153.0476, GNorm = 0.2386
Meta loss on this task batch = 2.6795e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 153.0505, GNorm = 0.2963
Meta loss on this task batch = 2.7511e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 153.0523, GNorm = 0.2852
Meta loss on this task batch = 2.6308e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 153.0551, GNorm = 0.2810
Meta loss on this task batch = 2.9811e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 153.0571, GNorm = 0.2864
Meta loss on this task batch = 2.7051e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 153.0589, GNorm = 0.2963
Meta loss on this task batch = 2.8124e-01, Meta loss averaged over last 500 steps = 2.6301e-01, PNorm = 153.0590, GNorm = 0.3443
Meta loss on this task batch = 2.9971e-01, Meta loss averaged over last 500 steps = 2.6305e-01, PNorm = 153.0582, GNorm = 0.3578
Meta loss on this task batch = 2.9700e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 153.0607, GNorm = 0.3794
Meta loss on this task batch = 2.4505e-01, Meta loss averaged over last 500 steps = 2.6298e-01, PNorm = 153.0661, GNorm = 0.3124
Meta loss on this task batch = 2.5584e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 153.0739, GNorm = 0.3188
Meta loss on this task batch = 3.4101e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 153.0822, GNorm = 0.3059
Meta loss on this task batch = 3.1932e-01, Meta loss averaged over last 500 steps = 2.6329e-01, PNorm = 153.0891, GNorm = 0.3720
Meta loss on this task batch = 2.3282e-01, Meta loss averaged over last 500 steps = 2.6324e-01, PNorm = 153.0950, GNorm = 0.2962
Took 128.93155145645142 seconds to complete one epoch of meta training
Took 137.31403350830078 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500358
Epoch 909
Meta loss on this task batch = 2.5192e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 153.1032, GNorm = 0.2961
Meta loss on this task batch = 2.9463e-01, Meta loss averaged over last 500 steps = 2.6320e-01, PNorm = 153.1107, GNorm = 0.3474
Meta loss on this task batch = 3.2491e-01, Meta loss averaged over last 500 steps = 2.6328e-01, PNorm = 153.1180, GNorm = 0.2822
Meta loss on this task batch = 2.7899e-01, Meta loss averaged over last 500 steps = 2.6337e-01, PNorm = 153.1260, GNorm = 0.3048
Meta loss on this task batch = 2.8983e-01, Meta loss averaged over last 500 steps = 2.6345e-01, PNorm = 153.1324, GNorm = 0.3809
Meta loss on this task batch = 2.5373e-01, Meta loss averaged over last 500 steps = 2.6335e-01, PNorm = 153.1390, GNorm = 0.2925
Meta loss on this task batch = 2.4157e-01, Meta loss averaged over last 500 steps = 2.6324e-01, PNorm = 153.1451, GNorm = 0.3103
Meta loss on this task batch = 2.4032e-01, Meta loss averaged over last 500 steps = 2.6311e-01, PNorm = 153.1506, GNorm = 0.2942
Meta loss on this task batch = 2.8711e-01, Meta loss averaged over last 500 steps = 2.6320e-01, PNorm = 153.1566, GNorm = 0.2921
Meta loss on this task batch = 2.2166e-01, Meta loss averaged over last 500 steps = 2.6319e-01, PNorm = 153.1619, GNorm = 0.2656
Meta loss on this task batch = 2.3868e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 153.1669, GNorm = 0.3065
Meta loss on this task batch = 2.7602e-01, Meta loss averaged over last 500 steps = 2.6317e-01, PNorm = 153.1704, GNorm = 0.2731
Meta loss on this task batch = 2.0125e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 153.1717, GNorm = 0.2632
Meta loss on this task batch = 3.4641e-01, Meta loss averaged over last 500 steps = 2.6336e-01, PNorm = 153.1689, GNorm = 0.3776
Meta loss on this task batch = 2.9754e-01, Meta loss averaged over last 500 steps = 2.6339e-01, PNorm = 153.1658, GNorm = 0.2833
Meta loss on this task batch = 2.2395e-01, Meta loss averaged over last 500 steps = 2.6324e-01, PNorm = 153.1631, GNorm = 0.2595
Meta loss on this task batch = 2.1523e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 153.1608, GNorm = 0.2612
Meta loss on this task batch = 2.5915e-01, Meta loss averaged over last 500 steps = 2.6309e-01, PNorm = 153.1602, GNorm = 0.2908
Meta loss on this task batch = 2.5501e-01, Meta loss averaged over last 500 steps = 2.6307e-01, PNorm = 153.1624, GNorm = 0.2934
Took 134.7936806678772 seconds to complete one epoch of meta training
Took 142.16927313804626 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477661
Epoch 910
Meta loss on this task batch = 2.6818e-01, Meta loss averaged over last 500 steps = 2.6310e-01, PNorm = 153.1654, GNorm = 0.2752
Meta loss on this task batch = 2.9550e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 153.1699, GNorm = 0.2982
Meta loss on this task batch = 2.6538e-01, Meta loss averaged over last 500 steps = 2.6325e-01, PNorm = 153.1759, GNorm = 0.2891
Meta loss on this task batch = 2.5804e-01, Meta loss averaged over last 500 steps = 2.6325e-01, PNorm = 153.1822, GNorm = 0.3071
Meta loss on this task batch = 2.2593e-01, Meta loss averaged over last 500 steps = 2.6318e-01, PNorm = 153.1890, GNorm = 0.2647
Meta loss on this task batch = 2.7563e-01, Meta loss averaged over last 500 steps = 2.6312e-01, PNorm = 153.1947, GNorm = 0.2985
Meta loss on this task batch = 2.7336e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 153.1979, GNorm = 0.3236
Meta loss on this task batch = 2.5619e-01, Meta loss averaged over last 500 steps = 2.6312e-01, PNorm = 153.2016, GNorm = 0.2819
Meta loss on this task batch = 2.8865e-01, Meta loss averaged over last 500 steps = 2.6319e-01, PNorm = 153.2048, GNorm = 0.3192
Meta loss on this task batch = 2.7215e-01, Meta loss averaged over last 500 steps = 2.6316e-01, PNorm = 153.2070, GNorm = 0.2976
Meta loss on this task batch = 2.5627e-01, Meta loss averaged over last 500 steps = 2.6318e-01, PNorm = 153.2093, GNorm = 0.3129
Meta loss on this task batch = 2.0703e-01, Meta loss averaged over last 500 steps = 2.6312e-01, PNorm = 153.2117, GNorm = 0.2465
Meta loss on this task batch = 2.7510e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 153.2160, GNorm = 0.2609
Meta loss on this task batch = 2.5635e-01, Meta loss averaged over last 500 steps = 2.6325e-01, PNorm = 153.2201, GNorm = 0.2726
Meta loss on this task batch = 3.1647e-01, Meta loss averaged over last 500 steps = 2.6333e-01, PNorm = 153.2227, GNorm = 0.3395
Meta loss on this task batch = 2.4549e-01, Meta loss averaged over last 500 steps = 2.6322e-01, PNorm = 153.2257, GNorm = 0.2686
Meta loss on this task batch = 2.2909e-01, Meta loss averaged over last 500 steps = 2.6316e-01, PNorm = 153.2289, GNorm = 0.2465
Meta loss on this task batch = 2.6035e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 153.2319, GNorm = 0.3163
Meta loss on this task batch = 3.0093e-01, Meta loss averaged over last 500 steps = 2.6317e-01, PNorm = 153.2338, GNorm = 0.3930
Took 130.28657746315002 seconds to complete one epoch of meta training
Took 138.90962433815002 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497531
Epoch 911
Meta loss on this task batch = 2.7867e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 153.2370, GNorm = 0.3240
Meta loss on this task batch = 2.9265e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 153.2418, GNorm = 0.2968
Meta loss on this task batch = 2.9939e-01, Meta loss averaged over last 500 steps = 2.6337e-01, PNorm = 153.2459, GNorm = 0.3334
Meta loss on this task batch = 2.2841e-01, Meta loss averaged over last 500 steps = 2.6342e-01, PNorm = 153.2504, GNorm = 0.2651
Meta loss on this task batch = 2.4000e-01, Meta loss averaged over last 500 steps = 2.6338e-01, PNorm = 153.2562, GNorm = 0.3114
Meta loss on this task batch = 2.6439e-01, Meta loss averaged over last 500 steps = 2.6344e-01, PNorm = 153.2619, GNorm = 0.2971
Meta loss on this task batch = 2.7690e-01, Meta loss averaged over last 500 steps = 2.6329e-01, PNorm = 153.2651, GNorm = 0.3234
Meta loss on this task batch = 2.3461e-01, Meta loss averaged over last 500 steps = 2.6327e-01, PNorm = 153.2679, GNorm = 0.2948
Meta loss on this task batch = 2.7747e-01, Meta loss averaged over last 500 steps = 2.6343e-01, PNorm = 153.2717, GNorm = 0.2858
Meta loss on this task batch = 2.7419e-01, Meta loss averaged over last 500 steps = 2.6341e-01, PNorm = 153.2749, GNorm = 0.3212
Meta loss on this task batch = 2.8020e-01, Meta loss averaged over last 500 steps = 2.6347e-01, PNorm = 153.2765, GNorm = 0.3049
Meta loss on this task batch = 2.8233e-01, Meta loss averaged over last 500 steps = 2.6360e-01, PNorm = 153.2793, GNorm = 0.3083
Meta loss on this task batch = 2.8029e-01, Meta loss averaged over last 500 steps = 2.6362e-01, PNorm = 153.2811, GNorm = 0.2947
Meta loss on this task batch = 2.7589e-01, Meta loss averaged over last 500 steps = 2.6366e-01, PNorm = 153.2848, GNorm = 0.3331
Meta loss on this task batch = 2.5068e-01, Meta loss averaged over last 500 steps = 2.6361e-01, PNorm = 153.2897, GNorm = 0.3176
Meta loss on this task batch = 3.2161e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 153.2955, GNorm = 0.3246
Meta loss on this task batch = 2.3063e-01, Meta loss averaged over last 500 steps = 2.6378e-01, PNorm = 153.3021, GNorm = 0.2498
Meta loss on this task batch = 2.2404e-01, Meta loss averaged over last 500 steps = 2.6374e-01, PNorm = 153.3096, GNorm = 0.2391
Meta loss on this task batch = 2.5658e-01, Meta loss averaged over last 500 steps = 2.6379e-01, PNorm = 153.3173, GNorm = 0.3337
Took 131.8581726551056 seconds to complete one epoch of meta training
Took 140.55458402633667 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504181
Epoch 912
Meta loss on this task batch = 2.7394e-01, Meta loss averaged over last 500 steps = 2.6367e-01, PNorm = 153.3240, GNorm = 0.2713
Meta loss on this task batch = 2.5837e-01, Meta loss averaged over last 500 steps = 2.6361e-01, PNorm = 153.3305, GNorm = 0.2886
Meta loss on this task batch = 2.9522e-01, Meta loss averaged over last 500 steps = 2.6363e-01, PNorm = 153.3360, GNorm = 0.3644
Meta loss on this task batch = 3.0071e-01, Meta loss averaged over last 500 steps = 2.6362e-01, PNorm = 153.3392, GNorm = 0.3459
Meta loss on this task batch = 2.8936e-01, Meta loss averaged over last 500 steps = 2.6367e-01, PNorm = 153.3409, GNorm = 0.2706
Meta loss on this task batch = 2.9947e-01, Meta loss averaged over last 500 steps = 2.6383e-01, PNorm = 153.3421, GNorm = 0.3171
Meta loss on this task batch = 2.3807e-01, Meta loss averaged over last 500 steps = 2.6380e-01, PNorm = 153.3433, GNorm = 0.2493
Meta loss on this task batch = 2.6419e-01, Meta loss averaged over last 500 steps = 2.6385e-01, PNorm = 153.3465, GNorm = 0.2446
Meta loss on this task batch = 2.5585e-01, Meta loss averaged over last 500 steps = 2.6382e-01, PNorm = 153.3494, GNorm = 0.2876
Meta loss on this task batch = 2.9988e-01, Meta loss averaged over last 500 steps = 2.6387e-01, PNorm = 153.3530, GNorm = 0.2946
Meta loss on this task batch = 2.8842e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 153.3555, GNorm = 0.2951
Meta loss on this task batch = 2.5913e-01, Meta loss averaged over last 500 steps = 2.6390e-01, PNorm = 153.3581, GNorm = 0.2605
Meta loss on this task batch = 2.7686e-01, Meta loss averaged over last 500 steps = 2.6396e-01, PNorm = 153.3613, GNorm = 0.2767
Meta loss on this task batch = 3.1296e-01, Meta loss averaged over last 500 steps = 2.6407e-01, PNorm = 153.3633, GNorm = 0.3337
Meta loss on this task batch = 2.9030e-01, Meta loss averaged over last 500 steps = 2.6418e-01, PNorm = 153.3652, GNorm = 0.3155
Meta loss on this task batch = 2.4002e-01, Meta loss averaged over last 500 steps = 2.6428e-01, PNorm = 153.3691, GNorm = 0.2853
Meta loss on this task batch = 2.3256e-01, Meta loss averaged over last 500 steps = 2.6425e-01, PNorm = 153.3743, GNorm = 0.2591
Meta loss on this task batch = 2.3046e-01, Meta loss averaged over last 500 steps = 2.6422e-01, PNorm = 153.3802, GNorm = 0.2964
Meta loss on this task batch = 2.4689e-01, Meta loss averaged over last 500 steps = 2.6411e-01, PNorm = 153.3857, GNorm = 0.2709
Took 133.09880113601685 seconds to complete one epoch of meta training
Took 141.49864196777344 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500241
Epoch 913
Meta loss on this task batch = 2.4445e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 153.3918, GNorm = 0.2519
Meta loss on this task batch = 2.3543e-01, Meta loss averaged over last 500 steps = 2.6400e-01, PNorm = 153.3988, GNorm = 0.3203
Meta loss on this task batch = 2.4590e-01, Meta loss averaged over last 500 steps = 2.6387e-01, PNorm = 153.4056, GNorm = 0.2756
Meta loss on this task batch = 2.4144e-01, Meta loss averaged over last 500 steps = 2.6385e-01, PNorm = 153.4115, GNorm = 0.2776
Meta loss on this task batch = 2.1836e-01, Meta loss averaged over last 500 steps = 2.6366e-01, PNorm = 153.4174, GNorm = 0.2988
Meta loss on this task batch = 2.2152e-01, Meta loss averaged over last 500 steps = 2.6354e-01, PNorm = 153.4240, GNorm = 0.2611
Meta loss on this task batch = 2.4797e-01, Meta loss averaged over last 500 steps = 2.6351e-01, PNorm = 153.4305, GNorm = 0.2655
Meta loss on this task batch = 3.2243e-01, Meta loss averaged over last 500 steps = 2.6373e-01, PNorm = 153.4358, GNorm = 0.3380
Meta loss on this task batch = 2.8439e-01, Meta loss averaged over last 500 steps = 2.6372e-01, PNorm = 153.4395, GNorm = 0.3572
Meta loss on this task batch = 2.6086e-01, Meta loss averaged over last 500 steps = 2.6382e-01, PNorm = 153.4419, GNorm = 0.2787
Meta loss on this task batch = 2.7026e-01, Meta loss averaged over last 500 steps = 2.6398e-01, PNorm = 153.4438, GNorm = 0.3000
Meta loss on this task batch = 2.8174e-01, Meta loss averaged over last 500 steps = 2.6407e-01, PNorm = 153.4445, GNorm = 0.2998
Meta loss on this task batch = 3.0922e-01, Meta loss averaged over last 500 steps = 2.6411e-01, PNorm = 153.4435, GNorm = 0.3673
Meta loss on this task batch = 2.9004e-01, Meta loss averaged over last 500 steps = 2.6421e-01, PNorm = 153.4421, GNorm = 0.3074
Meta loss on this task batch = 2.4864e-01, Meta loss averaged over last 500 steps = 2.6414e-01, PNorm = 153.4398, GNorm = 0.2862
Meta loss on this task batch = 3.0568e-01, Meta loss averaged over last 500 steps = 2.6418e-01, PNorm = 153.4378, GNorm = 0.3272
Meta loss on this task batch = 2.3416e-01, Meta loss averaged over last 500 steps = 2.6404e-01, PNorm = 153.4381, GNorm = 0.2936
Meta loss on this task batch = 2.5743e-01, Meta loss averaged over last 500 steps = 2.6398e-01, PNorm = 153.4398, GNorm = 0.2661
Meta loss on this task batch = 2.7848e-01, Meta loss averaged over last 500 steps = 2.6404e-01, PNorm = 153.4408, GNorm = 0.4062
Took 132.15036177635193 seconds to complete one epoch of meta training
Took 140.8240602016449 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517123
Epoch 914
Meta loss on this task batch = 2.9410e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 153.4421, GNorm = 0.2719
Meta loss on this task batch = 2.5841e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 153.4445, GNorm = 0.2685
Meta loss on this task batch = 3.0584e-01, Meta loss averaged over last 500 steps = 2.6414e-01, PNorm = 153.4465, GNorm = 0.2872
Meta loss on this task batch = 2.4942e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 153.4499, GNorm = 0.2937
Meta loss on this task batch = 2.5367e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 153.4525, GNorm = 0.3027
Meta loss on this task batch = 2.2568e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 153.4549, GNorm = 0.2352
Meta loss on this task batch = 2.1196e-01, Meta loss averaged over last 500 steps = 2.6410e-01, PNorm = 153.4582, GNorm = 0.2054
Meta loss on this task batch = 2.6177e-01, Meta loss averaged over last 500 steps = 2.6407e-01, PNorm = 153.4610, GNorm = 0.3522
Meta loss on this task batch = 2.6018e-01, Meta loss averaged over last 500 steps = 2.6404e-01, PNorm = 153.4645, GNorm = 0.2569
Meta loss on this task batch = 2.1635e-01, Meta loss averaged over last 500 steps = 2.6399e-01, PNorm = 153.4671, GNorm = 0.2560
Meta loss on this task batch = 2.3707e-01, Meta loss averaged over last 500 steps = 2.6387e-01, PNorm = 153.4703, GNorm = 0.2672
Meta loss on this task batch = 3.1078e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 153.4731, GNorm = 0.2911
Meta loss on this task batch = 2.5827e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 153.4765, GNorm = 0.2545
Meta loss on this task batch = 2.5584e-01, Meta loss averaged over last 500 steps = 2.6394e-01, PNorm = 153.4799, GNorm = 0.2840
Meta loss on this task batch = 2.8724e-01, Meta loss averaged over last 500 steps = 2.6409e-01, PNorm = 153.4836, GNorm = 0.3150
Meta loss on this task batch = 2.6703e-01, Meta loss averaged over last 500 steps = 2.6407e-01, PNorm = 153.4866, GNorm = 0.2622
Meta loss on this task batch = 3.0545e-01, Meta loss averaged over last 500 steps = 2.6416e-01, PNorm = 153.4909, GNorm = 0.2898
Meta loss on this task batch = 2.5140e-01, Meta loss averaged over last 500 steps = 2.6405e-01, PNorm = 153.4953, GNorm = 0.2609
Meta loss on this task batch = 2.8943e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 153.4986, GNorm = 0.3804
Took 134.32036972045898 seconds to complete one epoch of meta training
Took 142.4628620147705 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503030
Epoch 915
Meta loss on this task batch = 2.4671e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 153.5008, GNorm = 0.3008
Meta loss on this task batch = 2.2410e-01, Meta loss averaged over last 500 steps = 2.6399e-01, PNorm = 153.5041, GNorm = 0.2874
Meta loss on this task batch = 2.7347e-01, Meta loss averaged over last 500 steps = 2.6402e-01, PNorm = 153.5070, GNorm = 0.3701
Meta loss on this task batch = 2.3035e-01, Meta loss averaged over last 500 steps = 2.6390e-01, PNorm = 153.5120, GNorm = 0.2562
Meta loss on this task batch = 2.7179e-01, Meta loss averaged over last 500 steps = 2.6380e-01, PNorm = 153.5176, GNorm = 0.2591
Meta loss on this task batch = 2.3847e-01, Meta loss averaged over last 500 steps = 2.6375e-01, PNorm = 153.5246, GNorm = 0.2861
Meta loss on this task batch = 2.8441e-01, Meta loss averaged over last 500 steps = 2.6383e-01, PNorm = 153.5314, GNorm = 0.3172
Meta loss on this task batch = 2.6852e-01, Meta loss averaged over last 500 steps = 2.6393e-01, PNorm = 153.5402, GNorm = 0.3298
Meta loss on this task batch = 2.2716e-01, Meta loss averaged over last 500 steps = 2.6386e-01, PNorm = 153.5500, GNorm = 0.2813
Meta loss on this task batch = 2.5213e-01, Meta loss averaged over last 500 steps = 2.6373e-01, PNorm = 153.5586, GNorm = 0.2998
Meta loss on this task batch = 2.4226e-01, Meta loss averaged over last 500 steps = 2.6367e-01, PNorm = 153.5684, GNorm = 0.2848
Meta loss on this task batch = 2.5595e-01, Meta loss averaged over last 500 steps = 2.6364e-01, PNorm = 153.5770, GNorm = 0.2747
Meta loss on this task batch = 2.2684e-01, Meta loss averaged over last 500 steps = 2.6358e-01, PNorm = 153.5837, GNorm = 0.2533
Meta loss on this task batch = 2.5233e-01, Meta loss averaged over last 500 steps = 2.6363e-01, PNorm = 153.5885, GNorm = 0.2963
Meta loss on this task batch = 3.5704e-01, Meta loss averaged over last 500 steps = 2.6382e-01, PNorm = 153.5922, GNorm = 0.3642
Meta loss on this task batch = 2.9363e-01, Meta loss averaged over last 500 steps = 2.6392e-01, PNorm = 153.5941, GNorm = 0.3095
Meta loss on this task batch = 2.4546e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 153.5964, GNorm = 0.3200
Meta loss on this task batch = 2.9508e-01, Meta loss averaged over last 500 steps = 2.6389e-01, PNorm = 153.5967, GNorm = 0.3922
Meta loss on this task batch = 3.1857e-01, Meta loss averaged over last 500 steps = 2.6400e-01, PNorm = 153.5970, GNorm = 0.5154
Took 132.51823925971985 seconds to complete one epoch of meta training
Took 140.83299112319946 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484084
Epoch 916
Meta loss on this task batch = 2.7839e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 153.5976, GNorm = 0.2501
Meta loss on this task batch = 2.0865e-01, Meta loss averaged over last 500 steps = 2.6396e-01, PNorm = 153.5979, GNorm = 0.2457
Meta loss on this task batch = 2.7436e-01, Meta loss averaged over last 500 steps = 2.6396e-01, PNorm = 153.5995, GNorm = 0.2771
Meta loss on this task batch = 2.2381e-01, Meta loss averaged over last 500 steps = 2.6388e-01, PNorm = 153.6017, GNorm = 0.2519
Meta loss on this task batch = 2.6371e-01, Meta loss averaged over last 500 steps = 2.6386e-01, PNorm = 153.6038, GNorm = 0.2520
Meta loss on this task batch = 2.2705e-01, Meta loss averaged over last 500 steps = 2.6381e-01, PNorm = 153.6065, GNorm = 0.2840
Meta loss on this task batch = 2.7927e-01, Meta loss averaged over last 500 steps = 2.6386e-01, PNorm = 153.6094, GNorm = 0.3312
Meta loss on this task batch = 3.0880e-01, Meta loss averaged over last 500 steps = 2.6397e-01, PNorm = 153.6127, GNorm = 0.2965
Meta loss on this task batch = 2.4004e-01, Meta loss averaged over last 500 steps = 2.6396e-01, PNorm = 153.6165, GNorm = 0.2853
Meta loss on this task batch = 2.9893e-01, Meta loss averaged over last 500 steps = 2.6405e-01, PNorm = 153.6200, GNorm = 0.2700
Meta loss on this task batch = 2.9288e-01, Meta loss averaged over last 500 steps = 2.6408e-01, PNorm = 153.6239, GNorm = 0.3567
Meta loss on this task batch = 2.7759e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 153.6273, GNorm = 0.3061
Meta loss on this task batch = 2.2669e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 153.6304, GNorm = 0.2676
Meta loss on this task batch = 2.2232e-01, Meta loss averaged over last 500 steps = 2.6406e-01, PNorm = 153.6326, GNorm = 0.2665
Meta loss on this task batch = 2.7130e-01, Meta loss averaged over last 500 steps = 2.6407e-01, PNorm = 153.6351, GNorm = 0.2956
Meta loss on this task batch = 2.8277e-01, Meta loss averaged over last 500 steps = 2.6414e-01, PNorm = 153.6366, GNorm = 0.3541
Meta loss on this task batch = 2.5534e-01, Meta loss averaged over last 500 steps = 2.6409e-01, PNorm = 153.6377, GNorm = 0.3053
Meta loss on this task batch = 2.9037e-01, Meta loss averaged over last 500 steps = 2.6410e-01, PNorm = 153.6387, GNorm = 0.2641
Meta loss on this task batch = 2.3714e-01, Meta loss averaged over last 500 steps = 2.6392e-01, PNorm = 153.6389, GNorm = 0.3200
Took 132.94132447242737 seconds to complete one epoch of meta training
Took 141.21051502227783 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508223
Epoch 917
Meta loss on this task batch = 2.7213e-01, Meta loss averaged over last 500 steps = 2.6396e-01, PNorm = 153.6375, GNorm = 0.2956
Meta loss on this task batch = 2.8716e-01, Meta loss averaged over last 500 steps = 2.6389e-01, PNorm = 153.6349, GNorm = 0.2695
Meta loss on this task batch = 2.5913e-01, Meta loss averaged over last 500 steps = 2.6382e-01, PNorm = 153.6315, GNorm = 0.2918
Meta loss on this task batch = 2.5725e-01, Meta loss averaged over last 500 steps = 2.6383e-01, PNorm = 153.6279, GNorm = 0.2740
Meta loss on this task batch = 2.2458e-01, Meta loss averaged over last 500 steps = 2.6377e-01, PNorm = 153.6270, GNorm = 0.2537
Meta loss on this task batch = 2.7781e-01, Meta loss averaged over last 500 steps = 2.6368e-01, PNorm = 153.6271, GNorm = 0.2770
Meta loss on this task batch = 2.8712e-01, Meta loss averaged over last 500 steps = 2.6377e-01, PNorm = 153.6292, GNorm = 0.3206
Meta loss on this task batch = 2.2345e-01, Meta loss averaged over last 500 steps = 2.6365e-01, PNorm = 153.6330, GNorm = 0.3188
Meta loss on this task batch = 2.7546e-01, Meta loss averaged over last 500 steps = 2.6377e-01, PNorm = 153.6381, GNorm = 0.2852
Meta loss on this task batch = 2.0569e-01, Meta loss averaged over last 500 steps = 2.6365e-01, PNorm = 153.6435, GNorm = 0.2499
Meta loss on this task batch = 3.0715e-01, Meta loss averaged over last 500 steps = 2.6371e-01, PNorm = 153.6473, GNorm = 0.3758
Meta loss on this task batch = 2.6204e-01, Meta loss averaged over last 500 steps = 2.6381e-01, PNorm = 153.6528, GNorm = 0.3127
Meta loss on this task batch = 2.4784e-01, Meta loss averaged over last 500 steps = 2.6386e-01, PNorm = 153.6595, GNorm = 0.2886
Meta loss on this task batch = 2.7783e-01, Meta loss averaged over last 500 steps = 2.6388e-01, PNorm = 153.6674, GNorm = 0.2831
Meta loss on this task batch = 2.7933e-01, Meta loss averaged over last 500 steps = 2.6393e-01, PNorm = 153.6698, GNorm = 0.4602
Meta loss on this task batch = 2.6415e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 153.6728, GNorm = 0.2953
Meta loss on this task batch = 2.2894e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 153.6760, GNorm = 0.2538
Meta loss on this task batch = 2.4995e-01, Meta loss averaged over last 500 steps = 2.6392e-01, PNorm = 153.6788, GNorm = 0.3166
Meta loss on this task batch = 2.4263e-01, Meta loss averaged over last 500 steps = 2.6386e-01, PNorm = 153.6823, GNorm = 0.3233
Took 131.33579850196838 seconds to complete one epoch of meta training
Took 139.67463374137878 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508388
Epoch 918
Meta loss on this task batch = 2.6511e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 153.6858, GNorm = 0.2971
Meta loss on this task batch = 2.9900e-01, Meta loss averaged over last 500 steps = 2.6374e-01, PNorm = 153.6876, GNorm = 0.3631
Meta loss on this task batch = 2.3700e-01, Meta loss averaged over last 500 steps = 2.6370e-01, PNorm = 153.6906, GNorm = 0.3936
Meta loss on this task batch = 2.8284e-01, Meta loss averaged over last 500 steps = 2.6370e-01, PNorm = 153.6940, GNorm = 0.3089
Meta loss on this task batch = 2.5104e-01, Meta loss averaged over last 500 steps = 2.6368e-01, PNorm = 153.6981, GNorm = 0.2680
Meta loss on this task batch = 2.9817e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 153.7023, GNorm = 0.2777
Meta loss on this task batch = 2.5407e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 153.7066, GNorm = 0.2499
Meta loss on this task batch = 2.5148e-01, Meta loss averaged over last 500 steps = 2.6380e-01, PNorm = 153.7097, GNorm = 0.2678
Meta loss on this task batch = 2.2228e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 153.7129, GNorm = 0.2610
Meta loss on this task batch = 2.8278e-01, Meta loss averaged over last 500 steps = 2.6390e-01, PNorm = 153.7155, GNorm = 0.2991
Meta loss on this task batch = 2.4633e-01, Meta loss averaged over last 500 steps = 2.6372e-01, PNorm = 153.7186, GNorm = 0.2671
Meta loss on this task batch = 2.4030e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 153.7226, GNorm = 0.2505
Meta loss on this task batch = 2.6680e-01, Meta loss averaged over last 500 steps = 2.6381e-01, PNorm = 153.7272, GNorm = 0.3000
Meta loss on this task batch = 2.2721e-01, Meta loss averaged over last 500 steps = 2.6389e-01, PNorm = 153.7327, GNorm = 0.2809
Meta loss on this task batch = 2.5034e-01, Meta loss averaged over last 500 steps = 2.6393e-01, PNorm = 153.7380, GNorm = 0.2846
Meta loss on this task batch = 2.8158e-01, Meta loss averaged over last 500 steps = 2.6392e-01, PNorm = 153.7420, GNorm = 0.3299
Meta loss on this task batch = 2.5789e-01, Meta loss averaged over last 500 steps = 2.6389e-01, PNorm = 153.7447, GNorm = 0.3022
Meta loss on this task batch = 2.7546e-01, Meta loss averaged over last 500 steps = 2.6388e-01, PNorm = 153.7472, GNorm = 0.3294
Meta loss on this task batch = 3.1167e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 153.7493, GNorm = 0.4392
Took 132.37976217269897 seconds to complete one epoch of meta training
Took 140.5467598438263 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470617
Epoch 919
Meta loss on this task batch = 2.9132e-01, Meta loss averaged over last 500 steps = 2.6409e-01, PNorm = 153.7521, GNorm = 0.3248
Meta loss on this task batch = 2.6179e-01, Meta loss averaged over last 500 steps = 2.6412e-01, PNorm = 153.7539, GNorm = 0.2610
Meta loss on this task batch = 2.3053e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 153.7557, GNorm = 0.3026
Meta loss on this task batch = 2.3187e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 153.7580, GNorm = 0.3008
Meta loss on this task batch = 2.4682e-01, Meta loss averaged over last 500 steps = 2.6390e-01, PNorm = 153.7607, GNorm = 0.2811
Meta loss on this task batch = 2.8150e-01, Meta loss averaged over last 500 steps = 2.6389e-01, PNorm = 153.7615, GNorm = 0.3163
Meta loss on this task batch = 1.9415e-01, Meta loss averaged over last 500 steps = 2.6368e-01, PNorm = 153.7647, GNorm = 0.2823
Meta loss on this task batch = 2.4707e-01, Meta loss averaged over last 500 steps = 2.6368e-01, PNorm = 153.7690, GNorm = 0.2722
Meta loss on this task batch = 2.6447e-01, Meta loss averaged over last 500 steps = 2.6371e-01, PNorm = 153.7741, GNorm = 0.2488
Meta loss on this task batch = 2.8129e-01, Meta loss averaged over last 500 steps = 2.6379e-01, PNorm = 153.7812, GNorm = 0.2671
Meta loss on this task batch = 2.0924e-01, Meta loss averaged over last 500 steps = 2.6371e-01, PNorm = 153.7880, GNorm = 0.2754
Meta loss on this task batch = 2.7215e-01, Meta loss averaged over last 500 steps = 2.6367e-01, PNorm = 153.7965, GNorm = 0.2555
Meta loss on this task batch = 3.2461e-01, Meta loss averaged over last 500 steps = 2.6375e-01, PNorm = 153.8029, GNorm = 0.3008
Meta loss on this task batch = 2.5879e-01, Meta loss averaged over last 500 steps = 2.6365e-01, PNorm = 153.8097, GNorm = 0.2989
Meta loss on this task batch = 2.3910e-01, Meta loss averaged over last 500 steps = 2.6374e-01, PNorm = 153.8168, GNorm = 0.3311
Meta loss on this task batch = 3.2048e-01, Meta loss averaged over last 500 steps = 2.6390e-01, PNorm = 153.8208, GNorm = 0.3350
Meta loss on this task batch = 2.9104e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 153.8227, GNorm = 0.3381
Meta loss on this task batch = 2.6997e-01, Meta loss averaged over last 500 steps = 2.6383e-01, PNorm = 153.8250, GNorm = 0.3091
Meta loss on this task batch = 2.1231e-01, Meta loss averaged over last 500 steps = 2.6369e-01, PNorm = 153.8275, GNorm = 0.2986
Took 132.15500020980835 seconds to complete one epoch of meta training
Took 140.2760350704193 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508270
Epoch 920
Meta loss on this task batch = 2.5816e-01, Meta loss averaged over last 500 steps = 2.6382e-01, PNorm = 153.8289, GNorm = 0.3045
Meta loss on this task batch = 2.5401e-01, Meta loss averaged over last 500 steps = 2.6375e-01, PNorm = 153.8305, GNorm = 0.2622
Meta loss on this task batch = 2.3760e-01, Meta loss averaged over last 500 steps = 2.6368e-01, PNorm = 153.8328, GNorm = 0.2661
Meta loss on this task batch = 2.2058e-01, Meta loss averaged over last 500 steps = 2.6367e-01, PNorm = 153.8338, GNorm = 0.3091
Meta loss on this task batch = 2.7936e-01, Meta loss averaged over last 500 steps = 2.6383e-01, PNorm = 153.8345, GNorm = 0.2785
Meta loss on this task batch = 2.2955e-01, Meta loss averaged over last 500 steps = 2.6374e-01, PNorm = 153.8347, GNorm = 0.2684
Meta loss on this task batch = 2.7487e-01, Meta loss averaged over last 500 steps = 2.6384e-01, PNorm = 153.8358, GNorm = 0.3268
Meta loss on this task batch = 2.1739e-01, Meta loss averaged over last 500 steps = 2.6379e-01, PNorm = 153.8376, GNorm = 0.2422
Meta loss on this task batch = 3.2439e-01, Meta loss averaged over last 500 steps = 2.6386e-01, PNorm = 153.8420, GNorm = 0.3119
Meta loss on this task batch = 2.4553e-01, Meta loss averaged over last 500 steps = 2.6384e-01, PNorm = 153.8459, GNorm = 0.2642
Meta loss on this task batch = 2.5451e-01, Meta loss averaged over last 500 steps = 2.6372e-01, PNorm = 153.8509, GNorm = 0.3010
Meta loss on this task batch = 2.3544e-01, Meta loss averaged over last 500 steps = 2.6364e-01, PNorm = 153.8559, GNorm = 0.2727
Meta loss on this task batch = 2.2844e-01, Meta loss averaged over last 500 steps = 2.6354e-01, PNorm = 153.8609, GNorm = 0.2777
Meta loss on this task batch = 2.5433e-01, Meta loss averaged over last 500 steps = 2.6354e-01, PNorm = 153.8661, GNorm = 0.2754
Meta loss on this task batch = 2.4768e-01, Meta loss averaged over last 500 steps = 2.6339e-01, PNorm = 153.8727, GNorm = 0.2796
Meta loss on this task batch = 2.8113e-01, Meta loss averaged over last 500 steps = 2.6342e-01, PNorm = 153.8779, GNorm = 0.2798
Meta loss on this task batch = 3.0066e-01, Meta loss averaged over last 500 steps = 2.6359e-01, PNorm = 153.8817, GNorm = 0.2888
Meta loss on this task batch = 3.1620e-01, Meta loss averaged over last 500 steps = 2.6372e-01, PNorm = 153.8844, GNorm = 0.3200
Meta loss on this task batch = 2.2820e-01, Meta loss averaged over last 500 steps = 2.6360e-01, PNorm = 153.8883, GNorm = 0.3307
Took 132.63455820083618 seconds to complete one epoch of meta training
Took 141.48331356048584 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517410
Epoch 921
Meta loss on this task batch = 2.6446e-01, Meta loss averaged over last 500 steps = 2.6345e-01, PNorm = 153.8929, GNorm = 0.2341
Meta loss on this task batch = 2.8666e-01, Meta loss averaged over last 500 steps = 2.6353e-01, PNorm = 153.8989, GNorm = 0.3133
Meta loss on this task batch = 2.2306e-01, Meta loss averaged over last 500 steps = 2.6344e-01, PNorm = 153.9050, GNorm = 0.2500
Meta loss on this task batch = 2.5357e-01, Meta loss averaged over last 500 steps = 2.6336e-01, PNorm = 153.9116, GNorm = 0.2524
Meta loss on this task batch = 2.4233e-01, Meta loss averaged over last 500 steps = 2.6332e-01, PNorm = 153.9186, GNorm = 0.2711
Meta loss on this task batch = 2.3135e-01, Meta loss averaged over last 500 steps = 2.6310e-01, PNorm = 153.9253, GNorm = 0.2570
Meta loss on this task batch = 2.4960e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 153.9334, GNorm = 0.3101
Meta loss on this task batch = 2.5384e-01, Meta loss averaged over last 500 steps = 2.6302e-01, PNorm = 153.9398, GNorm = 0.2750
Meta loss on this task batch = 1.8363e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 153.9465, GNorm = 0.2408
Meta loss on this task batch = 2.8061e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 153.9506, GNorm = 0.3899
Meta loss on this task batch = 2.5169e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 153.9547, GNorm = 0.3084
Meta loss on this task batch = 2.7753e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 153.9574, GNorm = 0.2737
Meta loss on this task batch = 2.6415e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 153.9601, GNorm = 0.2951
Meta loss on this task batch = 3.2644e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 153.9624, GNorm = 0.2982
Meta loss on this task batch = 3.1114e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 153.9653, GNorm = 0.3811
Meta loss on this task batch = 2.7996e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 153.9679, GNorm = 0.3527
Meta loss on this task batch = 1.9659e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 153.9711, GNorm = 0.2341
Meta loss on this task batch = 2.8885e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 153.9738, GNorm = 0.3457
Meta loss on this task batch = 2.3999e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 153.9767, GNorm = 0.3147
Took 131.6715271472931 seconds to complete one epoch of meta training
Took 139.36467170715332 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.523113
Epoch 922
Meta loss on this task batch = 2.8258e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 153.9781, GNorm = 0.4277
Meta loss on this task batch = 2.2575e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 153.9806, GNorm = 0.2735
Meta loss on this task batch = 2.3797e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 153.9829, GNorm = 0.3232
Meta loss on this task batch = 2.9416e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 153.9849, GNorm = 0.2903
Meta loss on this task batch = 2.6893e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 153.9885, GNorm = 0.2893
Meta loss on this task batch = 3.1959e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 153.9921, GNorm = 0.3211
Meta loss on this task batch = 3.2079e-01, Meta loss averaged over last 500 steps = 2.6298e-01, PNorm = 153.9945, GNorm = 0.2998
Meta loss on this task batch = 2.9072e-01, Meta loss averaged over last 500 steps = 2.6310e-01, PNorm = 153.9974, GNorm = 0.2981
Meta loss on this task batch = 2.8444e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 154.0009, GNorm = 0.3038
Meta loss on this task batch = 3.1421e-01, Meta loss averaged over last 500 steps = 2.6337e-01, PNorm = 154.0031, GNorm = 0.3203
Meta loss on this task batch = 2.7210e-01, Meta loss averaged over last 500 steps = 2.6332e-01, PNorm = 154.0061, GNorm = 0.2889
Meta loss on this task batch = 2.0674e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 154.0105, GNorm = 0.2268
Meta loss on this task batch = 2.2762e-01, Meta loss averaged over last 500 steps = 2.6313e-01, PNorm = 154.0152, GNorm = 0.2827
Meta loss on this task batch = 2.5864e-01, Meta loss averaged over last 500 steps = 2.6316e-01, PNorm = 154.0191, GNorm = 0.2773
Meta loss on this task batch = 2.4859e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 154.0242, GNorm = 0.2706
Meta loss on this task batch = 2.4503e-01, Meta loss averaged over last 500 steps = 2.6308e-01, PNorm = 154.0294, GNorm = 0.3211
Meta loss on this task batch = 2.3647e-01, Meta loss averaged over last 500 steps = 2.6305e-01, PNorm = 154.0348, GNorm = 0.2557
Meta loss on this task batch = 2.8349e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 154.0408, GNorm = 0.2856
Meta loss on this task batch = 1.9543e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 154.0468, GNorm = 0.3190
Took 131.1501281261444 seconds to complete one epoch of meta training
Took 140.281831741333 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488319
Epoch 923
Meta loss on this task batch = 2.2106e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 154.0538, GNorm = 0.2599
Meta loss on this task batch = 2.4063e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 154.0596, GNorm = 0.3123
Meta loss on this task batch = 2.3012e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 154.0644, GNorm = 0.2681
Meta loss on this task batch = 2.4383e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 154.0682, GNorm = 0.3010
Meta loss on this task batch = 2.5026e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 154.0678, GNorm = 0.3538
Meta loss on this task batch = 2.8527e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 154.0669, GNorm = 0.3329
Meta loss on this task batch = 2.6864e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 154.0652, GNorm = 0.2916
Meta loss on this task batch = 2.3485e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 154.0634, GNorm = 0.2855
Meta loss on this task batch = 2.6025e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 154.0628, GNorm = 0.2726
Meta loss on this task batch = 2.2585e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 154.0632, GNorm = 0.3458
Meta loss on this task batch = 2.5063e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 154.0644, GNorm = 0.2906
Meta loss on this task batch = 2.4728e-01, Meta loss averaged over last 500 steps = 2.6259e-01, PNorm = 154.0686, GNorm = 0.2696
Meta loss on this task batch = 2.1968e-01, Meta loss averaged over last 500 steps = 2.6238e-01, PNorm = 154.0741, GNorm = 0.2968
Meta loss on this task batch = 2.2329e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 154.0816, GNorm = 0.2831
Meta loss on this task batch = 3.0626e-01, Meta loss averaged over last 500 steps = 2.6237e-01, PNorm = 154.0884, GNorm = 0.3252
Meta loss on this task batch = 2.4246e-01, Meta loss averaged over last 500 steps = 2.6225e-01, PNorm = 154.0972, GNorm = 0.2624
Meta loss on this task batch = 3.4788e-01, Meta loss averaged over last 500 steps = 2.6239e-01, PNorm = 154.1037, GNorm = 0.3369
Meta loss on this task batch = 3.1105e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 154.1081, GNorm = 0.3588
Meta loss on this task batch = 3.0519e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 154.1126, GNorm = 0.3969
Took 131.81459665298462 seconds to complete one epoch of meta training
Took 140.20168280601501 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484956
Epoch 924
Meta loss on this task batch = 2.7601e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 154.1165, GNorm = 0.2888
Meta loss on this task batch = 2.5321e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 154.1210, GNorm = 0.2997
Meta loss on this task batch = 2.3309e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 154.1261, GNorm = 0.2869
Meta loss on this task batch = 2.2012e-01, Meta loss averaged over last 500 steps = 2.6266e-01, PNorm = 154.1328, GNorm = 0.2703
Meta loss on this task batch = 2.3948e-01, Meta loss averaged over last 500 steps = 2.6265e-01, PNorm = 154.1394, GNorm = 0.2953
Meta loss on this task batch = 2.5506e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 154.1457, GNorm = 0.2550
Meta loss on this task batch = 2.6357e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 154.1505, GNorm = 0.2943
Meta loss on this task batch = 2.9176e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 154.1550, GNorm = 0.3232
Meta loss on this task batch = 2.9474e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 154.1584, GNorm = 0.2896
Meta loss on this task batch = 2.4807e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 154.1605, GNorm = 0.2896
Meta loss on this task batch = 2.8930e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 154.1621, GNorm = 0.3266
Meta loss on this task batch = 2.6382e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 154.1646, GNorm = 0.2868
Meta loss on this task batch = 3.0147e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 154.1667, GNorm = 0.2909
Meta loss on this task batch = 2.4207e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 154.1700, GNorm = 0.2902
Meta loss on this task batch = 2.6889e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 154.1721, GNorm = 0.2706
Meta loss on this task batch = 2.5425e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 154.1753, GNorm = 0.2744
Meta loss on this task batch = 2.4457e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 154.1789, GNorm = 0.2413
Meta loss on this task batch = 2.4614e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 154.1826, GNorm = 0.2696
Meta loss on this task batch = 2.5491e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 154.1871, GNorm = 0.3125
Took 132.2703993320465 seconds to complete one epoch of meta training
Took 141.02591109275818 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495523
Epoch 925
Meta loss on this task batch = 2.5426e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 154.1929, GNorm = 0.2557
Meta loss on this task batch = 2.3690e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 154.1994, GNorm = 0.2396
Meta loss on this task batch = 2.2928e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 154.2053, GNorm = 0.2524
Meta loss on this task batch = 2.0645e-01, Meta loss averaged over last 500 steps = 2.6245e-01, PNorm = 154.2106, GNorm = 0.2330
Meta loss on this task batch = 2.7027e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 154.2139, GNorm = 0.2681
Meta loss on this task batch = 2.8859e-01, Meta loss averaged over last 500 steps = 2.6250e-01, PNorm = 154.2139, GNorm = 0.3530
Meta loss on this task batch = 3.0044e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 154.2121, GNorm = 0.3027
Meta loss on this task batch = 3.0020e-01, Meta loss averaged over last 500 steps = 2.6265e-01, PNorm = 154.2096, GNorm = 0.3219
Meta loss on this task batch = 3.1874e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 154.2070, GNorm = 0.3152
Meta loss on this task batch = 2.5384e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 154.2041, GNorm = 0.2624
Meta loss on this task batch = 2.6385e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 154.2036, GNorm = 0.2868
Meta loss on this task batch = 2.1264e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 154.2040, GNorm = 0.2601
Meta loss on this task batch = 2.7009e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 154.2060, GNorm = 0.3008
Meta loss on this task batch = 2.9008e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 154.2117, GNorm = 0.3112
Meta loss on this task batch = 2.3831e-01, Meta loss averaged over last 500 steps = 2.6266e-01, PNorm = 154.2180, GNorm = 0.2456
Meta loss on this task batch = 2.0415e-01, Meta loss averaged over last 500 steps = 2.6249e-01, PNorm = 154.2268, GNorm = 0.2658
Meta loss on this task batch = 2.5538e-01, Meta loss averaged over last 500 steps = 2.6247e-01, PNorm = 154.2378, GNorm = 0.2998
Meta loss on this task batch = 3.0099e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 154.2500, GNorm = 0.3032
Meta loss on this task batch = 2.4232e-01, Meta loss averaged over last 500 steps = 2.6256e-01, PNorm = 154.2625, GNorm = 0.3248
Took 133.38547134399414 seconds to complete one epoch of meta training
Took 142.93734455108643 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490443
Epoch 926
Meta loss on this task batch = 2.7410e-01, Meta loss averaged over last 500 steps = 2.6260e-01, PNorm = 154.2744, GNorm = 0.3390
Meta loss on this task batch = 1.9116e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 154.2875, GNorm = 0.2885
Meta loss on this task batch = 2.6771e-01, Meta loss averaged over last 500 steps = 2.6258e-01, PNorm = 154.2989, GNorm = 0.3124
Meta loss on this task batch = 2.4057e-01, Meta loss averaged over last 500 steps = 2.6251e-01, PNorm = 154.3069, GNorm = 0.3672
Meta loss on this task batch = 2.4992e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 154.3136, GNorm = 0.3116
Meta loss on this task batch = 2.9450e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 154.3189, GNorm = 0.3698
Meta loss on this task batch = 2.7012e-01, Meta loss averaged over last 500 steps = 2.6246e-01, PNorm = 154.3197, GNorm = 0.3656
Meta loss on this task batch = 2.8427e-01, Meta loss averaged over last 500 steps = 2.6248e-01, PNorm = 154.3175, GNorm = 0.3576
Meta loss on this task batch = 2.3159e-01, Meta loss averaged over last 500 steps = 2.6243e-01, PNorm = 154.3134, GNorm = 0.2646
Meta loss on this task batch = 2.4466e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 154.3079, GNorm = 0.3519
Meta loss on this task batch = 3.0031e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 154.3021, GNorm = 0.2868
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.6256e-01, PNorm = 154.2975, GNorm = 0.2723
Meta loss on this task batch = 2.8205e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 154.2945, GNorm = 0.2700
Meta loss on this task batch = 2.6129e-01, Meta loss averaged over last 500 steps = 2.6258e-01, PNorm = 154.2935, GNorm = 0.2372
Meta loss on this task batch = 3.3288e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 154.2945, GNorm = 0.3259
Meta loss on this task batch = 2.6747e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 154.2969, GNorm = 0.2738
Meta loss on this task batch = 2.6712e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 154.3023, GNorm = 0.2884
Meta loss on this task batch = 3.0680e-01, Meta loss averaged over last 500 steps = 2.6305e-01, PNorm = 154.3095, GNorm = 0.2971
Meta loss on this task batch = 2.2880e-01, Meta loss averaged over last 500 steps = 2.6297e-01, PNorm = 154.3182, GNorm = 0.3230
Took 132.9312083721161 seconds to complete one epoch of meta training
Took 141.64591431617737 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499908
Epoch 927
Meta loss on this task batch = 2.9318e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 154.3283, GNorm = 0.2532
Meta loss on this task batch = 2.6276e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 154.3380, GNorm = 0.2370
Meta loss on this task batch = 2.7753e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 154.3484, GNorm = 0.2761
Meta loss on this task batch = 2.9668e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 154.3564, GNorm = 0.2705
Meta loss on this task batch = 2.3003e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 154.3654, GNorm = 0.2447
Meta loss on this task batch = 2.4371e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 154.3731, GNorm = 0.2863
Meta loss on this task batch = 2.2760e-01, Meta loss averaged over last 500 steps = 2.6255e-01, PNorm = 154.3803, GNorm = 0.3147
Meta loss on this task batch = 3.0312e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 154.3855, GNorm = 0.3710
Meta loss on this task batch = 2.9230e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 154.3882, GNorm = 0.3287
Meta loss on this task batch = 2.3587e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 154.3912, GNorm = 0.2422
Meta loss on this task batch = 3.0523e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 154.3938, GNorm = 0.2949
Meta loss on this task batch = 2.4641e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 154.3952, GNorm = 0.3042
Meta loss on this task batch = 2.5094e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 154.3986, GNorm = 0.2681
Meta loss on this task batch = 2.2763e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 154.4024, GNorm = 0.2621
Meta loss on this task batch = 2.9562e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 154.4048, GNorm = 0.2986
Meta loss on this task batch = 2.8413e-01, Meta loss averaged over last 500 steps = 2.6291e-01, PNorm = 154.4081, GNorm = 0.3454
Meta loss on this task batch = 2.2340e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 154.4115, GNorm = 0.2511
Meta loss on this task batch = 2.6638e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 154.4150, GNorm = 0.3309
Meta loss on this task batch = 2.6002e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 154.4196, GNorm = 0.3482
Took 132.5784924030304 seconds to complete one epoch of meta training
Took 142.32348132133484 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510358
Epoch 928
Meta loss on this task batch = 2.7600e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 154.4246, GNorm = 0.2748
Meta loss on this task batch = 2.7911e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 154.4295, GNorm = 0.3395
Meta loss on this task batch = 2.6164e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 154.4333, GNorm = 0.3181
Meta loss on this task batch = 2.6545e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 154.4380, GNorm = 0.2570
Meta loss on this task batch = 3.0523e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 154.4419, GNorm = 0.3612
Meta loss on this task batch = 2.2920e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 154.4455, GNorm = 0.2786
Meta loss on this task batch = 2.1776e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 154.4507, GNorm = 0.2473
Meta loss on this task batch = 2.7685e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 154.4551, GNorm = 0.2678
Meta loss on this task batch = 2.7325e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 154.4600, GNorm = 0.2825
Meta loss on this task batch = 2.7703e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 154.4638, GNorm = 0.2874
Meta loss on this task batch = 2.6558e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 154.4683, GNorm = 0.2427
Meta loss on this task batch = 2.6824e-01, Meta loss averaged over last 500 steps = 2.6291e-01, PNorm = 154.4739, GNorm = 0.3303
Meta loss on this task batch = 2.4610e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 154.4788, GNorm = 0.2553
Meta loss on this task batch = 2.6931e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 154.4838, GNorm = 0.3213
Meta loss on this task batch = 2.5905e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 154.4884, GNorm = 0.2873
Meta loss on this task batch = 2.4059e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 154.4910, GNorm = 0.2811
Meta loss on this task batch = 2.3224e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 154.4930, GNorm = 0.2617
Meta loss on this task batch = 2.3772e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 154.4962, GNorm = 0.2569
Meta loss on this task batch = 2.9205e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 154.4964, GNorm = 0.3854
Took 128.7340052127838 seconds to complete one epoch of meta training
Took 137.0368127822876 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499684
Epoch 929
Meta loss on this task batch = 2.4045e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 154.4963, GNorm = 0.2869
Meta loss on this task batch = 2.5655e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 154.4941, GNorm = 0.3238
Meta loss on this task batch = 2.4116e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 154.4941, GNorm = 0.2760
Meta loss on this task batch = 2.6536e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 154.4946, GNorm = 0.2899
Meta loss on this task batch = 2.9654e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 154.4945, GNorm = 0.3411
Meta loss on this task batch = 3.2497e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 154.4948, GNorm = 0.3035
Meta loss on this task batch = 2.5411e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 154.4951, GNorm = 0.2847
Meta loss on this task batch = 3.2050e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 154.4952, GNorm = 0.2945
Meta loss on this task batch = 2.6835e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 154.4965, GNorm = 0.3377
Meta loss on this task batch = 2.2192e-01, Meta loss averaged over last 500 steps = 2.6255e-01, PNorm = 154.4993, GNorm = 0.2615
Meta loss on this task batch = 2.2600e-01, Meta loss averaged over last 500 steps = 2.6254e-01, PNorm = 154.5035, GNorm = 0.2891
Meta loss on this task batch = 2.5377e-01, Meta loss averaged over last 500 steps = 2.6250e-01, PNorm = 154.5069, GNorm = 0.2712
Meta loss on this task batch = 2.2118e-01, Meta loss averaged over last 500 steps = 2.6237e-01, PNorm = 154.5128, GNorm = 0.2503
Meta loss on this task batch = 2.6339e-01, Meta loss averaged over last 500 steps = 2.6234e-01, PNorm = 154.5188, GNorm = 0.2593
Meta loss on this task batch = 2.4571e-01, Meta loss averaged over last 500 steps = 2.6234e-01, PNorm = 154.5249, GNorm = 0.3083
Meta loss on this task batch = 2.5559e-01, Meta loss averaged over last 500 steps = 2.6236e-01, PNorm = 154.5321, GNorm = 0.3005
Meta loss on this task batch = 2.5228e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 154.5401, GNorm = 0.3979
Meta loss on this task batch = 2.4904e-01, Meta loss averaged over last 500 steps = 2.6220e-01, PNorm = 154.5474, GNorm = 0.2707
Meta loss on this task batch = 2.3679e-01, Meta loss averaged over last 500 steps = 2.6211e-01, PNorm = 154.5537, GNorm = 0.3058
Took 133.75075364112854 seconds to complete one epoch of meta training
Took 142.27095293998718 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492828
Epoch 930
Meta loss on this task batch = 2.5093e-01, Meta loss averaged over last 500 steps = 2.6214e-01, PNorm = 154.5600, GNorm = 0.3081
Meta loss on this task batch = 2.7373e-01, Meta loss averaged over last 500 steps = 2.6219e-01, PNorm = 154.5659, GNorm = 0.3558
Meta loss on this task batch = 2.5165e-01, Meta loss averaged over last 500 steps = 2.6219e-01, PNorm = 154.5713, GNorm = 0.2839
Meta loss on this task batch = 3.6492e-01, Meta loss averaged over last 500 steps = 2.6235e-01, PNorm = 154.5742, GNorm = 0.3932
Meta loss on this task batch = 2.4891e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 154.5764, GNorm = 0.2507
Meta loss on this task batch = 2.2594e-01, Meta loss averaged over last 500 steps = 2.6234e-01, PNorm = 154.5792, GNorm = 0.2731
Meta loss on this task batch = 2.8149e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 154.5841, GNorm = 0.3106
Meta loss on this task batch = 2.7273e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 154.5906, GNorm = 0.2730
Meta loss on this task batch = 2.6779e-01, Meta loss averaged over last 500 steps = 2.6238e-01, PNorm = 154.5970, GNorm = 0.2570
Meta loss on this task batch = 2.7009e-01, Meta loss averaged over last 500 steps = 2.6241e-01, PNorm = 154.6009, GNorm = 0.4477
Meta loss on this task batch = 2.4332e-01, Meta loss averaged over last 500 steps = 2.6239e-01, PNorm = 154.6049, GNorm = 0.2754
Meta loss on this task batch = 2.3122e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 154.6076, GNorm = 0.2894
Meta loss on this task batch = 3.0936e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 154.6102, GNorm = 0.3383
Meta loss on this task batch = 2.5488e-01, Meta loss averaged over last 500 steps = 2.6243e-01, PNorm = 154.6122, GNorm = 0.2703
Meta loss on this task batch = 2.7509e-01, Meta loss averaged over last 500 steps = 2.6251e-01, PNorm = 154.6146, GNorm = 0.3400
Meta loss on this task batch = 2.4661e-01, Meta loss averaged over last 500 steps = 2.6249e-01, PNorm = 154.6176, GNorm = 0.2762
Meta loss on this task batch = 2.2804e-01, Meta loss averaged over last 500 steps = 2.6239e-01, PNorm = 154.6225, GNorm = 0.2872
Meta loss on this task batch = 2.6986e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 154.6270, GNorm = 0.2688
Meta loss on this task batch = 2.1889e-01, Meta loss averaged over last 500 steps = 2.6223e-01, PNorm = 154.6314, GNorm = 0.3300
Took 133.7274820804596 seconds to complete one epoch of meta training
Took 142.10335421562195 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503098
Epoch 931
Meta loss on this task batch = 2.8839e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 154.6345, GNorm = 0.2618
Meta loss on this task batch = 2.5305e-01, Meta loss averaged over last 500 steps = 2.6229e-01, PNorm = 154.6370, GNorm = 0.3089
Meta loss on this task batch = 2.4426e-01, Meta loss averaged over last 500 steps = 2.6221e-01, PNorm = 154.6388, GNorm = 0.2999
Meta loss on this task batch = 2.2650e-01, Meta loss averaged over last 500 steps = 2.6226e-01, PNorm = 154.6410, GNorm = 0.2520
Meta loss on this task batch = 2.7047e-01, Meta loss averaged over last 500 steps = 2.6226e-01, PNorm = 154.6438, GNorm = 0.2482
Meta loss on this task batch = 2.9214e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 154.6462, GNorm = 0.3409
Meta loss on this task batch = 1.9034e-01, Meta loss averaged over last 500 steps = 2.6218e-01, PNorm = 154.6493, GNorm = 0.2491
Meta loss on this task batch = 2.2616e-01, Meta loss averaged over last 500 steps = 2.6218e-01, PNorm = 154.6522, GNorm = 0.2631
Meta loss on this task batch = 3.2395e-01, Meta loss averaged over last 500 steps = 2.6225e-01, PNorm = 154.6545, GNorm = 0.3372
Meta loss on this task batch = 2.2102e-01, Meta loss averaged over last 500 steps = 2.6221e-01, PNorm = 154.6577, GNorm = 0.2516
Meta loss on this task batch = 2.4336e-01, Meta loss averaged over last 500 steps = 2.6218e-01, PNorm = 154.6614, GNorm = 0.2604
Meta loss on this task batch = 2.5700e-01, Meta loss averaged over last 500 steps = 2.6224e-01, PNorm = 154.6677, GNorm = 0.3033
Meta loss on this task batch = 3.0977e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 154.6746, GNorm = 0.2941
Meta loss on this task batch = 2.6875e-01, Meta loss averaged over last 500 steps = 2.6239e-01, PNorm = 154.6827, GNorm = 0.2835
Meta loss on this task batch = 2.3906e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 154.6912, GNorm = 0.3007
Meta loss on this task batch = 3.0966e-01, Meta loss averaged over last 500 steps = 2.6227e-01, PNorm = 154.7005, GNorm = 0.3107
Meta loss on this task batch = 2.5041e-01, Meta loss averaged over last 500 steps = 2.6235e-01, PNorm = 154.7108, GNorm = 0.3159
Meta loss on this task batch = 2.4553e-01, Meta loss averaged over last 500 steps = 2.6226e-01, PNorm = 154.7211, GNorm = 0.2883
Meta loss on this task batch = 2.7557e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 154.7306, GNorm = 0.3182
Took 133.05327463150024 seconds to complete one epoch of meta training
Took 141.61834073066711 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490167
Epoch 932
Meta loss on this task batch = 2.7029e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 154.7387, GNorm = 0.2794
Meta loss on this task batch = 2.7651e-01, Meta loss averaged over last 500 steps = 2.6239e-01, PNorm = 154.7450, GNorm = 0.2951
Meta loss on this task batch = 3.3633e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 154.7489, GNorm = 0.3512
Meta loss on this task batch = 2.8794e-01, Meta loss averaged over last 500 steps = 2.6258e-01, PNorm = 154.7504, GNorm = 0.3493
Meta loss on this task batch = 2.4925e-01, Meta loss averaged over last 500 steps = 2.6259e-01, PNorm = 154.7505, GNorm = 0.3027
Meta loss on this task batch = 2.1167e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 154.7484, GNorm = 0.2538
Meta loss on this task batch = 2.6998e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 154.7457, GNorm = 0.2783
Meta loss on this task batch = 2.4957e-01, Meta loss averaged over last 500 steps = 2.6249e-01, PNorm = 154.7430, GNorm = 0.3137
Meta loss on this task batch = 2.4860e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 154.7412, GNorm = 0.2862
Meta loss on this task batch = 2.3554e-01, Meta loss averaged over last 500 steps = 2.6248e-01, PNorm = 154.7408, GNorm = 0.2361
Meta loss on this task batch = 2.6057e-01, Meta loss averaged over last 500 steps = 2.6245e-01, PNorm = 154.7410, GNorm = 0.2835
Meta loss on this task batch = 2.2918e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 154.7415, GNorm = 0.2711
Meta loss on this task batch = 2.7793e-01, Meta loss averaged over last 500 steps = 2.6239e-01, PNorm = 154.7437, GNorm = 0.2965
Meta loss on this task batch = 2.4221e-01, Meta loss averaged over last 500 steps = 2.6238e-01, PNorm = 154.7463, GNorm = 0.2533
Meta loss on this task batch = 2.5164e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 154.7493, GNorm = 0.2597
Meta loss on this task batch = 2.6946e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 154.7531, GNorm = 0.3286
Meta loss on this task batch = 2.1433e-01, Meta loss averaged over last 500 steps = 2.6224e-01, PNorm = 154.7587, GNorm = 0.2439
Meta loss on this task batch = 2.7067e-01, Meta loss averaged over last 500 steps = 2.6222e-01, PNorm = 154.7653, GNorm = 0.3088
Meta loss on this task batch = 3.0407e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 154.7718, GNorm = 0.3791
Took 133.7985668182373 seconds to complete one epoch of meta training
Took 142.1471927165985 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493011
Epoch 933
Meta loss on this task batch = 2.4951e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 154.7768, GNorm = 0.2858
Meta loss on this task batch = 2.6852e-01, Meta loss averaged over last 500 steps = 2.6236e-01, PNorm = 154.7797, GNorm = 0.3069
Meta loss on this task batch = 2.8418e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 154.7796, GNorm = 0.3707
Meta loss on this task batch = 2.4634e-01, Meta loss averaged over last 500 steps = 2.6226e-01, PNorm = 154.7796, GNorm = 0.2406
Meta loss on this task batch = 2.6080e-01, Meta loss averaged over last 500 steps = 2.6221e-01, PNorm = 154.7797, GNorm = 0.2736
Meta loss on this task batch = 2.0301e-01, Meta loss averaged over last 500 steps = 2.6210e-01, PNorm = 154.7821, GNorm = 0.2814
Meta loss on this task batch = 2.9452e-01, Meta loss averaged over last 500 steps = 2.6214e-01, PNorm = 154.7842, GNorm = 0.3109
Meta loss on this task batch = 2.3877e-01, Meta loss averaged over last 500 steps = 2.6212e-01, PNorm = 154.7861, GNorm = 0.2726
Meta loss on this task batch = 3.1112e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 154.7873, GNorm = 0.3077
Meta loss on this task batch = 2.3505e-01, Meta loss averaged over last 500 steps = 2.6203e-01, PNorm = 154.7896, GNorm = 0.2495
Meta loss on this task batch = 2.4371e-01, Meta loss averaged over last 500 steps = 2.6208e-01, PNorm = 154.7935, GNorm = 0.2910
Meta loss on this task batch = 2.3591e-01, Meta loss averaged over last 500 steps = 2.6209e-01, PNorm = 154.7962, GNorm = 0.2604
Meta loss on this task batch = 2.8565e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 154.7982, GNorm = 0.3419
Meta loss on this task batch = 2.1080e-01, Meta loss averaged over last 500 steps = 2.6196e-01, PNorm = 154.8014, GNorm = 0.2625
Meta loss on this task batch = 2.4886e-01, Meta loss averaged over last 500 steps = 2.6192e-01, PNorm = 154.8032, GNorm = 0.2739
Meta loss on this task batch = 2.4674e-01, Meta loss averaged over last 500 steps = 2.6193e-01, PNorm = 154.8054, GNorm = 0.2674
Meta loss on this task batch = 2.8728e-01, Meta loss averaged over last 500 steps = 2.6209e-01, PNorm = 154.8092, GNorm = 0.3089
Meta loss on this task batch = 2.7345e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 154.8130, GNorm = 0.3034
Meta loss on this task batch = 3.8049e-01, Meta loss averaged over last 500 steps = 2.6248e-01, PNorm = 154.8172, GNorm = 0.4717
Took 135.48488020896912 seconds to complete one epoch of meta training
Took 143.88185596466064 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499793
Epoch 934
Meta loss on this task batch = 2.3161e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 154.8222, GNorm = 0.2833
Meta loss on this task batch = 2.6724e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 154.8283, GNorm = 0.3014
Meta loss on this task batch = 2.6990e-01, Meta loss averaged over last 500 steps = 2.6230e-01, PNorm = 154.8354, GNorm = 0.2869
Meta loss on this task batch = 2.5337e-01, Meta loss averaged over last 500 steps = 2.6222e-01, PNorm = 154.8416, GNorm = 0.2813
Meta loss on this task batch = 2.4670e-01, Meta loss averaged over last 500 steps = 2.6221e-01, PNorm = 154.8484, GNorm = 0.2349
Meta loss on this task batch = 2.6348e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 154.8540, GNorm = 0.2545
Meta loss on this task batch = 2.5172e-01, Meta loss averaged over last 500 steps = 2.6213e-01, PNorm = 154.8588, GNorm = 0.3061
Meta loss on this task batch = 2.6739e-01, Meta loss averaged over last 500 steps = 2.6220e-01, PNorm = 154.8626, GNorm = 0.3249
Meta loss on this task batch = 2.1111e-01, Meta loss averaged over last 500 steps = 2.6211e-01, PNorm = 154.8670, GNorm = 0.3037
Meta loss on this task batch = 2.3356e-01, Meta loss averaged over last 500 steps = 2.6217e-01, PNorm = 154.8707, GNorm = 0.2965
Meta loss on this task batch = 2.8640e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 154.8748, GNorm = 0.2583
Meta loss on this task batch = 2.4907e-01, Meta loss averaged over last 500 steps = 2.6234e-01, PNorm = 154.8795, GNorm = 0.2852
Meta loss on this task batch = 3.1131e-01, Meta loss averaged over last 500 steps = 2.6243e-01, PNorm = 154.8830, GNorm = 0.3152
Meta loss on this task batch = 3.1464e-01, Meta loss averaged over last 500 steps = 2.6251e-01, PNorm = 154.8856, GNorm = 0.2867
Meta loss on this task batch = 1.9737e-01, Meta loss averaged over last 500 steps = 2.6238e-01, PNorm = 154.8885, GNorm = 0.2569
Meta loss on this task batch = 2.1178e-01, Meta loss averaged over last 500 steps = 2.6220e-01, PNorm = 154.8922, GNorm = 0.2605
Meta loss on this task batch = 2.8125e-01, Meta loss averaged over last 500 steps = 2.6222e-01, PNorm = 154.8952, GNorm = 0.2853
Meta loss on this task batch = 2.5064e-01, Meta loss averaged over last 500 steps = 2.6216e-01, PNorm = 154.8986, GNorm = 0.2807
Meta loss on this task batch = 3.1049e-01, Meta loss averaged over last 500 steps = 2.6218e-01, PNorm = 154.9003, GNorm = 0.3160
Took 131.4844617843628 seconds to complete one epoch of meta training
Took 139.782817363739 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470067
Epoch 935
Meta loss on this task batch = 3.0481e-01, Meta loss averaged over last 500 steps = 2.6220e-01, PNorm = 154.9012, GNorm = 0.3434
Meta loss on this task batch = 2.8431e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 154.9026, GNorm = 0.2708
Meta loss on this task batch = 2.5031e-01, Meta loss averaged over last 500 steps = 2.6227e-01, PNorm = 154.9029, GNorm = 0.3539
Meta loss on this task batch = 2.7474e-01, Meta loss averaged over last 500 steps = 2.6213e-01, PNorm = 154.9040, GNorm = 0.3526
Meta loss on this task batch = 2.9762e-01, Meta loss averaged over last 500 steps = 2.6209e-01, PNorm = 154.9046, GNorm = 0.3167
Meta loss on this task batch = 3.0009e-01, Meta loss averaged over last 500 steps = 2.6223e-01, PNorm = 154.9062, GNorm = 0.3001
Meta loss on this task batch = 2.9290e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 154.9090, GNorm = 0.2909
Meta loss on this task batch = 2.2771e-01, Meta loss averaged over last 500 steps = 2.6217e-01, PNorm = 154.9130, GNorm = 0.2394
Meta loss on this task batch = 2.7647e-01, Meta loss averaged over last 500 steps = 2.6208e-01, PNorm = 154.9174, GNorm = 0.2798
Meta loss on this task batch = 2.4083e-01, Meta loss averaged over last 500 steps = 2.6200e-01, PNorm = 154.9219, GNorm = 0.2646
Meta loss on this task batch = 2.1057e-01, Meta loss averaged over last 500 steps = 2.6184e-01, PNorm = 154.9264, GNorm = 0.2556
Meta loss on this task batch = 1.9883e-01, Meta loss averaged over last 500 steps = 2.6173e-01, PNorm = 154.9318, GNorm = 0.2518
Meta loss on this task batch = 2.3198e-01, Meta loss averaged over last 500 steps = 2.6171e-01, PNorm = 154.9360, GNorm = 0.2877
Meta loss on this task batch = 2.7117e-01, Meta loss averaged over last 500 steps = 2.6177e-01, PNorm = 154.9400, GNorm = 0.2724
Meta loss on this task batch = 2.6232e-01, Meta loss averaged over last 500 steps = 2.6173e-01, PNorm = 154.9445, GNorm = 0.2920
Meta loss on this task batch = 2.5441e-01, Meta loss averaged over last 500 steps = 2.6179e-01, PNorm = 154.9505, GNorm = 0.2831
Meta loss on this task batch = 2.6891e-01, Meta loss averaged over last 500 steps = 2.6185e-01, PNorm = 154.9563, GNorm = 0.3070
Meta loss on this task batch = 2.3840e-01, Meta loss averaged over last 500 steps = 2.6178e-01, PNorm = 154.9637, GNorm = 0.3024
Meta loss on this task batch = 2.4654e-01, Meta loss averaged over last 500 steps = 2.6187e-01, PNorm = 154.9710, GNorm = 0.3399
Took 129.66961884498596 seconds to complete one epoch of meta training
Took 138.7269127368927 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488218
Epoch 936
Meta loss on this task batch = 2.9982e-01, Meta loss averaged over last 500 steps = 2.6177e-01, PNorm = 154.9763, GNorm = 0.3231
Meta loss on this task batch = 1.8210e-01, Meta loss averaged over last 500 steps = 2.6154e-01, PNorm = 154.9803, GNorm = 0.2958
Meta loss on this task batch = 2.5264e-01, Meta loss averaged over last 500 steps = 2.6160e-01, PNorm = 154.9832, GNorm = 0.2963
Meta loss on this task batch = 2.9213e-01, Meta loss averaged over last 500 steps = 2.6175e-01, PNorm = 154.9861, GNorm = 0.3287
Meta loss on this task batch = 2.4544e-01, Meta loss averaged over last 500 steps = 2.6173e-01, PNorm = 154.9883, GNorm = 0.2677
Meta loss on this task batch = 2.4105e-01, Meta loss averaged over last 500 steps = 2.6170e-01, PNorm = 154.9887, GNorm = 0.3337
Meta loss on this task batch = 2.6443e-01, Meta loss averaged over last 500 steps = 2.6169e-01, PNorm = 154.9885, GNorm = 0.2585
Meta loss on this task batch = 2.3154e-01, Meta loss averaged over last 500 steps = 2.6156e-01, PNorm = 154.9893, GNorm = 0.2539
Meta loss on this task batch = 2.8325e-01, Meta loss averaged over last 500 steps = 2.6160e-01, PNorm = 154.9909, GNorm = 0.2578
Meta loss on this task batch = 2.8506e-01, Meta loss averaged over last 500 steps = 2.6165e-01, PNorm = 154.9923, GNorm = 0.3045
Meta loss on this task batch = 2.2599e-01, Meta loss averaged over last 500 steps = 2.6165e-01, PNorm = 154.9950, GNorm = 0.2469
Meta loss on this task batch = 2.8124e-01, Meta loss averaged over last 500 steps = 2.6166e-01, PNorm = 154.9979, GNorm = 0.3251
Meta loss on this task batch = 2.2685e-01, Meta loss averaged over last 500 steps = 2.6157e-01, PNorm = 155.0011, GNorm = 0.2853
Meta loss on this task batch = 2.4539e-01, Meta loss averaged over last 500 steps = 2.6155e-01, PNorm = 155.0046, GNorm = 0.3137
Meta loss on this task batch = 2.5488e-01, Meta loss averaged over last 500 steps = 2.6148e-01, PNorm = 155.0090, GNorm = 0.2887
Meta loss on this task batch = 2.2063e-01, Meta loss averaged over last 500 steps = 2.6138e-01, PNorm = 155.0139, GNorm = 0.2786
Meta loss on this task batch = 2.2393e-01, Meta loss averaged over last 500 steps = 2.6131e-01, PNorm = 155.0207, GNorm = 0.2146
Meta loss on this task batch = 2.6793e-01, Meta loss averaged over last 500 steps = 2.6144e-01, PNorm = 155.0280, GNorm = 0.3212
Meta loss on this task batch = 3.0787e-01, Meta loss averaged over last 500 steps = 2.6150e-01, PNorm = 155.0330, GNorm = 0.4929
Took 131.0637686252594 seconds to complete one epoch of meta training
Took 139.61442732810974 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483907
Epoch 937
Meta loss on this task batch = 2.6907e-01, Meta loss averaged over last 500 steps = 2.6153e-01, PNorm = 155.0367, GNorm = 0.3139
Meta loss on this task batch = 2.5034e-01, Meta loss averaged over last 500 steps = 2.6139e-01, PNorm = 155.0422, GNorm = 0.2925
Meta loss on this task batch = 2.5028e-01, Meta loss averaged over last 500 steps = 2.6140e-01, PNorm = 155.0447, GNorm = 0.3290
Meta loss on this task batch = 2.2421e-01, Meta loss averaged over last 500 steps = 2.6139e-01, PNorm = 155.0473, GNorm = 0.2836
Meta loss on this task batch = 2.4345e-01, Meta loss averaged over last 500 steps = 2.6136e-01, PNorm = 155.0480, GNorm = 0.2775
Meta loss on this task batch = 2.9430e-01, Meta loss averaged over last 500 steps = 2.6135e-01, PNorm = 155.0483, GNorm = 0.3332
Meta loss on this task batch = 2.5446e-01, Meta loss averaged over last 500 steps = 2.6130e-01, PNorm = 155.0473, GNorm = 0.2983
Meta loss on this task batch = 2.8876e-01, Meta loss averaged over last 500 steps = 2.6129e-01, PNorm = 155.0469, GNorm = 0.3058
Meta loss on this task batch = 2.6672e-01, Meta loss averaged over last 500 steps = 2.6123e-01, PNorm = 155.0467, GNorm = 0.2853
Meta loss on this task batch = 2.8875e-01, Meta loss averaged over last 500 steps = 2.6135e-01, PNorm = 155.0475, GNorm = 0.2866
Meta loss on this task batch = 2.6113e-01, Meta loss averaged over last 500 steps = 2.6139e-01, PNorm = 155.0495, GNorm = 0.3242
Meta loss on this task batch = 2.8570e-01, Meta loss averaged over last 500 steps = 2.6143e-01, PNorm = 155.0530, GNorm = 0.2879
Meta loss on this task batch = 2.9814e-01, Meta loss averaged over last 500 steps = 2.6147e-01, PNorm = 155.0582, GNorm = 0.3147
Meta loss on this task batch = 2.4546e-01, Meta loss averaged over last 500 steps = 2.6150e-01, PNorm = 155.0647, GNorm = 0.2781
Meta loss on this task batch = 2.4779e-01, Meta loss averaged over last 500 steps = 2.6144e-01, PNorm = 155.0705, GNorm = 0.2916
Meta loss on this task batch = 2.7105e-01, Meta loss averaged over last 500 steps = 2.6143e-01, PNorm = 155.0756, GNorm = 0.3346
Meta loss on this task batch = 2.5652e-01, Meta loss averaged over last 500 steps = 2.6138e-01, PNorm = 155.0787, GNorm = 0.2934
Meta loss on this task batch = 2.1570e-01, Meta loss averaged over last 500 steps = 2.6125e-01, PNorm = 155.0815, GNorm = 0.2793
Meta loss on this task batch = 2.3530e-01, Meta loss averaged over last 500 steps = 2.6116e-01, PNorm = 155.0835, GNorm = 0.3231
Took 130.5668752193451 seconds to complete one epoch of meta training
Took 138.93738389015198 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486512
Epoch 938
Meta loss on this task batch = 2.5809e-01, Meta loss averaged over last 500 steps = 2.6112e-01, PNorm = 155.0865, GNorm = 0.2622
Meta loss on this task batch = 2.7911e-01, Meta loss averaged over last 500 steps = 2.6118e-01, PNorm = 155.0901, GNorm = 0.2640
Meta loss on this task batch = 2.2035e-01, Meta loss averaged over last 500 steps = 2.6098e-01, PNorm = 155.0939, GNorm = 0.2580
Meta loss on this task batch = 2.8320e-01, Meta loss averaged over last 500 steps = 2.6108e-01, PNorm = 155.0960, GNorm = 0.3099
Meta loss on this task batch = 3.2118e-01, Meta loss averaged over last 500 steps = 2.6128e-01, PNorm = 155.0953, GNorm = 0.3306
Meta loss on this task batch = 2.4736e-01, Meta loss averaged over last 500 steps = 2.6126e-01, PNorm = 155.0952, GNorm = 0.2851
Meta loss on this task batch = 2.9389e-01, Meta loss averaged over last 500 steps = 2.6130e-01, PNorm = 155.0947, GNorm = 0.3016
Meta loss on this task batch = 2.8298e-01, Meta loss averaged over last 500 steps = 2.6135e-01, PNorm = 155.0945, GNorm = 0.2766
Meta loss on this task batch = 2.4302e-01, Meta loss averaged over last 500 steps = 2.6124e-01, PNorm = 155.0956, GNorm = 0.2459
Meta loss on this task batch = 2.8906e-01, Meta loss averaged over last 500 steps = 2.6122e-01, PNorm = 155.0974, GNorm = 0.2727
Meta loss on this task batch = 2.6017e-01, Meta loss averaged over last 500 steps = 2.6116e-01, PNorm = 155.1004, GNorm = 0.2829
Meta loss on this task batch = 2.5789e-01, Meta loss averaged over last 500 steps = 2.6108e-01, PNorm = 155.1044, GNorm = 0.2943
Meta loss on this task batch = 2.4792e-01, Meta loss averaged over last 500 steps = 2.6110e-01, PNorm = 155.1102, GNorm = 0.2907
Meta loss on this task batch = 2.4383e-01, Meta loss averaged over last 500 steps = 2.6106e-01, PNorm = 155.1153, GNorm = 0.2708
Meta loss on this task batch = 2.6320e-01, Meta loss averaged over last 500 steps = 2.6107e-01, PNorm = 155.1199, GNorm = 0.2525
Meta loss on this task batch = 2.2013e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 155.1240, GNorm = 0.2827
Meta loss on this task batch = 1.7764e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 155.1294, GNorm = 0.2518
Meta loss on this task batch = 1.9808e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 155.1349, GNorm = 0.2463
Meta loss on this task batch = 2.9382e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 155.1409, GNorm = 0.3517
Took 127.93084073066711 seconds to complete one epoch of meta training
Took 136.24768018722534 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502807
Epoch 939
Meta loss on this task batch = 2.3102e-01, Meta loss averaged over last 500 steps = 2.6044e-01, PNorm = 155.1470, GNorm = 0.2999
Meta loss on this task batch = 2.4868e-01, Meta loss averaged over last 500 steps = 2.6036e-01, PNorm = 155.1525, GNorm = 0.2721
Meta loss on this task batch = 2.2310e-01, Meta loss averaged over last 500 steps = 2.6032e-01, PNorm = 155.1577, GNorm = 0.2805
Meta loss on this task batch = 2.8470e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 155.1615, GNorm = 0.3421
Meta loss on this task batch = 3.0222e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 155.1646, GNorm = 0.3155
Meta loss on this task batch = 2.8092e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 155.1672, GNorm = 0.2807
Meta loss on this task batch = 3.2342e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 155.1701, GNorm = 0.3428
Meta loss on this task batch = 2.3876e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 155.1731, GNorm = 0.2644
Meta loss on this task batch = 2.6209e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 155.1770, GNorm = 0.3155
Meta loss on this task batch = 2.5929e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 155.1816, GNorm = 0.2941
Meta loss on this task batch = 2.0241e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 155.1874, GNorm = 0.2816
Meta loss on this task batch = 2.0614e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 155.1947, GNorm = 0.2424
Meta loss on this task batch = 2.4441e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 155.2029, GNorm = 0.3188
Meta loss on this task batch = 2.8924e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 155.2117, GNorm = 0.2997
Meta loss on this task batch = 3.3088e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 155.2204, GNorm = 0.3331
Meta loss on this task batch = 2.8404e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 155.2300, GNorm = 0.3449
Meta loss on this task batch = 2.2870e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 155.2388, GNorm = 0.2855
Meta loss on this task batch = 2.7464e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 155.2462, GNorm = 0.2937
Meta loss on this task batch = 2.7145e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 155.2532, GNorm = 0.3488
Took 133.10089468955994 seconds to complete one epoch of meta training
Took 141.24878787994385 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513417
Epoch 940
Meta loss on this task batch = 2.5674e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 155.2589, GNorm = 0.2920
Meta loss on this task batch = 2.5580e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 155.2639, GNorm = 0.2442
Meta loss on this task batch = 2.6914e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 155.2669, GNorm = 0.2561
Meta loss on this task batch = 2.7537e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 155.2687, GNorm = 0.2777
Meta loss on this task batch = 3.2529e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 155.2683, GNorm = 0.3245
Meta loss on this task batch = 3.3196e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 155.2670, GNorm = 0.3281
Meta loss on this task batch = 2.1202e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 155.2659, GNorm = 0.2435
Meta loss on this task batch = 2.1084e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 155.2656, GNorm = 0.3187
Meta loss on this task batch = 2.5153e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 155.2649, GNorm = 0.2687
Meta loss on this task batch = 3.0935e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 155.2640, GNorm = 0.3212
Meta loss on this task batch = 2.2130e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 155.2627, GNorm = 0.2689
Meta loss on this task batch = 2.6637e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 155.2613, GNorm = 0.2601
Meta loss on this task batch = 2.4184e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 155.2599, GNorm = 0.2622
Meta loss on this task batch = 2.5891e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 155.2602, GNorm = 0.2884
Meta loss on this task batch = 2.8413e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 155.2608, GNorm = 0.2806
Meta loss on this task batch = 3.0233e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 155.2613, GNorm = 0.2767
Meta loss on this task batch = 2.2473e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 155.2610, GNorm = 0.3038
Meta loss on this task batch = 2.3873e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 155.2617, GNorm = 0.2558
Meta loss on this task batch = 2.4052e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 155.2615, GNorm = 0.3137
Took 131.12844586372375 seconds to complete one epoch of meta training
Took 139.59312224388123 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492678
Epoch 941
Meta loss on this task batch = 3.0397e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 155.2608, GNorm = 0.3109
Meta loss on this task batch = 2.8774e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 155.2614, GNorm = 0.3365
Meta loss on this task batch = 2.1463e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 155.2628, GNorm = 0.2624
Meta loss on this task batch = 2.8483e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 155.2674, GNorm = 0.3424
Meta loss on this task batch = 2.3918e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 155.2734, GNorm = 0.3189
Meta loss on this task batch = 3.1747e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 155.2805, GNorm = 0.2977
Meta loss on this task batch = 3.0843e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 155.2893, GNorm = 0.3178
Meta loss on this task batch = 2.7267e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 155.2979, GNorm = 0.3414
Meta loss on this task batch = 2.4833e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 155.3056, GNorm = 0.3025
Meta loss on this task batch = 2.5751e-01, Meta loss averaged over last 500 steps = 2.6095e-01, PNorm = 155.3123, GNorm = 0.2763
Meta loss on this task batch = 2.7254e-01, Meta loss averaged over last 500 steps = 2.6095e-01, PNorm = 155.3186, GNorm = 0.3029
Meta loss on this task batch = 2.7328e-01, Meta loss averaged over last 500 steps = 2.6102e-01, PNorm = 155.3233, GNorm = 0.3012
Meta loss on this task batch = 3.2542e-01, Meta loss averaged over last 500 steps = 2.6110e-01, PNorm = 155.3251, GNorm = 0.4123
Meta loss on this task batch = 2.2868e-01, Meta loss averaged over last 500 steps = 2.6102e-01, PNorm = 155.3251, GNorm = 0.2790
Meta loss on this task batch = 2.1159e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 155.3262, GNorm = 0.2490
Meta loss on this task batch = 2.0469e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 155.3283, GNorm = 0.2786
Meta loss on this task batch = 2.6128e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 155.3308, GNorm = 0.2905
Meta loss on this task batch = 2.3632e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 155.3334, GNorm = 0.2507
Meta loss on this task batch = 2.5129e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 155.3356, GNorm = 0.3056
Took 132.2970449924469 seconds to complete one epoch of meta training
Took 140.6063313484192 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477189
Epoch 942
Meta loss on this task batch = 3.2585e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 155.3373, GNorm = 0.3016
Meta loss on this task batch = 3.0545e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 155.3389, GNorm = 0.3163
Meta loss on this task batch = 2.6800e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 155.3406, GNorm = 0.2906
Meta loss on this task batch = 2.2081e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 155.3439, GNorm = 0.2508
Meta loss on this task batch = 2.0919e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 155.3489, GNorm = 0.3351
Meta loss on this task batch = 3.2447e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 155.3535, GNorm = 0.3350
Meta loss on this task batch = 2.2297e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 155.3570, GNorm = 0.2598
Meta loss on this task batch = 2.3613e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 155.3590, GNorm = 0.2248
Meta loss on this task batch = 2.9165e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 155.3628, GNorm = 0.2672
Meta loss on this task batch = 2.4662e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 155.3676, GNorm = 0.2531
Meta loss on this task batch = 2.6202e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 155.3712, GNorm = 0.2436
Meta loss on this task batch = 2.8720e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 155.3746, GNorm = 0.2333
Meta loss on this task batch = 2.5512e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 155.3769, GNorm = 0.3214
Meta loss on this task batch = 2.4587e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 155.3792, GNorm = 0.2575
Meta loss on this task batch = 2.3683e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 155.3815, GNorm = 0.2786
Meta loss on this task batch = 2.2905e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 155.3855, GNorm = 0.2519
Meta loss on this task batch = 2.9109e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 155.3884, GNorm = 0.2639
Meta loss on this task batch = 2.7496e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 155.3912, GNorm = 0.3009
Meta loss on this task batch = 2.8248e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 155.3931, GNorm = 0.3311
Took 131.1747441291809 seconds to complete one epoch of meta training
Took 139.82884907722473 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466213
Epoch 943
Meta loss on this task batch = 2.1492e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 155.3959, GNorm = 0.2447
Meta loss on this task batch = 2.8281e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 155.4002, GNorm = 0.3182
Meta loss on this task batch = 2.3902e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 155.4059, GNorm = 0.2506
Meta loss on this task batch = 3.0615e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 155.4125, GNorm = 0.2644
Meta loss on this task batch = 2.2631e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 155.4183, GNorm = 0.2349
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 155.4229, GNorm = 0.2568
Meta loss on this task batch = 2.8579e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 155.4277, GNorm = 0.2874
Meta loss on this task batch = 2.4386e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 155.4311, GNorm = 0.3443
Meta loss on this task batch = 2.4759e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 155.4344, GNorm = 0.2860
Meta loss on this task batch = 2.4908e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 155.4368, GNorm = 0.3459
Meta loss on this task batch = 2.8048e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 155.4362, GNorm = 0.3452
Meta loss on this task batch = 2.7696e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 155.4360, GNorm = 0.3544
Meta loss on this task batch = 2.7737e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 155.4356, GNorm = 0.3305
Meta loss on this task batch = 2.5848e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 155.4377, GNorm = 0.2904
Meta loss on this task batch = 2.7056e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 155.4410, GNorm = 0.2872
Meta loss on this task batch = 2.5370e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 155.4444, GNorm = 0.3155
Meta loss on this task batch = 2.6583e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 155.4496, GNorm = 0.3012
Meta loss on this task batch = 2.6238e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 155.4563, GNorm = 0.3539
Meta loss on this task batch = 2.7728e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 155.4639, GNorm = 0.3596
Took 154.492778301239 seconds to complete one epoch of meta training
Took 162.93520331382751 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498162
Epoch 944
Meta loss on this task batch = 3.3014e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 155.4728, GNorm = 0.3358
Meta loss on this task batch = 2.6224e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 155.4827, GNorm = 0.3249
Meta loss on this task batch = 2.6243e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 155.4923, GNorm = 0.3425
Meta loss on this task batch = 2.6961e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 155.5018, GNorm = 0.2971
Meta loss on this task batch = 2.2251e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 155.5106, GNorm = 0.2808
Meta loss on this task batch = 2.7443e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 155.5196, GNorm = 0.3128
Meta loss on this task batch = 2.3125e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 155.5276, GNorm = 0.2365
Meta loss on this task batch = 2.5927e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 155.5335, GNorm = 0.3480
Meta loss on this task batch = 2.2204e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 155.5391, GNorm = 0.2482
Meta loss on this task batch = 2.8758e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 155.5441, GNorm = 0.2892
Meta loss on this task batch = 3.0547e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 155.5472, GNorm = 0.3261
Meta loss on this task batch = 2.5246e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 155.5510, GNorm = 0.2657
Meta loss on this task batch = 2.4618e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 155.5543, GNorm = 0.2773
Meta loss on this task batch = 2.8365e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 155.5587, GNorm = 0.2696
Meta loss on this task batch = 2.1420e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 155.5622, GNorm = 0.2510
Meta loss on this task batch = 2.4727e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 155.5661, GNorm = 0.2958
Meta loss on this task batch = 2.7373e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 155.5691, GNorm = 0.3452
Meta loss on this task batch = 2.9486e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 155.5720, GNorm = 0.3220
Meta loss on this task batch = 2.9434e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 155.5741, GNorm = 0.3329
Took 139.06122732162476 seconds to complete one epoch of meta training
Took 148.66213059425354 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485517
Epoch 945
Meta loss on this task batch = 2.4761e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 155.5776, GNorm = 0.3061
Meta loss on this task batch = 2.8013e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 155.5815, GNorm = 0.2825
Meta loss on this task batch = 2.4765e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 155.5865, GNorm = 0.2916
Meta loss on this task batch = 2.6215e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 155.5929, GNorm = 0.3042
Meta loss on this task batch = 2.8327e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 155.5988, GNorm = 0.2735
Meta loss on this task batch = 2.6775e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 155.6041, GNorm = 0.3381
Meta loss on this task batch = 2.0986e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 155.6092, GNorm = 0.2491
Meta loss on this task batch = 3.2987e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 155.6130, GNorm = 0.3253
Meta loss on this task batch = 2.6924e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 155.6169, GNorm = 0.3055
Meta loss on this task batch = 2.6506e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 155.6217, GNorm = 0.3141
Meta loss on this task batch = 2.9956e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 155.6265, GNorm = 0.2865
Meta loss on this task batch = 2.7642e-01, Meta loss averaged over last 500 steps = 2.6108e-01, PNorm = 155.6317, GNorm = 0.2770
Meta loss on this task batch = 2.8060e-01, Meta loss averaged over last 500 steps = 2.6126e-01, PNorm = 155.6374, GNorm = 0.3243
Meta loss on this task batch = 2.6768e-01, Meta loss averaged over last 500 steps = 2.6130e-01, PNorm = 155.6434, GNorm = 0.2575
Meta loss on this task batch = 2.5919e-01, Meta loss averaged over last 500 steps = 2.6129e-01, PNorm = 155.6489, GNorm = 0.2881
Meta loss on this task batch = 2.7833e-01, Meta loss averaged over last 500 steps = 2.6128e-01, PNorm = 155.6556, GNorm = 0.2879
Meta loss on this task batch = 2.3863e-01, Meta loss averaged over last 500 steps = 2.6134e-01, PNorm = 155.6623, GNorm = 0.2589
Meta loss on this task batch = 2.4016e-01, Meta loss averaged over last 500 steps = 2.6128e-01, PNorm = 155.6664, GNorm = 0.3506
Meta loss on this task batch = 2.7346e-01, Meta loss averaged over last 500 steps = 2.6117e-01, PNorm = 155.6696, GNorm = 0.3618
Took 132.71508622169495 seconds to complete one epoch of meta training
Took 141.11397504806519 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491217
Epoch 946
Meta loss on this task batch = 2.4912e-01, Meta loss averaged over last 500 steps = 2.6116e-01, PNorm = 155.6711, GNorm = 0.2906
Meta loss on this task batch = 2.4677e-01, Meta loss averaged over last 500 steps = 2.6117e-01, PNorm = 155.6730, GNorm = 0.2789
Meta loss on this task batch = 3.1503e-01, Meta loss averaged over last 500 steps = 2.6116e-01, PNorm = 155.6755, GNorm = 0.2848
Meta loss on this task batch = 2.7920e-01, Meta loss averaged over last 500 steps = 2.6114e-01, PNorm = 155.6774, GNorm = 0.3102
Meta loss on this task batch = 3.1295e-01, Meta loss averaged over last 500 steps = 2.6122e-01, PNorm = 155.6803, GNorm = 0.2915
Meta loss on this task batch = 2.3605e-01, Meta loss averaged over last 500 steps = 2.6127e-01, PNorm = 155.6832, GNorm = 0.2636
Meta loss on this task batch = 3.0773e-01, Meta loss averaged over last 500 steps = 2.6137e-01, PNorm = 155.6870, GNorm = 0.3157
Meta loss on this task batch = 2.1986e-01, Meta loss averaged over last 500 steps = 2.6130e-01, PNorm = 155.6900, GNorm = 0.2613
Meta loss on this task batch = 2.3168e-01, Meta loss averaged over last 500 steps = 2.6129e-01, PNorm = 155.6941, GNorm = 0.2593
Meta loss on this task batch = 2.4832e-01, Meta loss averaged over last 500 steps = 2.6134e-01, PNorm = 155.6982, GNorm = 0.3032
Meta loss on this task batch = 2.5718e-01, Meta loss averaged over last 500 steps = 2.6130e-01, PNorm = 155.7020, GNorm = 0.2725
Meta loss on this task batch = 2.7981e-01, Meta loss averaged over last 500 steps = 2.6140e-01, PNorm = 155.7061, GNorm = 0.2914
Meta loss on this task batch = 2.5204e-01, Meta loss averaged over last 500 steps = 2.6135e-01, PNorm = 155.7102, GNorm = 0.2817
Meta loss on this task batch = 2.4369e-01, Meta loss averaged over last 500 steps = 2.6141e-01, PNorm = 155.7147, GNorm = 0.2867
Meta loss on this task batch = 3.1363e-01, Meta loss averaged over last 500 steps = 2.6139e-01, PNorm = 155.7187, GNorm = 0.3978
Meta loss on this task batch = 2.5495e-01, Meta loss averaged over last 500 steps = 2.6140e-01, PNorm = 155.7240, GNorm = 0.2819
Meta loss on this task batch = 2.4328e-01, Meta loss averaged over last 500 steps = 2.6138e-01, PNorm = 155.7297, GNorm = 0.2909
Meta loss on this task batch = 2.2253e-01, Meta loss averaged over last 500 steps = 2.6136e-01, PNorm = 155.7355, GNorm = 0.3274
Meta loss on this task batch = 2.3780e-01, Meta loss averaged over last 500 steps = 2.6138e-01, PNorm = 155.7410, GNorm = 0.3580
Took 135.19683194160461 seconds to complete one epoch of meta training
Took 143.70356225967407 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497310
Epoch 947
Meta loss on this task batch = 2.5897e-01, Meta loss averaged over last 500 steps = 2.6138e-01, PNorm = 155.7458, GNorm = 0.3288
Meta loss on this task batch = 2.6646e-01, Meta loss averaged over last 500 steps = 2.6142e-01, PNorm = 155.7488, GNorm = 0.3614
Meta loss on this task batch = 2.4082e-01, Meta loss averaged over last 500 steps = 2.6134e-01, PNorm = 155.7518, GNorm = 0.2921
Meta loss on this task batch = 2.6561e-01, Meta loss averaged over last 500 steps = 2.6127e-01, PNorm = 155.7550, GNorm = 0.3104
Meta loss on this task batch = 2.5178e-01, Meta loss averaged over last 500 steps = 2.6114e-01, PNorm = 155.7576, GNorm = 0.2817
Meta loss on this task batch = 2.0084e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 155.7598, GNorm = 0.2117
Meta loss on this task batch = 2.4532e-01, Meta loss averaged over last 500 steps = 2.6105e-01, PNorm = 155.7617, GNorm = 0.2866
Meta loss on this task batch = 2.5243e-01, Meta loss averaged over last 500 steps = 2.6098e-01, PNorm = 155.7634, GNorm = 0.2616
Meta loss on this task batch = 2.6891e-01, Meta loss averaged over last 500 steps = 2.6107e-01, PNorm = 155.7640, GNorm = 0.3098
Meta loss on this task batch = 2.6437e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 155.7635, GNorm = 0.2571
Meta loss on this task batch = 2.7606e-01, Meta loss averaged over last 500 steps = 2.6116e-01, PNorm = 155.7627, GNorm = 0.3189
Meta loss on this task batch = 3.0526e-01, Meta loss averaged over last 500 steps = 2.6131e-01, PNorm = 155.7606, GNorm = 0.3107
Meta loss on this task batch = 2.5899e-01, Meta loss averaged over last 500 steps = 2.6133e-01, PNorm = 155.7592, GNorm = 0.3019
Meta loss on this task batch = 2.1884e-01, Meta loss averaged over last 500 steps = 2.6126e-01, PNorm = 155.7575, GNorm = 0.2723
Meta loss on this task batch = 2.3615e-01, Meta loss averaged over last 500 steps = 2.6136e-01, PNorm = 155.7563, GNorm = 0.2590
Meta loss on this task batch = 3.2083e-01, Meta loss averaged over last 500 steps = 2.6144e-01, PNorm = 155.7561, GNorm = 0.2992
Meta loss on this task batch = 2.4428e-01, Meta loss averaged over last 500 steps = 2.6143e-01, PNorm = 155.7567, GNorm = 0.2669
Meta loss on this task batch = 2.7975e-01, Meta loss averaged over last 500 steps = 2.6143e-01, PNorm = 155.7579, GNorm = 0.2640
Meta loss on this task batch = 2.4385e-01, Meta loss averaged over last 500 steps = 2.6139e-01, PNorm = 155.7600, GNorm = 0.3294
Took 133.36936116218567 seconds to complete one epoch of meta training
Took 141.56641221046448 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502784
Epoch 948
Meta loss on this task batch = 2.8191e-01, Meta loss averaged over last 500 steps = 2.6130e-01, PNorm = 155.7635, GNorm = 0.2898
Meta loss on this task batch = 1.9430e-01, Meta loss averaged over last 500 steps = 2.6107e-01, PNorm = 155.7687, GNorm = 0.2580
Meta loss on this task batch = 2.8370e-01, Meta loss averaged over last 500 steps = 2.6108e-01, PNorm = 155.7743, GNorm = 0.3276
Meta loss on this task batch = 2.5581e-01, Meta loss averaged over last 500 steps = 2.6120e-01, PNorm = 155.7826, GNorm = 0.2686
Meta loss on this task batch = 2.6844e-01, Meta loss averaged over last 500 steps = 2.6116e-01, PNorm = 155.7913, GNorm = 0.2935
Meta loss on this task batch = 2.1217e-01, Meta loss averaged over last 500 steps = 2.6110e-01, PNorm = 155.8006, GNorm = 0.2828
Meta loss on this task batch = 2.4014e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 155.8104, GNorm = 0.3094
Meta loss on this task batch = 2.1824e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 155.8185, GNorm = 0.2839
Meta loss on this task batch = 2.7798e-01, Meta loss averaged over last 500 steps = 2.6108e-01, PNorm = 155.8255, GNorm = 0.3280
Meta loss on this task batch = 2.3991e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 155.8313, GNorm = 0.2850
Meta loss on this task batch = 2.6708e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 155.8371, GNorm = 0.2939
Meta loss on this task batch = 2.9850e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 155.8430, GNorm = 0.3614
Meta loss on this task batch = 2.4776e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 155.8466, GNorm = 0.3162
Meta loss on this task batch = 2.6132e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 155.8507, GNorm = 0.2594
Meta loss on this task batch = 2.3133e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 155.8562, GNorm = 0.2610
Meta loss on this task batch = 2.4708e-01, Meta loss averaged over last 500 steps = 2.6048e-01, PNorm = 155.8625, GNorm = 0.2790
Meta loss on this task batch = 2.5604e-01, Meta loss averaged over last 500 steps = 2.6045e-01, PNorm = 155.8701, GNorm = 0.2858
Meta loss on this task batch = 2.7315e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 155.8772, GNorm = 0.3611
Meta loss on this task batch = 2.6159e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 155.8834, GNorm = 0.3614
Took 132.8654842376709 seconds to complete one epoch of meta training
Took 141.42070698738098 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482705
Epoch 949
Meta loss on this task batch = 2.0642e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 155.8890, GNorm = 0.2575
Meta loss on this task batch = 2.6345e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 155.8947, GNorm = 0.2494
Meta loss on this task batch = 2.4485e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 155.9013, GNorm = 0.2511
Meta loss on this task batch = 2.7150e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 155.9081, GNorm = 0.2821
Meta loss on this task batch = 2.5023e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 155.9135, GNorm = 0.2600
Meta loss on this task batch = 2.3815e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 155.9180, GNorm = 0.2463
Meta loss on this task batch = 3.1398e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 155.9215, GNorm = 0.3090
Meta loss on this task batch = 2.5000e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 155.9239, GNorm = 0.2941
Meta loss on this task batch = 3.0066e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 155.9242, GNorm = 0.2907
Meta loss on this task batch = 2.4821e-01, Meta loss averaged over last 500 steps = 2.6102e-01, PNorm = 155.9247, GNorm = 0.2836
Meta loss on this task batch = 2.5333e-01, Meta loss averaged over last 500 steps = 2.6102e-01, PNorm = 155.9255, GNorm = 0.2970
Meta loss on this task batch = 2.2591e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 155.9265, GNorm = 0.2361
Meta loss on this task batch = 2.6986e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 155.9262, GNorm = 0.2626
Meta loss on this task batch = 2.9337e-01, Meta loss averaged over last 500 steps = 2.6102e-01, PNorm = 155.9264, GNorm = 0.2718
Meta loss on this task batch = 2.3934e-01, Meta loss averaged over last 500 steps = 2.6098e-01, PNorm = 155.9278, GNorm = 0.2552
Meta loss on this task batch = 2.1778e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 155.9307, GNorm = 0.2844
Meta loss on this task batch = 2.5416e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 155.9349, GNorm = 0.3072
Meta loss on this task batch = 2.3779e-01, Meta loss averaged over last 500 steps = 2.6095e-01, PNorm = 155.9408, GNorm = 0.2923
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 155.9454, GNorm = 0.3151
Took 131.03893113136292 seconds to complete one epoch of meta training
Took 139.21847414970398 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475628
Epoch 950
Meta loss on this task batch = 2.8851e-01, Meta loss averaged over last 500 steps = 2.6122e-01, PNorm = 155.9491, GNorm = 0.3381
Meta loss on this task batch = 2.5305e-01, Meta loss averaged over last 500 steps = 2.6111e-01, PNorm = 155.9533, GNorm = 0.3010
Meta loss on this task batch = 2.4188e-01, Meta loss averaged over last 500 steps = 2.6111e-01, PNorm = 155.9584, GNorm = 0.2661
Meta loss on this task batch = 3.0602e-01, Meta loss averaged over last 500 steps = 2.6102e-01, PNorm = 155.9629, GNorm = 0.3254
Meta loss on this task batch = 2.1606e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 155.9673, GNorm = 0.2500
Meta loss on this task batch = 2.6344e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 155.9745, GNorm = 0.4402
Meta loss on this task batch = 2.9378e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 155.9822, GNorm = 0.3426
Meta loss on this task batch = 2.6694e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 155.9878, GNorm = 0.2981
Meta loss on this task batch = 1.8612e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 155.9940, GNorm = 0.2762
Meta loss on this task batch = 2.7851e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 155.9993, GNorm = 0.2951
Meta loss on this task batch = 2.4659e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 156.0009, GNorm = 0.3544
Meta loss on this task batch = 2.8275e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 156.0008, GNorm = 0.3484
Meta loss on this task batch = 3.2774e-01, Meta loss averaged over last 500 steps = 2.6103e-01, PNorm = 155.9977, GNorm = 0.3558
Meta loss on this task batch = 2.4326e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 155.9929, GNorm = 0.3159
Meta loss on this task batch = 2.7912e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 155.9876, GNorm = 0.3184
Meta loss on this task batch = 2.4162e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 155.9838, GNorm = 0.2683
Meta loss on this task batch = 2.5430e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 155.9798, GNorm = 0.3096
Meta loss on this task batch = 2.5164e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 155.9776, GNorm = 0.2891
Meta loss on this task batch = 1.8320e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 155.9779, GNorm = 0.2673
Took 131.91978240013123 seconds to complete one epoch of meta training
Took 140.17701649665833 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480149
Epoch 951
Meta loss on this task batch = 2.9141e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 155.9797, GNorm = 0.2764
Meta loss on this task batch = 2.4940e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 155.9819, GNorm = 0.2836
Meta loss on this task batch = 2.5720e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 155.9858, GNorm = 0.2556
Meta loss on this task batch = 2.9790e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 155.9915, GNorm = 0.2571
Meta loss on this task batch = 3.0847e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 155.9952, GNorm = 0.3261
Meta loss on this task batch = 2.1887e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 155.9981, GNorm = 0.2568
Meta loss on this task batch = 2.9407e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 156.0015, GNorm = 0.2244
Meta loss on this task batch = 3.0349e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 156.0036, GNorm = 0.2967
Meta loss on this task batch = 2.3267e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 156.0067, GNorm = 0.2234
Meta loss on this task batch = 2.7009e-01, Meta loss averaged over last 500 steps = 2.6113e-01, PNorm = 156.0095, GNorm = 0.3074
Meta loss on this task batch = 2.4749e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 156.0124, GNorm = 0.2577
Meta loss on this task batch = 2.4793e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 156.0145, GNorm = 0.2861
Meta loss on this task batch = 2.6216e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 156.0162, GNorm = 0.2613
Meta loss on this task batch = 2.2669e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 156.0189, GNorm = 0.2657
Meta loss on this task batch = 3.0410e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 156.0201, GNorm = 0.4120
Meta loss on this task batch = 2.7216e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 156.0202, GNorm = 0.3145
Meta loss on this task batch = 2.8152e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 156.0219, GNorm = 0.2912
Meta loss on this task batch = 2.4606e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 156.0250, GNorm = 0.2785
Meta loss on this task batch = 2.0038e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 156.0286, GNorm = 0.4017
Took 129.02244544029236 seconds to complete one epoch of meta training
Took 137.6245732307434 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.519713
Epoch 952
Meta loss on this task batch = 2.6503e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 156.0336, GNorm = 0.3215
Meta loss on this task batch = 2.2337e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 156.0382, GNorm = 0.2928
Meta loss on this task batch = 2.7501e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 156.0436, GNorm = 0.2735
Meta loss on this task batch = 2.2687e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 156.0489, GNorm = 0.2498
Meta loss on this task batch = 2.4439e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 156.0539, GNorm = 0.2544
Meta loss on this task batch = 2.3714e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 156.0581, GNorm = 0.2532
Meta loss on this task batch = 2.6365e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 156.0604, GNorm = 0.3355
Meta loss on this task batch = 2.6949e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 156.0596, GNorm = 0.3483
Meta loss on this task batch = 2.4062e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 156.0569, GNorm = 0.3298
Meta loss on this task batch = 2.4820e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 156.0533, GNorm = 0.3000
Meta loss on this task batch = 3.4418e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 156.0494, GNorm = 0.3901
Meta loss on this task batch = 2.5960e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 156.0456, GNorm = 0.2790
Meta loss on this task batch = 2.3788e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 156.0428, GNorm = 0.2692
Meta loss on this task batch = 2.6173e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 156.0423, GNorm = 0.2856
Meta loss on this task batch = 2.9426e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 156.0443, GNorm = 0.2931
Meta loss on this task batch = 2.3964e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 156.0487, GNorm = 0.2826
Meta loss on this task batch = 2.8367e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 156.0556, GNorm = 0.3322
Meta loss on this task batch = 2.6892e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 156.0643, GNorm = 0.3185
Meta loss on this task batch = 3.3387e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 156.0740, GNorm = 0.4480
Took 132.30522632598877 seconds to complete one epoch of meta training
Took 139.8624620437622 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482061
Epoch 953
Meta loss on this task batch = 2.7292e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 156.0862, GNorm = 0.3809
Meta loss on this task batch = 2.8658e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 156.0997, GNorm = 0.3328
Meta loss on this task batch = 2.2655e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 156.1146, GNorm = 0.3005
Meta loss on this task batch = 2.5327e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 156.1291, GNorm = 0.3031
Meta loss on this task batch = 2.4582e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 156.1423, GNorm = 0.2744
Meta loss on this task batch = 2.3147e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 156.1539, GNorm = 0.2735
Meta loss on this task batch = 3.2454e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 156.1635, GNorm = 0.3278
Meta loss on this task batch = 2.5006e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 156.1703, GNorm = 0.2901
Meta loss on this task batch = 2.5842e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 156.1769, GNorm = 0.3031
Meta loss on this task batch = 3.0623e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 156.1814, GNorm = 0.3164
Meta loss on this task batch = 2.5432e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 156.1849, GNorm = 0.2977
Meta loss on this task batch = 2.9582e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 156.1866, GNorm = 0.3701
Meta loss on this task batch = 2.2699e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 156.1874, GNorm = 0.2500
Meta loss on this task batch = 2.9507e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.1874, GNorm = 0.2742
Meta loss on this task batch = 2.6230e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 156.1878, GNorm = 0.3280
Meta loss on this task batch = 2.6837e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 156.1898, GNorm = 0.2608
Meta loss on this task batch = 2.9465e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 156.1927, GNorm = 0.2549
Meta loss on this task batch = 2.7268e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 156.1956, GNorm = 0.3171
Meta loss on this task batch = 2.3599e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 156.1990, GNorm = 0.2825
Took 131.01994824409485 seconds to complete one epoch of meta training
Took 139.71466541290283 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472248
Epoch 954
Meta loss on this task batch = 2.6101e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 156.2036, GNorm = 0.2812
Meta loss on this task batch = 2.6234e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 156.2077, GNorm = 0.3075
Meta loss on this task batch = 2.5058e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 156.2127, GNorm = 0.2979
Meta loss on this task batch = 2.2788e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 156.2181, GNorm = 0.2706
Meta loss on this task batch = 2.8518e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 156.2243, GNorm = 0.2494
Meta loss on this task batch = 2.7483e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 156.2311, GNorm = 0.3718
Meta loss on this task batch = 2.4511e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 156.2392, GNorm = 0.2735
Meta loss on this task batch = 2.9118e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 156.2482, GNorm = 0.3042
Meta loss on this task batch = 2.8553e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 156.2571, GNorm = 0.3669
Meta loss on this task batch = 1.8562e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 156.2651, GNorm = 0.2389
Meta loss on this task batch = 2.8640e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 156.2705, GNorm = 0.3890
Meta loss on this task batch = 2.8310e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 156.2745, GNorm = 0.3232
Meta loss on this task batch = 2.3331e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 156.2786, GNorm = 0.2942
Meta loss on this task batch = 2.6857e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 156.2835, GNorm = 0.3313
Meta loss on this task batch = 3.0776e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 156.2882, GNorm = 0.3454
Meta loss on this task batch = 2.5376e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 156.2926, GNorm = 0.2801
Meta loss on this task batch = 2.7332e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 156.2958, GNorm = 0.2963
Meta loss on this task batch = 2.9740e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 156.2985, GNorm = 0.3148
Meta loss on this task batch = 2.4222e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 156.3018, GNorm = 0.4441
Took 132.33670377731323 seconds to complete one epoch of meta training
Took 140.78979063034058 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491830
Epoch 955
Meta loss on this task batch = 2.3512e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 156.3036, GNorm = 0.2720
Meta loss on this task batch = 2.1589e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 156.3066, GNorm = 0.2928
Meta loss on this task batch = 2.7630e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 156.3083, GNorm = 0.2784
Meta loss on this task batch = 2.3218e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 156.3119, GNorm = 0.2679
Meta loss on this task batch = 2.7900e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 156.3159, GNorm = 0.2796
Meta loss on this task batch = 2.2288e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 156.3207, GNorm = 0.2992
Meta loss on this task batch = 2.1564e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 156.3248, GNorm = 0.2958
Meta loss on this task batch = 2.2501e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 156.3282, GNorm = 0.2767
Meta loss on this task batch = 2.6734e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 156.3307, GNorm = 0.3038
Meta loss on this task batch = 3.0025e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 156.3319, GNorm = 0.2966
Meta loss on this task batch = 2.7211e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 156.3328, GNorm = 0.3149
Meta loss on this task batch = 2.6822e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 156.3339, GNorm = 0.2933
Meta loss on this task batch = 3.4558e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 156.3355, GNorm = 0.3634
Meta loss on this task batch = 2.4373e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 156.3382, GNorm = 0.2670
Meta loss on this task batch = 2.5263e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 156.3422, GNorm = 0.2888
Meta loss on this task batch = 2.7689e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 156.3457, GNorm = 0.3333
Meta loss on this task batch = 2.0011e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 156.3503, GNorm = 0.2410
Meta loss on this task batch = 3.0536e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 156.3559, GNorm = 0.3145
Meta loss on this task batch = 2.8104e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 156.3630, GNorm = 0.3441
Took 131.94154977798462 seconds to complete one epoch of meta training
Took 140.0173420906067 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494759
Epoch 956
Meta loss on this task batch = 2.8530e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 156.3709, GNorm = 0.2824
Meta loss on this task batch = 2.2608e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 156.3788, GNorm = 0.2712
Meta loss on this task batch = 2.5717e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 156.3874, GNorm = 0.2810
Meta loss on this task batch = 2.1404e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 156.3951, GNorm = 0.2477
Meta loss on this task batch = 2.8358e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 156.4033, GNorm = 0.3338
Meta loss on this task batch = 2.8901e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 156.4087, GNorm = 0.3197
Meta loss on this task batch = 2.6560e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 156.4127, GNorm = 0.2852
Meta loss on this task batch = 2.9648e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 156.4141, GNorm = 0.3249
Meta loss on this task batch = 2.2033e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 156.4141, GNorm = 0.3099
Meta loss on this task batch = 2.3165e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 156.4131, GNorm = 0.2453
Meta loss on this task batch = 2.5776e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 156.4102, GNorm = 0.3450
Meta loss on this task batch = 2.7986e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.4082, GNorm = 0.2994
Meta loss on this task batch = 2.8831e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 156.4073, GNorm = 0.2703
Meta loss on this task batch = 2.0639e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 156.4069, GNorm = 0.2824
Meta loss on this task batch = 3.1128e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 156.4079, GNorm = 0.3207
Meta loss on this task batch = 2.5559e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 156.4087, GNorm = 0.3237
Meta loss on this task batch = 2.8664e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 156.4087, GNorm = 0.2870
Meta loss on this task batch = 2.3170e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 156.4086, GNorm = 0.3107
Meta loss on this task batch = 2.8833e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 156.4102, GNorm = 0.3461
Took 129.90094351768494 seconds to complete one epoch of meta training
Took 137.9709062576294 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484991
Epoch 957
Meta loss on this task batch = 2.2245e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 156.4133, GNorm = 0.2269
Meta loss on this task batch = 3.0562e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 156.4177, GNorm = 0.2766
Meta loss on this task batch = 2.2706e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 156.4229, GNorm = 0.2748
Meta loss on this task batch = 2.2643e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 156.4290, GNorm = 0.2621
Meta loss on this task batch = 2.7284e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 156.4347, GNorm = 0.3248
Meta loss on this task batch = 2.4198e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 156.4395, GNorm = 0.2615
Meta loss on this task batch = 2.3839e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 156.4440, GNorm = 0.2721
Meta loss on this task batch = 2.5250e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 156.4479, GNorm = 0.2863
Meta loss on this task batch = 2.8217e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 156.4514, GNorm = 0.3036
Meta loss on this task batch = 2.2561e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 156.4551, GNorm = 0.2890
Meta loss on this task batch = 2.3861e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 156.4583, GNorm = 0.2881
Meta loss on this task batch = 2.5277e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 156.4615, GNorm = 0.2915
Meta loss on this task batch = 2.1259e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 156.4643, GNorm = 0.3017
Meta loss on this task batch = 2.7501e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 156.4658, GNorm = 0.3434
Meta loss on this task batch = 3.0446e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 156.4663, GNorm = 0.3242
Meta loss on this task batch = 3.1204e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 156.4674, GNorm = 0.2785
Meta loss on this task batch = 2.5712e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 156.4684, GNorm = 0.3864
Meta loss on this task batch = 2.7542e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 156.4706, GNorm = 0.2920
Meta loss on this task batch = 2.3229e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 156.4739, GNorm = 0.3545
Took 131.82508754730225 seconds to complete one epoch of meta training
Took 140.72063755989075 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486369
Epoch 958
Meta loss on this task batch = 2.4039e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 156.4763, GNorm = 0.2626
Meta loss on this task batch = 2.7986e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 156.4800, GNorm = 0.3359
Meta loss on this task batch = 2.6262e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 156.4845, GNorm = 0.2956
Meta loss on this task batch = 2.7721e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.4895, GNorm = 0.2898
Meta loss on this task batch = 2.7154e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 156.4950, GNorm = 0.3029
Meta loss on this task batch = 2.8857e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 156.5002, GNorm = 0.3317
Meta loss on this task batch = 2.4229e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 156.5040, GNorm = 0.3170
Meta loss on this task batch = 2.6465e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.5067, GNorm = 0.2959
Meta loss on this task batch = 2.0369e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 156.5093, GNorm = 0.2722
Meta loss on this task batch = 2.5761e-01, Meta loss averaged over last 500 steps = 2.6048e-01, PNorm = 156.5122, GNorm = 0.2809
Meta loss on this task batch = 2.5193e-01, Meta loss averaged over last 500 steps = 2.6048e-01, PNorm = 156.5154, GNorm = 0.2936
Meta loss on this task batch = 2.8331e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 156.5185, GNorm = 0.2701
Meta loss on this task batch = 2.4199e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 156.5226, GNorm = 0.2639
Meta loss on this task batch = 2.5849e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 156.5267, GNorm = 0.2583
Meta loss on this task batch = 2.8460e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 156.5299, GNorm = 0.2605
Meta loss on this task batch = 2.3610e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 156.5336, GNorm = 0.2528
Meta loss on this task batch = 2.6534e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 156.5368, GNorm = 0.2487
Meta loss on this task batch = 2.7494e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 156.5391, GNorm = 0.2945
Meta loss on this task batch = 2.9100e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 156.5426, GNorm = 0.3463
Took 129.588543176651 seconds to complete one epoch of meta training
Took 137.68240880966187 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485864
Epoch 959
Meta loss on this task batch = 2.7664e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 156.5455, GNorm = 0.2499
Meta loss on this task batch = 2.8037e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 156.5476, GNorm = 0.2471
Meta loss on this task batch = 2.7327e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 156.5496, GNorm = 0.2891
Meta loss on this task batch = 2.6864e-01, Meta loss averaged over last 500 steps = 2.6103e-01, PNorm = 156.5524, GNorm = 0.2578
Meta loss on this task batch = 2.1277e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 156.5542, GNorm = 0.2782
Meta loss on this task batch = 3.0477e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 156.5548, GNorm = 0.2379
Meta loss on this task batch = 2.5728e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 156.5560, GNorm = 0.3046
Meta loss on this task batch = 2.9074e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 156.5586, GNorm = 0.3358
Meta loss on this task batch = 2.1763e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 156.5616, GNorm = 0.2971
Meta loss on this task batch = 2.2537e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.5626, GNorm = 0.3392
Meta loss on this task batch = 3.0981e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 156.5640, GNorm = 0.2903
Meta loss on this task batch = 2.4763e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 156.5659, GNorm = 0.2990
Meta loss on this task batch = 2.2728e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 156.5674, GNorm = 0.2801
Meta loss on this task batch = 2.3149e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 156.5687, GNorm = 0.2600
Meta loss on this task batch = 3.1575e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 156.5679, GNorm = 0.3227
Meta loss on this task batch = 2.1647e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 156.5678, GNorm = 0.2419
Meta loss on this task batch = 2.4252e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 156.5694, GNorm = 0.3017
Meta loss on this task batch = 2.3348e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.5729, GNorm = 0.2979
Meta loss on this task batch = 2.9481e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 156.5756, GNorm = 0.4133
Took 129.4950590133667 seconds to complete one epoch of meta training
Took 138.26656079292297 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501912
Epoch 960
Meta loss on this task batch = 2.6943e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 156.5776, GNorm = 0.2779
Meta loss on this task batch = 2.6854e-01, Meta loss averaged over last 500 steps = 2.6098e-01, PNorm = 156.5779, GNorm = 0.3676
Meta loss on this task batch = 2.9050e-01, Meta loss averaged over last 500 steps = 2.6106e-01, PNorm = 156.5781, GNorm = 0.3919
Meta loss on this task batch = 2.5850e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 156.5806, GNorm = 0.2867
Meta loss on this task batch = 3.0067e-01, Meta loss averaged over last 500 steps = 2.6106e-01, PNorm = 156.5840, GNorm = 0.3007
Meta loss on this task batch = 2.4059e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 156.5896, GNorm = 0.3598
Meta loss on this task batch = 2.3540e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 156.5961, GNorm = 0.2737
Meta loss on this task batch = 2.7018e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.6027, GNorm = 0.3307
Meta loss on this task batch = 2.5155e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 156.6082, GNorm = 0.3110
Meta loss on this task batch = 2.7225e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.6114, GNorm = 0.3065
Meta loss on this task batch = 2.3244e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 156.6145, GNorm = 0.2751
Meta loss on this task batch = 2.5710e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 156.6188, GNorm = 0.2743
Meta loss on this task batch = 2.4200e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 156.6236, GNorm = 0.2395
Meta loss on this task batch = 2.9851e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.6271, GNorm = 0.2991
Meta loss on this task batch = 2.5659e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 156.6298, GNorm = 0.2859
Meta loss on this task batch = 2.4452e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 156.6336, GNorm = 0.2515
Meta loss on this task batch = 2.3762e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 156.6363, GNorm = 0.2722
Meta loss on this task batch = 2.7444e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 156.6376, GNorm = 0.3288
Meta loss on this task batch = 2.0541e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 156.6402, GNorm = 0.2847
Took 130.4640598297119 seconds to complete one epoch of meta training
Took 138.8651783466339 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489270
Epoch 961
Meta loss on this task batch = 2.4762e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 156.6436, GNorm = 0.2678
Meta loss on this task batch = 1.8999e-01, Meta loss averaged over last 500 steps = 2.6050e-01, PNorm = 156.6483, GNorm = 0.2500
Meta loss on this task batch = 2.9345e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 156.6517, GNorm = 0.2743
Meta loss on this task batch = 2.3248e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 156.6543, GNorm = 0.2771
Meta loss on this task batch = 2.2840e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 156.6580, GNorm = 0.2329
Meta loss on this task batch = 2.5774e-01, Meta loss averaged over last 500 steps = 2.6042e-01, PNorm = 156.6617, GNorm = 0.3060
Meta loss on this task batch = 3.1847e-01, Meta loss averaged over last 500 steps = 2.6045e-01, PNorm = 156.6642, GNorm = 0.3126
Meta loss on this task batch = 2.0343e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 156.6673, GNorm = 0.2677
Meta loss on this task batch = 2.7042e-01, Meta loss averaged over last 500 steps = 2.6033e-01, PNorm = 156.6699, GNorm = 0.3195
Meta loss on this task batch = 2.8001e-01, Meta loss averaged over last 500 steps = 2.6034e-01, PNorm = 156.6708, GNorm = 0.3173
Meta loss on this task batch = 2.9191e-01, Meta loss averaged over last 500 steps = 2.6032e-01, PNorm = 156.6692, GNorm = 0.3769
Meta loss on this task batch = 2.8101e-01, Meta loss averaged over last 500 steps = 2.6029e-01, PNorm = 156.6653, GNorm = 0.4384
Meta loss on this task batch = 2.8955e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 156.6621, GNorm = 0.3049
Meta loss on this task batch = 2.2217e-01, Meta loss averaged over last 500 steps = 2.6027e-01, PNorm = 156.6603, GNorm = 0.2514
Meta loss on this task batch = 2.7072e-01, Meta loss averaged over last 500 steps = 2.6026e-01, PNorm = 156.6584, GNorm = 0.2616
Meta loss on this task batch = 3.0403e-01, Meta loss averaged over last 500 steps = 2.6038e-01, PNorm = 156.6571, GNorm = 0.3380
Meta loss on this task batch = 2.4678e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 156.6578, GNorm = 0.2499
Meta loss on this task batch = 2.4771e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 156.6605, GNorm = 0.2779
Meta loss on this task batch = 2.4730e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 156.6666, GNorm = 0.3842
Took 132.65047097206116 seconds to complete one epoch of meta training
Took 141.37227487564087 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.519300
Epoch 962
Meta loss on this task batch = 3.0396e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 156.6747, GNorm = 0.3065
Meta loss on this task batch = 2.6697e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 156.6841, GNorm = 0.2602
Meta loss on this task batch = 2.5540e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 156.6945, GNorm = 0.2660
Meta loss on this task batch = 2.9236e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 156.7053, GNorm = 0.2436
Meta loss on this task batch = 2.6564e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 156.7167, GNorm = 0.2825
Meta loss on this task batch = 2.4020e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 156.7269, GNorm = 0.3098
Meta loss on this task batch = 2.1024e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 156.7374, GNorm = 0.2431
Meta loss on this task batch = 2.5115e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 156.7467, GNorm = 0.3027
Meta loss on this task batch = 2.6487e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 156.7520, GNorm = 0.3102
Meta loss on this task batch = 2.5777e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 156.7561, GNorm = 0.2906
Meta loss on this task batch = 2.5999e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 156.7594, GNorm = 0.2684
Meta loss on this task batch = 2.4826e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 156.7602, GNorm = 0.3609
Meta loss on this task batch = 2.9820e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 156.7583, GNorm = 0.3998
Meta loss on this task batch = 2.8017e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 156.7568, GNorm = 0.3132
Meta loss on this task batch = 2.5354e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 156.7562, GNorm = 0.2618
Meta loss on this task batch = 2.7948e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.7555, GNorm = 0.3244
Meta loss on this task batch = 2.3101e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 156.7550, GNorm = 0.2645
Meta loss on this task batch = 2.5773e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 156.7561, GNorm = 0.2718
Meta loss on this task batch = 2.7678e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 156.7593, GNorm = 0.3180
Took 131.51221179962158 seconds to complete one epoch of meta training
Took 140.15032720565796 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506188
Epoch 963
Meta loss on this task batch = 2.8493e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 156.7647, GNorm = 0.3092
Meta loss on this task batch = 3.3508e-01, Meta loss averaged over last 500 steps = 2.6110e-01, PNorm = 156.7712, GNorm = 0.2756
Meta loss on this task batch = 2.8067e-01, Meta loss averaged over last 500 steps = 2.6122e-01, PNorm = 156.7793, GNorm = 0.2571
Meta loss on this task batch = 2.1153e-01, Meta loss averaged over last 500 steps = 2.6120e-01, PNorm = 156.7877, GNorm = 0.2328
Meta loss on this task batch = 2.3320e-01, Meta loss averaged over last 500 steps = 2.6113e-01, PNorm = 156.7967, GNorm = 0.2783
Meta loss on this task batch = 3.3047e-01, Meta loss averaged over last 500 steps = 2.6118e-01, PNorm = 156.8056, GNorm = 0.3316
Meta loss on this task batch = 3.0179e-01, Meta loss averaged over last 500 steps = 2.6124e-01, PNorm = 156.8135, GNorm = 0.3174
Meta loss on this task batch = 2.5039e-01, Meta loss averaged over last 500 steps = 2.6124e-01, PNorm = 156.8220, GNorm = 0.2675
Meta loss on this task batch = 1.9880e-01, Meta loss averaged over last 500 steps = 2.6114e-01, PNorm = 156.8292, GNorm = 0.2745
Meta loss on this task batch = 2.7650e-01, Meta loss averaged over last 500 steps = 2.6124e-01, PNorm = 156.8351, GNorm = 0.2799
Meta loss on this task batch = 2.6822e-01, Meta loss averaged over last 500 steps = 2.6129e-01, PNorm = 156.8416, GNorm = 0.2998
Meta loss on this task batch = 2.5114e-01, Meta loss averaged over last 500 steps = 2.6121e-01, PNorm = 156.8484, GNorm = 0.2859
Meta loss on this task batch = 2.4931e-01, Meta loss averaged over last 500 steps = 2.6120e-01, PNorm = 156.8539, GNorm = 0.2606
Meta loss on this task batch = 1.9519e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 156.8591, GNorm = 0.2526
Meta loss on this task batch = 2.6705e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 156.8631, GNorm = 0.2825
Meta loss on this task batch = 2.8573e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 156.8661, GNorm = 0.3109
Meta loss on this task batch = 1.9758e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 156.8691, GNorm = 0.2305
Meta loss on this task batch = 2.7181e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 156.8712, GNorm = 0.3692
Meta loss on this task batch = 2.5898e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 156.8735, GNorm = 0.2843
Took 128.56801652908325 seconds to complete one epoch of meta training
Took 136.88667750358582 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504816
Epoch 964
Meta loss on this task batch = 2.6049e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.8764, GNorm = 0.3336
Meta loss on this task batch = 2.4774e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 156.8823, GNorm = 0.2589
Meta loss on this task batch = 2.4409e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 156.8890, GNorm = 0.3041
Meta loss on this task batch = 2.0275e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 156.8953, GNorm = 0.2199
Meta loss on this task batch = 3.0958e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 156.9010, GNorm = 0.2993
Meta loss on this task batch = 2.8221e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 156.9055, GNorm = 0.3082
Meta loss on this task batch = 2.7703e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 156.9084, GNorm = 0.3104
Meta loss on this task batch = 2.7125e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 156.9105, GNorm = 0.2627
Meta loss on this task batch = 2.6461e-01, Meta loss averaged over last 500 steps = 2.6103e-01, PNorm = 156.9117, GNorm = 0.2697
Meta loss on this task batch = 3.1170e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 156.9139, GNorm = 0.2826
Meta loss on this task batch = 2.9314e-01, Meta loss averaged over last 500 steps = 2.6103e-01, PNorm = 156.9156, GNorm = 0.3166
Meta loss on this task batch = 2.4313e-01, Meta loss averaged over last 500 steps = 2.6102e-01, PNorm = 156.9165, GNorm = 0.2997
Meta loss on this task batch = 2.8078e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 156.9160, GNorm = 0.3103
Meta loss on this task batch = 2.2798e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 156.9167, GNorm = 0.2981
Meta loss on this task batch = 2.7153e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 156.9190, GNorm = 0.3023
Meta loss on this task batch = 2.5960e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 156.9225, GNorm = 0.3063
Meta loss on this task batch = 2.8413e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 156.9239, GNorm = 0.3424
Meta loss on this task batch = 2.4175e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 156.9267, GNorm = 0.2844
Meta loss on this task batch = 2.0812e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 156.9304, GNorm = 0.3339
Took 132.52622961997986 seconds to complete one epoch of meta training
Took 141.12399864196777 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511847
Epoch 965
Meta loss on this task batch = 3.3121e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 156.9333, GNorm = 0.3520
Meta loss on this task batch = 2.6329e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 156.9366, GNorm = 0.2839
Meta loss on this task batch = 2.0741e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 156.9402, GNorm = 0.2600
Meta loss on this task batch = 1.9940e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 156.9448, GNorm = 0.2598
Meta loss on this task batch = 3.4562e-01, Meta loss averaged over last 500 steps = 2.6131e-01, PNorm = 156.9470, GNorm = 0.2932
Meta loss on this task batch = 2.7809e-01, Meta loss averaged over last 500 steps = 2.6128e-01, PNorm = 156.9508, GNorm = 0.3129
Meta loss on this task batch = 2.5985e-01, Meta loss averaged over last 500 steps = 2.6134e-01, PNorm = 156.9537, GNorm = 0.2356
Meta loss on this task batch = 2.1089e-01, Meta loss averaged over last 500 steps = 2.6126e-01, PNorm = 156.9580, GNorm = 0.2713
Meta loss on this task batch = 2.5470e-01, Meta loss averaged over last 500 steps = 2.6132e-01, PNorm = 156.9629, GNorm = 0.2590
Meta loss on this task batch = 1.4504e-01, Meta loss averaged over last 500 steps = 2.6104e-01, PNorm = 156.9682, GNorm = 0.2260
Meta loss on this task batch = 2.5068e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 156.9732, GNorm = 0.2681
Meta loss on this task batch = 2.8876e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 156.9782, GNorm = 0.2807
Meta loss on this task batch = 2.7516e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 156.9822, GNorm = 0.3291
Meta loss on this task batch = 2.3934e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 156.9861, GNorm = 0.2500
Meta loss on this task batch = 3.2891e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 156.9890, GNorm = 0.3206
Meta loss on this task batch = 2.6842e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 156.9907, GNorm = 0.3318
Meta loss on this task batch = 2.6455e-01, Meta loss averaged over last 500 steps = 2.6114e-01, PNorm = 156.9913, GNorm = 0.2717
Meta loss on this task batch = 2.9445e-01, Meta loss averaged over last 500 steps = 2.6131e-01, PNorm = 156.9908, GNorm = 0.3598
Meta loss on this task batch = 2.6182e-01, Meta loss averaged over last 500 steps = 2.6135e-01, PNorm = 156.9905, GNorm = 0.3198
Took 131.8612298965454 seconds to complete one epoch of meta training
Took 140.32330226898193 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497925
Epoch 966
Meta loss on this task batch = 2.6501e-01, Meta loss averaged over last 500 steps = 2.6130e-01, PNorm = 156.9905, GNorm = 0.2764
Meta loss on this task batch = 2.0180e-01, Meta loss averaged over last 500 steps = 2.6104e-01, PNorm = 156.9914, GNorm = 0.2391
Meta loss on this task batch = 3.0539e-01, Meta loss averaged over last 500 steps = 2.6108e-01, PNorm = 156.9909, GNorm = 0.2962
Meta loss on this task batch = 2.2317e-01, Meta loss averaged over last 500 steps = 2.6107e-01, PNorm = 156.9916, GNorm = 0.2875
Meta loss on this task batch = 2.4316e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 156.9939, GNorm = 0.2698
Meta loss on this task batch = 2.9522e-01, Meta loss averaged over last 500 steps = 2.6106e-01, PNorm = 156.9978, GNorm = 0.3378
Meta loss on this task batch = 2.2788e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 157.0018, GNorm = 0.2652
Meta loss on this task batch = 2.1165e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 157.0077, GNorm = 0.2804
Meta loss on this task batch = 2.4853e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 157.0142, GNorm = 0.2565
Meta loss on this task batch = 3.0604e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 157.0194, GNorm = 0.3431
Meta loss on this task batch = 2.5284e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 157.0237, GNorm = 0.2792
Meta loss on this task batch = 2.9593e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 157.0277, GNorm = 0.2759
Meta loss on this task batch = 2.4815e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 157.0315, GNorm = 0.3114
Meta loss on this task batch = 1.9783e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 157.0349, GNorm = 0.2481
Meta loss on this task batch = 2.8875e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 157.0381, GNorm = 0.2902
Meta loss on this task batch = 2.5939e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 157.0401, GNorm = 0.2715
Meta loss on this task batch = 2.7905e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 157.0417, GNorm = 0.3132
Meta loss on this task batch = 2.5202e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 157.0442, GNorm = 0.2942
Meta loss on this task batch = 3.6048e-01, Meta loss averaged over last 500 steps = 2.6106e-01, PNorm = 157.0459, GNorm = 0.3843
Took 134.0403754711151 seconds to complete one epoch of meta training
Took 142.27864241600037 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483356
Epoch 967
Meta loss on this task batch = 2.0080e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 157.0472, GNorm = 0.2443
Meta loss on this task batch = 2.5067e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 157.0492, GNorm = 0.2978
Meta loss on this task batch = 2.5472e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 157.0504, GNorm = 0.3290
Meta loss on this task batch = 2.1694e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 157.0523, GNorm = 0.2458
Meta loss on this task batch = 2.7921e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 157.0558, GNorm = 0.2906
Meta loss on this task batch = 2.5943e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 157.0600, GNorm = 0.2507
Meta loss on this task batch = 2.6715e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 157.0669, GNorm = 0.2769
Meta loss on this task batch = 2.8652e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 157.0752, GNorm = 0.3365
Meta loss on this task batch = 2.5833e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 157.0823, GNorm = 0.3091
Meta loss on this task batch = 2.7422e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 157.0885, GNorm = 0.2868
Meta loss on this task batch = 2.1193e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 157.0933, GNorm = 0.2556
Meta loss on this task batch = 2.5835e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 157.0968, GNorm = 0.3175
Meta loss on this task batch = 2.2785e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 157.0992, GNorm = 0.2650
Meta loss on this task batch = 2.7615e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 157.1010, GNorm = 0.2906
Meta loss on this task batch = 2.6987e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 157.1009, GNorm = 0.2758
Meta loss on this task batch = 2.2424e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 157.1001, GNorm = 0.3188
Meta loss on this task batch = 2.6001e-01, Meta loss averaged over last 500 steps = 2.6050e-01, PNorm = 157.0999, GNorm = 0.3009
Meta loss on this task batch = 3.5197e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 157.0993, GNorm = 0.3233
Meta loss on this task batch = 2.7852e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 157.1004, GNorm = 0.3631
Took 131.2641680240631 seconds to complete one epoch of meta training
Took 139.60115146636963 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490270
Epoch 968
Meta loss on this task batch = 2.8748e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 157.1017, GNorm = 0.2992
Meta loss on this task batch = 2.0449e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 157.1050, GNorm = 0.2301
Meta loss on this task batch = 2.1752e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 157.1090, GNorm = 0.2831
Meta loss on this task batch = 2.6842e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 157.1144, GNorm = 0.2997
Meta loss on this task batch = 2.5273e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 157.1209, GNorm = 0.2802
Meta loss on this task batch = 3.0896e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 157.1267, GNorm = 0.2969
Meta loss on this task batch = 2.4767e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 157.1321, GNorm = 0.3431
Meta loss on this task batch = 2.0209e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 157.1369, GNorm = 0.2416
Meta loss on this task batch = 2.5385e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 157.1421, GNorm = 0.2621
Meta loss on this task batch = 3.2652e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 157.1471, GNorm = 0.3349
Meta loss on this task batch = 2.6533e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 157.1533, GNorm = 0.2894
Meta loss on this task batch = 1.9431e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 157.1599, GNorm = 0.2624
Meta loss on this task batch = 2.6946e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 157.1652, GNorm = 0.2885
Meta loss on this task batch = 2.5030e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 157.1714, GNorm = 0.2698
Meta loss on this task batch = 2.4615e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 157.1780, GNorm = 0.2520
Meta loss on this task batch = 2.8396e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 157.1835, GNorm = 0.2726
Meta loss on this task batch = 2.5022e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 157.1886, GNorm = 0.2512
Meta loss on this task batch = 2.8403e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 157.1936, GNorm = 0.2752
Meta loss on this task batch = 2.8059e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 157.1990, GNorm = 0.3562
Took 130.62638783454895 seconds to complete one epoch of meta training
Took 138.9483938217163 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483645
Epoch 969
Meta loss on this task batch = 2.5431e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 157.2022, GNorm = 0.2724
Meta loss on this task batch = 2.5119e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 157.2052, GNorm = 0.3084
Meta loss on this task batch = 2.6654e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 157.2083, GNorm = 0.3139
Meta loss on this task batch = 2.7186e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 157.2102, GNorm = 0.3005
Meta loss on this task batch = 2.5657e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 157.2115, GNorm = 0.3104
Meta loss on this task batch = 2.3874e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 157.2123, GNorm = 0.3045
Meta loss on this task batch = 2.2416e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 157.2124, GNorm = 0.2618
Meta loss on this task batch = 2.9381e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 157.2127, GNorm = 0.3074
Meta loss on this task batch = 2.3943e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 157.2135, GNorm = 0.2563
Meta loss on this task batch = 2.8139e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 157.2139, GNorm = 0.2847
Meta loss on this task batch = 2.4606e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 157.2137, GNorm = 0.2551
Meta loss on this task batch = 2.8055e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 157.2140, GNorm = 0.2905
Meta loss on this task batch = 2.6276e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 157.2159, GNorm = 0.2971
Meta loss on this task batch = 2.4615e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 157.2185, GNorm = 0.2853
Meta loss on this task batch = 2.6182e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 157.2221, GNorm = 0.2783
Meta loss on this task batch = 2.9478e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 157.2282, GNorm = 0.2990
Meta loss on this task batch = 2.4089e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 157.2349, GNorm = 0.2852
Meta loss on this task batch = 2.7026e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 157.2430, GNorm = 0.3147
Meta loss on this task batch = 2.5473e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 157.2533, GNorm = 0.3259
Took 131.8352565765381 seconds to complete one epoch of meta training
Took 139.16835117340088 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486683
Epoch 970
Meta loss on this task batch = 2.6113e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 157.2626, GNorm = 0.2899
Meta loss on this task batch = 2.9031e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 157.2716, GNorm = 0.2587
Meta loss on this task batch = 2.9761e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 157.2805, GNorm = 0.2879
Meta loss on this task batch = 2.5103e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 157.2880, GNorm = 0.2781
Meta loss on this task batch = 2.5575e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 157.2968, GNorm = 0.3059
Meta loss on this task batch = 2.4258e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 157.3040, GNorm = 0.2529
Meta loss on this task batch = 2.7928e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 157.3083, GNorm = 0.3234
Meta loss on this task batch = 2.0087e-01, Meta loss averaged over last 500 steps = 2.6042e-01, PNorm = 157.3115, GNorm = 0.3044
Meta loss on this task batch = 2.3790e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 157.3150, GNorm = 0.2906
Meta loss on this task batch = 2.4190e-01, Meta loss averaged over last 500 steps = 2.6031e-01, PNorm = 157.3186, GNorm = 0.2422
Meta loss on this task batch = 2.5462e-01, Meta loss averaged over last 500 steps = 2.6038e-01, PNorm = 157.3223, GNorm = 0.3027
Meta loss on this task batch = 3.0541e-01, Meta loss averaged over last 500 steps = 2.6044e-01, PNorm = 157.3261, GNorm = 0.3155
Meta loss on this task batch = 2.8696e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 157.3300, GNorm = 0.3024
Meta loss on this task batch = 2.4045e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 157.3349, GNorm = 0.3280
Meta loss on this task batch = 2.3236e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 157.3395, GNorm = 0.2729
Meta loss on this task batch = 2.2897e-01, Meta loss averaged over last 500 steps = 2.6042e-01, PNorm = 157.3439, GNorm = 0.2644
Meta loss on this task batch = 2.8265e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 157.3477, GNorm = 0.3180
Meta loss on this task batch = 2.4270e-01, Meta loss averaged over last 500 steps = 2.6035e-01, PNorm = 157.3502, GNorm = 0.3377
Meta loss on this task batch = 2.5638e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 157.3555, GNorm = 0.3449
Took 130.94201731681824 seconds to complete one epoch of meta training
Took 139.12659692764282 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453370
Epoch 971
Meta loss on this task batch = 2.8297e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 157.3612, GNorm = 0.3198
Meta loss on this task batch = 2.6239e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 157.3682, GNorm = 0.2907
Meta loss on this task batch = 2.6827e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 157.3754, GNorm = 0.3148
Meta loss on this task batch = 2.7944e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 157.3835, GNorm = 0.3068
Meta loss on this task batch = 2.0756e-01, Meta loss averaged over last 500 steps = 2.6034e-01, PNorm = 157.3918, GNorm = 0.2456
Meta loss on this task batch = 2.8128e-01, Meta loss averaged over last 500 steps = 2.6032e-01, PNorm = 157.4004, GNorm = 0.3044
Meta loss on this task batch = 2.4815e-01, Meta loss averaged over last 500 steps = 2.6032e-01, PNorm = 157.4088, GNorm = 0.2562
Meta loss on this task batch = 3.2115e-01, Meta loss averaged over last 500 steps = 2.6040e-01, PNorm = 157.4165, GNorm = 0.3533
Meta loss on this task batch = 2.5037e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 157.4227, GNorm = 0.2957
Meta loss on this task batch = 2.6444e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 157.4264, GNorm = 0.3352
Meta loss on this task batch = 2.6237e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 157.4284, GNorm = 0.3282
Meta loss on this task batch = 3.4526e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 157.4306, GNorm = 0.3270
Meta loss on this task batch = 3.2069e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 157.4321, GNorm = 0.3158
Meta loss on this task batch = 2.1762e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 157.4348, GNorm = 0.2427
Meta loss on this task batch = 2.4744e-01, Meta loss averaged over last 500 steps = 2.6048e-01, PNorm = 157.4390, GNorm = 0.2735
Meta loss on this task batch = 2.5779e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 157.4456, GNorm = 0.2644
Meta loss on this task batch = 2.1660e-01, Meta loss averaged over last 500 steps = 2.6030e-01, PNorm = 157.4533, GNorm = 0.2782
Meta loss on this task batch = 2.6638e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 157.4618, GNorm = 0.2687
Meta loss on this task batch = 1.8991e-01, Meta loss averaged over last 500 steps = 2.6010e-01, PNorm = 157.4721, GNorm = 0.2943
Took 129.3109893798828 seconds to complete one epoch of meta training
Took 136.52016258239746 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489967
Epoch 972
Meta loss on this task batch = 2.4456e-01, Meta loss averaged over last 500 steps = 2.6005e-01, PNorm = 157.4819, GNorm = 0.3059
Meta loss on this task batch = 2.4743e-01, Meta loss averaged over last 500 steps = 2.6003e-01, PNorm = 157.4909, GNorm = 0.2548
Meta loss on this task batch = 2.6664e-01, Meta loss averaged over last 500 steps = 2.6000e-01, PNorm = 157.4975, GNorm = 0.2963
Meta loss on this task batch = 3.1604e-01, Meta loss averaged over last 500 steps = 2.6016e-01, PNorm = 157.5027, GNorm = 0.3228
Meta loss on this task batch = 3.0288e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 157.5043, GNorm = 0.3251
Meta loss on this task batch = 2.0461e-01, Meta loss averaged over last 500 steps = 2.6015e-01, PNorm = 157.5062, GNorm = 0.3112
Meta loss on this task batch = 2.9604e-01, Meta loss averaged over last 500 steps = 2.6024e-01, PNorm = 157.5061, GNorm = 0.3537
Meta loss on this task batch = 2.2992e-01, Meta loss averaged over last 500 steps = 2.6021e-01, PNorm = 157.5064, GNorm = 0.2634
Meta loss on this task batch = 2.3625e-01, Meta loss averaged over last 500 steps = 2.6005e-01, PNorm = 157.5075, GNorm = 0.2878
Meta loss on this task batch = 2.8728e-01, Meta loss averaged over last 500 steps = 2.6006e-01, PNorm = 157.5088, GNorm = 0.2867
Meta loss on this task batch = 2.4796e-01, Meta loss averaged over last 500 steps = 2.5993e-01, PNorm = 157.5104, GNorm = 0.2665
Meta loss on this task batch = 2.2402e-01, Meta loss averaged over last 500 steps = 2.5991e-01, PNorm = 157.5141, GNorm = 0.2831
Meta loss on this task batch = 2.1733e-01, Meta loss averaged over last 500 steps = 2.5973e-01, PNorm = 157.5186, GNorm = 0.2764
Meta loss on this task batch = 2.5291e-01, Meta loss averaged over last 500 steps = 2.5980e-01, PNorm = 157.5230, GNorm = 0.3100
Meta loss on this task batch = 3.0738e-01, Meta loss averaged over last 500 steps = 2.5995e-01, PNorm = 157.5255, GNorm = 0.3464
Meta loss on this task batch = 2.6363e-01, Meta loss averaged over last 500 steps = 2.5998e-01, PNorm = 157.5291, GNorm = 0.3032
Meta loss on this task batch = 2.2897e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 157.5328, GNorm = 0.3310
Meta loss on this task batch = 2.6506e-01, Meta loss averaged over last 500 steps = 2.5989e-01, PNorm = 157.5371, GNorm = 0.3033
Meta loss on this task batch = 3.4624e-01, Meta loss averaged over last 500 steps = 2.6008e-01, PNorm = 157.5411, GNorm = 0.4308
Took 131.12820553779602 seconds to complete one epoch of meta training
Took 139.57168817520142 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499025
Epoch 973
Meta loss on this task batch = 3.1728e-01, Meta loss averaged over last 500 steps = 2.6023e-01, PNorm = 157.5442, GNorm = 0.2883
Meta loss on this task batch = 2.3327e-01, Meta loss averaged over last 500 steps = 2.6007e-01, PNorm = 157.5457, GNorm = 0.2743
Meta loss on this task batch = 2.5123e-01, Meta loss averaged over last 500 steps = 2.6006e-01, PNorm = 157.5471, GNorm = 0.2820
Meta loss on this task batch = 2.5324e-01, Meta loss averaged over last 500 steps = 2.6008e-01, PNorm = 157.5493, GNorm = 0.2806
Meta loss on this task batch = 2.8087e-01, Meta loss averaged over last 500 steps = 2.6020e-01, PNorm = 157.5516, GNorm = 0.2931
Meta loss on this task batch = 2.3989e-01, Meta loss averaged over last 500 steps = 2.6020e-01, PNorm = 157.5552, GNorm = 0.2802
Meta loss on this task batch = 2.3123e-01, Meta loss averaged over last 500 steps = 2.6014e-01, PNorm = 157.5574, GNorm = 0.2497
Meta loss on this task batch = 3.1409e-01, Meta loss averaged over last 500 steps = 2.6024e-01, PNorm = 157.5576, GNorm = 0.2922
Meta loss on this task batch = 1.9188e-01, Meta loss averaged over last 500 steps = 2.6014e-01, PNorm = 157.5585, GNorm = 0.2295
Meta loss on this task batch = 1.9812e-01, Meta loss averaged over last 500 steps = 2.6001e-01, PNorm = 157.5599, GNorm = 0.2485
Meta loss on this task batch = 2.2211e-01, Meta loss averaged over last 500 steps = 2.5995e-01, PNorm = 157.5621, GNorm = 0.2543
Meta loss on this task batch = 2.1184e-01, Meta loss averaged over last 500 steps = 2.5997e-01, PNorm = 157.5621, GNorm = 0.3546
Meta loss on this task batch = 2.5551e-01, Meta loss averaged over last 500 steps = 2.5999e-01, PNorm = 157.5612, GNorm = 0.2626
Meta loss on this task batch = 3.1556e-01, Meta loss averaged over last 500 steps = 2.6012e-01, PNorm = 157.5602, GNorm = 0.3309
Meta loss on this task batch = 2.1555e-01, Meta loss averaged over last 500 steps = 2.6001e-01, PNorm = 157.5586, GNorm = 0.3037
Meta loss on this task batch = 2.9412e-01, Meta loss averaged over last 500 steps = 2.6007e-01, PNorm = 157.5572, GNorm = 0.3375
Meta loss on this task batch = 2.8450e-01, Meta loss averaged over last 500 steps = 2.6009e-01, PNorm = 157.5561, GNorm = 0.3185
Meta loss on this task batch = 2.9828e-01, Meta loss averaged over last 500 steps = 2.6007e-01, PNorm = 157.5542, GNorm = 0.3630
Meta loss on this task batch = 2.4402e-01, Meta loss averaged over last 500 steps = 2.6004e-01, PNorm = 157.5533, GNorm = 0.3256
Took 130.25952529907227 seconds to complete one epoch of meta training
Took 138.890132188797 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461268
Epoch 974
Meta loss on this task batch = 1.9322e-01, Meta loss averaged over last 500 steps = 2.5999e-01, PNorm = 157.5550, GNorm = 0.2396
Meta loss on this task batch = 2.6198e-01, Meta loss averaged over last 500 steps = 2.6004e-01, PNorm = 157.5583, GNorm = 0.2833
Meta loss on this task batch = 2.1449e-01, Meta loss averaged over last 500 steps = 2.5983e-01, PNorm = 157.5630, GNorm = 0.2790
Meta loss on this task batch = 2.8868e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 157.5691, GNorm = 0.2932
Meta loss on this task batch = 3.0657e-01, Meta loss averaged over last 500 steps = 2.5997e-01, PNorm = 157.5744, GNorm = 0.3292
Meta loss on this task batch = 2.9894e-01, Meta loss averaged over last 500 steps = 2.6008e-01, PNorm = 157.5808, GNorm = 0.2963
Meta loss on this task batch = 2.4329e-01, Meta loss averaged over last 500 steps = 2.6001e-01, PNorm = 157.5855, GNorm = 0.2969
Meta loss on this task batch = 2.6947e-01, Meta loss averaged over last 500 steps = 2.6016e-01, PNorm = 157.5908, GNorm = 0.2701
Meta loss on this task batch = 2.4548e-01, Meta loss averaged over last 500 steps = 2.6008e-01, PNorm = 157.5957, GNorm = 0.3254
Meta loss on this task batch = 2.8166e-01, Meta loss averaged over last 500 steps = 2.6013e-01, PNorm = 157.5994, GNorm = 0.3491
Meta loss on this task batch = 2.5976e-01, Meta loss averaged over last 500 steps = 2.6011e-01, PNorm = 157.6012, GNorm = 0.3332
Meta loss on this task batch = 2.4936e-01, Meta loss averaged over last 500 steps = 2.6019e-01, PNorm = 157.6032, GNorm = 0.2703
Meta loss on this task batch = 2.9138e-01, Meta loss averaged over last 500 steps = 2.6029e-01, PNorm = 157.6040, GNorm = 0.3197
Meta loss on this task batch = 2.6778e-01, Meta loss averaged over last 500 steps = 2.6039e-01, PNorm = 157.6003, GNorm = 0.3837
Meta loss on this task batch = 2.2362e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 157.5965, GNorm = 0.2386
Meta loss on this task batch = 2.7363e-01, Meta loss averaged over last 500 steps = 2.6035e-01, PNorm = 157.5922, GNorm = 0.3283
Meta loss on this task batch = 2.6237e-01, Meta loss averaged over last 500 steps = 2.6034e-01, PNorm = 157.5883, GNorm = 0.2729
Meta loss on this task batch = 2.8093e-01, Meta loss averaged over last 500 steps = 2.6030e-01, PNorm = 157.5865, GNorm = 0.2938
Meta loss on this task batch = 3.2490e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 157.5859, GNorm = 0.4336
Took 131.92198777198792 seconds to complete one epoch of meta training
Took 140.59200286865234 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478853
Epoch 975
Meta loss on this task batch = 2.5324e-01, Meta loss averaged over last 500 steps = 2.6044e-01, PNorm = 157.5871, GNorm = 0.2799
Meta loss on this task batch = 3.0156e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 157.5906, GNorm = 0.2961
Meta loss on this task batch = 2.1458e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 157.5951, GNorm = 0.2220
Meta loss on this task batch = 2.5908e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 157.6012, GNorm = 0.2782
Meta loss on this task batch = 2.4866e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 157.6094, GNorm = 0.2425
Meta loss on this task batch = 2.9070e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 157.6180, GNorm = 0.3088
Meta loss on this task batch = 2.2600e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 157.6274, GNorm = 0.2684
Meta loss on this task batch = 2.6983e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 157.6351, GNorm = 0.3037
Meta loss on this task batch = 2.3173e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 157.6435, GNorm = 0.2878
Meta loss on this task batch = 2.4803e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 157.6511, GNorm = 0.2807
Meta loss on this task batch = 2.8881e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 157.6562, GNorm = 0.2902
Meta loss on this task batch = 2.3905e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 157.6614, GNorm = 0.3023
Meta loss on this task batch = 2.5311e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 157.6681, GNorm = 0.2899
Meta loss on this task batch = 2.6475e-01, Meta loss averaged over last 500 steps = 2.6050e-01, PNorm = 157.6749, GNorm = 0.2789
Meta loss on this task batch = 2.8151e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 157.6812, GNorm = 0.3193
Meta loss on this task batch = 2.4773e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 157.6870, GNorm = 0.2893
Meta loss on this task batch = 2.8036e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 157.6932, GNorm = 0.2695
Meta loss on this task batch = 2.9082e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 157.7002, GNorm = 0.3280
Meta loss on this task batch = 3.1100e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 157.7055, GNorm = 0.3922
Took 131.30907726287842 seconds to complete one epoch of meta training
Took 139.78408408164978 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482628
Epoch 976
Meta loss on this task batch = 2.1402e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 157.7110, GNorm = 0.2647
Meta loss on this task batch = 2.6482e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 157.7168, GNorm = 0.2847
Meta loss on this task batch = 2.6411e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 157.7219, GNorm = 0.2822
Meta loss on this task batch = 2.6599e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 157.7274, GNorm = 0.2961
Meta loss on this task batch = 2.2991e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 157.7320, GNorm = 0.5467
Meta loss on this task batch = 2.9580e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 157.7364, GNorm = 0.3245
Meta loss on this task batch = 2.9252e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 157.7393, GNorm = 0.3080
Meta loss on this task batch = 2.3342e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 157.7424, GNorm = 0.2778
Meta loss on this task batch = 2.7995e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 157.7457, GNorm = 0.2738
Meta loss on this task batch = 2.0268e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 157.7489, GNorm = 0.2210
Meta loss on this task batch = 2.7830e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 157.7528, GNorm = 0.3134
Meta loss on this task batch = 3.2467e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 157.7566, GNorm = 0.3820
Meta loss on this task batch = 2.7213e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 157.7586, GNorm = 0.3162
Meta loss on this task batch = 2.4044e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 157.7607, GNorm = 0.2799
Meta loss on this task batch = 2.0663e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 157.7629, GNorm = 0.2586
Meta loss on this task batch = 3.3156e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 157.7656, GNorm = 0.2994
Meta loss on this task batch = 2.7500e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 157.7695, GNorm = 0.2776
Meta loss on this task batch = 2.6614e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 157.7745, GNorm = 0.3205
Meta loss on this task batch = 2.6851e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 157.7799, GNorm = 0.2868
Took 133.1288537979126 seconds to complete one epoch of meta training
Took 141.57893657684326 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483316
Epoch 977
Meta loss on this task batch = 2.8334e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 157.7858, GNorm = 0.2922
Meta loss on this task batch = 2.5613e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 157.7928, GNorm = 0.2483
Meta loss on this task batch = 2.8474e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 157.8001, GNorm = 0.2856
Meta loss on this task batch = 2.7129e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 157.8059, GNorm = 0.2691
Meta loss on this task batch = 2.9148e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 157.8116, GNorm = 0.2996
Meta loss on this task batch = 1.9313e-01, Meta loss averaged over last 500 steps = 2.6103e-01, PNorm = 157.8173, GNorm = 0.3269
Meta loss on this task batch = 2.1941e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 157.8222, GNorm = 0.2446
Meta loss on this task batch = 2.4060e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 157.8256, GNorm = 0.2447
Meta loss on this task batch = 2.9972e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 157.8301, GNorm = 0.3320
Meta loss on this task batch = 2.8776e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 157.8345, GNorm = 0.3385
Meta loss on this task batch = 2.4866e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 157.8393, GNorm = 0.2800
Meta loss on this task batch = 2.1918e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 157.8444, GNorm = 0.2985
Meta loss on this task batch = 2.6468e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 157.8493, GNorm = 0.2634
Meta loss on this task batch = 2.2819e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 157.8538, GNorm = 0.2594
Meta loss on this task batch = 2.5434e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 157.8581, GNorm = 0.2696
Meta loss on this task batch = 2.6135e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 157.8608, GNorm = 0.3089
Meta loss on this task batch = 2.8634e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 157.8623, GNorm = 0.3329
Meta loss on this task batch = 2.7558e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 157.8625, GNorm = 0.3179
Meta loss on this task batch = 2.5797e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 157.8629, GNorm = 0.3085
Took 130.91304278373718 seconds to complete one epoch of meta training
Took 139.3552074432373 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478744
Epoch 978
Meta loss on this task batch = 2.7733e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 157.8647, GNorm = 0.3029
Meta loss on this task batch = 2.7237e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 157.8659, GNorm = 0.2660
Meta loss on this task batch = 2.5396e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 157.8669, GNorm = 0.2865
Meta loss on this task batch = 2.8223e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 157.8669, GNorm = 0.3025
Meta loss on this task batch = 2.8214e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 157.8686, GNorm = 0.3463
Meta loss on this task batch = 3.1539e-01, Meta loss averaged over last 500 steps = 2.6106e-01, PNorm = 157.8711, GNorm = 0.3038
Meta loss on this task batch = 3.2703e-01, Meta loss averaged over last 500 steps = 2.6119e-01, PNorm = 157.8709, GNorm = 0.3705
Meta loss on this task batch = 2.8359e-01, Meta loss averaged over last 500 steps = 2.6131e-01, PNorm = 157.8708, GNorm = 0.2835
Meta loss on this task batch = 2.2461e-01, Meta loss averaged over last 500 steps = 2.6121e-01, PNorm = 157.8720, GNorm = 0.2403
Meta loss on this task batch = 2.7278e-01, Meta loss averaged over last 500 steps = 2.6130e-01, PNorm = 157.8740, GNorm = 0.2883
Meta loss on this task batch = 2.3710e-01, Meta loss averaged over last 500 steps = 2.6128e-01, PNorm = 157.8778, GNorm = 0.2692
Meta loss on this task batch = 2.5399e-01, Meta loss averaged over last 500 steps = 2.6132e-01, PNorm = 157.8827, GNorm = 0.2545
Meta loss on this task batch = 2.4149e-01, Meta loss averaged over last 500 steps = 2.6127e-01, PNorm = 157.8875, GNorm = 0.3018
Meta loss on this task batch = 2.9138e-01, Meta loss averaged over last 500 steps = 2.6132e-01, PNorm = 157.8924, GNorm = 0.3080
Meta loss on this task batch = 2.0539e-01, Meta loss averaged over last 500 steps = 2.6125e-01, PNorm = 157.8968, GNorm = 0.2444
Meta loss on this task batch = 2.6450e-01, Meta loss averaged over last 500 steps = 2.6128e-01, PNorm = 157.9018, GNorm = 0.2691
Meta loss on this task batch = 2.6846e-01, Meta loss averaged over last 500 steps = 2.6113e-01, PNorm = 157.9065, GNorm = 0.2629
Meta loss on this task batch = 2.1552e-01, Meta loss averaged over last 500 steps = 2.6104e-01, PNorm = 157.9108, GNorm = 0.2544
Meta loss on this task batch = 2.3101e-01, Meta loss averaged over last 500 steps = 2.6103e-01, PNorm = 157.9151, GNorm = 0.2799
Took 130.97238659858704 seconds to complete one epoch of meta training
Took 139.31248927116394 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498290
Epoch 979
Meta loss on this task batch = 2.9446e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 157.9196, GNorm = 0.3223
Meta loss on this task batch = 2.9949e-01, Meta loss averaged over last 500 steps = 2.6110e-01, PNorm = 157.9223, GNorm = 0.3038
Meta loss on this task batch = 2.2680e-01, Meta loss averaged over last 500 steps = 2.6108e-01, PNorm = 157.9240, GNorm = 0.2881
Meta loss on this task batch = 2.2468e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 157.9251, GNorm = 0.2572
Meta loss on this task batch = 2.7531e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 157.9251, GNorm = 0.2584
Meta loss on this task batch = 2.5532e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 157.9256, GNorm = 0.2500
Meta loss on this task batch = 2.9643e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 157.9235, GNorm = 0.3286
Meta loss on this task batch = 2.4450e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 157.9213, GNorm = 0.2655
Meta loss on this task batch = 2.7666e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 157.9200, GNorm = 0.2709
Meta loss on this task batch = 2.3476e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 157.9203, GNorm = 0.2705
Meta loss on this task batch = 2.4747e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 157.9202, GNorm = 0.2575
Meta loss on this task batch = 2.7149e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 157.9219, GNorm = 0.3257
Meta loss on this task batch = 3.1361e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 157.9249, GNorm = 0.2974
Meta loss on this task batch = 2.1824e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 157.9285, GNorm = 0.2856
Meta loss on this task batch = 2.3819e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 157.9340, GNorm = 0.3018
Meta loss on this task batch = 2.9129e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 157.9406, GNorm = 0.3119
Meta loss on this task batch = 2.2272e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 157.9462, GNorm = 0.2721
Meta loss on this task batch = 2.6242e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 157.9506, GNorm = 0.3502
Meta loss on this task batch = 3.4967e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 157.9537, GNorm = 0.4652
Took 131.9393982887268 seconds to complete one epoch of meta training
Took 140.263179063797 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481056
Epoch 980
Meta loss on this task batch = 2.7661e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 157.9570, GNorm = 0.2663
Meta loss on this task batch = 2.2606e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 157.9621, GNorm = 0.2666
Meta loss on this task batch = 3.2975e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 157.9657, GNorm = 0.3313
Meta loss on this task batch = 3.0418e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 157.9679, GNorm = 0.2870
Meta loss on this task batch = 2.8574e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 157.9702, GNorm = 0.2326
Meta loss on this task batch = 2.1361e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 157.9742, GNorm = 0.2726
Meta loss on this task batch = 3.0601e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 157.9790, GNorm = 0.2800
Meta loss on this task batch = 2.4378e-01, Meta loss averaged over last 500 steps = 2.6095e-01, PNorm = 157.9847, GNorm = 0.2637
Meta loss on this task batch = 2.7975e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 157.9843, GNorm = 1.0000
Meta loss on this task batch = 2.6769e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 157.9850, GNorm = 0.2806
Meta loss on this task batch = 2.6409e-01, Meta loss averaged over last 500 steps = 2.6105e-01, PNorm = 157.9879, GNorm = 0.3296
Meta loss on this task batch = 2.5021e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 157.9915, GNorm = 0.4112
Meta loss on this task batch = 2.6077e-01, Meta loss averaged over last 500 steps = 2.6103e-01, PNorm = 157.9961, GNorm = 0.2648
Meta loss on this task batch = 2.1928e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 158.0015, GNorm = 0.2439
Meta loss on this task batch = 2.2725e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.0067, GNorm = 0.2326
Meta loss on this task batch = 2.6402e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 158.0118, GNorm = 0.2913
Meta loss on this task batch = 2.7029e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 158.0171, GNorm = 0.3290
Meta loss on this task batch = 2.2673e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 158.0243, GNorm = 0.3873
Meta loss on this task batch = 2.5372e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 158.0307, GNorm = 0.2945
Took 130.63891625404358 seconds to complete one epoch of meta training
Took 138.8937633037567 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477468
Epoch 981
Meta loss on this task batch = 2.3157e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 158.0378, GNorm = 0.2909
Meta loss on this task batch = 2.3959e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 158.0445, GNorm = 0.2752
Meta loss on this task batch = 2.7147e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 158.0485, GNorm = 0.3285
Meta loss on this task batch = 1.9841e-01, Meta loss averaged over last 500 steps = 2.6050e-01, PNorm = 158.0541, GNorm = 0.2645
Meta loss on this task batch = 1.9185e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 158.0581, GNorm = 0.2725
Meta loss on this task batch = 2.9117e-01, Meta loss averaged over last 500 steps = 2.6038e-01, PNorm = 158.0624, GNorm = 0.3123
Meta loss on this task batch = 2.7030e-01, Meta loss averaged over last 500 steps = 2.6045e-01, PNorm = 158.0660, GNorm = 0.3148
Meta loss on this task batch = 2.4254e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 158.0701, GNorm = 0.2652
Meta loss on this task batch = 3.0081e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 158.0745, GNorm = 0.2894
Meta loss on this task batch = 2.8213e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 158.0769, GNorm = 0.3166
Meta loss on this task batch = 2.6041e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 158.0787, GNorm = 0.3442
Meta loss on this task batch = 2.6313e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 158.0791, GNorm = 0.3576
Meta loss on this task batch = 2.4895e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 158.0800, GNorm = 0.3164
Meta loss on this task batch = 2.2766e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.0809, GNorm = 0.2700
Meta loss on this task batch = 2.6792e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.0825, GNorm = 0.3196
Meta loss on this task batch = 2.6650e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 158.0852, GNorm = 0.2953
Meta loss on this task batch = 3.4536e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 158.0879, GNorm = 0.4423
Meta loss on this task batch = 2.5039e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 158.0929, GNorm = 0.2706
Meta loss on this task batch = 2.7369e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 158.0992, GNorm = 0.3551
Took 132.2339301109314 seconds to complete one epoch of meta training
Took 140.33187198638916 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468001
Epoch 982
Meta loss on this task batch = 2.4686e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 158.1059, GNorm = 0.2663
Meta loss on this task batch = 2.3927e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 158.1130, GNorm = 0.2770
Meta loss on this task batch = 2.1962e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 158.1223, GNorm = 0.3016
Meta loss on this task batch = 3.0356e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 158.1311, GNorm = 0.3134
Meta loss on this task batch = 3.1945e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.1397, GNorm = 0.3124
Meta loss on this task batch = 2.6177e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 158.1495, GNorm = 0.2731
Meta loss on this task batch = 2.9719e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 158.1574, GNorm = 0.3594
Meta loss on this task batch = 2.6649e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 158.1643, GNorm = 0.3520
Meta loss on this task batch = 2.6621e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 158.1701, GNorm = 0.3201
Meta loss on this task batch = 2.6625e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 158.1740, GNorm = 0.3223
Meta loss on this task batch = 2.7301e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 158.1770, GNorm = 0.3164
Meta loss on this task batch = 2.6329e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 158.1792, GNorm = 0.2939
Meta loss on this task batch = 2.7611e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 158.1808, GNorm = 0.3015
Meta loss on this task batch = 2.5797e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 158.1826, GNorm = 0.3262
Meta loss on this task batch = 2.9992e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 158.1818, GNorm = 0.2992
Meta loss on this task batch = 2.2052e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 158.1823, GNorm = 0.2687
Meta loss on this task batch = 2.3399e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 158.1860, GNorm = 0.4066
Meta loss on this task batch = 2.9436e-01, Meta loss averaged over last 500 steps = 2.6095e-01, PNorm = 158.1889, GNorm = 0.3220
Meta loss on this task batch = 2.5008e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 158.1917, GNorm = 0.3185
Took 130.85598993301392 seconds to complete one epoch of meta training
Took 139.5742678642273 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484829
Epoch 983
Meta loss on this task batch = 2.7755e-01, Meta loss averaged over last 500 steps = 2.6102e-01, PNorm = 158.1944, GNorm = 0.2418
Meta loss on this task batch = 2.5261e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 158.1968, GNorm = 0.3028
Meta loss on this task batch = 2.5875e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 158.2000, GNorm = 0.2463
Meta loss on this task batch = 2.6199e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 158.2029, GNorm = 0.3045
Meta loss on this task batch = 2.4460e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 158.2062, GNorm = 0.2649
Meta loss on this task batch = 1.9061e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 158.2095, GNorm = 0.2425
Meta loss on this task batch = 2.6480e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.2117, GNorm = 0.2693
Meta loss on this task batch = 2.5320e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 158.2129, GNorm = 0.2973
Meta loss on this task batch = 2.4087e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 158.2146, GNorm = 0.2644
Meta loss on this task batch = 2.9346e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 158.2163, GNorm = 0.3081
Meta loss on this task batch = 2.6244e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 158.2156, GNorm = 0.3161
Meta loss on this task batch = 2.8173e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 158.2155, GNorm = 0.3173
Meta loss on this task batch = 2.7912e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 158.2161, GNorm = 0.3085
Meta loss on this task batch = 2.7216e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 158.2182, GNorm = 0.3103
Meta loss on this task batch = 2.6984e-01, Meta loss averaged over last 500 steps = 2.6098e-01, PNorm = 158.2220, GNorm = 0.2641
Meta loss on this task batch = 2.7568e-01, Meta loss averaged over last 500 steps = 2.6108e-01, PNorm = 158.2257, GNorm = 0.2998
Meta loss on this task batch = 2.0898e-01, Meta loss averaged over last 500 steps = 2.6102e-01, PNorm = 158.2296, GNorm = 0.2434
Meta loss on this task batch = 2.2104e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 158.2344, GNorm = 0.3164
Meta loss on this task batch = 3.1787e-01, Meta loss averaged over last 500 steps = 2.6117e-01, PNorm = 158.2398, GNorm = 0.4017
Took 163.11593461036682 seconds to complete one epoch of meta training
Took 171.85080695152283 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.441082
Epoch 984
Meta loss on this task batch = 2.9484e-01, Meta loss averaged over last 500 steps = 2.6121e-01, PNorm = 158.2465, GNorm = 0.3169
Meta loss on this task batch = 2.4314e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 158.2520, GNorm = 0.2740
Meta loss on this task batch = 3.0374e-01, Meta loss averaged over last 500 steps = 2.6107e-01, PNorm = 158.2566, GNorm = 0.3206
Meta loss on this task batch = 2.5937e-01, Meta loss averaged over last 500 steps = 2.6107e-01, PNorm = 158.2604, GNorm = 0.3180
Meta loss on this task batch = 2.4189e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 158.2644, GNorm = 0.2363
Meta loss on this task batch = 2.4837e-01, Meta loss averaged over last 500 steps = 2.6104e-01, PNorm = 158.2665, GNorm = 0.2382
Meta loss on this task batch = 2.5806e-01, Meta loss averaged over last 500 steps = 2.6107e-01, PNorm = 158.2675, GNorm = 0.2864
Meta loss on this task batch = 2.9725e-01, Meta loss averaged over last 500 steps = 2.6111e-01, PNorm = 158.2675, GNorm = 0.3353
Meta loss on this task batch = 2.7768e-01, Meta loss averaged over last 500 steps = 2.6114e-01, PNorm = 158.2677, GNorm = 0.2896
Meta loss on this task batch = 2.6985e-01, Meta loss averaged over last 500 steps = 2.6113e-01, PNorm = 158.2678, GNorm = 0.2773
Meta loss on this task batch = 2.8009e-01, Meta loss averaged over last 500 steps = 2.6114e-01, PNorm = 158.2679, GNorm = 0.3167
Meta loss on this task batch = 1.8495e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 158.2673, GNorm = 0.2318
Meta loss on this task batch = 2.6623e-01, Meta loss averaged over last 500 steps = 2.6098e-01, PNorm = 158.2670, GNorm = 0.2888
Meta loss on this task batch = 2.2476e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 158.2677, GNorm = 0.2727
Meta loss on this task batch = 2.8027e-01, Meta loss averaged over last 500 steps = 2.6106e-01, PNorm = 158.2684, GNorm = 0.3378
Meta loss on this task batch = 2.4828e-01, Meta loss averaged over last 500 steps = 2.6104e-01, PNorm = 158.2687, GNorm = 0.3029
Meta loss on this task batch = 2.5017e-01, Meta loss averaged over last 500 steps = 2.6103e-01, PNorm = 158.2688, GNorm = 0.2461
Meta loss on this task batch = 3.0434e-01, Meta loss averaged over last 500 steps = 2.6108e-01, PNorm = 158.2685, GNorm = 0.2960
Meta loss on this task batch = 2.1705e-01, Meta loss averaged over last 500 steps = 2.6103e-01, PNorm = 158.2690, GNorm = 0.2733
Took 130.7329616546631 seconds to complete one epoch of meta training
Took 139.17567253112793 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486384
Epoch 985
Meta loss on this task batch = 2.6568e-01, Meta loss averaged over last 500 steps = 2.6104e-01, PNorm = 158.2688, GNorm = 0.2677
Meta loss on this task batch = 2.4624e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 158.2683, GNorm = 0.3005
Meta loss on this task batch = 2.1402e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 158.2680, GNorm = 0.2830
Meta loss on this task batch = 2.0290e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 158.2673, GNorm = 0.2783
Meta loss on this task batch = 2.6179e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.2668, GNorm = 0.3390
Meta loss on this task batch = 2.7169e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 158.2667, GNorm = 0.2863
Meta loss on this task batch = 2.4761e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 158.2679, GNorm = 0.2887
Meta loss on this task batch = 2.8405e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 158.2695, GNorm = 0.3207
Meta loss on this task batch = 3.1972e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.2720, GNorm = 0.3090
Meta loss on this task batch = 2.6161e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 158.2739, GNorm = 0.2909
Meta loss on this task batch = 2.9876e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 158.2752, GNorm = 0.3479
Meta loss on this task batch = 2.8837e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 158.2774, GNorm = 0.3407
Meta loss on this task batch = 3.1501e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 158.2821, GNorm = 0.3406
Meta loss on this task batch = 2.2126e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 158.2883, GNorm = 0.2418
Meta loss on this task batch = 2.3714e-01, Meta loss averaged over last 500 steps = 2.6091e-01, PNorm = 158.2954, GNorm = 0.2515
Meta loss on this task batch = 2.4779e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 158.3033, GNorm = 0.2643
Meta loss on this task batch = 2.6848e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 158.3102, GNorm = 0.3168
Meta loss on this task batch = 2.8989e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 158.3160, GNorm = 0.3281
Meta loss on this task batch = 3.0268e-01, Meta loss averaged over last 500 steps = 2.6111e-01, PNorm = 158.3210, GNorm = 0.3285
Took 132.20188879966736 seconds to complete one epoch of meta training
Took 139.68740367889404 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484781
Epoch 986
Meta loss on this task batch = 2.4896e-01, Meta loss averaged over last 500 steps = 2.6115e-01, PNorm = 158.3268, GNorm = 0.2856
Meta loss on this task batch = 2.7178e-01, Meta loss averaged over last 500 steps = 2.6106e-01, PNorm = 158.3318, GNorm = 0.3189
Meta loss on this task batch = 2.3453e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 158.3361, GNorm = 0.2500
Meta loss on this task batch = 2.2109e-01, Meta loss averaged over last 500 steps = 2.6105e-01, PNorm = 158.3403, GNorm = 0.2572
Meta loss on this task batch = 2.2612e-01, Meta loss averaged over last 500 steps = 2.6104e-01, PNorm = 158.3442, GNorm = 0.2618
Meta loss on this task batch = 2.7126e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 158.3469, GNorm = 0.2816
Meta loss on this task batch = 2.8637e-01, Meta loss averaged over last 500 steps = 2.6102e-01, PNorm = 158.3502, GNorm = 0.3179
Meta loss on this task batch = 2.4090e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 158.3535, GNorm = 0.2593
Meta loss on this task batch = 2.9345e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 158.3559, GNorm = 0.2934
Meta loss on this task batch = 2.8453e-01, Meta loss averaged over last 500 steps = 2.6103e-01, PNorm = 158.3570, GNorm = 0.3015
Meta loss on this task batch = 2.5540e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 158.3596, GNorm = 0.2793
Meta loss on this task batch = 1.9340e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 158.3629, GNorm = 0.3760
Meta loss on this task batch = 2.3957e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 158.3664, GNorm = 0.2451
Meta loss on this task batch = 2.6564e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 158.3709, GNorm = 0.3258
Meta loss on this task batch = 2.5875e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 158.3760, GNorm = 0.3359
Meta loss on this task batch = 2.6035e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 158.3819, GNorm = 0.3156
Meta loss on this task batch = 2.5335e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 158.3868, GNorm = 0.2790
Meta loss on this task batch = 2.5609e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 158.3912, GNorm = 0.2355
Meta loss on this task batch = 3.0144e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 158.3949, GNorm = 0.3629
Took 132.94824504852295 seconds to complete one epoch of meta training
Took 140.36113381385803 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464654
Epoch 987
Meta loss on this task batch = 2.6703e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 158.3974, GNorm = 0.2978
Meta loss on this task batch = 2.1347e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 158.4000, GNorm = 0.2568
Meta loss on this task batch = 2.6953e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 158.4028, GNorm = 0.3009
Meta loss on this task batch = 2.7418e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 158.4056, GNorm = 0.2719
Meta loss on this task batch = 2.5735e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 158.4078, GNorm = 0.2764
Meta loss on this task batch = 2.8949e-01, Meta loss averaged over last 500 steps = 2.6110e-01, PNorm = 158.4119, GNorm = 0.3228
Meta loss on this task batch = 2.2147e-01, Meta loss averaged over last 500 steps = 2.6104e-01, PNorm = 158.4157, GNorm = 0.2621
Meta loss on this task batch = 2.8091e-01, Meta loss averaged over last 500 steps = 2.6123e-01, PNorm = 158.4188, GNorm = 0.2944
Meta loss on this task batch = 2.0579e-01, Meta loss averaged over last 500 steps = 2.6105e-01, PNorm = 158.4221, GNorm = 0.2547
Meta loss on this task batch = 2.8072e-01, Meta loss averaged over last 500 steps = 2.6115e-01, PNorm = 158.4247, GNorm = 0.3043
Meta loss on this task batch = 2.4783e-01, Meta loss averaged over last 500 steps = 2.6119e-01, PNorm = 158.4261, GNorm = 0.2657
Meta loss on this task batch = 2.4746e-01, Meta loss averaged over last 500 steps = 2.6117e-01, PNorm = 158.4282, GNorm = 0.3103
Meta loss on this task batch = 2.3682e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 158.4309, GNorm = 0.2594
Meta loss on this task batch = 2.9127e-01, Meta loss averaged over last 500 steps = 2.6118e-01, PNorm = 158.4337, GNorm = 0.3321
Meta loss on this task batch = 2.5898e-01, Meta loss averaged over last 500 steps = 2.6116e-01, PNorm = 158.4368, GNorm = 0.3118
Meta loss on this task batch = 2.5706e-01, Meta loss averaged over last 500 steps = 2.6111e-01, PNorm = 158.4405, GNorm = 0.2603
Meta loss on this task batch = 2.7210e-01, Meta loss averaged over last 500 steps = 2.6107e-01, PNorm = 158.4443, GNorm = 0.2925
Meta loss on this task batch = 2.3129e-01, Meta loss averaged over last 500 steps = 2.6097e-01, PNorm = 158.4484, GNorm = 0.2940
Meta loss on this task batch = 3.3602e-01, Meta loss averaged over last 500 steps = 2.6106e-01, PNorm = 158.4503, GNorm = 0.3878
Took 132.19174933433533 seconds to complete one epoch of meta training
Took 140.50775504112244 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474496
Epoch 988
Meta loss on this task batch = 2.3350e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 158.4512, GNorm = 0.2882
Meta loss on this task batch = 2.4734e-01, Meta loss averaged over last 500 steps = 2.6104e-01, PNorm = 158.4512, GNorm = 0.2576
Meta loss on this task batch = 2.1852e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 158.4528, GNorm = 0.2697
Meta loss on this task batch = 2.5059e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 158.4548, GNorm = 0.2728
Meta loss on this task batch = 2.8766e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 158.4567, GNorm = 0.2465
Meta loss on this task batch = 2.5824e-01, Meta loss averaged over last 500 steps = 2.6098e-01, PNorm = 158.4582, GNorm = 0.2518
Meta loss on this task batch = 2.3358e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 158.4604, GNorm = 0.2530
Meta loss on this task batch = 2.5324e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 158.4628, GNorm = 0.3155
Meta loss on this task batch = 2.7821e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 158.4633, GNorm = 0.3137
Meta loss on this task batch = 2.4827e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.4642, GNorm = 0.2720
Meta loss on this task batch = 2.2744e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 158.4663, GNorm = 0.2766
Meta loss on this task batch = 2.7818e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.4680, GNorm = 0.3207
Meta loss on this task batch = 2.7536e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 158.4702, GNorm = 0.2980
Meta loss on this task batch = 2.9782e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 158.4736, GNorm = 0.3393
Meta loss on this task batch = 2.4477e-01, Meta loss averaged over last 500 steps = 2.6095e-01, PNorm = 158.4799, GNorm = 0.2656
Meta loss on this task batch = 3.1990e-01, Meta loss averaged over last 500 steps = 2.6107e-01, PNorm = 158.4879, GNorm = 0.3266
Meta loss on this task batch = 2.4568e-01, Meta loss averaged over last 500 steps = 2.6105e-01, PNorm = 158.4974, GNorm = 0.2983
Meta loss on this task batch = 2.9321e-01, Meta loss averaged over last 500 steps = 2.6114e-01, PNorm = 158.5054, GNorm = 0.2809
Meta loss on this task batch = 2.4540e-01, Meta loss averaged over last 500 steps = 2.6103e-01, PNorm = 158.5140, GNorm = 0.3227
Took 131.77388095855713 seconds to complete one epoch of meta training
Took 138.99357509613037 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481133
Epoch 989
Meta loss on this task batch = 2.3693e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 158.5225, GNorm = 0.2879
Meta loss on this task batch = 1.9552e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 158.5311, GNorm = 0.2669
Meta loss on this task batch = 2.8321e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 158.5382, GNorm = 0.3575
Meta loss on this task batch = 2.4947e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 158.5454, GNorm = 0.3083
Meta loss on this task batch = 3.1366e-01, Meta loss averaged over last 500 steps = 2.6098e-01, PNorm = 158.5509, GNorm = 0.3555
Meta loss on this task batch = 3.1305e-01, Meta loss averaged over last 500 steps = 2.6106e-01, PNorm = 158.5549, GNorm = 0.3822
Meta loss on this task batch = 2.9171e-01, Meta loss averaged over last 500 steps = 2.6107e-01, PNorm = 158.5593, GNorm = 0.2820
Meta loss on this task batch = 3.0559e-01, Meta loss averaged over last 500 steps = 2.6101e-01, PNorm = 158.5621, GNorm = 0.3174
Meta loss on this task batch = 2.4469e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 158.5628, GNorm = 0.3153
Meta loss on this task batch = 2.8387e-01, Meta loss averaged over last 500 steps = 2.6108e-01, PNorm = 158.5643, GNorm = 0.3248
Meta loss on this task batch = 3.0218e-01, Meta loss averaged over last 500 steps = 2.6122e-01, PNorm = 158.5656, GNorm = 0.2663
Meta loss on this task batch = 2.3170e-01, Meta loss averaged over last 500 steps = 2.6102e-01, PNorm = 158.5678, GNorm = 0.2548
Meta loss on this task batch = 2.6161e-01, Meta loss averaged over last 500 steps = 2.6094e-01, PNorm = 158.5693, GNorm = 0.3124
Meta loss on this task batch = 1.7728e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 158.5730, GNorm = 0.2690
Meta loss on this task batch = 2.4962e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 158.5770, GNorm = 0.2531
Meta loss on this task batch = 2.1274e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.5818, GNorm = 0.2915
Meta loss on this task batch = 2.6405e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 158.5851, GNorm = 0.2903
Meta loss on this task batch = 2.1182e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 158.5890, GNorm = 0.3121
Meta loss on this task batch = 2.4073e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 158.5920, GNorm = 0.3491
Took 131.03857684135437 seconds to complete one epoch of meta training
Took 139.81869196891785 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482415
Epoch 990
Meta loss on this task batch = 2.8187e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 158.5957, GNorm = 0.2939
Meta loss on this task batch = 2.5167e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 158.5984, GNorm = 0.2901
Meta loss on this task batch = 2.1198e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 158.6001, GNorm = 0.2641
Meta loss on this task batch = 1.9728e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 158.6007, GNorm = 0.2837
Meta loss on this task batch = 2.2821e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 158.6007, GNorm = 0.2954
Meta loss on this task batch = 3.0302e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 158.5987, GNorm = 0.4432
Meta loss on this task batch = 2.9849e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 158.5967, GNorm = 0.3440
Meta loss on this task batch = 3.1002e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 158.5941, GNorm = 0.3453
Meta loss on this task batch = 2.7016e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 158.5929, GNorm = 0.2858
Meta loss on this task batch = 2.7846e-01, Meta loss averaged over last 500 steps = 2.6107e-01, PNorm = 158.5923, GNorm = 0.2760
Meta loss on this task batch = 1.9852e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 158.5937, GNorm = 0.2746
Meta loss on this task batch = 2.6515e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 158.5963, GNorm = 0.2954
Meta loss on this task batch = 2.7340e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 158.5973, GNorm = 0.2997
Meta loss on this task batch = 2.6182e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 158.5979, GNorm = 0.3134
Meta loss on this task batch = 3.0514e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 158.5972, GNorm = 0.3025
Meta loss on this task batch = 2.1957e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 158.5973, GNorm = 0.2719
Meta loss on this task batch = 2.5773e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 158.5979, GNorm = 0.2565
Meta loss on this task batch = 2.8650e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 158.5998, GNorm = 0.2804
Meta loss on this task batch = 2.8464e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 158.6010, GNorm = 0.3655
Took 132.67704105377197 seconds to complete one epoch of meta training
Took 139.88904905319214 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483765
Epoch 991
Meta loss on this task batch = 2.5479e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 158.6029, GNorm = 0.2700
Meta loss on this task batch = 2.8046e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 158.6050, GNorm = 0.2891
Meta loss on this task batch = 2.9292e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 158.6072, GNorm = 0.2947
Meta loss on this task batch = 2.4910e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.6099, GNorm = 0.2493
Meta loss on this task batch = 2.5728e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 158.6114, GNorm = 0.2660
Meta loss on this task batch = 2.6540e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 158.6125, GNorm = 0.2487
Meta loss on this task batch = 2.5261e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 158.6136, GNorm = 0.2633
Meta loss on this task batch = 2.6618e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.6136, GNorm = 0.3029
Meta loss on this task batch = 2.7016e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 158.6128, GNorm = 0.2950
Meta loss on this task batch = 2.8132e-01, Meta loss averaged over last 500 steps = 2.6106e-01, PNorm = 158.6114, GNorm = 0.2756
Meta loss on this task batch = 2.5267e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 158.6106, GNorm = 0.2620
Meta loss on this task batch = 2.3045e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 158.6105, GNorm = 0.2565
Meta loss on this task batch = 2.2516e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 158.6123, GNorm = 0.2474
Meta loss on this task batch = 2.3961e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 158.6155, GNorm = 0.2335
Meta loss on this task batch = 2.3479e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 158.6201, GNorm = 0.2730
Meta loss on this task batch = 2.7644e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 158.6238, GNorm = 0.2991
Meta loss on this task batch = 2.5738e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 158.6288, GNorm = 0.2844
Meta loss on this task batch = 2.6379e-01, Meta loss averaged over last 500 steps = 2.6095e-01, PNorm = 158.6352, GNorm = 0.2894
Meta loss on this task batch = 2.9953e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 158.6403, GNorm = 0.4082
Took 131.2504460811615 seconds to complete one epoch of meta training
Took 139.49984669685364 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465347
Epoch 992
Meta loss on this task batch = 2.6204e-01, Meta loss averaged over last 500 steps = 2.6104e-01, PNorm = 158.6461, GNorm = 0.2694
Meta loss on this task batch = 2.4516e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 158.6510, GNorm = 0.2857
Meta loss on this task batch = 2.4595e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 158.6560, GNorm = 0.2951
Meta loss on this task batch = 2.8075e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 158.6604, GNorm = 0.3049
Meta loss on this task batch = 2.4272e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 158.6646, GNorm = 0.2727
Meta loss on this task batch = 2.6620e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.6698, GNorm = 0.2468
Meta loss on this task batch = 2.5814e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 158.6751, GNorm = 0.3079
Meta loss on this task batch = 2.0138e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 158.6783, GNorm = 0.3006
Meta loss on this task batch = 2.6617e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 158.6808, GNorm = 0.3073
Meta loss on this task batch = 2.8342e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 158.6834, GNorm = 0.2902
Meta loss on this task batch = 3.0711e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 158.6850, GNorm = 0.2952
Meta loss on this task batch = 2.6352e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 158.6867, GNorm = 0.2829
Meta loss on this task batch = 2.9443e-01, Meta loss averaged over last 500 steps = 2.6099e-01, PNorm = 158.6893, GNorm = 0.2518
Meta loss on this task batch = 2.7112e-01, Meta loss averaged over last 500 steps = 2.6111e-01, PNorm = 158.6895, GNorm = 0.5113
Meta loss on this task batch = 1.9180e-01, Meta loss averaged over last 500 steps = 2.6100e-01, PNorm = 158.6894, GNorm = 0.2790
Meta loss on this task batch = 2.6754e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 158.6892, GNorm = 0.2536
Meta loss on this task batch = 2.3099e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 158.6881, GNorm = 0.2618
Meta loss on this task batch = 2.1037e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 158.6875, GNorm = 0.2527
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 158.6898, GNorm = 0.3392
Took 129.5797426700592 seconds to complete one epoch of meta training
Took 137.6838321685791 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490805
Epoch 993
Meta loss on this task batch = 2.2735e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 158.6916, GNorm = 0.2523
Meta loss on this task batch = 2.2906e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 158.6951, GNorm = 0.2628
Meta loss on this task batch = 2.4379e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 158.6995, GNorm = 0.2419
Meta loss on this task batch = 2.7168e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 158.7042, GNorm = 0.2836
Meta loss on this task batch = 2.4372e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 158.7095, GNorm = 0.3144
Meta loss on this task batch = 2.1710e-01, Meta loss averaged over last 500 steps = 2.6032e-01, PNorm = 158.7147, GNorm = 0.2845
Meta loss on this task batch = 2.9778e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 158.7201, GNorm = 0.2877
Meta loss on this task batch = 2.7128e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 158.7250, GNorm = 0.3011
Meta loss on this task batch = 2.4197e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 158.7314, GNorm = 0.3068
Meta loss on this task batch = 2.8367e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 158.7382, GNorm = 0.3032
Meta loss on this task batch = 2.2558e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 158.7450, GNorm = 0.2827
Meta loss on this task batch = 2.5512e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 158.7513, GNorm = 0.3134
Meta loss on this task batch = 2.2296e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 158.7578, GNorm = 0.2429
Meta loss on this task batch = 2.8150e-01, Meta loss averaged over last 500 steps = 2.6045e-01, PNorm = 158.7648, GNorm = 0.3130
Meta loss on this task batch = 3.0153e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 158.7721, GNorm = 0.2933
Meta loss on this task batch = 2.4801e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 158.7794, GNorm = 0.2586
Meta loss on this task batch = 2.8933e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 158.7866, GNorm = 0.3072
Meta loss on this task batch = 2.5380e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 158.7945, GNorm = 0.2854
Meta loss on this task batch = 3.3291e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 158.8028, GNorm = 0.3943
Took 131.0575520992279 seconds to complete one epoch of meta training
Took 139.7329888343811 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481521
Epoch 994
Meta loss on this task batch = 3.0364e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 158.8117, GNorm = 0.3241
Meta loss on this task batch = 2.3856e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 158.8197, GNorm = 0.2332
Meta loss on this task batch = 2.8755e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 158.8266, GNorm = 0.3599
Meta loss on this task batch = 2.1710e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 158.8345, GNorm = 0.2589
Meta loss on this task batch = 2.3284e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 158.8430, GNorm = 0.2635
Meta loss on this task batch = 2.1393e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 158.8513, GNorm = 0.2493
Meta loss on this task batch = 2.6884e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 158.8583, GNorm = 0.2614
Meta loss on this task batch = 2.5460e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 158.8628, GNorm = 0.2932
Meta loss on this task batch = 2.5096e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 158.8660, GNorm = 0.2798
Meta loss on this task batch = 2.2183e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 158.8689, GNorm = 0.2720
Meta loss on this task batch = 2.2996e-01, Meta loss averaged over last 500 steps = 2.6050e-01, PNorm = 158.8728, GNorm = 0.2535
Meta loss on this task batch = 2.7667e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 158.8762, GNorm = 0.3408
Meta loss on this task batch = 2.8874e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 158.8802, GNorm = 0.2939
Meta loss on this task batch = 2.5765e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 158.8837, GNorm = 0.2996
Meta loss on this task batch = 2.5937e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 158.8875, GNorm = 0.3058
Meta loss on this task batch = 2.9998e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 158.8904, GNorm = 0.2963
Meta loss on this task batch = 3.1336e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 158.8931, GNorm = 0.3096
Meta loss on this task batch = 2.9313e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 158.8946, GNorm = 0.3886
Meta loss on this task batch = 2.3098e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 158.8951, GNorm = 0.3790
Took 131.97738599777222 seconds to complete one epoch of meta training
Took 140.18485522270203 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487567
Epoch 995
Meta loss on this task batch = 2.1822e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 158.8978, GNorm = 0.2580
Meta loss on this task batch = 2.6166e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.9006, GNorm = 0.3353
Meta loss on this task batch = 2.9698e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 158.9048, GNorm = 0.3151
Meta loss on this task batch = 2.8465e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 158.9101, GNorm = 0.3075
Meta loss on this task batch = 2.6353e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 158.9160, GNorm = 0.3126
Meta loss on this task batch = 3.1478e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 158.9224, GNorm = 0.2796
Meta loss on this task batch = 1.9953e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 158.9293, GNorm = 0.2749
Meta loss on this task batch = 2.9403e-01, Meta loss averaged over last 500 steps = 2.6087e-01, PNorm = 158.9362, GNorm = 0.2820
Meta loss on this task batch = 2.2656e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 158.9431, GNorm = 0.2901
Meta loss on this task batch = 2.3459e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 158.9483, GNorm = 0.2344
Meta loss on this task batch = 2.4573e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 158.9521, GNorm = 0.2661
Meta loss on this task batch = 2.5076e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 158.9541, GNorm = 0.2647
Meta loss on this task batch = 2.7389e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 158.9549, GNorm = 0.2936
Meta loss on this task batch = 2.2608e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 158.9548, GNorm = 0.2504
Meta loss on this task batch = 2.1691e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 158.9531, GNorm = 0.2885
Meta loss on this task batch = 3.4913e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 158.9487, GNorm = 0.3387
Meta loss on this task batch = 2.7501e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 158.9452, GNorm = 0.2869
Meta loss on this task batch = 2.7193e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 158.9421, GNorm = 0.3068
Meta loss on this task batch = 2.5207e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 158.9395, GNorm = 0.3277
Took 131.56662940979004 seconds to complete one epoch of meta training
Took 139.77202582359314 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471691
Epoch 996
Meta loss on this task batch = 2.6443e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 158.9381, GNorm = 0.2639
Meta loss on this task batch = 2.7893e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 158.9367, GNorm = 0.2932
Meta loss on this task batch = 1.8992e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 158.9366, GNorm = 0.2465
Meta loss on this task batch = 2.2899e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 158.9383, GNorm = 0.2784
Meta loss on this task batch = 2.7034e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 158.9404, GNorm = 0.2880
Meta loss on this task batch = 2.6579e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 158.9450, GNorm = 0.2856
Meta loss on this task batch = 1.9042e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 158.9510, GNorm = 0.2932
Meta loss on this task batch = 2.5181e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 158.9590, GNorm = 0.2990
Meta loss on this task batch = 2.9558e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 158.9689, GNorm = 0.3000
Meta loss on this task batch = 2.5579e-01, Meta loss averaged over last 500 steps = 2.6044e-01, PNorm = 158.9766, GNorm = 0.2741
Meta loss on this task batch = 2.7861e-01, Meta loss averaged over last 500 steps = 2.6048e-01, PNorm = 158.9838, GNorm = 0.2729
Meta loss on this task batch = 2.3560e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 158.9914, GNorm = 0.2493
Meta loss on this task batch = 2.7890e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 158.9994, GNorm = 0.2995
Meta loss on this task batch = 2.8181e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 159.0059, GNorm = 0.2993
Meta loss on this task batch = 3.0166e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 159.0127, GNorm = 0.3418
Meta loss on this task batch = 2.6843e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 159.0188, GNorm = 0.2585
Meta loss on this task batch = 2.9973e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 159.0243, GNorm = 0.3408
Meta loss on this task batch = 2.2986e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 159.0292, GNorm = 0.2812
Meta loss on this task batch = 2.0780e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 159.0341, GNorm = 0.2963
Took 130.98106145858765 seconds to complete one epoch of meta training
Took 139.61073398590088 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509035
Epoch 997
Meta loss on this task batch = 2.7847e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 159.0382, GNorm = 0.3498
Meta loss on this task batch = 2.2579e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 159.0432, GNorm = 0.2784
Meta loss on this task batch = 2.6035e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 159.0476, GNorm = 0.2926
Meta loss on this task batch = 2.3479e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 159.0527, GNorm = 0.3014
Meta loss on this task batch = 2.2359e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 159.0585, GNorm = 0.2708
Meta loss on this task batch = 3.0691e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 159.0618, GNorm = 0.3811
Meta loss on this task batch = 2.8110e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 159.0663, GNorm = 0.3236
Meta loss on this task batch = 2.8092e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 159.0702, GNorm = 0.2672
Meta loss on this task batch = 2.2912e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 159.0741, GNorm = 0.2923
Meta loss on this task batch = 2.8182e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 159.0765, GNorm = 0.2659
Meta loss on this task batch = 2.8186e-01, Meta loss averaged over last 500 steps = 2.6079e-01, PNorm = 159.0789, GNorm = 0.3211
Meta loss on this task batch = 2.6702e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 159.0806, GNorm = 0.3160
Meta loss on this task batch = 2.7022e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 159.0801, GNorm = 0.3863
Meta loss on this task batch = 2.4405e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 159.0794, GNorm = 0.2655
Meta loss on this task batch = 2.6140e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 159.0808, GNorm = 0.3173
Meta loss on this task batch = 2.5118e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 159.0818, GNorm = 0.2574
Meta loss on this task batch = 2.9430e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 159.0825, GNorm = 0.3296
Meta loss on this task batch = 2.4411e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 159.0827, GNorm = 0.2458
Meta loss on this task batch = 2.6332e-01, Meta loss averaged over last 500 steps = 2.6040e-01, PNorm = 159.0836, GNorm = 0.3612
Took 129.69919347763062 seconds to complete one epoch of meta training
Took 138.55171585083008 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484309
Epoch 998
Meta loss on this task batch = 2.4635e-01, Meta loss averaged over last 500 steps = 2.6045e-01, PNorm = 159.0842, GNorm = 0.2328
Meta loss on this task batch = 2.3824e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 159.0856, GNorm = 0.2815
Meta loss on this task batch = 2.4419e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 159.0866, GNorm = 0.2583
Meta loss on this task batch = 2.0266e-01, Meta loss averaged over last 500 steps = 2.6038e-01, PNorm = 159.0878, GNorm = 0.2403
Meta loss on this task batch = 3.0584e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 159.0896, GNorm = 0.3170
Meta loss on this task batch = 2.6752e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 159.0933, GNorm = 0.2722
Meta loss on this task batch = 2.7344e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 159.0970, GNorm = 0.2864
Meta loss on this task batch = 2.7848e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 159.1004, GNorm = 0.3185
Meta loss on this task batch = 2.5198e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 159.1035, GNorm = 0.2808
Meta loss on this task batch = 2.3618e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 159.1058, GNorm = 0.2760
Meta loss on this task batch = 2.6205e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 159.1083, GNorm = 0.2621
Meta loss on this task batch = 2.5980e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 159.1105, GNorm = 0.3210
Meta loss on this task batch = 2.5197e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 159.1131, GNorm = 0.3581
Meta loss on this task batch = 2.9986e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 159.1163, GNorm = 0.3093
Meta loss on this task batch = 2.0120e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 159.1198, GNorm = 0.2669
Meta loss on this task batch = 2.3637e-01, Meta loss averaged over last 500 steps = 2.6045e-01, PNorm = 159.1238, GNorm = 0.2302
Meta loss on this task batch = 2.8924e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 159.1267, GNorm = 0.2938
Meta loss on this task batch = 2.5378e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 159.1299, GNorm = 0.2811
Meta loss on this task batch = 2.2462e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 159.1319, GNorm = 0.3041
Took 128.16598749160767 seconds to complete one epoch of meta training
Took 136.70521140098572 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479943
Epoch 999
Meta loss on this task batch = 2.5496e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 159.1340, GNorm = 0.2737
Meta loss on this task batch = 2.9984e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 159.1358, GNorm = 0.2551
Meta loss on this task batch = 2.2470e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 159.1376, GNorm = 0.2348
Meta loss on this task batch = 2.4586e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 159.1388, GNorm = 0.2515
Meta loss on this task batch = 2.6283e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 159.1417, GNorm = 0.3113
Meta loss on this task batch = 2.3610e-01, Meta loss averaged over last 500 steps = 2.6033e-01, PNorm = 159.1449, GNorm = 0.2666
Meta loss on this task batch = 2.3473e-01, Meta loss averaged over last 500 steps = 2.6016e-01, PNorm = 159.1476, GNorm = 0.3002
Meta loss on this task batch = 2.4969e-01, Meta loss averaged over last 500 steps = 2.6020e-01, PNorm = 159.1508, GNorm = 0.2949
Meta loss on this task batch = 3.1372e-01, Meta loss averaged over last 500 steps = 2.6032e-01, PNorm = 159.1533, GNorm = 0.3315
Meta loss on this task batch = 2.3836e-01, Meta loss averaged over last 500 steps = 2.6029e-01, PNorm = 159.1569, GNorm = 0.2869
Meta loss on this task batch = 2.8857e-01, Meta loss averaged over last 500 steps = 2.6031e-01, PNorm = 159.1614, GNorm = 0.3264
Meta loss on this task batch = 2.6910e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 159.1649, GNorm = 0.2976
Meta loss on this task batch = 2.3189e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 159.1685, GNorm = 0.3245
Meta loss on this task batch = 2.5805e-01, Meta loss averaged over last 500 steps = 2.6026e-01, PNorm = 159.1720, GNorm = 0.3096
Meta loss on this task batch = 2.4608e-01, Meta loss averaged over last 500 steps = 2.6036e-01, PNorm = 159.1753, GNorm = 0.2911
Meta loss on this task batch = 2.3760e-01, Meta loss averaged over last 500 steps = 2.6044e-01, PNorm = 159.1778, GNorm = 0.2744
Meta loss on this task batch = 2.5740e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 159.1802, GNorm = 0.2927
Meta loss on this task batch = 2.9408e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 159.1821, GNorm = 0.4464
Meta loss on this task batch = 2.7184e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 159.1852, GNorm = 0.3534
Took 129.73617792129517 seconds to complete one epoch of meta training
Took 138.15006828308105 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465061
Best validation prc-auc = 0.539620 on epoch 224
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Beginning meta testing
Meta testing on task: CHEMBL1794355
New best model for test task CHEMBL1794355 at epoch 1 with val loss 0.36229977011680603
Val loss: 0.36238718032836914
Val loss: 0.3624773919582367
Val loss: 0.3625963628292084
Val loss: 0.3626795709133148
Val loss: 0.3627573549747467
Val loss: 0.3628324568271637
Val loss: 0.36291080713272095
Val loss: 0.36296775937080383
Val loss: 0.3630606532096863
Val loss: 0.36314857006073
Val loss: 0.36326315999031067
Val loss: 0.3633321225643158
Val loss: 0.3633989989757538
Val loss: 0.36348584294319153
Val loss: 0.36354124546051025
Val loss: 0.3636176288127899
Val loss: 0.3636761009693146
Val loss: 0.3637559413909912
Val loss: 0.36383846402168274
Val loss: 0.36394110321998596
Val loss: 0.36402812600135803
Val loss: 0.3641267418861389
Val loss: 0.3642060160636902
Val loss: 0.3642828166484833
Val loss: 0.36437734961509705
Val loss: 0.364459753036499
Val loss: 0.36452141404151917
Val loss: 0.36459338665008545
Val loss: 0.3646550476551056
Finished early stopping for task CHEMBL1794355, beginning testing
Meta testing on task: CHEMBL2098499
New best model for test task CHEMBL2098499 at epoch 1 with val loss 0.9775980710983276
New best model for test task CHEMBL2098499 at epoch 2 with val loss 0.9757036566734314
New best model for test task CHEMBL2098499 at epoch 3 with val loss 0.9737889766693115
New best model for test task CHEMBL2098499 at epoch 4 with val loss 0.971778929233551
New best model for test task CHEMBL2098499 at epoch 5 with val loss 0.9698099493980408
New best model for test task CHEMBL2098499 at epoch 6 with val loss 0.9678162336349487
New best model for test task CHEMBL2098499 at epoch 7 with val loss 0.9660294055938721
New best model for test task CHEMBL2098499 at epoch 8 with val loss 0.9640434384346008
New best model for test task CHEMBL2098499 at epoch 9 with val loss 0.9620465636253357
New best model for test task CHEMBL2098499 at epoch 10 with val loss 0.9599552750587463
New best model for test task CHEMBL2098499 at epoch 11 with val loss 0.9578944444656372
New best model for test task CHEMBL2098499 at epoch 12 with val loss 0.9556078314781189
New best model for test task CHEMBL2098499 at epoch 13 with val loss 0.953468918800354
New best model for test task CHEMBL2098499 at epoch 14 with val loss 0.9511301517486572
New best model for test task CHEMBL2098499 at epoch 15 with val loss 0.9490073919296265
New best model for test task CHEMBL2098499 at epoch 16 with val loss 0.9467030167579651
New best model for test task CHEMBL2098499 at epoch 17 with val loss 0.9445639848709106
New best model for test task CHEMBL2098499 at epoch 18 with val loss 0.9426158666610718
New best model for test task CHEMBL2098499 at epoch 19 with val loss 0.9404508471488953
New best model for test task CHEMBL2098499 at epoch 20 with val loss 0.9384905099868774
New best model for test task CHEMBL2098499 at epoch 21 with val loss 0.9365048408508301
New best model for test task CHEMBL2098499 at epoch 22 with val loss 0.9343743920326233
New best model for test task CHEMBL2098499 at epoch 23 with val loss 0.9324101805686951
New best model for test task CHEMBL2098499 at epoch 24 with val loss 0.9304794073104858
New best model for test task CHEMBL2098499 at epoch 25 with val loss 0.9284716844558716
New best model for test task CHEMBL2098499 at epoch 26 with val loss 0.9264370203018188
New best model for test task CHEMBL2098499 at epoch 27 with val loss 0.9244623780250549
New best model for test task CHEMBL2098499 at epoch 28 with val loss 0.922282338142395
New best model for test task CHEMBL2098499 at epoch 29 with val loss 0.9203470349311829
New best model for test task CHEMBL2098499 at epoch 30 with val loss 0.918394923210144
Finished early stopping for task CHEMBL2098499, beginning testing
Meta testing on task: CHEMBL1738131
New best model for test task CHEMBL1738131 at epoch 1 with val loss 0.8908330798149109
New best model for test task CHEMBL1738131 at epoch 2 with val loss 0.8829571008682251
Val loss: 0.9104575216770172
New best model for test task CHEMBL1738131 at epoch 4 with val loss 0.8621174097061157
Val loss: 0.9035396873950958
New best model for test task CHEMBL1738131 at epoch 6 with val loss 0.8269471526145935
Val loss: 0.8728544116020203
Val loss: 0.8450118899345398
Val loss: 0.8787091672420502
Val loss: 0.8545645475387573
New best model for test task CHEMBL1738131 at epoch 11 with val loss 0.8194990754127502
Val loss: 0.853188157081604
Val loss: 0.8290247917175293
Val loss: 0.8327824473381042
New best model for test task CHEMBL1738131 at epoch 15 with val loss 0.815477192401886
Val loss: 0.8285889029502869
New best model for test task CHEMBL1738131 at epoch 17 with val loss 0.8072901964187622
Val loss: 0.8299819231033325
New best model for test task CHEMBL1738131 at epoch 19 with val loss 0.7925691306591034
Val loss: 0.7992008328437805
Val loss: 0.7981814742088318
New best model for test task CHEMBL1738131 at epoch 22 with val loss 0.7783933579921722
Val loss: 0.7951418161392212
Val loss: 0.8090994656085968
Val loss: 0.8158709108829498
New best model for test task CHEMBL1738131 at epoch 26 with val loss 0.7646057605743408
Val loss: 0.7982147932052612
Val loss: 0.803557276725769
Val loss: 0.8062601089477539
Val loss: 0.7662734389305115
Finished early stopping for task CHEMBL1738131, beginning testing
Meta testing on task: CHEMBL918058
New best model for test task CHEMBL918058 at epoch 1 with val loss 1.1047253608703613
New best model for test task CHEMBL918058 at epoch 2 with val loss 1.083027958869934
New best model for test task CHEMBL918058 at epoch 3 with val loss 1.0633288621902466
New best model for test task CHEMBL918058 at epoch 4 with val loss 1.0447628498077393
New best model for test task CHEMBL918058 at epoch 5 with val loss 1.0279407501220703
New best model for test task CHEMBL918058 at epoch 6 with val loss 1.0120760202407837
New best model for test task CHEMBL918058 at epoch 7 with val loss 0.9978632926940918
New best model for test task CHEMBL918058 at epoch 8 with val loss 0.9845971465110779
New best model for test task CHEMBL918058 at epoch 9 with val loss 0.9735288619995117
New best model for test task CHEMBL918058 at epoch 10 with val loss 0.9632430672645569
New best model for test task CHEMBL918058 at epoch 11 with val loss 0.9532529711723328
New best model for test task CHEMBL918058 at epoch 12 with val loss 0.9442446827888489
New best model for test task CHEMBL918058 at epoch 13 with val loss 0.9358534812927246
New best model for test task CHEMBL918058 at epoch 14 with val loss 0.9276707768440247
New best model for test task CHEMBL918058 at epoch 15 with val loss 0.9206361174583435
New best model for test task CHEMBL918058 at epoch 16 with val loss 0.9130069017410278
New best model for test task CHEMBL918058 at epoch 17 with val loss 0.9068910479545593
New best model for test task CHEMBL918058 at epoch 18 with val loss 0.9008180499076843
New best model for test task CHEMBL918058 at epoch 19 with val loss 0.8954427242279053
New best model for test task CHEMBL918058 at epoch 20 with val loss 0.8902708888053894
New best model for test task CHEMBL918058 at epoch 21 with val loss 0.8854818344116211
New best model for test task CHEMBL918058 at epoch 22 with val loss 0.8806785345077515
New best model for test task CHEMBL918058 at epoch 23 with val loss 0.876166820526123
New best model for test task CHEMBL918058 at epoch 24 with val loss 0.8721863627433777
New best model for test task CHEMBL918058 at epoch 25 with val loss 0.8679570555686951
New best model for test task CHEMBL918058 at epoch 26 with val loss 0.8637761473655701
New best model for test task CHEMBL918058 at epoch 27 with val loss 0.8596515655517578
New best model for test task CHEMBL918058 at epoch 28 with val loss 0.8559237718582153
New best model for test task CHEMBL918058 at epoch 29 with val loss 0.8524773716926575
New best model for test task CHEMBL918058 at epoch 30 with val loss 0.8488114476203918
Finished early stopping for task CHEMBL918058, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL2095143
New best model for test task CHEMBL2095143 at epoch 1 with val loss 1.2095106840133667
New best model for test task CHEMBL2095143 at epoch 2 with val loss 1.2002049684524536
New best model for test task CHEMBL2095143 at epoch 3 with val loss 1.1917457580566406
New best model for test task CHEMBL2095143 at epoch 4 with val loss 1.1838467121124268
New best model for test task CHEMBL2095143 at epoch 5 with val loss 1.1765798330307007
New best model for test task CHEMBL2095143 at epoch 6 with val loss 1.169562816619873
New best model for test task CHEMBL2095143 at epoch 7 with val loss 1.1629921197891235
New best model for test task CHEMBL2095143 at epoch 8 with val loss 1.1567949056625366
New best model for test task CHEMBL2095143 at epoch 9 with val loss 1.1510409116744995
New best model for test task CHEMBL2095143 at epoch 10 with val loss 1.1456209421157837
New best model for test task CHEMBL2095143 at epoch 11 with val loss 1.1404088735580444
New best model for test task CHEMBL2095143 at epoch 12 with val loss 1.1354939937591553
New best model for test task CHEMBL2095143 at epoch 13 with val loss 1.1307382583618164
New best model for test task CHEMBL2095143 at epoch 14 with val loss 1.1262032985687256
New best model for test task CHEMBL2095143 at epoch 15 with val loss 1.1219589710235596
New best model for test task CHEMBL2095143 at epoch 16 with val loss 1.1178979873657227
New best model for test task CHEMBL2095143 at epoch 17 with val loss 1.1138756275177002
New best model for test task CHEMBL2095143 at epoch 18 with val loss 1.109971046447754
New best model for test task CHEMBL2095143 at epoch 19 with val loss 1.1061460971832275
New best model for test task CHEMBL2095143 at epoch 20 with val loss 1.1025573015213013
New best model for test task CHEMBL2095143 at epoch 21 with val loss 1.0990526676177979
New best model for test task CHEMBL2095143 at epoch 22 with val loss 1.0957804918289185
New best model for test task CHEMBL2095143 at epoch 23 with val loss 1.0926886796951294
New best model for test task CHEMBL2095143 at epoch 24 with val loss 1.0895546674728394
New best model for test task CHEMBL2095143 at epoch 25 with val loss 1.0865362882614136
New best model for test task CHEMBL2095143 at epoch 26 with val loss 1.0836254358291626
New best model for test task CHEMBL2095143 at epoch 27 with val loss 1.0806330442428589
New best model for test task CHEMBL2095143 at epoch 28 with val loss 1.0777790546417236
New best model for test task CHEMBL2095143 at epoch 29 with val loss 1.0748987197875977
New best model for test task CHEMBL2095143 at epoch 30 with val loss 1.072153925895691
Finished early stopping for task CHEMBL2095143, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1614170
New best model for test task CHEMBL1614170 at epoch 1 with val loss 0.8444671332836151
New best model for test task CHEMBL1614170 at epoch 2 with val loss 0.8443838953971863
New best model for test task CHEMBL1614170 at epoch 3 with val loss 0.8433384597301483
New best model for test task CHEMBL1614170 at epoch 4 with val loss 0.8267873227596283
Val loss: 0.8283833265304565
Val loss: 0.835590660572052
New best model for test task CHEMBL1614170 at epoch 7 with val loss 0.8163730800151825
Val loss: 0.8177765905857086
New best model for test task CHEMBL1614170 at epoch 9 with val loss 0.8115310370922089
New best model for test task CHEMBL1614170 at epoch 10 with val loss 0.8091295659542084
Val loss: 0.8128689527511597
New best model for test task CHEMBL1614170 at epoch 12 with val loss 0.7917341887950897
Val loss: 0.8270377516746521
Val loss: 0.8032976686954498
Val loss: 0.8094010055065155
Val loss: 0.7957284152507782
New best model for test task CHEMBL1614170 at epoch 17 with val loss 0.7876063585281372
Val loss: 0.810470312833786
Val loss: 0.796294242143631
Val loss: 0.8108443319797516
New best model for test task CHEMBL1614170 at epoch 21 with val loss 0.78684002161026
New best model for test task CHEMBL1614170 at epoch 22 with val loss 0.7731935977935791
Val loss: 0.7883085012435913
Val loss: 0.7832241952419281
Val loss: 0.7830159962177277
New best model for test task CHEMBL1614170 at epoch 26 with val loss 0.7677973508834839
Val loss: 0.7812044024467468
Val loss: 0.7847088277339935
Val loss: 0.7755850553512573
Val loss: 0.7763193547725677
Finished early stopping for task CHEMBL1614170, beginning testing
Meta testing on task: CHEMBL1963705
New best model for test task CHEMBL1963705 at epoch 1 with val loss 0.6263297398885092
New best model for test task CHEMBL1963705 at epoch 2 with val loss 0.6126066346963247
Val loss: 0.6133454144001007
Val loss: 0.7818339268366495
Val loss: 0.6198199391365051
Val loss: 0.6642219622929891
Val loss: 0.6209713419278463
Val loss: 0.6633627613385519
Val loss: 0.7017731070518494
Val loss: 0.6525533199310303
Val loss: 0.705875019232432
New best model for test task CHEMBL1963705 at epoch 12 with val loss 0.6081845362981161
Val loss: 0.7523650725682577
Val loss: 0.6087952057520548
New best model for test task CHEMBL1963705 at epoch 15 with val loss 0.5944991211096445
Val loss: 0.607223629951477
Val loss: 0.6989198327064514
Val loss: 0.6733486453692118
Val loss: 0.5991696317990621
Val loss: 0.6466282804807028
Val loss: 0.598600169022878
Val loss: 0.6129633188247681
Val loss: 0.6320076386133829
New best model for test task CHEMBL1963705 at epoch 24 with val loss 0.580922544002533
Val loss: 0.5821222066879272
Val loss: 0.6285081704457601
Val loss: 0.5891930858294169
Val loss: 0.5833407839139303
Val loss: 0.6579036315282186
Val loss: 0.59378449122111
Finished early stopping for task CHEMBL1963705, beginning testing
Meta testing on task: CHEMBL1909212
New best model for test task CHEMBL1909212 at epoch 1 with val loss 1.1559346119562786
New best model for test task CHEMBL1909212 at epoch 2 with val loss 1.1314698060353596
New best model for test task CHEMBL1909212 at epoch 3 with val loss 1.0884329477945964
New best model for test task CHEMBL1909212 at epoch 4 with val loss 1.0670037666956584
New best model for test task CHEMBL1909212 at epoch 5 with val loss 1.0205315947532654
New best model for test task CHEMBL1909212 at epoch 6 with val loss 0.9997552037239075
New best model for test task CHEMBL1909212 at epoch 7 with val loss 0.9898352026939392
New best model for test task CHEMBL1909212 at epoch 8 with val loss 0.9722962776819865
New best model for test task CHEMBL1909212 at epoch 9 with val loss 0.9550032019615173
New best model for test task CHEMBL1909212 at epoch 10 with val loss 0.9405076106389364
New best model for test task CHEMBL1909212 at epoch 11 with val loss 0.9290043711662292
New best model for test task CHEMBL1909212 at epoch 12 with val loss 0.9162637790044149
New best model for test task CHEMBL1909212 at epoch 13 with val loss 0.9054020444552103
New best model for test task CHEMBL1909212 at epoch 14 with val loss 0.9051594336827596
New best model for test task CHEMBL1909212 at epoch 15 with val loss 0.8887979785601298
New best model for test task CHEMBL1909212 at epoch 16 with val loss 0.8849960565567017
New best model for test task CHEMBL1909212 at epoch 17 with val loss 0.8774290283521017
New best model for test task CHEMBL1909212 at epoch 18 with val loss 0.8720160325368246
New best model for test task CHEMBL1909212 at epoch 19 with val loss 0.8578865130742391
New best model for test task CHEMBL1909212 at epoch 20 with val loss 0.851099689801534
Val loss: 0.8533417185147604
New best model for test task CHEMBL1909212 at epoch 22 with val loss 0.8487784663836161
New best model for test task CHEMBL1909212 at epoch 23 with val loss 0.8381602565447489
New best model for test task CHEMBL1909212 at epoch 24 with val loss 0.8330264091491699
New best model for test task CHEMBL1909212 at epoch 25 with val loss 0.8315655787785848
New best model for test task CHEMBL1909212 at epoch 26 with val loss 0.8242001930872599
New best model for test task CHEMBL1909212 at epoch 27 with val loss 0.8201557596524557
New best model for test task CHEMBL1909212 at epoch 28 with val loss 0.806896448135376
Val loss: 0.8137705524762472
New best model for test task CHEMBL1909212 at epoch 30 with val loss 0.7999866803487142
Finished early stopping for task CHEMBL1909212, beginning testing
Meta testing on task: CHEMBL2114797
New best model for test task CHEMBL2114797 at epoch 1 with val loss 0.8462771773338318
New best model for test task CHEMBL2114797 at epoch 2 with val loss 0.8456448912620544
New best model for test task CHEMBL2114797 at epoch 3 with val loss 0.8449715375900269
New best model for test task CHEMBL2114797 at epoch 4 with val loss 0.8443617820739746
New best model for test task CHEMBL2114797 at epoch 5 with val loss 0.8437142372131348
New best model for test task CHEMBL2114797 at epoch 6 with val loss 0.8429734706878662
New best model for test task CHEMBL2114797 at epoch 7 with val loss 0.8421745300292969
New best model for test task CHEMBL2114797 at epoch 8 with val loss 0.8414729833602905
New best model for test task CHEMBL2114797 at epoch 9 with val loss 0.840739369392395
New best model for test task CHEMBL2114797 at epoch 10 with val loss 0.8400348424911499
New best model for test task CHEMBL2114797 at epoch 11 with val loss 0.8392711281776428
New best model for test task CHEMBL2114797 at epoch 12 with val loss 0.83863365650177
New best model for test task CHEMBL2114797 at epoch 13 with val loss 0.8380936980247498
New best model for test task CHEMBL2114797 at epoch 14 with val loss 0.8375510573387146
New best model for test task CHEMBL2114797 at epoch 15 with val loss 0.8369724154472351
New best model for test task CHEMBL2114797 at epoch 16 with val loss 0.8364537358283997
New best model for test task CHEMBL2114797 at epoch 17 with val loss 0.8358298540115356
New best model for test task CHEMBL2114797 at epoch 18 with val loss 0.8351887464523315
New best model for test task CHEMBL2114797 at epoch 19 with val loss 0.8346571922302246
New best model for test task CHEMBL2114797 at epoch 20 with val loss 0.8341107964515686
New best model for test task CHEMBL2114797 at epoch 21 with val loss 0.8335627913475037
New best model for test task CHEMBL2114797 at epoch 22 with val loss 0.8330332636833191
New best model for test task CHEMBL2114797 at epoch 23 with val loss 0.8325241804122925
New best model for test task CHEMBL2114797 at epoch 24 with val loss 0.832028329372406
New best model for test task CHEMBL2114797 at epoch 25 with val loss 0.8315329551696777
New best model for test task CHEMBL2114797 at epoch 26 with val loss 0.8310337066650391
New best model for test task CHEMBL2114797 at epoch 27 with val loss 0.8304658532142639
New best model for test task CHEMBL2114797 at epoch 28 with val loss 0.8299612998962402
New best model for test task CHEMBL2114797 at epoch 29 with val loss 0.8295060396194458
New best model for test task CHEMBL2114797 at epoch 30 with val loss 0.8289769887924194
Finished early stopping for task CHEMBL2114797, beginning testing
Meta testing on task: CHEMBL1963934
New best model for test task CHEMBL1963934 at epoch 1 with val loss 0.33613014221191406
New best model for test task CHEMBL1963934 at epoch 2 with val loss 0.3361125588417053
New best model for test task CHEMBL1963934 at epoch 3 with val loss 0.3361048400402069
New best model for test task CHEMBL1963934 at epoch 4 with val loss 0.33607637882232666
New best model for test task CHEMBL1963934 at epoch 5 with val loss 0.3360685706138611
New best model for test task CHEMBL1963934 at epoch 6 with val loss 0.3360571265220642
New best model for test task CHEMBL1963934 at epoch 7 with val loss 0.336039662361145
New best model for test task CHEMBL1963934 at epoch 8 with val loss 0.33602410554885864
New best model for test task CHEMBL1963934 at epoch 9 with val loss 0.3359767198562622
New best model for test task CHEMBL1963934 at epoch 10 with val loss 0.3359706401824951
New best model for test task CHEMBL1963934 at epoch 11 with val loss 0.33596011996269226
New best model for test task CHEMBL1963934 at epoch 12 with val loss 0.3359532952308655
New best model for test task CHEMBL1963934 at epoch 13 with val loss 0.3359244167804718
New best model for test task CHEMBL1963934 at epoch 14 with val loss 0.3359077572822571
New best model for test task CHEMBL1963934 at epoch 15 with val loss 0.33587390184402466
New best model for test task CHEMBL1963934 at epoch 16 with val loss 0.33584827184677124
New best model for test task CHEMBL1963934 at epoch 17 with val loss 0.33579882979393005
New best model for test task CHEMBL1963934 at epoch 18 with val loss 0.3357831835746765
Val loss: 0.3358604311943054
Val loss: 0.33584219217300415
Val loss: 0.3358337879180908
Val loss: 0.33584582805633545
Val loss: 0.3358120024204254
New best model for test task CHEMBL1963934 at epoch 24 with val loss 0.33576589822769165
Val loss: 0.3358818590641022
Val loss: 0.3358604311943054
Val loss: 0.33584022521972656
Val loss: 0.3357834219932556
New best model for test task CHEMBL1963934 at epoch 29 with val loss 0.3357284665107727
New best model for test task CHEMBL1963934 at epoch 30 with val loss 0.335701048374176
Finished early stopping for task CHEMBL1963934, beginning testing
Meta testing on task: CHEMBL1909209
New best model for test task CHEMBL1909209 at epoch 1 with val loss 1.1053224007288616
New best model for test task CHEMBL1909209 at epoch 2 with val loss 1.0779517889022827
New best model for test task CHEMBL1909209 at epoch 3 with val loss 1.0574082533518474
New best model for test task CHEMBL1909209 at epoch 4 with val loss 1.0280181765556335
New best model for test task CHEMBL1909209 at epoch 5 with val loss 1.0040377577145894
New best model for test task CHEMBL1909209 at epoch 6 with val loss 1.0013631582260132
New best model for test task CHEMBL1909209 at epoch 7 with val loss 0.9692631165186564
New best model for test task CHEMBL1909209 at epoch 8 with val loss 0.9595938523610433
New best model for test task CHEMBL1909209 at epoch 9 with val loss 0.9479838609695435
New best model for test task CHEMBL1909209 at epoch 10 with val loss 0.9393537839253744
New best model for test task CHEMBL1909209 at epoch 11 with val loss 0.9284769694010416
Val loss: 0.930220345656077
New best model for test task CHEMBL1909209 at epoch 13 with val loss 0.908483107884725
New best model for test task CHEMBL1909209 at epoch 14 with val loss 0.8908601800600687
Val loss: 0.8916569352149963
New best model for test task CHEMBL1909209 at epoch 16 with val loss 0.8856847087542216
New best model for test task CHEMBL1909209 at epoch 17 with val loss 0.8739409844080607
New best model for test task CHEMBL1909209 at epoch 18 with val loss 0.8677031000455221
New best model for test task CHEMBL1909209 at epoch 19 with val loss 0.863445520401001
New best model for test task CHEMBL1909209 at epoch 20 with val loss 0.8525802493095398
New best model for test task CHEMBL1909209 at epoch 21 with val loss 0.8518514633178711
New best model for test task CHEMBL1909209 at epoch 22 with val loss 0.8442153533299764
New best model for test task CHEMBL1909209 at epoch 23 with val loss 0.8391959071159363
New best model for test task CHEMBL1909209 at epoch 24 with val loss 0.8324581782023112
New best model for test task CHEMBL1909209 at epoch 25 with val loss 0.8188390533129374
Val loss: 0.8188475171724955
New best model for test task CHEMBL1909209 at epoch 27 with val loss 0.8168988227844238
New best model for test task CHEMBL1909209 at epoch 28 with val loss 0.8117995063463846
New best model for test task CHEMBL1909209 at epoch 29 with val loss 0.8046426177024841
New best model for test task CHEMBL1909209 at epoch 30 with val loss 0.8027842044830322
Finished early stopping for task CHEMBL1909209, beginning testing
Meta testing on task: CHEMBL1909211
New best model for test task CHEMBL1909211 at epoch 1 with val loss 1.1255026658376057
New best model for test task CHEMBL1909211 at epoch 2 with val loss 1.0934024651845295
New best model for test task CHEMBL1909211 at epoch 3 with val loss 1.047018011411031
New best model for test task CHEMBL1909211 at epoch 4 with val loss 1.0409693320592244
New best model for test task CHEMBL1909211 at epoch 5 with val loss 1.0265202522277832
New best model for test task CHEMBL1909211 at epoch 6 with val loss 0.9987424612045288
New best model for test task CHEMBL1909211 at epoch 7 with val loss 0.9655899206797282
Val loss: 0.9763587117195129
New best model for test task CHEMBL1909211 at epoch 9 with val loss 0.9459959069887797
New best model for test task CHEMBL1909211 at epoch 10 with val loss 0.9406424760818481
New best model for test task CHEMBL1909211 at epoch 11 with val loss 0.9236794511477152
New best model for test task CHEMBL1909211 at epoch 12 with val loss 0.9175480405489603
New best model for test task CHEMBL1909211 at epoch 13 with val loss 0.8937581578890482
Val loss: 0.9030093550682068
New best model for test task CHEMBL1909211 at epoch 15 with val loss 0.8818333943684896
Val loss: 0.882569412390391
Val loss: 0.8826771775881449
New best model for test task CHEMBL1909211 at epoch 18 with val loss 0.8597509066263834
New best model for test task CHEMBL1909211 at epoch 19 with val loss 0.8584051728248596
New best model for test task CHEMBL1909211 at epoch 20 with val loss 0.8480473955472311
New best model for test task CHEMBL1909211 at epoch 21 with val loss 0.8467561602592468
New best model for test task CHEMBL1909211 at epoch 22 with val loss 0.8417250712712606
New best model for test task CHEMBL1909211 at epoch 23 with val loss 0.8354085087776184
New best model for test task CHEMBL1909211 at epoch 24 with val loss 0.830788771311442
New best model for test task CHEMBL1909211 at epoch 25 with val loss 0.8198866645495096
New best model for test task CHEMBL1909211 at epoch 26 with val loss 0.8162908355394999
New best model for test task CHEMBL1909211 at epoch 27 with val loss 0.8126794497172037
Val loss: 0.8170769611994425
New best model for test task CHEMBL1909211 at epoch 29 with val loss 0.8001957734425863
Val loss: 0.810084859530131
Finished early stopping for task CHEMBL1909211, beginning testing
Meta testing on task: CHEMBL1909085
New best model for test task CHEMBL1909085 at epoch 1 with val loss 1.0505566199620564
New best model for test task CHEMBL1909085 at epoch 2 with val loss 1.0108919739723206
New best model for test task CHEMBL1909085 at epoch 3 with val loss 0.9834315578142802
New best model for test task CHEMBL1909085 at epoch 4 with val loss 0.9581177830696106
New best model for test task CHEMBL1909085 at epoch 5 with val loss 0.956642766793569
New best model for test task CHEMBL1909085 at epoch 6 with val loss 0.9487163225809733
New best model for test task CHEMBL1909085 at epoch 7 with val loss 0.9269484480222067
New best model for test task CHEMBL1909085 at epoch 8 with val loss 0.9144206841786703
New best model for test task CHEMBL1909085 at epoch 9 with val loss 0.9011389414469401
New best model for test task CHEMBL1909085 at epoch 10 with val loss 0.8931093811988831
New best model for test task CHEMBL1909085 at epoch 11 with val loss 0.886127253373464
Val loss: 0.8865918517112732
New best model for test task CHEMBL1909085 at epoch 13 with val loss 0.8773739139238993
New best model for test task CHEMBL1909085 at epoch 14 with val loss 0.8650253216425577
New best model for test task CHEMBL1909085 at epoch 15 with val loss 0.857229471206665
New best model for test task CHEMBL1909085 at epoch 16 with val loss 0.8431205749511719
Val loss: 0.8508877158164978
New best model for test task CHEMBL1909085 at epoch 18 with val loss 0.8172170519828796
Val loss: 0.8445060849189758
Val loss: 0.831721027692159
Val loss: 0.8236300150553385
New best model for test task CHEMBL1909085 at epoch 22 with val loss 0.8067259987195333
Val loss: 0.8113890687624613
Val loss: 0.8167818586031595
New best model for test task CHEMBL1909085 at epoch 25 with val loss 0.7960376938184103
Val loss: 0.799802323182424
Val loss: 0.8063022891680399
Val loss: 0.7966269652048746
New best model for test task CHEMBL1909085 at epoch 29 with val loss 0.77928493420283
Val loss: 0.7872364322344462
Finished early stopping for task CHEMBL1909085, beginning testing
Meta testing on task: CHEMBL1738202
New best model for test task CHEMBL1738202 at epoch 1 with val loss 0.40333831310272217
New best model for test task CHEMBL1738202 at epoch 2 with val loss 0.40320855379104614
New best model for test task CHEMBL1738202 at epoch 3 with val loss 0.40315067768096924
New best model for test task CHEMBL1738202 at epoch 4 with val loss 0.4030856490135193
New best model for test task CHEMBL1738202 at epoch 5 with val loss 0.4030367136001587
New best model for test task CHEMBL1738202 at epoch 6 with val loss 0.4030132591724396
New best model for test task CHEMBL1738202 at epoch 7 with val loss 0.4029242694377899
New best model for test task CHEMBL1738202 at epoch 8 with val loss 0.40277692675590515
New best model for test task CHEMBL1738202 at epoch 9 with val loss 0.40273743867874146
New best model for test task CHEMBL1738202 at epoch 10 with val loss 0.40269485116004944
New best model for test task CHEMBL1738202 at epoch 11 with val loss 0.4026038944721222
New best model for test task CHEMBL1738202 at epoch 12 with val loss 0.4025459587574005
New best model for test task CHEMBL1738202 at epoch 13 with val loss 0.402488112449646
New best model for test task CHEMBL1738202 at epoch 14 with val loss 0.40244051814079285
New best model for test task CHEMBL1738202 at epoch 15 with val loss 0.4023512601852417
New best model for test task CHEMBL1738202 at epoch 16 with val loss 0.40228864550590515
New best model for test task CHEMBL1738202 at epoch 17 with val loss 0.4022444486618042
New best model for test task CHEMBL1738202 at epoch 18 with val loss 0.40219491720199585
New best model for test task CHEMBL1738202 at epoch 19 with val loss 0.40212395787239075
New best model for test task CHEMBL1738202 at epoch 20 with val loss 0.4020386338233948
New best model for test task CHEMBL1738202 at epoch 21 with val loss 0.4020252525806427
New best model for test task CHEMBL1738202 at epoch 22 with val loss 0.40190544724464417
New best model for test task CHEMBL1738202 at epoch 23 with val loss 0.4018719792366028
New best model for test task CHEMBL1738202 at epoch 24 with val loss 0.4018199145793915
New best model for test task CHEMBL1738202 at epoch 25 with val loss 0.4017415940761566
New best model for test task CHEMBL1738202 at epoch 26 with val loss 0.4016677439212799
New best model for test task CHEMBL1738202 at epoch 27 with val loss 0.401590496301651
New best model for test task CHEMBL1738202 at epoch 28 with val loss 0.4015127122402191
New best model for test task CHEMBL1738202 at epoch 29 with val loss 0.4014560282230377
New best model for test task CHEMBL1738202 at epoch 30 with val loss 0.4013464152812958
Finished early stopping for task CHEMBL1738202, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1738019
New best model for test task CHEMBL1738019 at epoch 1 with val loss 0.5164516568183899
Val loss: 0.5169607400894165
Val loss: 0.5172821283340454
Val loss: 0.5176231265068054
Val loss: 0.517968475818634
Val loss: 0.5184921622276306
Val loss: 0.5187909603118896
Val loss: 0.5192397832870483
Val loss: 0.5196311473846436
Val loss: 0.5202661752700806
Val loss: 0.5208240747451782
Val loss: 0.5212847590446472
Val loss: 0.521751880645752
Val loss: 0.5221685767173767
Val loss: 0.5225166082382202
Val loss: 0.5228402614593506
Val loss: 0.5231252908706665
Val loss: 0.5235119462013245
Val loss: 0.5237563252449036
Val loss: 0.524177074432373
Val loss: 0.5245112776756287
Val loss: 0.5248153209686279
Val loss: 0.5252105593681335
Val loss: 0.5255249738693237
Val loss: 0.5257698893547058
Val loss: 0.5261456966400146
Val loss: 0.5264737606048584
Val loss: 0.5268721580505371
Val loss: 0.5271836519241333
Val loss: 0.5274745225906372
Finished early stopping for task CHEMBL1738019, beginning testing
Meta testing on task: CHEMBL1909092
New best model for test task CHEMBL1909092 at epoch 1 with val loss 1.2119630972544353
New best model for test task CHEMBL1909092 at epoch 2 with val loss 1.1973313490549724
New best model for test task CHEMBL1909092 at epoch 3 with val loss 1.1765137910842896
New best model for test task CHEMBL1909092 at epoch 4 with val loss 1.1546754042307537
New best model for test task CHEMBL1909092 at epoch 5 with val loss 1.1389103730519612
New best model for test task CHEMBL1909092 at epoch 6 with val loss 1.12397297223409
New best model for test task CHEMBL1909092 at epoch 7 with val loss 1.0965798298517864
New best model for test task CHEMBL1909092 at epoch 8 with val loss 1.0848061243693035
New best model for test task CHEMBL1909092 at epoch 9 with val loss 1.072420358657837
New best model for test task CHEMBL1909092 at epoch 10 with val loss 1.056131362915039
New best model for test task CHEMBL1909092 at epoch 11 with val loss 1.0394368569056194
New best model for test task CHEMBL1909092 at epoch 12 with val loss 1.0288957357406616
New best model for test task CHEMBL1909092 at epoch 13 with val loss 1.022889773050944
New best model for test task CHEMBL1909092 at epoch 14 with val loss 1.0072744290033977
New best model for test task CHEMBL1909092 at epoch 15 with val loss 0.9880359570185343
Val loss: 0.9993300636609396
New best model for test task CHEMBL1909092 at epoch 17 with val loss 0.9798926115036011
New best model for test task CHEMBL1909092 at epoch 18 with val loss 0.9704873561859131
New best model for test task CHEMBL1909092 at epoch 19 with val loss 0.955308735370636
Val loss: 0.958105226357778
New best model for test task CHEMBL1909092 at epoch 21 with val loss 0.9389709035555521
New best model for test task CHEMBL1909092 at epoch 22 with val loss 0.9384299715360006
New best model for test task CHEMBL1909092 at epoch 23 with val loss 0.9321163495381674
New best model for test task CHEMBL1909092 at epoch 24 with val loss 0.9280408422152201
New best model for test task CHEMBL1909092 at epoch 25 with val loss 0.916021724541982
New best model for test task CHEMBL1909092 at epoch 26 with val loss 0.8984386126200358
Val loss: 0.8993575572967529
New best model for test task CHEMBL1909092 at epoch 28 with val loss 0.8980505267779032
New best model for test task CHEMBL1909092 at epoch 29 with val loss 0.8831962545712789
New best model for test task CHEMBL1909092 at epoch 30 with val loss 0.8771423498789469
Finished early stopping for task CHEMBL1909092, beginning testing
Meta testing on task: CHEMBL1909192
New best model for test task CHEMBL1909192 at epoch 1 with val loss 1.21205468972524
New best model for test task CHEMBL1909192 at epoch 2 with val loss 1.168585220972697
New best model for test task CHEMBL1909192 at epoch 3 with val loss 1.1580673058827717
New best model for test task CHEMBL1909192 at epoch 4 with val loss 1.1279586950937908
New best model for test task CHEMBL1909192 at epoch 5 with val loss 1.117181936899821
New best model for test task CHEMBL1909192 at epoch 6 with val loss 1.0893869002660115
New best model for test task CHEMBL1909192 at epoch 7 with val loss 1.0837502479553223
New best model for test task CHEMBL1909192 at epoch 8 with val loss 1.061149795850118
New best model for test task CHEMBL1909192 at epoch 9 with val loss 1.0459070603052776
New best model for test task CHEMBL1909192 at epoch 10 with val loss 1.0309178431828816
New best model for test task CHEMBL1909192 at epoch 11 with val loss 1.021613876024882
New best model for test task CHEMBL1909192 at epoch 12 with val loss 1.01161793867747
New best model for test task CHEMBL1909192 at epoch 13 with val loss 0.9937261740366617
New best model for test task CHEMBL1909192 at epoch 14 with val loss 0.9817637006441752
New best model for test task CHEMBL1909192 at epoch 15 with val loss 0.9712339838345846
New best model for test task CHEMBL1909192 at epoch 16 with val loss 0.9583013852437338
New best model for test task CHEMBL1909192 at epoch 17 with val loss 0.9510616064071655
New best model for test task CHEMBL1909192 at epoch 18 with val loss 0.9465408325195312
New best model for test task CHEMBL1909192 at epoch 19 with val loss 0.9287531574567159
New best model for test task CHEMBL1909192 at epoch 20 with val loss 0.9276647965113322
New best model for test task CHEMBL1909192 at epoch 21 with val loss 0.9146257042884827
Val loss: 0.9161953528722128
New best model for test task CHEMBL1909192 at epoch 23 with val loss 0.9101736545562744
New best model for test task CHEMBL1909192 at epoch 24 with val loss 0.8993515372276306
New best model for test task CHEMBL1909192 at epoch 25 with val loss 0.8812652826309204
New best model for test task CHEMBL1909192 at epoch 26 with val loss 0.8746758103370667
Val loss: 0.8783018787701925
New best model for test task CHEMBL1909192 at epoch 28 with val loss 0.8706137339274088
New best model for test task CHEMBL1909192 at epoch 29 with val loss 0.8615654309590658
Val loss: 0.8647461732228597
Finished early stopping for task CHEMBL1909192, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1614359
New best model for test task CHEMBL1614359 at epoch 1 with val loss 0.8125702440738678
New best model for test task CHEMBL1614359 at epoch 2 with val loss 0.7823345363140106
New best model for test task CHEMBL1614359 at epoch 3 with val loss 0.7641468942165375
New best model for test task CHEMBL1614359 at epoch 4 with val loss 0.6997623145580292
Val loss: 0.7679865062236786
Val loss: 0.8925463557243347
Val loss: 0.8061577677726746
Val loss: 0.8640220165252686
Val loss: 0.8547679781913757
Val loss: 0.816987931728363
Val loss: 0.7663054168224335
Val loss: 0.734859973192215
Val loss: 0.7248785495758057
Val loss: 0.7435234785079956
Val loss: 0.7449307441711426
Val loss: 0.7835161089897156
Val loss: 0.7423055171966553
Val loss: 0.79664346575737
Val loss: 0.8064576387405396
Val loss: 0.7787502408027649
Val loss: 0.758147120475769
New best model for test task CHEMBL1614359 at epoch 22 with val loss 0.6144471615552902
Val loss: 0.7162156999111176
Val loss: 0.8150196671485901
Val loss: 0.7745316028594971
Val loss: 0.7141440808773041
Val loss: 0.6864326894283295
Val loss: 0.7353448271751404
Val loss: 0.7415769994258881
Val loss: 0.7674753665924072
Finished early stopping for task CHEMBL1614359, beginning testing
Meta testing on task: CHEMBL2028077
New best model for test task CHEMBL2028077 at epoch 1 with val loss 1.1963651180267334
New best model for test task CHEMBL2028077 at epoch 2 with val loss 1.1856715679168701
New best model for test task CHEMBL2028077 at epoch 3 with val loss 1.1750826835632324
New best model for test task CHEMBL2028077 at epoch 4 with val loss 1.1655244827270508
New best model for test task CHEMBL2028077 at epoch 5 with val loss 1.1564366817474365
New best model for test task CHEMBL2028077 at epoch 6 with val loss 1.147844672203064
New best model for test task CHEMBL2028077 at epoch 7 with val loss 1.1396484375
New best model for test task CHEMBL2028077 at epoch 8 with val loss 1.1320347785949707
New best model for test task CHEMBL2028077 at epoch 9 with val loss 1.1241329908370972
New best model for test task CHEMBL2028077 at epoch 10 with val loss 1.1165746450424194
New best model for test task CHEMBL2028077 at epoch 11 with val loss 1.1091430187225342
New best model for test task CHEMBL2028077 at epoch 12 with val loss 1.101920485496521
New best model for test task CHEMBL2028077 at epoch 13 with val loss 1.0947270393371582
New best model for test task CHEMBL2028077 at epoch 14 with val loss 1.0879300832748413
New best model for test task CHEMBL2028077 at epoch 15 with val loss 1.0815417766571045
New best model for test task CHEMBL2028077 at epoch 16 with val loss 1.0748852491378784
New best model for test task CHEMBL2028077 at epoch 17 with val loss 1.0685482025146484
New best model for test task CHEMBL2028077 at epoch 18 with val loss 1.0625150203704834
New best model for test task CHEMBL2028077 at epoch 19 with val loss 1.0568225383758545
New best model for test task CHEMBL2028077 at epoch 20 with val loss 1.0510740280151367
New best model for test task CHEMBL2028077 at epoch 21 with val loss 1.045824408531189
New best model for test task CHEMBL2028077 at epoch 22 with val loss 1.0404586791992188
New best model for test task CHEMBL2028077 at epoch 23 with val loss 1.0349457263946533
New best model for test task CHEMBL2028077 at epoch 24 with val loss 1.0297901630401611
New best model for test task CHEMBL2028077 at epoch 25 with val loss 1.0244518518447876
New best model for test task CHEMBL2028077 at epoch 26 with val loss 1.0195560455322266
New best model for test task CHEMBL2028077 at epoch 27 with val loss 1.0148404836654663
New best model for test task CHEMBL2028077 at epoch 28 with val loss 1.0101313591003418
New best model for test task CHEMBL2028077 at epoch 29 with val loss 1.005456805229187
New best model for test task CHEMBL2028077 at epoch 30 with val loss 1.0009652376174927
Finished early stopping for task CHEMBL2028077, beginning testing
Meta testing on task: CHEMBL1794358
New best model for test task CHEMBL1794358 at epoch 1 with val loss 1.1859318017959595
New best model for test task CHEMBL1794358 at epoch 2 with val loss 1.1783839464187622
New best model for test task CHEMBL1794358 at epoch 3 with val loss 1.1711091995239258
New best model for test task CHEMBL1794358 at epoch 4 with val loss 1.1641596555709839
New best model for test task CHEMBL1794358 at epoch 5 with val loss 1.1571526527404785
New best model for test task CHEMBL1794358 at epoch 6 with val loss 1.150330901145935
New best model for test task CHEMBL1794358 at epoch 7 with val loss 1.1439403295516968
New best model for test task CHEMBL1794358 at epoch 8 with val loss 1.1375505924224854
New best model for test task CHEMBL1794358 at epoch 9 with val loss 1.1313024759292603
New best model for test task CHEMBL1794358 at epoch 10 with val loss 1.1253166198730469
New best model for test task CHEMBL1794358 at epoch 11 with val loss 1.119611382484436
New best model for test task CHEMBL1794358 at epoch 12 with val loss 1.1140575408935547
New best model for test task CHEMBL1794358 at epoch 13 with val loss 1.1082607507705688
New best model for test task CHEMBL1794358 at epoch 14 with val loss 1.1027252674102783
New best model for test task CHEMBL1794358 at epoch 15 with val loss 1.0972706079483032
New best model for test task CHEMBL1794358 at epoch 16 with val loss 1.0915275812149048
New best model for test task CHEMBL1794358 at epoch 17 with val loss 1.085941195487976
New best model for test task CHEMBL1794358 at epoch 18 with val loss 1.080521583557129
New best model for test task CHEMBL1794358 at epoch 19 with val loss 1.0751551389694214
New best model for test task CHEMBL1794358 at epoch 20 with val loss 1.0701676607131958
New best model for test task CHEMBL1794358 at epoch 21 with val loss 1.065028429031372
New best model for test task CHEMBL1794358 at epoch 22 with val loss 1.0599312782287598
New best model for test task CHEMBL1794358 at epoch 23 with val loss 1.0550962686538696
New best model for test task CHEMBL1794358 at epoch 24 with val loss 1.050336480140686
New best model for test task CHEMBL1794358 at epoch 25 with val loss 1.0459072589874268
New best model for test task CHEMBL1794358 at epoch 26 with val loss 1.0413382053375244
New best model for test task CHEMBL1794358 at epoch 27 with val loss 1.0369415283203125
New best model for test task CHEMBL1794358 at epoch 28 with val loss 1.0327593088150024
New best model for test task CHEMBL1794358 at epoch 29 with val loss 1.0285392999649048
New best model for test task CHEMBL1794358 at epoch 30 with val loss 1.0242185592651367
Finished early stopping for task CHEMBL1794358, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1738021
New best model for test task CHEMBL1738021 at epoch 1 with val loss 0.42394742369651794
Val loss: 0.42421460151672363
Val loss: 0.42451807856559753
Val loss: 0.42476171255111694
Val loss: 0.4250117838382721
Val loss: 0.42530861496925354
Val loss: 0.42565691471099854
Val loss: 0.425904244184494
Val loss: 0.42617812752723694
Val loss: 0.42638546228408813
Val loss: 0.4266403019428253
Val loss: 0.426943838596344
Val loss: 0.4271997809410095
Val loss: 0.4274410605430603
Val loss: 0.4276435077190399
Val loss: 0.4278716444969177
Val loss: 0.4281003475189209
Val loss: 0.4283390939235687
Val loss: 0.42862755060195923
Val loss: 0.42889657616615295
Val loss: 0.4291554391384125
Val loss: 0.42944762110710144
Val loss: 0.42966416478157043
Val loss: 0.4298761188983917
Val loss: 0.43010810017585754
Val loss: 0.43030044436454773
Val loss: 0.43057578802108765
Val loss: 0.43073609471321106
Val loss: 0.4309093654155731
Val loss: 0.4310726523399353
Finished early stopping for task CHEMBL1738021, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL3215116
New best model for test task CHEMBL3215116 at epoch 1 with val loss 1.0550696849822998
New best model for test task CHEMBL3215116 at epoch 2 with val loss 1.048171877861023
New best model for test task CHEMBL3215116 at epoch 3 with val loss 1.0415619611740112
New best model for test task CHEMBL3215116 at epoch 4 with val loss 1.0355801582336426
New best model for test task CHEMBL3215116 at epoch 5 with val loss 1.0295848846435547
New best model for test task CHEMBL3215116 at epoch 6 with val loss 1.0231678485870361
New best model for test task CHEMBL3215116 at epoch 7 with val loss 1.0173044204711914
New best model for test task CHEMBL3215116 at epoch 8 with val loss 1.0120258331298828
New best model for test task CHEMBL3215116 at epoch 9 with val loss 1.0067367553710938
New best model for test task CHEMBL3215116 at epoch 10 with val loss 1.0013359785079956
New best model for test task CHEMBL3215116 at epoch 11 with val loss 0.9961039423942566
New best model for test task CHEMBL3215116 at epoch 12 with val loss 0.9912120699882507
New best model for test task CHEMBL3215116 at epoch 13 with val loss 0.9861845970153809
New best model for test task CHEMBL3215116 at epoch 14 with val loss 0.9814914464950562
New best model for test task CHEMBL3215116 at epoch 15 with val loss 0.9766573905944824
New best model for test task CHEMBL3215116 at epoch 16 with val loss 0.9725643992424011
New best model for test task CHEMBL3215116 at epoch 17 with val loss 0.9679795503616333
New best model for test task CHEMBL3215116 at epoch 18 with val loss 0.9635491371154785
New best model for test task CHEMBL3215116 at epoch 19 with val loss 0.9593572616577148
New best model for test task CHEMBL3215116 at epoch 20 with val loss 0.9555726051330566
New best model for test task CHEMBL3215116 at epoch 21 with val loss 0.9515632390975952
New best model for test task CHEMBL3215116 at epoch 22 with val loss 0.947568416595459
New best model for test task CHEMBL3215116 at epoch 23 with val loss 0.9440488815307617
New best model for test task CHEMBL3215116 at epoch 24 with val loss 0.9404811859130859
New best model for test task CHEMBL3215116 at epoch 25 with val loss 0.9368783831596375
New best model for test task CHEMBL3215116 at epoch 26 with val loss 0.933556079864502
New best model for test task CHEMBL3215116 at epoch 27 with val loss 0.9301535487174988
New best model for test task CHEMBL3215116 at epoch 28 with val loss 0.9266104698181152
New best model for test task CHEMBL3215116 at epoch 29 with val loss 0.9232906103134155
New best model for test task CHEMBL3215116 at epoch 30 with val loss 0.9200190901756287
Finished early stopping for task CHEMBL3215116, beginning testing
Meta testing on task: CHEMBL1614202
New best model for test task CHEMBL1614202 at epoch 1 with val loss 0.3301294147968292
Val loss: 0.33034178614616394
Val loss: 0.33055397868156433
Val loss: 0.3307410180568695
Val loss: 0.3309164345264435
Val loss: 0.3310859203338623
Val loss: 0.3312363922595978
Val loss: 0.33138999342918396
Val loss: 0.3315623700618744
Val loss: 0.33172595500946045
Val loss: 0.33189404010772705
Val loss: 0.33203333616256714
Val loss: 0.3322254717350006
Val loss: 0.33244433999061584
Val loss: 0.3326057195663452
Val loss: 0.3328002691268921
Val loss: 0.3329327404499054
Val loss: 0.33306583762168884
Val loss: 0.3332417607307434
Val loss: 0.3334261178970337
Val loss: 0.3335794508457184
Val loss: 0.333758145570755
Val loss: 0.333903968334198
Val loss: 0.33403992652893066
Val loss: 0.3342289328575134
Val loss: 0.33439552783966064
Val loss: 0.3345114588737488
Val loss: 0.3346516489982605
Val loss: 0.3347887694835663
Val loss: 0.33491700887680054
Finished early stopping for task CHEMBL1614202, beginning testing
Meta testing on task: CHEMBL1794567
New best model for test task CHEMBL1794567 at epoch 1 with val loss 0.4429778754711151
Val loss: 0.4598607271909714
New best model for test task CHEMBL1794567 at epoch 3 with val loss 0.38028669357299805
Val loss: 0.4417862445116043
Val loss: 0.4515143036842346
Val loss: 0.38874639570713043
Val loss: 0.3843294382095337
Val loss: 0.3871055692434311
Val loss: 0.3937009572982788
Val loss: 0.3932196795940399
Val loss: 0.38509829342365265
New best model for test task CHEMBL1794567 at epoch 12 with val loss 0.38014036417007446
Val loss: 0.4446467012166977
Val loss: 0.441244512796402
Val loss: 0.3801889717578888
Val loss: 0.3987434357404709
Val loss: 0.4457833766937256
Val loss: 0.5279505401849747
Val loss: 0.4470546841621399
Val loss: 0.3859929293394089
Val loss: 0.39727982878685
Val loss: 0.38405194878578186
Val loss: 0.504909485578537
Val loss: 0.4460753947496414
Val loss: 0.39186929166316986
Val loss: 0.41131287813186646
Val loss: 0.44289183616638184
Val loss: 0.3847058564424515
Val loss: 0.39164309203624725
Val loss: 0.39700494706630707
Finished early stopping for task CHEMBL1794567, beginning testing
Meta testing on task: CHEMBL3215176
New best model for test task CHEMBL3215176 at epoch 1 with val loss 1.246079444885254
New best model for test task CHEMBL3215176 at epoch 2 with val loss 1.2415968179702759
New best model for test task CHEMBL3215176 at epoch 3 with val loss 1.236953854560852
New best model for test task CHEMBL3215176 at epoch 4 with val loss 1.2323975563049316
New best model for test task CHEMBL3215176 at epoch 5 with val loss 1.2277476787567139
New best model for test task CHEMBL3215176 at epoch 6 with val loss 1.2232489585876465
New best model for test task CHEMBL3215176 at epoch 7 with val loss 1.218441128730774
New best model for test task CHEMBL3215176 at epoch 8 with val loss 1.2135783433914185
New best model for test task CHEMBL3215176 at epoch 9 with val loss 1.2088041305541992
New best model for test task CHEMBL3215176 at epoch 10 with val loss 1.2041003704071045
New best model for test task CHEMBL3215176 at epoch 11 with val loss 1.1995728015899658
New best model for test task CHEMBL3215176 at epoch 12 with val loss 1.1946717500686646
New best model for test task CHEMBL3215176 at epoch 13 with val loss 1.1900309324264526
New best model for test task CHEMBL3215176 at epoch 14 with val loss 1.185451865196228
New best model for test task CHEMBL3215176 at epoch 15 with val loss 1.180874228477478
New best model for test task CHEMBL3215176 at epoch 16 with val loss 1.1764456033706665
New best model for test task CHEMBL3215176 at epoch 17 with val loss 1.171836495399475
New best model for test task CHEMBL3215176 at epoch 18 with val loss 1.1672351360321045
New best model for test task CHEMBL3215176 at epoch 19 with val loss 1.1628960371017456
New best model for test task CHEMBL3215176 at epoch 20 with val loss 1.1583019495010376
New best model for test task CHEMBL3215176 at epoch 21 with val loss 1.153854250907898
New best model for test task CHEMBL3215176 at epoch 22 with val loss 1.149674654006958
New best model for test task CHEMBL3215176 at epoch 23 with val loss 1.145462989807129
New best model for test task CHEMBL3215176 at epoch 24 with val loss 1.1413140296936035
New best model for test task CHEMBL3215176 at epoch 25 with val loss 1.1369616985321045
New best model for test task CHEMBL3215176 at epoch 26 with val loss 1.1325591802597046
New best model for test task CHEMBL3215176 at epoch 27 with val loss 1.1284300088882446
New best model for test task CHEMBL3215176 at epoch 28 with val loss 1.1246342658996582
New best model for test task CHEMBL3215176 at epoch 29 with val loss 1.1206505298614502
New best model for test task CHEMBL3215176 at epoch 30 with val loss 1.116895079612732
Finished early stopping for task CHEMBL3215176, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1963741
New best model for test task CHEMBL1963741 at epoch 1 with val loss 0.9433188041051229
New best model for test task CHEMBL1963741 at epoch 2 with val loss 0.923759917418162
New best model for test task CHEMBL1963741 at epoch 3 with val loss 0.9116822679837545
New best model for test task CHEMBL1963741 at epoch 4 with val loss 0.901009718577067
New best model for test task CHEMBL1963741 at epoch 5 with val loss 0.8830151359240214
New best model for test task CHEMBL1963741 at epoch 6 with val loss 0.8694775303204855
New best model for test task CHEMBL1963741 at epoch 7 with val loss 0.856753945350647
New best model for test task CHEMBL1963741 at epoch 8 with val loss 0.8464186191558838
New best model for test task CHEMBL1963741 at epoch 9 with val loss 0.8423961798350016
New best model for test task CHEMBL1963741 at epoch 10 with val loss 0.8319632013638815
New best model for test task CHEMBL1963741 at epoch 11 with val loss 0.829726497332255
New best model for test task CHEMBL1963741 at epoch 12 with val loss 0.8175194263458252
New best model for test task CHEMBL1963741 at epoch 13 with val loss 0.8107635378837585
New best model for test task CHEMBL1963741 at epoch 14 with val loss 0.8052404920260111
New best model for test task CHEMBL1963741 at epoch 15 with val loss 0.8022371133168539
New best model for test task CHEMBL1963741 at epoch 16 with val loss 0.7918244401613871
New best model for test task CHEMBL1963741 at epoch 17 with val loss 0.7891872326532999
New best model for test task CHEMBL1963741 at epoch 18 with val loss 0.7845431168874105
New best model for test task CHEMBL1963741 at epoch 19 with val loss 0.7816055019696554
New best model for test task CHEMBL1963741 at epoch 20 with val loss 0.7814536889394125
New best model for test task CHEMBL1963741 at epoch 21 with val loss 0.7778079907099406
New best model for test task CHEMBL1963741 at epoch 22 with val loss 0.7748414675394694
New best model for test task CHEMBL1963741 at epoch 23 with val loss 0.7697039445241293
New best model for test task CHEMBL1963741 at epoch 24 with val loss 0.765791138013204
New best model for test task CHEMBL1963741 at epoch 25 with val loss 0.757756769657135
Val loss: 0.7580822308858236
Val loss: 0.7618544499079386
New best model for test task CHEMBL1963741 at epoch 28 with val loss 0.7513605157534281
Val loss: 0.7516220410664877
New best model for test task CHEMBL1963741 at epoch 30 with val loss 0.7507175207138062
Finished early stopping for task CHEMBL1963741, beginning testing
Took 654.4464588165283 seconds to complete meta testing
Model test prc-auc = 0.527242
1-fold cross validation
Seed 0 ==> test prc-auc = 0.527242
Overall test prc-auc = 0.527242 +/- 0.000000
Total running time was 150312.1543059349 seconds
